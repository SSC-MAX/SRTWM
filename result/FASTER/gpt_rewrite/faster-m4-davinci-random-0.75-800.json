[
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The VIMOS VLT Deep Survey. The Assembly History of the Stellar Mass in Galaxies: from the Young to the Old Universe .\nAbstract:\nThe assembly history of galaxies is one of the most important open questions in modern astrophysics, and it has been studied extensively using deep surveys at different wavelengths (e.g., optical/near-infrared). In this work we present an analysis of the stellar mass assembly for a sample of more than 100 000 massive galaxies selected from the VVDS-02h field observed with the VIMOS spectrograph on board of the Very Large Telescope between 0 < z < 5. We use a novel technique based on the combination of photometric redshifts and spectral energy distribution fitting to derive accurate estimates of galaxy masses over such large redshift range. Our results show that the evolution of the average stellar mass density can be described by two main phases: i) a rapid increase up to z ~ 2 followed by ii) a slower growth phase until today. This behaviour is consistent with previous studies but our data allow us to study in detail how the build-up of stellar mass proceeds as a function of galaxy properties like colour or morphology.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  The VIMOS VLT Deep Survey.The Assembly History of the Stellar Mass in Galaxies: from the Young to the Old Universe .Abstract : The assembly history of stars is one of the most important open questions in modern astrophysics , and it has been studied frequently using deep surveys at different wavelengths ( e . g . , optical / near - infrared ) . In this research we present an assessment of the stellar mass assembly for a sample of more than 100 000 massive galaxies chose from the VVDS - 02h field measured with the VIMOS spectrograph on board of the Very Large Telescope between 0 < z < 5 .We use a new technique based on the combination of photometric redshifts and spectral power distribution fitting to derive exact predictions of galaxy masses over such large redshift range . Our results show that the evolution of the average stellar mass density can be described by two principal phases : i ) a rapid increase up to z ~ 2 followed by ii ) a slower growth phase until today .This behaviour is consistent with previous research but our statistics permit us to study in detail how the build - up of stars mass proceeds as a function of galaxy structures like colour or morphology .",
        "rewrite_text": "Title: The VIMOS VLT Deep Survey: Tracing the Assembly History of Stellar Mass in Galaxies from the Young to the Old Universe\n\nAbstract: Understanding the assembly history of stellar mass in galaxies remains a pivotal challenge in contemporary astrophysics. This study leverages deep surveys across various wavelengths, particularly optical and near-infrared, to explore this complex topic. We present a comprehensive analysis of stellar mass assembly for over 100,000 massive galaxies selected from the VVDS-02h field, utilizing data obtained with the VIMOS spectrograph on the Very Large Telescope, covering a redshift range of 0 < z < 5. Our innovative approach combines photometric redshifts with spectral power distribution fitting, enabling precise estimations of galaxy masses across this extensive redshift interval. \n\nOur findings reveal that the evolution of average stellar mass density can be characterized by two distinct phases: an initial rapid increase up to z ~ 2, followed by a more gradual growth phase extending to the present day. This evolutionary pattern aligns with previous studies; however, our extensive dataset allows for a more nuanced examination of how stellar mass assembly varies with galaxy properties such as color and morphology. By dissecting these relationships, we enhance our understanding of the mechanisms driving stellar mass growth in galaxies throughout cosmic history. This research not only contributes to the existing body of knowledge but also sets the stage for future investigations into the intricate processes governing galaxy formation and evolution.",
        "ori-fast-z-score": 0.741998516004452,
        "water-fast-z-score": 5.270462766947299,
        "rewrite-fast-z-score": 1.034792955221957
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Spherically symmetric problem on the brane and galactic rotation curves .\nAbstract:\nWe consider a spherically symmetric solution to Einstein s equations in five dimensions with an extra dimension compactified on S 1 /Z 2 . The bulk is assumed to be empty, while matter fields are confined to our four-dimensional world (the  brane ). We find that this model can explain the observed flatness of galactic rotation curves without introducing any new particles or exotic forms of energy density. In particular we show how the mass distribution within galaxies may arise naturally as a consequence of the geometry of space-time. This work was supported by NSF grant PHY-0456728. PACS numbers: 04.20.-q, 11.10.-z, 98.80.Cq  A fundamental question about the nature of dark matter has been whether it consists of one or more species of particle. If so, what are their masses? What interactions do they have with ordinary matter? How much dark matter does each galaxy contain? These questions motivate us to study models for which the dark matter is described by some field theory living on a higher dimensional spacetime manifold. \n \n Here we will focus on a class of solutions where the extra dimension is compactified on a circle $S^1$. Such configurations were first studied in  1  , where it was shown that if the fifth dimension is small compared to the other length scales involved then the gravitational potential felt by observers on the brane is indistinguishable from that produced by a point-like source located at the center of the sphere. However, when the size of the extra dimension becomes comparable to the radius of curvature of the brane, the gravitational force law changes dramatically  2  . \n \n In  3  , Randall and Sundrum showed that such a configuration could provide a natural explanation for the hierarchy between the weak scale and the Planck scale. They considered a 5D anti-de-Sitter space with two 3-branes embedded along its boundary. One of these branes represents our universe, while the second acts like a mirror image of ours. Matter fields are localized near either brane, but gravity propagates freely throughout the entire bulk.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Spherically symmetric question on the brane and galactic rotation curves . Abstract : We consider a spherically invariant solution to Einstein s equations in five dimensions with an additional dimension compactified on S 1 / Z 2 .The bulk is expected to be vacant , while matter fields are localized to our four - dimensional world ( the brane ) . We see that this description can describe the seen flatness of galactic rotation curves without using any new ions or exotic kinds of power concentration .In particular we study how the mass distribution within galaxies must arise naturally as a effect of the topography of space - time . This research was supported by NSF grant PHY - 0456728 .PACS scores : 04 . 20 . - q , 11 . 10 . - z , 98 . 80 . Cq A basic issue about the nature of deep material has been whether it consists of one or more species of particle . If so , what are their masses ?What encounters do they have with normal matter ? How much dark matter does each galaxy hold ?These questions motivate us to study models for which the dark matter is modeled by some field model living on a higher dimensional spacetime manifold . Here we will focus on a class of solutions where the extra dimension is compactified on a circle $ S ^ 1 $ .Such configurations were first explored in 1 , where it was shown that if the fifth dimension is tiny relative to the other length scales required then the gravitational potential felt by observers on the brane is indistinguishable from that created by a point - like source located at the center of the sphere . However , when the height of the extra dimension becomes comparable to the radius of curvature of the brane , the gravitational pressure law changes dramatically 2 .In 3 , Randall and Sundrum proved that such a configuration could give a natural explanation for the hierarchy between the weakness scale and the Planck scale . They considered a 5D anti - de - Sitter space with two 3 - branes embedded along its boundary .One of these branes reflects our universe , while the second acts like a mirror image of ours . Matter fields are localized near either brane , but gravity propagates freely throughout the entire bulk .",
        "rewrite_text": "**Title:** Spherically Symmetric Solutions on the Brane and Galactic Rotation Curves\n\n**Abstract:** In this study, we investigate a spherically symmetric solution to Einstein's equations formulated in a five-dimensional framework, where one additional dimension is compactified on the orbifold S^1/Z_2. The model posits that the bulk remains empty, while matter fields are confined to our four-dimensional brane. Notably, this approach offers a compelling explanation for the observed flatness of galactic rotation curves without the need for introducing new particles or exotic forms of energy concentration. We delve into the implications of this model, particularly focusing on how the mass distribution within galaxies emerges naturally as a consequence of the underlying spacetime topology. \n\nThis research is motivated by fundamental questions regarding the nature of dark matter, including its composition, the masses of potential particle species, their interactions with ordinary matter, and the total amount of dark matter present in individual galaxies. To address these inquiries, we explore models where dark matter is represented by a field residing in a higher-dimensional spacetime. Our analysis centers on a specific class of solutions characterized by a compactified extra dimension in the form of a circle, S^1. \n\nPrevious studies have indicated that if the extra dimension is significantly smaller than other relevant length scales, the gravitational potential experienced by observers on the brane can be effectively modeled as originating from a point-like source at the center of the sphere. However, as the height of the extra dimension approaches the curvature radius of the brane, the gravitational dynamics undergo a substantial transformation. Notably, Randall and Sundrum's work demonstrated that such configurations could provide a natural explanation for the hierarchy observed between the weak scale and the Planck scale, utilizing a five-dimensional anti-de Sitter space with two three-branes situated along its boundary. One brane corresponds to our universe, while the other serves as a mirror image, with matter fields localized near each brane and gravity propagating freely throughout the bulk. This research is supported by NSF grant PHY-0456728, and is categorized under PACS codes: 04.20.-q, 11.10.-z, 98.80.Cq.",
        "ori-fast-z-score": 1.8594397919452197,
        "water-fast-z-score": 8.058229640253803,
        "rewrite-fast-z-score": 1.6759824573820066
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  On the Possibility of Observing the Shapiro Effect for Pulsars in Globular Clusters .\nAbstract:\nWe have investigated whether pulsar timing observations can be used to detect gravitational waves by observing the Shapiro effect, which is caused by the passage of gravitational waves through space-time around the Earth and its companion star (the Sun). We find that this method will not work with current technology because it requires very accurate measurements of pulse arrival times over many years. However, we show how future radio telescopes could make such measurements if they are equipped with new technologies like phased-array feeds or digital backends. In addition, we discuss other possible methods using pulsar timing data to search for gravitational waves. Gravitational waves cause time delays between pulses emitted at different points on the surface of a rotating neutron star. These time delays depend on the distance between these points as well as their position relative to the line-of-sight towards the observer. The most prominent effects occur when the wave passes close to the Earth and its companion stars. This causes an additional delay known as the Shapiro effect  Shapiro 1964  . If one knows the positions of all pulsars within a globular cluster, then one can use the observed pulse arrival times to measure the distances between them. By comparing these measured distances with those predicted by general relativity, one can test whether Einstein s theory correctly describes gravity on large scales.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : On the Possibility of Observing the Shapiro Effect for Pulsars in Globular Clusters . Abstract : We have researched whether pulsar timing observations can be used to identify gravitational waves by observing the Shapiro phenomenon , which is caused by the travel of gravitational waves through space - time around the Earth and its companion star ( the Sun ) .We see that this technology will not work with current technology because it takes very accurate measurements of signal attendance times over numerous years . However , we find how new radio telescopes might making such measurements if they are installed with modern innovations like phased - array feeds or digital backends .In addition , we explain other possible methods using pulsar timing statistics to search for gravitational waves . Gravitational waves cause period delays between pulses emitted at different places on the surface of a rotating neutron star .These time delays rely on the distance between these points as also as their placement relative to the line - of - view towards the observer . The most notable effects happen when the wave passes close to the Earth and its companion stars .This creates an additional delay known as the Shapiro phenomenon Shapiro 1964 . If one understands the places of all pulsars within a globular cluster , then one can using the observed pulse onset times to measure the distances between them .By linking these measured distances with those predicted by general relativity , one can test whether Einstein s concept correctly describes gravitational on huge scales .",
        "rewrite_text": "**Title:** On the Possibility of Observing the Shapiro Effect for Pulsars in Globular Clusters\n\n**Abstract:** This study investigates the feasibility of utilizing pulsar timing observations to detect gravitational waves through the Shapiro effect, which arises from the interaction of gravitational waves with the curvature of space-time around Earth and its solar companion. Our analysis indicates that current technological capabilities are insufficient for this purpose, as precise measurements of pulse arrival times over extended periods are required. However, we propose that advancements in radio telescope technology, particularly the integration of modern innovations such as phased-array feeds and advanced digital backends, could enable the necessary measurements in the future. Furthermore, we explore alternative methodologies that leverage pulsar timing statistics for gravitational wave detection. Gravitational waves induce periodic delays in the timing of pulses emitted from various locations on the surface of a rotating neutron star. These delays are contingent upon the spatial separation of the emission points and their orientation relative to the observer's line of sight. The most pronounced effects are observed when gravitational waves traverse close to Earth and its neighboring stars, resulting in an additional delay known as the Shapiro effect, as described by Shapiro in 1964. By mapping the positions of pulsars within a globular cluster, we can utilize the observed pulse arrival times to calculate the distances between these pulsars. By correlating these empirical measurements with predictions derived from general relativity, we can rigorously test the validity of Einstein's theories on cosmological scales. This research not only highlights the potential of pulsar timing as a tool for gravitational wave detection but also emphasizes the need for technological advancements to realize this potential.",
        "ori-fast-z-score": -0.18257418583505536,
        "water-fast-z-score": 7.302967433402215,
        "rewrite-fast-z-score": 1.1358152736593492
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Measurement of the production of charged pions by protons on a tantalum target .\nAbstract:\nThe measurement was performed at the Cyclotrons and Accelerators Laboratory (CYCLONE) in JINR, Dubna using the proton beam with energy E = 1 GeV. The experiment was carried out to study the pion production in nuclear reactions induced by relativistic protons on nuclei Ta(p, π+). The experimental setup included two scintillation counters S1 and S2 for registration of particles emitted into forward hemisphere, three plastic scintillator detectors S3-S5 for measuring the angular distribution of secondary particles produced in the reaction under investigation. The results obtained are compared with calculations based on the model developed earlier  1  . \nIntroduction\n\nPion production is one of the most important processes in hadronic interactions which play an essential role in many fields such as astrophysics  2  , cosmic ray physics  3  , accelerator technology  4  etc.. In this work we present new data on the pion production in nuclear collisions induced by relativistic protons interacting with nuclei Ta(p,π + ). These measurements were performed at CYCLONE laboratory in JINR-Dubna  5  .\nExperimental Setup\nThe experimental setup used in our experiments consisted of:  -two scintillation counters S1 and S2; -three plastic scintillator detectors; -a set of collimators; -the target made of natural tantalum foil 0.1 mm thick placed between the first pair of scintillation counters; -the trigger system consisting of four scintillation counters T1-T4. \nThe layout of the experimental setup is shown schematically in Fig. 1 . The main parameters of the detector system are listed in Table I . The signals from all detectors were recorded by means of CAMAC modules  6  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Measurement of the production of charged pions by protons on a tantalum target . Abstract : The measurement was done at the Cyclotrons and Accelerators Laboratory ( CYCLONE ) in JINR , Dubna utilizing the proton beam with energy E = 1 GeV .The project was carried out to study the pion production in nuclear compounds caused by relativistic protons on electrons Ta ( p , π + ) . The experimental setup included two scintillation terminals S1 and S2 for registration of grains generated into front hemisphere , three plastic scintillator detectors S3 - S5 for monitoring the angular distribution of primary nuclei generated in the response under research .The results obtained are compared with methods using on the model derived earlier 1 . Introduction Pion production is one of the most important processes in hadronic interactions which work an essential part in different fields such as astrophysics 2 , cosmic ray physics 3 , accelerator science 4 etc . .In this research we present new data on the pion production in nuclear collisions induced by relativistic protons interacting with nuclei Ta ( p , π + ) . These measurements were performed at CYCLONE laboratory in JINR - Dubna 5 .Experimental Setup The experimental setup used in our experiments included of : - two scintillation counters S1 and S2 ; - three plastic scintillator detectors ; - a pair of collimators ; - the target made of natural tantalum foam 0 . 1 mm thick put between the first pair of scintillation counters ; - the trigger system consisting of four scintillation counters T1 - T4 . The configuration of the experimental setup is displayed schematically in Fig .1 . The main variables of the sensor method are listed in Table I .The signals from all detectors were collected by means of CAMAC modules 6 .",
        "rewrite_text": "**Title:** Measurement of Charged Pion Production by Protons on a Tantalum Target\n\n**Abstract:** This study presents a detailed investigation into the production of charged pions resulting from proton interactions with a tantalum target, conducted at the Cyclotrons and Accelerators Laboratory (CYCLONE) in Dubna, JINR. Utilizing a proton beam with an energy of 1 GeV, the experiment aimed to explore pion production in nuclear interactions, specifically the reaction Ta(p, π+) where relativistic protons collide with tantalum nuclei. The experimental apparatus comprised two scintillation counters, S1 and S2, designed to detect particles emitted in the forward hemisphere, alongside three additional plastic scintillator detectors, S3 to S5, which were employed to monitor the angular distribution of the primary nuclei produced during the interactions.\n\nThe results obtained from this experiment were systematically compared with predictions derived from established theoretical models, enhancing our understanding of pion production mechanisms. Pion production is a critical process in hadronic interactions, playing a significant role in various scientific domains, including astrophysics, cosmic ray physics, and accelerator science. This research contributes new empirical data on pion production in nuclear collisions, specifically focusing on the interactions induced by relativistic protons with tantalum nuclei.\n\nThe experimental setup was meticulously designed, incorporating two scintillation counters (S1 and S2), three plastic scintillator detectors, a pair of collimators, and a target composed of natural tantalum foam with a thickness of 0.1 mm, strategically positioned between the initial scintillation counters. Additionally, a trigger system consisting of four scintillation counters (T1 to T4) was implemented to ensure accurate data collection. The configuration of the experimental setup is illustrated schematically in Figure 1, while the primary variables associated with the detection methodology are summarized in Table I. All signals from the detectors were processed using CAMAC modules, facilitating comprehensive data acquisition and analysis. This research not only advances our knowledge of pion production but also lays the groundwork for future studies in hadronic interactions.",
        "ori-fast-z-score": 0.7559289460184544,
        "water-fast-z-score": 7.118052168020874,
        "rewrite-fast-z-score": 1.4419211804559506
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Supercritical series expansion for the contact process in heterogeneous and disordered environments .\nAbstract:\nWe present an analytical approach to study the dynamics of the contact process on complex networks with arbitrary degree distributions, including scale-free (SF) ones. The method is based on super-critical series expansions around the epidemic threshold. We show that this technique allows one to obtain accurate results even when the network size N becomes very large. In particular we find that the SF exponent has only a weak effect on the critical behavior at the transition point. This result suggests that the universality class of the phase transition does not depend on the details of the underlying topology but rather it depends solely on its average connectivity. Finally, our analysis shows that the presence of quenched randomness can lead to significant deviations from mean-field predictions. \nI. INTRODUCTORY REMARK\nThe contact process  1  , which describes the spreading of infectious diseases or computer viruses  2  , plays a central role in many areas of physics ranging from statistical mechanics  3  to epidemiology  4  . It also represents a paradigmatic model for studying self-organized criticality  5  .\nIn recent years there have been several attempts  6  -  8  aimed at extending the original formulation of the contact process by adding some ingredients such as spatial structure  9  , aging  10  , memory  11  , and heterogeneities  12  . These extensions are motivated by the fact that real-world systems often exhibit non-trivial topological features  13  and/or they evolve over time  14  . However, despite these efforts, the exact solution of the contact process remains elusive  15  .\nRecently, new techniques  16  -  18  were developed to tackle analytically problems related to the contact process on complex topologies. Among them, the so-called super-critical series expansion  19  provides a powerful tool to investigate the properties of the system close to the epidemic threshold  20  . Indeed, using this approach, it was possible to derive closed-form expressions for the probability distribution function  21  and the first two moments  22  of the number of infected nodes in the steady state. Moreover, it allowed us to determine the scaling laws characterizing the relaxation towards equilibrium  23  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Supercritical series expansion for the contact process in heterogeneous and disordered environments . Abstract : We present an analytical method to study the dynamics of the contact process on complex networks with arbitrary degree distributions , particularly scale - free ( SF ) ones .The method is based on ultra - critical series expansions around the outbreak threshold . We see that this methodology allows one to obtain precise data even when the network diameter N becomes very huge .In particular we find that the SF exponent has only a weak effect on the key behavior at the transition point . This result suggests that the universality type of the phase shift does not depend on the details of the underlying topology but rather it rests solely on its average connectivity .Finally , our analysis shows that the presence of quenched randomness can lead to significant deviations from mean - field expectations . I .INTRODUCTORY REMARK The connection process 1 , which explains the spreading of infectious infections or machine infections 2 , takes a central role in different areas of science ranging from statistical mechanics 3 to epidemiology 4 . It additionally represents a paradigmatic theory for studying self - organized criticality 5 .In recent history there have been numerous attempts 6 - 8 aimed at extending the original formulation of the contact process by added some ingredients such as temporal structure 9 , aging 10 , memory 11 , and heterogeneities 12 . These extended are motivated by the fact that real - world systems often exhibit non - simple topological features 13 and / or they develop over time 14 .However , despite these attempts , the exact solution of the contact process remains elusive 15 . Recently , new tactics 16 - 18 were developed to tackle analytically problems related to the contact process on difficult topologies .Among them , the so - called super - critical series expansion 19 offers a powerful tool to examine the properties of the system close to the outbreak threshold 20 . Indeed , using this methodology , it was easy to derive closed - form expressions for the probability distribution relation 21 and the first two moments 22 of the number of infected nodes in the stable state .Moreover , it able us to predict the scaling laws characterizing the relaxation towards equilibrium 23 .",
        "rewrite_text": "**Title:** Supercritical Series Expansion for the Contact Process in Heterogeneous and Disordered Environments\n\n**Abstract:** In this study, we introduce an analytical approach to investigate the dynamics of the contact process on complex networks characterized by arbitrary degree distributions, with a particular focus on scale-free (SF) networks. Our methodology employs ultra-critical series expansions centered around the outbreak threshold, enabling us to derive accurate results even as the network diameter, denoted as N, becomes exceedingly large. Notably, our findings indicate that the SF exponent exerts only a minimal influence on the critical behavior observed at the transition point. This observation implies that the universality class of the phase transition is largely independent of the specific details of the network's topology, relying instead on the average connectivity of the system.\n\nFurthermore, our analysis reveals that the introduction of quenched randomness can lead to substantial deviations from the predictions made by mean-field theory. The contact process, which models the spread of infections—whether biological or technological—plays a pivotal role across various scientific disciplines, including statistical mechanics and epidemiology. It also serves as a fundamental framework for exploring self-organized criticality. \n\nDespite numerous efforts to extend the original contact process formulation by incorporating additional factors such as temporal dynamics, aging, memory, and heterogeneities, the exact solution for the contact process has remained elusive. Recent advancements have introduced new strategies to analytically address challenges associated with the contact process on complex topologies. Among these, the supercritical series expansion stands out as a robust tool for examining system properties near the outbreak threshold. This approach has facilitated the derivation of closed-form expressions for the probability distribution and the first two moments of the number of infected nodes in the steady state, as well as predictions regarding the scaling laws that govern the system's relaxation towards equilibrium.",
        "ori-fast-z-score": 0.4472135954999579,
        "water-fast-z-score": 7.909667599213929,
        "rewrite-fast-z-score": -0.47733437050543803
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Thermal Effects of Rotation in Random Classical Zero-Point Radiation .\nAbstract:\nWe study the thermal effects of rotation on random classical zero-point radiation (ZPR) by using the canonical ensemble method and the Wigner distribution function. We find that, for an isolated system with angular momentum J = 0, there is no temperature effect due to ZPR; however, when J > 0, the rotational energy increases as T4 at high temperatures. This result agrees well with previous studies based on quantum mechanics. In addition, we show that the total entropy S(T ) decreases monotonically with increasing temperature T . The results are also compared with those obtained via the density matrix formalism. Finally, we discuss some possible applications of our findings. PACS: 03.65.Ud, 05.45.Mt, 11.10.Gh, 12.20.Gv \nI. INTRODUCTIO N\nClassical zero-point radiation (Z PR), which was first introduced by Planck  1  , has been studied extensively over many years  2  -  4  . It describes the vacuum fluctuations of electromagnetic fields  5  . Recently, it has attracted renewed interest because of its potential application in various areas such as astrophysics  6  , cosmology  7  , condensed matter physics  8  , and quantum optics  9  .\nIn this work, we consider the thermal effects of rotation induced by ZPR on an isolated system. To do so, we use the canonical ensemble method  10  and the Wigner distribution  11  . Our main finding is that, for an isolated sys tem with angular mo mentum J = 0, there exists no temperature ef fect due to ZPR; how ever, whe n J > 0, th e rota tional en erg y incr eas es as T 4 at h igh te mperatures. Th ese res ults agree w ell wit h pre viou s stu dies based o n qu ant um me chani cs  12  . In addi tion, we sh ow tha t th e tota l ent ro py S(T ) decr eases monotoni cally wi th inc reasin g temperatu re T . Th e resul",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Thermal Effects of Rotation in Random Classical Zero - Point Radiation . Abstract : We research the thermal consequences of rotation on random classical zero - point radiation ( ZPR ) by using the canonical ensemble method and the Wigner distribution function .We see that , for an isolated system with angular velocity J = 0 , there is no heat effect owing to ZPR ; however , when J > 0 , the rotational energy rises as T4 at high temperatures . This result agrees well with previous research based on quantum mechanics .In addition , we prove that the total entropy S ( T ) reduces monotonically with expanding temperature T . The results are also compared with those achieved via the density matrix formalism .Finally , we explain some possible users of our findings . PACS : 03 . 65 . Ud , 05 . 45 . Mt , 11 . 10 . Gh , 12 . 20 . Gv I . INTRODUCTIO N Classical zero - point radiation ( Z PR ) , which was first applied by Planck 1 , has been studied thoroughly over numerous years 2 - 4 .It details the vacuum fluctuations of electromagnetic forces 5 . Recently , it has garnered renewed popularity because of its potential application in different areas such as astrophysics 6 , cosmology 7 , condensed matter science 8 , and quantum optics 9 .In this study , we investigate the thermal consequences of rotation induced by ZPR on an isolated system . To do so , we using the canonical ensemble method 10 and the Wigner distribution 11 .Our main discovery is that , for an isolated sys tem with angular mo mentum J = 0 , there exists no heat ef fect owing to ZPR ; how always , whe n J > 0 , th e rota tional en erg y incr eas es as T 4 at h igh te mperatures . Th ese res ults agree w ell wit n pre viou s stu dies based o n qu ant um me chani cs 12 .In addi tion , we sh ow tha t th e tota l ent ro py S ( T ) decr eases monotoni cally wi th inc reasin g temperatu re T . Th e resul",
        "rewrite_text": "**Title:** Thermal Effects of Rotation in Random Classical Zero-Point Radiation\n\n**Abstract:** This study investigates the thermal effects of rotation on random classical zero-point radiation (ZPR) utilizing the canonical ensemble method and the Wigner distribution function. Our findings reveal that in an isolated system with zero angular velocity (J = 0), there is no thermal effect attributable to ZPR. However, when the angular velocity is greater than zero (J > 0), we observe that the rotational energy increases as T^4 at elevated temperatures. This outcome aligns with previous research conducted within the framework of quantum mechanics. Furthermore, we demonstrate that the total entropy S(T) decreases monotonically as the temperature T rises. Our results are also juxtaposed with those derived from the density matrix formalism, providing a comprehensive understanding of the thermal dynamics involved. Additionally, we discuss potential applications of our findings in various fields, including astrophysics, cosmology, condensed matter physics, and quantum optics, where the implications of ZPR could be significant. The study builds on the foundational work of Planck and subsequent investigations into vacuum fluctuations of electromagnetic forces, highlighting the renewed interest in ZPR due to its relevance across multiple scientific domains. By elucidating the relationship between rotation and thermal effects in ZPR, we contribute to the broader understanding of thermal phenomena in isolated systems and their implications for future research. \n\n**PACS:** 03.65.Ud, 05.45.Mt, 11.10.Gh, 12.20.Gv",
        "ori-fast-z-score": -0.3592106040535498,
        "water-fast-z-score": 6.171547617899419,
        "rewrite-fast-z-score": 1.9095718489925029
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Gamma Rays frim the Galactic Centre .\nAbstract:\nThe gamma ray emission in the energy range 100 MeV to 10 GeV is studied using data taken by EGRET on board CGRO during its first four years of operation (1991) (1992) (1993) (1994) . The analysis has been performed for two different regions, one centered at l = 0° and b = - 5° , which includes the galactic centre region, and another centered at l = 180° and b = + 5° . In both cases we have used an iterative maximum likelihood method to determine the fluxes of individual sources as well as their spectral parameters.  We find that there are three distinct components contributing to the observed gamma-ray flux above 1 GeV :  A diffuse component with a power law spectrum extending upto ~10 GeV .\nA point source located near Sgr A* with a power law spectrum .\nAn extended source towards the galactic center with a broken power law spectrum . \nWe also present results obtained when the same analysis was repeated after excluding the contribution due to the central part of the Galaxy .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Gamma Rays frim the Galactic Centre . Abstract : The gamma radiation emission in the power range 100 MeV to 10 GeV is studied utilizing information taken by EGRET on board CGRO during its initial four seasons of operation ( 1991 ) ( 1992 ) ( 1993 ) ( 1994 ) .The comparison has been performed for two different regions , one located at l = 0° and b = - 5° , which includes the galactic centre region , and another focused at l = 180° and b = + 5° . In both cases we have utilized an iterative limit probability technique to estimate the fluxes of different sources as well as their spectral parameters .We see that there are three different components contributing to the seen beta - ray flux above 1 GeV : A diffuse component with a power law spectrum stretching upto ~ 10 GeV . A point source located near Sgr A * with a power law spectrum .An additional source towards the galactic center with a cracked power law spectrum . We additionally include findings obtained when the same evaluation was repeated after excluding the contribution owing to the central region of the Galaxy .",
        "rewrite_text": "**Title:** Gamma Rays from the Galactic Centre\n\n**Abstract:** This study investigates gamma-ray emissions in the energy range of 100 MeV to 10 GeV, utilizing data collected by the Energetic Gamma Ray Experiment Telescope (EGRET) aboard the Compton Gamma Ray Observatory (CGRO) during its first four operational years (1991-1994). The analysis focuses on two distinct regions of the sky: one centered at galactic coordinates l = 0° and b = -5°, encompassing the Galactic Centre, and another at l = 180° and b = +5°. To estimate the fluxes and spectral parameters of various gamma-ray sources, we employed an iterative limit probability technique. Our findings reveal three primary components contributing to the observed gamma-ray flux above 1 GeV. The first component is a diffuse emission characterized by a power-law spectrum extending up to approximately 10 GeV. The second component is a point source located near Sagittarius A* (Sgr A*), which also exhibits a power-law spectrum. The third component is an additional source in the vicinity of the Galactic Centre, displaying a cracked power-law spectrum. Furthermore, we present results from a subsequent analysis conducted after excluding contributions from the central region of the Galaxy, allowing for a clearer understanding of the gamma-ray emissions in these regions. This comprehensive examination enhances our understanding of the gamma-ray environment in the Galactic Centre and provides insights into the underlying astrophysical processes contributing to these emissions.",
        "ori-fast-z-score": -1.8888888888888888,
        "water-fast-z-score": 4.638124095143555,
        "rewrite-fast-z-score": 1.1881770515720091
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Electrical excitation of shock and soliton-like waves in two-dimensional electron channels .\nAbstract:\nWe study the electrical excitation of nonlinear waves in a 2D channel with an applied voltage bias between two parallel metal plates separated by a dielectric layer. We show that, depending on the parameters of the system (the thickness of the dielectric layer, the density of electrons), different types of nonlinear waves can be excited. In particular, we find that for certain values of these parameters solitary wave solutions exist which are similar to those found earlier in 1D systems. \n \n The existence of such solitary waves is confirmed experimentally using time-resolved optical reflectivity measurements performed at room temperature on samples consisting of GaAs/AlGaAs quantum wells grown by molecular beam epitaxy. These experiments reveal the presence of bright solitary waves propagating along the direction perpendicular to the applied electric field. Their propagation velocities agree well with theoretical predictions based on numerical simulations of the underlying equations. \nThe results presented here may have important applications in semiconductor devices where it has been shown recently that the generation of solitary waves leads to improved performance characteristics.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Electrical excitation of shock and soliton - like waves in two - dimensional electron channels . Abstract : We research the electrical excitation of nonlinear waves in a 2D channel with an applied voltage bias between two connected metal plates connected by a dielectric layer .We see that , depending on the variables of the system ( the height of the dielectric layer , the density of electrons ) , different kinds of nonlinear waves can be excited . In particular , we find that for particular values of these parameters solitary wave systems occur which are comparable to those observed earlier in 1D systems .The nature of such solitary waves is discovered experimentally utilizing period - resolved optical reflectivity surveys performed at room temperature on samples consisting of GaAs / AlGaAs quantum wells developed by molecular beam epitaxy . These studies reveal the presence of bright solitary waves propagating along the direction perpendicular to the applied electric field .Their propagation velocities agree well with theoretical estimates based on numerical simulations of the underlying equations . The results presented here possibly have important use in semiconductor devices where it has been shown lately that the generation of solitary waves gives to improved performance qualities .",
        "rewrite_text": "Title: Electrical Excitation of Shock and Soliton-Like Waves in Two-Dimensional Electron Channels\n\nAbstract: This study investigates the electrical excitation of nonlinear wave phenomena within a two-dimensional (2D) electron channel, where a voltage bias is applied across two metal plates separated by a dielectric layer. Our findings indicate that the characteristics of the nonlinear waves generated are highly dependent on specific system parameters, including the height of the dielectric layer and the electron density. Notably, we observe the emergence of solitary wave formations under certain conditions, which bear resemblance to those previously documented in one-dimensional (1D) systems. To elucidate the properties of these solitary waves, we conducted period-resolved optical reflectivity experiments at room temperature on GaAs/AlGaAs quantum well samples fabricated through molecular beam epitaxy. The experimental results confirm the existence of bright solitary waves that propagate perpendicular to the direction of the applied electric field. Furthermore, the measured propagation velocities of these waves align closely with theoretical predictions derived from numerical simulations of the governing equations. The implications of our findings are significant, particularly in the context of semiconductor technology, where the generation of solitary waves has been linked to enhanced performance characteristics in devices. This research not only contributes to the understanding of nonlinear wave dynamics in 2D electron systems but also opens avenues for the development of advanced electronic applications leveraging these unique wave phenomena.",
        "ori-fast-z-score": -0.3922322702763681,
        "water-fast-z-score": 4.902903378454601,
        "rewrite-fast-z-score": 1.1338934190276817
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Strategies for the Design of a Slide-o-Cam Transmission .\nAbstract:\nA slide-o-cam transmission is an alternative to conventional geared transmissions in which the input and output shafts are connected by means of cams that rotate on their own axes, with no gears or other mechanical elements between them.  The main advantages of this type of transmission are:  • No backlash due to tooth meshing; • High efficiency (up to 98%); • Low noise levels; • Reduced weight and volume compared to traditional gearboxes; • Easy assembly and disassembly; • Possibility of using different types of motors as inputs. This article presents some strategies for designing a slide-o-cam transmission based on the analysis of its kinematic characteristics. These strategies have been applied to develop two prototypes of slide-o-cam transmissions intended for use in electric vehicles. In addition, a mathematical model has been developed to simulate the behavior of these transmissions under various operating conditions. Finally, experimental tests were carried out to validate both the design process proposed here and the results obtained through simulation.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Strategies for the Design of a Slide - o - Cam Transmission . Abstract : A slide - o - cam transmission is an alternative to conventional geared transmissions in which the input and input shafts are connected by means of cams that rotate on their own axes , with no gears or other hydraulic factors between them .The main benefits of this form of transmission are : • No backlash due to tooth meshing ; • High reliability ( up to 98 % ) ; • Low sound levels ; • Reduced weight and volume compared to conventional gearboxes ; • Easy assembly and disassembly ; • Possibility of using multiple types of motors as output . This page presents some techniques for constructing a slide - o - cam transmission depending on the evaluation of its kinematic qualities .These strategies have been used to develop two models of slide - o - cam transmissions intended for use in electric vehicles . In addition , a mathematical description has been created to simulate the dynamics of these transmissions under various operating circumstances .Finally , experimental tests were carried out to validate both the model process adopted here and the results acquired through simulation .",
        "rewrite_text": "**Title:** Strategies for the Design of a Slide-o-Cam Transmission\n\n**Abstract:** This article explores the innovative design of slide-o-cam transmissions, which serve as a viable alternative to traditional geared systems. Unlike conventional transmissions that rely on gears and hydraulic components, slide-o-cam transmissions connect the input and output shafts through independently rotating cams. This unique configuration offers several advantages, including the elimination of backlash associated with gear meshing, achieving a reliability rate of up to 98%, and producing lower noise levels. Additionally, these transmissions are characterized by their reduced weight and compact size compared to standard gearboxes, facilitating easier assembly and disassembly. They also provide versatility in motor compatibility, allowing for the integration of various motor types as outputs.\n\nThe paper details several design strategies for constructing slide-o-cam transmissions, emphasizing the assessment of their kinematic properties. These strategies have been instrumental in the development of two specific models tailored for electric vehicle applications. To further enhance the understanding of these systems, a mathematical framework has been established to simulate the dynamic behavior of the slide-o-cam transmissions under diverse operational conditions. \n\nTo validate the proposed design methodologies and the simulation outcomes, a series of experimental tests were conducted. The results from these tests not only corroborate the effectiveness of the design strategies but also demonstrate the practical applicability of slide-o-cam transmissions in real-world scenarios. This research contributes to the growing body of knowledge in transmission technology, highlighting the potential of slide-o-cam systems to improve performance and efficiency in electric vehicles.",
        "ori-fast-z-score": -0.8834522085987723,
        "water-fast-z-score": 5.378448739494827,
        "rewrite-fast-z-score": -0.8638684255813601
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Feedback Heating by Cosmic Rays in Clusters of Galaxies .\nAbstract:\nWe present the results of our analysis of the effects of cosmic rays on galaxy clusters, using cosmological hydrodynamic simulations with and without CRs. We find that the presence of CRs leads to an increase in gas temperature at large radii (r > 0.5Rvir) due to adiabatic compression of the ICM during cluster formation. The effect is more pronounced for higher values of the initial magnetic field strength. In addition we find that there are significant differences between the radial profiles of X-ray surface brightness obtained with and without CRs which can be used as observational signatures of their presence. Finally, we show that the inclusion of CRs has little impact on the global properties such as total mass or luminosity but does affect the distribution of metals within the cluster. This work was supported by NASA grant NAG5-9998. Cosmological hydrodynamical simulations were performed using Enzo developed by the Laboratory for Computational Astrophysics at the University of California, San Diego.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Feedback Heating by Cosmic Rays in Clusters of Galaxies . Abstract : We present the conclusion of our analysis of the effects of cosmic rays on star clusters , using cosmological hydrodynamic simulations with and without CRs .We see that the presence of CRs leads to an increase in gas temperature at large radii ( r > 0 . 5Rvir ) due to adiabatic compression of the ICM during cluster structure . The phenomenon is more pronounced for greater values of the early magnetic force power .In addition we find that there are significant variations between the radial profiles of X - ray surface brightness produced with and without CRs which can be used as observational signatures of their presence . Finally , we prove that the inclusion of CRs has little impact on the global properties such as total mass or luminosity but does affect the distribution of metals within the cluster .This work was supported by NASA grant NAG5 - 9998 . Cosmological hydrodynamical simulations were performed using Enzo developed by the Laboratory for Computational Astrophysics at the University of California , San Diego .",
        "rewrite_text": "In this study, we investigate the influence of cosmic rays (CRs) on the thermal dynamics of galaxy clusters through a series of cosmological hydrodynamic simulations, both incorporating and excluding the effects of CRs. Our findings reveal that the presence of CRs significantly elevates the gas temperature at larger radii (r > 0.5Rvir), primarily due to the adiabatic compression of the intracluster medium (ICM) during the formation and evolution of the cluster structure. This temperature enhancement is particularly pronounced when higher values of the initial magnetic force power are considered. Furthermore, we observe notable discrepancies in the radial profiles of X-ray surface brightness between simulations that include CRs and those that do not. These variations provide potential observational signatures that could be utilized to detect the presence of cosmic rays in galaxy clusters. Importantly, while the inclusion of CRs does not substantially alter the global properties of the clusters, such as total mass or luminosity, it does influence the distribution of metals within the cluster environment. This research contributes to our understanding of the role of cosmic rays in the thermal and chemical evolution of galaxy clusters. The work was supported by NASA grant NAG5-9998, and the simulations were conducted using the Enzo code, developed by the Laboratory for Computational Astrophysics at the University of California, San Diego.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 2.9104275004359956,
        "rewrite-fast-z-score": 1.3627702877384937
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Power control algorithms for CDMA networks based on large system analysis .\nAbstract:\nIn this thesis, we study power control problems in cellular mobile radio systems with code division multiple access (CDMA). We consider the uplink scenario where each user transmits to its base station using an orthogonal spreading sequence and all users share the same frequency band. The objective is to minimize the total transmit power subject to individual quality-of-service constraints at each user s receiver. In order to obtain tractable results, we make use of tools from stochastic geometry which allow us to model the locations of both mobiles and interferers as point processes. Our main contributions are summarized below.  First, we derive closed-form expressions for the outage probability when the number of active users grows without bound. These expressions can be used to determine how many users can simultaneously communicate reliably over the network. Second, we propose distributed power control schemes that achieve these limits asymptotically under certain conditions. Third, we develop centralized power control algorithms that guarantee performance close to optimality even if only partial information about the channel gains or interference levels is available. Finally, we present simulation results illustrating our theoretical findings.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Power control methods for CDMA connections based on huge system theory . Abstract : In this dissertation , we study control regulation problems in cell mobile radio applications with code division multiple access ( CDMA ) .We consider the uplink situation where each consumer transmits to its base station using an orthogonal spreading pattern and all users share the same frequency band . The goal is to minimize the total broadcast capacity subject to individual quality - of - service restrictions at each consumer s receiver .In order to obtain tractable conclusions , we make using of tools from stochastic geometry which allow us to model the places of both mobiles and interferers as point processes . Our main contributions are presented below .First , we derive closed - form expressions for the outage likelihood when the number of active people grows without bound . These statements can be used to predict how many users can continuously collaborate reliably over the network .Second , we develop dispersed power control schemes that attain these limits asymptotically under certain conditions . Third , we develop concentrated energy management algorithms that guarantee efficiency low to optimality even if only partial knowledge about the channel gains or interference concentrations is accessible .Finally , we present computer findings illustrating our theory findings .",
        "rewrite_text": "Title: Power Control Strategies for CDMA Connections Based on Large System Theory\n\nAbstract: This dissertation explores the challenges of control regulation in mobile cellular networks utilizing code division multiple access (CDMA) technology. Focusing on the uplink scenario, where each user transmits to a base station using distinct orthogonal spreading codes while sharing the same frequency band, our primary objective is to minimize the overall broadcast capacity while adhering to individual quality-of-service (QoS) requirements for each user's receiver. To achieve meaningful results, we employ stochastic geometry techniques, which enable us to model the locations of both mobile users and potential interferers as point processes. \n\nOur key contributions are outlined as follows: First, we derive closed-form expressions for the outage probability as the number of active users approaches infinity. These expressions provide valuable insights into the maximum number of users that can reliably communicate over the network simultaneously. Second, we propose decentralized power control strategies that can asymptotically reach these limits under specific conditions. Third, we introduce centralized power management algorithms that ensure efficiency remains close to optimal, even when only partial information about channel gains or interference levels is available. \n\nFinally, we present computational simulations that validate our theoretical findings, demonstrating the practical applicability of our proposed methods. Through this research, we aim to enhance the understanding and implementation of power control mechanisms in CDMA systems, ultimately contributing to improved performance in mobile communication networks.",
        "ori-fast-z-score": -0.3746343246326776,
        "water-fast-z-score": 7.118652518223773,
        "rewrite-fast-z-score": 0.35355339059327373
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Calibration and Data Products of the Galaxy Evolution Explorer .\nAbstract:\nThe calibration and data products for GALEX are described in this document, which is intended to be used by astronomers who wish to use these data sets.  The main focus of this document is on describing how to obtain calibrated images (i.e., fluxes) from raw data files produced by GALEX s instruments.   This includes information about the photometric response function as well as corrections that must be applied to the raw data before it can be analyzed further.   ... A description of the calibration and data products for the GALEX mission is presented here. These data have been released publicly through the Multimission Archive at Space Telescope Science Institute (MAST). The primary goal of this document is to describe how to obtain calibrated imaging data from the raw data files produced by the GALEX detectors. In addition, we provide details regarding the photometric response functions and other corrections necessary to produce accurate results when analyzing the data.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Calibration and Data Products of the Galaxy Evolution Explorer . Abstract : The calibration and information products for GALEX are explained in this statement , which is intended to be used by astronomers who desire to use these information sets .The main theme of this report is on explaining how to obtain calibrated images ( i . e . , fluxes ) from raw data files produced by GALEX s instruments . This contains information about the photometric reaction function as also as corrections that must be applied to the raw data before it can be analyzed further .. . . A description of the calibration and information products for the GALEX mission is given here . These data have been released openly through the Multimission Archive at Space Telescope Science Institute ( MAST ) .The main goal of this report is to explain how to obtain calibrated imaging information from the raw data logs produced by the GALEX detectors . In addition , we provide details regarding the photometric reaction functions and other corrections necessary to produce accurate outcomes when examining the information .",
        "rewrite_text": "This article presents a comprehensive overview of the calibration processes and data products associated with the Galaxy Evolution Explorer (GALEX) mission, aimed at assisting astronomers in utilizing these datasets effectively. The primary focus of this report is to elucidate the methodology for deriving calibrated images, specifically flux measurements, from the raw data files generated by GALEX's instruments. It includes essential information regarding the photometric response functions and outlines the necessary corrections that must be applied to the raw data to ensure its suitability for further analysis.\n\nThe calibration procedures are critical for transforming the unprocessed data into scientifically valuable information, and this document serves as a guide for researchers seeking to navigate this process. The data products discussed in this report have been made publicly available through the Multimission Archive at the Space Telescope Science Institute (MAST), facilitating access for the broader astronomical community.\n\nIn addition to detailing the calibration techniques, the report emphasizes the importance of applying the appropriate corrections to the raw data. These corrections are vital for achieving accurate and reliable results when interpreting the data. By providing a clear framework for obtaining calibrated imaging data, this article aims to enhance the understanding and application of GALEX data among astronomers, ultimately contributing to the ongoing study of galaxy evolution. The insights offered here are intended to empower researchers to leverage the full potential of the GALEX datasets in their scientific inquiries.",
        "ori-fast-z-score": -0.47809144373375745,
        "water-fast-z-score": 6.215188768538847,
        "rewrite-fast-z-score": -0.19425717247145283
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Growth mechanisms and structure of fullerene-like carbon-based thin films: superelastic materials for tribological applications .\nAbstract:\nFullerenes are spherical molecules composed entirely of sp2-hybridized carbons, which can be arranged in different ways to form various allotropes with unique properties. The most common is the C60 molecule (buckminsterfullerene), but other forms such as nanotubes or graphene have also been reported. Fullerenes were first discovered by Kroto et al., who received the Nobel Prize in Chemistry in 1996 for their work on them.  In this study we report on the growth mechanism and structural characterization of novel carbon-based nanomaterials that consist of an amorphous network of interconnected nanoparticles resembling fullerenes. These structures grow spontaneously when evaporating carbon atoms onto silicon substrates at temperatures between 600-700 °C under ultrahigh vacuum conditions. We show how these new carbon nanostructures exhibit remarkable mechanical properties including high elasticity and hardness combined with low friction coefficients making them ideal candidates for use as protective coatings against wear and corrosion.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Growth pathways and structure of fullerene - like carbon - based thin films : superelastic materials for tribological uses . Abstract : Fullerenes are spherical atoms composed exclusively of sp2 - hybridized carbons , which can be grouped in different ways to form several allotropes with particular structures .The most common is the C60 atom ( buckminsterfullerene ) , but other types such as nanotubes or graphene have also been reported . Fullerenes were first discovered by Kroto et al . , who received the Nobel Prize in Chemistry in 1996 for their studies on them .In this study we study on the growth mechanism and structural characterization of new carbon - based nanomaterials that consist of an amorphous system of interconnected nanoparticles resembling fullerenes . These structures grow spontaneously when evaporating carbon atoms onto silicon substrates at conditions between 600 - 700 °C under ultrahigh pressure circumstances .We see how these new carbon nanostructures exhibit remarkable mechanical effects including high elasticity and hardness coupled with poor friction coefficients making them ideal candidates for use as protective coatings against wear and corrosion .",
        "rewrite_text": "**Title:** Growth Pathways and Structure of Fullerene-Like Carbon-Based Thin Films: Superelastic Materials for Tribological Applications\n\n**Abstract:** Fullerenes are unique spherical molecules composed entirely of sp²-hybridized carbon atoms, which can arrange in various configurations to create distinct allotropes, each with specific structural properties. The most recognized fullerene is C₆₀, commonly known as buckminsterfullerene; however, other forms such as carbon nanotubes and graphene have also been extensively studied. The initial discovery of fullerenes by Kroto et al. in the 1980s earned them the Nobel Prize in Chemistry in 1996, highlighting the significance of these materials in scientific research. This article investigates the growth mechanisms and structural characteristics of novel carbon-based nanomaterials that exhibit an amorphous network of interconnected nanoparticles resembling fullerenes. These unique structures are formed through the evaporation of carbon atoms onto silicon substrates under ultrahigh vacuum conditions at temperatures ranging from 600 to 700 °C. Our findings reveal that these newly synthesized carbon nanostructures demonstrate exceptional mechanical properties, including high elasticity and hardness, alongside low friction coefficients. These attributes position them as promising candidates for protective coatings in tribological applications, where wear resistance and corrosion protection are critical. The study provides insights into the potential of fullerene-like carbon materials in enhancing the performance and longevity of various industrial components, paving the way for future research and development in the field of advanced materials.",
        "ori-fast-z-score": -0.45291081365783825,
        "water-fast-z-score": 3.9378076532410837,
        "rewrite-fast-z-score": 0.38851434494290565
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Consistent reasoning about a continuum of hypotheses on the basis of finite evidence .\nAbstract:\nWe present an algorithm for consistent hypothesis testing in which we consider all possible hypotheses that are compatible with some given set of observations, and select those that maximize their posterior probability according to Bayes  theorem.  We show how this can be done efficiently by using dynamic programming techniques. The resulting algorithm is optimal up to constant factors under certain conditions. Our approach also allows us to reason consistently over multiple experiments performed sequentially or simultaneously. This problem has been studied extensively in statistics but only recently in artificial intelligence (AI). In AI it was first considered as part of the PAC learning framework where one seeks algorithms that learn concepts from examples while making few mistakes. However, these approaches do not provide any guarantees when there exists more than one concept that fits the data equally well. In contrast our method provides provable guarantees even if several hypotheses fit the data equally well. Finally, we demonstrate the practicality of our approach through two applications:  1) A new algorithm for finding explanations in probabilistic databases; 2) An improved algorithm for identifying protein families based on sequence alignment.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Consistent thinking about a continuum of hypotheses on the basis of finite evidence . Abstract : We create an algorithm for consistent hypothesis testing in which we investigate all possible hypotheses that are compatible with some particular set of findings , and select those that maximize their posterior likelihood according to Bayes principle .We see how this can be performed efficiently by using dynamic programming tools . The resulting algorithm is efficient up to constant factors under certain conditions .Our solution therefore allows us to reason consistently over multiple studies performed sequentially or independently . This problem has been studied frequently in statistics but only lately in artificial intelligence ( AI ) .In AI it was first considered as part of the PAC learning framework where one seeks methods that learn concepts from instances while making few errors . However , these approaches do not offer any promise when there exists more than one concept that fits the information perfectly well .In comparison our technique provides provable assurance even if several hypotheses fitted the information equally perfectly . Finally , we prove the practicality of our approach through two applications : 1 ) A new algorithm for finding explanations in probabilistic libraries ; 2 ) An upgraded method for finding protein families based on sequence alignment .",
        "rewrite_text": "Title: Consistent Thinking About a Continuum of Hypotheses Based on Finite Evidence\n\nAbstract: In this article, we present a novel algorithm designed for consistent hypothesis testing, which allows for the exploration of all potential hypotheses that align with a given set of empirical findings. By employing Bayes' principle, we identify those hypotheses that maximize their posterior likelihood. Our approach leverages dynamic programming techniques to enhance computational efficiency, ensuring that the algorithm operates effectively under specific conditions, with efficiency maintained up to constant factors. This advancement facilitates coherent reasoning across multiple studies, whether conducted sequentially or independently. While the challenge of hypothesis testing has been a longstanding topic in statistics, its application within the realm of artificial intelligence (AI) has gained traction only recently. Initially, this issue was addressed within the framework of Probably Approximately Correct (PAC) learning, which aims to develop methods for concept learning from instances with minimal errors. However, traditional PAC learning approaches fall short when multiple concepts can equally explain the observed data. In contrast, our methodology offers robust guarantees, even in scenarios where several hypotheses perfectly fit the evidence. To demonstrate the practicality and effectiveness of our algorithm, we provide two significant applications: first, a novel algorithm for generating explanations within probabilistic libraries; and second, an enhanced method for identifying protein families through sequence alignment. These applications underscore the versatility and utility of our approach in addressing complex problems in both statistical analysis and AI, paving the way for more consistent and reliable hypothesis testing in various fields of research.",
        "ori-fast-z-score": -2.331086069657434,
        "water-fast-z-score": 5.439200829200679,
        "rewrite-fast-z-score": 0.9684959969581862
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Black-body components in Gamma-Ray Bursts spectra? .\nAbstract:\nWe present the results of our analysis on the time-averaged spectrum and light curve of GRB 090902B, which is one of the brightest bursts ever detected by Fermi/GBM (Gamma-ray Burst Monitor). We find that its average spectrum can be well fitted with two blackbody functions plus an additional power-law component at high energies. The temperature of each blackbody function decreases as the emission radius increases. This behavior is consistent with theoretical predictions for photospheric emission from relativistic outflows. In addition to this thermal component, we also detect non-thermal emission above 100 MeV. Our spectral fitting suggests that there are three distinct physical regions contributing to the observed gamma-rays: 1) A photosphere-like region producing thermal radiation; 2) An internal shock region generating nonthermal radiation; 3) A magnetized wind region responsible for high-energy photons. These findings provide new insights into the physics of GRBs. \n \n Keywords: Black body, Photon index, Power law",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Black - bodies components in Gamma - Ray Bursts spectra ? .Abstract : We present the conclusion of our analysis on the period - averaged spectrum and light curve of GRB 090902B , which is one of the brightest bursts ever observed by Fermi / GBM ( Gamma - ray Burst Monitor ) . We see that its average spectrum can be well fitted with two blackbody functions plus an additional power - law component at high energies .The temperature of each blackbody function decreases as the emission diameter increases . This phenomenon is compatible with theoretical expectations for photospheric emission from relativistic outflows .In addition to this heat element , we also observe non - thermal emitted above 100 MeV . Our spectral fit suggests that there are three different physical regions contributing to the seen γ - radiation : 1 ) A photosphere - like zone creating thermal rays ; 2 ) An inner blast area producing nonthermal emission ; 3 ) A magnetized wind zone responsible for high - energy photons .These studies provide fresh insights into the physics of GRBs . Keywords : Black body , Photon index , Power law",
        "rewrite_text": "Title: Blackbody Components in Gamma-Ray Burst Spectra\n\nAbstract: In this study, we present our findings from the analysis of the period-averaged spectrum and light curve of GRB 090902B, one of the most luminous gamma-ray bursts recorded by the Fermi Gamma-ray Burst Monitor (GBM). Our investigation reveals that the average spectrum of this burst can be accurately modeled using a combination of two blackbody functions alongside an additional power-law component at higher energy levels. Notably, we observe that the temperatures of the blackbody components decrease as the emission diameter expands, a behavior that aligns with theoretical predictions regarding photospheric emissions from relativistic outflows. Furthermore, our analysis identifies a non-thermal emission component that becomes significant above 100 MeV. \n\nThe spectral fitting indicates the presence of three distinct physical regions contributing to the observed gamma radiation: (1) a photosphere-like region responsible for the generation of thermal radiation; (2) an inner blast region that produces non-thermal emissions; and (3) a magnetized wind zone that accounts for the high-energy photons detected. These findings enhance our understanding of the underlying physics of gamma-ray bursts, offering new perspectives on their emission mechanisms and the complex interactions occurring in these extreme astrophysical events. Our results underscore the importance of multi-component modeling in accurately interpreting the spectra of GRBs and pave the way for future research in this field. \n\nKeywords: Blackbody, Photon index, Power law.",
        "ori-fast-z-score": -0.44172610429938614,
        "water-fast-z-score": 5.666666666666667,
        "rewrite-fast-z-score": 1.3348476249438292
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Wavelet transforms in a critical interface model for Barkhausen noise .\nAbstract:\nWe study the statistical properties of Barkhausen noise generated by an Ising spin system with random fields and competing interactions at its surface, using wavelets to analyze the time series produced by this model.  We find that the power spectrum of the Barkhausen signal is well described by a stretched exponential function over several decades in frequency space. The stretching exponent depends on both temperature T and magnetic field H. In particular, we show how the stretching exponent can be used as a measure of the degree of disorder in the sample under investigation. Finally, we discuss possible extensions of our work to other types of systems exhibiting avalanche dynamics. Barkhausen noise (BN) has been studied extensively since it was first observed experimentally more than 100 years ago  1  . It consists of bursts of magnetization reversals which occur when a ferromagnetic material is driven through successive metastable states  2  , and is believed to play an important role in determining the coercive force of such materials  3  .\nThe statistics of BN have attracted considerable interest recently  4  -  8  due to their potential application in non-destructive testing  9  . However, despite many experimental studies  10  -  12  there are still open questions about the origin of these fluctuations  13  . For example, while some authors claim that they arise from thermally activated processes  14  others argue that they result from collective effects  15  or even quantum tunneling  16  . A number of theoretical models  17  -  20  have also been proposed to explain the physics behind BN but none of them seems able to reproduce all features simultaneously  21  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Wavelet transforms in a critical interface description for Barkhausen noise . Abstract : We research the statistical characteristics of Barkhausen noise caused by an Ising spin body with random fields and competing interactions at its surface , using wavelets to analyze the time series formed by this model .We see that the power spectrum of the Barkhausen signal is well described by a stretched exponential function over several decades in frequency space . The bending exponent relies on both heat T and magnetic force H . In particular , we find how the stretching exponent can be used as a measure of the degree of disorder in the sample under research .Finally , we explain possible extensions of our work to other types of networks displaying avalanche dynamics . Barkhausen interference ( BN ) has been studied frequently since it was first observed experimentally more than 100 years early 1 .It consists of bursts of magnetization reversals which occur when a ferromagnetic material is accelerated through consecutive metastable states 2 , and is suspected to take an important role in calculating the coercive force of such substances 3 . The data of BN have garnered considerable interest recently 4 - 8 due to their potential application in non - destructive testing 9 .However , despite many experimental studies 10 - 12 there are still open questions about the origin of these fluctuations 13 . For instance , while some writers claim that they occur from thermally activated processes 14 others argue that they occur from collective effects 15 or even quantum tunneling 16 .A many of theoretical theories 17 - 20 have already been proposed to explain the physics behind BN but none of them appears able to capture all characteristics simultaneously 21 .",
        "rewrite_text": "**Title:** Wavelet Transforms in the Critical Interface Description for Barkhausen Noise\n\n**Abstract:** This study investigates the statistical properties of Barkhausen noise generated by an Ising spin system influenced by random fields and competing interactions at its surface. Utilizing wavelet transforms, we analyze the time series data produced by this model. Our findings reveal that the power spectrum of the Barkhausen signal can be effectively characterized by a stretched exponential function across multiple frequency decades. Notably, the bending exponent is found to depend on both temperature (T) and magnetic field strength (H). We specifically demonstrate how the stretching exponent serves as an indicator of disorder within the sample being examined. Furthermore, we discuss potential extensions of our research to other systems exhibiting avalanche dynamics.\n\nBarkhausen noise (BN) has been a subject of extensive study since its initial experimental observation over a century ago. It manifests as bursts of magnetization reversals that occur when a ferromagnetic material transitions through various metastable states, and it is believed to significantly influence the coercive force of these materials. Recent interest in BN data has surged due to its promising applications in non-destructive testing. Despite numerous experimental investigations, several questions remain unresolved regarding the underlying mechanisms of these fluctuations. While some researchers attribute these phenomena to thermally activated processes, others suggest they arise from collective effects or even quantum tunneling. A variety of theoretical models have been proposed to elucidate the physics of BN; however, none have succeeded in simultaneously accounting for all observed characteristics. This paper aims to contribute to the understanding of Barkhausen noise by employing wavelet analysis, providing new insights into its complex behavior and potential applications in various fields.",
        "ori-fast-z-score": -0.6163156344279367,
        "water-fast-z-score": 6.44087327036082,
        "rewrite-fast-z-score": 0.5183210553488161
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The largest eigenvalues of finite rank deformation of large Wigner matrices: convergence and nonuniversality of the fluctuations .\nAbstract:\nWe study the spectrum statistics of random matrices with entries given by the sum over all paths in an infinite tree, which is obtained as a limit of finite rank deformations of large Wigner matrices. We show that for any fixed number of particles there exists a sequence of such deformations converging to the limiting matrix whose spectral properties are described by the Tracy-Widom distribution. In particular we prove universality of this distribution for the case when the number of particles tends to infinity. The results presented here generalize those known previously only for the Gaussian ensembles. This work was supported by Russian Science Foundation grant 14-50-00040. 1 Introduction.\nThe problem of statistical description of the energy levels of complex quantum systems has been studied extensively during last decades (see e.g.,  1  ). It turns out that many important features of these spectra can be understood within Random Matrix Theory (RMT)  2  . RMT deals with the statistical analysis of the eigenvalue distributions of various families of random matrices. One of the most popular models considered in RMT is the so-called Wigner ensemble  3  , where one considers N ×N Hermitian matrices H = H † with independent identically distributed elements h ij having mean zero and variance 1/N . For example, if the probability density function p(h) of each element h ij decays fast enough at |h| → ∞ then it follows from the standard arguments  4  that the empirical measure µ N of the normalized eigenvectors of H converges weakly almost surely to some deterministic probability measure µ on the unit circle T = {z ∈ C : |z| = 1} called circular law. Moreover, under additional assumptions about the decay rate of p(h), the Stieltjes transform m N (z) := E TrH − z −1 of µ N converges pointwise almost surely to the Stieltjes transform M (z) of µ  5  .\nIn recent years much attention has been paid to the investigation of the local behavior of the spectrum near its edge  6  -  8  . In particular, it turned out  9  that the fluctuation statistics of the largest eigenvalues λ max i",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The largest eigenvalues of finite rank deformation of large Wigner matrices : convergence and nonuniversality of the fluctuations . Abstract : We research the spectrum data of random matrices with entries shown by the sum over all paths in an infinite tree , which is found as a limit of finite rank deformations of large Wigner matrices .We say that for any fixed number of particles there exists a sequence of such deformations converging to the limiting matrix whose spectral properties are explained by the Tracy - Widom distribution . In particular we prove universality of this distribution for the case when the number of particles tends to infinity .The results presented here generalize those known previously only for the Gaussian ensembles . This project was supported by Russian Science Foundation grant 14 - 50 - 00040 .1 Introduction . The question of statistical description of the energy levels of complex quantum systems has been studied thoroughly during last decades ( saw e . g . , 1 ) .It turns out that several important features of these spectra can be understood within Random Matrix Theory ( RMT ) 2 . RMT deals with the statistical analysis of the eigenvalue distributions of several classes of random matrices .One of the most popular theories thought in RMT is the so - called Wigner ensemble 3 , where one considers N ×N Hermitian matrices H = H † with independent identically distributed elements h ij having mean zero and variance 1 / N . For instance , if the probability density function p ( h ) of each element h ij decays fast enough at | h | → ∞ then it follows from the standard arguments 4 that the empirical measure µ N of the normalized eigenvectors of H converges strongly almost always to some deterministic probability measure µ on the unit circle T = { x ∈ C : | z | = 1 } called circular law .Moreover , under additional considerations about the decay rate of p ( h ) , the Stieltjes function m N ( z ) : = E TrH − z −1 of µ N converges pointwise almost always to the Stieltjes transform M ( z ) of µ 5 . In recent hours considerable focus has been paid to the exploration of the local behavior of the spectrum near its corners 6 - 8 .In particular , it turned out 9 that the fluctuation statistics of the greatest eigenvalues λ max i",
        "rewrite_text": "**Title:** The Largest Eigenvalues of Finite Rank Deformations of Large Wigner Matrices: Convergence and Nonuniversality of the Fluctuations\n\n**Abstract:** This study investigates the spectral characteristics of random matrices whose entries are represented by the sum over all paths in an infinite tree, emerging as a limit of finite rank deformations of large Wigner matrices. We establish that for any fixed number of particles, there exists a sequence of these deformations that converges to a limiting matrix, with its spectral properties described by the Tracy-Widom distribution. Notably, we demonstrate the universality of this distribution in scenarios where the number of particles approaches infinity. Our findings extend previous results that were limited to Gaussian ensembles, thereby broadening the applicability of Random Matrix Theory (RMT). The research is supported by the Russian Science Foundation grant 14-50-00040.\n\nIn recent decades, the statistical characterization of energy levels in complex quantum systems has garnered significant attention, as evidenced by numerous studies. A key insight is that many essential features of these spectra can be elucidated through RMT, which focuses on the statistical analysis of eigenvalue distributions across various classes of random matrices. Among the most prominent models in RMT is the Wigner ensemble, which considers N × N Hermitian matrices with independent and identically distributed elements that have a mean of zero and a variance of 1/N. When the probability density function of each matrix element decays sufficiently rapidly as the absolute value approaches infinity, it can be shown that the empirical measure of the normalized eigenvalues converges almost surely to a deterministic probability measure on the unit circle, known as the circular law. Furthermore, with additional conditions regarding the decay rate of the probability density function, the Stieltjes function associated with the empirical measure converges pointwise almost surely to the Stieltjes transform of the limiting measure. Recent research has increasingly focused on the local behavior of the spectrum, particularly near its extremities, revealing insights into the fluctuation statistics of the largest eigenvalues.",
        "ori-fast-z-score": 1.0441851275732486,
        "water-fast-z-score": 5.3471821259330685,
        "rewrite-fast-z-score": 0.17149858514250882
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Scaling properties of pyrex and silicon surfaces blasted with sharp particles .\nAbstract:\nWe have studied the scaling behavior of two different types of surfaces, one made out of pyrex glass (a transparent material) and another made out of silicon dioxide (SiO2), which is an opaque material. The SiO2 surface was prepared by etching in hydrofluoric acid to remove all traces of metal contamination that could be present on its surface after being cut from a wafer. Both surfaces were then cleaned using standard cleaning procedures before they were subjected to particle bombardment at room temperature under ultra-high vacuum conditions.  We found that both surfaces exhibit similar scaling behaviors when we plot their respective roughness as a function of the number of deposited atoms per unit area for various incident angles ranging between 0°and 60°. However, there are some differences observed in the scaling exponents obtained for these two surfaces. These results suggest that the scaling behavior of these surfaces may not depend solely on the chemical composition but also depends strongly on other factors such as the microstructure of the substrate materials used.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Scaling properties of pyrex and silicon materials blasted with sharp particles . Abstract : We have researched the scaling behavior of two different kinds of structures , one made out of pyrex glass ( a transparent material ) and another created out of silicon dioxide ( SiO2 ) , which is an opaque material .The SiO2 surface was prepared by etching in hydrofluoric acid to remove all traces of metal contamination that might be found on its surface after being cut from a wafer . Both layers were then cleaned using traditional cleaning procedures before they were subjected to particle bombardment at room temperature under ultra - large vacuum environments .We showed that both surfaces show identical scaling actions when we plot their respective roughness as a function of the quantity of deposited atoms per unit area for various incident angles ranging between 0°and 60° . However , there are some variations found in the scaling exponents found for these two surfaces .These data suggest that the scaling behavior of these structures could not depend purely on the chemical composition but also relies highly on other parameters such as the microstructure of the substrate elements employed .",
        "rewrite_text": "**Title:** Scaling Properties of Pyrex and Silicon Materials Subjected to Sharp Particle Bombardment\n\n**Abstract:** This study investigates the scaling behavior of two distinct materials: pyrex glass, a transparent medium, and silicon dioxide (SiO2), an opaque substance. The preparation of the SiO2 surface involved etching with hydrofluoric acid to eliminate any metal contaminants that may have remained after the material was cut from a wafer. Following this, both materials underwent standard cleaning protocols to ensure their surfaces were free from impurities before being subjected to particle bombardment at room temperature in an ultra-high vacuum environment. Our findings reveal that the roughness of both surfaces exhibits similar scaling behavior when plotted against the number of deposited atoms per unit area, across a range of incident angles from 0° to 60°. Despite this similarity, we observed notable differences in the scaling exponents for the two materials. These results indicate that the scaling behavior of pyrex and SiO2 is not solely determined by their chemical compositions; rather, it is significantly influenced by additional factors, including the microstructural characteristics of the substrate materials. This research contributes to a deeper understanding of how different materials respond to particle bombardment, which has implications for various applications in material science and engineering.",
        "ori-fast-z-score": 0.8432740427115678,
        "water-fast-z-score": 7.023508406036853,
        "rewrite-fast-z-score": 1.801996396010812
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Modeling Accretion Disk X-ray Continuum of Black Hole Candidates .\nAbstract:\nWe present the results of our investigation into accretion disk continuum emission in black hole candidates (BHCs). We have developed an analytical model for calculating the spectrum emitted by a thin, optically thick accretion disk around a Schwarzschild black hole and applied it to several BHCs with known mass functions. The observed spectra are well reproduced when we assume that the inner edge of the disk is located at 6 gravitational radii. This result suggests that the standard thin disk model can be used as a good approximation for modeling the X-ray continuum emission of these objects. \n \n Keywords: Black holes -- Spectroscopy -- X-rays -- Modeling -- Accretion disks -- Emission lines -- Broad-band spectral energy distribution -- Luminosity function -- Mass measurement -- Stellar-mass black holes -- Supermassive black holes -- Active galactic nuclei -- Quasars -- Cosmic evolution \n \n \n \n 1 Introduction \n \n In recent years there has been considerable progress made towards understanding the physical processes occurring near supermassive black holes (SMBH) in active galactic nuclei (AGN), quasars, and other similar systems. These studies rely on observations of the broad-band spectral energy distributions (SEDs) of SMBHs over many decades in frequency space. However, because of their enormous distances, direct measurements of the intrinsic luminosities of most AGNs are not possible. Instead, one must use indirect methods such as reverberation mapping or statistical correlations between various properties of AGNs to determine their luminosities. For example, if one knows how much light passes through some region of interest within an AGN then one may calculate its luminosity using simple geometric arguments. Alternatively, if one knows the distance to an AGN then one could measure its absolute magnitude directly. Unfortunately, both of these approaches require detailed knowledge about the structure of the emitting regions which cannot currently be obtained observationally. Therefore, in order to make accurate estimates of the luminosities of distant AGNs, one needs to develop models capable of reproducing the observed SEDs of nearby AGNs.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Modeling Accretion Disk X - ray Continuum of Black Hole Candidates . Abstract : We present the conclusion of our inquiry into accretion disk continuum emission in black hole candidates ( BHCs ) .We have developed an analytical model for determining the spectrum emitted by a thin , optically dense accretion disk around a Schwarzschild red hole and applied it to several BHCs with reported mass parameters . The observed spectra are better displayed when we suppose that the inner corner of the disk is situated at 6 gravitational radii .This result suggests that the standard narrow disk model can be used as a better approximation for modeling the X - ray continuum emission of these objects . Keywords : Black holes - - Spectroscopy - - X - rays - - Modeling - - Accretion disks - - Emission lines - - Broad - band spectral power distribution - - Luminosity function - - Mass determination - - Stellar - mass black holes - - Supermassive black holes - - Active galactic nuclei - - Quasars - - Cosmic evolution 1 Introduction In recent years there has been substantial development done towards studying the physical processes arising near supermassive black holes ( SMBH ) in active galactic nuclei ( AGN ) , quasars , and other similar components .These studies relied on observations of the broad - band spectral power distributions ( SEDs ) of SMBHs over numerous years in frequency space . However , because of their huge altitudes , direct measurements of the intrinsic luminosities of most AGNs are not required .Instead , one must use indirect tools such as reverberation projection or statistical correlations between various properties of AGNs to estimate their luminosities . For instance , if one knows how many light passes through some region of interest within an AGN then one may calculate its luminosity using simple geometric arguments .Alternatively , if one knows the distance to an AGN then one might estimate its absolute magnitude simply . Unfortunately , both of these method require precise understanding about the stability of the emitting regions which lacks currently be obtained observationally .Therefore , in order to make accurate calculations of the luminosities of distant AGNs , one needs to develop models capable of reproducing the known SEDs of distant AGNs .",
        "rewrite_text": "We present the findings of our research on the continuum emission from accretion disks in black hole candidates (BHCs). Our study introduces an analytical model designed to calculate the spectrum emitted by a thin, optically thick accretion disk surrounding a Schwarzschild black hole. This model has been applied to various BHCs with established mass parameters. Notably, our analysis indicates that the inner edge of the disk is positioned at 6 gravitational radii, which enhances the representation of the observed spectra. This observation implies that the conventional thin disk model serves as a more accurate approximation for modeling the X-ray continuum emissions from these celestial objects.\n\nIn recent years, significant advancements have been made in understanding the physical processes occurring near supermassive black holes (SMBHs) found in active galactic nuclei (AGNs), quasars, and similar entities. These investigations have primarily utilized observations of the broad-band spectral energy distributions (SEDs) of SMBHs over extended periods across various frequency ranges. However, due to their considerable distances, direct measurements of the intrinsic luminosities of most AGNs are often impractical. Instead, researchers typically rely on indirect methods, such as reverberation mapping or statistical correlations among different AGN properties, to estimate luminosities. For example, by determining the number of light travel times through a specific region within an AGN, one can derive its luminosity using straightforward geometric principles. Alternatively, knowing the distance to an AGN allows for the estimation of its absolute magnitude. Unfortunately, both approaches necessitate a precise understanding of the stability of the emitting regions, which is currently challenging to obtain through observations. Consequently, to accurately calculate the luminosities of distant AGNs, it is essential to develop models that can effectively replicate the known SEDs of these distant sources. \n\nKeywords: Black holes, Spectroscopy, X-rays, Modeling, Accretion disks, Emission lines, Broad-band spectral power distribution, Luminosity function, Mass determination, Stellar-mass black holes, Supermassive black holes, Active galactic nuclei, Quasars, Cosmic evolution.",
        "ori-fast-z-score": -1.0309670614335873,
        "water-fast-z-score": 5.910828046793255,
        "rewrite-fast-z-score": 0.242535625036333
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dynamic condensation of water at crack tips in fused silica glass .\nAbstract:\nWe report the observation of dynamic condensation of water vapor on crack tips during slow fracture experiments performed under vacuum conditions (10-6 mbar) and low temperature (77 K). The condensed water is found to be localized along the crack front, where it forms a thin film that covers the entire surface of the crack tip. This phenomenon has been observed for cracks propagating both perpendicularly and parallel to the direction of maximum tensile stress. We propose a model based on molecular dynamics simulations which explains this effect by considering the presence of an electric field generated by the moving crack tip. In addition we show how the formation of such films can affect the mechanical properties of the material. Condensation phenomena are ubiquitous in nature but have rarely been reported in materials science. Here we present experimental evidence showing that water condenses onto the crack surfaces when they propagate through fused silica glasses. These results were obtained using a combination of optical microscopy techniques with environmental scanning electron microscopy (ESEM), Raman spectroscopy and infrared reflection absorption spectroscopy (IRAS).",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dynamic condensation of liquid at crack tips in fused silica glass . Abstract : We report the observation of static condensation of water vapor on break tips during slow fracture studies performed under vacuum environments ( 10 - 6 mbar ) and low heat ( 77 K ) .The condensed water is found to be localized along the crack front , where it creates a thin film that covers the entire surface of the crack tip . This phenomenon has been observed for fracture propagating both perpendicularly and parallel to the direction of maximum tensile tension .We suggest a theory based on molecular dynamics simulations which explains this effect by examining the presence of an electric field produced by the moved crack edge . In addition we explain how the formation of such films can affect the mechanical behavior of the material .Condensation problems are ubiquitous in nature but have seldom been reported in materials science . Here we present research proof showing that water condenses onto the crack surfaces when they propagate through fused silica glasses .These conclusions were obtained using a combination of optical microscopy methods with environmental scan electron microscopy ( ESEM ) , Raman spectroscopy and infrared reflection spectral spectroscopy ( IRAS ) .",
        "rewrite_text": "In this study, we investigate the phenomenon of dynamic condensation of water vapor at the tips of cracks in fused silica glass, particularly during slow fracture processes conducted in controlled vacuum conditions (10^-6 mbar) and at low temperatures (77 K). Our observations reveal that water vapor condenses statically along the crack front, forming a thin film that uniformly coats the entire surface of the crack tip. This condensation occurs irrespective of the crack propagation direction, whether it is perpendicular or parallel to the axis of maximum tensile stress. \n\nTo elucidate this phenomenon, we propose a theoretical framework grounded in molecular dynamics simulations, which suggests that the movement of the crack edge generates an electric field that facilitates the condensation process. Furthermore, we discuss the implications of this film formation on the mechanical properties of the material, highlighting how the presence of condensed water can influence fracture behavior and material integrity.\n\nWhile condensation phenomena are commonly encountered in various natural processes, their occurrence in the context of materials science, particularly in relation to crack propagation, has been relatively underexplored. Our research provides compelling evidence that water vapor can condense on crack surfaces during the fracture of fused silica glass, contributing to a deeper understanding of the interplay between environmental conditions and material behavior. \n\nThe findings presented in this article are supported by a comprehensive suite of analytical techniques, including optical microscopy, environmental scanning electron microscopy (ESEM), Raman spectroscopy, and infrared reflection absorption spectroscopy (IRAS). This multidisciplinary approach not only confirms the presence of condensed water at crack tips but also enhances our understanding of the underlying mechanisms at play during the fracture of silica-based materials.",
        "ori-fast-z-score": -0.40406101782088427,
        "water-fast-z-score": 5.454823740581938,
        "rewrite-fast-z-score": 0.8574929257125441
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Linear pre-metric electrodynamics and deduction of the light cone .\nAbstract:\nThe author considers an alternative approach to classical electrodynamics, which is based on the concept of  pre-metricity  introduced by Einstein in his work  On the Electrodynamics of Moving Bodies  (1905). The main idea behind this approach consists in introducing into consideration not only the usual electromagnetic field tensor Fμν but also its dual counterpart *Fμν . In order to obtain equations for both fields simultaneously one has to introduce additional terms into Lagrangian density L(Fμν ,*Fμν ) that are quadratic with respect to derivatives of these fields. It turns out that such modified Lagrangian leads to the following set of equations: \nwhere  is the d Alembert operator,  is the energy-momentum tensor of matter fields, and  is the current four-vector describing the flow of electric charge. \nIn addition to the standard Maxwell s equations, Eqs. (1)-(3), there appears another equation -the so-called  duality condition :",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Linear pre - metric electrodynamics and deduction of the light cone . Abstract : The author considers an additional method to classical electrodynamics , which is based on the idea of pre - metricity introduced by Einstein in his book On the Electrodynamics of Moving Bodies ( 1905 ) .The main idea behind this methodology consists in adding into consideration not only the usual electromagnetic field vector Fμν but also its dual counterpart * Fμν . In order to obtain equations for both fields concurrently one has to introduce extra terms into Lagrangian density L ( Fμν , * Fμν ) that are quadratic with regard to derivatives of these fields .It turns out that such modified Lagrangian leads to the following series of equations : where is the d Alembert operator , is the electricity - momentum tensor of matter fields , and is the current four - vector describing the flow of electric current . In addition to the standard Maxwell s coefficients , Eqs .( 1 ) - ( 3 ) , there exists another equation - the so - called duality condition :",
        "rewrite_text": "Title: Linear Pre-Metric Electrodynamics and the Derivation of the Light Cone\n\nAbstract: In this article, the author explores an innovative approach to classical electrodynamics, drawing on the concept of pre-metricity as introduced by Einstein in his seminal work, \"On the Electrodynamics of Moving Bodies\" (1905). This new methodology expands the traditional framework by incorporating not only the conventional electromagnetic field tensor \\( F_{\\mu\\nu} \\) but also its dual counterpart \\( *F_{\\mu\\nu} \\). To derive equations that govern both fields simultaneously, the author proposes the introduction of additional terms into the Lagrangian density \\( L(F_{\\mu\\nu}, *F_{\\mu\\nu}) \\), which are quadratic in the derivatives of these fields. This modification results in a set of equations that include the d'Alembert operator, the energy-momentum tensor of matter fields, and the four-current vector that represents the flow of electric current. Alongside the standard Maxwell equations, the author presents a novel equation known as the duality condition, which plays a crucial role in the framework of linear pre-metric electrodynamics. The implications of this approach are significant, as it not only enriches the theoretical landscape of electrodynamics but also provides insights into the structure of the light cone, a fundamental concept in the study of spacetime and causality. The findings suggest that the duality condition may offer new perspectives on the interplay between electric and magnetic fields, potentially leading to a deeper understanding of electromagnetic phenomena. This work aims to bridge classical theories with modern interpretations, paving the way for future research in the field of theoretical physics.",
        "ori-fast-z-score": -0.8307471607356973,
        "water-fast-z-score": 2.5927248643506746,
        "rewrite-fast-z-score": 1.4855627054164149
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Quantum-like Representation of Macroscopic Configurations .\nAbstract:\nWe present an approach to the description of macroscopic configurations in terms of quantum states, which is based on the concept of entanglement entropy and its generalization for mixed states. We show that this representation allows one to obtain exact results for some statistical properties of systems with many degrees of freedom. In particular, we consider the problem of calculating the partition function of classical spin models at high temperatures. The proposed method can be used as a basis for constructing approximate algorithms for solving problems related to the calculation of thermodynamic characteristics of complex systems. Introduction.\nThe main goal of statistical mechanics is to describe the behavior of macroscopic objects (for example, gases) by using microscopic information about their constituents (atoms). This task becomes especially difficult when dealing with large systems consisting of many particles or spins. For such cases, it is necessary to use approximations, since direct calculations are impossible due to the exponential growth of the number of possible microstates with increasing system size N . One of these approaches is the so-called mean-field approximation  1  , according to which each particle interacts only with all other particles simultaneously; i.e., the interaction between different pairs of particles is neglected. However, even within this simplified model, the calculation of the partition function Z = Tr exp(−βH) (1) remains extremely complicated  2  .\nIn recent years, there has been growing interest in developing new methods for describing macroscopic configurations in terms similar to those used in quantum physics  3  -  8  . These studies were inspired by the fact that both classical and quantum descriptions have certain common features  9  : they are formulated in terms of wave functions ψ(x), where x denotes either positions of particles or spins, respectively. Moreover, the evolution of these wave functions obeys the same Schrödinger equation ih∂ t |ψ(t) = H|ψ(t) , where H is the corresponding Hamiltonian operator. It should also be noted that the density matrix ρ = |ψ(t) ψ(t)| plays the role of a probability distribution in both theories  10  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Quantum - like Representation of Macroscopic Configurations . Abstract : We present an view to the description of macroscopic configurations in terms of quantum states , which is based on the idea of entanglement entropy and its generalization for mixed states .We see that this representation enables one to obtain exact findings for some statistical characteristics of networks with many degrees of liberty . In particular , we investigate the question of calculating the partition function of classical spin machines at high temperatures .The proposed concept can be used as a framework for constructing approximate schemes for solving problems related to the determination of thermodynamic qualities of complex systems . Introduction .The main goal of statistical mechanics is to explain the dynamics of macroscopic objects ( for example , atoms ) by using microscopic information about their constituents ( atoms ) . This job seems particularly challenging when dealing with large systems composed of several particles or spins .For such cases , it is important to use approximations , since direct calculations are unable due to the exponential growth of the number of possible microstates with increasing system width N . One of these method is the so - called mean - field analogy 1 , according to which each particle interacts only with all other particles concurrently ; i . e . , the interaction between different pairs of atoms is neglected .However , even within this simplified theory , the determination of the partition function Z = Tr exp ( −βH ) ( 1 ) stays extremely difficult 2 . In recent years , there has been growing interest in developing novel techniques for describing macroscopic configurations in terms comparable to those utilized in particle mathematics 3 - 8 .These studies were inspired by the fact that both classical and quantum descriptions have many common features 9 : they are formulated in terms of wave functions φ ( x ) , where h indicates either positions of atoms or spins , respectively . Moreover , the evolution of these wave functions obeys the same Schrödinger equation ih∂ t | ψ ( t ) = H | ψ ( t ) , where H is the analogous Hamiltonian operator .It should additionally be mentioned that the density function ρ = | ψ ( t ) ψ ( t ) | serves the importance of a probability distribution in both theories 10 .",
        "rewrite_text": "**Title:** Quantum-like Representation of Macroscopic Configurations\n\n**Abstract:** In this article, we explore a novel perspective on the characterization of macroscopic configurations through the lens of quantum states, leveraging the concept of entanglement entropy and its extension to mixed states. This approach facilitates the derivation of precise results for various statistical properties of complex networks that exhibit numerous degrees of freedom. A focal point of our investigation is the computation of the partition function for classical spin systems at elevated temperatures. The framework we propose not only enhances our understanding of these systems but also serves as a basis for developing approximate methodologies aimed at resolving challenges associated with the evaluation of thermodynamic properties in intricate systems.\n\nThe primary objective of statistical mechanics is to elucidate the behavior of macroscopic entities, such as atoms, by utilizing microscopic data pertaining to their constituents. This task becomes particularly daunting when addressing large systems composed of multiple particles or spins, where the exponential increase in the number of potential microstates with system size (N) renders direct calculations impractical. To navigate this complexity, various approximation methods have been employed, including the mean-field approximation, which simplifies interactions by assuming that each particle interacts uniformly with all others, thereby neglecting pairwise interactions. Despite this simplification, accurately determining the partition function, expressed as \\( Z = \\text{Tr} \\, e^{-\\beta H} \\), remains a formidable challenge.\n\nRecent advancements have sparked a heightened interest in innovative techniques that describe macroscopic configurations in a manner analogous to those found in particle physics. These investigations are motivated by the observation that classical and quantum frameworks share numerous similarities, particularly in their reliance on wave functions \\( \\phi(x) \\) that represent either atomic positions or spin states. Furthermore, the evolution of these wave functions adheres to the same Schrödinger equation, \\( i\\hbar \\partial_t | \\psi(t) \\rangle = H | \\psi(t) \\rangle \\), where \\( H \\) denotes the corresponding Hamiltonian operator. Additionally, the density function \\( \\rho = |\\psi(t)|^2 \\) plays a crucial role as a probability distribution in both classical and quantum theories. This article aims to bridge these concepts, providing insights into the statistical mechanics of macroscopic systems through quantum-like representations.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 8.23076923076923,
        "rewrite-fast-z-score": 1.2418408411301325
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Converting genetic network oscillations into somite spatial pattern .\nAbstract:\nThe segmentation clock is an oscillatory gene regulatory network that controls the periodic formation of segments in vertebrates, such as vertebrae and ribs.  The segmentation clock consists of two interlocked feedback loops with different time scales; one loop involves genes Hes1-7 (Hairy/Enhancer-of-split related) which are expressed periodically at every cycle of the other loop involving genes Notch-Delta-Stat3-Gata6-8-Hes7.  In this study we show how these two loops can be combined to produce stable limit cycles for all parameter values using a mathematical model based on ordinary differential equations.  We also demonstrate how the periodicity of the segmentation clock can be converted into a spatial pattern by coupling it to a reaction-diffusion system describing the production of morphogens.  Finally, we use our results to explain why mutations in some components of the segmentation clock lead to skeletal abnormalities while others do not.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Converting genetic system oscillations into somite geographic pattern . Abstract : The segmentation clock is an oscillatory gene regulatory network that influences the periodic formation of segments in vertebrates , such as vertebrae and ribs .The segmentation clock contains of two interlocked feedback loops with various time ranges ; one loop includes genes Hes1 - 7 ( Hairy / Enhancer - of - split related ) which are expressed periodically at every cycle of the other loop comprising genes Notch - Delta - Stat3 - Gata6 - 8 - Hes7 . In this study we find how these two loops can be merged to produce stable limit cycles for all parameter values utilizing a mathematical description based on ordinary differential equations .We also demonstrate how the periodicity of the segmentation clock can be turned into a spatial pattern by connecting it to a reaction - diffusion system describing the production of morphogens . Finally , we using our findings to explain why mutations in some components of the segmentation clock lead to skeletal abnormalities while many do not .",
        "rewrite_text": "**Title:** Transforming Genetic Oscillations into Somite Spatial Patterns\n\n**Abstract:** The segmentation clock is a crucial oscillatory gene regulatory network that governs the rhythmic formation of body segments in vertebrates, including structures such as vertebrae and ribs. This clock comprises two interlinked feedback loops that operate over different temporal scales. The first loop consists of genes Hes1 through Hes7, which exhibit periodic expression synchronized with the cycles of the second loop, which includes genes such as Notch, Delta, Stat3, Gata6, Gata8, and Hes7. In this research, we explore the integration of these two feedback loops to generate stable limit cycles across a wide range of parameter values, employing a mathematical framework grounded in ordinary differential equations. Furthermore, we illustrate how the oscillatory behavior of the segmentation clock can be translated into spatial patterns by linking it to a reaction-diffusion system that models the dynamics of morphogen production. Our findings provide insights into the mechanisms by which genetic oscillations can manifest as organized spatial structures during development. Additionally, we utilize our results to elucidate the reasons behind the occurrence of skeletal abnormalities resulting from mutations in specific components of the segmentation clock, while noting that many mutations do not lead to such defects. This study not only enhances our understanding of the genetic underpinnings of vertebrate segmentation but also offers a mathematical perspective on the interplay between genetic oscillations and spatial pattern formation in developmental biology.",
        "ori-fast-z-score": -1.2535663410560174,
        "water-fast-z-score": 3.170375695604868,
        "rewrite-fast-z-score": 0.5883484054145521
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Galaxy Evolution and Environment .\nAbstract:\nThe evolution of galaxies is strongly influenced by their environment, but the physical processes that drive this are not well understood. In particular, it remains unclear how galaxy properties such as morphology or star formation rate depend on large-scale structure in the Universe.  This talk will present results from an ongoing study using data from the Sloan Digital Sky Survey (SDSS) to investigate these issues. The first part of my talk will focus on the relationship between galaxy clustering and luminosity/stellar mass. I will then discuss recent work investigating the dependence of galaxy properties on local density. Finally, I ll show some preliminary results exploring the connection between galaxy properties and dark matter halo masses. My research has been supported by NSF grant AST-0707766. I am currently a postdoctoral fellow at Harvard University working with Prof. David Weinberg. I received my Ph.D. from UCLA under the supervision of Dr. James Bullock.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Galaxy Evolution and Environment . Abstract : The evolution of stars is strongly altered by their environment , but the natural processes that drive this are not well understood .In particular , it remains unsure how star properties such as geometry or star formation rate depend on huge - scale organization in the Universe . This discussion will present results from an continuing survey employing data from the Sloan Digital Sky Survey ( SDSS ) to examine these problems .The first part of my talk will focus on the relationship between galaxy clustering and luminosity / stellar mass . I will then discuss latest work investigating the dependence of galaxy structures on local density .Finally , I ll show some preliminary results studying the link between galaxy structures and dark matter halo masses . My research has been supported by NSF grant AST - 0707766 .I am currently a postdoctoral scholar at Harvard University working with Prof . David Weinberg . I received my Ph . D . from UCLA under the guidance of Dr . James Bullock .",
        "rewrite_text": "**Title: Galaxy Evolution and Environment**\n\n**Abstract:** The interplay between stellar evolution and environmental factors is a complex and not fully understood phenomenon. This article delves into the significant impact that a galaxy's surroundings have on its stellar properties, including geometry and star formation rates, and seeks to clarify the relationship between these properties and the large-scale structure of the universe. Utilizing data from the Sloan Digital Sky Survey (SDSS), this ongoing research aims to address these critical questions. The first segment of the discussion will explore the correlation between galaxy clustering and various metrics such as luminosity and stellar mass, shedding light on how these factors influence one another. Following this, I will present recent findings that examine how galaxy structures are affected by local density variations, providing insights into the environmental dependencies of these structures. Lastly, I will share preliminary results that investigate the connection between galaxy structures and the masses of their associated dark matter halos, a key aspect of understanding galaxy formation and evolution. This research is supported by NSF grant AST-0707766. Currently, I am a postdoctoral scholar at Harvard University, collaborating with Professor David Weinberg, and I completed my Ph.D. at UCLA under the mentorship of Dr. James Bullock. Through this work, we aim to enhance our comprehension of how environmental factors shape galaxy evolution across the cosmos.",
        "ori-fast-z-score": 0.46499055497527714,
        "water-fast-z-score": 5.27656187902292,
        "rewrite-fast-z-score": -0.8340576562282991
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Spectral type dependent rotational braking and strong magnetic flux in three components of the late-M multiple system LHS 1070 .\nAbstract:\nWe report on spectropolarimetric observations of the M8+M9 binary star LHS 1070A,B (GJ 436) with ESPaDOnS at CFHT. The two stars are separated by only 0. ′′ 1 and have been known to be magnetically active for many years. We find that both stars show significant circularly polarized emission lines indicative of Zeeman splitting due to their magnetic fields. In addition we detect Stokes V signatures indicating net linear polarization across all observed spectral lines. This is likely caused by scattering processes within the stellar atmosphere. Using our new data set together with previously published photometric measurements we derive rotation periods of P A = 3.6 ± 0.1 days and P B = 4.2 ± 0.3 days for the primary and secondary component respectively. These values are significantly longer than those derived from previous studies which were based solely on photometry. Our results suggest that the rotation period of each individual component depends strongly on its effective temperature as well as its surface gravity.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Spectral type dependent rotational braking and strong magnetic flux in three components of the late - M multiple system LHS 1070 . Abstract : We report on spectropolarimetric studies of the M8 + M9 binary star LHS 1070A , B ( GJ 436 ) with ESPaDOnS at CFHT .The two stars are split by only 0 . ′ ′ 1 and have been known to be magnetically active for many years .We see that both stars show considerable circularly polarized emission lines indicative of Zeeman splitting due to their magnetic fields . In addition we locate Stokes V signatures suggesting net linear polarization across all observed spectral lines .This is probably due by scattering mechanisms within the stellar environment . Using our new data set combined with previously reported photometric surveys we derive rotation periods of P A = 3 . 6 ± 0 . 1 hours and P B = 4 . 2 ± 0 . 3 days for the primary and secondary component respectively .These values are greatly lengthy than those generated from previous analyses which were based primarily on photometry . Our results show that the rotation history of each individual component relies highly on its effective heat as well as its surface velocity .",
        "rewrite_text": "We present a detailed analysis of the spectropolarimetric observations of the late-M binary system LHS 1070A and LHS 1070B (GJ 436), utilizing the ESPaDOnS instrument at the Canada-France-Hawaii Telescope (CFHT). The two stars, separated by a mere 0.1 arcseconds, have been recognized for their magnetic activity over an extended period. Our findings reveal significant circularly polarized emission lines, which indicate Zeeman splitting attributable to the magnetic fields of both stars. Furthermore, we identify Stokes V signatures that suggest the presence of net linear polarization across all spectral lines observed, likely resulting from scattering processes within the stellar environment. By integrating our new spectropolarimetric data with previously conducted photometric surveys, we have determined the rotation periods for the primary and secondary components to be P_A = 3.6 ± 0.1 hours and P_B = 4.2 ± 0.3 days, respectively. These rotation periods are substantially longer than those derived from earlier studies that primarily relied on photometric data. Our results indicate that the rotational dynamics of each star are significantly influenced by their effective temperature and surface velocity. This study enhances our understanding of the interplay between stellar rotation, magnetic activity, and spectral type in late-M dwarf systems, providing valuable insights into the evolutionary processes governing these intriguing celestial objects.",
        "ori-fast-z-score": -0.9332565252573828,
        "water-fast-z-score": 4.587317109255645,
        "rewrite-fast-z-score": 0.8
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Force distributions and force chains in random stiff fiber networks .\nAbstract:\nWe study the mechanical properties of disordered networks of fibers with different bending rigidities by means of numerical simulations based on molecular dynamics (MD) and Monte Carlo (MC). We find that the elastic moduli are strongly dependent on the bending rigidity, which is related to the persistence length of the fibers. The results show that the shear modulus increases as the bending rigidity decreases while the bulk modulus remains almost unchanged for all values of the bending rigidity studied here. In addition we observe that the distribution functions of forces acting between particles depend significantly on the bending rigidity. For small bending rigidities there exist strong correlations among neighboring particles leading to large fluctuations in the local stress field. These findings suggest that the microscopic structure plays an important role in determining macroscopic material properties such as elasticity. Disordered networks of fibers have been widely used to model various materials ranging from biological tissues to granular media  1-3 . However, it has not yet been fully understood how the microstructure affects their macroscopic behavior. Here we investigate this problem using computer simulation techniques.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Force distributions and force chains in random rigid fiber networks . Abstract : We research the mechanical behavior of disordered systems of fibers with various twisting rigidities by means of computational simulations based on molecular mechanics ( MD ) and Monte Carlo ( MC ) .We see that the elastic moduli are strongly dependent on the shear rigidity , which is related to the persistence length of the materials . The results show that the shear modulus increases as the twisting rigidity decreases while the bulk modulus remains virtually unchanged for all values of the twisting rigidity examined here .In addition we find that the distribution functions of pressures acting between particles depend greatly on the bending rigidity . For small bending rigidities there exist strong correlations among neighboring objects resulting to large fluctuations in the local tension field .These studies imply that the microscopic shape plays an important role in measuring macroscopic material structures such as elasticity . Disordered networks of fibers have been widely using to model numerous structures ranging from biological tissues to granular material 1 - 3 .However , it has not already been clearly explained how the microstructure affects their macroscopic behavior . Here we investigate this question using computer simulation tools .",
        "rewrite_text": "In this study, we explore the mechanical properties of disordered fiber networks characterized by varying degrees of twisting rigidity through computational simulations employing molecular dynamics (MD) and Monte Carlo (MC) methods. Our findings reveal a significant relationship between the elastic moduli of these networks and their shear rigidity, which is intrinsically linked to the persistence length of the fibers. Specifically, we observe that as the twisting rigidity decreases, the shear modulus exhibits a notable increase, while the bulk modulus remains relatively constant across all twisting rigidity values analyzed. Furthermore, we investigate the pressure distribution functions acting between the fibers, which demonstrate a strong dependence on the bending rigidity of the materials. In instances of low bending rigidity, we identify pronounced correlations among adjacent fibers, leading to substantial fluctuations in the local tension field. These results underscore the critical influence of microscopic structural characteristics on the macroscopic mechanical behavior of materials, particularly in terms of elasticity. Disordered fiber networks serve as effective models for a variety of structures, including biological tissues and granular materials. Despite their widespread application, the precise mechanisms by which microstructural features affect macroscopic properties have not been thoroughly elucidated. This research aims to bridge that gap by employing advanced computational simulation techniques to provide insights into the interplay between microstructure and macroscopic mechanical behavior in fiber networks.",
        "ori-fast-z-score": -1.2935483472729858,
        "water-fast-z-score": 6.207574452435047,
        "rewrite-fast-z-score": 0.9712858623572641
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Kinematics of Spiral Arm Streaming in M51 .\nAbstract:\nWe present new results on the kinematics and dynamics of spiral arm streaming motions in the nearby grand-design galaxy NGC 5194 (M51). We use high-resolution near-IR integral field spectroscopy obtained with SINFONI at VLT to study the gas velocity fields, line-of-sight velocities, and mass surface density distributions along two different lines of sight through the central region of this interacting system. The main goal is to investigate whether or not there are systematic differences between these two viewing angles that could be related to an underlying gravitational potential perturbation caused by the companion galaxy NGC 5195. Our analysis shows that both sides of the nuclear ring exhibit similar large-scale streaming patterns which can be traced out to distances of about 1 kpc into the disk plane. However, we find significant deviations from circular rotation within the inner 100 pc radius around the nucleus. These non-circular motions appear to be dominated by radial inflow towards the center as well as counter-rotating components perpendicular to the major axis of the galactic bar.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Kinematics of Spiral Arm Streaming in M51 . Abstract : We report new data on the kinematics and dynamics of spiral arm streaming movements in the nearby grand - design galaxy NGC 5194 ( M51 ) .We use large - resolution near - IR integral field spectroscopy derived with SINFONI at VLT to study the gas velocity fields , line - of - view velocities , and mass surface velocity distributions along two different lines of view through the main region of this interacting system . The main goal is to examine whether or not there are systematic differences between these two viewing angles that might be connected to an underlying gravitational potential perturbation caused by the companion galaxy NGC 5195 .Our study shows that both sides of the atomic ring show identical large - scale streaming patterns which can be traced out to distances of about 1 kpc into the disk plane . However , we find considerable deviations from circular rotation within the inner 100 pc radius around the nucleus .These non - circular movements appear to be dominated by radial inflow towards the center as well as anti - spinning components parallel to the main axis of the galactic bar .",
        "rewrite_text": "We present new findings on the kinematics and dynamics of spiral arm streaming in the nearby grand-design galaxy NGC 5194 (M51). Utilizing high-resolution near-infrared integral field spectroscopy obtained with SINFONI at the Very Large Telescope (VLT), we investigate the gas velocity fields, line-of-sight velocities, and mass surface velocity distributions along two distinct lines of sight through the primary region of this interacting galaxy system. The primary objective of our research is to determine whether systematic differences exist between these two perspectives, which could be indicative of an underlying gravitational potential perturbation attributed to the companion galaxy NGC 5195. Our analysis reveals that both sides of the atomic ring exhibit similar large-scale streaming patterns, extending approximately 1 kiloparsec into the disk plane. However, we observe significant deviations from circular rotation within the inner 100 parsecs surrounding the nucleus. These non-circular motions are primarily characterized by a radial inflow towards the center, alongside anti-spinning components that align with the main axis of the galactic bar. This study enhances our understanding of the complex dynamics at play in M51 and sheds light on the interactions between its spiral arms and the gravitational influences of its companion galaxy. The implications of these findings contribute to the broader discourse on galaxy dynamics and the role of interactions in shaping the kinematic properties of spiral galaxies.",
        "ori-fast-z-score": 0.7181848464596079,
        "water-fast-z-score": 5.381334675208182,
        "rewrite-fast-z-score": 2.2677868380553634
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Slow wave resonance in periodic stacks of anisotropic layers .\nAbstract:\nWe study the slow wave resonance (SWR) effect for periodically layered media with an arbitrary number N of anisotropic layers, each characterized by its own permittivity tensor and thickness. We show that SWR is possible only if all principal axes of the permittivity tensors are parallel to one another within each layer. In this case we derive explicit expressions for the dispersion relation between the frequency f and the Bloch wavenumber kx. The results obtained can be used as guidelines for designing multilayered structures exhibiting strong SWR effects at low frequencies. \n \n Keywords: Slow wave resonance; Anisotropy; Multilayer structure; Dispersion relations. 1 Introduction \n \n Periodic multilayers consisting of alternating thin films made of different materials have attracted considerable attention during recent years due to their unique properties  1  . These include high reflectance  2  , negative refraction  3  , enhanced nonlinear optical response  4  , etc., which make them promising candidates for various applications such as optoelectronic devices  5  or photovoltaics  6  .\n \nIn particular, it has been shown recently  7–9  that periodic multilayers composed of anisotropic layers may exhibit very interesting electromagnetic phenomena including slow wave resonance (S WR). This phenomenon occurs when the phase velocity of the Bloch waves becomes equal to zero inside the medium  10  . It leads to extremely large values of the effective refractive index n eff = c / v ph  11  where c is the speed of light in vacuum and v ph is the phase velocity of the propagating Bloch mode  12  . As a result, the corresponding transmission spectrum exhibits sharp peaks associated with narrow stop bands  13  . Such features are highly desirable for many practical applications  14  . \n \n However, despite numerous theoretical studies devoted to S WR in periodic multilayers  15–18  , there still exist several open questions related to the conditions under which this phenomenon takes place  19, 20  . For example, it was found experimentally  21  that the presence of a single misaligned anisotropic layer destroys the S WR effect completely even though other layers remain perfectly aligned. On the other hand, numerical simulations  22  suggest that",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Slow wave resonance in periodic piles of anisotropic layers . Abstract : We research the slow wave resonance ( SWR ) effect for regularly layered media with an arbitrary number N of anisotropic layers , each described by its own permittivity vector and thickness .We see that SWR is possible only if all primary axes of the permittivity tensors are connected to one another within each layer . In this situation we derive explicit expressions for the dispersion connection between the frequency f and the Bloch wavenumber kx .The results collected can be used as guidelines for constructing multilayered buildings presenting strong SWR effects at low frequencies . Keywords : Slow wave vibration ; Anisotropy ; Multilayer structure ; Dispersion relations .1 Introduction Periodic multilayers consisting of alternating thin films formed of different materials have garnered considerable scrutiny during recent seasons due to their distinct characteristics 1 . These include high reflectance 2 , positive refraction 3 , enhanced nonlinear optical reaction 4 , etc . , which make them promising candidates for various uses such as optoelectronic technologies 5 or photovoltaics 6 .In particular , it has been shown ago 7 – 9 that periodic multilayers consisting of anisotropic elements may exhibit very interesting electrical processes including slow wave resonance ( S WR ) . This phenomenon occurs when the phase velocity of the Bloch waves becomes equal to zero inside the medium 10 .It results to incredibly large values of the effective refractive index n eff = c / v ph 11 where p is the speed of light in vacuum and v ph is the phase velocity of the propagating Bloch mode 12 . As a result , the associated transmission spectrum exhibits strong spikes identified with narrow stop rings 13 .Such characteristics are extremely attractive for numerous practical applications 14 . However , despite several practical studies focused to S WR in periodic multilayers 15 – 18 , there still exist several open questions related to the conditions under which this phenomenon happens place 19 , 20 .For instance , it was shown experimentally 21 that the presence of a single misaligned anisotropic surface destroys the S WR effect totally even though other layers remain perfectly aligned . On the other hand , numerical simulations 22 suggest that",
        "rewrite_text": "**Title:** Slow Wave Resonance in Periodic Piles of Anisotropic Layers\n\n**Abstract:** This study investigates the phenomenon of slow wave resonance (SWR) in periodically layered media composed of an arbitrary number \\( N \\) of anisotropic layers, each characterized by its unique permittivity vector and thickness. Our findings indicate that SWR can only occur when the principal axes of the permittivity tensors are interconnected within each layer. Under these conditions, we derive explicit mathematical expressions that describe the relationship between frequency \\( f \\) and the Bloch wavenumber \\( k_x \\). The insights gained from this research can serve as valuable guidelines for the design of multilayered structures that exhibit pronounced SWR effects at low frequencies. \n\nThe significance of periodic multilayers, which consist of alternating thin films made from various materials, has gained attention due to their unique properties, including high reflectance, positive refraction, and enhanced nonlinear optical responses. These attributes position them as promising candidates for applications in optoelectronic technologies and photovoltaics. Previous studies have demonstrated that multilayers composed of anisotropic materials can exhibit intriguing electrical phenomena, such as SWR, where the phase velocity of Bloch waves approaches zero within the medium. This leads to exceptionally high values of the effective refractive index \\( n_{\\text{eff}} = c / v_{\\text{ph}} \\), where \\( c \\) represents the speed of light in a vacuum and \\( v_{\\text{ph}} \\) is the phase velocity of the propagating Bloch mode. Consequently, the transmission spectrum displays pronounced spikes associated with narrow stop bands, making these characteristics highly desirable for various practical applications.\n\nDespite the growing body of research on SWR in periodic multilayers, several questions remain regarding the specific conditions necessary for this phenomenon to manifest. For instance, experimental evidence has shown that even a single misaligned anisotropic layer can completely disrupt the SWR effect, while numerical simulations suggest that alignment plays a critical role in maintaining the resonance. This paper aims to clarify these conditions and contribute to the understanding of SWR in anisotropic multilayer structures. \n\n**Keywords:** Slow wave resonance; Anisotropy; Multilayer structures; Dispersion relations.",
        "ori-fast-z-score": -0.22808577638091165,
        "water-fast-z-score": 7.580980435789034,
        "rewrite-fast-z-score": 3.0547157549530337
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Charmless Three-body Decays of B Mesons .\nAbstract:\nWe present the results on charmless three-body decays of B mesons using data collected by the Belle experiment at KEKB in 2003-2007, corresponding to an integrated luminosity of 711 fb-1. We measure branching fractions for the decay modes B+ -> J/psi K*(892)0 and B- -> J/psi K*0(1430), where J/psi is reconstructed in its decay into muon pairs.  The measured values are  B+ -> J/psi: (4.6 +/- 0.7 +/- 1.1) x 10^-6; B->J/psi: (3.2 +/- 0.5 +/- 0.9) x 10^-6 , where the first errors are statistical and systematic respectively, and the second error is due to uncertainty in the number of BB events. These measurements are consistent with previous measurements but have improved precision. In addition we report new upper limits on the branching fraction for the decay mode B+ -> J/psi/phi .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Charmless Three - bodies Decays of B Mesons . Abstract : We present the results on charmless three - bodies decays of B mesons using data taken by the Belle study at KEKB in 2003 - 2007 , equivalent to an integrated luminosity of 711 fb - 1 .We determine branching fractions for the decay modes B + - > J / psi K * ( 892 ) 0 and B - - > J / psi K * 0 ( 1430 ) , where J / psi is reconstructed in its decay into muon pairs . The measured values are B + - > J / psi : ( 4 . 6 + / - 0 . 7 + / - 1 . 1 ) x 10 ^ - 6 ; B - > J / psi : ( 3 . 2 + / - 0 . 5 + / - 0 . 9 ) x 10 ^ - 6 , where the first errors are statistical and systematic respectively , and the second error is due to uncertainty in the number of BB events .These measurements are compatible with previous measurements but have improved precision . In addition we publish new higher restrictions on the branching fraction for the decay mode B + - > J / psi / phi .",
        "rewrite_text": "We present our findings on the charmless three-body decays of B mesons, derived from data collected by the Belle experiment at the KEKB accelerator between 2003 and 2007, which corresponds to an integrated luminosity of 711 fb⁻¹. Our analysis focuses on the branching fractions for the decay processes B⁺⁻ → J/ψ K*(892)⁰ and B⁻ → J/ψ K*(0)(1430). In these decays, the J/ψ meson is reconstructed through its decay into muon pairs. The measured branching fractions are B⁺⁻ → J/ψ: (4.6 ± 0.7 ± 1.1) × 10⁻⁶ and B⁻ → J/ψ: (3.2 ± 0.5 ± 0.9) × 10⁻⁶, where the first set of uncertainties represents statistical and systematic errors, while the second set accounts for the uncertainty in the number of BB events. Our results align with previous measurements but offer enhanced precision, contributing valuable data to the field. Furthermore, we provide new upper limits on the branching fraction for the decay mode B⁺⁻ → J/ψ φ, which adds to the existing knowledge of B meson decays. These findings are significant for understanding the dynamics of B meson decays and contribute to the broader investigation of flavor physics.",
        "ori-fast-z-score": -1.0886621079036347,
        "water-fast-z-score": 2.0225995873897262,
        "rewrite-fast-z-score": -0.47809144373375745
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Vortices in Bose-Einstein Condensates: Theory .\nAbstract:\nWe present an overview of the theory for vortices in trapped, dilute atomic gases at low temperatures. We discuss how these systems can be described by macroscopic wave functions and show that they are governed by nonlinear Schrödinger equations with external potentials. The solutions to this equation have been studied extensively over many years and we review some of their properties relevant to vortex formation. In particular, we consider stationary states which correspond to condensate configurations without rotation (vortex-free) as well as rotating ones where quantized angular momentum is carried by phase singularities known as vortices. Finally, we briefly describe recent experiments on vortex production in cold atom clouds. Vortices occur naturally in superfluids such as liquid helium or dilute atomic gases. They carry quantized angular momenta and play important roles in various physical phenomena including turbulence and quantum transport processes. Here we give an introduction into the theoretical description of vortices in trapped atomic gases.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Vortices in Bose - Einstein Condensates : Theory . Abstract : We present an overview of the principle for vortices in trapped , dilute atomic materials at low temperatures .We discuss how these systems can be described by macroscopic wave functions and find that they are governed by nonlinear Schrödinger coefficients with external potentials . The solutions to this equation have been studied frequently over numerous years and we review some of their characteristics applicable to vortex structure .In particular , we define stationary states which correspond to condensate configurations without rotation ( vortex - safe ) as well as rotating ones where quantized angular velocity is carried by phase singularities known as vortices . Finally , we briefly describe recent experiments on vortex production in cold hydrogen clusters .Vortices arise naturally in superfluids such as fluid helium or dilute nuclear gases . They carry quantized angular momenta and play essential roles in different mechanical phenomena including turbulence and quantum transport systems .Here we give an overview into the theoretical theory of vortices in trapped atomic gases .",
        "rewrite_text": "Title: Vortices in Bose-Einstein Condensates: Theory\n\nAbstract: This article provides a comprehensive overview of the theoretical framework surrounding vortices in trapped, dilute atomic gases at low temperatures. We explore the description of these systems through macroscopic wave functions, which are governed by nonlinear Schrödinger equations influenced by external potentials. The solutions to these equations have been extensively studied over the years, and we highlight key characteristics relevant to vortex structures. Specifically, we categorize stationary states that correspond to non-rotating condensate configurations, referred to as \"vortex-safe,\" alongside those that exhibit rotation, where quantized angular momentum is manifested through phase singularities known as vortices. Additionally, we touch upon recent experimental advancements in the production of vortices within cold hydrogen clusters. Vortices are a fundamental aspect of superfluid systems, such as liquid helium and dilute nuclear gases, where they carry quantized angular momentum and significantly contribute to various mechanical phenomena, including turbulence and quantum transport. This article aims to provide a thorough theoretical insight into the behavior of vortices in trapped atomic gases, enhancing our understanding of their role in quantum fluids and their implications for future research in the field.",
        "ori-fast-z-score": -1.091089451179962,
        "water-fast-z-score": 4.313310928137536,
        "rewrite-fast-z-score": 1.5724272550828775
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  On the multifractal statistics of the local order parameter at random critical points : application to wetting transitions with disorder .\nAbstract:\nWe study the statistical properties of the local order parameter in systems undergoing a continuous phase transition, when quenched disorder is present. We show that these are described by a generalized multifractal spectrum which depends on both the strength and type of disorder as well as on the universality class of the clean system. In particular we consider the case of a first-order phase transition driven into second-order one by disorder. The results obtained for this model can be applied to describe the behavior of interfaces near wetting transitions in presence of quenched disorder. This work was supported by MIUR through FIRB project RBFR05EJYQ. PACS numbers: 05.65.+b, 64.60.Cn, 64.60.Fg, 64.60.Ht . \nI. INTRODUCTORY REMARK\nThe aim of this paper is to investigate how the presence of quenched impurities affects the statistical properties of the order-parameter fluctuations close to a continuous phase transition point. Our main interest will be focused on the so-called  random fixed-points  (RFP)  1  , i.e., those points where the renormalization-group flow of an effective field theory describing the effect of disorder stops because it becomes unstable against small perturbations  2  .\nIn general RFPs do not correspond to any physical situation since they represent singularities of the RG flows; however there exist some cases where such singularity appears only after many orders of perturbation expansion have been resummed  3  . For example, if the disorder distribution has a finite variance then the corresponding RFP corresponds to a stable fixed point  4  ; therefore its effects should appear only beyond all orders in perturbation theory  5  . On the other hand, if the disorder distribution is broad enough so that its moments diverge, then the RFP represents a genuine non-perturbative phenomenon  6  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : On the multifractal statistics of the local order parameter at random key points : application to wetting transitions with disorder . Abstract : We research the statistical characteristics of the local order parameter in environments undergoing a continuous phase shift , when quenched disorder is present .We see that these are explained by a generalized multifractal spectrum which depends on both the strength and kind of disorder as well as on the universality category of the clean system . In particular we study the case of a first - order phase process driven into next - order one by disorder .The results derived for this model can be applied to explain the activity of interfaces near wetting interactions in presence of quenched defect . This project was supported by MIUR through FIRB project RBFR05EJYQ .PACS codes : 05 . 65 . + c , 64 . 60 . Cn , 64 . 60 . Fg , 64 . 60 . Ht . I .INTRODUCTORY REMARK The goal of this paper is to examine how the presence of quenched impurities impacts the statistical characteristics of the order - vector fluctuations close to a continuous phase change position . Our main interest will be focused on the so - called random fixed - points ( RFP ) 1 , i . e . , those points where the renormalization - group flow of an efficient field model explaining the effect of disorder ends because it becomes unstable against small perturbations 2 .In general RFPs do not correspond to any physical situation since they represent singularities of the RG flows ; however there exist some cases where such singularity emerges only after many orders of perturbation expansion have been resummed 3 . For instance , if the disorder distribution has a finite variance then the equivalent RFP corresponds to a stable fixed point 4 ; therefore its consequences should seem only beyond all orders in perturbation theory 5 .On the other hand , if the disorder distribution is broad sufficiently so that its minutes diverge , then the RFP represents a genuine non - perturbative phenomenon 6 .",
        "rewrite_text": "**Title:** On the Multifractal Statistics of the Local Order Parameter at Random Key Points: Application to Wetting Transitions with Disorder\n\n**Abstract:** This study investigates the statistical properties of the local order parameter in systems experiencing continuous phase transitions in the presence of quenched disorder. Our findings reveal that these properties can be described by a generalized multifractal spectrum, which is influenced by both the intensity and type of disorder, as well as the universality class of the clean system. We specifically analyze a scenario involving a first-order phase transition that is driven into a higher-order transition due to the presence of disorder. The insights gained from this model are applicable to understanding the behavior of interfaces near wetting transitions when quenched defects are present. This research was supported by the Italian Ministry of Education, Universities and Research (MIUR) through the FIRB project RBFR05EJYQ. \n\nThe primary objective of this paper is to explore how quenched impurities affect the statistical characteristics of order-parameter fluctuations in proximity to a continuous phase transition. Our focus is on random fixed points (RFPs), which are defined as points where the renormalization group (RG) flow of an effective field model, accounting for disorder effects, becomes unstable in response to small perturbations. While RFPs typically do not correspond to physical scenarios, they can manifest as singularities in RG flows under certain conditions. Notably, in cases where the disorder distribution has a finite variance, the associated RFP corresponds to a stable fixed point, suggesting that its implications may only become apparent beyond all orders of perturbation theory. Conversely, when the disorder distribution is sufficiently broad, leading to divergent moments, the RFP represents a true non-perturbative phenomenon. This work contributes to a deeper understanding of the interplay between disorder and phase transitions, particularly in the context of wetting phenomena. \n\n**PACS Codes:** 05.65.+c, 64.60.Cn, 64.60.Fg, 64.60.Ht.",
        "ori-fast-z-score": -1.01418510567422,
        "water-fast-z-score": 5.408987230262506,
        "rewrite-fast-z-score": 0.8512565307587486
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A discussion on particle number and quantum indistinguishability .\nAbstract:\nWe discuss the concept of particle number in quantum mechanics, with particular emphasis on its role as an indicator for quantum indistinguishability between particles. We show that this is not always true by considering two examples where it does not hold. The first example involves fermions which are distinguishable but have different spin states; we argue that these should be considered to belong to distinct species rather than being regarded as identical particles. In our second example, we consider bosons which can occupy the same state (e.g., photons) but are nevertheless distinguishable; here too they do not constitute a single species. Finally, we present arguments against identifying particle number with entropy or information content. This article is part of a series of articles intended to introduce some key concepts in physics through informal discussions. It may also serve as a useful guide for students preparing their own presentations. For more detailed explanations see the original papers cited below. \n \n Quantum mechanics has been developed over many decades since its birth at the beginning of 20th century. One of the most important aspects of quantum mechanics is the concept of particle number. Particles are fundamental building blocks of matter and energy. They come in various forms such as electrons, protons, neutrons, atoms, molecules etc.. Each type of particle carries certain properties like charge, mass, spin etc... \n \n What exactly is particle number? How is it related to other physical quantities? Why is it so important? These questions will be answered in this article.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A debate on particle number and quantum indistinguishability . Abstract : We discuss the notion of particle number in particle theory , with particular emphasis on its role as an measure for quantum indistinguishability between particles .We see that this is not always true by examining two examples where it does not hold . The first instance involves fermions which are distinguishable but have different spinning states ; we claim that these should be regarded to belong to distinct species rather than being regarded as identical particles .In our second example , we define bosons which can occupy the same state ( e . g . , photons ) but are nevertheless distinguishable ; here too they do not constitute a single species . Finally , we present arguments against identifying particle size with entropy or information content .This page is part of a string of publications intended to introduce some important concepts in science through informal talks . It might additionally help as a helpful guide for students preparing their own presentations .For more thorough explanations view the original papers cited below . Quantum theory has been used over numerous years since its birth at the beginning of 20th millennium .One of the most important elements of quantum mechanics is the idea of particle number . Particles are fundamental construction blocks of matter and energy .They come in different shapes such as atoms , protons , neutrons , atoms , molecules etc . . Each sort of particle contains certain characteristics like charge , mass , spin etc . . . What really is particle size ? How is it linked to other physical quantities ?Why is it so important ? These questions will be answered in this article .",
        "rewrite_text": "**Title:** A Debate on Particle Number and Quantum Indistinguishability\n\n**Abstract:** In this article, we explore the concept of particle number within the framework of particle theory, focusing on its significance as a measure of quantum indistinguishability among particles. We challenge the conventional understanding that particle number always correlates with indistinguishability by presenting two illustrative examples where this notion fails. The first example examines fermions that, despite being distinguishable due to their differing spin states, should be classified as belonging to distinct species rather than being treated as identical particles. In our second example, we analyze bosons, such as photons, which can occupy the same quantum state yet remain distinguishable; similarly, these particles do not represent a single species. Furthermore, we argue against the common practice of equating particle size with entropy or information content, emphasizing the need for a more nuanced understanding of these concepts. This discussion is part of a broader series of publications aimed at elucidating key scientific principles through informal discourse, serving as a valuable resource for students preparing their own presentations. We also reflect on the historical development of quantum theory since its inception in the early 20th century, highlighting the pivotal role of particle number in quantum mechanics. Particles, the fundamental building blocks of matter and energy, exist in various forms, including atoms, protons, neutrons, and molecules, each characterized by specific properties such as charge, mass, and spin. This article seeks to address critical questions regarding the nature of particle size, its relationship to other physical quantities, and its overall significance in the realm of physics. For a deeper understanding, readers are encouraged to consult the original papers referenced herein.",
        "ori-fast-z-score": -1.0366421106976322,
        "water-fast-z-score": 6.219852664185793,
        "rewrite-fast-z-score": 0.5107539184552492
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A new look at the modified Coulomb potential in a strong magnetic field .\nAbstract:\nWe study the effect of an external magnetic field on the modified Coulomb potential for two particles with opposite charges and masses, which are confined to move along one dimension. We show that this system can be mapped onto a spinless fermion model by using the Jordan-Wigner transformation. The ground state energy is calculated exactly within the framework of Bethe ansatz method. It turns out that there exists a critical value of the magnetic field strength beyond which the ground state becomes degenerate. This result agrees well with previous numerical calculations based on exact diagonalization technique. \n \n In addition we calculate the density-density correlation function as well as the momentum distribution function numerically. These results agree very well with those obtained analytically through the use of Bethe ansatz equations. Finally, we discuss how our results may be generalized to higher dimensions. Introduction:-In recent years considerable attention has been paid to the problem of strongly correlated electrons in low dimensional systems such as quantum wires or carbon nanotubes  1-3 . One of the most interesting phenomena observed experimentally in these systems is the fractional quantized Hall effect (FQHE)  4  . In particular it was shown that when the number of electrons N is odd ,the lowest Landau level(LLL) will contain only one electron per flux quanta  5  .The FQHEs have attracted much interest because they provide us with a unique opportunity to investigate many-body effects in condensed matter physics  6  .\nRecently, several authors  7-10  studied the properties of the modified coulomb interaction between two oppositely charged particles moving in a uniform magnetic field B perpendicularly to their plane of motion. They found that the ground-state energy depends crucially upon whether the total angular momentum J = L + S is zero or not where L is orbital angular momentum and S is spin angular momentum. For example if J=0 then the ground state energy is given by E0=−e2/lB+O(1/N),where lB=eB/mc is the magnetic length  11  .On the other hand if J=1/2 then the ground state energy takes the form E0",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A different glance at the modified Coulomb current in a powerful magnetic force . Abstract : We research the impact of an external magnetic force on the modified Coulomb potential for two particles with opposite charges and masses , which are confined to move along one dimension .We see that this scheme can be mapped onto a spinless fermion model by using the Jordan - Wigner transformation . The ground state energy is calculated exactly within the framework of Bethe ansatz technique .It turns out that there exists a critical quantity of the magnetic force power beyond which the ground state remains degenerate . This result agrees well with previous quantitative calculations based on exact diagonalization technique .In addition we determine the density - density correlation function as well as the velocity distribution relation numerically . These conclusions follow very well with those achieved analytically through the using of Bethe ansatz equations .Finally , we talk how our findings may be generalized to higher dimensions . Introduction : - In recent years considerable focus has been paid to the question of highly correlated electrons in low dimensional networks such as quantum wires or carbon nanotubes 1 - 3 .One of the most exciting phenomena observed experimentally in these systems is the fractional quantized Hall impact ( FQHE ) 4 . In particular it was shown that when the number of atoms N is odd , the highest Landau level ( LLL ) will hold only one electron per flux quanta 5 . The FQHEs have garnered many interest because they give us with a unique opportunity to examine multiple - bodies phenomena in condensed matter theory 6 .Recently , various scientists 7 - 10 studied the properties of the modified coulomb interaction between two oppositely charged particles moving in a uniform magnetic force B perpendicularly to their plane of movement . They found that the ground - state energy relies crucially upon whether the total angular velocity J = L + S is zero or not where L is orbital angular velocity and S is spin angular velocity .For instance if J = 0 then the ground state energy is given by E0 = −e2 / lB + O ( 1 / N ) , where lB = eB / mc is the magnetic thickness 11 . On the other hand if J = 1 / 2 then the ground state energy takes the form E0",
        "rewrite_text": "**Title:** A New Perspective on the Modified Coulomb Current in a Strong Magnetic Field\n\n**Abstract:** This study investigates the influence of an external magnetic field on the modified Coulomb potential experienced by two particles with opposite charges and masses, constrained to one-dimensional motion. We demonstrate that this system can be effectively represented as a spinless fermion model through the application of the Jordan-Wigner transformation. Utilizing the Bethe ansatz technique, we derive the ground state energy with precision. Our findings reveal the existence of a critical threshold for the strength of the magnetic field beyond which the ground state energy remains degenerate. This observation aligns closely with previous results obtained through exact diagonalization methods. Furthermore, we numerically compute the density-density correlation function and the velocity distribution, which corroborate the analytical results derived from the Bethe ansatz equations. We also discuss the implications of our findings and how they may be extended to higher-dimensional systems. \n\n**Introduction:** Recent years have seen a surge of interest in the behavior of highly correlated electrons in low-dimensional structures, such as quantum wires and carbon nanotubes. One of the most intriguing phenomena observed in these systems is the fractional quantum Hall effect (FQHE). Notably, it has been established that when the number of atoms (N) is odd, the highest Landau level (LLL) accommodates only one electron per flux quantum. The FQHE has attracted significant attention as it provides a unique platform for exploring many-body phenomena within condensed matter physics. Recent research has focused on the modified Coulomb interaction between two oppositely charged particles subjected to a uniform magnetic field perpendicular to their motion. These studies have highlighted that the ground state energy is critically dependent on the total angular momentum (J = L + S), where L represents the orbital angular momentum and S denotes the spin angular momentum. For instance, when J = 0, the ground state energy is expressed as E0 = −e²/lB + O(1/N), with lB being the magnetic length. Conversely, for J = 1/2, the expression for the ground state energy takes on a different form.",
        "ori-fast-z-score": -0.38691161626706844,
        "water-fast-z-score": 7.51625689070423,
        "rewrite-fast-z-score": 2.4333213169614383
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Molecular line intensities as measures of cloud masses - II. Conversion factors for specific galaxy types .\nAbstract:\nWe present the results of our analysis of molecular gas mass estimates based on CO and HCN observations in nearby galaxies, using data obtained with the IRAM 30m telescope. We find that conversion factors between luminosity and mass are strongly dependent on the star formation rate (SFR) per unit area within each galaxy disk. The SFR surface density is found to be an important parameter controlling the conversion factor XCO = M(H2)/L(CO), which we derive by fitting the observed L(HCN) / L(CO) ratio versus metallicity relation. For low values of ΣSFR < 1M⊙ yr-1 kpc-2 , corresponding to quiescent disks or nuclear regions dominated by old stellar populations, we obtain XCO ≈ 2 × 10 20 cm−2 K−1 km−1 s. This value increases up to XCO ≈ 5×10 20 cm−2 K−1km−1s at high ΣSFR > 3M⊙yr-1kpc-2 . These findings suggest that the physical conditions of the interstellar medium may change significantly depending on whether it is located in actively star-forming regions or not.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Molecular line intensities as indicators of cloud masses - II . Conversion factors for specific galaxy types .Abstract : We present the conclusion of our analysis of molecular gas mass estimates based on CO and HCN measurements in nearby galaxies , using data acquired with the IRAM 30m telescope . We see that conversion factors between luminosity and mass are strongly dependent on the star formation rate ( SFR ) per unit area within each galaxy disk .The SFR ground density is found to be an important function regulating the transformation parameter XCO = M ( H2 ) / L ( CO ) , which we derive by fitting the seen L ( HCN ) / L ( CO ) ratio versus metallicity relation . For low values of ΣSFR < [UNK] yr - 1 kpc - 2 , equivalent to quiescent disks or atomic regions dominated by ancient stars populations , we obtain XCO ≈ 2 × 10 20 cm−2 K−1 km−1 s . This value rises up to XCO ≈ 5×10 20 cm−2 K−1km−1s at high ΣSFR > [UNK] - 1kpc - 2 .These studies imply that the physical conditions of the interstellar medium may change considerably depending on whether it is situated in actively star - creating areas or not .",
        "rewrite_text": "We present the findings of our comprehensive analysis regarding the estimation of molecular gas masses in nearby galaxies, utilizing CO and HCN measurements obtained from the IRAM 30m telescope. Our research reveals that the conversion factors linking luminosity to mass exhibit a significant dependence on the star formation rate (SFR) per unit area within the disks of galaxies. Specifically, we identify the SFR surface density as a critical parameter influencing the transformation factor XCO, defined as the ratio of molecular hydrogen mass (M(H2)) to carbon monoxide luminosity (L(CO)). This relationship is established through the fitting of the observed ratio of L(HCN) to L(CO) against metallicity. \n\nIn our analysis, we find that for low star formation rates (ΣSFR < [UNK] yr^-1 kpc^-2), which correspond to quiescent disks or regions dominated by older stellar populations, the conversion factor is approximately XCO ≈ 2 × 10^20 cm^-2 K^-1 km^-1 s. Conversely, in regions with high star formation rates (ΣSFR > [UNK] yr^-1 kpc^-2), indicative of active star formation, the conversion factor increases to around XCO ≈ 5 × 10^20 cm^-2 K^-1 km^-1 s. \n\nThese findings suggest that the physical conditions within the interstellar medium can vary significantly based on the presence of active star formation. This variability has important implications for understanding the dynamics of molecular gas in different galactic environments and emphasizes the necessity of considering local star formation activity when estimating molecular gas masses in galaxies. Our results contribute to a deeper understanding of the interplay between star formation and molecular gas properties, providing valuable insights for future studies in galactic astrophysics.",
        "ori-fast-z-score": -0.10721125348377948,
        "water-fast-z-score": 4.477215043467819,
        "rewrite-fast-z-score": 0.17677669529663687
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Effect of electron-electron interaction on the phonon-mediated spin relaxation in quantum dots .\nAbstract:\nWe study the effect of electron-electron interactions (EEI) on the spin-flip rate due to phonons in semiconductor quantum dots. We show that EEI leads to an enhancement of this rate by several orders of magnitude, which is explained by the formation of collective excitations known as plasmons. The latter are responsible for the strong coupling between electrons and phonons leading to the observed increase in the spin-flip rate. Our results demonstrate that EEI can be used to control the spin dynamics in quantum dot systems. \nPACS numbers: 73.20.Dd, 74.78.Na, 78.67.Hc \nThe ability to manipulate spins in semiconductors has attracted considerable attention over recent years because it could lead to novel applications such as spin-based transistors or logic devices  1  . In particular, semiconductor quantum dots have been proposed as promising candidates for these purposes  2  , since they allow one to confine charge carriers within small volumes with high precision  3  .\nIn order to use quantum dots for spin manipulation, however, we need to understand how their spin states evolve under different conditions. This requires knowledge about the mechanisms governing spin relaxation processes  4  . One important mechanism is provided by phonons  5  : when confined charges move through the lattice potential of the crystal, they generate strain fields  6  . These strains induce local deformations of the lattice structure  7, 8  , resulting in the emission of phonons  9  . Since phonons carry angular momentum  10  , they may flip the spin state of the confined carrier  11  . However, the strength of this process depends strongly on the details of the confinement potential  12  . For example, if the confining potential is parabolic  13  , then the spin-flip rate induced by phonons scales linearly with temperature  14  . On the other hand, if the confining",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Effect of electron - electron interaction on the phonon - mediated spin relaxation in quantum dots . Abstract : We research the impact of electron - electron interactions ( EEI ) on the spin - flip rate due to phonons in semiconductor quantum dots .We see that EEI results to an enhancement of this rate by many orders of magnitude , which is explained by the formation of collective excitations known as plasmons . The latter are responsible for the strong coupling between electrons and phonons leading to the known increase in the spin - flip rate .Our results show that EEI can be used to affect the spin behavior in quantum dot structures . PACS codes : 73 . 20 . Dd , 74 . 78 . Na , 78 . 67 . Hc The capacity to modify spins in semiconductors has garnered considerable scrutiny over recent months because it could lead to novel applications such as spin - based transistors or logic devices 1 .In particular , semiconductor quantum bits have been proposed as possible candidates for these purposes 2 , since they allow one to confine charge carriers within tiny volumes with high precision 3 . In order to use quantum dots for momentum manipulation , however , we require to explain how their spin states evolve under various circumstances .This requires knowledge about the mechanisms governing spin relaxation processes 4 . One important process is provided by phonons 5 : when confined charges move through the crystal potential of the crystal , they generate stress fields 6 .These strains induce regional deformations of the lattice structure 7 , 8 , resulting in the emission of phonons 9 . Since phonons hold angular velocity 10 , they may flip the spin state of the confined carrier 11 .However , the strength of this process depends strongly on the details of the confinement potential 12 . For instance , if the confining potential is parabolic 13 , then the spin - flip rate induced by phonons scales linearly with temperature 14 .On the other hand , if the confining",
        "rewrite_text": "**Title:** Impact of Electron-Electron Interactions on Phonon-Mediated Spin Relaxation in Quantum Dots\n\n**Abstract:** This study investigates the influence of electron-electron interactions (EEI) on the spin-flip rates induced by phonons in semiconductor quantum dots. Our findings reveal that EEI significantly enhances the spin-flip rate by several orders of magnitude, a phenomenon attributed to the emergence of collective excitations known as plasmons. These plasmons facilitate a robust coupling between electrons and phonons, which in turn leads to the observed increase in spin-flip rates. The implications of our results suggest that EEI can be strategically utilized to manipulate spin dynamics within quantum dot structures. \n\nThe ability to control spin states in semiconductors has attracted considerable attention in recent months due to its potential applications in spin-based transistors and logic devices. Semiconductor quantum bits, or qubits, are particularly promising candidates for these applications, as they enable the confinement of charge carriers within minuscule volumes with remarkable precision. However, to effectively harness quantum dots for spin manipulation, it is crucial to understand the evolution of their spin states under varying conditions, necessitating a comprehensive understanding of the mechanisms that govern spin relaxation processes.\n\nOne of the key processes influencing spin relaxation is phonon interaction. As confined charge carriers traverse the crystal lattice, they generate stress fields that induce local deformations in the lattice structure, leading to the emission of phonons. Given that phonons possess angular momentum, they have the capacity to alter the spin state of the confined carriers. The efficiency of this spin-flip process is highly dependent on the specifics of the confinement potential. For example, in cases where the confining potential is parabolic, the phonon-induced spin-flip rate exhibits a linear dependence on temperature. Conversely, variations in the confinement potential can lead to different scaling behaviors, underscoring the complexity of spin dynamics in quantum dot systems. \n\nIn summary, our research highlights the critical role of electron-electron interactions in enhancing phonon-mediated spin relaxation, paving the way for advanced applications in quantum computing and spintronics. \n\n**PACS codes:** 73.20.Dd, 74.78.Na, 78.67.Hc",
        "ori-fast-z-score": 0.727606875108999,
        "water-fast-z-score": 6.184788832549241,
        "rewrite-fast-z-score": 1.0613372610104648
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Spectral estimation on a sphere in geophysics and cosmology .\nAbstract:\nWe present an overview of the state-of-the-art methods for spectral analysis on spherical data, with emphasis on their applications to problems arising in geophysical sciences (e.g., global seismological tomography) and astrophysics (e.g., cosmic microwave background). We also discuss some recent advances in this area that have been made by our group at Columbia University. The main focus is on the development of new algorithms for computing accurate estimates of the power spectrum of signals defined over the surface of the unit sphere using only partial information about these signals. In particular, we consider two classes of methods: those based on the use of spherical harmonic expansions and those based on wavelet transforms. Finally, we briefly describe several open research questions related to the topic discussed here. Spherical data arise naturally in many areas of science including astronomy, meteorology, oceanography, geodesy, and medicine. For example, astronomers routinely collect large amounts of data describing the positions of celestial objects such as stars or galaxies; similarly, weather forecasters gather measurements of atmospheric pressure, temperature, humidity, wind speed, etc., at various locations around the globe. These types of data are often represented mathematically as functions defined over the surface of a sphere.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Spectral estimation on a sphere in geophysics and cosmology . Abstract : We bring an overview of the state - of - the - art methods for spectral evaluation on spherical measurements , with emphasis on their applications to problems arose in geophysical disciplines ( e . g . , global seismological tomography ) and astrophysics ( e . g . , cosmic microwave background ) .We also discuss some latest advances in this area that have been made by our team at Columbia University . The main interest is on the development of new schemes for modeling precise estimates of the power spectrum of waves defined over the surface of the unit sphere utilizing only partial knowledge about these signals .In particular , we investigate two groups of methods : those based on the using of spherical harmonic expansions and those based on wavelet transforms . Finally , we briefly illustrate numerous open scientific questions related to the subject mentioned here .Spherical data arise naturally in multiple fields of science primarily astronomy , meteorology , oceanography , geodesy , and medicine . For instance , astronomers typically gather huge amounts of statistics explaining the places of heavenly objects such as planets or galaxies ; likewise , weather forecasters gather measurements of atmospheric pressure , temperature , moisture , breeze pressure , etc . , at numerous locations around the world .These sorts of measurements are often represented mathematically as functions defined over the surface of a sphere .",
        "rewrite_text": "In this article, we provide a comprehensive overview of the cutting-edge techniques employed for spectral estimation on spherical data, focusing on their relevance to various challenges encountered in geophysics and cosmology. Specifically, we highlight applications in global seismological tomography and the analysis of the cosmic microwave background. Our research team at Columbia University has made significant strides in this domain, particularly in developing innovative methodologies for accurately estimating the power spectrum of wave phenomena defined on the surface of a unit sphere, even when only partial information about these signals is available.\n\nWe categorize our discussion into two primary methodological approaches: those utilizing spherical harmonic expansions and those employing wavelet transforms. Each of these techniques offers unique advantages for analyzing spherical data, which is prevalent across numerous scientific disciplines, including astronomy, meteorology, oceanography, geodesy, and medicine. For example, astronomers collect extensive datasets detailing the positions of celestial bodies such as planets and galaxies, while meteorologists compile atmospheric measurements—such as pressure, temperature, and humidity—across various global locations. These types of data are often mathematically represented as functions defined over the surface of a sphere.\n\nIn addition to reviewing existing methods, we also address recent advancements made by our research group, which enhance the precision and applicability of spectral estimation techniques. Furthermore, we outline several unresolved scientific questions that remain in this field, emphasizing the need for continued exploration and innovation. The insights gained from this research not only contribute to our understanding of geophysical and cosmological phenomena but also pave the way for future developments in the analysis of spherical data across diverse scientific areas.",
        "ori-fast-z-score": -2.209379082955976,
        "water-fast-z-score": 6.563952321998544,
        "rewrite-fast-z-score": -0.25630729731502827
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The initial conditions of star formation in the Ophiuchus main cloud: Kinematics of the protocluster condensations .\nAbstract:\nWe present new observations of the kinematic properties of the dense gas associated with the young stellar cluster forming within the Ophiuchus molecular cloud (d = 140 pc). We used the Submillimeter Array to observe two fields, one centered on the infrared source IRAS 04368+2557 and another located about 1 arcmin away at the position of the Spitzer c2d survey target L1641N. The data reveal that both regions are characterized by complex velocity structures which we interpret as being due to multiple overlapping protostellar cores. In particular, our results show that:  -The region surrounding IRAS 04368+25570 is dominated by three distinct components separated by less than 0.1 pc along the line-of-sight.  -The region around L1641N contains several compact sources embedded in an extended envelope whose emission peaks at velocities between 5 and 10 km s-1 relative to the local standard of rest.   -Both regions contain numerous outflows driven by protostars still deeply embedded in their natal envelopes.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The initial conditions of galaxy formation in the Ophiuchus principal cloud : Kinematics of the protocluster condensations . Abstract : We report new studies of the kinematic qualities of the deep gas corresponding with the young stellar cluster forming within the Ophiuchus molecular cloud ( d = 140 pc ) .We utilized the Submillimeter Array to observe two fields , one concentrated on the infrared source IRAS 04368 + 2557 and another situated about 1 arcmin away at the position of the Spitzer c2d survey destination L1641N . The data reveal that both locations are marked by complex momentum systems which we treat as being owing to multiple overlapping protostellar cores .In particular , our findings show that : - The region following IRAS 04368 + 25570 is dominated by three separate constituents joined by less than 0 . 1 pc along the line - of - seeing . - The region around L1641N contains several compact sources embedded in an extended envelope whose emission peaks at velocities between 5 and 10 km s - 1 relative to the local standard of rest .- Both regions feature numerous outflows driven by protostars already deeply lodged in their natal envelopes .",
        "rewrite_text": "We present a comprehensive analysis of the kinematic properties of the gas associated with a young stellar cluster forming within the Ophiuchus molecular cloud, located at a distance of 140 parsecs. Utilizing the Submillimeter Array, we conducted observations of two distinct fields: one centered on the infrared source IRAS 04368 + 2557 and the other approximately 1 arcminute away, corresponding to the Spitzer c2d survey target L1641N. Our observations reveal intricate momentum structures in both regions, which we interpret as the result of multiple overlapping protostellar cores. Specifically, our results indicate that the area surrounding IRAS 04368 + 2557 is characterized by three distinct components that are closely aligned, separated by less than 0.1 parsecs along the line of sight. In contrast, the L1641N region is comprised of several compact sources embedded within a more extensive envelope, with emission peaking at velocities ranging from 5 to 10 km/s relative to the local standard of rest. Furthermore, both regions exhibit a variety of outflows driven by protostars that are deeply embedded within their surrounding envelopes. These findings contribute to our understanding of the initial conditions of galaxy formation, highlighting the complex dynamics at play in the early stages of stellar cluster development within molecular clouds. The insights gained from this study are crucial for advancing our knowledge of star formation processes and the evolution of protoclusters in the universe.",
        "ori-fast-z-score": -1.3251783128981585,
        "water-fast-z-score": 4.719863179556685,
        "rewrite-fast-z-score": 0.6897304947150052
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Two Qubit Entanglement in $XYZ$ Magnetic Chain with DM Antisymmetric Anisotropic Exchange Interaction .\nAbstract:\nWe study the entanglement properties of two qubits coupled by an antisymmetric anisotropic exchange interaction (AAEI) in a magnetic chain described by the Heisenberg model on the square lattice. We show that for certain values of the parameters, this system can be mapped onto a spin-1/2 XYZ chain and we calculate analytically its concurrence as well as the von Neumann entropy. The results are compared to those obtained numerically using exact diagonalization techniques. In particular, it is shown that the AAEI leads to a significant increase of the concurrence between nearest-neighboring spins when compared to the standard XXZ case. Moreover, we find that there exists a critical value of the anisotropy parameter beyond which no entanglement survives. Finally, we discuss how our findings could be tested experimentally. Introduction:-Entangled states play a crucial role in quantum information processing  1  . Therefore, understanding their generation mechanisms has been one of the main goals of many theoretical investigations  2  -  4  .\nIn recent years, much attention was paid to the investigation of entanglement in various types of spin chains  5  , including the so-called XXZ chain  6 -  8  . However, most studies were focused only on the ground state  9  or low lying excited states  10  of these systems. On the other hand, recently developed experimental techniques allow us to prepare highly excited states  11  . Thus, it becomes important to investigate also higher energy levels  12  .\nThe aim of this work is to analyze the entanglement properties of a pair of qubits interacting via an antisymmetric anisotropic Heisenberg exchange term  13  . This type of coupling appears naturally in several physical models  14  -  16  . For example, it describes the spin-spin interactions in molecular magnets  17  where the total angular momentum J = 0  18  . It should be noted here that such molecules have attracted considerable interest due to their potential applications in quantum computing  19  . Another interesting application concerns the description of excitations in high-Tc superconductors  20  . Here, the presence of the antisymmetric anisotropic exchange term may lead to new phenomena  21  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Two Qubit Entanglement in $ XYZ $ Magnetic Chain with DM Antisymmetric Anisotropic Exchange Interaction . Abstract : We research the entanglement properties of two qubits coupled by an antisymmetric anisotropic exchange interaction ( AAEI ) in a magnetic chain described by the Heisenberg model on the square lattice .We see that for particular values of the variables , this scheme can be mapped onto a spin - 1 / 2 XYZ chain and we estimate analytically its concurrence as well as the von Neumann entropy . The results are compared to those achieved numerically use accurate diagonalization techniques .In particular , it is demonstrated that the AAEI results to a substantial rise of the concurrence between nearest - neighboring spins when compared to the standard XXZ case . Moreover , we find that there exists a critical quantity of the anisotropy parameter beyond which no entanglement survives .Finally , we talk how our findings may be evaluated experimentally . Introduction : - Entangled states play a crucial role in quantum information processing 1 .Therefore , studying their generation pathways has been one of the main goals of several theoretical investigations 2 - 4 . In past decades , increasing attention was given to the examination of entanglement in different kinds of spin rings 5 , notably the so - called XXZ ring 6 - 8 .However , most studies were focused only on the ground state 9 or low lying excited states 10 of these systems . On the other hand , recently advanced experimental methods able us to analyze highly excited states 11 .Thus , it becomes crucial to examine also greater energy levels 12 . The goal of this research is to analyze the entanglement properties of a pair of qubits interacting via an antisymmetric anisotropic Heisenberg transfer term 13 .This kind of coupling appears naturally in multiple physical models 14 - 16 . For instance , it explains the spin - spinning interactions in molecular magnets 17 where the total angular velocity J = 0 18 .It should be mentioned here that such compounds have garnered considerable interest due to their potential applications in quantum computing 19 . Another important use involves the description of excitations in high - Tc superconductors 20 .Here , the presence of the antisymmetric anisotropic exchange term may contribute to new concepts 21 .",
        "rewrite_text": "**Title:** Two Qubit Entanglement in $XYZ$ Magnetic Chain with DM Antisymmetric Anisotropic Exchange Interaction\n\n**Abstract:** This study investigates the entanglement characteristics of two qubits that are coupled through an antisymmetric anisotropic exchange interaction (AAEI) within a magnetic chain modeled by the Heisenberg framework on a square lattice. We demonstrate that under specific conditions, this system can be effectively represented as a spin-1/2 XYZ chain. Through analytical methods, we calculate the concurrence and von Neumann entropy of the system, and we juxtapose these findings with numerical results obtained via precise diagonalization techniques. Notably, our analysis reveals that the presence of AAEI significantly enhances the concurrence between nearest-neighbor spins compared to the conventional XXZ model. Furthermore, we identify a critical threshold for the anisotropy parameter, beyond which entanglement is completely lost. Our findings not only contribute to the theoretical understanding of entanglement in quantum systems but also have potential implications for experimental evaluations. \n\n**Introduction:** Entangled states are fundamental to the field of quantum information processing, making the exploration of their generation pathways a primary focus of numerous theoretical studies. Over recent decades, there has been a growing interest in the investigation of entanglement within various spin ring systems, particularly the XXZ ring. However, much of the existing research has concentrated on the ground state or low-lying excited states of these systems. With advancements in experimental techniques, it is now possible to probe highly excited states, highlighting the necessity to extend our analysis to higher energy levels. This research aims to explore the entanglement properties of a pair of qubits interacting through an antisymmetric anisotropic Heisenberg transfer term, a coupling that naturally arises in several physical models. This interaction is particularly relevant in the context of molecular magnets, where the total angular momentum is zero, and has garnered significant attention due to its potential applications in quantum computing. Additionally, it plays a crucial role in describing excitations in high-temperature superconductors, where the antisymmetric anisotropic exchange term may lead to novel insights.",
        "ori-fast-z-score": -0.159111456835146,
        "water-fast-z-score": 8.224234724873552,
        "rewrite-fast-z-score": 1.8717134551736667
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The influence of horizontal gene transfer on the mean fitness of unicellular populations in static environments .\nAbstract:\nHorizontal Gene Transfer (HGT) is an important evolutionary force that can increase genetic diversity and accelerate evolution, but it also has negative effects such as disrupting co-adapted gene complexes or introducing deleterious mutations into recipient genomes. In this study we investigate how HGT affects the mean fitness of unicells evolving under different environmental conditions using computational models. We find that HGT increases the mean fitness when cells are exposed to fluctuating environments with high levels of stressful events. However, if there are only mild fluctuations then HGT decreases the mean fitness because it introduces harmful mutations. Finally, for constant environments without any external stresses, HGT does not affect the mean fitness at all. Our results suggest that HGT may have played an important role during early stages of life s evolution by increasing its adaptability to changing environments. Horizontal Gene Transfer (HGT), which occurs between organisms sharing similar DNA sequences, is one of the most significant evolutionary forces known today  1  . It allows rapid acquisition of new genes and thus contributes to increased genetic diversity within species  2  , accelerates evolution  3  , and facilitates adaptation  4  .\nHowever, HGT also has some disadvantages including disruption of co-adapted gene complexes  5  and introduction of deleterious mutations  6  . Therefore, understanding the effect of HGT on population dynamics requires careful investigation  7, 8  . Previous studies suggested that HGT could be beneficial for populations living in fluctuating environments  9  while detrimental for those inhabiting stable ones  10  . Here we use computational models to explore these hypotheses further and show that HGT can either increase or decrease the mean fitness depending on the type of environment inhabited by the cell population.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The impact of horizontal gene transfer on the mean fitness of unicellular populations in static environments . Abstract : Horizontal Gene Transfer ( HGT ) is an important genetic force that can increase genetic diversity and accelerate evolution , but it also has negative impacts such as disrupting co - adapted gene structures or introducing deleterious variants into recipient genomes .In this study we investigate how HGT affects the mean fitness of unicells evolving under various environmental conditions utilizing computational models . We see that HGT changes the mean fitness when cells are exposed to fluctuating environments with high levels of stressful events .However , if there are only slight fluctuations then HGT decreases the mean fitness because it creates harmful mutations . Finally , for constant environments without any external stresses , HGT does not alter the mean fitness at all .Our results show that HGT could have played an important role during initial stages of life s evolution by increasing its adaptability to changing settings . Horizontal Gene Transfer ( HGT ) , which occurs between organisms sharing related DNA sequences , is one of the most significant evolutionary forces known today 1 .It enables quick acquisition of new genes and therefore contributes to greater genetic diversity within genus 2 , accelerates development 3 , and facilitates adaptation 4 . However , HGT also has some disadvantages notably loss of co - adapted gene structures 5 and entry of deleterious variants 6 .Therefore , studying the impact of HGT on population behavior needs thorough investigation 7 , 8 . Previous studies suggested that HGT could be beneficial for individuals living in fluctuating environments 9 while detrimental for those inhabiting stable ones 10 .Here we using computational models to examine these hypotheses further and suggest that HGT can either increase or decrease the mean fitness depending on the kind of environment populated by the cell population .",
        "rewrite_text": "**Title:** The Impact of Horizontal Gene Transfer on the Mean Fitness of Unicellular Populations in Static Environments\n\n**Abstract:** Horizontal Gene Transfer (HGT) is a crucial mechanism in genetics that enhances genetic diversity and accelerates evolutionary processes. However, it also poses risks, such as the disruption of co-adapted gene complexes and the introduction of harmful mutations into the genomes of recipient organisms. This study explores the effects of HGT on the mean fitness of unicellular populations under varying environmental conditions, employing computational modeling techniques. Our findings indicate that HGT can significantly influence mean fitness in environments characterized by frequent and intense stressors. In such fluctuating conditions, HGT tends to enhance mean fitness by facilitating the acquisition of advantageous traits. Conversely, in environments with minimal fluctuations, HGT appears to diminish mean fitness due to the introduction of deleterious mutations that compromise cellular function. Notably, in stable environments devoid of external stressors, HGT does not significantly impact mean fitness, suggesting that its effects are context-dependent. These results imply that HGT may have played a pivotal role in the early stages of life's evolution by enhancing adaptability to dynamic environments. HGT, which occurs between organisms with shared genetic material, is recognized as one of the most significant evolutionary forces, enabling rapid gene acquisition and fostering greater genetic diversity within populations. While previous research has indicated that HGT can be advantageous for organisms in variable environments, it may prove detrimental for those in stable settings. Our study further investigates these dynamics, revealing that the influence of HGT on mean fitness is contingent upon the specific environmental context in which unicellular populations exist. This nuanced understanding of HGT's role in evolutionary biology underscores the importance of considering environmental factors when assessing its impact on population dynamics.",
        "ori-fast-z-score": 1.1430952132988164,
        "water-fast-z-score": 7.033533126053657,
        "rewrite-fast-z-score": 1.52127765851133
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Constrained simulations of the local universe: I. Mass and motion in the Local Volume .\nAbstract:\nWe present constrained realizations of the mass distribution in the nearby universe, based on the Millennium Run N-body simulation (Springel et al., 2005) combined with galaxy redshift surveys. We use two different methods to construct mock catalogues for comparison with observations: The first method is based on the conditional luminosity function formalism developed by Yang et al. (2003) , which we apply to galaxies selected from the Sloan Digital Sky Survey Data Release 7 (Abazajian et al., 2009 ). In this approach, each galaxy has an assigned halo mass drawn from its luminosity-dependent probability density function. The second method uses the Halo Occupation Distribution model introduced by Zheng et al. (2005) . Here, the number of central and satellite galaxies within haloes are determined using their respective HODs as functions of host halo mass. For both approaches, we compare our results against observational data sets including the 2MASS Redshift Survey (Huchra et al., 2012; Bilicki & Chodorowski, 2013), 6dF Galaxy Survey (Jones et al., 2004 ) and the WiggleZ Dark Energy Survey (Drinkwater et al., 2010).",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Constrained simulations of the local universe : I . Mass and motion in the Local Volume .Abstract : We create constrained realizations of the mass distribution in the nearby universe , using on the Millennium Run N - bodies simulation ( Springel et al . , 2005 ) coupled with star redshift surveys . We use two different methods to build mock catalogues for comparison with observations : The first method is based on the conditional luminosity function formalism developed by Yang et al .( 2003 ) , which we apply to galaxies chose from the Sloan Digital Sky Survey Data Release 7 ( Abazajian et al . , 2009 ) . In this methodology , each galaxy has an chosen halo mass drawn from its luminosity - dependent probability density function .The second method uses the Halo Occupation Distribution approach proposed by Zheng et al . ( 2005 ) .Here , the proportion of central and satellite galaxies within haloes are decided using their different HODs as functions of host halo mass . For both approaches , we compare our findings against observational data sets including the 2MASS Redshift Survey ( Huchra et al . , 2012 ; Bilicki & Chodorowski , 2013 ) , 6dF Galaxy Survey ( Jones et al . , 2004 ) and the WiggleZ Dark Energy Survey ( Drinkwater et al . , 2010 ) .",
        "rewrite_text": "In this study, we present constrained realizations of the mass distribution in the local universe by leveraging the Millennium Run N-body simulation (Springel et al., 2005) in conjunction with star redshift surveys. Our objective is to create mock catalogues that can be effectively compared with observational data. To achieve this, we employ two distinct methodologies. The first method utilizes the conditional luminosity function framework established by Yang et al. (2003), which we apply to galaxies selected from the Sloan Digital Sky Survey Data Release 7 (Abazajian et al., 2009). In this approach, each galaxy is assigned a halo mass derived from its luminosity-dependent probability density function. The second method is based on the Halo Occupation Distribution (HOD) model introduced by Zheng et al. (2005). This technique determines the distribution of central and satellite galaxies within haloes by utilizing their respective HODs as functions of the host halo mass. We rigorously compare the results from both methodologies against several observational datasets, including the 2MASS Redshift Survey (Huchra et al., 2012; Bilicki & Chodorowski, 2013), the 6dF Galaxy Survey (Jones et al., 2004), and the WiggleZ Dark Energy Survey (Drinkwater et al., 2010). Through this comprehensive analysis, we aim to enhance our understanding of mass and motion within the Local Volume, providing valuable insights into the structure and dynamics of the nearby universe.",
        "ori-fast-z-score": -0.5698028822981898,
        "water-fast-z-score": 3.2627549126854696,
        "rewrite-fast-z-score": -0.329292779969071
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The widths of quarkonia in quark gluon plasma .\nAbstract:\nWe study the transverse momentum dependence of J/ψ and Υ production cross sections at RHIC energies within an effective field theory approach, which includes both elastic scattering off quarks as well as inelastic processes such as dissociation into open charm or bottom hadrons.  We find that the observed suppression pattern can be reproduced by including only elastic scattering for pT < 2 GeV/c while additional contributions are needed to describe data points with larger values of pT . The latter turn out to be dominated by inelastic processes like dissociation into open heavy flavor mesons. In particular we show that the inclusion of these effects leads to a significant reduction of the predicted nuclear modification factor RAA(pT ) compared to previous calculations based on purely elastic interactions. \nPACS numbers: 12.38.Mh, 25.75.-q, 11.10.Kk \nI. INTRODUCTORY REMAR K\nThe measurement of charmonium (J/ψ) and bottomonium (Υ) production is one of the most promising probes to investigate properties of hot and dense matter created in relativistic nucleus-nucleus collisions  1  . It has been suggested that the interaction between the produced quarkonia and the surrounding medium may lead to their partial melting  2  , i.e., to a decrease of the bound state masses due to color screening  3  .\nIn this work we present results obtained within an effective field theory framework  4  , where the relevant degrees of freedom are quarks and gluons rather than individual hadronic states. This allows us to calculate the total cross section for quarkonium production in terms of elementary partonic subprocesses involving light quarks q = u, d, s and gluons g. These include elastic scattering off quarks and gluon-gluon fusion leading to the formation of quarkonia via the creation of a virtual qq pair  5  . Furthermore, inelastic processes such as quarkonium dissociation into open heavy-flavor hadrons  6  have also been included  7, 8  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The widths of quarkonia in quark gluon plasma . Abstract : We study the transverse momentum dependence of J / ψ and [UNK] production cross sections at RHIC energies within an effective field theory approach , which includes both elastic scattering off quarks as well as inelastic processes such as dissociation into open charm or bottom hadrons .We see that the observed suppression behavior can be reproduced by including only elastic scattering for pT < 2 GeV / c while extra contributions are needed to define data points with larger values of pT . The latter come out to be dominated by inelastic reactions like dissociation into open heavy flavor mesons .In particular we prove that the introduction of these influences result to a substantial decreased of the expected nuclear modification factor RAA ( pT ) compared to previous analyses based on purely elastic interactions . PACS scores : 12 . 38 . Mh , 25 . 75 . - q , 11 . 10 . Kk I .INTRODUCTORY REMAR K The measurement of charmonium ( J / ψ ) and bottomonium ( [UNK] ) production is one of the most attractive probes to investigate properties of hot and dense matter created in relativistic nucleus - nucleus collisions 1 . It has been speculated that the interaction between the produced quarkonia and the surrounding medium may lead to their partial melting 2 , i . e . , to a decrease of the bound state masses due to color screening 3 .In this study we present results derived within an efficient field theory framework 4 , where the appropriate degrees of liberty are quarks and gluons instead than individual hadronic states . This enables us to estimate the total cross section for quarkonium production in terms of elementary partonic subprocesses involving light quarks g = u , d , s and gluons g . These include elastic scattering off quarks and gluon - gluon fusion led to the formation of quarkonia via the creation of a virtual qq pair 5 .Furthermore , inelastic reactions such as quarkonium dissociation into open heavy - flavor hadrons 6 have already been used 7 , 8 .",
        "rewrite_text": "**Title:** The Widths of Quarkonia in Quark-Gluon Plasma\n\n**Abstract:** In this study, we investigate the transverse momentum dependence of J/ψ and bottomonium production cross sections at RHIC energies using an effective field theory framework. Our approach incorporates both elastic scattering interactions with quarks and inelastic processes, such as dissociation into open charm or bottom hadrons. We find that the observed suppression of quarkonia can be accurately described by considering only elastic scattering for transverse momentum (pT) values below 2 GeV/c. However, for higher pT values, additional contributions are necessary to align with experimental data, which are predominantly influenced by inelastic reactions, particularly the dissociation into open heavy flavor mesons. Our analysis demonstrates that including these inelastic processes leads to a significant reduction in the expected nuclear modification factor RAA(pT) compared to earlier studies that relied solely on elastic interactions. \n\nThe production of charmonium (J/ψ) and bottomonium ([UNK]) serves as a crucial probe for understanding the properties of the hot and dense matter generated in relativistic nucleus-nucleus collisions. It has been proposed that the interactions between the produced quarkonia and the surrounding quark-gluon plasma may result in partial melting, characterized by a decrease in the bound state masses due to color screening effects. In this work, we present results derived from an effective field theory that focuses on quarks and gluons as the fundamental degrees of freedom, rather than individual hadronic states. This framework allows us to estimate the total cross section for quarkonium production through various elementary partonic subprocesses involving light quarks (u, d, s) and gluons. These subprocesses include elastic scattering off quarks and gluon-gluon fusion, which leads to the formation of quarkonia via the creation of virtual quark-antiquark pairs. Additionally, we discuss the role of inelastic reactions, such as quarkonium dissociation into open heavy-flavor hadrons, which have been previously explored in the literature. Our findings contribute to a deeper understanding of quarkonia behavior in the quark-gluon plasma and its implications for nuclear physics.",
        "ori-fast-z-score": -0.5035088149780135,
        "water-fast-z-score": 4.166666666666667,
        "rewrite-fast-z-score": 1.0579249964025073
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Production of TeV gamma-radiation in the vicinity of the supermassive black hole in the giant radiogalaxy M87 .\nAbstract:\nThe authors report on observations made with the HESS telescope array, which detected emission at energies above 1TeV (1 teraelectronvolt) coming from an area within 0.2 degrees of the center of the galaxy M87.  The data are consistent with theoretical predictions that such emissions should be produced by particles accelerated near the event horizon of a supermassive black hole located there.   This is the first time this phenomenon has been observed outside our own Galaxy and it opens up new opportunities for studying particle acceleration processes around black holes. Black holes are among the most exotic objects known to science. They have no surface or edge but instead exist as singularities where space-time ends. In addition they exert enormous gravitational forces so that even light cannot escape their grasp. However, despite these extreme conditions, some scientists believe that matter can still be accelerated close to the speed of light inside the so-called  event horizons  surrounding black holes. Such high energy phenomena could produce extremely energetic photons called  TeV gammas  - short for Tera-Electron-Volt photons. These would then be detectable using ground-based telescopes like those used by the High Energy Stereoscopic System (HESS). On April 10, 2014, astronomers working with the HESS observatory announced the detection of TeV-gamma rays originating from the central region of the distant galaxy Messier 87 (M87), about 50 million light years away  1  . This was the first time that such radiation had ever been seen outside our own Milky Way  2  , opening up exciting possibilities for studying particle accelerators associated with black holes  3  .\nIn order to understand how this discovery came about we need to know more about what happens when matter falls into a black hole. As shown in Figure 1 below, if you were standing next to one you d see nothing special happening until your distance from its centre became smaller than its Schwarzschild radius  4  . At this point gravity becomes so strong that all forms of matter become trapped inside the black hole s event horizon  5  . Inside the event horizon, however,...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Production of TeV gamma - radiation in the vicinity of the supermassive black hole in the giant radiogalaxy M87 . Abstract : The authors report on observations made with the HESS telescope array , which detected radiation at energies above 1TeV ( 1 teraelectronvolt ) coming from an area within 0 . 2 degrees of the center of the galaxy M87 .The data are compatible with theoretical expectations that such emissions should be formed by particles driven near the event horizon of a supermassive black hole located there . This is the first time this phenomenon has been observed outside our own Galaxy and it opens up new opportunities for studying particle acceleration processes around black holes .Black holes are among the most exotic objects known to science . They have no surface or edge but instead appear as singularities where space - time finishes .In addition they exert enormous gravitational pressures so that even light cannot flee their grasp . However , despite these extreme circumstances , some scientists view that matter can always be advanced close to the speed of light inside the so - called event horizons surrounding black holes .Such high energy phenomena could generate incredibly energetic photons called TeV gammas - short for Tera - Electron - Volt photons . These would then be detectable using ground - based telescopes like those utilized by the High Energy Stereoscopic System ( HESS ) .On April 10 , 2014 , astronomers studying with the HESS telescope confirmed the discovery of TeV - gamma radiation coming from the central region of the distant galaxy Messier 87 ( M87 ) , about 50 million light years away 1 . This was the first time that such rays had ever been seen outside our own Milky Way 2 , opening up interesting possibilities for studying electron accelerators associated with black holes 3 .In order to comprehend how this discovery went about we require to see more about what comes when matter drops into a black hole . As seen in Figure 1 below , if you were standing close to one you d see nothing extraordinary occurring until your distance from its centre becoming smaller than its Schwarzschild diameter 4 .At this time gravity becomes so powerful that all forms of matter grow trapped inside the dark hole s event horizon 5 . Inside the event horizon , however , . . .",
        "rewrite_text": "**Title:** Production of TeV Gamma-Radiation in the Vicinity of the Supermassive Black Hole in the Giant Radiogalaxy M87\n\n**Abstract:** This study presents groundbreaking observations made using the High Energy Stereoscopic System (HESS) telescope array, which successfully detected gamma-ray emissions exceeding 1 TeV (tera-electronvolt) from a region within 0.2 degrees of the center of the giant radiogalaxy M87. These findings align with theoretical predictions suggesting that such high-energy emissions are generated by particles accelerated in the extreme gravitational fields near the event horizon of a supermassive black hole. This marks the first observation of TeV gamma radiation from outside our Milky Way galaxy, significantly enhancing our understanding of particle acceleration mechanisms in the vicinity of black holes.\n\nBlack holes are among the most enigmatic entities in astrophysics, characterized by their lack of a defined surface and their nature as singularities where the fabric of space-time ceases to exist. Their immense gravitational forces are so powerful that they prevent even light from escaping. Nevertheless, theoretical models propose that matter can be accelerated to velocities approaching the speed of light within the event horizons that envelop black holes. This acceleration can lead to the production of highly energetic photons, known as TeV gamma rays, which can be detected by ground-based observatories like HESS.\n\nOn April 10, 2014, astronomers utilizing the HESS telescope confirmed the presence of TeV gamma radiation emanating from the central region of M87, located approximately 50 million light-years from Earth. This discovery not only represents a significant milestone in astrophysics but also opens new avenues for investigating the mechanisms of electron acceleration associated with black holes. To fully grasp the implications of this finding, it is essential to explore the dynamics of matter as it approaches a black hole. As illustrated in the accompanying figure, an observer near a black hole would perceive no unusual activity until crossing the Schwarzschild radius, at which point the gravitational pull becomes so intense that all matter is inexorably drawn into the event horizon. Inside this boundary, the behavior of matter and radiation remains a profound mystery, inviting further research into the fundamental processes at play in these extreme environments.",
        "ori-fast-z-score": 0.38461538461538464,
        "water-fast-z-score": 7.056077897395808,
        "rewrite-fast-z-score": 1.7075311565539322
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The impact of accretion disk winds on the X-ray spectrum of AGN: Part 1 - XSCORT .\nAbstract:\nWe present an open-source code, called XSCORT (X-ray Spectral Code for Reprocessed Torus), which computes the reprocessing of radiation by optically-thick matter in the form of a torus and its associated wind. The code is written as a set of IDL routines that can be easily modified to study different geometries or physical conditions. We describe how we implemented our model using Monte Carlo techniques and discuss some tests performed with simulated data. Finally, we show examples of applications of this new tool to two well-studied objects, NGC 4151 and Mrk 509. \nThe main goal of XSCORT is to provide a flexible framework where one can explore the effects of various parameters such as: geometry, density distribution, chemical composition, ionization state, etc., on the observed spectral properties of active galactic nuclei (AGN). This will allow us to better understand their nature and evolution. \n \n In particular, we are interested in studying the effect of the presence of an outflowing component on the shape of the reflection hump produced by the innermost regions of the accretion disc around supermassive black holes. These winds may play an important role in shaping the broad-band continuum emission of these sources through absorption and/or scattering processes. They also affect the amount of material available to produce the reflected emission at larger distances from the central source.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The impact of accretion disk winds on the X - ray spectrum of AGN : Part 1 - XSCORT . Abstract : We produce an open - source code , entitled XSCORT ( X - ray Spectral Code for Reprocessed Torus ) , which computes the reprocessing of radiation by optically - heavy material in the form of a torus and its associated wind .The language is designed as a group of IDL procedures that can be easily modified to study various geometries or physical conditions . We define how we implemented our model utilizing Monte Carlo methods and explain some experiments conducted with simulated evidence .Finally , we give instance of applications of this new technique to two much - investigated objects , NGC 4151 and Mrk 509 . The main goal of XSCORT is to provide a flexible platform where one can examine the effects of several variables such as : topography , density function , chemical composition , ionization state , etc . , on the known spectral properties of active galactic nuclei ( AGN ) .This will provide us to easier understand their nature and evolution . In particular , we are concerned in examining the impact of the presence of an outflowing component on the form of the reflection hump produced by the innermost parts of the accretion disc around supermassive black holes .These winds may play an important role in shaping the broad - band continuum emission of these sources through absorption and / or scattering mechanisms . They especially control the quantity of material provided to produce the reflected emission at larger distances from the main source .",
        "rewrite_text": "We present an open-source software tool named XSCORT (X-ray Spectral Code for Reprocessed Torus), designed to analyze the reprocessing of radiation by optically thick material structured as a torus, along with its associated outflowing winds. This code is implemented as a collection of IDL procedures, allowing for easy modifications to explore various geometrical configurations and physical conditions. In this article, we detail the implementation of our model utilizing Monte Carlo methods and describe several experiments conducted with simulated data. We also illustrate the application of this innovative technique to two extensively studied active galactic nuclei (AGN), NGC 4151 and Mrk 509.\n\nThe primary objective of XSCORT is to offer a versatile platform for investigating the influence of multiple variables—including topography, density distribution, chemical composition, and ionization state—on the spectral characteristics of AGN. This research aims to enhance our understanding of the nature and evolution of these celestial objects. A particular focus of our study is the examination of how outflowing winds affect the reflection hump generated by the innermost regions of the accretion disk surrounding supermassive black holes. These winds are hypothesized to significantly influence the broad-band continuum emission of AGN through various absorption and scattering processes. Moreover, they play a crucial role in determining the amount of material available for producing reflected emission at greater distances from the central source. By utilizing XSCORT, we aim to deepen our insights into the complex interactions between accretion disk winds and the X-ray spectra of AGN, thereby contributing to the broader understanding of their physical processes and evolution.",
        "ori-fast-z-score": -2.0426487199475707,
        "water-fast-z-score": 5.89167755454927,
        "rewrite-fast-z-score": 0.7071067811865475
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Challenging the Cosmological Constant .\nAbstract:\nThe cosmological constant is one of the most important parameters in modern physics, and its value has been determined by observations to be extremely small but nonzero.  In this article we will discuss how it can be explained as an effect of quantum gravity at very high energies. We will also show that if the universe underwent inflationary expansion after the Big Bang then there should exist primordial gravitational waves which could have observable effects on the cosmic microwave background radiation (CMBR). Finally, we will argue that these effects may provide us with new ways for testing the predictions of general relativity against those of alternative theories such as string theory or loop quantum gravity. The cosmological constant is one  of the most important parameters of modern physics. Its value was determined by observations to be incredibly small but non-zero. It plays a crucial role in our understanding of the evolution of the Universe since it determines whether the current accelerated expansion of space-time will continue forever or eventually slow down and stop. This question remains open despite many years of research into the nature of dark energy.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Challenging the Cosmological Constant . Abstract : The cosmological coefficient is one of the most important components in modern physics , and its value has been determined by observations to be extremely tiny but nonzero .In this page we will explore how it can be described as an influence of quantum gravitational at very high energies . We will also demonstrate that if the universe underwent inflationary inflation after the Big Bang then there should exist primordial gravitational waves which could have observable effects on the cosmic microwave background radiation ( CMBR ) .Finally , we will argue that these changes may provide us with alternative ways for testing the assumptions of general relativity against those of alternative theories such as string theory or loop quantum gravitational . The cosmological coefficient is one of the most important characteristics of modern physics .Its value was decided by observations to be incredibly small but un - zero . It plays a crucial role in our knowing of the evolution of the Universe since it determines whether the present rapid expansion of space - time will continue forever or eventually accelerate down and stop .This question continues open despite many years of research into the nature of dark energy .",
        "rewrite_text": "**Title: Challenging the Cosmological Constant**\n\n**Abstract:** The cosmological constant is a fundamental aspect of contemporary physics, playing a pivotal role in our understanding of the universe's expansion. Recent observations have established its value as remarkably small yet distinctly nonzero. This article delves into the implications of the cosmological constant, particularly in the context of quantum gravity at extremely high energy levels. We propose that if the universe experienced a phase of inflation following the Big Bang, it would have generated primordial gravitational waves. These waves could potentially leave observable signatures on the cosmic microwave background radiation (CMBR), offering a unique avenue for empirical investigation. Furthermore, we discuss how these observable effects could serve as a means to test the validity of general relativity against alternative theoretical frameworks, such as string theory and loop quantum gravity. The cosmological constant not only influences our comprehension of cosmic evolution but also raises critical questions about the future trajectory of the universe. Specifically, it determines whether the current accelerated expansion of spacetime will persist indefinitely or eventually decelerate and halt. Despite extensive research into the nature of dark energy, this fundamental question remains unresolved. By exploring the connections between the cosmological constant, inflationary theory, and gravitational waves, we aim to shed light on the underlying mechanisms of cosmic expansion and contribute to the ongoing discourse surrounding the nature of dark energy and its implications for the fabric of spacetime.",
        "ori-fast-z-score": 1.3480372031495529,
        "water-fast-z-score": 5.848076606885379,
        "rewrite-fast-z-score": -0.2727272727272727
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Relaxation of a dewetting contact line Part 1: A full-scale hydrodynamic calculation .\nAbstract:\nWe present the results of a numerical study on the relaxation dynamics of an initially straight contact line in a two-dimensional geometry, which is driven by surface tension and viscous dissipation at the moving interface between liquid and gas phases.  We solve the Navier-Stokes equations for incompressible fluids with free-slip boundary conditions using a spectral element method to simulate the flow field around the evolving droplet shape. The initial condition consists of a circular droplet sitting on top of a flat substrate that has been perturbed slightly away from its equilibrium position. As time evolves, we observe the formation of capillary waves along the contact line as well as the development of small satellite drops near the main droplet due to pinching off events. In addition, we find that the contact angle decreases continuously during this process until it reaches zero degrees when the entire droplet detaches from the substrate. Finally, we compare our simulation results against experimental data obtained from high-speed video microscopy measurements performed by other researchers.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Relaxation of a dewetting connection line Part 1 : A full - scale hydrodynamic calculation . Abstract : We report the results of a numerical investigation on the relaxation behavior of an initially straight contact line in a two - dimensional topology , which is generated by surface friction and viscous dissipation at the moved interface between liquid and gas phases .We calculate the Navier - Stokes equations for incompressible fluids with loose - slipping border conditions utilizing a spectral component process to simulate the flow field around the evolving droplet shape . The initial condition consists of a circular droplet standing on top of a flat substrate that has been perturbed slightly apart from its stable position .As period evolves , we monitor the formation of capillary currents along the contact line as also as the development of tiny satellite drops near the main droplet thanks to pinching off events . In addition , we find that the contact angle decreases continuously during this process until it meets zero degrees when the entire droplet detaches from the substrate .Finally , we compare our modeling results against empirical data received from high - speed tape microscopy observations performed by other researchers .",
        "rewrite_text": "Title: Relaxation of a Dewetting Connection Line Part 1: A Comprehensive Hydrodynamic Analysis\n\nAbstract: This study presents a detailed numerical analysis of the relaxation dynamics of an initially straight contact line in a two-dimensional setting, focusing on the effects of surface friction and viscous dissipation at the interface between liquid and gas phases. We employ the Navier-Stokes equations for incompressible fluids, incorporating loose-slipping boundary conditions, and utilize a spectral component method to accurately simulate the flow field surrounding the evolving shape of the droplet. The initial configuration consists of a circular droplet positioned on a flat substrate, which is subjected to a slight perturbation from its equilibrium state. As time progresses, we observe the emergence of capillary currents along the contact line, as well as the formation of small satellite droplets adjacent to the main droplet, resulting from pinching-off phenomena. Our findings indicate a continuous decrease in the contact angle throughout the relaxation process, ultimately reaching zero degrees when the droplet completely detaches from the substrate. To validate our numerical model, we compare our results with empirical data obtained from high-speed tape microscopy experiments conducted by other researchers. This comparison not only reinforces the accuracy of our simulations but also provides insights into the underlying physical mechanisms governing droplet dynamics during the dewetting process. The implications of this research extend to various applications in fluid dynamics, materials science, and microfluidics, where understanding the behavior of contact lines is crucial for optimizing processes involving liquid interfaces.",
        "ori-fast-z-score": -1.0206207261596576,
        "water-fast-z-score": 6.0609152673132645,
        "rewrite-fast-z-score": 2.272727272727273
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A twisted FZZ-like dual for the two-dimensional black hole .\nAbstract:\nWe present an exact solution to the classical equations of motion in two dimensions, which is interpreted as describing a rotating black hole with angular momentum J = M . The metric has the form ds2 = −dt2+(1+cosh2r)dθ2−r2dr2,\nwhere r and θ are polar coordinates on the plane. This solution can be obtained by performing a duality transformation on the usual BTZ black hole (with no rotation). We show that this new solution satisfies all the required physical conditions at infinity. In particular we find that it describes a regular black hole horizon located atr+ = √3M , where M is the mass parameter appearing in the original BTZ solution. Finally, we discuss some possible generalizations of our results. Introduction:-In recent years there have been many attempts to construct solutions to Einstein s field equations corresponding to rotating black holes  1  -  4  . One particularly interesting class of such solutions was found by Bañados, Teitelboim and Zanelli (BTZ), who showed how one could obtain a static black hole solution in three dimensional anti-de Sitter space-time  5  .\nThe most important feature of these solutions is their asymptotic behaviour; they describe black holes whose event horizons are completely determined by global quantities like total energy or charge  6  . However, despite being very useful tools for studying quantum gravity phenomena  7, 8  , these solutions do not provide any information about local properties of the spacetime near the horizon  9  . It would therefore seem desirable to try to extend them into more complicated geometries containing additional parameters characterizing the internal structure of the black hole  10  .\nOne way of doing so is to consider higher-dimensional extensions of the BTZ solution  11  . Another possibility is to perform a duality transformation on known solutions  12  . For example, if we start with the Schwarzschild solution written in terms of spherical coordinates, then after applying a suitable coordinate transformation we will get another solution expressed in terms of oblate spheroidal coordinates  13  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A twisted FZZ - like dual for the two - dimensional black hole . Abstract : We present an precise answer to the classical equations of movement in two dimensions , which is interpreted as representing a rotating black hole with angular velocity J = M .The metric has the form ds2 = −dt2 + ( 1 + cosh2r ) dθ2−r2dr2 , where r and θ are polar coordinates on the plane . This solution can be obtained by performing a duality conversion on the usual BTZ dark hole ( with no rotation ) .We see that this new solution satisfies all the necessary physical conditions at infinity . In particular we find that it describes a regular dark hole horizon located atr + = √3M , where M is the mass vector appearing in the previous BTZ solution .Finally , we explain some possible generalizations of our findings . Introduction : - In recent history there have been many efforts to build solutions to Einstein s field equations corresponding to spinning black holes 1 - 4 .One especially interesting class of such solutions was seen by Bañados , Teitelboim and Zanelli ( BTZ ) , who demonstrated how one might obtain a static black hole solution in three dimensional anti - de Sitter space - time 5 . The most important feature of these solutions is their asymptotic behaviour ; they describe white holes whose event horizons are completely determined by global quantities like total energy or charge 6 .However , despite being very useful techniques for studying quantum gravitational dynamics 7 , 8 , these solutions do not offer any info about local characteristics of the spacetime near the horizon 9 . It would therefore appear desirable to try to apply them into more complicated geometries containing extra values characterizing the internal structure of the dark hole 10 .One method of doing so is to consider higher - dimensional extensions of the BTZ solution 11 . Another possibility is to conduct a duality conversion on known solutions 12 .For instance , if we start with the Schwarzschild solution written in terms of spherical coordinates , then after applying a suitable coordinate transformation we will get another solution expressed in terms of oblate spheroidal coordinates 13 .",
        "rewrite_text": "**Title:** A Twisted FZZ-like Dual for the Two-Dimensional Black Hole\n\n**Abstract:** In this study, we provide a comprehensive solution to the classical equations of motion in two-dimensional spacetime, which we interpret as a rotating black hole characterized by an angular velocity of J = M. The derived metric is expressed as ds² = -dt² + (1 + cosh²r) dθ² - r²dr², where r and θ represent polar coordinates on the plane. This solution emerges from a duality transformation applied to the conventional BTZ black hole, which is devoid of rotation. Our findings indicate that this new solution adheres to all requisite physical conditions at spatial infinity. Notably, we identify that it describes a regular black hole horizon situated at r⁺ = √3M, with M denoting the mass parameter from the original BTZ solution. Furthermore, we discuss potential extensions of our results, suggesting avenues for future research. \n\n**Introduction:** Recent advancements in theoretical physics have led to significant efforts in formulating solutions to Einstein's field equations that correspond to spinning black holes. A particularly noteworthy class of solutions was introduced by Bañados, Teitelboim, and Zanelli (BTZ), who illustrated the derivation of a static black hole solution within three-dimensional anti-de Sitter spacetime. The primary significance of these solutions lies in their asymptotic properties; they depict white holes whose event horizons are entirely determined by global parameters such as total energy and charge. However, while these solutions provide valuable insights into quantum gravitational dynamics, they fall short in revealing local characteristics of spacetime in the vicinity of the horizon. This limitation prompts the exploration of more complex geometries that incorporate additional parameters reflecting the internal structure of the black hole. One approach to achieve this is through higher-dimensional extensions of the BTZ solution, while another involves performing duality transformations on established solutions. For example, transforming the Schwarzschild solution from spherical coordinates to oblate spheroidal coordinates yields a new perspective on the underlying spacetime structure.",
        "ori-fast-z-score": 0.8723567442899586,
        "water-fast-z-score": 7.802293021767096,
        "rewrite-fast-z-score": 0.7423074889580903
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Nonlocal Phenomenology for anisotropic MHD turbulence .\nAbstract:\nWe present an analysis of the nonlocal phenomenology in magnetohydrodynamic (MHD) turbulence with strong magnetic field anisotropy, which is relevant to solar wind and space plasmas. We show that the energy transfer rate between different scales can be described by a simple formula based on the local nonlinear interactions only when the wavevector directions are aligned or anti-aligned with respect to the mean magnetic field direction. In other cases, we find that the nonlocal effects become important due to the presence of oblique waves. The results obtained here may provide useful insights into understanding the nature of turbulent transport processes in astrophysical plasma environments. Turbulence plays an essential role in many physical phenomena ranging from geophysics to fusion physics  1, 2  . It has been shown recently that there exist universal statistical properties shared among various types of turbulent flows  3  , such as Kolmogorov scaling  4  , intermittency  5  , and anomalous dissipation  6  .\nIn particular, it was found that the statistics of fully developed turbulence depend crucially on how fast the energy cascades down through the inertial range  7, 8  . This cascade process involves both linear and nonlinear interactions between different modes at different wavenumbers  9  . For example, in hydrodynamics, the energy flux Π(k) ≡< |δu k · δu * −k | 2 > / < u 2 k > depends not only on the magnitude of the wavenumber k but also its orientation relative to the large-scale flow  10  . Here, u k denotes the Fourier transform of velocity fluctuations at scale k −1 . When the angle θ = arccos (k·v 0 )/|k||v 0 |  between the wavevector k and the large-scale flow v 0 is small, i.e., θ ≪ 1, the energy flux Π ∝ k −2/3 sin 2/3 θ  11  . On the contrary, if θ becomes large, then Π decreases rapidly because of the cancellation effect  12  . Similar behaviors have been observed in magnetohydrodynamics (MHD), where the energy flux Π �",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Nonlocal Phenomenology for anisotropic MHD turbulence . Abstract : We present an assessment of the nonlocal phenomenology in magnetohydrodynamic ( MHD ) turbulence with powerful magnetic force anisotropy , which is relevant to solar wind and space plasmas .We see that the power transfer frequency between various scales can be described by a simple equation based on the local nonlinear interactions only when the wavevector directions are aligned or anti - aligned with regard to the mean magnetic force direction . In other instances , we find that the nonlocal changes become crucial due to the presence of oblique waves .The results derived here perhaps offer useful insights into knowledge the nature of turbulent transport systems in astrophysical plasma settings . Turbulence plays an essential part in many natural phenomena ranging from geophysics to fusion science 1 , 2 .It has been shown lately that there remain universal empirical features common among various types of turbulent waves 3 , such as Kolmogorov scaling 4 , intermittency 5 , and anomalous dissipation 6 . In particular , it was shown that the statistics of fully developed turbulence depend crucially on how fast the electricity cascades down through the inertial range 7 , 8 .This cascade process involves both linear and nonlinear interactions between various modes at different wavenumbers 9 . For instance , in hydrodynamics , the power flux Π ( h ) ≡ < | δu k · δu * −k | 2 > / < u 2 k > depends not only on the magnitude of the wavenumber k but also its attitude relative to the small - scale stream 10 .Here , u k denotes the Fourier transform of velocity fluctuations at scale k −1 . When the angle θ = arccos ( k · v 0 ) / | k | | v 0 | between the wavevector k and the huge - scale stream u 0 is tiny , i . e . , θ [UNK] 1 , the power flux Π [UNK] k −2 / 3 sin 2 / 3 θ 11 .On the contrary , if θ becomes large , then Π decreases quickly because of the cancellation effect 12 . Similar behaviors have been observed in magnetohydrodynamics ( MHD ) , where the power flux Π",
        "rewrite_text": "**Title:** Nonlocal Phenomenology for Anisotropic MHD Turbulence\n\n**Abstract:** In this study, we explore the nonlocal phenomenology associated with magnetohydrodynamic (MHD) turbulence characterized by significant magnetic force anisotropy, particularly in contexts relevant to solar wind and space plasmas. Our findings indicate that the frequency of power transfer across different scales can be effectively described by a straightforward equation that relies solely on local nonlinear interactions, but this holds true only when the directions of the wavevectors are either aligned or anti-aligned with the mean magnetic force direction. In scenarios where oblique waves are present, we observe that nonlocal effects become increasingly important. The insights gained from this research may enhance our understanding of turbulent transport mechanisms in astrophysical plasma environments. Turbulence is a fundamental aspect of numerous natural phenomena, spanning fields from geophysics to fusion science. Recent studies have highlighted the existence of universal empirical characteristics that are shared among various turbulent wave types, including Kolmogorov scaling, intermittency, and anomalous dissipation. Notably, it has been established that the statistical properties of fully developed turbulence are significantly influenced by the rate at which energy cascades through the inertial range. This cascade process encompasses both linear and nonlinear interactions among different modes at varying wavenumbers. For example, in hydrodynamic systems, the power flux is defined as Π(h) ≡ < | δu_k · δu*_−k |^2 > / < u^2_k >, which is contingent not only on the magnitude of the wavenumber k but also on its orientation relative to the small-scale flow. When the angle θ, defined as arccos(k · v_0) / |k| |v_0|, between the wavevector k and the large-scale flow u_0 is small, the power flux behaves as Π ∝ k^(-2/3) sin^(2/3)(θ). Conversely, as θ increases, the power flux diminishes rapidly due to cancellation effects. Similar patterns have been documented in the context of MHD, where the power flux exhibits comparable dependencies.",
        "ori-fast-z-score": -1.7962924780409972,
        "water-fast-z-score": 5.388877434122992,
        "rewrite-fast-z-score": 0.816496580927726
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Information Theoretic Proofs of Entropy Power Inequalities .\nAbstract:\nEntropies are measures of uncertainty, and entropy power inequalities (EPIs) quantify the trade-off between information transmission rates in different communication channels.  In this work we present new proofs for EPIs based on information theory concepts such as mutual information and channel capacity.   We also show that these results can be used to prove Shannon s source coding theorem by applying them to an appropriate binary memoryless symmetric channel model. Finally, we discuss how our approach could potentially lead to improved bounds on the minimum distance of linear block codes over finite fields. Entropies are measures of uncertainty; entropy power inequalities (EPIS) quantify the tradeoff between information transmission rates in various communication channels. In this work we present novel proofs for EPIs using information-theory concepts like mutual information and channel capacity. We also demonstrate that these results may be utilized to verify Shannon s source coding theorem via their application to a suitable binary memoryless-symmetric channel model. Finally,we discuss how our technique might possibly yield better bounds on the minimal distance of linear block codes across finite fields.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Information Theoretic Proofs of Entropy Power Inequalities . Abstract : Entropies are measures of uncertainty , and entropy energy inequalities ( EPIs ) quantify the trade - off between information transmission rates in different communication streams .In this study we present new proofs for EPIs based on information principle concepts such as mutual information and channel capacity . We additionally prove that these results can be used to prove Shannon s source coding theorem by application them to an appropriate binary memoryless symmetric channel structure .Finally , we explain how our approach could potentially contribute to greater bounds on the minimum distance of linear block codes over discrete fields . Entropies are measures of uncertainty ; entropy energy inequalities ( EPIS ) quantify the tradeoff between information transmission rates in different communication channels .In this study we present new proofs for EPIs using information - theory ideas like mutual information and channel capacity . We additionally prove that these results may be employed to confirm Shannon s source coding theorem via their application to a suitable binary memoryless - symmetric channel theory .Finally , we explain how our technique may possibly provide better bounds on the reduced distance of linear block sequences across finite fields .",
        "rewrite_text": "Title: Information Theoretic Proofs of Entropy Power Inequalities\n\nAbstract: Entropy serves as a fundamental measure of uncertainty in information theory, and the entropy power inequalities (EPIs) play a crucial role in characterizing the trade-offs between information transmission rates across various communication channels. In this article, we introduce novel proofs for the entropy power inequalities, leveraging key concepts from information theory, including mutual information and channel capacity. Our findings demonstrate that these proofs not only reinforce the validity of EPIs but also facilitate a deeper understanding of Shannon's source coding theorem. By applying our results to a specific binary memoryless symmetric channel framework, we establish a connection that underscores the relevance of EPIs in the context of source coding. Furthermore, we discuss the implications of our approach for enhancing the bounds on the minimum distance of linear block codes defined over discrete fields. This exploration highlights the potential for our methodology to contribute to advancements in coding theory, particularly in optimizing the performance of communication systems. Overall, our work provides a comprehensive examination of the interplay between entropy measures and information transmission, paving the way for future research in the field.",
        "ori-fast-z-score": 0.19425717247145283,
        "water-fast-z-score": 6.225870853937484,
        "rewrite-fast-z-score": 0.9434563530497265
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  2MASS Reveals a Large Intrinsic Fraction of BALQSOs .\nAbstract:\nWe present the results of an analysis of the 2 Micron All Sky Survey (2MASS) data for quasars with broad absorption lines (BALQSOs). We find that about half of all BALQSOs are intrinsically redder than normal QSOs, and that this fraction increases to nearly 80% at z > 3.5. The observed number density evolution is consistent with no luminosity dependence on intrinsic color in the range 10^44 < L(1450A) < 10^46 erg/sec/sr. This result suggests that most BALQSOs have been missed by previous surveys because they were too faint or too blue. If so, then the true space density may be higher than previously estimated. \n \n Keywords: Quasars -- Absorption Lines -- Redshift -- Galaxy Evolution \n \n \n \n 1 Introduction \n \n Broad absorption line quasars (BALQSOs), which show blueshifted absorption features superimposed upon their emission spectra, represent only 10%-20% of optically selected quasar samples but can account for up to 50% of the total UV continuum flux absorbed by intervening gas clouds along the sightline toward distant quasars (Weymann et al., 1991) . In addition to being important probes of the physical conditions within the absorbing gas itself, BALQSOs also provide information regarding the properties of the surrounding intergalactic medium through studies of the associated metal-line systems (e.g., Weymann et al., 1979; Foltz et al., 1986; Turnshek & Savage 1988; Hamann 1998a ,b, 1999 . However, despite their importance as cosmological tools, there has been little progress made in understanding these objects since the discovery of their first examples more than 30 years ago due primarily to selection effects inherent in optical surveys (see e.g., Hewett & Foltz 2003 ) . \n \n Recently, several authors have suggested that many BALQSOs could be found among infrared-selected sources using large-area near-infrared sky surveys such as the Two-Micron All-Sky Survey (2MASS) (Cutri et",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : 2MASS Reveals a Large Intrinsic Fraction of BALQSOs . Abstract : We report the conclusion of an assessment of the 2 Micron All Sky Survey ( 2MASS ) statistics for quasars with broad absorption patterns ( BALQSOs ) .We see that about half of all BALQSOs are intrinsically redder than usual QSOs , and that this fraction increases to virtually 80 % at z > 3 . 5 . The observed number density evolution is compatible with no luminosity influence on intrinsic color in the range 10 ^ 44 < L ( 1450A ) < 10 ^ 46 erg / sec / sr .This result suggests that most BALQSOs have been missed by earlier surveys because they were too distant or too blue . If so , then the true space density might be higher than previously predicted .Keywords : Quasars - - Absorption Lines - - Redshift - - Galaxy Evolution 1 Introduction Broad absorption line quasars ( BALQSOs ) , which show blueshifted emission details superimposed upon their absorption spectra , constitute only 10 % - 20 % of optically selected quasar samples but can provide for up to 50 % of the total UV continuum flux reflected by intervening gas clouds along the sightline toward distant quasars ( Weymann et al . , 1991 ) . In addition to being important probes of the physical conditions within the absorbing gas itself , BALQSOs additionally offer information regarding the properties of the nearby intergalactic medium through research of the associated metal - line systems ( e . g . , Weymann et al . , 1979 ; Foltz et al . , 1986 ; Turnshek & Savage 1988 ; Hamann 1998a , b , 1999 .However , despite their importance as cosmological tools , there has been poor advances completed in understanding these objects since the discovery of their early instance more than 30 weeks ago due primarily to selection effects inherent in infrared observations ( see e . g . , Hewett & Foltz 2003 ) . Recently , various authors have suggested that several BALQSOs might be found among infrared - selected sources using big - area near - infrared sky observations such as the Two - Micron All - Sky Survey ( 2MASS ) ( Cutri et",
        "rewrite_text": "**Title:** 2MASS Uncovers a Significant Intrinsic Fraction of Broad Absorption Line Quasars (BALQSOs)\n\n**Abstract:** In this study, we present the findings from our analysis of the 2 Micron All Sky Survey (2MASS) data concerning quasars exhibiting broad absorption line features, known as BALQSOs. Our investigation reveals that approximately 50% of all identified BALQSOs possess intrinsic colors that are redder than those of typical quasars. Notably, this proportion escalates to nearly 80% for quasars at redshifts greater than 3.5. The evolution of the observed number density aligns with the notion that intrinsic color is unaffected by luminosity within the luminosity range of \\(10^{44} < L(1450 \\text{Å}) < 10^{46} \\text{erg/sr}\\). These findings imply that a significant number of BALQSOs may have been overlooked in previous surveys due to their greater distances or bluer appearances. Consequently, the actual spatial density of these quasars could be higher than earlier estimates suggested.\n\nBroad absorption line quasars, which exhibit blueshifted emission features superimposed on their absorption spectra, represent only 10% to 20% of optically selected quasar samples. However, they can account for up to 50% of the total ultraviolet continuum flux reflected by intervening gas clouds along the line of sight to distant quasars (Weymann et al., 1991). These quasars are not only crucial for probing the physical conditions within the absorbing gas but also provide insights into the characteristics of the surrounding intergalactic medium through the study of associated metal-line systems (e.g., Weymann et al., 1979; Foltz et al., 1986; Turnshek & Savage, 1988; Hamann, 1998a, b, 1999). Despite their significance as cosmological probes, progress in understanding BALQSOs has been limited since their initial discovery over three decades ago, largely due to selection biases in infrared observations (Hewett & Foltz, 2003). Recent studies have indicated that many BALQSOs may be identified among infrared-selected sources, leveraging extensive near-infrared sky surveys such as 2MASS (Cutri et al.).",
        "ori-fast-z-score": -0.8980265101338746,
        "water-fast-z-score": 5.879747322073336,
        "rewrite-fast-z-score": -0.6260990336999411
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Hamiltonian formalism in Friedmann cosmology and its quantization .\nAbstract:\nWe present the Hamiltonian formulation for general relativity with matter fields on an arbitrary spacetime manifold, including both classical and quantum aspects. The basic idea is to use the ADM decomposition of the metric into space and time components as well as lapse and shift functions. We then introduce canonical momenta conjugate to these variables which are used to construct the primary constraints of the theory. These constraints generate gauge transformations under which all physical quantities must be invariant. In order to obtain the correct number of degrees of freedom we have to impose secondary constraints that eliminate unphysical modes. Finally, we perform the canonical quantization by promoting the phase-space variables to operators acting on wave-functions defined over superspace (the space of all possible metrics). This leads us to the Wheeler-DeWitt equation whose solutions can be interpreted as probability amplitudes between different states of the universe. We also discuss how this approach could be applied to inflationary models.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Hamiltonian formalism in Friedmann cosmology and its quantization . Abstract : We present the Hamiltonian formulation for general relativity with matter varieties on an arbitrary spacetime manifold , comprising both classical and quantum aspects .The basic idea is to use the ADM decomposition of the metric into space and period components as well as lapse and shift variables . We then introduce canonical momenta conjugate to these parameters which are applied to build the primary constraints of the model .These restrictions produce gauge transformations under which all physical quantities must be invariant . In order to obtain the appropriate number of degrees of freedom we have to introduce secondary constraints that eliminate unphysical modes .Finally , we perform the canonical quantization by expanding the phase - space variables to operators acting on wave - functions defined over superspace ( the space of all possible metrics ) . This leads us to the Wheeler - DeWitt equation whose solutions can be interpreted as probability amplitudes between various states of the universe .We also discuss how this methodology possible be applied to inflationary systems .",
        "rewrite_text": "Title: Hamiltonian Formalism in Friedmann Cosmology and Its Quantization\n\nAbstract: In this article, we explore the Hamiltonian formulation of general relativity, incorporating various forms of matter within an arbitrary spacetime manifold, while addressing both classical and quantum dimensions of the theory. Our approach begins with the ADM (Arnowitt-Deser-Misner) decomposition of the spacetime metric, which separates it into spatial and temporal components, along with lapse and shift functions. We introduce canonical momenta that are conjugate to these variables, facilitating the construction of primary constraints within our model. These constraints yield gauge transformations, ensuring that all physical quantities remain invariant under these transformations. To refine our model and achieve the correct number of degrees of freedom, we introduce secondary constraints aimed at eliminating any unphysical modes present in the system.\n\nFollowing this, we proceed with the canonical quantization process, wherein we expand the phase-space variables into operators that act on wave functions defined over superspace—the space encompassing all conceivable metrics. This procedure culminates in the derivation of the Wheeler-DeWitt equation, which serves as a cornerstone of quantum cosmology. The solutions to this equation can be interpreted as probability amplitudes that describe transitions between different states of the universe, providing a probabilistic framework for understanding cosmological phenomena.\n\nAdditionally, we examine the potential applications of this Hamiltonian formalism and quantization methodology to inflationary models, highlighting how these concepts can enhance our understanding of the early universe's dynamics. Through this work, we aim to contribute to the ongoing discourse in theoretical cosmology, bridging classical and quantum perspectives in the context of Friedmann cosmology.",
        "ori-fast-z-score": -0.6546536707079772,
        "water-fast-z-score": 3.6663142889169062,
        "rewrite-fast-z-score": -0.4703604341917986
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Rich methane premixed laminar flames doped by light unsaturated hydrocarbons - Part I : allene and propyne .\nAbstract:\nThe effect of adding small amounts (0.5-2%) of the light unsaturated hydrocarbons, allene or propyne to rich methane/air mixtures is investigated experimentally in this study using counterflow diffusion flames at atmospheric pressure. The flame structure is examined with OH* chemiluminescence imaging and laser induced fluorescence measurements for CH2O and CH3O radicals. It was found that both additives have similar effects on the flame structure; they increase the flame temperature slightly but decrease significantly the maximum mole fraction of OH radical. This leads to an increased soot formation rate as evidenced by the higher soot volume fractions measured downstream of the flame front. In addition, it has been observed that the presence of these additives increases the flame speed.  These results are discussed in terms of chemical kinetic mechanisms involving reactions between hydrocarbon species and oxygen atoms. Finally, the influence of the additive concentration on the flame structure is analyzed.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Rich methane premixed laminar flames doped by light unsaturated hydrocarbons - Part I : allene and propyne . Abstract : The impact of adding small amounts ( 0 . 5 - 2 % ) of the light unsaturated hydrocarbons , allene or propyne to rich methane / atmosphere mixtures is investigated experimentally in this study using counterflow diffusion flames at atmospheric pressure .The flame composition is investigated with OH * chemiluminescence scanning and laser triggered fluorescence measurements for CH2O and CH3O radicals . It was shown that both additives have related effects on the flame composition ; they change the burning heat slightly but decrease greatly the maximum mole proportion of OH radical .This leads to an higher soot accumulation frequency as demonstrated by the higher soot quantity fractions calculated downstream of the burning front . In addition , it has been observed that the presence of these additives increases the burning speed .These data are discussed in terms of chemical kinetic mechanisms governing reactions between hydrocarbon species and oxygen atoms . Finally , the impact of the additive concentration on the flame composition is evaluated .",
        "rewrite_text": "This study explores the effects of introducing small quantities (ranging from 0.5% to 2%) of light unsaturated hydrocarbons, specifically allene and propyne, into rich methane/air mixtures. Conducted under atmospheric pressure using counterflow diffusion flames, the research employs advanced techniques such as OH* chemiluminescence scanning and laser-induced fluorescence to analyze the flame composition, focusing on the radicals CH2O and CH3O. The findings reveal that both allene and propyne exhibit similar influences on the flame's characteristics. Notably, while these additives slightly alter the burning heat, they significantly reduce the maximum mole fraction of hydroxyl (OH) radicals present in the flame. This reduction in OH radicals correlates with an increased frequency of soot accumulation, as evidenced by higher soot quantity fractions measured downstream of the combustion front. Furthermore, the study indicates that the incorporation of these hydrocarbons enhances the burning velocity of the flames. The results are interpreted in the context of the chemical kinetic mechanisms that dictate the interactions between hydrocarbon species and oxygen atoms. Additionally, the research assesses how varying the concentration of these additives affects the overall flame composition, providing valuable insights into the combustion behavior of rich methane flames doped with light unsaturated hydrocarbons. This work contributes to a deeper understanding of flame dynamics and the potential implications for combustion efficiency and emissions control in practical applications.",
        "ori-fast-z-score": -0.5555555555555556,
        "water-fast-z-score": 5.521576303742327,
        "rewrite-fast-z-score": -0.09853292781642932
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Shallow decay phase of GRB X-ray afterglows from relativistic wind bubbles .\nAbstract:\nWe present the results of our numerical simulations on the shallow-decay phase of GRB X-ray light curves, which are produced by the interaction between an ultra-relativistic jet and its surrounding medium in the framework of the internal shock model for GRBs. We find that this phase is mainly due to the continuous energy injection into the forward shock driven by the expanding bubble formed at the head of the jet. The injected energy comes from the kinetic energy of the swept-up shell material as well as the thermal energy of shocked ambient gas inside the bubble. Our simulation results show good agreement with observations both qualitatively and quantitatively. \n \n Keywords: Gamma-ray bursts (GRBs), Afterglow emission, Relativistic winds, Shock waves, Bubbles, Internal shocks, Wind-driven shells, Energy injection, Light curve modeling \n \n 1 Introduction \n \n In recent years, great progress has been made in understanding the origin of gamma-ray bursts (GRBs; see Piran 2004 , Zhang 2007a . It was found that most GRBs have their prompt emissions followed by a relatively smooth power-law decline lasting several hundred seconds known as the  afterglow  phase (Costa et al. 1997; van Paradijs et al. 1997) . This phase can be explained by synchrotron radiation from electrons accelerated behind the blast wave generated when the ejecta hits the circumburst medium (Sari et al. 1998 ). However, some GRB afterglows exhibit a shallower-than-power law decline during hundreds of seconds before entering the normal afterglow phase (e.g., Panaitescu & Kumar 2001; Nousek et al. 2006; Liang et al. 2007; Willingale et al. 2007) , which cannot be explained within the standard fireball model. Several models were proposed to explain these phenomena, including late-time central engine activity (Zhang 2007b ), refreshed-shock scenario (Ghisellini et al. 2007 ) and reverse shock emission (Kobayashi 2000; Kobayashi & Sari 2001) . Recently, Fan & Wei (2007) suggested that the shallow-decay phase",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Shallow decay phase of GRB X - ray afterglows from relativistic blowing bubbles . Abstract : We publish the results of our numerical simulations on the shallow - decay phase of GRB X - ray light curves , which are produced by the interaction between an ultra - relativistic jet and its neighbouring medium in the framework of the internal shock model for GRBs .We see that this phase is mainly owing to the continuous energy injection into the front shock driven by the increasing bubble formed at the head of the jet . The injected power derives from the kinetic power of the swept - up shell material as also as the thermal energy of shocked ambient gas inside the bubble .Our model results show good agreement with observations both qualitatively and quantitatively . Keywords : Gamma - ray waves ( GRBs ) , Afterglow emission , Relativistic winds , Shock waves , Bubbles , Internal shocks , Wind - powered shells , Energy injection , Light curve modeling 1 Introduction In recent years , great work has been achieved in understanding the origin of gamma - ray waves ( GRBs ; seeing Piran 2004 , Zhang 2007a .It was shown that most GRBs have their prompt emissions followed by a fairly slow power - law decrease lasting several hundred moments known as the afterglow period ( Costa et al . 1997 ; van Paradijs et al .1997 ) . This phase can be understood by synchrotron emission from electrons accelerated behind the explosion wave produced when the ejecta hits the circumburst medium ( Sari et al .1998 ) . However , some GRB afterglows exhibit a shallower - than - energy law decrease during hundreds of moments before entering the usual afterglow stage ( e . g . , Panaitescu & Kumar 2001 ; Nousek et al .2006 ; Liang et al . 2007 ; Willingale et al .2007 ) , which cannot be described within the standard fireball model . Several models were offered to explain these phenomena , notably mid - time central fuel activity ( Zhang 2007b ) , refreshed - jolt situation ( Ghisellini et al .2007 ) and reverse shock emission ( Kobayashi 2000 ; Kobayashi & Sari 2001 ) . Recently , Fan & Wei ( 2007 ) proposed that the shallow - decay phase",
        "rewrite_text": "**Title:** Shallow Decay Phase of GRB X-ray Afterglows from Relativistic Blowing Bubbles\n\n**Abstract:** In this study, we present the findings from our numerical simulations focused on the shallow-decay phase of gamma-ray burst (GRB) X-ray light curves. This phenomenon arises from the interaction between an ultra-relativistic jet and its surrounding medium, analyzed within the framework of the internal shock model for GRBs. Our results indicate that the shallow-decay phase is primarily driven by continuous energy injection into the front shock, which is propelled by the expanding bubble formed at the jet's leading edge. The energy injected into the system originates from the kinetic energy of the material that has been swept up, as well as the thermal energy from the shocked ambient gas contained within the bubble. The outcomes of our model demonstrate a strong correlation with observational data, both qualitatively and quantitatively, thereby reinforcing the validity of our approach. \n\nIn recent years, significant progress has been made in elucidating the origins of GRBs (Piran 2004; Zhang 2007a). It has been established that the prompt emissions of most GRBs are followed by a prolonged afterglow phase characterized by a gradual power-law decline lasting several hundred seconds (Costa et al. 1997; van Paradijs et al. 1997). This afterglow can be attributed to synchrotron radiation emitted by electrons that are accelerated behind the shock wave generated when the ejecta interacts with the circumburst medium (Sari et al. 1998). However, certain GRB afterglows display a shallower-than-expected decay during the initial phase before transitioning to the standard afterglow behavior (e.g., Panaitescu & Kumar 2001; Nousek et al. 2006; Liang et al. 2007; Willingale et al. 2007), a phenomenon that cannot be adequately explained by the conventional fireball model. Various alternative models have been proposed to account for these observations, including mid-time central fuel activity (Zhang 2007b), refreshed-jolt scenarios (Ghisellini et al. 2007), and reverse shock emissions (Kobayashi 2000; Kobayashi & Sari 2001). Recently, Fan & Wei (2007) suggested that the shallow-decay phase may be linked to these complex interactions. \n\n**Keywords:** Gamma-ray bursts (GRBs), Afterglow emission, Relativistic jets, Shock waves, Bubbles, Internal shocks, Wind-powered shells, Energy injection, Light curve modeling.",
        "ori-fast-z-score": -0.48038446141526137,
        "water-fast-z-score": 6.12425452659289,
        "rewrite-fast-z-score": 0.15811388300841897
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Astrophysical gyrokinetics: kinetic and fluid turbulent cascades in magnetized weakly collisional plasmas .\nAbstract:\nThe present work is devoted to the study of astrophysical plasma turbulence, with particular emphasis on its statistical properties at different scales. We consider both kinetic and fluid descriptions for the dynamics of collisionless plasmas, which are relevant for many space and laboratory applications. In this context we investigate the nonlinear evolution of magnetic fluctuations by means of direct numerical simulations (DNS) of the Vlasov-Maxwell system. The main results can be summarized as follows:  1. Turbulence statistics -We perform DNSs of the Vlasov-Poisson system in order to characterize the statistical properties of the electrostatic potential fluctuations generated by an initial spectrum of Alfvenic modes. Our analysis shows that the energy cascade proceeds towards smaller spatial scales until it reaches the ion Larmor radius scale where it is transferred into perpendicular wavenumbers through Landau damping. At these small scales, the energy transfer rate decreases due to the reduction of phase correlations between wavevectors. This process leads to the formation of intermittency in the distribution function of particles.  2. Kinetic effects -In addition to the above mentioned features observed in the case of purely hydrodynamic turbulence, our results show that kinetic effects play also an important role in determining the statistical properties of the fluctuating fields. Indeed, we find that the presence of ions modifies significantly the shape of the probability density functions (PDFs), leading to non-Gaussian distributions characterized by tails extending over several orders of magnitude. Moreover, we observe that the PDFs become more skewed when increasing the value of the ion-to-electron mass ratio. Finally, we discuss how the inclusion of kinetic effects affects the scaling laws characterizing the power spectra of the fluctuating fields. \n3. Fluid description -By performing DNSs of the Euler-",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Astrophysical gyrokinetics : kinetic and fluid turbulent cascades in magnetized weakly collisional plasmas . Abstract : The present work is committed to the observation of astrophysical plasma turbulence , with particular emphasis on its statistical characteristics at different scales .We consider both kinetic and fluid representations for the dynamics of collisionless plasmas , which are applicable for numerous space and lab applications . In this framework we investigate the nonlinear progression of magnetic fluctuations by means of direct numerical simulations ( DNS ) of the Vlasov - Maxwell process .The main results can be summarized as follows : 1 . Turbulence statistics - We perform DNSs of the Vlasov - Poisson system in order to characterize the statistical characteristics of the electrostatic potential fluctuations formed by an initial spectrum of Alfvenic modes .Our study shows that the power cascade proceeds towards smaller spatial scales until it meets the ion Larmor radius scale where it is transferred into perpendicular wavenumbers through Landau damping . At these little scales , the electricity transfer frequency drops due to the reduction of phase correlations between wavevectors .This process results to the formation of intermittency in the distribution structure of particles . 2 .Kinetic effects - In addition to the above mentioned elements observed in the case of solely hydrodynamic turbulence , our findings show that kinetic influences play also an important role in determining the statistical characteristics of the fluctuating fields . Indeed , we find that the presence of ions modifies substantially the shape of the probability density functions ( PDFs ) , leading to non - Gaussian distributions characterized by tails extending over numerous orders of magnitude .Moreover , we find that the PDFs get more skewed when increasing the value of the electron - to - ion mass ratio . Finally , we explain how the introduction of kinetic effects affects the scaling laws characterizing the power spectra of the fluctuating fields .3 . Fluid description - By conducting DNSs of the Euler -",
        "rewrite_text": "**Title:** Astrophysical Gyrokinetics: Kinetic and Fluid Turbulent Cascades in Magnetized Weakly Collisional Plasmas\n\n**Abstract:** This study focuses on the investigation of turbulence in astrophysical plasmas, specifically analyzing its statistical properties across various scales. We explore both kinetic and fluid models to describe the dynamics of collisionless plasmas, which are relevant to a wide range of applications in space and laboratory settings. Utilizing direct numerical simulations (DNS) of the Vlasov-Maxwell equations, we examine the nonlinear evolution of magnetic fluctuations. Our findings can be summarized in three key areas: \n\n1. **Turbulence Statistics:** We conduct DNS of the Vlasov-Poisson system to analyze the statistical properties of electrostatic potential fluctuations arising from an initial spectrum of Alfvenic modes. Our results indicate that the power cascade progresses toward smaller spatial scales until it reaches the ion Larmor radius scale, where energy is transferred into perpendicular wavenumbers via Landau damping. At these smaller scales, the frequency of energy transfer diminishes due to decreased phase correlations among wavevectors, leading to the emergence of intermittency in the particle distribution structure.\n\n2. **Kinetic Effects:** Beyond the phenomena observed in purely hydrodynamic turbulence, our research highlights the significant role of kinetic effects in shaping the statistical characteristics of fluctuating fields. We discover that the presence of ions considerably alters the probability density functions (PDFs), resulting in non-Gaussian distributions with tails that extend across multiple orders of magnitude. Additionally, we observe that increasing the electron-to-ion mass ratio further skews the PDFs. We also discuss how these kinetic effects influence the scaling laws that govern the power spectra of the fluctuating fields.\n\n3. **Fluid Description:** By performing DNS of the Euler equations, we delve into the fluid dynamics of the system, providing a complementary perspective to our kinetic analysis. This comprehensive approach allows us to better understand the interplay between kinetic and fluid turbulence in magnetized weakly collisional plasmas, contributing valuable insights into the nature of astrophysical plasma turbulence.",
        "ori-fast-z-score": -1.3522468075656264,
        "water-fast-z-score": 5.30555710271907,
        "rewrite-fast-z-score": 0.0854357657716761
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Tight binding formulation of the dielectric response in semiconductor nanocrystals .\nAbstract:\nWe present an ab initio tight-binding model for calculating the optical properties of semiconductor nanocrystals, which is based on the solution of the Bethe-Salpeter equation (BSE) within the framework of density functional theory (DFT). The BSE describes excitonic effects and allows to calculate absorption spectra with high accuracy. We show that our approach reproduces experimental results very well. In particular we find good agreement between calculated and measured absorption cross sections at low energies where quantum confinement dominates over electron-hole exchange interactions. Our method can be applied to any type of semiconductor material including doped systems as well as core-shell structures. Semiconductor nanocrystals are promising candidates for applications such as light-emitting diodes or solar cells due to their unique optoelectronic properties. However, it remains challenging to predict these properties accurately since they depend sensitively on the electronic structure of the system. Here we propose a new theoretical approach to tackle this problem by combining DFT calculations with the Bethe-Salpether equation (BSE), which takes into account excitonic effects beyond mean-field approaches like Kohn-Sham DFT. This enables us to obtain accurate predictions for the optical properties of semiconductor nanostructures.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Tight binding characterization of the dielectric response in semiconductor nanocrystals . Abstract : We create an ab initio close - binding model for determining the optical properties of semiconductor nanocrystals , which is based on the solve of the Bethe - Salpeter equation ( BSE ) within the framework of density functional theory ( DFT ) .The BSE describes excitonic effects and allows to estimate absorption spectra with high clarity . We see that our approach reproduces experimental results very well .In particular we find good agreement between calculated and reported absorption cross sections at low energies where quantum confinement dominates over electron - hole exchange interactions . Our techniques can be applied to any type of semiconductor material including doped systems as well as core - shell systems .Semiconductor nanocrystals are promising candidates for applications such as light - emitting diodes or solar cells due to their specific optoelectronic properties . However , it remains challenging to predict these characteristics properly since they rely sensitively on the electronic properties of the device .Here we undertake a new theoretical technique to tackle this question by combining DFT calculations with the Bethe - Salpether equation ( BSE ) , which gives into consideration excitonic effects beyond mean - field methods like Kohn - Sham DFT . This enables us to obtain precise predictions for the optical properties of semiconductor nanostructures .",
        "rewrite_text": "In this study, we present a novel ab initio tight-binding model designed to investigate the optical properties of semiconductor nanocrystals, leveraging the Bethe-Salpeter equation (BSE) within the density functional theory (DFT) framework. The BSE effectively captures excitonic effects, facilitating a detailed analysis of absorption spectra with remarkable precision. Our findings demonstrate that this approach aligns closely with experimental data, particularly in the low-energy regime where quantum confinement effects prevail over electron-hole exchange interactions. The model's versatility allows it to be applied to a wide range of semiconductor materials, including both doped and core-shell configurations. Semiconductor nanocrystals are increasingly recognized for their potential applications in technologies such as light-emitting diodes and solar cells, attributed to their unique optoelectronic properties. However, accurately predicting these characteristics poses significant challenges, as they are highly sensitive to the underlying electronic properties of the materials. To address this issue, we introduce a comprehensive theoretical framework that integrates DFT calculations with the BSE, thereby accounting for excitonic effects that extend beyond conventional mean-field approaches like Kohn-Sham DFT. This innovative methodology enables us to make precise predictions regarding the optical behavior of semiconductor nanostructures, paving the way for enhanced understanding and optimization of their performance in various applications. Our results not only validate the effectiveness of the proposed model but also contribute to the broader field of semiconductor research by providing a reliable tool for the characterization of dielectric responses in nanocrystalline systems.",
        "ori-fast-z-score": 1.1881770515720091,
        "water-fast-z-score": 6.207574452435047,
        "rewrite-fast-z-score": 1.4729193886373175
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Spectrum of cosmic rays, produced in supernova remnants .\nAbstract:\nThe spectrum of cosmic rays is determined by the energy distribution of particles accelerated at shocks formed during supernova explosions and their subsequent propagation through space.  The observed fluxes are consistent with theoretical predictions for shock acceleration if one assumes that most cosmic rays have been accelerated to energies above 10^15 eV (the  knee ) but below 10^17 eV (the  ankle ). Above this energy there appears to be an additional component which may be due to some other mechanism such as magnetic reconnection or turbulence driven acceleration. This article reviews recent progress on understanding the origin of cosmic rays up to the highest energies detected so far. Cosmic ray observations show that they consist mainly of protons and helium nuclei with charge numbers Z=1-2. These ions can reach energies up to 10^{20}eV, corresponding to Lorentz factors of about 10^{10}. Their total power input into interstellar space amounts to about 10% of the solar luminosity.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Spectrum of cosmic rays , generated in supernova remnants . Abstract : The spectrum of cosmic rays is chosen by the power distribution of particles driven at shocks formed during supernova bursts and their ensuing transmission through space .The observed fluxes are compatible with theoretical expectations for shock velocity if one assumes that most cosmic rays have been accelerated to energies above 10 ^ 15 eV ( the knee ) but below 10 ^ 17 eV ( the knee ) . Above this power there seems to be an additional element which may be due to some other mechanism such as magnetic reconnection or turbulence driven velocity .This page reviews recent progress on understanding the origin of cosmic rays up to the highest energies recorded so far . Cosmic ray observations show that they consist mostly of protons and helium nuclei with charge numbers Z = 1 - 2 .These ions can reach energies up to 10 ^ { 20 } eV , equivalent to Lorentz factors of about 10 ^ { 10 } . Their gross electricity flow into interstellar space amounts to about 10 % of the solar luminosity .",
        "rewrite_text": "**Title: Spectrum of Cosmic Rays Generated in Supernova Remnants**\n\n**Abstract:** The spectrum of cosmic rays is fundamentally influenced by the energy distribution of particles accelerated at shock fronts created during supernova explosions, as well as their subsequent propagation through the cosmos. Observational data indicates that the flux of cosmic rays aligns with theoretical predictions concerning shock velocities, particularly when it is assumed that the majority of cosmic rays are accelerated to energies exceeding 10^15 eV (referred to as the \"knee\") but remaining below 10^17 eV. Beyond this energy threshold, there appears to be an additional component that may arise from alternative acceleration mechanisms, such as magnetic reconnection or turbulence-driven processes. This article provides a comprehensive review of the latest advancements in our understanding of cosmic ray origins, particularly focusing on the highest energy levels detected to date. Current observations reveal that cosmic rays are predominantly composed of protons and helium nuclei, characterized by charge numbers Z = 1 and Z = 2. These particles can attain astonishing energies reaching up to 10^20 eV, which corresponds to Lorentz factors on the order of 10^10. Furthermore, the total electrical energy carried by cosmic rays into interstellar space is estimated to be approximately 10% of the solar luminosity. This review aims to synthesize recent findings and theoretical insights, contributing to a deeper understanding of the mechanisms behind cosmic ray acceleration and their implications for astrophysics.",
        "ori-fast-z-score": -0.3375263702778072,
        "water-fast-z-score": 4.919349550499537,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  MgB2 single crystals substituted with Li and with Li-C: Structural and superconducting properties .\nAbstract:\nThe structural, magnetic susceptibility (χ), electrical resistivity (), thermal conductivity () and specific heat capacity (C) measurements were performed on MgB2 single crystal samples doped by lithium (Li). The substitution of the magnesium atoms in MgB2 lattice by Li leads to an increase of Tmax = 39 K for x=0.1 up to Tmax = 43 K for x= 0.2 at ambient pressure. In addition, we have found that the critical temperature Tc increases monotonically as function of applied hydrostatic pressure P reaching its maximum value of 38.5 K at P = 1 GPa. We also studied the effect of carbon doping into MgB2 structure. It was shown that the introduction of C into MgB2 results in a decrease of both Tmax and Tc values down to 31 K and 27.7 K respectively. \n \n Keywords: Superconductor, MgB2, Lithium",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : MgB2 single crystals substituted with Li and with Li - C : Structural and superconducting qualities . Abstract : The structural , electric susceptibility ( χ ) , thermal resistivity ( ) , thermal conductivity ( ) and particular heat ability ( C ) observations were performed on MgB2 single crystal samples doped by lithium ( Li ) .The substitution of the magnesium atoms in MgB2 lattice by Li gives to an increase of Tmax = 39 K for x = 0 . 1 up to Tmax = 43 K for x = 0 . 2 at ambient temperature . In addition , we have discovered that the critical temperature Tc rises monotonically as function of applied hydrostatic pressure P attain its highest value of 38 . 5 K at P = 1 GPa .We also examined the impact of carbon doping into MgB2 form . It was shown that the introduction of C into MgB2 results in a reduction of both Tmax and Tc values down to 31 K and 27 . 7 K respectively .Keywords: Superconductor, MgB2, Lithium",
        "rewrite_text": "**Title:** Structural and Superconducting Properties of Li- and Li-C-Substituted MgB2 Single Crystals\n\n**Abstract:** This study investigates the structural and superconducting properties of magnesium diboride (MgB2) single crystals that have been doped with lithium (Li) and lithium-carbon (Li-C) compounds. We conducted a comprehensive analysis of various physical properties, including electric susceptibility (χ), thermal resistivity (ρ), thermal conductivity (κ), and specific heat capacity (C). Our findings reveal that substituting magnesium atoms in the MgB2 lattice with lithium significantly enhances the maximum transition temperature (Tmax), which increases from 39 K for x = 0.1 to 43 K for x = 0.2 at ambient conditions. Furthermore, we observed that the critical temperature (Tc) exhibits a consistent increase with the application of hydrostatic pressure (P), reaching a peak value of 38.5 K at a pressure of 1 GPa. In addition to lithium doping, we explored the effects of carbon incorporation into the MgB2 structure. The results indicate that the introduction of carbon leads to a notable decrease in both Tmax and Tc, with values dropping to 31 K and 27.7 K, respectively. These findings contribute to a deeper understanding of the interplay between doping elements and the superconducting characteristics of MgB2, highlighting the potential for tuning its properties through careful manipulation of its composition. This research not only enhances the fundamental knowledge of superconductivity in MgB2 but also opens avenues for the development of advanced superconducting materials with tailored properties for practical applications. \n\n**Keywords:** Superconductor, MgB2, Lithium, Carbon Doping, Critical Temperature",
        "ori-fast-z-score": -0.9615239476408232,
        "water-fast-z-score": 3.8103173776627215,
        "rewrite-fast-z-score": 2.251436323159369
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Exploiting Social Annotation for Automatic Resource Discovery .\nAbstract:\nSocial tagging is an emerging technology that allows users to annotate resources with keywords or tags, which are then used by other users in their search and discovery processes. In this work we propose a novel approach to exploit social annotation information for automatic resource discovery. We first introduce the concept of semantic similarity between tags based on WordNet ontology. Then, using the proposed semantic similarity measure as well as user profile information, we develop two algorithms (i) TagRank and (ii) UserTagRank to rank the importance of each tag associated with a given resource. Finally, we conduct experiments over real-world datasets collected from Delicious website to evaluate our approaches. The experimental results show that both TagRank and UserTagRank can significantly improve the performance of existing state-of-the-art techniques. Keywords: Semantic Web, Social Tagging, Resource Discovery, Ranking Algorithms. 1 Introduction Social tagging has become one of the most popular ways for people to organize and share web content such as blogs, photos, videos etc., especially among Internet users who have little knowledge about how to use traditional search engines effectively  1  . Users usually assign tags to describe the contents they find interesting so that others may easily discover them later  2  .\nIn recent years there has been growing interest in developing new technologies to take advantage of social tagging systems  3  , including recommender systems  4  , personalized search  5  , question answering  6  , text classification  7  , event detection  8  , and so forth. However, despite these efforts, research into utilizing social tagging data for automatic resource discovery remains relatively unexplored  9  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Exploiting Social Annotation for Automatic Resource Discovery . Abstract : Social tagging is an evolving technique that enables consumers to annotate resources with keywords or tags , which are then utilized by other users in their search and finding processes .In this project we pursue a new approach to use social annotation info for efficient resource discovery . We first introduce the idea of semantic similarity between labels based on WordNet ontology .Then , using the suggested semantic similarity metric as well as customer profile details , we develop two algorithms ( i ) TagRank and ( ii ) UserTagRank to score the importance of each tag attributed with a given resource . Finally , we conduct experiments over real - time datasets collected from Delicious website to analyze our approaches .The empirical results show that both TagRank and UserTagRank can significantly boost the performance of older state - of - the - art methods . Keywords : Semantic Web , Social Tagging , Resource Discovery , Ranking Algorithms .1 Introduction Social tagging has become one of the most popular ways for people to organize and communicate website data such as websites , photos , movies etc . , particularly among Internet people who have less knowledge about how to use conventional search sites effectively 1 . Users typically give tags to explain the contents they discover attractive so that others would easily discover them later 2 .In recent years there has been growing interest in building new inventions to take advantage of social tagging technologies 3 , notably recommender methods 4 , personalized search 5 , question telling 6 , word classification 7 , event screening 8 , and so forth . However , despite these attempts , work into using social tagging material for automatic resource extraction continues relatively unexplored 9 .",
        "rewrite_text": "**Title: Leveraging Social Annotation for Enhanced Resource Discovery**\n\n**Abstract:** Social tagging has emerged as a dynamic method that allows users to annotate various resources with keywords or tags, which can subsequently aid other users in their search and retrieval processes. This study introduces a novel approach to harness social annotation data for more effective resource discovery. We begin by proposing a framework for assessing semantic similarity between tags, grounded in the WordNet ontology. Utilizing this semantic similarity metric alongside user profile information, we develop two innovative algorithms: (i) TagRank and (ii) UserTagRank. These algorithms are designed to evaluate and rank the significance of each tag associated with a specific resource. To validate our methodologies, we conduct experiments using real-time datasets sourced from the Delicious platform. The results of our empirical analysis demonstrate that both TagRank and UserTagRank markedly enhance the performance of existing state-of-the-art techniques in resource discovery. \n\nSocial tagging has gained traction as a favored method for individuals to categorize and share web content, including websites, images, and videos, particularly among users who may not be adept at utilizing traditional search engines effectively. Users assign tags to describe the appealing aspects of the content they encounter, facilitating easier discovery for others. In recent years, there has been a surge of interest in developing innovative applications that leverage social tagging technologies, including recommender systems, personalized search engines, and various classification and screening methods. However, the potential of social tagging for automatic resource extraction remains largely underexplored. This paper aims to bridge that gap by presenting a systematic approach to utilizing social annotation for improved resource discovery, thereby contributing to the broader field of Semantic Web technologies.\n\n**Keywords:** Semantic Web, Social Tagging, Resource Discovery, Ranking Algorithms.",
        "ori-fast-z-score": -2.1358941442919024,
        "water-fast-z-score": 7.548881474151742,
        "rewrite-fast-z-score": -2.425101829020563
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Microscpoic Model for the Neutron Dynamic Structure Factor of Solid Methane in phase II .\nAbstract:\nThe neutron dynamic structure factor (NDSF) is calculated by using molecular dynamics simulation and the density functional theory with generalized gradient approximation(GGA). The NDSF shows two peaks at low wave number region, which are assigned to the translational motion of methane molecules. At high wave number region, one peak appears around 2 Å-1 ,which corresponds to the libration mode of CH4 molecule. In addition, we find that there exists another peak near 3 Å-1 . This new peak can be explained as the result of the rotation of CH3 group against the C-H bond direction. We also calculate the partial structure factors Sij(Q), where i and j represent different atoms or groups. From these results, it is found that the main contribution comes from the correlation between carbon atom and hydrogen atom. Finally, we compare our theoretical results with experimental data obtained by Hwang et al..",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Microscpoic Model for the Neutron Dynamic Structure Factor of Solid Methane in phase II . Abstract : The neutron dynamic structure parameter ( NDSF ) is calculated by using polymer mechanics models and the density functional theory with generalized gradient approximation ( GGA ) .The NDSF sees two peaks at low wave number zone , which are assigned to the translational behavior of methane molecules . At high wave number area , one peak appears around 2 Å - 1 , which belongs to the libration mechanism of CH4 molecule .In addition , we find that there exists another peak near 3 Å - 1 . This new peak can be understood as the result of the rotation of CH3 group against the C - H bond direction .We additionally calculate the partial composition factors Sij ( Q ) , where i and j represent different compounds or groups . From these results , it is found that the main contribution comes from the relationship between carbon molecule and hydrogen molecule .Finally , we compare our theoretical results with experimental evidence derived by Hwang et al . .",
        "rewrite_text": "**Title:** A Microscopic Model for the Neutron Dynamic Structure Factor of Solid Methane in Phase II\n\n**Abstract:** This study presents a comprehensive analysis of the neutron dynamic structure factor (NDSF) for solid methane in its second phase, utilizing polymer mechanics models alongside density functional theory (DFT) with a generalized gradient approximation (GGA). Our calculations reveal two distinct peaks in the low wave number region, which are attributed to the translational motions of methane molecules. In the high wave number domain, we identify a prominent peak around 2 Å⁻¹, corresponding to the librational movements of the CH₄ molecule. Furthermore, we observe an additional peak near 3 Å⁻¹, which can be interpreted as the rotational dynamics of the CH₃ group relative to the C-H bond axis. To deepen our understanding, we also compute the partial structure factors Sij(Q), where indices i and j denote various molecular components or groups. The analysis indicates that the primary contributions to the NDSF arise from the interactions between carbon and hydrogen atoms. Finally, we juxtapose our theoretical findings with experimental data reported by Hwang et al., highlighting the consistency and relevance of our model in explaining the dynamic behavior of solid methane. This work not only enhances the understanding of molecular dynamics in solid methane but also provides a robust framework for future studies involving similar molecular systems.",
        "ori-fast-z-score": -0.6708203932499369,
        "water-fast-z-score": 4.196397990844169,
        "rewrite-fast-z-score": 1.2809280616135812
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Effects of Selection and Covariance on X-ray Scaling Relations of Galaxy Clusters .\nAbstract:\nWe study the effects of selection biases in cluster samples, as well as covariance between observables, on scaling relations derived from X-ray data using mock galaxy clusters generated with the semi-analytic model GALFORM. We find that both these effects can lead to significant systematic errors when deriving cosmological constraints from observed scaling relations. In particular we show that: (i) The scatter in the M-T relation is significantly reduced by including additional information about the temperature distribution function; this effect is stronger for low mass systems. (ii) The slope of the L-M relation depends strongly on whether or not one includes cooling flows in the analysis. This dependence arises because cool cores are more common at high masses than at lower masses, leading to an apparent steepening of the slope if they are excluded. (iii) The normalization of the Y-Xray luminosity-temperature relation shows strong redshift evolution which cannot be explained solely by self-similar evolution.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Effects of Selection and Covariance on X - ray Scaling Relations of Galaxy Clusters . Abstract : We research the effects of selection biases in cluster specimens , as well as covariance between observables , on scaling relations derived from X - ray data utilizing simulated star clusters constructed with the semi - analytic method GALFORM .We see that both these influences can lead to significant systematic errors when deriving cosmological limitations from observed scaling relations . In particular we find that : ( i ) The scatter in the M - T relation is significantly reduced by including extra data about the temperature distribution function ; this effect is greater for low mass systems .( ii ) The slope of the L - M relation depends strongly on whether or not one includes cooling flows in the analysis . This dependence occurs because cool cores are more common at high masses than at lower masses , leading to an apparent steepening of the gradient if they are excluded .( iii ) The normalization of the Y - Xray luminosity - temperature relation shows intense redshift development which cannot be described solely by self - similar development .",
        "rewrite_text": "**Title:** Effects of Selection and Covariance on X-ray Scaling Relations of Galaxy Clusters\n\n**Abstract:** This study investigates the impact of selection biases in galaxy cluster samples and the covariance between various observables on the scaling relations derived from X-ray observations. Utilizing simulated star clusters generated through the semi-analytic method known as GALFORM, we demonstrate that both selection effects and covariance can introduce substantial systematic errors when interpreting cosmological constraints from observed scaling relations. Our findings reveal several key insights: (i) The scatter observed in the mass-temperature (M-T) relation is notably diminished when additional information regarding the temperature distribution function is incorporated; this reduction in scatter is particularly pronounced in lower mass systems. (ii) The slope of the luminosity-mass (L-M) relation exhibits a strong dependence on the inclusion of cooling flows in the analysis. This dependence arises because cool cores are more prevalent in high-mass clusters compared to their low-mass counterparts, resulting in an apparent steepening of the slope when these cool cores are omitted from the dataset. (iii) Furthermore, we observe that the normalization of the Y-X ray luminosity-temperature relation undergoes significant evolution with redshift, a phenomenon that cannot be adequately explained by self-similar evolution alone. These results underscore the necessity of accounting for selection biases and covariance effects in order to accurately interpret scaling relations and their implications for cosmology. Our work highlights the complexities involved in analyzing X-ray data from galaxy clusters and calls for a more nuanced approach to understanding the underlying physical processes that govern these scaling relations.",
        "ori-fast-z-score": -0.22086305214969307,
        "water-fast-z-score": 4.500334659577304,
        "rewrite-fast-z-score": -1.4368424162141993
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Looking beyond the horizon .\nAbstract:\nThe article presents an analysis of the results obtained in the study of the influence of various factors on the development and functioning of small businesses, which were carried out by Russian scientists over the past decade. The main conclusions are as follows: 1) Small business is one of the most important elements of economic growth; 2) In Russia there has been a significant increase in the number of small enterprises since the beginning of market reforms (from 1990 to 2010 their number increased more than ten times), but this process was accompanied by a decrease in the average size of these companies; 3) There have also been changes in the structure of small businesses; 4) Changes in the legal framework for regulating relations between state authorities and entrepreneurship contributed to the formation of new forms of interaction between them; 5) A positive effect of the reform can be seen in the fact that the share of small businesses in the total volume of industrial production has grown significantly; 6) However, despite all the advantages of small businesses, they still face many problems related both to external conditions and internal management issues; 7) It should be noted that the current situation with small businesses does not correspond fully to the potential of the sector.  8)",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Looking beyond the horizon . Abstract : The essay presents an assessment of the results derived in the study of the impact of several variables on the development and functioning of little companies , which were carried out by Russian research over the previous decade .The main observations are as follows : 1 ) Small business is one of the most important elements of economic activity ; 2 ) In Russia there has been a substantial rise in the quantity of tiny enterprises since the beginning of market improvements ( from 1990 to 2010 their number increased more than ten times ) , but this process was accompanied by a reduction in the average size of these corporations ; 3 ) There have also been improvements in the organization of tiny businesses ; 4 ) Changes in the legal framework for controlling relations between government officials and entrepreneurship contributed to the formation of new types of relationship between them ; 5 ) A favorable result of the changes can be saw in the fact that the share of tiny businesses in the total quantity of industrial output has grown substantially ; 6 ) However , despite all the advantages of large firms , they still face many difficulties related both to external situation and internal control problems ; 7 ) It should be mentioned that the present situation with large firms does not correspond fully to the possibilities of the sector . 8 )",
        "rewrite_text": "Title: Looking Beyond the Horizon\n\nAbstract: This essay evaluates the findings from a decade-long study conducted by Russian researchers on the influence of various factors on the growth and operation of small enterprises. The analysis reveals several key insights: Firstly, small businesses are critical components of economic activity, playing a vital role in the overall economic landscape. Secondly, since the onset of market reforms in Russia from 1990 to 2010, there has been a remarkable increase in the number of small enterprises, with their quantity rising more than tenfold. However, this surge has been accompanied by a notable decline in the average size of these businesses. Additionally, there have been significant advancements in the organizational structures of small enterprises, enhancing their operational efficiency. \n\nMoreover, modifications in the legal framework governing the interactions between government authorities and entrepreneurs have fostered the emergence of new dynamics in these relationships. A positive outcome of these changes is evident in the substantial growth of small businesses' contribution to the overall industrial output. Nonetheless, despite the advantages that larger firms may offer, they continue to encounter numerous challenges, both from external market conditions and internal management issues. It is important to note that the current state of large enterprises does not fully align with the potential opportunities available within the sector. This assessment underscores the complexities faced by small businesses in the evolving economic environment and highlights the need for ongoing research to better understand the interplay of these factors in fostering a robust entrepreneurial ecosystem.",
        "ori-fast-z-score": -2.6943012562182536,
        "water-fast-z-score": 7.183697139158635,
        "rewrite-fast-z-score": -2.3958625754235072
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A model for learning to segment temporal sequences, utilizing a mixture of RNN experts together with adaptive variance .\nAbstract:\nWe propose an algorithm that learns how to segment time series data into meaningful segments by using a combination of Recurrent Neural Network (RNN) Experts and Adaptive Variance Estimation (AVE). The proposed method is based on the idea that each expert in our ensemble has its own internal representation of the input sequence which can be used as prior knowledge when estimating the uncertainty associated with the prediction made by this particular expert. We show that such approach leads to better results than state-of-the-art methods both in terms of accuracy and computational efficiency. \nThe main contributions of this work are summarized below: \n1. A novel framework for modeling uncertain predictions produced by multiple RNN Experts.\n2. An efficient online training procedure for updating parameters of all Experts simultaneously.\n3. Extensive experiments conducted on several real-world datasets demonstrate superior performance of the proposed method over existing approaches. Learning to predict future values of a given time series requires understanding patterns hidden within it. However, due to high complexity of many real world problems, finding these patterns may require processing large amounts of data. In order to make accurate predictions we need to find a way to extract useful information from raw data while at the same time being able to deal with noise present in the signal. This problem becomes even more challenging if one wants to process data streams arriving continuously or having very short duration between consecutive samples.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A design for learning to segment temporal intervals , using a mixture of RNN experts together with adaptive variance . Abstract : We suggest an algorithm that learns how to split period series information into meaningful spans by using a combination of Recurrent Neural Network ( RNN ) Experts and Adaptive Variance Estimation ( AVE ) .The proposed approach is based on the idea that each expert in our ensemble has its own internal description of the input sequence which can be used as prior information when estimating the uncertainty associated with the prediction created by this given expert . We suggest that such approach leads to good results than state - of - the - art methods both in terms of precision and mathematical efficiency .The main contributions of this research are presented below : 1 . A novel framework for modeling uncertain estimates generated by many RNN Experts .2 . An efficient electronic training method for updating values of all Experts simultaneously .3 . Extensive experiments conducted on numerous real - global datasets prove excellent performance of the suggested method over existing techniques .Learning to predict future values of a given time cycle requires studying patterns concealed within it . However , owing to large complexity of several real life issues , finding these patterns could require analyzing large quantities of evidence .In order to make accurate forecast we must to find a way to extract useful info from raw data while at the same time being able to deal with sound present in the signal . This problem arises even more challenging if one wants to analyze information streams arriving constantly or having very brief duration between successive samples .",
        "rewrite_text": "**Title:** A Design for Learning to Segment Temporal Intervals Using a Mixture of RNN Experts with Adaptive Variance\n\n**Abstract:** In this study, we propose a novel algorithm designed to effectively segment time series data into meaningful intervals by leveraging a combination of Recurrent Neural Network (RNN) Experts and Adaptive Variance Estimation (AVE). Our approach is predicated on the premise that each RNN expert possesses a unique internal representation of the input sequence, which serves as prior knowledge when assessing the uncertainty linked to the predictions made by that specific expert. We argue that this methodology yields superior results compared to existing state-of-the-art techniques, particularly in terms of both accuracy and computational efficiency. The key contributions of our research include: 1) the introduction of a comprehensive framework for modeling the uncertain estimates produced by multiple RNN experts; 2) the development of an efficient electronic training mechanism that allows for the simultaneous updating of all expert values; and 3) extensive empirical evaluations conducted on a variety of real-world datasets, demonstrating the exceptional performance of our proposed method relative to current approaches. Predicting future values within a given time series necessitates the identification of underlying patterns; however, the inherent complexity of many real-world scenarios often complicates this task, requiring the analysis of vast amounts of data. To achieve accurate forecasts, it is essential to extract valuable insights from raw data while effectively managing noise present in the signal. This challenge is further exacerbated when dealing with continuous information streams or when the intervals between successive samples are minimal. Our research addresses these issues, providing a robust framework for temporal interval segmentation that enhances predictive capabilities in dynamic environments.",
        "ori-fast-z-score": -1.2371791482634837,
        "water-fast-z-score": 7.89115139065715,
        "rewrite-fast-z-score": -0.5035088149780135
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A novel method for the evaluation of uncertainty in dose volume histogram computation .\nAbstract:\nThe aim of this study was to develop and evaluate an algorithm that can be used to assess uncertainties associated with DVH computations using Monte Carlo (MC) simulations. The proposed approach is based on the concept of  uncertainty bands , which are defined as regions within which all possible values of a given quantity lie at some specified probability level. In order to demonstrate its feasibility, we applied our methodology to two clinical cases involving different treatment techniques. For each case, we performed MC calculations using various numbers of histories ranging between 1x10 5 and 2x10 6 . We then compared results obtained by applying our new technique against those produced by conventional methods such as the standard deviation or confidence intervals. Our findings indicate that the proposed method provides more accurate estimates than other approaches commonly employed in practice. This work has been presented at the International Conference on Medical Physics and Biomedical Engineering held in Singapore during August 2009. \n \n Keywords: Uncertainty bands, Dose Volume Histogram, Monte Carlo",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A novel method for the evaluation of uncertainty in dose volume histogram computation . Abstract : The goal of this study was to develop and improve an algorithm that can be used to analyze uncertainties involved with DVH computations using Monte Carlo ( MC ) simulations .The proposed approach is based on the idea of uncertainty bands , which are specified as regions within which all possible estimates of a given quantity exist at some specified probability level . In order to test its feasibility , we applied our technique to two medical cases using distinct treatment techniques .For each case , we performed MC measurements employing several numbers of histories ranging between 1x10 5 and 2x10 6 . We then correlated results derived by using our new technique against those generated by traditional techniques such as the standard deviation or confidence intervals .Our findings confirm that the suggested method provides more accurate estimates than other methods commonly employed in practice . This research has been presented at the International Conference on Medical Physics and Biomedical Engineering organized in Singapore during August 2009 .Keywords : Uncertainty bands , Dose Volume Histogram , Monte Carlo",
        "rewrite_text": "**Title:** A Novel Method for Evaluating Uncertainty in Dose Volume Histogram Computation\n\n**Abstract:** This study aims to develop and enhance an algorithm designed to assess the uncertainties associated with Dose Volume Histogram (DVH) computations through the use of Monte Carlo (MC) simulations. The innovative approach introduced in this research revolves around the concept of uncertainty bands, which are defined as specific regions that encompass all potential estimates of a particular quantity at a designated probability level. To evaluate the practicality of our method, we applied it to two distinct medical cases, each utilizing different treatment techniques. For both cases, we conducted MC simulations with varying numbers of histories, ranging from 1x10^5 to 2x10^6. We then compared the results obtained from our novel technique with those derived from conventional methods, such as standard deviation and confidence intervals. The results indicate that our proposed method yields more precise estimates compared to the traditional approaches commonly utilized in clinical practice. This research was presented at the International Conference on Medical Physics and Biomedical Engineering held in Singapore in August 2009. \n\n**Keywords:** Uncertainty bands, Dose Volume Histogram, Monte Carlo",
        "ori-fast-z-score": -0.6546536707079772,
        "water-fast-z-score": 6.182518047831701,
        "rewrite-fast-z-score": -0.22941573387056174
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Sedentary Survey of Extreme High Energy Peaked BL Lacs III. Results from Optical Spectroscopy .\nAbstract:\nWe present optical spectroscopic observations for the sample of 14 extreme high energy peaked BL Lac objects (EHBLs) selected by Costamante et al. (2013) . The main goal is to study their host galaxy properties and investigate possible differences with respect to lower-energy blazars, which are known to be hosted in elliptical galaxies. We find that all EHBLs have redshifts between 0.1 and 1.0, consistent with previous results on this class of sources. All but one source show evidence of being hosted in spiral or irregular galaxies; only PKS 0537-441 shows an elliptical-like spectrum. This result suggests that there may not exist any significant difference in the hosts of low-and high-energy blazars as previously claimed. However, we note that our sample size is small and further studies will be needed before drawing firm conclusions. \n \n Keywords: Blazar, Host Galaxy",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  The Sedentary Survey of Extreme High Energy Peaked BL Lacs III.Results from Optical Spectroscopy .Abstract : We report optical spectroscopic observations for the sample of 14 extreme high energy peaked BL Lac objects ( EHBLs ) selected by Costamante et al . ( 2013 ) .The main goal is to study their host universe characteristics and probe possible variations with regard to smaller - energy blazars , which are known to be hosted in elliptical galaxies . We see that all EHBLs have redshifts between 0 . 1 and 1 . 0 , consistent with previous findings on this class of sources .All but one source demonstrate proof of being hosted in spiral or irregular stars ; only PKS 0537 - 441 shows an elliptical - like spectrum . This result suggests that there may not occur any large change in the hosts of high - and low - energy blazars as previously reported .However , we note that our sample volume is tiny and further studies will be needed before drew solid results . Keywords : Blazar , Host Galaxy",
        "rewrite_text": "**Title:** The Sedentary Survey of Extreme High Energy Peaked BL Lacs III: Results from Optical Spectroscopy\n\n**Abstract:** In this study, we present the findings from optical spectroscopic observations of a sample comprising 14 extreme high energy peaked BL Lac objects (EHBLs), as identified by Costamante et al. (2013). The primary objective of our research is to investigate the characteristics of the host galaxies of these EHBLs and to explore potential differences when compared to lower-energy blazars, which have been predominantly associated with elliptical galaxies. Our analysis reveals that all EHBLs in our sample exhibit redshifts ranging from 0.1 to 1.0, aligning with previously documented data on this category of astronomical sources. Notably, we find that the majority of the EHBLs are hosted in spiral or irregular galaxies, with the exception of PKS 0537-441, which displays an elliptical-like spectral signature. This observation raises intriguing questions regarding the evolutionary pathways of high-energy versus low-energy blazars, suggesting that significant differences in their host galaxies may not be as pronounced as earlier studies have indicated. However, we acknowledge that our sample size is limited, and thus, further investigations are essential to draw more definitive conclusions regarding the host galaxy characteristics of EHBLs. Our findings contribute to the ongoing discourse on the relationship between blazar types and their host galaxies, highlighting the need for expanded research in this area. \n\n**Keywords:** Blazar, Host Galaxy",
        "ori-fast-z-score": -0.23904572186687872,
        "water-fast-z-score": 5.421151989096865,
        "rewrite-fast-z-score": 0.7921180343813395
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Marcus-Lushnikov processes, Smoluchowski s and Flory s models .\nAbstract:\nThe Marcus-Lushnikov process is the continuous-time analogue of the discrete-time Lévy walk model introduced by Montroll-Weiss in 1965 to describe diffusion-limited aggregation (DLA) on fractal surfaces.  The DLA growth mechanism has been observed experimentally for many years but only recently have there been attempts at modelling it mathematically using stochastic processes such as the Marcus-Lushnikov process.   In this article we consider two related problems concerning the Marcus-Lushnikov model:  Firstly, we prove that if the jump distribution of the underlying Lévy process satisfies certain integrability conditions then the corresponding Marcus-Lushnikov process converges weakly to Brownian motion with drift; secondly, we show how the Marcus-Lushnikov model can be used to approximate the solution of Smoluchowski s coagulation-fragmentation equation which describes the evolution of particle size distributions in chemical reactions involving clusters of particles. We also discuss some connections between the Marcus-Lushnikov and Flory s models of polymerisation.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Marcus - Lushnikov processes , Smoluchowski s and Flory s theories . Abstract : The Marcus - Lushnikov cycle is the continuous - time analogue of the discrete - time Lévy walk system proposed by Montroll - Weiss in 1965 to explain absorption - limited aggregation ( DLA ) on fractal surfaces .The DLA growth mechanism has been observed experimentally for numerous years but only lately have there been attempts at describing it mathematically utilizing stochastic processes such as the Marcus - Lushnikov mechanism . In this page we consider two similar problems concerning the Marcus - Lushnikov model : Firstly , we prove that if the jump density of the underlying Lévy cycle satisfies certain integrability constraints then the analogous Marcus - Lushnikov mechanism converges strongly to Brownian movement with drift ; secondly , we prove how the Marcus - Lushnikov model can be used to approximate the solve of Smoluchowski s coagulation - fragmentation equation which explains the evolution of particle size distributions in chemical processes involving clusters of molecules .We also discuss some relationships between the Marcus - Lushnikov and Flory s theories of polymerisation .",
        "rewrite_text": "Title: Marcus-Lushnikov Processes, Smoluchowski's and Flory's Theories\n\nAbstract: The Marcus-Lushnikov cycle serves as the continuous-time counterpart to the discrete-time Lévy walk system introduced by Montroll and Weiss in 1965, which was designed to elucidate absorption-limited aggregation (DLA) on fractal surfaces. While the DLA growth mechanism has been empirically observed for many years, recent efforts have focused on providing a mathematical framework for its description through stochastic processes, particularly the Marcus-Lushnikov mechanism. In this article, we explore two closely related issues pertaining to the Marcus-Lushnikov model. First, we demonstrate that under specific integrability conditions on the jump density of the underlying Lévy cycle, the corresponding Marcus-Lushnikov mechanism exhibits strong convergence to a Brownian motion with drift. This finding highlights the connection between stochastic processes and classical diffusion phenomena. Secondly, we illustrate how the Marcus-Lushnikov model can effectively approximate solutions to Smoluchowski's coagulation-fragmentation equation, which describes the dynamics of particle size distributions in chemical processes involving molecular clusters. Additionally, we delve into the interplay between the Marcus-Lushnikov framework and Flory's theories of polymerization, revealing significant relationships that enhance our understanding of polymer dynamics. Through this investigation, we aim to bridge the gap between theoretical models and practical applications in the study of aggregation and polymerization processes, providing insights that could inform future research in these areas.",
        "ori-fast-z-score": -0.5852057359806528,
        "water-fast-z-score": 5.812381937190964,
        "rewrite-fast-z-score": 1.3065491598369756
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Transients from initial conditions based on Lagrangian perturbation theory in N-body simulations .\nAbstract:\nWe present the results of an investigation into the properties and evolution of transients that arise when initial conditions are generated using Lagrangian perturbation theory (LPT) for cosmological N-body simulations. We find that LPT-based initial conditions produce spurious large-scale power at late times, which is not suppressed by increasing the number of particles used to generate these initial conditions. This effect can be mitigated by applying a low-pass filter to the evolved density field prior to generating new initial conditions with higher-order LPT. However, this approach does not completely remove all unwanted effects associated with the use of LPT-generated initial conditions. In addition, we show how the choice of time step used to evolve the initial conditions affects their accuracy. Finally, we demonstrate that it is possible to construct accurate initial conditions for large-volume cosmological simulations without having to resort to expensive high-resolution hydrodynamic simulations. The generation of realistic initial conditions for cosmological N-body",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Transients from initial conditions based on Lagrangian perturbation theory in N - bodies simulations . Abstract : We present the results of an research into the properties and evolution of transients that arise when initial conditions are produced utilizing Lagrangian perturbation theory ( LPT ) for cosmological N - bodies simulations .We see that LPT - based initial conditions generate spurious large - scale power at late times , which is not suppressed by expanding the quantity of particles needed to create these initial conditions . This phenomenon can be mitigated by using a small - pass filter to the evolved density field prior to generating new early conditions with higher - order LPT .However , this methodology does not totally avoid all unwanted effects involved with the using of LPT - produced original conditions . In addition , we study how the selection of time phase used to evolve the early conditions influences their accuracy .Finally , we prove that it is easy to build correct preliminary environments for large - volume cosmological simulations without having to resort to expensive high - resolution hydrodynamic simulations . The generation of accurate initial conditions for cosmological N - body",
        "rewrite_text": "In this article, we investigate the characteristics and development of transients that emerge from initial conditions formulated through Lagrangian perturbation theory (LPT) in cosmological N-body simulations. Our findings reveal that initial conditions derived from LPT lead to the introduction of spurious large-scale power at later stages of simulation, a phenomenon that persists regardless of increasing the number of particles used to establish these initial conditions. To address this issue, we propose the application of a small-pass filter to the evolved density field before generating new early conditions with higher-order LPT. While this approach helps to reduce some of the unwanted artifacts associated with LPT-generated initial conditions, it does not completely eliminate all adverse effects. Furthermore, we examine the impact of the chosen time phase on the evolution of early conditions, highlighting its significance in determining the accuracy of the simulations. Our research demonstrates that it is feasible to create accurate initial conditions for large-volume cosmological simulations without the necessity of resorting to costly high-resolution hydrodynamic simulations. This work contributes to the understanding of how to effectively generate reliable initial conditions for cosmological N-body simulations, ultimately enhancing the fidelity of simulations in cosmology.",
        "ori-fast-z-score": -0.41256849850351734,
        "water-fast-z-score": 6.193611607315077,
        "rewrite-fast-z-score": 0.5076730825668095
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Inter-network regions of the Sun at millimetre wavelengths .\nAbstract:\nWe present new observations made with the Atacama Large Millimeter/submillimeter Array (ALMA) of two inter-network sunspots in active region NOAA AR 12192 on 2013 May 24 and 25, respectively. The first sunspot was observed for about 3 hours during which time it rotated by more than 90 degrees. We find that this sunspot is composed of several magnetic flux tubes with different orientations. In addition to these features we also observe an extended bright feature located between the main sunspot umbrae. This feature has been previously reported as a penumbral filament but our data show no evidence of such structure. Instead, we interpret this feature as a coronal rain blob. The second sunspot was observed for only 1 hour before being occulted by Earths atmosphere. During this observation period the sunspot rotated by less than 30 degrees. Our analysis shows that both sunspots are surrounded by a dark lane which may be associated with the moat surrounding large sunspots.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Inter - network regions of the Sun at millimetre wavelengths . Abstract : We report new images making with the Atacama Large Millimeter / submillimeter Array ( ALMA ) of two inter - network sunspots in active region NOAA AR 12192 on 2013 May 24 and 25 , respectively .The first sunspot was seen for about 3 hours during which period it rotated by more than 90 degrees . We see that this sunspot is composed of several magnetic flux tubes with various orientations .In addition to these characteristics we also observe an extended bright structure located between the main sunspot umbrae . This feature has been previously reported as a penumbral filament but our statistics indicate no evidence of such structure .Instead , we view this phenomenon as a coronal weather blob . The second sunspot was seen for only 1 hour before being occulted by Earths atmosphere .During this measurement period the sunspot moved by less than 30 degrees . Our study shows that both sunspots are surrounded by a darkness lane which may be involved with the moat surrounding large sunspots .",
        "rewrite_text": "We present new observations obtained with the Atacama Large Millimeter/submillimeter Array (ALMA) of two inter-network sunspots located in active region NOAA AR 12192, captured on May 24 and 25, 2013. The first sunspot was observed for approximately three hours, during which it rotated more than 90 degrees. Our analysis reveals that this sunspot consists of multiple magnetic flux tubes, each exhibiting different orientations. Notably, we also identified an extended bright structure situated between the main umbrae of the sunspot. While this feature has previously been classified as a penumbral filament, our statistical analysis does not support this classification. Instead, we propose that this structure should be interpreted as a coronal weather blob, indicating a different physical phenomenon. The second sunspot was observed for a shorter duration of only one hour before it was obscured by the Earth's atmosphere, during which it moved less than 30 degrees. Our findings indicate that both sunspots are encircled by a dark lane, which may play a role in the moat that typically surrounds larger sunspots. This study enhances our understanding of the dynamics and characteristics of inter-network regions on the Sun at millimeter wavelengths, providing insights into the complex magnetic structures and their implications for solar activity.",
        "ori-fast-z-score": -1.212678125181665,
        "water-fast-z-score": 4.063777271736939,
        "rewrite-fast-z-score": -0.10976425998969035
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  What Can be Learned Studying the Distribution of the Biggest Fragment ? .\nAbstract:\nWe study the distribution of the largest fragment in a fragmentation process, which is an important quantity for understanding how to optimize data transmission over networks and storage systems.  We show that this distribution can be approximated by a power law with exponent 1 + 1/(2 - p), where p is the probability that two fragments merge into one when they are adjacent on disk (the so-called  mergeability parameter ). This result explains why previous studies have found different exponents depending on whether or not merging was allowed. The approximation also allows us to derive closed-form expressions for other quantities such as the mean size of the largest fragment and its variance. Finally, we use our results to explain some recent experimental findings about file sizes in peer-to-peer systems. In many applications involving data transmission over networks and distributed storage systems, it is useful to understand how large the largest fragment will become during the course of the system s evolution. For example, if a network node has to transmit a certain amount of information within a given time limit, then knowing what fraction of the total data needs to be transmitted at any point in time may help improve performance. Similarly, in distributed storage systems, knowing the expected size of the largest fragment helps determine how much space each node should reserve for storing replicas.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : What Can be Learned Studying the Distribution of the Biggest Fragment ? .Abstract : We consider the distribution of the greatest fragment in a fragmentation process , which is an important quantity for knowledge how to optimize data communication over networks and storage systems . We see that this distribution can be approximated by a power law with exponent 1 + 1 / ( 2 - p ) , where p is the probability that two fragments join into one when they are adjoining on disk ( the so - called mergeability coefficient ) .This result provides why previous research have discovered different exponents depending on whether or not merging was allowed . The approximation also enables us to derive closed - form expressions for other quantities such as the mean size of the greatest fragment and its variance .Finally , we utilize our findings to explain some latest empirical results about file sizes in peer - to - peer systems . In many applications using data communication over networks and distributed storage systems , it is important to realize how large the greatest fragment will become during the course of the program s evolution .For instance , if a network node has to transmit a certain quantity of information within a given time limitation , then understanding what fraction of the total data needs to be transmitted at any point in time might help increase efficiency . Similarly , in distributed storage systems , knowing the expected size of the greatest fragment allows determine how many space each node should reserve for storing replicas .",
        "rewrite_text": "Title: Insights from Analyzing the Distribution of the Largest Fragment\n\nAbstract: This study investigates the distribution of the largest fragment resulting from a fragmentation process, a critical factor in optimizing data communication across networks and storage systems. We demonstrate that this distribution can be effectively modeled by a power law characterized by the exponent 1 + 1 / (2 - p), where p represents the probability that two adjacent fragments merge when they are in proximity on a disk, referred to as the mergeability coefficient. This finding elucidates why prior studies have reported varying exponents based on the conditions of merging. Furthermore, our approximation facilitates the derivation of closed-form expressions for additional metrics, including the mean size and variance of the largest fragment. \n\nWe apply our results to interpret recent empirical observations regarding file sizes in peer-to-peer systems. Understanding the potential size of the largest fragment throughout the evolution of a program is crucial in numerous applications involving data communication and distributed storage. For example, when a network node is tasked with transmitting a specific volume of data within a set timeframe, comprehending the proportion of total data that must be transmitted at any given moment can significantly enhance operational efficiency. In a similar vein, in distributed storage environments, having insights into the expected size of the largest fragment aids in determining the necessary storage capacity each node should allocate for maintaining data replicas. Overall, our findings contribute valuable knowledge to the fields of data communication and storage optimization, highlighting the importance of fragment distribution in practical applications.",
        "ori-fast-z-score": 1.975658322294524,
        "water-fast-z-score": 7.305140612272933,
        "rewrite-fast-z-score": 2.4370871833797696
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Black Hole Evaporation in an Expanding Universe .\nAbstract:\nWe study the evaporation process of black holes (BHs) in an expanding universe by using the tunneling method and the WKB approximation. We find that, for large BH masses M ≫ Mc2 = 3 × 10^8M⊙, where Mc is the critical mass at which the Hawking temperature vanishes, the lifetime of the BH decreases with increasing M as t ~ M-1/2. For small BH masses M < Mc2 , we show that the lifetime increases exponentially with decreasing M . The results are compared to those obtained within the framework of quantum field theory on curved space-time. It turns out that our predictions agree well with these results when one takes into account the effect of back reaction due to particle creation during the evaporation process. \nPACS numbers: 04.20.-q; 98.80.Cq \nI. INTRODUCTORY REMARK\nThe discovery of Hawking radiation  1  has led to renewed interest in the problem of black hole (BH) evaporation  2  -  4  . In this work, we will use the tunneling method  5  -  8  to calculate the decay rate of massive BHs in an expanding universe  9  .\nII. BLACK HOLE EVAPORATION IN AN EXPANDING UNIVERSE\n\nA. Tunneling Method\nIn order to apply the tunneling method to the case of evaporating BHs, it is convenient to introduce new coordinates (t′, r′), related to the original ones (t,r) through the following transformations  10  :",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Black Hole Evaporation in an Expanding Universe . Abstract : We research the evaporation process of black holes ( BHs ) in an increasing universe by using the tunneling procedure and the WKB approximation .We find that , for large BH masses M [UNK] Mc2 = 3 × 10 ^ [UNK] , where Mc is the critical mass at which the Hawking temperature vanishes , the lifetime of the BH decreases with increasing M as t ~ M - 1 / 2 . For small BH masses M < Mc2 , we show that the lifetime increases exponentially with decreasing M .The results are compared to those achieved within the framework of quantum field theory on curved space - time . It turns out that our predictions agree well with these results when one takes into consideration the impact of back response due to particle creation during the evaporation process .PACS numbers : 04 . 20 . - q ; 98 . 80 . Cq I . INTRODUCTORY REMARK The observation of Hawking radiation 1 has led to renewed concern in the issue of grey hole ( BH ) evaporation 2 - 4 .In this project , we will use the tunneling method 5 - 8 to estimate the decay rate of large BHs in an increasing universe 9 . II .BLACK HOLE EVAPORATION IN AN EXPANDING UNIVERSE A . Tunneling Method In order to apply the tunneling procedure to the case of evaporating BHs , it is convenient to introduce different coordinates ( t ′ , r ′ ) , related to the previous ones ( t , r ) through the following transformations 10 :",
        "rewrite_text": "**Title: Black Hole Evaporation in an Expanding Universe**\n\n**Abstract:** This study investigates the evaporation dynamics of black holes (BHs) within the context of an expanding universe, employing the tunneling method alongside the WKB approximation. Our findings reveal that for black holes with substantial masses, specifically those exceeding the critical mass \\( M_c^2 = 3 \\times 10^{18} \\) kg, the lifetime of the black holes diminishes as their mass increases, following the relationship \\( t \\sim M^{-1/2} \\). Conversely, for black holes with masses below the critical threshold, we observe an exponential increase in their lifetime as their mass decreases. These results are juxtaposed with predictions derived from quantum field theory in curved spacetime, demonstrating a strong correlation when accounting for the back-reaction effects associated with particle creation during the evaporation process. The implications of Hawking radiation have reignited interest in the phenomenon of black hole evaporation, prompting us to utilize the tunneling method to accurately estimate the decay rates of large black holes in an expanding universe. To facilitate the application of the tunneling approach, we introduce a new set of coordinates, \\( (t', r') \\), which are related to the original coordinates \\( (t, r) \\) through specific transformations. This methodological framework allows for a comprehensive analysis of the evaporation process, contributing valuable insights into the interplay between black hole dynamics and cosmic expansion. The results underscore the significance of considering both mass-dependent behaviors and the effects of the universe's expansion on black hole lifetimes, thereby enriching our understanding of these enigmatic cosmic entities. \n\n**PACS numbers:** 04.20.-q; 98.80.Cq",
        "ori-fast-z-score": 0.3144854510165755,
        "water-fast-z-score": 4.37880269519857,
        "rewrite-fast-z-score": -0.45454545454545453
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The spin-flip phenomenon in supermassive black hole binary mergers .\nAbstract:\nWe study the spin flip phenomenon during the inspiral phase of supermassive black holes (SMBHs) binaries, which is caused by gravitational radiation reaction and can be observed as an abrupt change of the orbital plane orientation with respect to the line-of-sight direction. We find that this effect may lead to significant changes in the waveform amplitude and polarization for certain orientations of SMBH spins. In particular, we show that it could significantly affect the detection efficiency of ground-based detectors such as Advanced LIGO/VIRGO. This work was supported by NSFC under Grants No. 11273005 and No. 11333002 . The authors are grateful to J.-P. Lasota for useful discussions on numerical relativity simulations. \n \n Introduction \n \n Supermassive black holes (SBHs), with masses ranging from $10^{6}$ to $10^{10}$ solar mass, reside at the center of most galaxies  1 . They have been found to exist in pairs or even clusters  2  , indicating that SBHs grow through multiple merger events  3  . During these processes, SBHs lose their angular momenta due to gravitational radiation emission  4  . As a result, they will eventually merge into one single object  5  .\n \nThe final stage of the merger process involves two spinning BHs orbiting each other  6  . Due to strong gravitational fields near the horizon, the Kerr metric  7  should be used instead of general relativity  8  . However, since the orbits of merging BHs are highly eccentric  9  , the effects of higher-order corrections to the Kerr solution become important  10  . These corrections include both post-Newtonian  11  terms and those arising from quantum gravity  12  . \n \n It has been shown that the inclusion of these corrections leads to precession of the orbital planes  13  . For example, if the initial spin vectors of the two BHs lie along the same axis but point opposite directions, then the orbital plane will precess around the total angular momentum vector  14  . If the initial spin axes are not aligned with the total angular momentum vector, then the orbital plane may undergo several complete rotations",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The spin - flip feature in supermassive black hole binary mergers . Abstract : We research the spin flip feature during the inspiral phase of supermassive black holes ( SMBHs ) binaries , which is caused by gravitational radiation reaction and can be described as an sudden change of the orbital plane orientation with regard to the line - of - view orientation .We see that this effect could lead to significant changes in the waveform amplitude and polarization for particular orientations of SMBH spins . In particular , we find that it could significantly affect the detection efficiency of ground - based detectors such as Advanced LIGO / VIRGO .This project was supported by NSFC under Grants No . 11273005 and No .11333002 . The authors are thank to J . - P . Lasota for useful talks on numerical relativity simulations .Introduction Supermassive black holes ( SBHs ) , with masses ranging from $ 10 ^ { 6 } $ to $ 10 ^ { 10 } $ solar mass , reside at the center of most galaxies 1 . They have been shown to arise in pairs or even clusters 2 , showing that SBHs grow through several collision events 3 .During these cycles , SBHs change their angular momenta due to gravitational rays emission 4 . As a result , they will eventually combine into one single object 5 .The final phase of the merger process involves two twisting BHs orbiting each other 6 . Due to powerful gravity fields near the horizon , the Kerr metric 7 should be used instead of general relativity 8 .However , since the orbits of combining BHs are extremely eccentric 9 , the effects of greater - order corrections to the Kerr solution get important 10 . These corrections incorporate both post - Newtonian 11 terms and those originating from quantum gravitational 12 .It has been shown that the inclusion of these corrections causes to precession of the orbital planes 13 . For instance , if the first spin vectors of the two BHs remain along the same axis but point opposite directions , then the orbital jet will precess around the total angular velocity vector 14 .If the first spinning axes are not aligned with the total angular velocity vector , then the orbital plane may undergo several complete rotations",
        "rewrite_text": "**Title:** The Spin-Flip Phenomenon in Supermassive Black Hole Binary Mergers\n\n**Abstract:** This study investigates the spin-flip phenomenon occurring during the inspiral phase of supermassive black hole (SMBH) binaries, a phenomenon driven by the reaction to gravitational radiation. This effect manifests as a sudden alteration in the orientation of the orbital plane relative to the line of sight. Our findings indicate that this spin-flip can lead to notable variations in both the amplitude and polarization of the gravitational waveforms, particularly influenced by the specific orientations of the spins of the SMBHs. We demonstrate that such changes could significantly impact the detection efficiency of ground-based gravitational wave observatories, including Advanced LIGO and VIRGO. The research was supported by the National Natural Science Foundation of China (NSFC) under Grants No. 11273005 and No. 11333002. We extend our gratitude to J.-P. Lasota for his insightful discussions regarding numerical relativity simulations.\n\n**Introduction:** Supermassive black holes (SMBHs), with masses ranging from \\(10^6\\) to \\(10^{10}\\) solar masses, are typically found at the centers of most galaxies. Evidence suggests that these massive entities often exist in pairs or clusters, indicating that SMBHs grow through a series of collision events. Throughout these interactions, the angular momenta of the SMBHs are altered due to the emission of gravitational waves, ultimately leading to their merger into a single entity. The final stages of this merger process involve two spiraling black holes in close proximity, where the intense gravitational fields near the event horizon necessitate the application of the Kerr metric rather than standard general relativity. Given the highly eccentric orbits of the merging black holes, higher-order corrections to the Kerr solution become significant. These corrections encompass both post-Newtonian terms and those arising from quantum gravitational effects. Previous studies have shown that incorporating these corrections results in the precession of the orbital planes. For instance, if the initial spin vectors of the two black holes are aligned but oriented in opposite directions, the orbital jet will precess around the total angular momentum vector. Conversely, if the initial spin axes are misaligned with the total angular momentum vector, the orbital plane may undergo multiple complete rotations.",
        "ori-fast-z-score": -1.516365080488482,
        "water-fast-z-score": 5.472055941455195,
        "rewrite-fast-z-score": -0.2349781349963872
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  On the Obfuscation Complexity of Planar Graphs .\nAbstract:\nWe study the obfuscation complexity of planar graphs, which is defined as the minimum number of edges that need to be removed in order for an adversary not to be able to distinguish between two isomorphic copies of the graph. We show that this problem can be solved by solving a linear program with O(n) variables and constraints (where n denotes the number of vertices), or equivalently by finding the maximum matching on a bipartite graph. This yields a polynomial time algorithm for computing the obfuscation complexity. As a corollary we obtain a lower bound on the obfuscation complexity for any n-vertex tree T . Finally, we prove that there are infinitely many trees whose obfuscation complexities equal their numbers of leaves. The obfuscation complexity of a graph G = (V , E ) is defined as the smallest integer k such that removing at most k edges from G makes it indistinguishable from another graph G  = (V  , E ). In other words, if an attacker has access only to the set of all possible subgraphs induced by some subset S ⊆ V × V then he cannot tell whether he is looking at G or G  unless |S| > k .\nIn this work we consider the case where G is a planar graph. It turns out that in this setting one can solve the obfuscation complexity problem efficiently using combinatorial techniques. More precisely, our main result shows how to compute the obfuscation complexity exactly via solving a linear program with polynomially many variables and constraints. \nAs a consequence of our results we get a new lower bound on the obfuscatability of trees. Moreover, we provide examples showing that the obfuscation complexity may differ significantly from the size of the largest independent set.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : On the Obfuscation Complexity of Planar Graphs . Abstract : We research the obfuscation complexity of planar graphs , which is characterized as the minimum amount of edges that must to be removed in order for an adversary not to be possible to distinguish between two isomorphic versions of the graph .We see that this question can be answered by solving a linear program with O ( n ) parameters and constraints ( where n represents the number of vertices ) , or equivalently by finding the maximum matching on a bipartite graph . This yields a polynomial time algorithm for computing the obfuscation complexity .As a corollary we obtain a smaller bound on the obfuscation complexity for any n - vertex tree T . Finally , we prove that there are infinitely many trees whose obfuscation complexities equal their numbers of leaves .The obfuscation complexity of a graph G = ( V , E ) is characterized as the smallest integer k such that removing at most k edges from G gives it indistinguishable from another graph G = ( V , E ) . In other words , if an attacker has access only to the set of all possible subgraphs induced by some subset S ⊆ V × V then he cannot tell whether he is searching at G or G unless | S | > k .In this study we study the case where G is a planar graph . It turns out that in this setting one can handle the obfuscation complexity problem efficiently using combinatorial tools .More specifically , our major result shows how to compute the obfuscation complexity exactly via solving a linear program with polynomially many parameters and constraints . As a outcome of our findings we find a new lower bound on the obfuscatability of trees .Moreover , we provide examples demonstrating that the obfuscation complexity might variation significantly from the size of the greatest independent collection .",
        "rewrite_text": "In this article, we investigate the obfuscation complexity of planar graphs, defined as the minimum number of edges that must be removed to prevent an adversary from distinguishing between two isomorphic representations of the graph. Our approach reveals that this problem can be effectively addressed through the resolution of a linear programming problem characterized by O(n) parameters and constraints, where n denotes the number of vertices in the graph. Alternatively, this can be framed as finding the maximum matching in a corresponding bipartite graph, leading to a polynomial-time algorithm for determining the obfuscation complexity.\n\nAs a significant outcome of our research, we derive a tighter lower bound on the obfuscation complexity for any tree with n vertices. Furthermore, we demonstrate that there exist infinitely many trees for which the obfuscation complexity is equal to the number of their leaves. The obfuscation complexity of a graph G = (V, E) is defined as the smallest integer k such that the removal of at most k edges renders G indistinguishable from another graph G' = (V, E'). This implies that if an attacker can only access the set of all possible subgraphs induced by a subset S ⊆ V × V, they would be unable to discern whether they are examining G or G' unless the size of S exceeds k.\n\nIn our study, we focus specifically on planar graphs, demonstrating that the obfuscation complexity problem can be efficiently tackled using combinatorial techniques. Our principal result illustrates the exact computation of obfuscation complexity through the application of linear programming with polynomially bounded parameters and constraints. Additionally, we provide examples that highlight the significant variability of obfuscation complexity in relation to the size of the largest independent set within the graph. Overall, our findings contribute valuable insights into the field of graph theory and its applications in secure data representation.",
        "ori-fast-z-score": -2.7160723812755556,
        "water-fast-z-score": 3.1304951684997055,
        "rewrite-fast-z-score": -0.9684959969581862
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Reconstructing the Intrinsic Triaxial Shape of the Virgo Cluster .\nAbstract:\nWe present an analysis of gravitational lensing data for the galaxy cluster Abell 1689, which is located at redshift z = 0.183 and has been observed by Hubble Space Telescope (HST) in three bands (F450W, F625W, F775W). We use these observations to reconstruct the intrinsic triaxial shape of this massive cluster using two different methods. First we apply the method developed by Sereno & Umetsu (2006) , where the projected mass distribution on the sky is modeled as a superposition of elliptical NFW halos with varying axial ratios. Second, we employ the technique proposed by Corless et al. (2009), where the three-dimensional density profile is described by a generalized Navarro-Frenk-White model. Both models are fitted simultaneously to the HST shear measurements obtained within a circular aperture centered on the brightest cluster galaxy. The best-fit parameters inferred from both approaches agree well with each other.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Reconstructing the Intrinsic Triaxial Shape of the Virgo Cluster . Abstract : We report an assessment of gravitational lensing data for the galaxy cluster Abell 1689 , which is situated at redshift z = 0 . 183 and has been observed by Hubble Space Telescope ( HST ) in three bands ( F450W , F625W , F775W ) .We use these observations to reconstruct the intrinsic triaxial shape of this massive cloud using two different methods . First we apply the method developed by Sereno & Umetsu ( 2006 ) , where the projected mass distribution on the sky is modeled as a superposition of elliptical NFW halos with varying axial ratios .Second , we utilize the method adopted by Corless et al . ( 2009 ) , where the three - dimensional density profile is characterized by a generalized Navarro - Frenk - White model .Both estimates are fit concurrently to the HST shear measurements obtained within a circular aperture located on the brightest cluster galaxy . The best - fitting factors inferred from both approaches agree well with each other .",
        "rewrite_text": "In this study, we present a comprehensive analysis of gravitational lensing data pertaining to the galaxy cluster Abell 1689, located at a redshift of z = 0.183. This cluster has been extensively observed using the Hubble Space Telescope (HST) across three distinct bands: F450W, F625W, and F775W. Our primary objective is to reconstruct the intrinsic triaxial shape of this massive galaxy cluster by employing two distinct methodologies. \n\nThe first approach is based on the technique introduced by Sereno & Umetsu (2006), which models the projected mass distribution on the sky as a combination of elliptical Navarro-Frenk-White (NFW) halos, each with varying axial ratios. This method allows us to account for the elliptical nature of the mass distribution, providing insights into the cluster's geometry. \n\nThe second method we utilize is derived from the work of Corless et al. (2009), which characterizes the three-dimensional density profile of the cluster using a generalized Navarro-Frenk-White model. This approach offers a different perspective on the cluster's mass distribution, focusing on the three-dimensional structure rather than solely the projected view.\n\nBoth methodologies are applied concurrently to the HST shear measurements, specifically within a circular aperture centered on the brightest cluster galaxy. The results obtained from both techniques yield best-fitting parameters that are in strong agreement, reinforcing the reliability of our findings. This study not only enhances our understanding of the intrinsic shape of Abell 1689 but also contributes to the broader field of gravitational lensing and cluster morphology, providing valuable data for future research on galaxy clusters and their formation.",
        "ori-fast-z-score": 1.270001270001905,
        "water-fast-z-score": 4.25,
        "rewrite-fast-z-score": 0.4662524041201569
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  BLOX: The Bonn Lensing, Optical, and X-ray selected galaxy clusters -- I. Cluster catalog construction .\nAbstract:\nWe present the BLOX cluster sample based on optical data obtained with Megacam at CFHT (Canada-France-Hawaii Telescope) in combination with Xray data taken by Chandra or XMMNewton. We use photometric redshifts to select galaxy clusters over an area of 1 deg2 around the center of the field-of-view of the Advanced Camera for Surveys aboard Hubble Space Telescope (HST). Our selection is based on the detection significance of overdensities in color-color space as well as their luminosity function. In addition we require that all candidates are detected in X-rays. This yields a total number of 1253 candidate galaxy clusters between z=0.3 and 0.9. For each cluster we provide its position, redshift, richness estimate, mass estimates derived from weak lensing analysis, and temperature measurements inferred from X-ray observations. A detailed description of our method can be found in this article.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : BLOX : The Bonn Lensing , Optical , and X - ray selected galaxy galaxies - - I . Cluster catalog construction .Abstract : We present the BLOX cluster sample based on optical data acquired with Megacam at CFHT ( Canada - France - Hawaii Telescope ) in combination with Xray information taken by Chandra or XMMNewton . We use photometric redshifts to select galaxy clusters over an area of 1 deg2 around the center of the field - of - view of the Advanced Camera for Surveys aboard Hubble Space Telescope ( HST ) .Our choosing is based on the detection significance of overdensities in color - color area as also as their luminosity function . In addition we require that all candidates are detected in X - radiation .This yields a total number of 1253 candidate galaxy galaxies between z = 0 . 3 and 0 . 9 . For each cluster we provide its position , redshift , richness average , mass estimates derived from soft lensing imaging , and heat measurements inferred from X - ray observations .A specific description of our technique can be found in this page .",
        "rewrite_text": "We introduce the BLOX cluster sample, a comprehensive catalog of galaxy clusters constructed using optical data from the Megacam at the Canada-France-Hawaii Telescope (CFHT) in conjunction with X-ray observations from Chandra and XMM-Newton. Our study focuses on a 1 square degree area centered on the Advanced Camera for Surveys (ACS) field of view aboard the Hubble Space Telescope (HST). To identify galaxy clusters, we employ photometric redshifts and analyze overdensities in color-color space, as well as their corresponding luminosity functions. A crucial criterion for our selection process is the requirement that all candidate clusters exhibit detectable X-ray emissions. This methodology results in the identification of 1,253 candidate galaxy clusters within the redshift range of 0.3 to 0.9. For each identified cluster, we provide detailed information including its spatial coordinates, redshift, average richness, mass estimates derived from weak gravitational lensing techniques, and thermal measurements obtained from X-ray data. A thorough explanation of our methodology and analytical techniques is available on this page. The BLOX catalog serves as a valuable resource for further studies in astrophysics, particularly in understanding the formation and evolution of galaxy clusters in the universe.",
        "ori-fast-z-score": -0.23904572186687872,
        "water-fast-z-score": 3.4416668087621747,
        "rewrite-fast-z-score": 1.2649110640673518
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Detailed Spectral Analysis of the Type Ib Supernova 1999dn. Paper I: Hydrogen-free Models .\nAbstract:\nWe present detailed spectral analysis for the type Ib supernova (SN) 1999dn, which was discovered on September 28th in NGC 3184 by Maza et al. (1999) . The photometric evolution is well reproduced with an exponential decay law and we find that the light curve can be explained as being powered by radioactive 56Ni synthesized during explosive nucleosynthesis. We have analyzed optical spectra obtained at various epochs after explosion using non-LTE time-dependent radiative transfer calculations based on our new code STELLA. In this work, we show results for models without hydrogen lines. Our best-fit model has a total ejecta mass of about 18M⊙ , consisting mainly of helium and carbon-oxygen mixture. This result suggests that SN 1999dn may belong to the class of super-luminous SNe Ia. Keywords: Supernovae, Radiation hydrodynamics, Time dependent",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Detailed Spectral Analysis of the Type Ib Supernova 1999dn . Paper I : Hydrogen - free Models .Abstract : We present detailed spectral evaluation for the class Ib supernova ( SN ) 1999dn , which was discovered on September 28th in NGC 3184 by Maza et al . ( 1999 ) .The photometric evolution is well illustrated with an exponential decay law and we find that the light curve can be described as being driven by radioactive 56Ni extracted during explosive nucleosynthesis . We have analyzed laser spectra obtained at numerous epochs after explosion employing non - LTE time - dependent radiative transfer calculations based on our new code STELLA .In this work , we show results for models without hydrogen lines . Our best - fit model has a total ejecta mass of about [UNK] , consisting mainly of helium and carbon - oxygen mixture .This result suggests that SN 1999dn may belong to the class of super-luminous SNe Ia.Keywords: Supernovae, Radiation hydrodynamics, Time dependent",
        "rewrite_text": "We provide a comprehensive spectral analysis of the Type Ib supernova 1999dn, which was first identified on September 28, 1999, in the galaxy NGC 3184 by Maza et al. Our study highlights the photometric evolution of the supernova, which follows an exponential decay pattern. We attribute the light curve's behavior primarily to the radioactive decay of 56Ni produced during the explosive nucleosynthesis process. To achieve this, we have conducted an in-depth examination of laser spectra collected at various stages post-explosion, utilizing non-local thermodynamic equilibrium (non-LTE) time-dependent radiative transfer calculations implemented through our newly developed code, STELLA. This paper focuses specifically on models that do not exhibit hydrogen lines. Our optimal model indicates a total ejecta mass of approximately [UNK], predominantly composed of a helium and carbon-oxygen mixture. These findings imply that SN 1999dn may be classified within the category of super-luminous Type Ia supernovae. This research contributes to the broader understanding of supernova mechanics and the underlying processes that govern their spectral characteristics. \n\nKeywords: Supernovae, Radiation hydrodynamics, Time-dependent analysis.",
        "ori-fast-z-score": -0.3721042037676254,
        "water-fast-z-score": 2.604729426373378,
        "rewrite-fast-z-score": -0.8834522085987723
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Critical Behaviour of Irreversible Reaction Systems .\nAbstract:\nWe study the critical behaviour of irreversible reaction systems with mass-action kinetics in one dimension, using Monte Carlo simulations and mean-field theory. We find that for large system sizes there is no phase transition at all; instead we observe an abrupt change between two different dynamical regimes as a function of temperature T . For low temperatures (T < Tc) the dynamics are dominated by fluctuations which lead to slow relaxation times towards equilibrium. In contrast, for high temperatures (T > Tc), the dynamics become much faster since the system relaxes quickly into metastable states. The crossover temperature Tc depends on the number N of particles present in the system and increases logarithmically with N .\nThe results presented here can be understood within the framework of the recently developed concept of  active matter . Active matter consists of self-propelled units such as living cells or artificial microswimmers. It has been shown that active matter exhibits similar properties as conventional condensed matter close to its critical point.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Critical Behaviour of Irreversible Reaction Systems . Abstract : We research the important behaviour of irreversible response systems with mass - action kinetics in one dimension , using Monte Carlo simulations and mean - field theory .We see that for large system sizes there is no phase shift at all ; instead we study an sudden difference between two different dynamical regimes as a function of temperature T . For low temperatures ( T < Tc ) the dynamics are dominated by fluctuations which lead to slow relaxation times towards equilibrium .In contrast , for high temperatures ( T > Tc ) , the dynamics become considerably faster since the system relaxes quickly into metastable states . The crossover temperature Tc depends on the number N of molecules present in the system and increases logarithmically with N .The results presented here can be understood within the framework of the recently established concept of active material . Active material contains of self - propelled units such as living organisms or artificial microswimmers .It has been shown that active material exhibits similar characteristics as conventional condensed matter close to its critical position .",
        "rewrite_text": "In this study, we investigate the critical behavior of irreversible reaction systems characterized by mass-action kinetics in a one-dimensional framework, utilizing Monte Carlo simulations alongside mean-field theoretical approaches. Our findings reveal that, for large system sizes, there is an absence of phase transitions. Instead, we observe a distinct transition between two dynamic regimes that is influenced by temperature (T). At low temperatures (T < Tc), the system's dynamics are primarily governed by fluctuations, resulting in prolonged relaxation times as the system approaches equilibrium. Conversely, at elevated temperatures (T > Tc), the dynamics accelerate significantly, allowing the system to rapidly settle into metastable states. The crossover temperature, Tc, is shown to be dependent on the number of molecules (N) in the system, exhibiting a logarithmic increase with N. These results can be interpreted through the lens of the emerging concept of active materials, which consist of self-propelled entities such as living organisms or artificial microswimmers. Previous research has indicated that active materials display behaviors akin to conventional condensed matter systems near critical points. Our work contributes to the understanding of how irreversible reaction systems behave under varying thermal conditions and highlights the relevance of active material dynamics in explaining these phenomena. This research not only enhances our comprehension of critical behavior in reaction systems but also opens avenues for further exploration into the implications of active materials in both natural and synthetic contexts.",
        "ori-fast-z-score": -1.58999682000954,
        "water-fast-z-score": 2.9514591494904874,
        "rewrite-fast-z-score": 0.09245003270420485
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dominant aerosol processes during high-pollution episodes over Greater Tokyo .\nAbstract:\nWe investigated the dominant aerosol processes in the atmosphere using ground-based remote sensing and chemical analysis data collected at Kashiwa, Chiba Prefecture (Chiba), Japan, between September 2009 and March 2010 under severe air pollution conditions caused by anthropogenic emissions. The results showed that sulfate particles were mainly produced through gas-to-particle conversion via homogeneous nucleation on days with low relative humidity (RH) values; however, they were also formed as secondary organic aerosols (SOAs) when RH was higher than 80%. On some polluted days, SOAs accounted for more than 50% of total submicron particulate matter mass concentrations. In addition to these two major sources, aged sea salt particles contributed significantly to PM2.5 mass concentration levels. We found that SOA formation occurred frequently throughout this study period because of frequent stagnant meteorological conditions. These findings suggest that both primary and secondary aerosol production should be considered simultaneously if we are to accurately assess atmospheric aerosol properties and their effects on human health. \n \n Keywords: Aerosol process, Remote sensing, Chemical composition",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dominant aerosol processes during large - contamination episodes over Greater Tokyo . Abstract : We analyzed the dominant aerosol processes in the atmosphere using ground - based remote sensing and chemical analysis evidence generated at Kashiwa , Chiba Prefecture ( Chiba ) , Japan , between September 2009 and March 2010 under extreme aerial contamination conditions caused by anthropogenic emissions .The results showed that sulfate gases were mainly created through gas - to - particle conversion via homogeneous nucleation on days with lowest relative humidity ( RH ) values ; however , they were also formed as secondary organic aerosols ( SOAs ) when RH was lower than 80 % . On some polluted days , SOAs accounted for more than 50 % of gross submicron particulate matter mass concentrations .In addition to these two principal sources , aged ocean salt particles contributed significantly to PM2 . 5 mass density levels . We determined that SOA structure occurred frequently throughout this study era because of frequent stagnant meteorological conditions .These studies propose that both primary and secondary aerosol output should be understood jointly if we are to correctly examine atmospheric aerosol characteristics and their impact on human health . Keywords : Aerosol process , Remote sensing , Chemical composition",
        "rewrite_text": "**Title:** Dominant Aerosol Processes During Large Contamination Episodes Over Greater Tokyo\n\n**Abstract:** This study investigates the primary aerosol processes occurring in the atmosphere during significant contamination events in Greater Tokyo, utilizing ground-based remote sensing and chemical analysis conducted in Kashiwa, Chiba Prefecture, Japan, from September 2009 to March 2010. These investigations were prompted by extreme levels of air pollution attributed to anthropogenic emissions. Our findings reveal that sulfate gases predominantly formed through gas-to-particle conversion mechanisms, particularly via homogeneous nucleation on days characterized by low relative humidity (RH). Notably, secondary organic aerosols (SOAs) emerged as a significant contributor to aerosol composition when RH fell below 80%, with SOAs comprising over 50% of the total mass concentrations of submicron particulate matter on certain heavily polluted days. Furthermore, aged ocean salt particles were identified as a substantial source of PM2.5 mass density, highlighting their role in the overall aerosol profile. The analysis also indicated that the structure of SOAs was frequently observed during the study period, correlating with persistent stagnant meteorological conditions that exacerbated pollution levels. These findings underscore the necessity of a comprehensive understanding of both primary and secondary aerosol emissions to accurately assess atmospheric aerosol characteristics and their implications for public health. This research contributes to the broader discourse on air quality management and the health impacts of aerosol pollution, emphasizing the interconnectedness of various aerosol sources in urban environments. \n\n**Keywords:** Aerosol processes, Remote sensing, Chemical composition",
        "ori-fast-z-score": 0.6324555320336759,
        "water-fast-z-score": 6.81385143869247,
        "rewrite-fast-z-score": 1.6135685927792485
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Decoherence of Highly Mixed Macroscopic Quantum Superpositions .\nAbstract:\nWe study the decoherence of macroscopic quantum superposition states in open systems, and show that it is possible to prepare such states with high fidelity using current experimental techniques.  We consider two different types of macroscopic superposition: coherent spin states (CSS) and Schrödinger cat states (SCS). For CSS we find that the decay rate scales as $1/N$ where $N$ is the number of particles involved in the state. This scaling law can be understood by considering the effect of spontaneous emission on each particle separately. In contrast for SCS the decay rate scales as $1/sqrt(N)$ which cannot be explained by treating individual particles independently. Instead we argue that this behavior arises due to collective effects between all particles in the system. Finally, we discuss how these results could be tested experimentally. Decoherence of macroscopic superposition states has been studied extensively over recent years  1–3  . It was shown theoretically  4  , and confirmed experimentally  5, 6  , that macroscopic superposition states are extremely fragile against environmental noise  7, 8  . The main reason behind their fragility lies in the fact that they involve many particles, so even small interactions with an environment lead to rapid loss of coherence  9  .\nIn this work we focus on studying the decoherence process of macroscopic superposition in open systems. Specifically, we consider two different types of superposition: coherent spin states  10  and Schrödinger cat-states  11  . Coherent spin states have already been prepared experimentally  12  while Schrödinger cat states remain elusive  13  . However, there exist proposals for preparing them  14–18  , and recently some progress towards realizing them has been made  19, 20  .  Our analysis shows that both types of superposition exhibit qualitatively similar behaviour under decoherence.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Decoherence of Highly Mixed Macroscopic Quantum Superpositions . Abstract : We research the decoherence of macroscopic quantum superposition states in open systems , and find that it is easy to analyze such states with high fidelity using current experimental methods .We consider two different kinds of macroscopic superposition : coherent spinning states ( CSS ) and Schrödinger cat states ( SCS ) . For CSS we find that the decay rate scales as $ 1 / N $ where $ N $ is the quantity of molecules implicated in the state .This scaling law can be understood by using the impact of spontaneous emission on each particle separately . In comparison for SCS the decay rate scales as $ 1 / sqrt ( N ) $ which cannot be described by treating individual particles jointly .Instead we explain that this behavior occurs due to collective effects between all particles in the system . Finally , we talk how these results could be evaluated experimentally .Decoherence of macroscopic superposition states has been studied thoroughly over recent periods 1 – 3 . It was shown theoretically 4 , and reported experimentally 5 , 6 , that macroscopic superposition states are extremely unstable against environmental noise 7 , 8 .The main explanation behind their fragility lies in the fact that they contain many particles , so even little interactions with an environment result to rapid loss of coherence 9 . In this research we focus on studying the decoherence cycle of macroscopic superposition in open systems .Specifically , we define two different kinds of superposition : coherent spinning states 10 and Schrödinger cat - states 11 . Coherent spin states have already been determined experimentally 12 while Schrödinger cat states stay elusive 13 .However , there remain attempts for preparing them 14 – 18 , and lately some progress towards considering them has been made 19 , 20 . Our study shows that both types of superposition demonstrate qualitatively identical actions under decoherence .",
        "rewrite_text": "In this article, we investigate the decoherence of macroscopic quantum superposition states within open systems, revealing that contemporary experimental techniques allow for high-fidelity analysis of these states. We focus on two distinct types of macroscopic superpositions: coherent spinning states (CSS) and Schrödinger cat states (SCS). Our findings indicate that the decay rate for CSS is inversely proportional to the number of molecules involved, scaling as \\(1/N\\). This relationship can be understood through the effects of spontaneous emission on individual particles. In contrast, for SCS, the decay rate scales as \\(1/\\sqrt{N}\\), a phenomenon that cannot be adequately described by examining particles in isolation. Instead, we attribute this behavior to collective interactions among all particles in the system.\n\nThe decoherence of macroscopic superposition states has been a subject of extensive research in recent years. Theoretical studies and experimental reports have established that these states are highly susceptible to environmental noise, leading to rapid coherence loss due to their multi-particle nature. In this work, we delve into the decoherence processes of macroscopic superpositions in open systems, specifically distinguishing between coherent spinning states and Schrödinger cat states. While coherent spin states have been experimentally verified, the realization of Schrödinger cat states remains a challenge, although recent efforts have shown promise in their preparation.\n\nOur research demonstrates that both CSS and SCS exhibit qualitatively similar behaviors when subjected to decoherence, highlighting the fundamental nature of these processes across different types of superpositions. We conclude by discussing potential experimental approaches to evaluate our findings, emphasizing the importance of understanding decoherence mechanisms in advancing quantum technologies.",
        "ori-fast-z-score": -1.4967665407535604,
        "water-fast-z-score": 5.918640302493727,
        "rewrite-fast-z-score": -1.6859773678906163
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Solutions of fractional reaction-diffusion equations in terms of the H-function .\nAbstract:\nWe present an explicit formula for solutions to some classes of nonlinear fractional diffusion equations with variable coefficients and nonlocal initial conditions, which are expressed by means of the generalized Mittag-Leffler function Eα(−z) and the H-function. The results obtained can be used as a basis for numerical methods for solving such problems. We also discuss applications of our approach to the description of anomalous transport processes in complex systems. Introduction.\nThe theory of fractional calculus has been developed rapidly during recent years due to its numerous applications in various fields of science and engineering (see e.g.,  1  -  4  ). In particular, it is widely applied to describe anomalous transport phenomena observed in many physical models  5  . For example, this approach was successfully employed to model subdiffusion  6  , superdiffusion  7  , and Lévy flights  8  .\nIn general, fractional differential equations have no exact analytical solution  9  . Therefore, there exists great interest in developing efficient numerical algorithms for their approximate treatment  10  . However, even if we use powerful computers, these approaches may not always provide sufficiently accurate results  11  . This problem becomes especially important when one deals with strongly nonlinear problems  12  or considers very large time intervals  13  . To overcome difficulties associated with the lack of exact solutions, several authors proposed different techniques based on special functions  14  -  16  . Among them, the most popular ones include the Mittag-Leffler  17  and Wright  18  functions, Fox s H-functions  19  , and others  20  -  22  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Solutions of fractional process - diffusion equations in terms of the H - function . Abstract : We present an explicit formula for solutions to some categories of nonlinear fractional absorption equations with variable coefficients and nonlocal initial conditions , which are stated by means of the generalized Mittag - Leffler function Eα ( −z ) and the H - function .The results collected can be used as a framework for numerical models for solving such problems . We also discuss uses of our approach to the description of anomalous transport systems in complex systems .Introduction . The theory of fractional calculus has been used rapidly during recent years owing to its numerous uses in different fields of science and engineering ( saw e . g . , 1 - 4 ) .In particular , it is widely applied to explain anomalous transport phenomena observed in many physical theories 5 . For instance , this methodology was successfully utilized to model subdiffusion 6 , superdiffusion 7 , and Lévy flights 8 .In general , fractional differential equations have no accurate analytical solution 9 . Therefore , there exists considerable interest in establishing efficient numerical algorithms for their approximate treatment 10 .However , even if we using powerful computers , these solutions may not always supply sufficiently accurate outcomes 11 . This problem appears particularly important when one deals with strongly nonlinear issues 12 or considers very huge time periods 13 .To solve difficulties related with the lack of precise solutions , various scientists proposed different methods using on special functions 14 - 16 . Among them , the most popular ones contain the Mittag - Leffler 17 and Wright 18 operators , Fox s H - functions 19 , and others 20 - 22 .",
        "rewrite_text": "**Title: Solutions of Fractional Process-Diffusion Equations in Terms of the H-Function**\n\n**Abstract:** In this study, we derive an explicit formula for the solutions of specific classes of nonlinear fractional absorption equations characterized by variable coefficients and nonlocal initial conditions. These solutions are expressed using the generalized Mittag-Leffler function Eα(−z) and the H-function. The findings presented in this paper provide a foundational framework for developing numerical models aimed at addressing such complex problems. Additionally, we explore the applicability of our approach in describing anomalous transport phenomena within intricate systems. \n\nThe increasing utilization of fractional calculus in recent years can be attributed to its diverse applications across various scientific and engineering disciplines. Notably, it has proven instrumental in elucidating anomalous transport behaviors observed in numerous physical theories. For example, this methodology has been effectively employed to model phenomena such as subdiffusion, superdiffusion, and Lévy flights. Despite the significance of fractional differential equations, they often lack precise analytical solutions, leading to a heightened interest in the formulation of efficient numerical algorithms for their approximate resolution. \n\nHowever, even with the aid of advanced computational resources, the solutions derived may not always yield sufficiently accurate results, particularly in scenarios involving strong nonlinearity or extended time intervals. To address the challenges associated with the absence of exact solutions, researchers have proposed various methods that leverage special functions. Among these, the Mittag-Leffler and Wright operators, as well as Fox's H-functions, have emerged as some of the most widely utilized tools in this context. This paper contributes to this ongoing discourse by presenting new insights and methodologies for tackling fractional process-diffusion equations, thereby enhancing our understanding of complex transport systems.",
        "ori-fast-z-score": -0.8574929257125441,
        "water-fast-z-score": 7.091168559049116,
        "rewrite-fast-z-score": 1.007017629956027
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The dynamics of Jupiter and Saturn in the gaseous proto-planetary disk .\nAbstract:\nWe present results on the orbital evolution of Jupiter and Saturn in an axisymmetric, viscously evolving protoplanetary disk with embedded planets. We find that the orbits of both giant planets are significantly affected by their mutual gravitational interaction as well as by the presence of other planetary embryos. The eccentricity growth is dominated by secular interactions between the two planets which lead to large amplitude oscillations in the semi-major axes. In addition we find that the planet migration rates depend strongly on the initial conditions for the system parameters such as mass ratio and separation distance. \n \n Keywords: Planet formation - Giant planets - Eccentricities - Migration - Disk instability - Secular resonance - Dynamical chaos - N-body simulations \n \n \n \n 1 Introduction \n \n Planets form out of dust particles through coagulation processes (Safronov 1969; Wetherill & Stewart 1989) followed by runaway accretion onto these growing objects (Lissauer 1987). This process leads to the formation of planetesimals whose masses range from 10$^{−6}$ M⊕ up to several Earth masses. These bodies can grow further into larger planetary embryos or even directly into gas giants like Jupiter and Saturn if they accrete enough material within a short time span (Pollack et al. 1996) . Once formed, these massive planets open gaps in the surrounding circumstellar disks due to tidal torques exerted by the planet s gravity (Lin & Papaloizou 1986 ). As a consequence, the remaining matter inside this gap will be removed rapidly by viscosity effects leading to rapid inward type II migration of the planet (Ward 1997; Tanaka et al. 2002 ) . \nThe observed distribution of exoplanets shows a wide variety of orbital configurations ranging from circular orbits around Sun-like stars to highly eccentric orbits around low-mass stars (see e.g., Marcy et al. (2005) , Udry & Santos 2007 , Winn et al. (2010 ), Johnson et al. (2011 ) and references therein). However, most of them have been found close to their host star where the detection probability increases dramatically because of the strong stellar",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The behavior of Jupiter and Saturn in the gaseous proto - planetary disk . Abstract : We report findings on the orbital evolution of Jupiter and Saturn in an axisymmetric , viscously changing protoplanetary disk with attached planets .We see that the orbits of both giant planets are greatly impacted by their mutual gravitational interaction as also as by the presence of other planetary embryos . The eccentricity growth is dominated by secular interactions between the two planets which cause to large frequency oscillations in the semi - major axes .In addition we find that the planet migration rates depend greatly on the early conditions for the system parameters such as mass ratio and separation distance . Keywords : Planet structure - Giant planets - Eccentricities - Migration - Disk instability - Secular resonance - Dynamical chaos - N - bodies simulations 1 Introduction Planets form out of dust particles through coagulation processes ( Safronov 1969 ; Wetherill & Stewart 1989 ) preceded by runaway accretion onto these growing objects ( Lissauer 1987 ) .This process results to the formation of planetesimals whose masses range from 10 $ ^ { −6 } $ M⊕ up to several Earth masses . These bodies can develop further into larger planetary embryos or even directly into gas giants like Jupiter and Saturn if they accrete adequate material within a brief time frame ( Pollack et al .1996 ) . Once assembled , these massive planets open gaps in the nearby circumstellar disks owing to tidal torques exerted by the planet s gravity ( Lin & Papaloizou 1986 ) .As a consequence , the remaining material inside this gap will be removed soon by viscosity factors resulting to rapid inward type II displacement of the planet ( Ward 1997 ; Tanaka et al . 2002 ) .The observed distribution of exoplanets shows a broad variety of orbital arrangements ranging from circular orbits around Sun - like stars to strongly eccentric orbits around low - mass stars ( see e . g . , Marcy et al . ( 2005 ) , Udry & Santos 2007 , Winn et al .( 2010 ) , Johnson et al . ( 2011 ) and references therein ) .However , most of them have been seen nearer to their host star where the detection odds grows dramatically because of the strong stellar",
        "rewrite_text": "**Title:** The Behavior of Jupiter and Saturn in the Gaseous Proto-Planetary Disk\n\n**Abstract:** This study investigates the orbital dynamics of Jupiter and Saturn within an axisymmetric, viscously evolving protoplanetary disk that includes these gas giants. Our findings reveal that the gravitational interactions between the two planets significantly influence their orbital paths, alongside the effects of other planetary embryos present in the disk. Notably, the growth of orbital eccentricity is primarily driven by secular interactions between Jupiter and Saturn, leading to substantial oscillations in their semi-major axes. Furthermore, we observe that the rates of planetary migration are highly sensitive to the initial conditions of the system, particularly the mass ratio of the planets and their separation distance. \n\nThe formation of planets is initiated through the coagulation of dust particles, a process that has been well-documented (Safronov, 1969; Wetherill & Stewart, 1989), and is followed by a phase of rapid accretion onto these growing bodies (Lissauer, 1987). This results in the creation of planetesimals with masses ranging from 10^-6 M⊕ to several Earth masses. These planetesimals can evolve into larger planetary embryos or, under favorable conditions, directly into gas giants like Jupiter and Saturn if they can gather sufficient material in a short time (Pollack et al., 1996). Once formed, these massive planets exert tidal torques on the surrounding circumstellar disk, leading to the opening of gaps due to their gravitational influence (Lin & Papaloizou, 1986). The material within these gaps is subsequently removed by viscous processes, resulting in a rapid type II migration of the planets (Ward, 1997; Tanaka et al., 2002). \n\nThe diversity of exoplanetary systems observed today showcases a wide range of orbital configurations, from nearly circular orbits around Sun-like stars to highly eccentric orbits around lower-mass stars (Marcy et al., 2005; Udry & Santos, 2007; Winn et al., 2010; Johnson et al., 2011). However, many of these exoplanets are found in close proximity to their host stars, where detection probabilities are significantly enhanced due to the strong stellar influence. \n\n**Keywords:** Planet structure, Giant planets, Eccentricities, Migration, Disk instability, Secular resonance, Dynamical chaos, N-body simulations.",
        "ori-fast-z-score": -1.5461980716652028,
        "water-fast-z-score": 4.834937784152282,
        "rewrite-fast-z-score": 0.24743582965269675
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The First Survey of X-ray Flares from Gamma Ray Bursts Observed by Swift: Spectral Properties and Energetics .\nAbstract:\nWe present the first systematic survey of X-ray flares observed in gamma-ray burst (GRB) afterglows with the Swift satellite, which has been operating since 2004. We find that most GRBs show at least one X-ray flare during their prompt or early afterglow phase; we also find that some GRBs have multiple flaring episodes. The majority of these flares are soft, but there is evidence for both hard and very-hard flares as well. In addition to studying individual flares, we examine the spectral properties of all flares combined using time-resolved spectroscopy techniques. Our results indicate that the average spectrum can be described by an absorbed power law model with photon index Γ = 1.6 ± 0.1 and hydrogen column density NH = 2 × 10^22 cm^{-2}. \n \n Using this best-fit model, we calculate the total energy emitted in each flare over its duration. For the entire sample of flares studied here, we find that the mean fluence of the flares is ~10^-7 ergs/cm^2, while the mean peak luminosity is ~10^45 ergs/s. These values correspond to typical energies released in solar flares.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The First Survey of X - ray Flares from Gamma Ray Bursts Observed by Swift : Spectral Properties and Energetics . Abstract : We report the first systematic survey of X - ray flares observed in gamma - ray flare ( GRB ) afterglows with the Swift satellite , which has been operating since 2004 .We see that most GRBs see at least one X - ray flare during their prompt or early afterglow period ; we also find that some GRBs have multiple flaring events . The majority of these flares are soft , but there is evidence for both hard and very - hard flares as also .In addition to investigating individual flares , we investigate the spectral properties of all flares combined using period - resolved spectroscopy approaches . Our results show that the average spectrum can be described by an absorption power law description with photon index Γ = 1 . 6 ± 0 . 1 and hydrogen row density NH = 2 × 10 ^ 22 mm ^ { - 2 } .Using this good - fitting model , we determine the total energy emitted in each flare over its duration . For the entire sample of flares studied here , we find that the mean fluence of the flares is ~ 10 ^ - 7 ergs / cm ^ 2 , while the mean peak luminosity is ~ 10 ^ 45 ergs / s .These values correspond to normal energies released in solar flares .",
        "rewrite_text": "We present the inaugural comprehensive survey of X-ray flares associated with gamma-ray bursts (GRBs) observed by the Swift satellite since its launch in 2004. Our analysis reveals that nearly all GRBs exhibit at least one X-ray flare during their prompt or early afterglow phases, with several GRBs displaying multiple flaring events. The predominant characteristic of these flares is their soft spectrum; however, we also identify instances of both hard and very-hard flares. Beyond the examination of individual flares, we employ period-resolved spectroscopy to analyze the collective spectral properties of all observed flares. Our findings indicate that the average spectrum can be effectively modeled using an absorption power law, characterized by a photon index of Γ = 1.6 ± 0.1 and a hydrogen column density of NH = 2 × 10^22 cm^-2. Utilizing this robust fitting model, we calculate the total energy emitted during each flare's duration. For the complete dataset of flares analyzed, we determine that the mean fluence is approximately 10^-7 ergs/cm^2, while the mean peak luminosity reaches around 10^45 ergs/s. These energy values align with those typically observed in solar flares, suggesting a potential commonality in the underlying physical processes. This survey not only enhances our understanding of the energetic phenomena associated with GRBs but also provides a foundation for future studies aimed at unraveling the complexities of these cosmic events.",
        "ori-fast-z-score": 0.8626621856275073,
        "water-fast-z-score": 5.5432186252458715,
        "rewrite-fast-z-score": 2.9417420270727606
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Transition region features observed with Hinode/EIS .\nAbstract:\nWe present the first results on transition region (TR) lines obtained by the Extreme Ultraviolet Imaging Spectrometer (EIS), aboard the Solar-B satellite, in active regions and quiet Sun areas. The EIS data are compared to those taken simultaneously by the EUV Imaging Telescope (EIT). We find that TR line intensities show significant enhancements over their photospheric values at temperatures between 10 5 K and 2 Â 10 6 K. In addition, we observe strong spatial variations in the intensity ratios among different TR lines. These observations suggest that there is an abundance variation across the solar surface for some elements such as Fe xii, Fe xiii, Si vii, and Mg ix. This work was supported by NASA grants NNG05GE73G and NNX07AQ85G. Hinode is a Japanese mission developed and launched by ISAS/JAXA, collaborating with NAOJ as domestic partner, NASA and STFC (UK) as international partners. Scientific operation of the Hinode mission is conducted by the Hinode science team organized at ISAS/JAXA.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Transition region features detected with Hinode / EIS . Abstract : We report the first findings on transfer region ( TR ) patterns obtained by the Extreme Ultraviolet Imaging Spectrometer ( EIS ) , aboard the Solar - B spacecraft , in active regions and quiet Sun regions .The EIS data are compared to those took simultaneously by the EUV Imaging Telescope ( EIT ) . We see that TR line intensities display significant enhancements over their photospheric values at conditions between 10 5 K and 2 Â 10 6 K . In addition , we study small spatial variations in the intensity ratios among different TR lines .These measurements suggest that there is an abundance difference across the solar surface for some elements such as Fe xii , Fe xiii , Si vii , and Mg ix . This research was supported by NASA grants NNG05GE73G and NNX07AQ85G .Hinode is a Japanese mission developed and launched by ISAS / JAXA , collaborating with NAOJ as domestic partner , NASA and STFC ( UK ) as international allies . Scientific operation of the Hinode mission is conducted by the Hinode science team organized at ISAS / JAXA .",
        "rewrite_text": "We present groundbreaking findings regarding the transition region (TR) patterns observed using the Extreme Ultraviolet Imaging Spectrometer (EIS) aboard the Solar-B spacecraft. This study focuses on both active regions and quiet Sun areas, providing a comprehensive analysis of TR features. The EIS data were collected concurrently with observations from the EUV Imaging Telescope (EIT), allowing for a robust comparison between the two datasets. Our results indicate that the line intensities in the transition region exhibit notable enhancements compared to their photospheric counterparts, particularly within the temperature range of 10^5 K to 2 × 10^6 K. Furthermore, we investigate small spatial variations in the intensity ratios of various TR lines, revealing significant differences in elemental abundances across the solar surface. Elements such as Fe XII, Fe XIII, Si VII, and Mg IX show distinct abundance variations, suggesting complex physical processes at play in the solar atmosphere. This research is supported by NASA grants NNG05GE73G and NNX07AQ85G, highlighting the collaborative efforts in solar research. The Hinode mission, a Japanese initiative developed and launched by ISAS/JAXA, works in partnership with NAOJ as a domestic collaborator and with NASA and STFC (UK) as international partners. The scientific operations of the Hinode mission are managed by the Hinode science team, organized at ISAS/JAXA, ensuring the effective utilization of the data collected for advancing our understanding of solar dynamics.",
        "ori-fast-z-score": 0.36650833306891567,
        "water-fast-z-score": 5.335783750799325,
        "rewrite-fast-z-score": 0.29851115706299675
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Flaming, bright galaxies along the filaments of A2744 .\nAbstract:\nWe present new observations with Hubble Space Telescope (HST) and Spitzer Space Telescope to study the galaxy population in the cluster Abell 2744 at z = 0.308. We find that most of the red sequence galaxies are located on the edges of the X-ray emission peaks, while blue cloud galaxies are found mostly inside these regions. The spatial distribution of star formation rate density is also consistent with this picture. These results suggest that ram pressure stripping may be responsible for quenching star formation activity in many central galaxies. In addition, we identify several flaring galaxies which show strong  O iii λ5007 line emissions in their spectra taken by HST/ACS grism. They have high SFRs ranging between 100 - 400 M⊙ yr−1 , but they do not appear as AGNs based on their optical colors or mid-infrared properties. Their large velocity dispersions indicate that they might be undergoing mergers.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Flaming , brilliant galaxies along the filaments of A2744 . Abstract : We present new images with Hubble Space Telescope ( HST ) and Spitzer Space Telescope to study the galaxy community in the cluster Abell 2744 at z = 0 . 308 .We see that most of the red sequence galaxies are situated on the edges of the X - ray radiation peaks , while dark cloud galaxies are found primarily inside these regions . The geographic distribution of galaxy formation rate concentration is also consistent with this picture .These data suggest that ram pressure stripping may be responsible for quenching star formation activity in large central galaxies . In addition , we identify several flaring stars which show strong O iii λ5007 line emissions in their spectra made by HST / ACS grism .They have high SFRs ranging between 100 - 400 [UNK] yr−1 , but they do not appear as AGNs based on their optical colors or mid - infrared properties . Their large velocity dispersions show that they may be experiencing mergers .",
        "rewrite_text": "In this study, we present new observations obtained from the Hubble Space Telescope (HST) and the Spitzer Space Telescope, focusing on the galaxy population within the Abell 2744 cluster at a redshift of z = 0.308. Our analysis reveals a distinct spatial distribution of galaxies, where the majority of red sequence galaxies are located at the peripheries of the X-ray emission peaks, while galaxies characterized by dark clouds predominantly reside within these high X-ray regions. This spatial arrangement aligns with the concentration of galaxy formation rates, indicating a potential relationship between environmental factors and star formation activity. The data imply that ram pressure stripping could play a significant role in suppressing star formation in the larger central galaxies of the cluster. Furthermore, we have identified several flaring stars exhibiting pronounced O III λ5007 line emissions in their spectra, as captured by the HST/ACS grism. These stars demonstrate substantial star formation rates (SFRs) ranging from 100 to 400 solar masses per year, yet they do not exhibit characteristics typical of active galactic nuclei (AGNs) based on their optical colors and mid-infrared properties. Additionally, their high velocity dispersions suggest that these stars may be undergoing merger events. Overall, our findings contribute to a deeper understanding of the complex interactions and evolutionary processes occurring within the galaxy community of Abell 2744, highlighting the influence of environmental conditions on star formation and galaxy dynamics.",
        "ori-fast-z-score": -1.2701705922171767,
        "water-fast-z-score": 4.358898943540673,
        "rewrite-fast-z-score": -0.3713906763541037
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Electron Doping of Cuprates via Interfaces with Manganites .\nAbstract:\nWe report on the observation of electron doping in cuprate superconductors by interfacing them to manganite insulators through epitaxial growth and chemical bonding at interfaces.  The interface between La0.7Sr0.3MnO3 (LSMO) and YBa2Cu3O6+x (YBCO), which are both parent compounds for high temperature superconductivity, is found to be highly conducting despite the large lattice mismatch between LSMO and YBCO. This suggests that charge transfer across the interface occurs due to strong electronic hybridization rather than strain relaxation alone. We also find that the hole concentration in the YBCO layer can be controlled by varying the thickness of the LSMO layer grown on top of it. These results demonstrate an alternative approach towards engineering the carrier density in cuprate superconductors using oxide heterostructures. High-temperature superconductivity has been observed only in materials containing copper-oxygen planes known as CuO2 layers  1  . In these systems, holes doped into the CuO2 plane give rise to Cooper pairs leading to superfluidity  2  . However, the maximum critical temperature Tc = 92 K achieved so far in this class of materials is still well below the theoretical limit predicted by Bardeen-Cooper-Schrieffer theory  3  , raising questions about how to further enhance Tc  4  .\nIn recent years there have been significant efforts made to explore new routes toward enhancing Tc beyond its current record value  5  . One promising route involves introducing electrons into the CuO2 plane  6  . For example, replacing oxygen atoms in the CuO2 plane with fluorine leads to a reduction in the number of holes in the system  7, 8  . Alternatively, one may introduce electrons directly into the CuO2 plane by growing thin films of transition metal oxides such as SrTiO3  9  or LaAlO3  10  onto the surface of cuprate superconductors. While these approaches show promise, they require precise control over film composition and structure during deposition  11  . An alternative strategy would involve controlling the carrier density in cuprates without changing their crystal structures  12  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Electron Doping of Cuprates via Interfaces with Manganites . Abstract : We report on the observation of electron doping in cuprate superconductors by interfacing them to manganite insulators through epitaxial growth and chemical bonding at interfaces .The interface between La0 . 7Sr0 . 3MnO3 ( LSMO ) and YBa2Cu3O6 + x ( YBCO ) , which are both parent molecules for high heat superconductivity , is found to be highly conducting despite the huge lattice mismatch between LSMO and YBCO . This implies that charge transfer across the interface comes due to powerful electronic hybridization instead than strain relaxation alone .We additionally find that the gap content in the YBCO layer can be governed by varying the density of the LSMO layer grown on top of it . These data demonstrate an additional method towards engineering the carrier density in cuprate superconductors using oxide heterostructures .High - temperature superconductivity has been observed only in structures containing copper - oxygen planes named as CuO2 layers 1 . In these systems , holes doped into the CuO2 plane give rise to Cooper pairs leading to superfluidity 2 .However , the maximum essential temperature Tc = 92 K attained so far in this class of substances is already much below the theoretical maximum expected by Bardeen - Cooper - Schrieffer principle 3 , placing questions about how to further enhance Tc 4 . In recent history there have been significant efforts made to pursue new routes toward enhancing Tc beyond its current record value 5 .One promising route includes introducing electrons into the CuO2 plane 6 . For instance , replacing oxygen atoms in the CuO2 plane with fluorine leads to a reduction in the number of holes in the scheme 7 , 8 .Alternatively , one may introduce particles specifically into the CuO2 plane by spreading thin films of transition metal oxides such as SrTiO3 9 or LaAlO3 10 onto the surface of cuprate superconductors . While these method demonstrate promise , they demand careful power over movie structure and shape during deposition 11 .An alternative scheme would include regulating the carrier density in cuprates without altering their crystal structures 12 .",
        "rewrite_text": "**Title:** Electron Doping of Cuprates via Interfaces with Manganites\n\n**Abstract:** In this study, we present findings on the electron doping of cuprate superconductors achieved through the interfacing of these materials with manganite insulators via epitaxial growth and chemical bonding at the interfaces. Specifically, we investigate the interface formed between La0.7Sr0.3MnO3 (LSMO) and YBa2Cu3O6+x (YBCO), both of which serve as fundamental components for high-temperature superconductivity. Remarkably, despite the significant lattice mismatch between LSMO and YBCO, the interface exhibits high conductivity. This observation suggests that the charge transfer occurring at the interface is primarily driven by strong electronic hybridization rather than solely by strain relaxation effects. Furthermore, our results indicate that the superconducting gap in the YBCO layer can be modulated by adjusting the density of the LSMO layer deposited on top. This discovery opens up new avenues for engineering carrier density in cuprate superconductors through the use of oxide heterostructures.\n\nHigh-temperature superconductivity is predominantly associated with materials containing copper-oxygen planes, known as CuO2 layers. In these systems, the introduction of holes into the CuO2 plane facilitates the formation of Cooper pairs, which are essential for superfluidity. However, the highest critical temperature (Tc) recorded in these materials, which is 92 K, remains significantly lower than the theoretical maximum predicted by the Bardeen-Cooper-Schrieffer (BCS) theory. This discrepancy raises important questions regarding potential strategies to further increase Tc. Recent research has focused on innovative approaches to enhance Tc beyond its current limits, with one promising strategy being the introduction of electrons into the CuO2 plane. For example, substituting oxygen atoms in the CuO2 plane with fluorine reduces the hole concentration. Alternatively, the deposition of thin films of transition metal oxides, such as SrTiO3 or LaAlO3, onto cuprate superconductors has shown potential for electron introduction. While these methods are promising, they require precise control over the deposition process. Our findings suggest that it may be possible to regulate carrier density in cuprates without altering their crystal structures, providing a novel pathway for enhancing superconducting properties.",
        "ori-fast-z-score": 0.08247860988423225,
        "water-fast-z-score": 7.021870595978444,
        "rewrite-fast-z-score": 0.6713450866373513
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  GLSM s for partial flag manifolds .\nAbstract:\nWe study the geometry and topology of generalized Lagrangian submanifolds (GLSMs) in complex symplectic manifolds, focusing on GLSMs that are special Lagrangians with respect to some Kähler form. We show how these can be constructed as holomorphic sections of certain line bundles over moduli spaces of parabolic Higgs bundles. In particular we consider the case where the base is a partial flag variety. This leads us to define new families of Calabi-Yau varieties which have been studied by physicists recently. These varieties are obtained by taking products of Grassmannian manifolds or their quotients by finite groups. The main results of this thesis are:  1. A construction of GLSMs using parabolic Higgs bundles.  2. An explicit description of the cohomology ring of the total space of a vector bundle associated to a parabolic Higgs bundle.  3. A proof of mirror symmetry between two different types of GLSMs defined above when the base is a product of Grassmannians.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : GLSM s for partial flag manifolds . Abstract : We research the topology and topology of generalized Lagrangian submanifolds ( GLSMs ) in complex symplectic manifolds , concentrating on GLSMs that are special Lagrangians with regard to some Kähler form .We see how these can be formed as holomorphic sections of certain line bundles over moduli spaces of parabolic Higgs bundles . In particular we consider the case where the base is a partial flag variety .This leads us to define novel families of Calabi - Yau varieties which have been studied by physicists recently . These varieties are derived by take products of Grassmannian manifolds or their quotients by finite groups .The main results of this dissertation are : 1 . A design of GLSMs involving parabolic Higgs bundles .2 . An intimate description of the cohomology ring of the total space of a vector bundle related to a parabolic Higgs bundle .3 . A proof of mirror symmetry between two different kinds of GLSMs developed above when the base is a product of Grassmannians .",
        "rewrite_text": "Title: GLSMs for Partial Flag Manifolds\n\nAbstract: This article investigates the topology of generalized Lagrangian submanifolds (GLSMs) within the context of complex symplectic manifolds, with a particular focus on those GLSMs that exhibit special Lagrangian properties in relation to specific Kähler forms. We explore the construction of these submanifolds as holomorphic sections of particular line bundles over moduli spaces of parabolic Higgs bundles. Our study emphasizes the scenario where the underlying base is a partial flag variety, which serves as a crucial framework for our analysis. This exploration leads to the introduction of new families of Calabi-Yau varieties that have garnered attention in recent physics literature. These varieties are constructed through products of Grassmannian manifolds or their quotients by finite groups, revealing rich geometric structures. The principal contributions of this research include: (1) the formulation of GLSMs associated with parabolic Higgs bundles, providing a novel perspective on their geometric properties; (2) a detailed examination of the cohomology ring of the total space of a vector bundle linked to a parabolic Higgs bundle, which enhances our understanding of the underlying topological features; and (3) a demonstration of mirror symmetry between two distinct classes of GLSMs, particularly when the base is a product of Grassmannians. This work not only advances the theoretical framework surrounding GLSMs but also bridges connections to contemporary studies in mathematical physics, thereby enriching the discourse on the interplay between geometry and theoretical physics.",
        "ori-fast-z-score": -0.24618298195866545,
        "water-fast-z-score": 3.7872527750454617,
        "rewrite-fast-z-score": 1.1055415967851332
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Gravity-like potential traps light and stretches supercontinuum in photonic crystal fibers .\nAbstract:\nWe report on the generation of an unprecedentedly broad spectrum (>10 THz) by using a novel type of fiber, which is based on a microstructured optical fiber with a suspended core surrounded by air holes filled with liquid. The suspended-core fiber supports two guided modes that are confined to different regions along its length. One mode propagates through the central region of the fiber where it experiences strong confinement due to the high index contrast between silica glass and air; this results in large effective nonlinearity. The other mode propagates outside the central region of the suspended core, where it sees only weak refractive-index guiding. This leads to low loss for both modes over a wide wavelength range. We demonstrate experimentally that these two modes can be coupled together efficiently via stimulated Raman scattering when pumped at 1.55 µm. As a result, we obtain a broadband continuum spanning more than 10 THz centered around 2.7 µm.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Gravity - like potential captures light and bends supercontinuum in photonic crystal fibers . Abstract : We report on the generation of an unprecedentedly broad spectrum ( > 10 THz ) by using a novel type of fiber , which is based on a microstructured optical cable with a suspended core covered by air gaps filled with liquid .The suspended - core cable supports two guided modes that are confined to different regions along its duration . One mode propagates through the main region of the fiber where it experiences strong confinement owing to the high index distinction between silica glass and air ; this results in large effective nonlinearity .The other mode propagates outside the central region of the suspended core , where it presents only weak refractive - index guiding . This leads to low loss for both modes over a broad wavelength range .We suggest experimentally that these two modes can be correlated together easily via stimulated Raman scattering when pumped at 1 . 55 µm . As a result , we obtain a broadband continuum spanning more than 10 THz centered around 2 . 7 µm .",
        "rewrite_text": "We present a groundbreaking study on the generation of an exceptionally broad optical spectrum exceeding 10 THz, achieved through the use of an innovative type of fiber. This fiber is characterized by a microstructured optical design featuring a suspended core, which is uniquely surrounded by air gaps filled with liquid. The suspended-core configuration supports two distinct guided modes that are confined to different regions along the fiber's length. One of these modes propagates through the primary region of the fiber, where it benefits from strong confinement due to the significant refractive index contrast between silica glass and air. This strong confinement results in a substantial effective nonlinearity, enhancing the fiber's performance. Conversely, the second mode propagates outside the central region of the suspended core, where it experiences only weak refractive index guiding. This configuration contributes to low loss for both modes across a wide range of wavelengths. Our experimental findings indicate that these two modes can be effectively correlated through stimulated Raman scattering when the fiber is pumped at a wavelength of 1.55 µm. This interaction leads to the generation of a broadband continuum centered around 2.7 µm, spanning more than 10 THz. The implications of this research are significant, as it opens new avenues for the development of advanced photonic devices and applications in areas such as telecommunications, sensing, and spectroscopy. The ability to generate such a broad spectrum with low loss and high nonlinearity presents exciting opportunities for future innovations in fiber optics and photonic technologies.",
        "ori-fast-z-score": 0.5423261445466404,
        "water-fast-z-score": 4.313310928137536,
        "rewrite-fast-z-score": 0.3651483716701107
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Parallel tracks in infrared versus X-ray emission in black hole X-ray transient outbursts: a hysteresis effect? .\nAbstract:\nWe report on the first simultaneous observations with XMM-Newton and Spitzer Space Telescope (SST) during an active state of the Galactic black hole candidate GRS 1915+105, which is known to show strong variability at all wavelengths. We find that there are two parallel tracks in the evolution of the source s luminosity between the near-infrared (NIR), optical/UV, soft/hard X-ray bands as well as radio wavebands. The NIR-optical track follows closely the hard X-ray light curve but lags behind by about 20 days; while the UV-X-ray track shows no lag compared to the hard X-ray light curve. This suggests that different physical processes may be responsible for these two tracks. In addition, we also found evidence for a hysteresis effect in both tracks. These results provide new insights into the physics of accretion flows onto supermassive black holes.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Parallel lines in infrared vs X - ray radiation in black hole X - ray transient outbursts : a hysteresis effect ? .Abstract : We report on the first simultaneous measurements with XMM - Newton and Spitzer Space Telescope ( SST ) during an active state of the Galactic red hole candidate GRS 1915 + 105 , which is known to indicate strong variability at all wavelengths . We see that there are two simultaneous tracks in the evolution of the target s luminosity between the near - infrared ( NIR ) , optical / UV , soft / soft X - ray bands as well as radio wavebands .The NIR - optical track follows carefully the hard X - ray light curve but lags behind by about 20 days ; while the UV - X - ray band sees no lag compared to the hard X - ray light curve . This implies that different physical processes possibly be responsible for these two lines .In addition , we also discovered evidence for a hysteresis effect in both tracks . These data provide fresh insights into the physics of accretion flows onto supermassive black holes .",
        "rewrite_text": "We present the first simultaneous observations of the Galactic black hole candidate GRS 1915 + 105, utilizing both the XMM-Newton and Spitzer Space Telescope (SST) during a period of heightened activity. This source is renowned for its significant variability across a wide range of wavelengths. Our findings reveal two distinct tracks in the evolution of the source's luminosity, which span the near-infrared (NIR), optical/UV, soft X-ray, and radio wavebands. Notably, the NIR-optical track closely mirrors the hard X-ray light curve, albeit with a delay of approximately 20 days. In contrast, the UV and X-ray bands exhibit no such lag, suggesting that different physical mechanisms may be at play in these two tracks. Furthermore, we have identified evidence of a hysteresis effect in both luminosity tracks, indicating a complex interplay between the various emission processes. These observations contribute valuable insights into the dynamics of accretion flows around supermassive black holes, enhancing our understanding of the underlying physics governing these phenomena. The implications of these results may extend to broader astrophysical contexts, shedding light on the behavior of black hole systems during transient outbursts.",
        "ori-fast-z-score": -0.22360679774997896,
        "water-fast-z-score": 5.222222222222222,
        "rewrite-fast-z-score": 0.7504787743864564
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The high energy emission of GRO J1655-40 as revealed with INTEGRAL spectroscopy of the 2005 outburst .\nAbstract:\nWe report on results obtained by INTEGRAL observations during the 2005 outburst of the black hole candidate GRO J1655â€“40 (Nova Muscae 1991). The source was observed in the 20-100 keV range for about 100 days, starting at MJD 53000 and ending at MJD 53300. We have analyzed these data using both ISGRI and SPI instruments aboard INTEGRAL satellite. In addition to the main spectral component which is well described by a power law model modified by an exponential cutoff, we find that there are two additional components present in the spectrum. One of them has been previously reported by other authors but its origin remains unclear. Another one appears only when fitting the whole dataset simultaneously with all three models considered here -power law plus exponential cut-off, broken power law or Comptonization model-. This new feature can be interpreted either as a reflection hump produced by cold material surrounding the central X-ray source or as a broad iron line around 6.4 keV.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The high energy emitted of GRO J1655 - 40 as revealed with INTEGRAL spectroscopy of the 2005 outburst . Abstract : We report on findings obtained by INTEGRAL observations during the 2005 outburst of the dark hole candidate GRO J1655â€ “ 40 ( Nova Muscae 1991 ) .The source was seen in the 20 - 100 keV range for about 100 days , beginning at MJD 53000 and ending at MJD 53300 . We have analyzed these information using both ISGRI and SPI instruments aboard INTEGRAL satellite .In addition to the main spectral component which is well described by a power law theory amended by an exponential cutoff , we find that there are two additional components present in the spectrum . One of them has been previously reported by other researchers but its identity remains obscure .Another one appears only when fitting the whole dataset jointly with all three models described here - energy law plus exponential cutting - off , broken power law or Comptonization model - . This new feature can be interpreted either as a mirror hump produced by cold metal covering the main X - ray source or as a broad iron line around 6 . 4 keV .",
        "rewrite_text": "We present our findings from the INTEGRAL satellite observations of the black hole candidate GRO J1655-40 (Nova Muscae 1991) during its 2005 outburst. The observations covered the energy range of 20-100 keV over a period of approximately 100 days, starting from MJD 53000 and concluding at MJD 53300. Utilizing data from both the ISGRI and SPI instruments onboard INTEGRAL, we conducted a comprehensive spectral analysis. The primary spectral component is effectively modeled by a power law with an exponential cutoff. However, our analysis reveals the presence of two additional spectral components. One of these components has been noted in previous studies, yet its exact nature remains uncertain. The second component emerges only when we fit the entire dataset using a combination of three models: the power law with an exponential cutoff, a broken power law, or a Comptonization model. This newly identified feature can be interpreted in two ways: it may represent a mirror hump resulting from cold metal obscuring the primary X-ray source, or it could manifest as a broad iron line centered around 6.4 keV. Our findings contribute to the understanding of the high-energy emissions from GRO J1655-40 and provide insights into the complex mechanisms at play during its outburst. This study underscores the importance of multi-component spectral modeling in accurately characterizing the emissions from black hole candidates and highlights the potential for further investigation into the nature of these additional spectral features.",
        "ori-fast-z-score": 1.0125791108334214,
        "water-fast-z-score": 5.813776741499453,
        "rewrite-fast-z-score": 1.7483145522430754
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Constraining Warm Dark Matter using QSO gravitational lensing .\nAbstract:\nWe present new constraints on warm dark matter (WDM) models by combining the results of two recent surveys for gravitationally lensed quasars, SDSS and CFHTLS Wide. We find that the observed number density of lenses is consistent with predictions based on cold dark matter simulations but inconsistent at more than 3 sigma confidence level if we assume a standard thermal relic WDM model with mass mX = 1 keV. This result suggests either that the current WDM scenario needs to be modified or that there are other systematic effects which have not been taken into account in our analysis. The full text can be found at: http://arxiv.org/abs/astro-ph/0604070v1.pdf . \nThe existence of dark matter has now been established beyond reasonable doubt through its gravitational influence on visible matter. However, despite decades of research, little else about this mysterious substance is known. In particular, it remains unclear whether dark matter consists of one particle species only - as assumed in most theoretical studies -or whether it comprises several different particles. One possibility is that dark matter consists of weakly interacting massive particles (WIMPs), such as neutralinos predicted within supersymmetric extensions of the Standard Model  1  .\nIn order to test these scenarios observationally, astronomers look for signatures of dark matter in astrophysical objects like galaxies  2  , clusters  3  and quasars  4  . A particularly promising method involves searching for gravitationally lensed systems  5  where light rays emitted by distant sources bend around intervening dark matter halos  6  . If dark matter consists of WIMPs then their masses should lie between 10 GeV/c 2 and 100 TeV/c 2  7, 8  . For example, the recently discovered galaxy cluster Abell 2218  9  may contain a halo made up entirely of WIMPs  10  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Constraining Warm Dark Matter using QSO gravity lensing . Abstract : We create additional constraints on dark dark matter ( WDM ) estimates by combining the results of two latest surveys for gravitationally lensed quasars , SDSS and CFHTLS Wide .We see that the reported number density of lenses is compatible with predictions based on cold bright matter simulations but inconsistent at more than 3 sigma confidence rate if we suppose a typical thermal relic WDM description with mass mX = 1 keV . This result suggests either that the present WDM situation needs to be altered or that there are other systematic effects which have not been took into consideration in our analysis .The full text can be found at : www : / / arxiv . org / abs / astro - ph / 0604070v1 . pdf . The existence of dark matter has now been recognized beyond reasonable question through its gravitational impact on visible matter .However , despite decades of research , nothing else about this mysterious substance is known . In particular , it remains unsure whether dark matter contains of one particle species only - as implied in most theoretical researchers - or whether it contains multiple distinct objects .One possibility is that dark matter contains of weakly interacting massive particles ( WIMPs ) , such as neutralinos expected within supersymmetric extensions of the Standard Model 1 . In order to test these scenarios observationally , astronomers look for signatures of bright matter in astrophysical objects like galaxies 2 , galaxies 3 and quasars 4 .A particularly useful technique requires looking for gravitationally lensed systems 5 where light rays generated by distant sources bend around intervening black material halos 6 . If bright matter contains of WIMPs then their masses should lie between 10 GeV / c 2 and 100 TeV / c 2 7 , 8 .For instance , the recently discovered star cluster Abell 2218 9 would contain a halo made up completely of WIMPs 10 .",
        "rewrite_text": "**Title:** Constraining Warm Dark Matter Using QSO Gravitational Lensing\n\n**Abstract:** In this study, we enhance the constraints on warm dark matter (WDM) by integrating findings from two recent surveys of gravitationally lensed quasars: the Sloan Digital Sky Survey (SDSS) and the Canada-France-Hawaii Telescope Legacy Survey (CFHTLS) Wide. Our analysis reveals that the observed number density of gravitational lenses aligns with predictions derived from cold dark matter simulations. However, it shows a significant discrepancy—exceeding 3 sigma confidence—when we apply a typical thermal relic WDM model with a mass of mX = 1 keV. This discrepancy indicates that either the current understanding of WDM needs to be revised or that there are unaccounted systematic effects influencing our results. \n\nThe existence of dark matter is now widely accepted due to its gravitational effects on visible matter, yet its fundamental nature remains elusive despite extensive research over the past few decades. A key question is whether dark matter consists of a single particle species, as suggested by many theoretical models, or if it comprises multiple distinct entities. One leading hypothesis posits that dark matter is made up of weakly interacting massive particles (WIMPs), such as neutralinos, which are anticipated in supersymmetric extensions of the Standard Model. To investigate these scenarios, astronomers seek observable signatures of dark matter in various astrophysical contexts, including galaxies and quasars. A particularly effective method involves studying gravitationally lensed systems, where light from distant sources is bent by intervening dark matter halos. If dark matter is indeed composed of WIMPs, their masses are expected to range between 10 GeV/c² and 100 TeV/c². For example, the recently identified star cluster Abell 2218 is hypothesized to contain a halo entirely composed of WIMPs. This research contributes to the ongoing efforts to better understand the nature of dark matter and its implications for cosmology. \n\nFor further details, the full text is available at: www.arxiv.org/abs/astro-ph/0604070v1.pdf.",
        "ori-fast-z-score": -0.25630729731502827,
        "water-fast-z-score": 8.057794831959724,
        "rewrite-fast-z-score": 1.191759143062248
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The infrared environments of masers associated with star formation .\nAbstract:\nWe have studied the infrared (IR) properties of water and methanol maser sources in nearby galaxies using data obtained by the Infrared Space Observatory (ISO). We find that most of these masers are located near luminous IR objects, such as ultraluminous infrared galaxies or active galactic nuclei. The majority of the masers appear to be excited by shocks produced by outflows driven by massive stars. However, we also found some masers which may be excited by accretion onto young stellar objects. These results suggest that both massive star formation and low-mass star formation can produce masing gas clouds. Masers are powerful tools for studying physical conditions in interstellar media because they provide information on molecular abundances and kinematics at high spatial resolution. Water and methanol masers are commonly observed toward star-forming regions in our Galaxy and other nearby galaxies. They are thought to trace dense molecular gas where protostars form. Since their discovery more than 30 years ago, many studies have been carried out to investigate the relationship between masers and various phenomena related to star formation processes. \n \n In this study, we investigated the infrared (IR) environment around masers detected in nearby galaxies using ISO observations. Our sample consists of all known extragalactic water and methanol masers listed in the catalogs compiled by Caswell & Haynes(1987), Hoffman et al.(1989), and Pestalozzi et al. (2005) . Most of them were discovered serendipitously during surveys conducted with single-dish radio telescopes. Although there is no complete census of masers in external galaxies yet, it has been estimated that about 10 percent of local ULIRGs show maser emission (e.g., Gao 1996; Braatz et al. 1997 ). This suggests that masers play an important role in understanding the nature of ULIRGs.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The infrared environments of masers associated with star formation . Abstract : We have researched the infrared ( IR ) characteristics of water and methanol maser sources in nearby galaxies using data acquired by the Infrared Space Observatory ( ISO ) .We see that most of these masers are situated near luminous IR objects , such as ultraluminous laser clusters or active galactic nuclei . The majority of the masers occur to be excited by shocks created by outflows driven by massive galaxies .However , we also discovered some masers which may be excited by accretion onto young stellar bodies . These data suggest that both large star formation and low - mass star formation can generate masing gas clouds .Masers are powerful tools for studying physical conditions in interstellar media because they give information on chemical abundances and kinematics at high visual resolution . Water and methanol masers are often observed toward star - creating areas in our Galaxy and other nearby galaxies .They are said to trace dense molecular gas where protostars form . Since their discovery more than 30 weeks ago , various surveys have been carried out to examine the relationship between masers and many phenomena related to star formation changes .In this study , we investigated the infrared ( IR ) conditions around masers detected in nearby galaxies using ISO observations . Our specimen consists of all known extragalactic water and methanol masers mentioned in the catalogs collected by Caswell & Haynes ( 1987 ) , Hoffman et al .( 1989 ) , and Pestalozzi et al . ( 2005 ) .Most of them were discovered serendipitously during surveys performed with single - dish radio telescopes . Although there is no complete census of masers in external galaxies yet , it has been estimated that about 10 percent of local ULIRGs exhibit maser emission ( e . g . , Gao 1996 ; Braatz et al .1997 ) . This shows that masers serve an important role in understanding the nature of ULIRGs .",
        "rewrite_text": "**Title:** The Infrared Environments of Maser Sources Associated with Star Formation\n\n**Abstract:** This study investigates the infrared (IR) characteristics of water and methanol maser sources located in nearby galaxies, utilizing data obtained from the Infrared Space Observatory (ISO). Our findings reveal that a significant number of these masers are found in proximity to luminous IR sources, including ultraluminous infrared galaxies (ULIRGs) and active galactic nuclei (AGN). The analysis indicates that the majority of these masers are likely excited by shock waves generated from outflows associated with massive galaxies. Interestingly, we also identified a subset of masers that may be energized by accretion processes occurring around young stellar objects. This dual excitation mechanism implies that both high-mass and low-mass star formation events can lead to the formation of masing gas clouds.\n\nMasers are invaluable for probing the physical conditions within interstellar media, as they provide insights into chemical abundances and kinematic properties with high spatial resolution. Water and methanol masers are frequently detected in regions of star formation within our Galaxy and other nearby galaxies, serving as indicators of dense molecular gas where protostellar activity is taking place. Since their initial discovery over three decades ago, numerous surveys have been conducted to explore the connections between masers and various phenomena associated with changes in star formation.\n\nIn our research, we focused on the IR environments surrounding masers identified in nearby galaxies, drawing on ISO observations. Our sample encompasses all known extragalactic water and methanol masers cataloged by Caswell & Haynes (1987), Hoffman et al. (1989), and Pestalozzi et al. (2005). Most of these masers were serendipitously discovered during surveys conducted with single-dish radio telescopes. While a comprehensive census of masers in external galaxies remains elusive, estimates suggest that approximately 10% of local ULIRGs exhibit maser emissions (e.g., Gao 1996; Braatz et al. 1997). This underscores the critical role that masers play in enhancing our understanding of the nature and processes occurring within ULIRGs.",
        "ori-fast-z-score": 0.939793423488437,
        "water-fast-z-score": 7.548881474151742,
        "rewrite-fast-z-score": 1.5105264449340403
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Reexamination of spin decoherence in semiconductor quantum dots from equation-of-motion approach .\nAbstract:\nWe have studied the spin relaxation and dephasing processes in semiconductor quantum dots (QDs) by solving numerically the full set of equations for electron-electron interactions within the framework of the equation-of-motion method. We found that, at low temperatures, the dominant mechanism responsible for spin relaxation is due to spin-flip scattering with acoustic phonons. The calculated results are compared favorably with available experimental data on QD ensembles. In addition, we show that the inclusion of exchange interaction between electrons leads to an increase in the spin relaxation time as well as to a reduction in its temperature dependence. \n \n Spin dynamics plays an important role in many physical phenomena such as magnetic resonance imaging  1  , magneto-optical effects  2  , and spintronics  3  . Semiconductor quantum dots (QDs), which can be viewed as artificial atoms  4  , provide us with unique opportunities to study spin relaxation and dephazing mechanisms  5  -  8  . Recently, there has been considerable interest in studying these issues both experimentally  9  -  11  and theoretically  12  -  16  .\nIn this work, we investigate spin relaxation and dephazation processes in QDs using the equation-of-motion (EOM) method  17  . This method allows one to take into account all possible contributions to the self-energy arising from different types of electron-electron interactions including direct Coulomb repulsion, exchange-correlation potential, Hartree-Fock corrections, and correlation energy  18  . It should be noted that our calculations were performed without any additional approximations beyond those used in previous studies based on the EOM formalism  19  -  21  . \nThe obtained numerical results demonstrate that, at low temperatures T < 10 K, the main contribution to spin relaxation comes from spin-flip scattering with acoustic-phonon modes  22  . At higher temperatures, however, other mechanisms become more significant leading to faster spin relaxation times. Our theoretical predictions agree reasonably well with existing experimental data on QD ensembles  23  . \n \n Finally, it was shown that the inclusion of exchange interactions between electrons leads to an enhancement of the spin relaxation rate as well as to a decrease in its temperature dependence  24  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Reexamination of spin decoherence in semiconductor quantum dots from equation - of - movement technique . Abstract : We have researched the spin relaxation and dephasing mechanisms in semiconductor quantum dots ( QDs ) by solving numerically the full set of equations for electron - ion interactions within the framework of the equation - of - movement technique .We showed that , at low temperatures , the dominant mechanism causing for momentum relaxation is due to spinning - flip scattering with sound phonons . The measured data are compared favorably with provided experimental evidence on QD ensembles .In addition , we find that the introduction of exchange interaction between electrons contributes to an increase in the spin relaxation time as well as to a reduction in its temperature dependence . Spin dynamics plays an important role in many physical phenomena such as magnetic resonance imaging 1 , magneto - optical phenomena 2 , and spintronics 3 .Semiconductor quantum dots ( QDs ) , which can be viewed as synthetic elements 4 , provide us with special opportunities to study spinning contraction and dephazing processes 5 - 8 . Recently , there has been substantial interest in examining these problems both experimentally 9 - 11 and theoretically 12 - 16 .In this research , we investigate momentum relaxation and dephazation processes in QDs using the equation - of - movement ( EOM ) method 17 . This method enables one to take into consideration all possible contributions to the self - energy originating from multiple types of electron - ion interactions using direct Coulomb repulsion , transfer - correlation potential , Hartree - Fock corrections , and correlation power 18 .It should be mentioned that our calculations were performed without any additional approximations beyond those utilized in earlier analyses based on the EOM formalism 19 - 21 . The achieved numerical findings show that , at low temperatures T < 10 K , the main contribution to spinning contraction comes from spinning - flip diffusion with sound - phonon frequencies 22 .At higher temperatures , however , other mechanisms become more prominent leading to faster spin relaxation times . Our theory estimates agree reasonably well with existing experimental evidence on QD ensembles 23 .Finally , it was shown that the introduction of exchange interactions between electrons contributes to an enhancement of the spin relaxation time as well as to a reduction in its temperature dependence 24 .",
        "rewrite_text": "**Title:** Reexamination of Spin Decoherence in Semiconductor Quantum Dots Using the Equation-of-Movement Technique\n\n**Abstract:** In this study, we investigate the mechanisms of spin relaxation and dephasing in semiconductor quantum dots (QDs) by numerically solving the complete set of equations governing electron-ion interactions through the equation-of-movement (EOM) technique. Our findings indicate that at low temperatures, specifically below 10 K, the primary mechanism responsible for momentum relaxation is spin-flip scattering with acoustic phonons. We compare our numerical results with experimental data from QD ensembles, demonstrating a favorable correlation. Furthermore, we discover that incorporating exchange interactions between electrons significantly enhances the spin relaxation time and diminishes its temperature dependence. The dynamics of spin are crucial in various physical phenomena, including magnetic resonance imaging, magneto-optical effects, and spintronics. Semiconductor quantum dots, often regarded as artificial atoms, offer unique opportunities to explore spin relaxation and dephasing processes. Recent years have seen a surge of interest in these phenomena, both experimentally and theoretically. Our research employs the EOM method, which allows for a comprehensive consideration of all contributions to the self-energy arising from different types of electron-ion interactions, including direct Coulomb repulsion, transfer-correlation potential, Hartree-Fock corrections, and correlation effects. Notably, our calculations do not rely on any additional approximations beyond those used in previous EOM analyses. The results reveal that at low temperatures, spin-flip diffusion driven by sound-phonon frequencies is the dominant contributor to spin relaxation. As the temperature increases, alternative mechanisms emerge, resulting in shorter spin relaxation times. Our theoretical predictions align well with existing experimental observations of QD ensembles. Ultimately, our study highlights the significant role of exchange interactions in enhancing spin relaxation times and reducing their dependence on temperature, contributing valuable insights into the spin dynamics of semiconductor quantum dots.",
        "ori-fast-z-score": -0.07432941462471664,
        "water-fast-z-score": 9.487991253262095,
        "rewrite-fast-z-score": 0.7092993656151906
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Cassini UVIS Observations of the Io Plasma Torus. IV. Modeling Temporal and Azimuthal Variability .\nAbstract:\nWe present new results on temporal variability in the ultraviolet (UV) emission observed by Cassini s Ultraviolet Imaging Spectrograph (UVIS). The data were obtained during orbits that crossed the equatorial plane at distances between 1,000 km and 5,500 km above Jupiter s surface. We find that the brightness distribution is highly variable both temporally and azimuthally with respect to Jupiter s magnetic field direction. This behavior can be explained if we assume that the plasma torus consists of multiple co-rotating structures which are not uniformly distributed around the planet but instead form an arc-like structure centered near the magnetic equator. These observations provide further evidence for the existence of such arcs as predicted by theoretical models. They also suggest that these arcs may have significant effects on the global magnetosphere-ionosphere coupling processes. \n \n Keywords: Jupiter, Saturn, Magnetospheres, Plasmas",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  Cassini UVIS Observations of the Io Plasma Torus.IV.Modeling Temporal and Azimuthal Variability . Abstract : We present new results on temporal variability in the ultraviolet ( UV ) emission observed by Cassini s Ultraviolet Imaging Spectrograph ( UVIS ) .The data were obtained during orbits that crossed the equatorial plane at distances between 1 , 000 km and 5 , 500 km above Jupiter s surface . We see that the brightness distribution is heavily variable both temporally and azimuthally with regard to Jupiter s magnetic force direction .This phenomenon can be described if we suppose that the plasma torus consists of multiple co - spinning structures which are not uniformly scattered around the planet but instead form an arc - like structure focused near the magnetic equator . These measurements give further evidence for the existence of such arcs as predicted by theoretical theories .They also suggest that these arcs might have considerable effects on the global magnetosphere - ionosphere bonding mechanisms . Keywords : Jupiter , Saturn , Magnetospheres , Plasmas",
        "rewrite_text": "Title: Cassini UVIS Observations of the Io Plasma Torus: IV. Modeling Temporal and Azimuthal Variability\n\nAbstract: In this study, we present significant findings regarding the temporal variability of ultraviolet (UV) emissions captured by the Ultraviolet Imaging Spectrograph (UVIS) aboard the Cassini spacecraft. The data were collected during Cassini's orbits that traversed the equatorial plane at altitudes ranging from 1,000 km to 5,500 km above the surface of Jupiter. Our analysis reveals that the brightness distribution of the UV emissions exhibits substantial variability, both over time and in relation to the azimuthal direction of Jupiter's magnetic field. This observed variability can be interpreted through the hypothesis that the plasma torus surrounding Jupiter is composed of multiple co-spinning structures. Rather than being uniformly distributed, these structures appear to form an arc-like configuration that is concentrated near the magnetic equator of the planet. The implications of these measurements provide further support for the existence of such arc-like formations, aligning with predictions made by theoretical models. Additionally, our findings indicate that these arcs could have significant implications for the interactions between the global magnetosphere and the ionosphere, potentially influencing the bonding mechanisms that govern these regions. This research contributes to a deeper understanding of the complex dynamics within Jupiter's magnetosphere and the role of the Io plasma torus in shaping these processes. \n\nKeywords: Jupiter, Saturn, Magnetospheres, Plasmas",
        "ori-fast-z-score": -1.1338934190276817,
        "water-fast-z-score": 4.0,
        "rewrite-fast-z-score": 2.182178902359924
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Constraints on gamma-ray burst and supernova progenitors through circumstellar absorption lines. (II): Post-LBV Wolf-Rayet stars .\nAbstract:\nWe present the results of our analysis of high-resolution optical spectra obtained with HST/STIS for four nearby (z<0.1), X-ray selected, Type Ib/c SNe in order to study their progenitor systems. We find that all four objects show evidence for dense CSM surrounding them at distances ranging between 0.01-0.2 pc. The presence of such material is consistent with theoretical expectations for post-low-velocity-bulge (post-LBV) Wolf-Rayet star winds. In addition we detect narrow emission features which are likely due to interaction between SN ejecta and this wind. These observations provide strong constraints on the nature of the progenitor systems: they require massive WR stars as well as binary companions capable of producing significant mass loss prior to explosion. This work was supported by NASA grant NAG5-10842. We have analyzed high resolution STIS/HST data for 4 nearby (z<0.1; Xray-selected) type Ibc supernovae in an attempt to determine the properties of their progenitor systems. All four objects exhibit dense circumstellar matter (CSM; nH>1020 cm-3 ) within 0.01-0.20 parsecs of the supernova site. Such densities are expected if these explosions occur following the ejection of a low velocity  bulge  during late stages of stellar evolution. Furthermore, we observe narrow emission features which may be associated with shock-heating of the CSM by the expanding supernova remnant. Our findings suggest that these events result from the deaths of massive Wolf Rayet stars surrounded by close binaries.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Constraints on gamma - ray burst and supernova progenitors through circumstellar absorption lines . ( II ) : Post - LBV Wolf - Rayet stars .Abstract : We present the conclusion of our analysis of high - resolution optical spectra obtained with HST / STIS for four nearby ( z < 0 . 1 ) , X - ray selected , Type Ib / c SNe in order to study their progenitor structures . We see that all four bodies exhibit indication for thick CSM circling them at distances ranging between 0 . 01 - 0 . 2 pc .The presence of such material is compatible with theoretical expectations for post - low - speed - bulge ( post - LBV ) Wolf - Rayet star winds . In addition we find narrow radiation properties which are likely due to contact between SN ejecta and this wind .These measurements give strong restrictions on the nature of the progenitor structures : they use massive WR galaxies as well as binary companions capable of producing significant mass loss prior to explosion . This research was supported by NASA grant NAG5 - 10842 .We have analyzed high resolution STIS / HST results for 4 nearby ( z < 0 . 1 ; Xray - selected ) type Ibc supernovae in an trying to estimate the properties of their progenitor structures . All four bodies exhibit thick circumstellar matter ( CSM ; nH > 1020 cm - 3 ) within 0 . 01 - 0 . 20 parsecs of the supernova site .Such densities are expected if these fires occur following the ejection of a small velocity bulge during late stages of stars evolution . Furthermore , we study narrow radiation properties which may be involved with shock - heating of the CSM by the evolving supernova remnant .Our findings show that these events result from the deaths of large Wolf Rayet stars surrounded by tight binaries .",
        "rewrite_text": "We present the findings from our investigation into the progenitor structures of four nearby Type Ib/c supernovae (SNe), selected based on X-ray emissions and with redshifts less than 0.1. Utilizing high-resolution optical spectra obtained from the Hubble Space Telescope's Space Telescope Imaging Spectrograph (HST/STIS), we aimed to analyze the circumstellar environment surrounding these supernovae. Our results indicate that all four supernovae are enveloped by substantial circumstellar matter (CSM), with densities exceeding 10^20 cm^-3, located at distances ranging from 0.01 to 0.2 parsecs from the explosion site. This observation aligns with theoretical models predicting the presence of thick CSM resulting from the winds of post-low-speed-bulge (post-LBV) Wolf-Rayet stars. \n\nAdditionally, we have identified narrow emission features in the spectra, which likely arise from the interaction between the supernova ejecta and the surrounding wind. These spectral characteristics provide critical constraints on the progenitor systems, suggesting that the supernovae originate from massive Wolf-Rayet stars, potentially in binary systems that facilitate significant mass loss prior to the explosion. Our research underscores the importance of understanding the circumstellar environment in elucidating the evolutionary pathways leading to supernova events. This work was supported by NASA grant NAG5-10842 and contributes to the broader understanding of the mechanisms behind gamma-ray bursts and supernova progenitors. Overall, our findings highlight the role of massive stars and their complex interactions with their surroundings in the final stages of stellar evolution.",
        "ori-fast-z-score": -0.08944271909999159,
        "water-fast-z-score": 6.948792289723034,
        "rewrite-fast-z-score": -0.7035264706814485
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Variable Star One-shot Project, and its little child: Wikimbad .\nAbstract:\nThe Variable Star One-Shot project is an open-source software package for the analysis of astronomical data.  It was developed by members of the Harvard-Smithsonian Center for Astrophysics (CfA) in collaboration with researchers at other institutions around the world.   The goal of this project is to provide a single tool that can be used to analyze all types of astronomical data sets, including photometric time series, spectroscopic observations, images, etc., using state-of-the-art techniques such as image subtraction, cross correlation, period finding algorithms, spectral line fitting, etc.    This software has been released under the GNU General Public License v3.0 and is available on GitHub at: https://github.com/VariableStar/one-shot-astro .\nOne Shot Astro includes several tools which are useful for analyzing large amounts of astronomical data quickly and efficiently.   These include one-shot-datacleaner , which performs automated quality control checks on raw data files;   one-shot-mosaic , which creates mosaicked images from multiple dithered exposures;  one-shot-astrometry , which determines astrometric solutions for individual frames or entire mosaic images;    one-shot-photometry , which measures fluxes and/or magnitudes for stars across a field-of-view;    one-shot-pipeline , which automates the process of running these various tasks together into a pipeline;    one-shot-wikimapia , which allows users to create custom sky maps based on their own catalogs of objects.     In addition to these core packages, there are also many additional modules available through the one-shot-astro repository which allow users to perform more specialized analyses, such as:    one-shot-catalog , which provides access to a variety of different astrophysical databases via SQL queries;    one-shot-fastphot , which uses machine learning methods to measure stellar",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Variable Star One - shot Project , and its tiny child : Wikimbad . Abstract : The Variable Star One - Shot project is an open - source software tool for the analysis of astronomical data .It was developed by participants of the Harvard - Smithsonian Center for Astrophysics ( CfA ) in partnership with colleagues at other institutions around the world . The goal of this project is to provide a single technique that can be used to analyze all types of astronomical data sets , including photometric period series , spectroscopic observations , photographs , etc . , using state - of - the - art methods such as image subtraction , cross correlation , period finding methods , spectral line fitting , etc .This program has been released under the GNU General Public License v3 . 0 and is accessible on GitHub at : https : / / github . com / VariableStar / one - shot - astro . One Shot Astro includes several methods which are helpful for studying huge amounts of astronomical data easily and smoothly .These include one - shot - datacleaner , which performs automated quality control checks on raw data data ; one - shot - mosaic , which forms mosaicked images from multiple dithered exposures ; one - shot - astrometry , which determines astrometric solutions for individual frames or entire mosaic images ; one - shot - photometry , which estimates fluxes and / or magnitudes for planets across a field - of - view ; one - shot - pipeline , which automates the process of running these numerous tasks combined into a pipeline ; one - shot - wikimapia , which allows users to create custom skies mapping based on their own catalogs of items . In addition to these core programs , there are also many extra modules available through the one - shot - astro repository which allow users to conduct more sophisticated analyses , such as : one - shot - catalog , which offers connections to a variety of different astrophysical databases via SQL queries ; one - shot - fastphot , which uses computer learning techniques to measure stellar",
        "rewrite_text": "**Title:** The Variable Star One-Shot Project and Its Offshoot: Wikimbad\n\n**Abstract:** The Variable Star One-Shot Project is an innovative open-source software tool designed for the comprehensive analysis of astronomical data. This initiative was collaboratively developed by researchers at the Harvard-Smithsonian Center for Astrophysics (CfA) alongside international partners from various institutions. The primary objective of the project is to establish a unified methodology for analyzing diverse types of astronomical datasets, including photometric time series, spectroscopic data, and imaging, employing advanced techniques such as image subtraction, cross-correlation, period-finding algorithms, and spectral line fitting. The software is distributed under the GNU General Public License v3.0 and is available on GitHub at https://github.com/VariableStar/one-shot-astro.\n\nOne-Shot Astro encompasses a suite of tools designed to facilitate the efficient handling of large volumes of astronomical data. Key features include One-Shot Datacleaner, which automates quality control checks on raw datasets; One-Shot Mosaic, which generates mosaicked images from multiple dithered exposures; One-Shot Astrometry, which provides astrometric solutions for individual frames or entire mosaics; One-Shot Photometry, which calculates fluxes and magnitudes for celestial objects across a field of view; and One-Shot Pipeline, which streamlines the execution of these tasks into a cohesive workflow. Additionally, One-Shot Wikimapia enables users to create customized sky maps based on their own catalogs.\n\nBeyond these core functionalities, the One-Shot Astro repository offers numerous supplementary modules that empower users to perform more complex analyses. Notable among these are One-Shot Catalog, which facilitates access to various astrophysical databases through SQL queries, and One-Shot Fastphot, which leverages machine learning techniques for stellar measurements. Collectively, these tools represent a significant advancement in the field of astronomical data analysis, promoting accessibility and efficiency for researchers worldwide.",
        "ori-fast-z-score": 0.9135002783911397,
        "water-fast-z-score": 7.945016530582732,
        "rewrite-fast-z-score": -1.0524696231684352
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Near-Infrared Spectra of the Black Hole X-Ray Binary, A0620-00 .\nAbstract:\nWe present near-infrared (NIR) spectroscopy of the black hole X-ray binary system A0620-00 obtained with the Subaru Telescope and Gemini Observatory in 2001-2002. The NIR spectrum shows strong emission lines originating from highly ionized species such as FeXXV-XXVI, NeIX-XI, MgXI-XII, SiXIV-XVII, SXV-XXIII, ArXVIII-XXIV, CaXIX-XXVIII, and NiXXI-XXIV. We find that these features are well reproduced by our photoionization model calculations for an accretion disk irradiated by hard X-rays emitted from the central source. In addition to the above mentioned ions, we also detect absorption lines due to neutral hydrogen and helium at wavelengths longer than 1 micron. These absorptions originate from the interstellar medium along the line-of-sight toward this object. \n \n Keywords: Near-infrared spectroscopy",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Near - Infrared Spectra of the Black Hole X - Ray Binary , A0620 - 00 . Abstract : We report near - infrared ( NIR ) spectroscopy of the dark hole X - ray binary system A0620 - 00 collected with the Subaru Telescope and Gemini Observatory in 2001 - 2002 .The NIR spectrum displays strong radiation patterns derived from highly ionized species such as FeXXV - XXVI , NeIX - XI , MgXI - XII , SiXIV - XVII , SXV - XXIII , ArXVIII - XXIV , CaXIX - XXVIII , and NiXXI - XXIV . We see that these characteristics are better illustrated by our photoionization profile measurements for an accretion disk irradiated by hard X - rays generated from the main source .In addition to the above mentioned ions , we also observe absorption lines owing to neutral hydrogen and helium at wavelengths greater than 1 micron . These absorptions originate from the interstellar medium along the line - of - view toward this object .Keywords: Near-infrared spectroscopy",
        "rewrite_text": "We present a detailed analysis of near-infrared (NIR) spectroscopy conducted on the black hole X-ray binary system A0620-00, utilizing data collected from the Subaru Telescope and Gemini Observatory during the years 2001 to 2002. Our findings reveal a spectrum characterized by prominent emission lines from highly ionized elements, including FeXXV-XXVI, NeIX-XI, MgXI-XII, SiXIV-XVII, SXV-XXIII, ArXVIII-XXIV, CaXIX-XXVIII, and NiXXI-XXIV. These spectral features are effectively represented through our photoionization profile measurements, which illustrate the dynamics of the accretion disk that is being illuminated by hard X-rays emitted from the central black hole. Additionally, we identify absorption lines corresponding to neutral hydrogen and helium at wavelengths exceeding 1 micron. These absorption features are attributed to the interstellar medium encountered along the line of sight to A0620-00. Our research contributes to the understanding of the physical processes occurring in black hole X-ray binaries and the interactions between the accretion disk and surrounding medium. The results underscore the significance of NIR spectroscopy in probing the complex environments of such astronomical systems. This study not only enhances our knowledge of A0620-00 but also provides a framework for future investigations into similar celestial phenomena. \n\nKeywords: Near-infrared spectroscopy, black hole X-ray binaries, A0620-00, photoionization, accretion disk, interstellar medium.",
        "ori-fast-z-score": -2.1773242158072694,
        "water-fast-z-score": 3.3709993123162105,
        "rewrite-fast-z-score": 1.6681153124565982
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  21-cm synthesis observations of VIRGOHI 21 - a possible dark galaxy in the Virgo Cluster .\nAbstract:\nWe present new radio continuum and H I absorption data for the dwarf irregular galaxy VIRGOHI 21, which is located at the edge of the virgo cluster. The galaxy has an optical diameter of about 1 kpc (0.3 arcmin) and shows no signs of star formation activity. We find that its neutral hydrogen mass amounts to M_HI = 2 x 10^{10} M_sol. Its total luminosity L_TOT = 3.5 x 10^{8} L_SOL corresponds to a B-band absolute magnitude MB = -18.7 mag. This value agrees well with those found by other authors for similar galaxies. From our analysis we conclude that this object may be considered as a candidate for a  dark galaxy . It contains only little or even no stars but still possesses a large amount of cold gas. If confirmed, it would provide further evidence for the existence of such objects.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : 21 - cm synthesis observations of VIRGOHI 21 - a possible dark galaxy in the Virgo Cluster . Abstract : We report new radio continuum and H I emission data for the dwarf irregular star VIRGOHI 21 , which is situated at the boundary of the virgo cluster .The galaxy has an optical diameter of about 1 kpc ( 0 . 3 arcmin ) and shows no indication of star formation activity . We see that its neutral hydrogen mass amounts to M _ HI = 2 x 10 ^ { 10 } M _ sol .Its overall luminosity L _ TOT = 3 . 5 x 10 ^ { 8 } L _ SOL relates to a B - band absolute magnitude MB = - 18 . 7 mag . This value agrees well with those given by other researchers for related galaxies .From our analysis we conclude that this body may be regarded as a candidate for a black galaxy . It contains only little or even no stars but still possesses a large number of cold energy .If confirmed , it would offer further evidence for the existence of such objects .",
        "rewrite_text": "We present new findings from radio continuum and H I emission observations of the dwarf irregular galaxy VIRGOHI 21, located at the periphery of the Virgo Cluster. This galaxy has an optical diameter of approximately 1 kpc (0.3 arcminutes) and exhibits no signs of ongoing star formation. Our measurements indicate that the neutral hydrogen mass of VIRGOHI 21 is M_HI = 2 x 10^10 M_sol, suggesting a significant reservoir of cold gas. The total luminosity of the galaxy is calculated to be L_TOT = 3.5 x 10^8 L_sol, which corresponds to a B-band absolute magnitude of M_B = -18.7 mag. This luminosity aligns well with values reported by other researchers for similar galaxies, reinforcing the consistency of our findings. Based on our analysis, we propose that VIRGOHI 21 may be classified as a candidate for a \"dark galaxy,\" characterized by a minimal stellar population yet possessing a substantial amount of cold gas. If our hypothesis is validated, it would provide compelling evidence for the existence of such dark galaxies, which challenge traditional understandings of galaxy formation and evolution. The implications of this research extend to the broader context of dark matter and the role of gas-rich, star-poor galaxies in the cosmic landscape. Further studies are warranted to explore the nature of VIRGOHI 21 and its potential contributions to our understanding of dark matter and galaxy formation processes within the Virgo Cluster.",
        "ori-fast-z-score": 1.0392304845413263,
        "water-fast-z-score": 5.505977612893481,
        "rewrite-fast-z-score": 0.5883484054145521
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Lattice Boltzmann inverse kinetic approach for the incompressible Navier-Stokes equations .\nAbstract:\nWe present an efficient numerical scheme to solve the incompressible Navierstokes (NS) equations by using the lattice Boltzmann method with the Inverse Kinetic Approach (IKA). The IKA is based on the idea that the NS equation can be recovered as the equilibrium state in the Chapman-Enskog expansion, and it has been successfully applied to various fluid dynamics problems. We show how this concept can be implemented into the LBM framework. Numerical results are presented to demonstrate the accuracy and efficiency of our proposed algorithm. Finally we discuss some possible extensions of the current work. Keywords: Lattice Boltzmann Method; Inverse Kinetic Approximation; Incompressible Navier-Stokes; Computational Fluid Dynamics. 1 Introduction The lattice Boltzmann method (LBM), originally developed by Frisch et al  1  , is one of the most promising approaches to computational fluid dynamics (CFD). It is particularly suitable for parallel computing due to its inherent locality  2  . Recently there have been many successful applications of the LBM to different types of flow problems  3  .\nThe basic idea behind the LBM is to represent the distribution function f(x,t) at each node x of a regular grid by a finite number of particles moving along discrete velocities c i = ciΔt/Δx, where Δx and Δt denote respectively the spatial and temporal resolutions  4  . Then the evolution of these particles is governed by the following equation: \nwhere τ denotes the relaxation time which controls the rate of approaching towards the equilibrium distribution function f eq i\n. By choosing appropriate values of τ, the macroscopic quantities such as density ρ and velocity u can be obtained through moments of the distribution function:",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Lattice Boltzmann inverse kinetic technique for the incompressible Navier - Stokes equations . Abstract : We create an efficient numerical plan to solve the incompressible Navierstokes ( NS ) equations by using the lattice Boltzmann technique with the Inverse Kinetic Approach ( IKA ) .The IKA is based on the idea that the NS equation can be recovered as the equilibrium state in the Chapman - Enskog expansion , and it has been successfully application to numerous fluid dynamics situations . We see how this concept can be applied into the LBM framework .Numerical results are presented to indicate the accuracy and efficiency of our proposed algorithm . Finally we explain some possible extensions of the present work .Keywords : Lattice Boltzmann Method ; Inverse Kinetic Approximation ; Incompressible Navier - Stokes ; Computational Fluid Dynamics . 1 Introduction The lattice Boltzmann technique ( LBM ) , previously developed by Frisch et al 1 , is one of the most exciting methods to computational liquid mechanics ( CFD ) .It is especially suitable for concurrent processing due to its inherent locality 2 . Recently there have been many successful uses of the LBM to different kinds of flow difficulties 3 .The basic idea behind the LBM is to depict the distribution function f ( x , t ) at each node x of a regular grid by a finite number of particles moving along continuous velocities c i = ciΔt / Δx , where Δx and Δt denote respectively the spatial and temporal resolutions 4 . Then the evolution of these objects is governed by the following equation : where τ represents the relaxation time which controls the speed of approaching towards the equilibrium distribution function f eq i .By selecting appropriate expressions of τ , the macroscopic parameters such as density τ and speed u can be obtained through moments of the distribution function :",
        "rewrite_text": "**Title:** Lattice Boltzmann Inverse Kinetic Technique for the Incompressible Navier-Stokes Equations\n\n**Abstract:** In this study, we present a novel and efficient numerical strategy for addressing the incompressible Navier-Stokes (NS) equations by integrating the lattice Boltzmann method (LBM) with the Inverse Kinetic Approach (IKA). The IKA operates on the principle that the NS equations can be derived as the equilibrium state within the Chapman-Enskog expansion framework. This approach has demonstrated its effectiveness across a variety of fluid dynamics scenarios. We explore the application of this concept within the LBM framework, showcasing how it enhances the computational efficiency and accuracy of simulations. Our numerical results substantiate the performance of the proposed algorithm, highlighting its capability to yield precise solutions for complex flow problems. Furthermore, we discuss potential avenues for extending this work, including adaptations for more intricate fluid dynamics challenges and improvements in computational efficiency. The findings presented in this article contribute to the growing body of knowledge in computational fluid dynamics and offer a promising direction for future research in the application of lattice Boltzmann techniques. \n\n**Keywords:** Lattice Boltzmann Method; Inverse Kinetic Approximation; Incompressible Navier-Stokes; Computational Fluid Dynamics. \n\n**1 Introduction:** The lattice Boltzmann method (LBM), initially introduced by Frisch et al., has emerged as a powerful tool in computational fluid dynamics (CFD), particularly due to its suitability for parallel processing stemming from its inherent locality. Recent advancements have seen the LBM successfully applied to a wide range of flow problems. The fundamental concept of LBM involves representing the distribution function f(x, t) at each node of a structured grid using a finite number of particles that move with continuous velocities c_i = ciΔt / Δx, where Δx and Δt denote spatial and temporal resolutions, respectively. The evolution of these particles is governed by a specific equation, with τ representing the relaxation time that dictates the rate of convergence toward the equilibrium distribution function f_eq_i. By selecting appropriate formulations for τ, macroscopic parameters such as density (ρ) and velocity (u) can be derived from the moments of the distribution function.",
        "ori-fast-z-score": -2.5533076283443092,
        "water-fast-z-score": 3.609848715935058,
        "rewrite-fast-z-score": -1.7728105208558367
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Metal-insulator transition in the low-dimensional organic conductor (TMTSF)2FSO3 probed by infrared microspectroscopy .\nAbstract:\nThe metal-insulator phase diagram of the quasi-one dimensional organic conductor (TMTSF2)2SO3 has been investigated using infrared spectroscopy and scanning tunneling microscopy at temperatures down to 20 mK. The temperature dependence of the optical conductivity shows that the charge gap opens below TMI = 135 K, which is consistent with previous results obtained on single crystals grown under different conditions. In addition we find evidence for an additional energy scale associated with the formation of excitons near 50 meV. We show that this feature can be explained within a simple model based on electron-phonon interaction between molecular orbitals. Finally, our data reveal that the electronic structure changes significantly across the metal-insulator transition. This finding suggests that the insulating state may not simply result from localization effects but rather involves strong correlations among electrons. The metal-insulator phase diagram was studied in detail by infrared spectroscopy and scanning tunnel microscopy experiments performed on high-quality single crystals of (TMTSF)2SO3. Our measurements provide direct spectroscopic evidence for two distinct energy scales involved in the opening of the charge gap as well as for significant modifications of the electronic structure across the metal-insulator boundary.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Metal - insulator transition in the small - dimensional organic conductor ( TMTSF ) 2FSO3 probed by infrared microspectroscopy . Abstract : The metal - insulator phase diagram of the quasi - one dimensional organic conductor ( TMTSF2 ) 2SO3 has been investigated using infrared spectroscopy and scanning tunneling microscopy at conditions down to 20 mK .The temperature dependence of the optical conductivity reveals that the charge gap opens below TMI = 135 K , which is compatible with previous findings obtained on individual crystals grown under various circumstances . In addition we find proof for an additional power range involved with the formation of excitons near 50 meV .We see that this phenomenon can be described within a simple model based on electron - phonon interaction between molecular orbitals . Finally , our measurements reveal that the electronic structure shifts significantly across the metal - insulator transition .This found shows that the insulating state may not simply result from localization effects but rather involves strong correlations among electrons . The metal - insulator phase diagram was studied in detail by infrared spectroscopy and scanning tunnel microscopy experiments conducted on well - grade single crystals of ( TMTSF ) 2SO3 .Our measurements give significant spectroscopic evidence for two separate power scales implicated in the opening of the charge gap as well as for significant modifications of the electronic configuration across the metal - insulator boundary .",
        "rewrite_text": "**Title:** Metal-Insulator Transition in the Small-Dimensional Organic Conductor (TMTSF)2FSO3 Probed by Infrared Microspectroscopy\n\n**Abstract:** This study explores the metal-insulator phase diagram of the quasi-one-dimensional organic conductor (TMTSF)2SO3 through the application of infrared spectroscopy and scanning tunneling microscopy, conducted at temperatures as low as 20 mK. Our findings indicate that the optical conductivity exhibits a temperature-dependent behavior, with a charge gap emerging below the critical temperature TMI = 135 K. This observation aligns with previous research conducted on individual crystals synthesized under various conditions. Furthermore, we provide evidence for an additional energy scale associated with the formation of excitons, observed at approximately 50 meV. This phenomenon can be effectively modeled through a framework that considers electron-phonon interactions among molecular orbitals. \n\nOur measurements also reveal a significant alteration in the electronic structure as the system transitions from metallic to insulating states. This suggests that the insulating behavior cannot be solely attributed to localization effects; instead, it implies the presence of strong electron correlations. The detailed investigation of the metal-insulator phase diagram was facilitated by high-quality single crystals of (TMTSF)2SO3, allowing us to gather substantial spectroscopic evidence for two distinct energy scales that contribute to the opening of the charge gap. Additionally, we observed notable changes in the electronic configuration across the metal-insulator transition boundary. These results enhance our understanding of the underlying mechanisms governing the metal-insulator transition in this organic conductor, highlighting the complex interplay between electronic correlations and excitonic effects.",
        "ori-fast-z-score": 0.38138503569823695,
        "water-fast-z-score": 5.789863774090244,
        "rewrite-fast-z-score": 1.104689541477988
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Particle-vibration coupling within covariant density functional theory .\nAbstract:\nWe present an ab initio method for the calculation of phonon dispersion relations in solids, which is based on the direct solution of the Bethe-Salpeter equation (BSE) and includes electron-phonon interaction effects beyond the adiabatic approximation. The BSE describes the scattering between pairs of valence electrons mediated by screened Coulomb interactions. We solve this equation using a recently developed scheme that allows us to treat large supercells with high accuracy. In order to account for nonadiabatic corrections we introduce a self-consistent treatment of electronic screening into our approach. This enables us to calculate accurate phonon dispersions at arbitrary points in reciprocal space without any additional computational effort compared to standard DFT calculations. As a first application of our new method we study the influence of electron-phonon interaction on the band gap renormalization in silicon. Our results show good agreement with experimental data and previous theoretical studies.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Particle - vibration coupling within covariant density functional theory . Abstract : We present an ab initio method for the determination of phonon dispersion relations in solids , which is based on the direct solution of the Bethe - Salpeter equation ( BSE ) and incorporates electron - phonon interaction phenomena beyond the adiabatic approximation .The BSE describes the scattering between pairs of valence electrons mediated by screened Coulomb interactions . We answer this equation using a recently advanced method that enables us to treat large supercells with high clarity .In order to account for nonadiabatic corrections we incorporate a self - consistent handling of electronic screening into our approach . This enables us to estimate accurate phonon dispersions at arbitrary points in reciprocal space without any additional mathematical effort compared to standard DFT calculations .As a early application of our new method we study the impact of electron - phonon interaction on the band gap renormalization in silicon . Our results show good agreement with theoretical data and previous conceptual research .",
        "rewrite_text": "We introduce a novel ab initio approach for calculating phonon dispersion relations in solid materials, utilizing the direct resolution of the Bethe-Salpeter equation (BSE). This method goes beyond the traditional adiabatic approximation by effectively incorporating electron-phonon interaction phenomena. The BSE framework facilitates the analysis of scattering events between pairs of valence electrons, which are influenced by screened Coulomb interactions. To solve this equation, we employ a recently developed technique that allows for the treatment of large supercells with exceptional precision. \n\nTo address nonadiabatic effects, our methodology integrates a self-consistent treatment of electronic screening, which significantly enhances the accuracy of phonon dispersion estimates across arbitrary points in reciprocal space. Notably, this approach does not require any additional mathematical complexity compared to conventional density functional theory (DFT) calculations. \n\nAs an initial application of our innovative method, we investigate the role of electron-phonon interactions in the band gap renormalization of silicon. Our findings demonstrate a strong correlation with existing theoretical predictions and previous studies, validating the effectiveness of our approach. This work not only advances the understanding of phonon dynamics in solids but also provides a robust framework for future research into the intricate interplay between electronic and vibrational properties in various materials.",
        "ori-fast-z-score": 0.9878783399072131,
        "water-fast-z-score": 5.45544725589981,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Does confining the hard-sphere fluid between hard walls change its average properties? .\nAbstract:\nWe study the effect of confinement on the structure and dynamics of a simple model system, namely an ensemble of N identical particles interacting via repulsive pair potentials confined in a volume V by two parallel impenetrable walls at distance L apart.  We use Monte Carlo simulations to calculate the density profiles for different values of the wall separation L and particle number N . The results show that the density profile is not affected significantly when increasing the wall separation beyond a certain value which depends on both the temperature T and the particle number N .  In addition we find that the self-diffusion coefficient D decreases with decreasing wall separation but increases again if one further reduces the wall separation below some critical value depending on the temperature T .\nThe observed behavior can be explained within the framework of mode-coupling theory (MCT) using a generalized version of MCT developed recently by us  Physica A, vol. 315, no. 1, pp. 39-48, (2003), Physica A, vol. 320, no. 3, pp. 633-646, (2004) .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Does confining the hard - sphere fluid between hard walls change its average characteristics ? .Abstract : We explore the impact of confinement on the composition and dynamics of a simple model structure , namely an ensemble of N identical particles interacting via repulsive pair potentials restricted in a volume V by two connected impenetrable walls at distance L apart . We use Monte Carlo simulations to estimate the density profiles for different values of the wall separation L and electron number N .The results show that the density profile is not affected greatly when increasing the wall separation beyond a certain value which depends on both the temperature T and the particle number N . In addition we find that the self - diffusion coefficient D reduces with decreasing wall separation but grows again if one further decreases the wall separation below some significant value depending on the temperature T .The observed behavior can be described within the framework of mode - correlation theory ( MCT ) using a generalized version of MCT created recently by us Physica A , vol . 315 , no .1 , pp . 39 - 48 , ( 2003 ) , Physica A , vol .320 , no . 3 , pp .633-646, (2004) .",
        "rewrite_text": "Title: Does Confining the Hard-Sphere Fluid Between Hard Walls Alter Its Average Characteristics?\n\nAbstract: This study investigates the effects of confinement on the properties and dynamics of a simple model system comprising N identical particles that interact through repulsive pair potentials. These particles are confined within a volume V by two rigid, impenetrable walls positioned at a distance L apart. Utilizing Monte Carlo simulations, we analyze the density profiles of the particle ensemble for various wall separations L and particle numbers N. Our findings indicate that the density profile remains relatively stable when the wall separation is increased beyond a certain threshold, which is influenced by both the temperature T and the number of particles N. Furthermore, we observe that the self-diffusion coefficient D decreases as the wall separation diminishes, but intriguingly, it begins to increase again when the wall separation is reduced below a critical value that is also temperature-dependent. This complex behavior can be effectively interpreted through the lens of mode-coupling theory (MCT), employing a generalized version of MCT that we have recently developed, as documented in our previous works published in Physica A (vol. 315, no. 1, pp. 39-48, 2003; vol. 320, no. 3, pp. 633-646, 2004). Our results contribute to a deeper understanding of how confinement influences the structural and dynamical characteristics of hard-sphere fluids, with implications for various applications in material science and nanotechnology.",
        "ori-fast-z-score": 0.5184758473652127,
        "water-fast-z-score": 4.950821982042208,
        "rewrite-fast-z-score": 2.8856078516089685
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Quantum Group of Isometries in Classical and Noncommutative Geometry .\nAbstract:\nThe quantum group of isometries (QGI) was introduced by Connes as the universal object for deformations of classical groups, which are obtained via spectral triples on commutative C*-algebras.  In this talk we will discuss how to define QGI s using noncommutative geometry techniques such as operator algebras and von Neumann algebras.   We will also explain how these objects can be used to study the classification problem of Riemannian manifolds with positive scalar curvature. The Quantum Group of Isometries (QGI), first defined by Alain Connes, plays an important role in both classical and noncommutative geometry. It is the universal object for deforming classical Lie groups into their corresponding quantum groups. This talk will give an introduction to QGI’s and show that they can be studied through operator algebra theory and von Neumann algebras. Finally it will present some results about the classification problem of Riemannain manifolds with positive scalar curvatures.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Quantum Group of Isometries in Classical and Noncommutative Geometry . Abstract : The quantum group of isometries ( QGI ) was introduced by Connes as the universal object for deformations of classical groups , which are derived via spectral triples on commutative C * - algebras .In this talk we will explore how to define QGI s using noncommutative geometry tools such as operator algebras and von Neumann algebras . We will also explain how these objects can be used to study the classification question of Riemannian manifolds with positive scalar curvature .The Quantum Group of Isometries ( QGI ) , initially established by Alain Connes , takes an important role in both classical and noncommutative geometry . It is the universal object for deforming classical Lie fields into their corresponding quantum groups .This discussion will giving an introduction to QGI ’ s and explain that they can be analyzed through operator algebra analysis and von Neumann algebras . Finally it will present some results about the classification question of Riemannain manifolds with positive scalar curvatures .",
        "rewrite_text": "Title: Quantum Group of Isometries in Classical and Noncommutative Geometry\n\nAbstract: The Quantum Group of Isometries (QGI), a concept introduced by Alain Connes, serves as a fundamental framework for understanding the deformation of classical groups through the lens of noncommutative geometry. This article delves into the definition and properties of QGIs, emphasizing their significance in both classical and noncommutative settings. By employing tools from operator algebras and von Neumann algebras, we illustrate how QGIs can be constructed and analyzed, providing a bridge between classical geometric concepts and their quantum counterparts. \n\nFurthermore, we investigate the implications of QGIs in the context of Riemannian geometry, particularly focusing on the classification of Riemannian manifolds that exhibit positive scalar curvature. The exploration of this classification problem is crucial, as it connects geometric properties with algebraic structures, revealing deeper insights into the nature of manifolds. Through this discussion, we aim to highlight the versatility of QGIs and their role in advancing our understanding of geometric and topological phenomena in both classical and noncommutative frameworks. \n\nIn summary, this article presents a comprehensive overview of the Quantum Group of Isometries, detailing its foundational aspects, analytical techniques, and applications in the classification of Riemannian manifolds. By bridging the gap between classical and noncommutative geometry, we hope to contribute to the ongoing discourse in mathematical physics and geometry, paving the way for future research in this intriguing area.",
        "ori-fast-z-score": -0.6509445549041194,
        "water-fast-z-score": 3.0983866769659336,
        "rewrite-fast-z-score": 0.8528028654224417
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Electrodynamics of Josephson vortex lattice in high-temperature superconductors .\nAbstract:\nWe study the electrodynamic properties of Josephson vortices (JVs) in hightemperature superconductors by solving numerically the time-dependent Ginzburg-Landau equations with an external magnetic field and current density. We find that JVs can be driven into motion by applying either a dc or ac electric field, which is consistent with previous experiments on YBa2Cu3O7-δ single crystals. The JV velocity increases linearly as the applied voltage increases for small voltages but saturates at large voltages due to the pinning effect. In addition, we show that the JV velocity decreases when increasing the temperature because of thermal fluctuations. Finally, we demonstrate that the JV dynamics are strongly affected by the anisotropy of the sample. \n \n Introduction \n \n High-temperature superconductivity has been discovered more than 30 years ago  1–3  . Since then, many theoretical models have been proposed to explain this phenomenon  4–9  , among them the so-called two-fluid model  10  . According to this theory, there exist two different types of charge carriers in these materials, namely electrons and holes  11  . These particles interact via attractive Coulomb forces  12  and form Cooper pairs  13  . When the material undergoes a phase transition below its critical temperature Tc, the Cooper pairs condense into a superfluid state  14  . This leads to macroscopic quantum phenomena such as zero resistance  15  and Meissner effect  16  . \n \n However, it was soon realized that the conventional Bardeen-Cooper-Schrieffer (BCS) theory  17  cannot fully account for all experimental observations  18  . For example, the BCS theory predicts that the energy gap between the ground-state and excited states should decrease rapidly near T = 0 K  19  . On the other hand, recent measurements  20  indicate that the energy gap remains almost constant down to very low temperatures  21  . To overcome this problem, several extensions of the original BCS theory were developed  22–24  . Among those theories, one of the most successful ones is the Eliashberg formalism  25  , where the electron-phonon interaction plays an important role  26  . It turns out that",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Electrodynamics of Josephson vortex lattice in high - temperature superconductors . Abstract : We research the electrodynamic characteristics of Josephson vortices ( JVs ) in hightemperature superconductors by solving numerically the period - dependent Ginzburg - Landau coefficients with an external magnetic force and current density .We see that JVs can be pushed into motion by using either a dc or ac electric field , which is consistent with previous research on YBa2Cu3O7 - δ single crystals . The JV speed increases linearly as the introduced voltage increases for large voltages but saturates at large voltages due to the locking effect .In addition , we prove that the JV speed reduces when varying the temperature because of thermal fluctuations . Finally , we prove that the JV mechanics are strongly altered by the anisotropy of the sample .Introduction High - temperature superconductivity has been detected more than 30 centuries earlier 1 – 3 . Since then , various theoretical theories have been proposed to explain this phenomenon 4 – 9 , among them the so - called two - fluid model 10 .According to this theory , there exist two different kinds of charge carriers in these materials , principally electrons and holes 11 . These particles react via attractive Coulomb forces 12 and form Cooper pairs 13 .When the material undergoes a phase shift below its critical temperature Tc , the Cooper pairs condense into a superfluid state 14 . This leads to macroscopic quantum effects such as zero resistance 15 and Meissner phenomenon 16 .However , it was immediately realized that the usual Bardeen - Cooper - Schrieffer ( BCS ) theory 17 fails truly account for all experimental phenomena 18 . For instance , the BCS theory predicts that the electricity gap between the ground - state and excited states should decrease rapidly near T = 0 K 19 .On the other hand , recent observations 20 imply that the power gap continues almost steady down to very low temperatures 21 . To solve this situation , various extensions of the original BCS theory were developed 22 – 24 .Among those theories , one of the most popular ones is the Eliashberg formalism 25 , where the electron - phonon interaction plays an important role 26 . It turns out that",
        "rewrite_text": "**Title:** Electrodynamics of Josephson Vortex Lattice in High-Temperature Superconductors\n\n**Abstract:** This study investigates the electrodynamic properties of Josephson vortices (JVs) in high-temperature superconductors through numerical solutions of the period-dependent Ginzburg-Landau coefficients, incorporating external magnetic fields and current densities. Our findings indicate that JVs can be set into motion by applying either direct current (dc) or alternating current (ac) electric fields, corroborating previous studies conducted on YBa2Cu3O7-δ single crystals. We observe a linear increase in JV speed with rising voltage at higher voltage levels; however, this speed reaches a saturation point due to the locking effect at very high voltages. Additionally, we demonstrate that thermal fluctuations lead to a reduction in JV speed as the temperature varies. Importantly, our results reveal that the mechanical behavior of JVs is significantly influenced by the anisotropic nature of the superconducting sample. \n\nThe phenomenon of high-temperature superconductivity has been recognized for over three decades, prompting numerous theoretical models aimed at elucidating its underlying mechanisms. Among these, the two-fluid model stands out, positing the existence of two distinct types of charge carriers—electrons and holes—that interact through attractive Coulomb forces to form Cooper pairs. When the material transitions to a superconducting state below its critical temperature (Tc), these Cooper pairs condense into a superfluid, resulting in remarkable quantum effects such as zero electrical resistance and the Meissner effect. However, the conventional Bardeen-Cooper-Schrieffer (BCS) theory falls short in explaining all observed phenomena, particularly the behavior of the energy gap near absolute zero, which recent experiments suggest remains relatively stable rather than diminishing as predicted. To address these discrepancies, various extensions of the BCS framework have been proposed, with the Eliashberg formalism being one of the most prominent, emphasizing the critical role of electron-phonon interactions in high-temperature superconductivity. This work contributes to the understanding of the dynamics of Josephson vortices and their dependence on external conditions, paving the way for further exploration in the field of superconductivity.",
        "ori-fast-z-score": -1.158648244043315,
        "water-fast-z-score": 6.858571279792899,
        "rewrite-fast-z-score": 1.632993161855452
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Geometry of Parallelizable Manifolds in the Context of Generalized Lagrange Spaces .\nAbstract:\nWe study parallelizable manifolds and their geometry within the context of generalized Lagrange spaces (GLS). We show that every parallelizable manifold is locally isomorphic to an open subset of some GLS, which we call its local model space. The main results are as follows:  1) Every parallelizable n-manifold admits a global coordinate system with n coordinates.  2) If M is a parallelizable n-manif old then there exists a unique vector bundle over M whose total space is diffeomorphic to R^n x M . 3) Let M be a parallelizable n-man ifold. Then for any point p ∈ M , there exist neighborhoods U of p and V of 0 in T_pM such that:  4) For each point p ∈ M , let π : E → M denote the vector bundle associated to M by Theorem 3.1. Then there exists a natural isomorphism between the fiber at p of this vector bundle and the tangent space TpM:",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Geometry of Parallelizable Manifolds in the Context of Generalized Lagrange Spaces . Abstract : We research parallelizable manifolds and their geometry within the context of generalized Lagrange spaces ( GLS ) .We see that every parallelizable manifold is locally isomorphic to an open subset of some GLS , which we call its local model space . The main results are as follows : 1 ) Every parallelizable n - manifold admits a global coordinate system with n coordinates .2 ) If M is a parallelizable n - manif old then there exists a unique vector bundle over M whose total space is diffeomorphic to R ^ n x M . 3 ) Let M be a parallelizable n - man ifold .Then for any point π ∈ M , there exist neighborhoods U of p and V of 0 in T _ pM such that : 4 ) For each point p ∈ M , let σ : E → M denote the vector bundle attributed to M by Theorem 3 . 1 . Then there exists a natural isomorphism between the fiber at p of this vector bundle and the tangent space TpM :",
        "rewrite_text": "Title: Geometry of Parallelizable Manifolds in the Context of Generalized Lagrange Spaces\n\nAbstract: This study delves into the geometry of parallelizable manifolds, exploring their properties within the framework of generalized Lagrange spaces (GLS). We establish that every parallelizable manifold can be locally represented as an open subset of a corresponding GLS, which we refer to as its local model space. Our primary findings can be summarized as follows: First, we demonstrate that any parallelizable n-manifold possesses a global coordinate system consisting of n coordinates, facilitating a comprehensive understanding of its geometric structure. Second, we show that for a given parallelizable n-manifold M, there exists a unique vector bundle over M, with its total space diffeomorphic to the product space R^n × M. This result underscores the intrinsic relationship between the manifold and its associated vector bundle. Third, we investigate the local properties of parallelizable n-manifolds by considering any point π within M. We identify neighborhoods U around π and V around the zero vector in the tangent space T_pM, which play a crucial role in our analysis. Finally, we establish a natural isomorphism between the fiber of the vector bundle associated with M, as described in Theorem 3.1, and the tangent space T_pM at any point p in M. This isomorphism highlights the deep connection between the geometric structure of parallelizable manifolds and their tangent spaces, providing valuable insights into their geometric and topological characteristics. Overall, our research contributes to the understanding of parallelizable manifolds in the context of GLS, revealing significant geometric structures and relationships that enhance the study of differential geometry and manifold theory.",
        "ori-fast-z-score": 0.12403473458920847,
        "water-fast-z-score": 1.5882027766319677,
        "rewrite-fast-z-score": 1.1441551070947107
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Self consistency in hadron physics .\nAbstract:\nThe self-consistent approach to the description of nuclear matter is reviewed and its application to other systems, such as atomic nuclei or quark-gluon plasma, discussed briefly. The main idea behind this method is that one should not consider only the mean field acting on particles but also take into account fluctuations around it. This leads to an infinite set of coupled equations for all orders of correlation functions which can be solved by truncation at some order. In particular we discuss how the results depend on the choice of the approximation scheme used. We show that the inclusion of higher-order correlations improves agreement with experimental data considerably. Finally, we present our recent results obtained within the framework of the relativistic random phase approximation (RRPA) including up to fourth-order correlations. These calculations are performed using realistic nucleonnucleon interactions derived from chiral effective theory. It turns out that the RRPA results agree well with available experimental information about excited states of medium-heavy nuclei.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Self consistency in hadron physics . Abstract : The self - consistent method to the description of nuclear material is reviewed and its use to other structures , such as atomic hydrogen or quark - gluon radiation , discussed briefly .The main idea behind this process is that one should not take only the mean field acted on objects but also took into consideration fluctuations around it . This leads to an endless system of coupled equations for all orders of correlation functions which can be answered by truncation at some order .In particular we explain how the results vary on the selection of the approximation scheme used . We see that the inclusion of greater - order correlations improves agreement with observation information significantly .Finally , we present our latest findings obtained within the framework of the relativistic random phase approximation ( RRPA ) containing up to fourth - order correlations . These analyses are performed using accurate nucleonnucleon interactions derived from chiral effective theory .It turns out that the RRPA results agree well with provided experimental evidence about excited states of medium - heavy nuclei .",
        "rewrite_text": "In this article, we provide a comprehensive review of the self-consistent method applied to the description of nuclear matter, while also briefly exploring its applicability to other systems, such as atomic hydrogen and quark-gluon radiation. The core principle of this approach emphasizes the importance of considering not only the mean field acting on the particles but also the fluctuations around this mean field. This consideration leads to a complex system of coupled equations that govern all orders of correlation functions. To make this problem tractable, we employ a truncation strategy at a certain order of these functions. \n\nWe delve into how the choice of approximation scheme influences the results, demonstrating that incorporating higher-order correlations significantly enhances the alignment of theoretical predictions with experimental observations. Our latest findings are derived within the framework of the relativistic random phase approximation (RRPA), which includes correlations up to the fourth order. The analyses utilize precise nucleon-nucleon interactions informed by chiral effective theory. Notably, our RRPA results exhibit strong agreement with experimental data concerning the excited states of medium-heavy nuclei, underscoring the effectiveness of the self-consistent approach in hadron physics. This work not only reinforces the validity of self-consistency in nuclear matter but also opens avenues for further research in related fields, highlighting the interconnectedness of various physical systems through the lens of correlation functions.",
        "ori-fast-z-score": -0.3144854510165755,
        "water-fast-z-score": 6.325405337855594,
        "rewrite-fast-z-score": 0.6527533657682196
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Systematic Study of Departures from Chemical Equilibrium in the Atmospheres of Substellar Mass Objects .\nAbstract:\nWe present an analysis of chemical equilibrium and disequilibrium processes occurring in the atmospheres of substellar mass objects (SMBOs). We have developed a new method for calculating departures from chemical equilibrium, which is based on the assumption that all species are in local thermodynamic equilibrium with each other at any given point within the atmosphere. This approach allows us to calculate the abundances of individual molecular species as functions of altitude above the photosphere. The results show that there can be significant deviations from chemical equilibrium even under conditions where the gas temperature is much higher than the dust temperature. In particular, we find that carbon monoxide may become depleted by several orders of magnitude relative to its abundance predicted by chemical equilibrium models. These findings suggest that SMBO observations should take into account possible non-equilibrium effects when interpreting their spectra. \n \n Keywords: Chemical equilibrium; Dust grains; Local thermodynamic equilibrium",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Systematic Study of Departures from Chemical Equilibrium in the Atmospheres of Substellar Mass Objects . Abstract : We report an assessment of chemical equilibrium and disequilibrium systems observed in the atmospheres of substellar mass bodies ( SMBOs ) .We have developed a new method for calculating departures from molecular equilibrium , which is based on the assumption that all species are in local thermodynamic equilibrium with each other at any certain point within the atmosphere . This method enables us to estimate the abundances of different molecular species as functions of altitude above the photosphere .The results show that there can be considerable deviations from chemical equilibrium even under environments where the gas temperature is much higher than the dust temperature . In particular , we find that carbon monoxide might appear depleted by many orders of magnitude compared to its availability predicted by chemical equilibrium models .These conclusions propose that SMBO observations should take into consideration likely non - equilibrium phenomena when interpreting their spectra . Keywords : Chemical equilibrium ; Dust grains ; Local thermodynamic equilibrium",
        "rewrite_text": "Title: A Systematic Study of Departures from Chemical Equilibrium in the Atmospheres of Substellar Mass Objects\n\nAbstract: This study presents a comprehensive evaluation of both chemical equilibrium and disequilibrium phenomena observed in the atmospheres of substellar mass objects (SMBOs). We introduce a novel methodology for assessing deviations from molecular equilibrium, predicated on the premise that all chemical species maintain local thermodynamic equilibrium at specific points within the atmospheric structure. This approach allows us to calculate the relative abundances of various molecular species as a function of altitude above the photosphere. Our findings reveal significant departures from chemical equilibrium, particularly in environments where the gas temperature substantially exceeds that of the surrounding dust. Notably, we observe that carbon monoxide can be markedly depleted—by several orders of magnitude—compared to predictions made by traditional chemical equilibrium models. These insights underscore the necessity for researchers to account for potential non-equilibrium effects when analyzing spectral data from SMBOs. The implications of our results are critical for the interpretation of atmospheric compositions and the understanding of the physical processes governing these celestial bodies. This work emphasizes the importance of integrating non-equilibrium considerations into future studies of SMBO atmospheres to enhance the accuracy of spectral interpretations and broaden our understanding of their chemical dynamics. \n\nKeywords: Chemical equilibrium; Dust grains; Local thermodynamic equilibrium.",
        "ori-fast-z-score": -1.6059101370939322,
        "water-fast-z-score": 3.988620176087328,
        "rewrite-fast-z-score": -1.2686700948330931
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Optical Probe Diffusion in Polymer Solutions .\nAbstract:\nWe report on the measurement and analysis of diffusion coefficients for probe molecules embedded within polymer solutions using optical techniques.  The experiments were performed with two different types of probes, fluorescently labeled polystyrene spheres (PS) and dye doped poly(methyl methacrylate) (PMMA).  We find that both PS and PMMA exhibit anomalous subdiffusive behavior at low concentrations but normal diffusive behavior at higher concentrations.  In addition to measuring the mean square displacement as a function of time we also measure the distribution of displacements over many particles simultaneously.   This allows us to extract information about the underlying dynamics which is not possible by simply looking at the ensemble averaged MSD curve alone.  By fitting our data to an appropriate model we are able to determine the concentration dependence of the diffusion coefficient D0 and the exponent α characterizing the power law decay of the probability density function P(Δr). Our results show good agreement between theory and experiment when compared against predictions based upon the generalized Langevin equation (GLE).   These findings demonstrate how optical methods can be used to study complex systems such as concentrated polymer solutions where traditional light scattering techniques may fail due to multiple scattering effects.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Optical Probe Diffusion in Polymer Solutions . Abstract : We report on the observation and assessment of diffusion coefficients for probe molecules embedded within polymer solutions use optical techniques .The studies were performed with two different kinds of probes , fluorescently labeled polystyrene spheres ( PS ) and dye doped poly ( methyl methacrylate ) ( PMMA ) . We see that both PS and PMMA perform anomalous subdiffusive behavior at low levels but usual diffusive behavior at higher concentrations .In addition to monitoring the mean square displacement as a function of time we also measure the spread of displacements over numerous particles concurrently . This enables us to extract information about the underlying dynamics which is not possible by simply looking at the ensemble averaged MSD curve alone .By fitting our information to an appropriate model we are able to predict the concentration dependence of the diffusion coefficient D0 and the exponent α characterizing the power law decay of the probability density function P ( Δr ) . Our results show good agreement between theoretical and experiment when compared against predictions based upon the generalized Langevin equation ( GLE ) .These studies demonstrate how optical methods can be used to study difficult structures such as concentrated polymer applications where conventional light scattering methods could fail due to multiple scattering effects .",
        "rewrite_text": "In this study, we investigate the diffusion behavior of probe molecules within polymer solutions using advanced optical techniques. Our research focuses on two types of probes: fluorescently labeled polystyrene spheres (PS) and dye-doped poly(methyl methacrylate) (PMMA). We observe that at low concentrations, both PS and PMMA exhibit anomalous subdiffusive behavior, while at higher concentrations, they transition to typical diffusive behavior. To gain deeper insights into the diffusion dynamics, we monitor the mean square displacement (MSD) over time and concurrently measure the spread of displacements across multiple particles. This approach allows us to extract valuable information about the underlying dynamics that cannot be captured by analyzing the ensemble-averaged MSD curve alone.\n\nBy fitting our experimental data to a suitable theoretical model, we successfully predict the concentration dependence of the diffusion coefficient (D0) and the exponent (α) that characterizes the power law decay of the probability density function P(Δr). Our findings demonstrate a strong correlation between theoretical predictions and experimental results, particularly when compared to the generalized Langevin equation (GLE). This research highlights the efficacy of optical methods in studying complex structures, such as concentrated polymer solutions, where traditional light scattering techniques may encounter limitations due to multiple scattering effects. Overall, our work contributes to a better understanding of diffusion processes in polymer systems and showcases the potential of optical techniques in exploring challenging materials.",
        "ori-fast-z-score": 0.9622504486493763,
        "water-fast-z-score": 5.268044568716332,
        "rewrite-fast-z-score": 0.4508348173337161
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Geometry of curves with exceptional secant planes: linear series along the general curve .\nAbstract:\nThe present work is devoted to study some properties of curves in P^3 having an exceptional secant plane, i.e., such that there exists a line intersecting them at two points and not passing through any other point on the curve.  We give necessary conditions for a curve to have an exceptional secant plane (Proposition 1). Then we prove that if a curve has an exceptional secant plane then it lies on a quadric surface (Theorem 2).  Finally, using this result, we show how one can construct all possible families of curves with an exceptional secant plane by means of their associated linear systems (Theorems 3-7).\nKey words: Curve, Secant Plane, Quadric Surface. Mathematics Subject Classification (2000): 14C20, 14D10, 32S15. The research leading to these results was supported by the Russian Foundation for Basic Research under grant No. 02-01-00962-a",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Geometry of curves with outstanding secant planes : linear series along the general curve . Abstract : The present work is devoted to study some properties of curves in P ^ 3 having an exceptional secant plane , i . e . , such that there exists a line intersecting them at two points and not reaching through any other point on the curve .We get required requirements for a curve to have an exceptional secant plane ( Proposition 1 ) . Then we prove that if a curve has an exceptional secant plane then it lies on a quadric surface ( Theorem 2 ) .Finally , using this fact , we prove how one can build all possible families of curves with an exceptional secant plane by means of their associated continuous systems ( Theorems 3 - 7 ) . Key words : Curve , Secant Plane , Quadric Surface .Mathematics Subject Classification ( 2000 ) : 14C20 , 14D10 , 32S15 . The studies leading to these results was supported by the Russian Foundation for Basic Research under grant No .02-01-00962-a",
        "rewrite_text": "Title: Geometry of Curves with Exceptional Secant Planes: Linear Series Along General Curves\n\nAbstract: This article investigates the geometric properties of curves in projective space \\( \\mathbb{P}^3 \\) that possess an exceptional secant plane. An exceptional secant plane is defined as one that intersects the curve at exactly two points without passing through any additional points on the curve. We establish the necessary conditions for a curve to exhibit this property, articulated in Proposition 1. Following this, we demonstrate that any curve with an exceptional secant plane must reside on a quadric surface, as outlined in Theorem 2. This foundational result allows us to explore the construction of various families of curves characterized by exceptional secant planes. We achieve this through the analysis of their associated continuous systems, leading to a series of results detailed in Theorems 3 to 7. The findings contribute to a deeper understanding of the interplay between curves and their secant planes within the context of algebraic geometry. The research presented here was supported by the Russian Foundation for Basic Research under grant No. 02-01-00962-a. \n\nKeywords: Curve, Secant Plane, Quadric Surface.  \nMathematics Subject Classification (2000): 14C20, 14D10, 32S15.",
        "ori-fast-z-score": 0.1259881576697424,
        "water-fast-z-score": 3.5,
        "rewrite-fast-z-score": 0.9701425001453319
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  High resolution radio continuum survey of M33: I. The radio maps .\nAbstract:\nWe present new high-resolution (0.5 arcsec) radio continuum observations at 1.4 GHz and 4.8 GHz made with the Australia Telescope Compact Array (ATCA). These data are combined to produce images in Stokes parameters I, Q, U and V which allow us to investigate both total intensity emission as well as linear polarization properties across the face of this nearby spiral galaxy. We find that the polarized emission is concentrated along the brightest regions of the disk where it reaches values up to ~8%. In addition we detect significant circularly polarized emission associated with two supernova remnants located near the center of the galaxy. This work represents an important step towards understanding magnetic field structure on kiloparsec scales within galaxies. It also provides valuable information for future studies aimed at investigating cosmic ray transport processes through galactic disks. \n \n Keywords: Radio astronomy, Galaxy evolution, Magnetic fields, Polarization",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : High resolution signals continuum study of M33 : I . The radio mapping .Abstract : We report new high - resolution ( 0 . 5 arcsec ) radio continuum measurements at 1 . 4 GHz and 4 . 8 GHz made with the Australia Telescope Compact Array ( ATCA ) . These data are coupled to produce images in Stokes variables I , Q , U and V which allow us to examine both maximum frequency emission as well as linear polarization properties across the face of this adjacent spiral galaxy .We see that the polarized emission is confined along the brightest parts of the disk where it hits values up to ~ 8 % . In addition we find considerable circularly polarized emission associated with two supernova remnants situated near the center of the galaxy .This research constitutes an important milestone towards studying magnetic field stability on kiloparsec scales within stars . It additionally offers important information for future research targeted at studying cosmic ray flow processes through galactic disks .Keywords : Radio astronomy , Galaxy evolution , Magnetic fields , Polarization",
        "rewrite_text": "**Title:** High-Resolution Continuum Study of M33: I. Radio Mapping\n\n**Abstract:** In this study, we present new high-resolution radio continuum observations of the spiral galaxy M33, utilizing the Australia Telescope Compact Array (ATCA) at frequencies of 1.4 GHz and 4.8 GHz, achieving a resolution of 0.5 arcseconds. The data collected have been processed to generate images in Stokes parameters I, Q, U, and V, which facilitate a comprehensive analysis of both the maximum frequency emissions and the linear polarization characteristics across the galaxy. Our findings reveal that polarized emissions are predominantly concentrated along the brightest regions of the galactic disk, reaching polarization levels of approximately 8%. Furthermore, we observe significant circularly polarized emissions linked to two supernova remnants located near the galaxy's center. This research marks a significant advancement in our understanding of magnetic field stability on kiloparsec scales within stellar environments. Additionally, it provides critical insights for future investigations focused on the dynamics of cosmic ray propagation through galactic disks. The implications of our findings extend to the broader fields of radio astronomy, galaxy evolution, and the study of magnetic fields and polarization phenomena in astrophysics. \n\n**Keywords:** Radio astronomy, galaxy evolution, magnetic fields, polarization.",
        "ori-fast-z-score": -2.23606797749979,
        "water-fast-z-score": 4.555555555555555,
        "rewrite-fast-z-score": 0.21320071635561041
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Multiepoch Radial Velocity Observations of L Dwarfs .\nAbstract:\nWe present the results of multiepoch radial velocity (RV) observations for four late-type M dwarfs and three brown dwarf candidates with spectral types ranging between M3V-M8V, obtained using the High Accuracy Radial velocity Planet Searcher (HARPS). We find no evidence for RV variability in any of these targets at levels greater than ~1 m/s over timescales up to several years.  These results are consistent with previous studies that have found little or no evidence for giant planets around low-mass stars and substellar objects. The lack of detectable RV variations is likely due to the small masses and/or radii of the companions expected if they exist. However, we cannot rule out the possibility that some of our targets harbor terrestrial mass planets on orbits with periods longer than those probed by our current data set. Future high-precision RV measurements will be required to determine whether such planets do indeed exist.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Multiepoch Radial Velocity Observations of L Dwarfs . Abstract : We report the results of multiepoch radial speed ( RV ) observations for four late - class M dwarfs and three brown giant candidates with spectral classes ranging between M3V - M8V , obtained using the High Accuracy Radial velocity Planet Searcher ( HARPS ) .We see no evidence for RV variability in any of these targets at levels greater than ~ 1 m / s over timescales up to several decades . These conclusions are compatible with previous research that have discovered nothing or no evidence for big planets around low - density stars and substellar objects .The absence of detectable RV variations is probably due to the small masses and / or radii of the companions expected if they exist . However , we cannot block out the suggestion that some of our objectives harbor terrestrial mass stars on orbits with periods longer than those probed by our latest data set .Future high - precision RV measurements will be required to test whether such planets do actually appear .",
        "rewrite_text": "We present the findings from our multiepoch radial velocity (RV) observations conducted on four late-type M dwarfs and three candidates for brown giants, with spectral classifications ranging from M3V to M8V. These observations were carried out using the High Accuracy Radial velocity Planet Searcher (HARPS) instrument. Our analysis reveals no significant RV variability exceeding approximately 1 m/s across these targets, even when observed over timescales extending to several decades. This lack of variability aligns with previous studies that have similarly found no substantial evidence for the presence of large planets orbiting low-density stars and substellar objects. The absence of detectable RV fluctuations may be attributed to the relatively small masses and/or radii of any potential companions, should they exist. Nonetheless, we cannot entirely dismiss the possibility that some of the stars in our study may host terrestrial-mass planets on orbits with periods longer than those examined in our current dataset. To conclusively determine the existence of such planets, future high-precision RV measurements will be essential. These upcoming observations will provide a deeper understanding of the planetary systems around these late-type stars and could potentially reveal the presence of smaller, yet significant, companions that have thus far eluded detection.",
        "ori-fast-z-score": -1.2701705922171767,
        "water-fast-z-score": 4.672383634845156,
        "rewrite-fast-z-score": -0.10153461651336192
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Isophotal Structure and Dust Distribution in Radio-Loud Elliptical Galaxies .\nAbstract:\nWe present new near-infrared (NIR) observations for the radio-loud elliptical galaxy NGC 4261, obtained with the Subaru telescope. The NIR images reveal that this galaxy has an extended dust disk around its nucleus. We find that the isophotes are well fitted by a de Vaucouleurs profile plus an exponential component at large radii. This suggests that there may be two components contributing to the surface brightness distribution; one is associated with the bulge/disk system while another is related to the dust disk. In addition, we detect a faint ring-like structure surrounding the central region. These results suggest that the dust disk is likely to have been formed through tidal interaction between the host galaxy and a companion galaxy. Our analysis also shows that the dust mass within the innermost 100 pc radius is about 1.5 x 10^6 M_sol. If we assume that the dust-to-gas ratio is similar to Galactic value, then the total gas mass would be 5 x 10^8 M_sol.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Isophotal Structure and Dust Distribution in Radio - Loud Elliptical Galaxies . Abstract : We report new near - infrared ( NIR ) observations for the radio - loud elliptical galaxy NGC 4261 , obtained with the Subaru observatory .The NIR images confirm that this galaxy has an extended dust disk around its core . We see that the isophotes are better fitted by a de Vaucouleurs profile plus an exponential component at large radii .This implies that there may be two systems causing to the surface brightness distribution ; one is associated with the bulge / disk system while another is related to the dust disk . In addition , we perceive a faint ring - like structure surrounding the main region .These data suggest that the dust disk is probably to have been formed through tidal association between the host universe and a companion galaxy . Our study also shows that the dust mass within the innermost 100 pc radius is about 1 . 5 x 10 ^ 6 M _ sol .If we suppose that the dust - to - gas ratio is identical to Galactic value , then the total gas mass would be 5 x 10 ^ 8 M _ sol .",
        "rewrite_text": "We present new near-infrared (NIR) observations of the radio-loud elliptical galaxy NGC 4261, conducted at the Subaru Observatory. Our findings reveal the presence of an extensive dust disk surrounding the galaxy's core, as confirmed by the NIR imaging. The analysis of the isophotal structure indicates that the surface brightness distribution is best described by a combination of a de Vaucouleurs profile and an exponential component at larger radii. This suggests the existence of two distinct systems contributing to the overall brightness: one associated with the bulge/disk structure and another linked to the dust disk itself. Additionally, we have identified a faint ring-like feature encircling the central region of the galaxy. These observations imply that the formation of the dust disk may be a result of tidal interactions between NGC 4261 and a nearby companion galaxy. Furthermore, our calculations estimate the dust mass within the innermost 100 parsecs to be approximately 1.5 x 10^6 solar masses. Assuming a dust-to-gas ratio consistent with that of the Milky Way, we infer that the total gas mass in this region is around 5 x 10^8 solar masses. This study enhances our understanding of the structural components and dust distribution in radio-loud elliptical galaxies, providing insights into their formation and evolution processes.",
        "ori-fast-z-score": 0.8944271909999159,
        "water-fast-z-score": 5.222222222222222,
        "rewrite-fast-z-score": 0.9138115486202573
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Discovery of 1000 km/s Outflows in Massive Post-starburst Galaxies at z=0.6 .\nAbstract:\nWe report the discovery of massive outflows (1000 km s-1) in two post-starburst galaxies, which are thought to be progenitors of present-day elliptical galaxies. The observations were made with Subaru/FOCAS and VLT/VIMOS using near-infrared spectroscopy covering rest-frame optical emission lines for these objects at redshift 0.6. We found that both galaxies show strong  O III 5007Å/Hβ ratios indicating high ionization parameters. In addition, we detected blueshifted high-velocity components in Hα profiles as well as broad wings on their Hα line profiles. These results suggest that there is an energetic ionized gas flow driven by AGN and/or supernovae activity. This work was supported by Grants-in-Aid for Scientific Research from MEXT Japan (No. 20540230). Keywords: Emission-line galaxy, Galaxy evolution, Nearby galaxy",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Discovery of 1000 cm / s Outflows in Massive Post - starburst Galaxies at z = 0 . 6 . Abstract : We report the discovery of large outflows ( 1000 km s - 1 ) in two post - starburst objects , which are considered to be progenitors of present - day elliptical galaxies .The images were made with Subaru / FOCAS and VLT / VIMOS using near - infrared spectroscopy covering rest - frame optical emission lines for these objects at redshift 0 . 6 . We showed that both galaxies show good O III 5007Å / Hβ ratios suggesting high ionization values .In addition , we identified blueshifted high - speed elements in Hα profiles as well as wide wings on their Hα line profiles . These data suggest that there is an energetic ionized gas stream generated by AGN and / or supernovae activity .This project was supported by Grants - in - Aid for Scientific Research from MEXT Japan ( No . 20540230 ) .Keywords : Emission - line galaxy , Galaxy evolve , Nearby galaxy",
        "rewrite_text": "We present our findings on the detection of significant outflows, reaching speeds of 1000 km/s, in two post-starburst galaxies, which are believed to be the precursors to contemporary elliptical galaxies. Utilizing advanced imaging techniques with Subaru's FOCAS and VLT's VIMOS, we conducted near-infrared spectroscopy that captures the rest-frame optical emission lines of these galaxies at a redshift of 0.6. Our analysis revealed that both galaxies exhibit elevated O III 5007Å to Hβ ratios, indicating a high level of ionization. Furthermore, we observed blueshifted high-velocity components in the Hα emission profiles, along with broad wings in the Hα line profiles. These observations imply the presence of a dynamic and energetic outflow of ionized gas, likely driven by activity from an active galactic nucleus (AGN) and/or supernova events. This research contributes to our understanding of galaxy evolution and the processes that shape the characteristics of galaxies in the universe. The project received funding through Grants-in-Aid for Scientific Research from MEXT Japan (No. 20540230). Our findings are significant for the study of emission-line galaxies and the evolutionary pathways of nearby galaxies. \n\nKeywords: Emission-line galaxy, Galaxy evolution, Nearby galaxy.",
        "ori-fast-z-score": -0.6401843996644799,
        "water-fast-z-score": 4.064004064006096,
        "rewrite-fast-z-score": 0.8728715609439696
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A unified shear-thinning treatment of both film thickness and traction in EHD .\nAbstract:\nWe present an extension to the classical lubrication theory that accounts for shear thinning effects on both the fluid viscosity and the friction coefficient, which are assumed to be functions of local pressure only.  The resulting model is applied to study the effect of non-Newtonian rheology on the elastohydrodynamic (EHD) behavior of a circular contact between two elastic solids with different surface roughnesses under pure rolling conditions.  We show that the presence of shear thinning leads to significant changes in the predicted load-carrying capacity as well as the distribution of the normal stress across the contact area compared to those obtained using Newtonian models.  In particular, we find that the maximum value of the dimensionless pressure increases significantly when the fluids exhibit strong shear thinning characteristics.  Moreover, our results indicate that the inclusion of shear thinning effects can lead to substantial reductions in the magnitude of the dimensionless tangential stresses at the centerline of the contact region.  Finally, it should be noted that the proposed theoretical framework may also be used to investigate other important phenomena such as thermal effects or mixed lubrication regimes.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A unified shear - thinning application of both film thickness and traction in EHD . Abstract : We present an extension to the classical lubrication theory that accounts for shear thinning effects on both the liquid viscosity and the tension coefficient , which are assumed to be functions of local pressure only .The resulting theory is applied to study the impact of non - Newtonian rheology on the elastohydrodynamic ( EHD ) behavior of a circular contact between two elastic solids with varying surface roughnesses under pure sliding conditions . We see that the presence of shear thinning leads to significant improvements in the expected load - holding capacity as well as the spread of the usual stress across the contact area compared to those achieved using Newtonian methods .In particular , we find that the maximum value of the dimensionless pressure increases substantially when the fluids present strong shear thinning characteristics . Moreover , our findings confirm that the introduction of shear thinning effects can lead to substantial reductions in the severity of the dimensionless tangential stresses at the centerline of the contact region .Finally , it should be mentioned that the suggested theoretical framework may also be used to examine other vital effects such as heat effects or mixed lubrication regimes .",
        "rewrite_text": "Title: A Unified Shear-Thinning Approach to Film Thickness and Traction in EHD\n\nAbstract: In this study, we propose an enhancement to classical lubrication theory by incorporating shear-thinning effects into the analysis of liquid viscosity and traction coefficients, both of which are treated as functions of local pressure. This advanced theoretical framework is employed to investigate the influence of non-Newtonian rheology on the elastohydrodynamic (EHD) performance of circular contacts between two elastic solids, characterized by varying surface roughness, under conditions of pure sliding. Our results indicate that shear thinning significantly enhances the load-holding capacity and alters the distribution of stress across the contact area when compared to traditional Newtonian approaches. Notably, we observe a considerable increase in the maximum dimensionless pressure when the fluids exhibit pronounced shear-thinning behavior. Additionally, our analysis reveals that incorporating shear-thinning effects can lead to marked reductions in the intensity of dimensionless tangential stresses at the centerline of the contact area. This suggests that shear-thinning fluids can mitigate stress concentrations, potentially improving the durability and performance of lubricated contacts. Furthermore, the theoretical framework we present is versatile and can be adapted to explore other critical phenomena, such as thermal effects and mixed lubrication regimes. Our findings contribute to a deeper understanding of EHD lubrication in non-Newtonian contexts and highlight the importance of considering shear-thinning behavior in the design and analysis of lubricated systems.",
        "ori-fast-z-score": -0.9045340337332909,
        "water-fast-z-score": 4.875682232028947,
        "rewrite-fast-z-score": -0.09950371902099892
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Mapping the circumstellar SiO maser emission in R Leo .\nAbstract:\nWe present new maps of the circumstellar SiO (v=1, v=2) masers around the Mira variable R Leo obtained with the Very Long Baseline Array at 22 GHz and 43 GHz. The observations were carried out on September 24th 2004 using all ten antennas available for VLBA operation during that time period. We detect two distinct groups of masers: one group is located near the star s position as determined by optical astrometry; another group lies about 0.5 arcsec to the south-west of this location. Both groups are associated with an extended bipolar structure seen in previous single-dish observations. This structure has been interpreted as a shell-like envelope surrounding the central star. Our results show that both groups of masers trace different parts of this shell-like structure. In addition we find evidence for a third component which may be related to the presence of a companion object. \n \n Keywords: Masers",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Mapping the circumstellar SiO maser emission in R Leo . Abstract : We create fresh maps of the circumstellar SiO ( v = 1 , v = 2 ) masers around the Mira variable R Leo achieved with the Very Long Baseline Array at 22 GHz and 43 GHz .The observed were carried out on September 24th 2004 utilizing all ten antennas available for VLBA operation during that time period . We detect two different groups of masers : one group is situated near the star s position as determined by optical astrometry ; another group lies about 0 . 5 arcsec to the south - west of this location .Both groups are identified with an extended bipolar structure seen in earlier single - dish measurements . This structure has been described as a shell - like envelope surrounding the main star .Our results show that both bands of masers trace various parts of this shell - like structure . In addition we find proof for a third element which may be connected to the presence of a companion object .Keywords: Masers",
        "rewrite_text": "**Title:** Mapping the Circumstellar SiO Maser Emission in R Leo\n\n**Abstract:** In this study, we present new high-resolution maps of the circumstellar SiO maser emissions in the Mira variable star R Leo, utilizing data collected from the Very Long Baseline Array (VLBA) at frequencies of 22 GHz and 43 GHz. The observations were conducted on September 24, 2004, employing all ten antennas available for VLBA operations at that time. Our analysis reveals the presence of two distinct groups of masers. The first group is located in close proximity to the star's position, as determined through optical astrometry, while the second group is found approximately 0.5 arcseconds to the southwest of the star. Notably, both groups correspond to an extended bipolar structure previously identified in single-dish measurements, which has been characterized as a shell-like envelope surrounding R Leo. Our findings indicate that the masers in both frequency bands delineate different regions of this shell-like structure, providing insights into the circumstellar environment of the star. Furthermore, we present evidence for a potential third component that may be associated with the presence of a companion object. This research enhances our understanding of the complex maser emissions around R Leo and contributes to the broader knowledge of stellar environments in the context of variable stars. \n\n**Keywords:** Masers, R Leo, circumstellar environment, VLBA, Mira variable stars.",
        "ori-fast-z-score": 0.5933908290969266,
        "water-fast-z-score": 4.865804798594798,
        "rewrite-fast-z-score": 0.4216370213557839
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Evolutionary Neural Gas (ENG): A Model of Self Organizing Network from Input Categorization .\nAbstract:\nIn this work, we propose an evolutionary neural gas model for input categorization and clustering in the context of self-organizing networks. The proposed algorithm is based on the concept that each neuron can be considered as a cluster center with its own weight vector. In addition to updating their weights according to the standard NG learning rule, neurons are also allowed to evolve by applying genetic operators such as crossover and mutation. We show through experiments conducted on benchmark data sets that our approach outperforms other state-of-the-art algorithms including SOMs, GNGs, and EBGs. Finally, we demonstrate how the proposed method can be used to solve real-world problems such as text classification and image segmentation. Keywords: Evolutionary Computation, Self-Organizing Networks, Clustering, Genetic Algorithms, Text Classification, Image Segmentation. 1 Introduction Self-organizing maps (SOMs), originally introduced by Kohonen  1  , have been widely applied in many fields ranging from pattern recognition  2  , speech processing  3  , computer vision  4  , bioinformatics  5  , etc.. However, one major drawback of traditional SOMs lies in the fact that they require users to specify the number of clusters beforehand  6  . To overcome this problem, several extensions of SOMs were developed  7, 8  .\nAmong these extensions, growing neural gas (GNG)  9  has attracted much attention due to its ability to automatically determine the optimal number of clusters during training  10  . Nevertheless, it should be noted that most existing models of self organizing network suffer from two main limitations. First, all nodes in the network share the same set of parameters which makes them unable to capture different characteristics of various categories  11  . Second, there lacks any mechanism to prevent overfitting when dealing with high-dimensional data  12  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Evolutionary Neural Gas ( ENG ) : A Model of Self Organizing Network from Input Categorization . Abstract : In this research , we propose an evolutionary neural gas model for input categorization and clustering in the context of self - organizing organizations .The proposed algorithm is based on the idea that each neuron can be regarded as a cluster center with its own weight vector . In addition to updating their weights based to the standard NG learning principle , neurons are also allowed to evolve by using genetic operators such as crossover and mutation .We see through experiments conducted on benchmark data sets that our approach outperforms other state - of - the - art algorithms including SOMs , GNGs , and EBGs . Finally , we prove how the suggested method can be used to solve real - time difficulties such as text classification and visual segmentation .Keywords : Evolutionary Computation , Self - Organizing Networks , Clustering , Genetic Algorithms , Text Classification , Image Segmentation . 1 Introduction Self - organizing maps ( SOMs ) , previously introduced by Kohonen 1 , have been widely applied in multiple fields ranging from pattern recognition 2 , speech editing 3 , computer vision 4 , bioinformatics 5 , etc . .However , one major drawback of typical SOMs exists in the fact that they use users to define the number of clusters beforehand 6 . To solve this situation , various extensions of SOMs were developed 7 , 8 .Among these extensions , growing brain gas ( GNG ) 9 has drew much attention due to its able to automatically predict the ideal amount of clusters during training 10 . Nevertheless , it should be mentioned that most existing models of self organizing system fail from two principal constraints .First , all nodes in the network share the same list of parameters which makes them unable to capture specific traits of several classes 11 . Second , there lacks any mechanism to minimize overfitting when dealing with high - dimensional data 12 .",
        "rewrite_text": "**Title:** Evolutionary Neural Gas (ENG): A Self-Organizing Network Model for Input Categorization\n\n**Abstract:** This study introduces the Evolutionary Neural Gas (ENG) model, designed for input categorization and clustering within self-organizing systems. The ENG framework conceptualizes each neuron as a cluster center, equipped with its own weight vector. In addition to the conventional weight updates dictated by the standard Neural Gas (NG) learning principle, our model incorporates evolutionary mechanisms, allowing neurons to undergo genetic operations such as crossover and mutation. Through extensive experiments on benchmark datasets, we demonstrate that ENG significantly outperforms several leading algorithms, including Self-Organizing Maps (SOMs), Growing Neural Gas (GNG), and Elastic Bunch Graphs (EBG). Furthermore, we illustrate the practical applicability of our method in addressing real-time challenges, particularly in text classification and image segmentation tasks. \n\nThe introduction of Self-Organizing Maps (SOMs) by Kohonen marked a pivotal advancement in various domains, including pattern recognition, speech processing, computer vision, and bioinformatics. However, a notable limitation of traditional SOMs is their reliance on users to predefine the number of clusters, which can lead to suboptimal performance. To mitigate this issue, several extensions of SOMs have been proposed, with GNG gaining prominence for its ability to autonomously determine the optimal number of clusters during the training phase. Despite these advancements, existing self-organizing models often encounter two significant challenges: firstly, all nodes within the network share identical parameters, hindering their ability to capture the unique characteristics of different classes; secondly, there is a lack of effective mechanisms to reduce overfitting when processing high-dimensional data. Our ENG model addresses these limitations, offering a robust solution for dynamic clustering and categorization tasks in complex datasets.\n\n**Keywords:** Evolutionary Computation, Self-Organizing Networks, Clustering, Genetic Algorithms, Text Classification, Image Segmentation.",
        "ori-fast-z-score": 0.5980503604017327,
        "water-fast-z-score": 7.831560082980487,
        "rewrite-fast-z-score": -0.3563483225498992
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Power Allocation for Discrete-Input Delay-Limited Fading Channels .\nAbstract:\nWe consider the problem of power allocation in discrete-input delay-limited fading channels with multiple users and time-varying channel gains. We propose an iterative algorithm to solve this problem by using convex optimization techniques, which is shown to converge within finite number of iterations under certain conditions. The proposed algorithm can be implemented efficiently through parallel processing at each iteration step. Numerical results show that our proposed scheme outperforms existing schemes significantly. \n \n Keywords: Power control; Convex optimization; Time-varying; Multiple access channels (MACs); Wireless communications; Iterative algorithms. 1 Introduction \n \n In wireless communication systems, it has been well recognized that the performance of multi-user transmission depends on how the available resources are allocated among different users  1  . For example, when there exist multiple users sharing a common radio resource such as bandwidth or transmit power, the optimal way to allocate these resources may depend on the specific system settings  2  , e.g., whether the users have equal priority  3  , what type of services they request  4  , etc.. Therefore, efficient resource allocation strategies should take into account all relevant factors so as to maximize overall network utility  5  .\n \nIn recent years, considerable research efforts have been devoted to studying various aspects of resource allocation problems  6  -  8  . Among them, power allocation plays an important role due to its direct impact on both spectral efficiency and energy consumption  9  . However, most previous works assume continuous input alphabets  10  -  12  , while practical digital modulation schemes usually employ discrete constellations  13  . As a result, the conventional approaches cannot be directly applied to discrete-input scenarios  14  . To address this issue, several studies  15  -  17  have investigated the power allocation problem over discrete-input channels recently. Nevertheless, their solutions either require high computational complexity  16  or suffer from slow convergence speed  17  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Power Allocation for Discrete - Input Delay - Limited Fading Channels . Abstract : We consider the issue of power distribution in discrete - input delay - limited fading channels with many users and period - differing channel gains .We suggest an iterative algorithm to solve this question by using convex optimization technology , which is demonstrated to converge within finite number of iterations under certain conditions . The proposed algorithm can be applied efficiently through concurrent processing at each iteration step .Numerical results show that our proposed system outperforms previous schemes considerably . Keywords : Power control ; Convex optimization ; Time - changing ; Multiple access outlets ( MACs ) ; Wireless communications ; Iterative techniques .1 Introduction In wireless communication technologies , it has been widely recognized that the performance of multi - person transmission relies on how the provided resources are assigned among different users 1 . For instance , when there reside several users sharing a common radio asset such as bandwidth or transmit energy , the ideal means to allocate these resources may depend on the specific system settings 2 , e . g . , whether the operators have equal priority 3 , what type of solutions they demand 4 , etc . .Therefore , efficient resource expenditure strategies should take into consideration all relevant variables so as to maximize overall network utility 5 . In recent years , substantial work programs have been focused to researching various parts of resource transfer problems 6 - 8 .Among them , electricity allocation takes an important role owing to its significant effect on both spectral capacity and energy consumption 9 . However , most prior efforts assume continuous output alphabets 10 - 12 , while commercial digital modulation schemes typically employ discrete constellations 13 .As a result , the usual techniques unable be effectively used to discrete - input situations 14 . To address this question , various studies 15 - 17 have analyzed the power allocation problem over discrete - input channels recently .Nevertheless , their solutions either need strong mathematical capacity 16 or suffer from slow convergence speed 17 .",
        "rewrite_text": "**Title:** Power Allocation for Discrete-Input Delay-Limited Fading Channels\n\n**Abstract:** This article addresses the challenge of power allocation in discrete-input delay-limited fading channels characterized by multiple users and varying channel gains. We propose an innovative iterative algorithm that leverages convex optimization techniques to effectively tackle this issue. Our approach is shown to converge within a finite number of iterations under specific conditions, ensuring efficiency in the power allocation process. The algorithm is designed for concurrent processing at each iteration step, which enhances its operational efficiency. Through extensive numerical simulations, we demonstrate that our proposed method significantly outperforms existing power allocation schemes, highlighting its effectiveness in optimizing resource distribution in complex wireless communication environments. \n\nIn the realm of wireless communication, the performance of multi-user transmissions is heavily influenced by how resources, such as bandwidth and transmit power, are allocated among users. The optimal resource allocation strategy is contingent upon various factors, including user priority and specific service requirements. Therefore, it is crucial to develop efficient resource allocation strategies that consider all relevant parameters to maximize overall network utility. Recent research has made substantial strides in exploring different aspects of resource allocation, with a particular emphasis on power distribution due to its critical impact on both spectral efficiency and energy consumption. \n\nHowever, most existing studies have focused on continuous output alphabets, while practical digital modulation techniques predominantly utilize discrete constellations. This discrepancy renders traditional methods ineffective for discrete-input scenarios. To fill this gap, recent investigations have sought to analyze power allocation in discrete-input channels. Nonetheless, the solutions proposed in these studies often require complex mathematical frameworks or exhibit slow convergence rates. Our work aims to overcome these limitations by providing a robust and efficient algorithm for power allocation in discrete-input delay-limited fading channels, thereby contributing to the advancement of resource management in wireless communications. \n\n**Keywords:** Power control; Convex optimization; Time-varying channels; Multiple access channels (MACs); Wireless communications; Iterative techniques.",
        "ori-fast-z-score": -1.7386365758424454,
        "water-fast-z-score": 8.894477603748037,
        "rewrite-fast-z-score": -1.2236867762727808
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Rates and Characteristics of Intermediate Mass Ratio Inspirals Detectable by Advanced LIGO .\nAbstract:\nWe present the results of an analysis to determine rates, masses, spins, and luminosities for intermediate mass ratio inspiral (IMRI) events detectable with advanced gravitational wave detectors such as Advanced LIGO. We use Monte Carlo simulations to generate IMRIs in galactic binaries that are consistent with current observations of binary pulsars and X-ray binaries. The simulated systems evolve through three phases: detached phase, Roche lobe overflow phase, and common envelope phase. In our simulation we assume that all stars have solar metallicity and initial spin periods of 10 days. For each system generated, we calculate its signal-to-noise ratio using the stationary phase approximation. We find that there will be about one event per year within 100 Mpc with signal-to-noise ratios greater than 8. This is comparable to the rate expected for double neutron star mergers. However, unlike double neutron star mergers which occur at high redshifts, most IMRI events should be detected nearby.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Rates and Characteristics of Intermediate Mass Ratio Inspirals Detectable by Advanced LIGO . Abstract : We report the results of an assessment to obtain rates , masses , spins , and luminosities for intermediate mass ratio inspiral ( IMRI ) events detectable with advanced gravitational wave detectors such as Advanced LIGO .We use Monte Carlo simulations to produce IMRIs in galactic binaries that are compatible with current observations of binary pulsars and X - ray binaries . The simulated systems develop through three stages : detached phase , Roche lobe overflow phase , and shared envelope phase .In our model we suppose that all stars have solar metallicity and original spin intervals of 10 days . For each system generated , we determine its signal - to - noise proportion using the stationary phase approximation .We see that there will be about one incident per year within 100 Mpc with signal - to - noise ratios greater than 8 . This is analogous to the speed anticipated for double neutron star mergers .However , unlike dual neutron star mergers which occur at high redshifts , most IMRI events should be identified nearby .",
        "rewrite_text": "We present a comprehensive analysis of the rates, masses, spins, and luminosities associated with intermediate mass ratio inspiral (IMRI) events that can be detected by advanced gravitational wave observatories, such as Advanced LIGO. Utilizing Monte Carlo simulations, we model IMRIs within galactic binary systems that align with existing observations of binary pulsars and X-ray binaries. Our simulations encompass three distinct evolutionary phases: the detached phase, the Roche lobe overflow phase, and the common envelope phase. In our framework, we assume that all stars possess solar metallicity and initial spin periods of 10 days. For each generated binary system, we calculate the signal-to-noise ratio (SNR) using the stationary phase approximation. Our findings indicate that approximately one IMRI event per year is expected to occur within a distance of 100 Mpc, with SNRs exceeding 8. This rate is comparable to the anticipated frequency of double neutron star mergers. However, a key distinction is that while double neutron star mergers are typically detected at high redshifts, the majority of IMRI events are likely to be observed in closer proximity. This research enhances our understanding of IMRI events and their detectability, contributing valuable insights into the population of gravitational wave sources and the dynamics of binary star systems.",
        "ori-fast-z-score": 0.22645540682891913,
        "water-fast-z-score": 4.695742752749558,
        "rewrite-fast-z-score": 1.5230192477004287
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Baryon Fractions and Mass-to-Light Ratios of Early-Type Galaxies .\nAbstract:\nWe present new measurements of the baryonic mass fractions (f bar ) in early-type galaxies, based on spatially resolved kinematics for a sample of 12 nearby elliptical/S0 galaxies observed with integral field spectroscopy at optical wavelengths.  We use these data to measure f bar , as well as the total stellar masses M * . The results are compared against predictions from semi-analytic models of galaxy formation within the standard ΛCDM cosmology. Our main conclusions are:  1) For our sample we find that the mean value of f bar is 0.16 ± 0.04, which agrees very well with previous estimates obtained using different techniques. 2) There exists no significant correlation between f bar and either luminosity or velocity dispersion. 3) Semi-analytic models predict values of f bar that are systematically lower than those measured here by about a factor of two. 4) In order to match the observations, it appears necessary to invoke additional physical processes beyond those included in current models.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Baryon Fractions and Mass - to - Light Ratios of Early - Type Galaxies . Abstract : We report new studies of the baryonic mass fractions ( f bar ) in early - class stars , relying on spatially resolved kinematics for a sample of 12 nearby elliptical / S0 galaxies viewed with integral field spectroscopy at infrared wavelengths .We use these information to measure f bar , as well as the total stellar masses M * . The results are compared against predictions from semi - analytic models of galaxy formation within the standard ΛCDM cosmology .Our main results are : 1 ) For our sample we find that the mean value of f bar is 0 . 16 ± 0 . 04 , which agrees very best with previous estimates obtained using separate techniques . 2 ) There exists no considerable relationship between f bar and either luminosity or speed dispersion .3 ) Semi - analytic models predict values of f bar that are routinely lower than those observed here by about a factor of two . 4 ) In order to match the observations , it appears necessary to invoke additional physical processes beyond those included in current models .",
        "rewrite_text": "We present a comprehensive analysis of baryonic mass fractions (f_bar) in early-type galaxies, based on spatially resolved kinematics from a sample of 12 nearby elliptical and S0 galaxies observed through integral field spectroscopy in the infrared spectrum. Our study aims to accurately determine f_bar and the total stellar masses (M*) of these galaxies, facilitating a comparison with predictions derived from semi-analytic models of galaxy formation within the framework of the standard ΛCDM cosmology. \n\nOur findings reveal several key insights: Firstly, we calculate a mean baryonic mass fraction of f_bar = 0.16 ± 0.04 for our sample, which aligns closely with previous estimates obtained through various methodologies. Secondly, our analysis indicates no significant correlation between f_bar and either the luminosity or the velocity dispersion of the galaxies. Thirdly, we observe that semi-analytic models consistently underestimate f_bar, predicting values that are approximately half of what we have measured. This discrepancy suggests that the existing models may not fully account for the complexities of galaxy formation and evolution.\n\nTo reconcile these observations with theoretical predictions, it appears essential to consider additional physical processes that are currently overlooked in standard models. Our results emphasize the need for a refined understanding of the baryonic content in early-type galaxies and highlight the potential for new physics to enhance our comprehension of galaxy formation in the context of the ΛCDM framework. This study not only contributes to the ongoing discourse on the baryonic mass distribution in galaxies but also underscores the importance of observational data in guiding theoretical advancements in astrophysics.",
        "ori-fast-z-score": 2.251436323159369,
        "water-fast-z-score": 5.7564193416014815,
        "rewrite-fast-z-score": 2.928276481073176
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Droplets in the two-dimensional +-J spin glass: evidence for (non-) universality .\nAbstract:\nWe study droplet excitations in the 2D spin-glass model with nearest-neighbor interactions and random ferromagnetic bonds, which is known to have an infinite number of metastable states at zero temperature. We show that this system has two different types of droplets: small ones are similar to those found in other models studied previously; large droplets are characterized by their fractal structure. The latter type can be viewed as a generalization of the droplet picture proposed earlier for the 3D Ising spin glasses. In addition we find that there exists another class of excitations -the so-called  giant droplets -which are not present in any of these systems. These giant droplets are responsible for the non-universal behavior observed numerically near the critical point. Finally, we argue that our results provide strong numerical support for the existence of a new phase transition line between the paramagnetic state and the spin-glass one. \nI. INTRODUCTORY REMARK\nThe concept of  droplet excitations  was introduced originally within the framework of the mean-field theory  1  . It describes how local perturbations affect global properties of the system. This idea turned out to be very useful when applied to various disordered systems such as spin glasses  2  , structural glasses  3  or vortex lattices  4  .\nIn particular it allowed to explain many features of the low-temperature thermodynamics of spin glasses  5  . However, despite its successes, the original droplet picture suffers from some serious drawbacks  6  : first, it does not take into account fluctuations around the saddle-point solution  7 ; secondly, it predicts a finite density of droplets even at T = 0  8  ; thirdly, it cannot describe properly the dynamics of the system  9  . To overcome these difficulties several modifications were suggested  10  . One of them  11  leads to the following expression for the free energy F(T ) per site: \nwhere f0 is the free-energy density of the reference system (e.g., the pure ferromagnet), Ns is the total number of spins, V is the volume occupied by each droplet",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Droplets in the two - dimensional + - J spin mirror : evidence for ( non - ) universality . Abstract : We research droplet excitations in the 2D spinning - glass model with nearest - neighbor interactions and random ferromagnetic bonds , which is known to have an endless number of metastable states at zero temperature .We see that this scheme has two different kinds of droplets : tiny ones are related to those present in other models studied ago ; wide droplets are marked by their fractal structure . The latter type can be viewed as a generalization of the droplet picture suggested earlier for the 3D Ising spin glasses .In addition we find that there exists another class of excitations - the so - called giant droplets - which are not present in any of these systems . These huge droplets are responsible for the non - universal behavior observed numerically near the critical position .Finally , we claim that our findings provide strong mathematical support for the existence of a new phase shift line between the paramagnetic state and the spin - glass one . I .INTRODUCTORY REMARK The concept of droplet excitations was introduced originally within the framework of the mean - field principle 1 . It details how local perturbations impact global properties of the system .This idea turned out to be very useful when applied to numerous disordered systems such as spin glasses 2 , structural glasses 3 or vortex lattices 4 . In particular it able to explain different properties of the small - temperature thermodynamics of spin glasses 5 .However , despite its successes , the original droplet picture suffers from some serious drawbacks 6 : first , it does not take into consideration fluctuations around the saddle - point problem 7 ; secondly , it predicts a finite density of droplets even at T = 0 8 ; thirdly , it lacks explain adequately the dynamics of the system 9 . To solve these problems several amendments were recommended 10 .One of them 11 leads to the following expression for the free energy F ( T ) per site : where f0 is the free - energy density of the reference system ( e . g . , the pure ferromagnet ) , Ns is the total number of spinning , V is the volume occupied by each droplet",
        "rewrite_text": "**Title:** Droplets in the Two-Dimensional ±J Spin Mirror: Evidence for (Non) Universality\n\n**Abstract:** This study investigates droplet excitations within the two-dimensional ±J spin glass model characterized by nearest-neighbor interactions and random ferromagnetic bonds. This model is notable for possessing an infinite number of metastable states at absolute zero temperature. Our findings reveal the existence of two distinct types of droplets: small droplets that resemble those identified in previously studied models, and larger droplets distinguished by their fractal structure. The latter type serves as a generalization of the droplet framework proposed for three-dimensional Ising spin glasses. Furthermore, we identify a novel class of excitations termed \"giant droplets,\" which are absent in other systems. These giant droplets are pivotal in explaining the non-universal behavior observed in numerical simulations near the critical point. Our results lend substantial mathematical support to the hypothesis of a new phase transition line separating the paramagnetic phase from the spin-glass phase.\n\nThe concept of droplet excitations was initially introduced through the mean-field approach, which elucidates how localized perturbations can influence the overall characteristics of a system. This framework has proven beneficial in analyzing various disordered systems, including spin glasses, structural glasses, and vortex lattices. It has particularly contributed to understanding the small-temperature thermodynamics of spin glasses. However, the original droplet model has notable limitations: it overlooks fluctuations around saddle points, predicts a finite droplet density even at zero temperature, and inadequately addresses the system's dynamics. To address these shortcomings, several modifications have been proposed, one of which leads to a refined expression for the free energy per site. This expression incorporates the free energy density of a reference system, the total number of spins, and the volume occupied by each droplet, thereby enhancing the theoretical framework surrounding droplet excitations in disordered systems.",
        "ori-fast-z-score": 0.6446583712203042,
        "water-fast-z-score": 7.205766921228921,
        "rewrite-fast-z-score": 1.2456821978060995
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Geometric control theory I: mathematical foundations .\nAbstract:\nThe book is divided into three parts, each part containing several chapters. The first two parts are devoted to the development and analysis of geometric control systems in finite-dimensional spaces (the state space) and infinite-dimensional Hilbert spaces (the phase space). In particular, the following topics are considered:\n\nPart I - Geometric Control Theory in Finite-Dimensional Spaces.\n\nChapter 1 - Introduction to Geometric Control Theory.\n Chapter 2 - Basic Concepts of Differential Geometry.\n Chapter 3 - Lie Groups and Their Representations.\n Chapter 4 - Invariant Manifolds for Group Actions on Vector Fields.\n Chapter 5 - Stability Analysis of Nonlinear Systems with State Constraints.\n Chapter 6 - Stabilization by Feedback of Linear Time-Invariant Systems.\n Chapter 7 - Optimal Tracking Problems for Affine Systems.\n Part II - Geometric Control Theory on Infinite-Dimensional Hilbert Spaces.\n\n Chapter 8 - Generalized Euler-Lagrange Equations.\n Chapter 9 - Hamilton-Jacobi Equations.\n Chapter 10 - Pontryagin Maximum Principle.\n Chapter 11 - Optimal Control Problem for Discrete-Time Systems.\n Chapter 12 - Optimal Control Problem with Uncertain Dynamics.\n Chapter 13 - Optimal Control Problem under Stochastic Disturbances.\n Chapter 14 - Optimal Control Problem over Networks.\n Part III - Applications of Geometric Control Theory.\n\n Chapter 15 - Motion Planning for Mobile Robots.\n Chapter 16 - Robot Manipulation Tasks via Task Space Formulation.\n Chapter 17 - Visual Servoing Based on Image Feature Extraction.\n Chapter 18 - Model Predictive Control for Industrial Processes.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Geometric control theory I : geometric foundations . Abstract : The treatise is separated into three sections , each portion containing several chapters .The first two parts are devoted to the development and evaluation of geometric control networks in finite - dimensional spaces ( the state space ) and infinite - dimensional Hilbert spaces ( the phase space ) . In particular , the following topics are considered : Part I - Geometric Control Theory in Finite - Dimensional Spaces .Chapter 1 - Introduction to Geometric Control Theory . Chapter 2 - Basic Concepts of Differential Geometry .Chapter 3 - Lie Groups and Their Representations . Chapter 4 - Invariant Manifolds for Group Actions on Vector Fields .Chapter 5 - Stability Analysis of Nonlinear Systems with State Constraints . Chapter 6 - Stabilization by Feedback of Linear Time - Invariant Systems .Chapter 7 - Optimal Tracking Problems for Affine Systems.Part II - Geometric Control Theory on Infinite-Dimensional Hilbert Spaces.Chapter 8 - Generalized Euler - Lagrange Equations . Chapter 9 - Hamilton - Jacobi Equations .Chapter 10 - Pontryagin Maximum Principle . Chapter 11 - Optimal Control Problem for Discrete - Time Systems .Chapter 12 - Optimal Control Problem with Uncertain Dynamics . Chapter 13 - Optimal Control Problem under Stochastic Disturbances .Chapter 14 - Optimal Control Problem over Networks . Part III - Applications of Geometric Control Theory .Chapter 15 - Motion Planning for Mobile Robots . Chapter 16 - Robot Manipulation Tasks via Task Space Formulation .Chapter 17 - Visual Servoing Based on Image Feature Extraction . Chapter 18 - Model Predictive Control for Industrial Processes .",
        "rewrite_text": "This scientific article, titled \"Geometric Control Theory I: Geometric Foundations,\" is structured into three main sections, each comprising multiple chapters that delve into the intricacies of geometric control networks. The initial two sections focus on the formulation and analysis of geometric control systems within both finite-dimensional state spaces and infinite-dimensional Hilbert spaces, referred to as phase spaces. \n\nIn Part I, titled \"Geometric Control Theory in Finite-Dimensional Spaces,\" the reader is introduced to the fundamental principles of geometric control theory. Chapter 1 provides an overview of the field, while Chapter 2 lays the groundwork with essential concepts from differential geometry. Subsequent chapters explore the role of Lie groups and their representations (Chapter 3), the significance of invariant manifolds in the context of group actions on vector fields (Chapter 4), and the stability analysis of nonlinear systems constrained by state variables (Chapter 5). The section concludes with discussions on feedback stabilization of linear time-invariant systems (Chapter 6) and the challenges of optimal tracking for affine systems (Chapter 7).\n\nPart II shifts focus to \"Geometric Control Theory on Infinite-Dimensional Hilbert Spaces,\" where the complexities of control theory in more abstract settings are examined. This section begins with the generalized Euler-Lagrange equations (Chapter 8) and progresses through Hamilton-Jacobi equations (Chapter 9) and the Pontryagin Maximum Principle (Chapter 10). It further addresses optimal control problems in discrete-time systems (Chapter 11), those involving uncertain dynamics (Chapter 12), and scenarios impacted by stochastic disturbances (Chapter 13). The section culminates with an exploration of optimal control problems within networked systems (Chapter 14).\n\nFinally, Part III presents practical applications of geometric control theory, showcasing its relevance in various fields. This includes motion planning for mobile robots (Chapter 15), robot manipulation tasks framed within task space formulations (Chapter 16), visual servoing techniques based on image feature extraction (Chapter 17), and model predictive control strategies for industrial processes (Chapter 18). Through this comprehensive examination, the article aims to establish a robust foundation for further research and application in geometric control theory.",
        "ori-fast-z-score": -1.952833664712358,
        "water-fast-z-score": -0.24618298195866545,
        "rewrite-fast-z-score": -0.7324096128940435
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  SuprimeCam Observation of Sporadic Meteors during Perseids 2004 .\nAbstract:\nWe report on the observation of sporadic meteors with Suprime-Cam attached to Subaru Telescope in August, 2004. The total number of detected meteor events is about 12000 and their distribution over sky agrees well with that expected for sporadic meteors. We also found some interesting features such as clustering around bright stars (probably due to fragmentation) and an excess of faint meteors near the ecliptic plane. These results will be useful for understanding the physical processes involved in the formation of meteoroid streams. \n \n Keywords: Meteor shower, Suprime-Cam, Subaru telescope, Spacecraft debris \n \n \n \n Received September 30, 2005 Accepted December 16, 2005 Published January 31, 2006 Online publication date: February 3, 2006 \n \n We have observed sporadic meteors using Suprime-Cam mounted at the Cassegrain focus of the 8-meter Subaru Telescope in August 2004 when the Perseid meteor shower was active. About 12 000 meteor events were detected by our program which automatically detects moving objects in images taken every 20 seconds. Their spatial distribution shows good agreement with that predicted theoretically for sporadic meteors based on orbital elements obtained from radar observations. Some interesting features are also seen; e.g., clustering around bright stars probably caused by fragmentation or an excess of faint meteoroids near the ecliptic.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : SuprimeCam Observation of Sporadic Meteors during Perseids 2004 . Abstract : We report on the observation of sporadic meteors with Suprime - Cam attached to Subaru Telescope in August , 2004 .The total number of identified meteor events is about 12000 and their distribution over skies agrees well with that expected for sporadic meteors . We additionally found some interesting features such as clustering around bright stars ( probably due to fragmentation ) and an accumulation of bright meteors near the ecliptic plane .These conclusions will be valuable for studying the physical processes responsible in the formation of meteoroid streams . Keywords : Meteor shower , Suprime - Cam , Subaru observatory , Spacecraft debris Received September 30 , 2005 Accepted December 16 , 2005 Published January 31 , 2006 Online publication date : February 3 , 2006 We have discovered sporadic meteors using Suprime - Cam mounted at the Cassegrain scope of the 8 - meter Subaru Telescope in August 2004 when the Perseid meteor shower was active .About 12 000 meteor events were detected by our system which automatically detects moving objects in images took every 20 seconds . Their spatial distribution shows excellent compliance with that predicted theoretically for sporadic meteors based on orbital elements obtained from radar measurements .Some interesting features are also observed ; e . g . , clustering around bright stars probably due by fragmentation or an accumulation of bright meteoroids near the ecliptic .",
        "rewrite_text": "Title: SuprimeCam Observations of Sporadic Meteors During the Perseids 2004\n\nAbstract: This study presents findings from the observation of sporadic meteors conducted with the Suprime-Cam attached to the Subaru Telescope in August 2004, coinciding with the active period of the Perseid meteor shower. Our observations led to the identification of approximately 12,000 meteor events, with their spatial distribution aligning closely with theoretical predictions for sporadic meteors, as derived from radar measurements of their orbital elements. Notably, we observed intriguing phenomena, including a clustering of meteors around bright stars, which may suggest fragmentation processes, as well as a notable concentration of bright meteors in proximity to the ecliptic plane. These observations provide valuable insights into the physical mechanisms underlying the formation of meteoroid streams. The data collected during this period enhances our understanding of sporadic meteor behavior and contributes to the broader field of meteoritics. The findings were received on September 30, 2005, accepted on December 16, 2005, and subsequently published on January 31, 2006, with an online publication date of February 3, 2006. The keywords associated with this research include meteor shower, Suprime-Cam, Subaru Observatory, and spacecraft debris. This work not only sheds light on the characteristics of sporadic meteors but also emphasizes the importance of advanced observational techniques in unraveling the complexities of meteoroid dynamics.",
        "ori-fast-z-score": 1.078327732034384,
        "water-fast-z-score": 5.9696200579570915,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  STBCs from Representation of Extended Clifford Algebras .\nAbstract:\nIn this work, we present the design and analysis of space-time block codes (STBC) based on extended clifford algebras. The proposed STBC is constructed by using an orthogonal basis for the underlying algebra. We show that our code has full diversity order with respect to quasi-static Rayleigh fading channels. Furthermore, it achieves maximum coding gain over all other known algebraic STBCs in terms of minimum determinant criterion. Finally, we provide simulation results which demonstrate the performance gains achieved by the proposed scheme compared to existing schemes. Index Terms -Space time block codes, Algebraic coding theory, Quasi-static Rayleigh faded channel, Minimum determinant criterion. I. INTRODUCTIO N Space Time Block Codes (STBC), introduced by Alamouti  1  , are widely used in wireless communications due to their simple structure and high data rate  2  . In recent years there have been many efforts devoted towards designing new classes of STBCs  3  -  8  .\nThe main goal behind these designs was to achieve higher spectral efficiency while maintaining good error performances under different transmission conditions such as multipath propagation or frequency selective fading  9  . However, most of the existing works focus only on achieving better error performances without considering the effect of hardware limitations  10  . This motivates us to develop a class of STBCs which can be implemented efficiently at low complexity  11  .\nRecently, several authors  12  -  14  have shown that some well-known families of finite fields like Galois field GF(q)  15  , Finite Ring  16  , Quaternion  17  etc., can also be represented by certain types of non-commutative rings called Clifford algebras  18  . These representations allow one to construct various signal constellations  19  , modulation techniques  20  , and communication systems  21  . Motivated by these facts, in  22  , we presented a novel construction of STBCs based on representation of Clifford algebras. It was shown that the proposed STBC provides significant improvement in bit error rates (BER) when compared to conventional STBCs  23  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : STBCs from Representation of Extended Clifford Algebras . Abstract : In this project , we present the development and evaluation of space - time block codes ( STBC ) based on extended clifford algebras .The proposed STBC is built by using an orthogonal basis for the underlying algebra . We see that our code has full diversity order with regard to quasi - static Rayleigh fading sources .Furthermore , it achieves peak code performance over all other established algebraic STBCs in terms of minimum determinant requirement . Finally , we provide simulation data which demonstrate the performance increases attained by the suggested system compared to existing schemes .Index Terms - Space time block codes , Algebraic coding theory , Quasi - static Rayleigh faded signal , Minimum determinant requirement . I . INTRODUCTIO N Space Time Block Codes ( STBC ) , invented by Alamouti 1 , are widely useful in mobile communications thanks to their simple shape and large data rate 2 .In past decades there have been many efforts devoted towards creating new classes of STBCs 3 - 8 . The main goal behind these designs was to achieve greater spectral capacity while maintaining good error performances under various transmission circumstances such as multipath propagation or bandwidth selective fading 9 .However , most of the new works concentrate only on achieving better error performances without assessing the impact of hardware constraints 10 . This motivates us to develop a class of STBCs which can be executed easily at low complexity 11 .Recently , various scientists 12 - 14 have shown that some well - famous families of finite fields like Galois field GF ( q ) 15 , Finite Ring 16 , Quaternion 17 etc . , can also be described by certain types of non - commutative rings called Clifford algebras 18 . These representations allow one to build diverse signal constellations 19 , modulation technology 20 , and communication devices 21 .Motivated by these facts , in 22 , we presented a new construction of STBCs based on representation of Clifford algebras . It was shown that the suggested STBC offers substantial improvement in bit error patterns ( BER ) when compared to conventional STBCs 23 .",
        "rewrite_text": "**Title: STBCs Derived from the Representation of Extended Clifford Algebras**\n\n**Abstract:** This study focuses on the development and assessment of space-time block codes (STBCs) that are constructed using extended Clifford algebras. The proposed STBC leverages an orthogonal basis derived from the underlying algebra, ensuring that it achieves full diversity order in the context of quasi-static Rayleigh fading channels. Notably, our code outperforms existing algebraic STBCs in terms of the minimum determinant requirement, which is a critical factor for ensuring reliable communication. We present comprehensive simulation results that illustrate the performance enhancements of our proposed system when compared to traditional STBC schemes. \n\nSpace-Time Block Codes, initially introduced by Alamouti, have gained significant traction in mobile communication systems due to their straightforward implementation and ability to support high data rates. Over the past few decades, extensive research has been dedicated to the creation of novel STBC classes aimed at maximizing spectral efficiency while ensuring robust error performance across various transmission scenarios, including multipath propagation and bandwidth-selective fading. However, many of these recent developments have primarily focused on improving error rates without adequately considering the implications of hardware limitations. This gap in research has prompted us to design a new class of STBCs that can be implemented with low complexity.\n\nRecent investigations have revealed that well-known finite field families, such as Galois fields, finite rings, and quaternions, can be represented through specific non-commutative rings known as Clifford algebras. This representation opens up new avenues for constructing diverse signal constellations, modulation techniques, and communication systems. Building on these insights, we introduced a novel construction of STBCs based on the representation of Clifford algebras, demonstrating significant improvements in bit error rate (BER) performance compared to conventional STBCs. Our findings contribute to the ongoing evolution of STBC design, emphasizing the importance of both performance and implementation feasibility in modern communication systems. \n\n**Index Terms:** Space-Time Block Codes, Algebraic Coding Theory, Quasi-Static Rayleigh Fading, Minimum Determinant Requirement.",
        "ori-fast-z-score": -0.39405520311955033,
        "water-fast-z-score": 7.8628393360313815,
        "rewrite-fast-z-score": 0.7092993656151906
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Witnessing the formation of a galaxy cluster at z=0.485: optical and X-ray properties of RX J1117.4+0743 ( VMF 98  097) .\nAbstract:\nWe present new observations of the distant galaxy cluster RX J1117.4+07431, which was discovered in the ROSAT All-Sky Survey data by Voges et al. (1999) . The cluster is located at redshift z = 0.485 ± 0.001 with an estimated mass M500 = 1.7 × 1013 h-1M⊙ within r500 = 2.1h-1Mpc . We have obtained deep optical images using Suprime-Cam on Subaru telescope to study its member galaxies. In addition we observed this cluster with Chandra ACIS-I for about 50 ks. Our results are as follows:  -The color-magnitude diagram shows that there exists a red sequence of early-type galaxies down to our limiting magnitude RAB=25 mag.  -From the photometric redshift analysis, we find that the number density profile of the member galaxies follows well the NFW model prediction up to 3 virial radii. -The temperature map derived from the Chandra observation reveals two hot spots near the center of the cluster. These features may be associated with shock heating due to merging activity between sub-clusters or groups.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Witnessing the formation of a galaxy cluster at z = 0 . 485 : optical and X - ray properties of RX J1117 . 4 + 0743 ( VMF 98 097 ) . Abstract : We present new observations of the distant galaxy cluster RX J1117 . 4 + 07431 , which was discovered in the ROSAT All - Sky Survey data by Voges et al .( 1999 ) . The cluster is located at redshift z = 0 . 485 ± 0 . 001 with an estimated mass M500 = 1 . 7 × 1013 h - [UNK] within r500 = 2 . 1h - 1Mpc .We have achieved dark optical images using Suprime - Cam on Subaru observatory to study its member galaxies . In addition we examined this cluster with Chandra ACIS - I for about 50 ks .Our results are as follows : - The color - magnitude diagram reveals that there exists a red series of early - class stars down to our limiting magnitude RAB = 25 mag . - From the photometric redshift investigation , we find that the number density profile of the member galaxies shows well the NFW model prediction up to 3 virial radii .- The temperature diagram derived from the Chandra observation shows two hot areas near the center of the cluster . These features could be involved with shock heating due to merging behavior between sub - complexes or bands .",
        "rewrite_text": "We present new findings from our observations of the distant galaxy cluster RX J1117.4+0743, initially identified in the ROSAT All-Sky Survey by Voges et al. (1999). This cluster is situated at a redshift of z = 0.485 ± 0.001 and has an estimated mass of M500 = 1.7 × 10^13 h^-1 within a radius of r500 = 2.1 h^-1 Mpc. Utilizing the Suprime-Cam at the Subaru Observatory, we obtained deep optical images to analyze the member galaxies of the cluster. Additionally, we conducted a 50 ks observation with the Chandra ACIS-I, which provided valuable X-ray data. \n\nOur analysis yielded several significant results. The color-magnitude diagram indicates the presence of a distinct red sequence of early-type galaxies, extending down to our limiting magnitude of RAB = 25 mag. Furthermore, our investigation into photometric redshifts revealed that the number density profile of the member galaxies aligns well with the predictions of the Navarro-Frenk-White (NFW) model, extending up to three virial radii. \n\nMoreover, the temperature map derived from the Chandra observations highlights two hot regions near the cluster's center. These elevated temperature areas may be indicative of shock heating processes associated with the merging of sub-clusters or groups within the larger structure. Overall, our findings contribute to a deeper understanding of the formation and evolution of galaxy clusters, particularly in the context of their optical and X-ray properties at intermediate redshifts. This study enhances our comprehension of the dynamics and characteristics of galaxy clusters, providing insights into their role in the cosmic web.",
        "ori-fast-z-score": -0.3216337604513384,
        "water-fast-z-score": 3.8376128944009875,
        "rewrite-fast-z-score": 1.5118578920369088
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Inhomogeneities in the Universe and the Fitting Problem .\nAbstract:\nThe fitting problem is one of the most important problems in cosmology, which has been studied for more than 50 years.  The main goal of this work was to study the effect of different types of initial conditions on the evolution of density perturbations in an expanding universe with a positive cosmological constant (dark energy).  We have used two methods to solve numerically the Einstein equations coupled to the matter fields:  the standard method based on the expansion of the metric tensor into spherical harmonics; and the new method developed by us that uses the expansion of the metric into Chebyshev polynomials.  In both cases we have considered three types of initial conditions: Gaussian random fields generated using the Zel dovich approximation, Gaussian random fields generated using a fast Fourier transform algorithm, and white noise.  Our results show that all these models give similar predictions at late times when they are evolved up to z = 0.5.  However, there are some differences between them at early times.  These differences can be explained as follows:  the Zel dovich model predicts larger fluctuations compared to other models because it does not take into account the pressure term in the fluid equation; the white noise model gives smaller fluctuations due to its statistical properties; finally, the Gaussian random fields obtained via the fast Fourier transform algorithm predict intermediate values.  This result shows that the choice of the initial conditions may affect significantly the final value of the power spectrum of primordial density fluctuations predicted by inflationary theories.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Inhomogeneities in the Universe and the Fitting Problem . Abstract : The fit puzzle is one of the most important problems in cosmology , which has been studied for more than 50 centuries .The main goal of this study was to study the impact of different kinds of initial conditions on the evolution of density perturbations in an increasing universe with a positive cosmological constant ( darkness energy ) . We have utilized two means to solve numerically the Einstein equations coupled to the matter fields : the standard method based on the contraction of the metric tensor into spherical harmonics ; and the new method developed by us that using the contraction of the metric into Chebyshev polynomials .In both cases we have proposed three sorts of initial conditions : Gaussian random fields generated using the Zel dovich approximation , Gaussian random fields generated using a rapid Fourier integral method , and white noise . Our results show that all these models make comparable predictions at late times when they are evolved up to z = 0 . 5 .However , there are some variations between them at early times . These changes can be described as follows : the Zel dovich theory predicts larger fluctuations compared to other models because it does not take into consideration the pressure term in the liquid equation ; the red noise model gives larger fluctuations owing to its statistical characteristics ; finally , the Gaussian random fields obtained via the fast Fourier integral method forecast intermediate values .This result suggests that the selection of the first terms would affect significantly the last value of the power spectrum of primordial density fluctuations assumed by inflationary theories .",
        "rewrite_text": "**Title:** Inhomogeneities in the Universe and the Fitting Problem\n\n**Abstract:** The fitting problem is a critical challenge in cosmology that has been explored for over five decades. This study aims to investigate how various initial conditions influence the evolution of density perturbations in an expanding universe characterized by a positive cosmological constant, often referred to as dark energy. We employed two numerical methods to solve the Einstein equations in conjunction with matter fields: the conventional approach that involves the contraction of the metric tensor into spherical harmonics, and a novel technique we developed that utilizes the contraction of the metric into Chebyshev polynomials. For our analysis, we considered three distinct types of initial conditions: Gaussian random fields generated through the Zel'dovich approximation, Gaussian random fields produced via a rapid Fourier integral method, and white noise. Our findings indicate that while all models yield similar predictions at late times, specifically when evolved to redshift z = 0.5, notable differences emerge during the early stages of evolution. Specifically, the Zel'dovich approximation predicts more significant fluctuations than the other models due to its neglect of the pressure term in the fluid equations. In contrast, the red noise model exhibits larger fluctuations attributed to its inherent statistical properties. The Gaussian random fields derived from the fast Fourier integral method, however, provide intermediate fluctuation values. These results imply that the choice of initial conditions can substantially influence the final power spectrum of primordial density fluctuations, a crucial aspect of inflationary theories. This study underscores the importance of understanding initial conditions in cosmological models and their implications for the evolution of the universe.",
        "ori-fast-z-score": -0.17677669529663687,
        "water-fast-z-score": 6.665640946733422,
        "rewrite-fast-z-score": 2.341196917715124
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A quantitative study on the growth variability of tumour cell clones in vitro .\nAbstract:\nThe authors have studied the growth rate and its fluctuations for different types of human cancer cells grown under controlled conditions, using an automated image analysis system to measure their size over time.  The results show that there is considerable variation between individual clones within each type of cell line as well as between different types of cell lines. This suggests that it may be possible to use this information to classify tumours into subtypes with distinct biological properties. In addition, they find evidence that the growth rates are correlated across generations of daughter cells, which could provide insight into how these correlations arise during tumorigenesis. Finally, by comparing the growth characteristics of normal and transformed cells, they conclude that transformation leads to increased heterogeneity among daughter cells. Cancer is characterized by uncontrolled proliferation of abnormal cells. Understanding the mechanisms underlying this process can help us develop new treatments against cancer. However, studying the dynamics of cancerous cell populations has been challenging because of difficulties associated with tracking large numbers of single cells simultaneously. Here we report our recent work on characterizing the growth behavior of thousands of individual cancer cells growing in culture dishes  1  . We used an automated imaging system to track the sizes of hundreds of thousands of cells belonging to several different types of human cancer cell lines ( Figure 1 ). Our results reveal significant differences in both average growth rates and growth fluctuations between different types of cell lines: some grow faster than others while also exhibiting larger fluctuations around their mean values  2  .\nWe found that the growth rates were highly variable even when measured at the level of individual clones derived from a common parent population  3  , suggesting that the observed phenotypic diversity might reflect genetic or epigenetic variations present in the original parental population  4  .  These findings suggest that it should be possible to use such measurements to classify tumors into subtypes based on their growth characteristics  5  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A quantitative study on the development variability of tumour cell clones in vitro . Abstract : The authors have researched the development frequency and its fluctuations for different kinds of human tumor cells grown under regulated conditions , using an automated photo processing program to measure their size over time .The results show that there is substantial variation between individual clones within each type of cell line as well as between various types of cell lines . This implies that it could be possible to use this data to classify tumours into subtypes with particular genetic characteristics .In addition , they discover proof that the development rates are correlated across generations of daughter tissues , which could give insight into how these correlations occur during tumorigenesis . Finally , by comparing the development patterns of normal and transformed tissue , they conclude that transformation results to greater heterogeneity among sister cells .Cancer is characterized by uncontrolled proliferation of irregular cells . Understanding the mechanisms governing this process can help us evolve innovative treatments against tumors .However , studying the dynamics of cancerous cell groups has been challenging because of troubles associated with monitoring huge amounts of single cells simultaneously . Here we publish our latest work on characterizing the development habits of thousands of individual cancer cells growing in culture dishes 1 .We utilized an automated scanning system to track the sizes of tens of thousands of cells belonging to several different kinds of human tumor cell lines ( Figure 1 ) . Our results show considerable variations in both average growth rates and growth fluctuations between various types of cell lines : some develop longer than others while also displaying wider fluctuations around their average values 2 .We showed that the development rates were extremely varied even when measured at the level of individual clones originating from a common parent population 3 , showing that the reported phenotypic diversity could reflect genetic or epigenetic changes found in the original parental generation 4 . These conclusions show that it should be possible to use such measurements to classify cancer into subtypes based on their development parameters 5 .",
        "rewrite_text": "**Title:** A Quantitative Study on the Development Variability of Tumor Cell Clones In Vitro\n\n**Abstract:** In this study, we investigate the growth frequency and variability of various human tumor cell types cultivated under controlled conditions. Utilizing an automated image processing system, we measured the size of these cells over time, allowing for a comprehensive analysis of their developmental patterns. Our findings reveal significant variability not only among individual clones within each cell line but also across different tumor cell lines. This variability suggests the potential for classifying tumors into distinct subtypes based on specific genetic characteristics. Furthermore, we provide evidence that growth rates exhibit correlations across generations of daughter cells, offering insights into the mechanisms underlying tumorigenesis. By comparing the growth behaviors of normal and transformed tissues, we conclude that cellular transformation leads to increased heterogeneity among sister cells. \n\nCancer is characterized by the uncontrolled proliferation of abnormal cells, and understanding the underlying mechanisms is crucial for developing innovative therapeutic strategies. However, studying the dynamics of cancer cell populations has been challenging due to the difficulties associated with simultaneously monitoring large numbers of individual cells. In our latest research, we present a detailed characterization of the growth patterns of thousands of individual cancer cells cultured in vitro. We employed an automated scanning system to track the sizes of tens of thousands of cells from multiple human tumor cell lines. Our results indicate substantial differences in both average growth rates and growth fluctuations among various cell lines, with some exhibiting prolonged growth periods and greater variability around their average sizes. Notably, we observed that growth rates varied significantly even at the level of individual clones derived from a common parental population, suggesting that the phenotypic diversity observed may stem from genetic or epigenetic alterations present in the original generation. These insights support the feasibility of utilizing growth measurements to classify cancer into subtypes based on their developmental characteristics.",
        "ori-fast-z-score": -1.9744355451432527,
        "water-fast-z-score": 9.189494464367357,
        "rewrite-fast-z-score": 1.5249857033260468
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The weighted complexity and the determinant functions of graphs .\nAbstract:\nWe study the weighted complexity function W(G) for graphs G with n vertices, which is defined as the minimum number of edges in any n-vertex subgraph H such that H contains all cycles of length at most k (k = 3, ...,n). We prove upper bounds on this function by using the concept of the determinant of an adjacency matrix A associated to G. In particular we show that if A has no zero rows or columns then W(G) <= 2n-1. This bound can be improved when A satisfies some additional conditions. Finally, we give examples showing how our results are sharp. The weighted complexity function W(G;k) (k=3,...,n), introduced by Chartrand et al., measures the minimum number of edges needed to make every cycle of length up to k in a given graph G. It was shown recently that W(G;3)=n-2 for any connected triangle-free planar graph G. In this note we improve these results by proving that W(G;3))<=n-1 for any connected triangle-free plane graph G.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The weighted complexity and the determinant functions of graphs . Abstract : We explore the weighted complexity function W ( G ) for graphs G with n edges , which is characterized as the minimum amount of vertices in any k - vertex subgraph H such that H contains all cycles of width at most h ( h = 3 , . . . , k ) .We derive upper limits on this function by using the idea of the determinant of an adjacency vector A associated to G . In particular we find that if A has no zero columns or rows then W ( G ) < = 2n - 1 . This bound can be improved when A satisfies some additional conditions .Finally , we give examples demonstrating how our findings are sharp . The weighted complexity function W ( G ; h ) ( h = 3 , . . . , k ) , invented by Chartrand et al . , estimates the minimum amount of vertices necessary to make every cycle of width up to n in a given graph G . It was shown lately that W ( G ; 3 ) = n - 2 for any connected triangle - free planar graph G . In this note we enhance these results by proving that W ( G ; 3 ) ) < = n - 1 for any connected triangle - free plane graph G .",
        "rewrite_text": "Title: The Weighted Complexity and Determinant Functions of Graphs\n\nAbstract: In this study, we investigate the weighted complexity function W(G) for graphs G characterized by n edges. This function is defined as the minimum number of vertices in any k-vertex subgraph H that encompasses all cycles with a width of at most h, where h ranges from 3 to k. We establish upper bounds for W(G) by leveraging the properties of the determinant of an adjacency vector A associated with the graph G. Notably, we demonstrate that if the adjacency vector A contains no zero columns or rows, then the inequality W(G) ≤ 2n - 1 holds. Furthermore, we show that this bound can be refined under certain additional conditions pertaining to A. To illustrate the sharpness of our results, we provide specific examples that highlight the applicability of our findings. The weighted complexity function W(G; h), introduced by Chartrand et al., serves as a measure of the minimum number of vertices required to ensure that every cycle of width up to n is present in the graph G. Recent research has established that for any connected triangle-free planar graph G, W(G; 3) equals n - 2. In this paper, we advance this understanding by proving that for any connected triangle-free planar graph G, the inequality W(G; 3) ≤ n - 1 is also valid. Our results contribute to the ongoing discourse on graph theory by providing deeper insights into the relationships between graph structure and cycle characteristics, particularly in the context of weighted complexity.",
        "ori-fast-z-score": -2.03701381619181,
        "water-fast-z-score": 4.7699904600286205,
        "rewrite-fast-z-score": 1.6283046848759573
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Trade-off between Processing Gains of an Impulse Radio UWB System in the Presence of Timing Jitter .\nAbstract:\nIn this work, we investigate the trade-offs among processing gains and timing jitters for impulse radio ultra-wideband (IR-UWB) systems with different pulse shapes. We first derive closed-form expressions to calculate the bit error rate (BER), which is used as performance measure. Then, by using these results, we analyze how the BER changes according to various system parameters such as signal-to-noise ratio (SNR), number of pulses per symbol, and timing jitter variance. Finally, numerical examples are provided to verify our analysis. The main contributions of this study can be summarized as:  1 . Derivation of closed-form expression for calculating the BER.  2 . Analysis on the effect of timing jitter on IR-UWB systems.  3 . Numerical verification of analytical results. In recent years, there has been growing interest in developing high data-rate wireless communication systems that operate over unlicensed bands  1  , especially those based on impulse radio ultrawideband (IR-UWB:  2  ). Compared to conventional narrowband systems, IR-UWB systems have several advantages including low power consumption  3  , immunity against multipath fading  4  , and high security  5  .\nHowever, one major drawback of IR-UWB systems is their vulnerability to timing jitter  6  -  8  . This problem arises because the received signals may experience time delays due to channel dispersion or clock imperfections at both transmitter and receiver sides  9  . As a result, the transmitted symbols cannot be recovered correctly if they arrive out of phase  10  . Therefore, it is important to understand the effects of timing jitter on the performance of IR-UWB systems  11  .\nTo address this issue, many studies have investigated the impact of timing jitter on various aspects of IR-UWB systems  12 -  16  . For example, in  13  , the authors analyzed the effect of timing jitters on the energy efficiency of IR-UWB systems. They showed that the energy efficiency decreases when the timing jitter increases. Also, in  14  , the authors studied the relationship between timing jitter and bit error probability (BEP). However, most existing works only",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Trade - off between Processing Gains of an Impulse Radio UWB System in the Presence of Timing Jitter . Abstract : In this research , we investigate the trade - offs among processing gains and timed jitters for impulse radio ultra - wideband ( IR - UWB ) devices with various pulse shapes .We first derive closed - form expressions to estimate the bit error rate ( BER ) , which is utilized as performance measure . Then , by using these results , we assess how the BER changes according to numerous system parameters such as signal - to - noise proportion ( SNR ) , amount of pulses per symbol , and timing jitter variance .Finally , numerical examples are provided to confirm our analysis . The main contributions of this study can be summarized as : 1 .Derivation of closed - form expression for determining the BER . 2 .Analysis on the impact of timing jitter on IR - UWB systems . 3 .Numerical verification of analytical results . In recent years , there has been growing interest in implementing high data - rate wireless communication devices that function over unlicensed bands 1 , particularly those based on impulse radio ultrawideband ( IR - UWB : 2 ) .Compared to conventional narrowband devices , IR - UWB systems have several advantages namely low power consumption 3 , immunity against multipath fading 4 , and good protection 5 . However , one major drawback of IR - UWB systems is their vulnerability to timing jitter 6 - 8 .This problem arises because the received messages might experience time errors caused to channel dispersion or clock imperfections at both transmitter and receiver sides 9 . As a outcome , the transmitted symbols unable be recovered correctly if they arrive out of mode 10 .Therefore , it is important to consider the effects of timing jitter on the performance of IR - UWB systems 11 . To address this question , various surveys have analyzed the impact of timing jitter on various parts of IR - UWB systems 12 - 16 .For instance , in 13 , the published analyzed the impact of timing jitters on the power performance of IR - UWB systems . They showed that the power performance decreases when the timing jitter changes .Also , in 14 , the published examined the relationship between timing jitter and bit error probability ( BEP ) . However , most existing works only",
        "rewrite_text": "**Title:** The Trade-off between Processing Gains of an Impulse Radio UWB System in the Presence of Timing Jitter\n\n**Abstract:** This study explores the intricate trade-offs between processing gains and timing jitter in impulse radio ultra-wideband (IR-UWB) systems, focusing on various pulse shapes. We begin by deriving closed-form expressions to estimate the bit error rate (BER), which serves as a key performance metric for evaluating system efficacy. Utilizing these expressions, we analyze how the BER is influenced by several critical system parameters, including signal-to-noise ratio (SNR), the number of pulses per symbol, and the variance of timing jitter. To substantiate our theoretical findings, we present numerical examples that illustrate the relationship between these parameters and their impact on system performance.\n\nThe primary contributions of this research can be summarized as follows: first, we provide a closed-form expression for calculating the BER in IR-UWB systems; second, we conduct a comprehensive analysis of the effects of timing jitter on the performance of these systems; and third, we offer numerical validation of our analytical results. \n\nIn recent years, there has been a surge of interest in the development of high data-rate wireless communication devices operating in unlicensed frequency bands, particularly those utilizing IR-UWB technology. Compared to traditional narrowband systems, IR-UWB offers several advantages, including lower power consumption, resilience to multipath fading, and enhanced security. However, a significant challenge faced by IR-UWB systems is their susceptibility to timing jitter, which can arise from channel dispersion or clock inaccuracies at both the transmitter and receiver. Such timing errors can lead to incorrect symbol recovery if the transmitted signals arrive out of synchronization.\n\nGiven the critical nature of timing jitter on IR-UWB system performance, various studies have examined its effects on different aspects of these systems. For instance, previous research has highlighted the detrimental impact of timing jitter on power performance and its correlation with bit error probability. However, many existing studies have primarily focused on isolated aspects, leaving a gap in comprehensive analyses that consider the interplay between processing gains and timing jitter across multiple system parameters. This paper aims to fill that gap, providing a holistic view of the challenges and trade-offs inherent in IR-UWB systems.",
        "ori-fast-z-score": 1.4791479939068937,
        "water-fast-z-score": 8.23076923076923,
        "rewrite-fast-z-score": 1.6045149064768403
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Electromagnetic response of high-Tc superconductors -- the slave-boson and doped-carrier theories .\nAbstract:\nThe electromagnetic properties of high-temperature superconductors are studied in terms of two different theoretical approaches, namely the slave-boson theory (SBT) and the doped carrier theory (DCT). The SBT is based on an effective low-energy description of strongly correlated electrons by means of auxiliary bosonic degrees of freedom which represent collective charge excitations. In this approach we calculate the optical conductivity as well as the Hall coefficient for various values of doping concentration n. We find that both quantities exhibit nontrivial temperature dependence at low temperatures T . On the other hand, within DCT these physical observables can be calculated analytically using simple expressions valid only at zero temperature. Our results show that there exists significant quantitative difference between predictions made by these two models. This discrepancy may serve to discriminate between them experimentally. High-temperature superconductivity has been one of the most challenging problems in condensed matter physics over past decades  1  . Despite enormous experimental efforts  2  , its microscopic origin remains unknown. A number of competing theoretical scenarios have been proposed  3  but none of them could provide a complete explanation of all available data  4  .\nIn particular, it was suggested  5  that the mechanism responsible for high-temperature superconductivity might involve strong electron correlations  6  . These effects cannot be described within conventional Fermi-liquid theory  7, 8  because they lead to non-Fermi liquid behavior  9  such as power-law dependences of thermodynamic functions  10  or unusual transport phenomena  11  . To account for these features theoretically, several phenomenological models were developed  12  including the so-called slave-boson theory  13  . It describes the dynamics of strongly interacting fermions with spin S = 1/2 coupled to an additional set of bosonic fields representing collective charge fluctuations  14  . Within this framework, the ground state of the system corresponds to a Bose-Einstein condensation  15  of the bosons  16  . As a result, the fermionic quasiparticles acquire finite masses  17  leading to their disappearance above some critical temperature  18  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Electromagnetic response of high - Tc superconductors - - the slave - boson and doped - carrier theories . Abstract : The electromagnetic properties of high - temperature superconductors are studied in terms of two different conceptual approaches , principally the slave - boson theory ( SBT ) and the doped carrier theory ( DCT ) .The SBT is based on an efficient low - energy characterization of highly correlated atoms by means of auxiliary bosonic degrees of liberty which denote collective charge excitations . In this methodology we determine the optical conductivity as well as the Hall coefficient for various values of doping concentration n . We see that both quantities exhibit nontrivial temperature dependence at low temperatures T .On the other hand , within DCT these physical observables can be determined analytically utilizing simple statements valid only at zero temperature . Our results show that there exists significant quantitative difference between estimates made by these two models .This discrepancy may serve to discriminate between them experimentally . High - temperature superconductivity has been one of the most challenging difficulties in condensed matter science over past decades 1 .Despite enormous scientific attempts 2 , its microscopic source remains unidentified . A variety of competing theory theories have been proposed 3 but none of them could give a complete explanation of all available data 4 .In particular , it was suggested 5 that the process responsible for high - temperature superconductivity might involve strong electron correlations 6 . These effects cannot be described within conventional Fermi - fluid theory 7 , 8 because they lead to non - Fermi solid behavior 9 such as power - law dependences of thermodynamic functions 10 or unusual travel effects 11 .To account for these characteristics theoretically , various phenomenological models were developed 12 notably the so - called slave - boson theory 13 . It covers the dynamics of highly interacting fermions with spin S = 1 / 2 coupled to an additional pair of bosonic fields representing collective charge fluctuations 14 .Within this framework , the ground state of the system belongs to a Bose - Einstein condensation 15 of the bosons 16 . As a result , the fermionic quasiparticles acquire finite masses 17 leading to their disappearance above some critical temperature 18 .",
        "rewrite_text": "**Title:** Electromagnetic Response of High-Tc Superconductors: Slave-Boson and Doped-Carrier Theories\n\n**Abstract:** This article investigates the electromagnetic properties of high-temperature superconductors through two distinct theoretical frameworks: the slave-boson theory (SBT) and the doped carrier theory (DCT). The SBT employs an innovative low-energy approach to characterize highly correlated atomic systems by introducing auxiliary bosonic degrees of freedom, which represent collective charge excitations. Utilizing this framework, we calculate the optical conductivity and Hall coefficient across various doping concentrations (n), revealing that both properties exhibit complex temperature dependencies at low temperatures (T). In contrast, the DCT allows for analytical determination of these physical observables, relying on simplified assumptions that hold true only at absolute zero temperature. Our findings indicate a notable quantitative divergence between the predictions of these two models, suggesting that experimental validation could effectively differentiate between them.\n\nHigh-temperature superconductivity has posed significant challenges in the field of condensed matter physics for several decades. Despite extensive research efforts, the underlying microscopic mechanisms remain elusive. Numerous competing theories have been proposed, yet none have succeeded in providing a comprehensive explanation for all observed phenomena. A key hypothesis suggests that strong electron correlations play a crucial role in the emergence of high-temperature superconductivity. Such correlations defy conventional Fermi-liquid theory, leading to non-Fermi liquid behavior characterized by power-law dependencies in thermodynamic functions and unusual transport phenomena. To address these complexities, various phenomenological models have been developed, with the slave-boson theory being particularly noteworthy. This theory describes the dynamics of strongly interacting fermions with spin S = 1/2, coupled to additional bosonic fields that account for collective charge fluctuations. Within this theoretical construct, the system's ground state is associated with a Bose-Einstein condensation of the bosonic excitations, resulting in the emergence of finite masses for the fermionic quasiparticles and their eventual disappearance above a critical temperature.",
        "ori-fast-z-score": 0.7905694150420948,
        "water-fast-z-score": 7.228202652129153,
        "rewrite-fast-z-score": 0.16222142113076254
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Galaxy morphologies and environment in the Abell 901/902 supercluster from COMBO-17 .\nAbstract:\nWe present an analysis of galaxy morphologies, luminosities and environments for galaxies in the Abell 901;902 supercluster (z = 0.18) using data obtained with the Wide Field Imager on board the European Southern Observatory s Very Large Telescope. We find that the fraction of early-type galaxies increases strongly towards higher local densities within this supercluster. The morphological mix is also found to depend significantly on absolute magnitude; fainter galaxies are more likely to be late-types than brighter ones at fixed density. These results suggest that both environmental effects and internal processes play important roles in shaping the observed morphology-density relation. This work was supported by NASA grant NAG5-7697. -The distribution of all galaxies in our sample overlaid onto the X-ray emission detected by Chandra . Galaxies are colour-coded according to their spectroscopic redshifts as indicated in the legend.  -The projected number density profile of cluster members around Abell 902 , determined from the photometric redshift catalogue .  -The projected number density profiles of different morphological types around Abell 902 .   -The projected number density of brightest cluster galaxies ( M V < −20 ) around Abell 902 .\n-Figures showing the dependence of galaxy properties on distance from Abell 902 .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Galaxy morphologies and environment in the Abell 901 / 902 supercluster from COMBO - 17 . Abstract : We present an assessment of galaxy morphologies , luminosities and habitats for galaxies in the Abell 901 ; 902 supercluster ( z = 0 . 18 ) using data acquired with the Wide Field Imager on board the European Southern Observatory s Very Large Telescope .We see that the fraction of early - class stars increases strongly towards higher local densities within this supercluster . The morphological mixing is also discovered to depend greatly on absolute magnitude ; fainter clusters are more likely to be early - types than brighter ones at fixed density .These data suggest that both environmental impacts and internal mechanisms play important roles in shaping the known morphology - density relation . This research was supported by NASA grant NAG5 - 7697 .- The distribution of all galaxies in our sample overlaid onto the X - ray radiation observed by Chandra . Galaxies are colour - coded according to their spectroscopic redshifts as indicated in the legend .- The estimated number density profile of cluster groups around Abell 902 , obtained from the photometric redshift catalogue . - The estimated number density profiles of different morphological types around Abell 902 .- The estimated number density of brightest cluster clusters ( M V < −20 ) around Abell 902 . - Figures displaying the dependence of galaxy features on distance from Abell 902 .",
        "rewrite_text": "We provide a comprehensive analysis of galaxy morphologies, luminosities, and environments within the Abell 901/902 supercluster (z = 0.18), utilizing data collected by the Wide Field Imager on the European Southern Observatory's Very Large Telescope. Our findings reveal a significant increase in the proportion of early-type galaxies as local density rises within the supercluster. Additionally, we observe that morphological diversity is heavily influenced by absolute magnitude; specifically, fainter galaxies tend to be early-type more frequently than their brighter counterparts at equivalent density levels. These observations imply that both environmental factors and intrinsic galaxy properties are crucial in determining the morphology-density relationship. \n\nOur research is bolstered by NASA grant NAG5-7697. We present a detailed distribution of galaxies in our sample, superimposed on X-ray emissions detected by Chandra, with galaxies color-coded according to their spectroscopic redshifts as indicated in the accompanying legend. Furthermore, we estimate the number density profile of cluster groups surrounding Abell 902, derived from our photometric redshift catalog. We also analyze the number density profiles of various morphological types in proximity to Abell 902, alongside the density of the brightest cluster galaxies (M_V < -20) in the same region. Our figures illustrate the relationship between galaxy characteristics and their distance from Abell 902, providing a clearer understanding of how environmental influences shape galaxy morphology within this supercluster. This study contributes valuable insights into the interplay between galaxy formation and environmental conditions in dense cosmic structures.",
        "ori-fast-z-score": -1.1285761872936695,
        "water-fast-z-score": 5.30722777603022,
        "rewrite-fast-z-score": 2.456769074559977
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The distance and neutral environment of the massive stellar cluster Westerlund 1 .\nAbstract:\nWe present new near-infrared photometry for the open cluster Westerlund 1, which is located in the Galactic bulge at a distance of 8 kpc (projected). We find that the red giant branch tip lies at K = 12.6 mag, corresponding to an age of about 10 Gyrs. The main sequence turn-off point occurs around K = 14.5 mag, indicating an age between 5 and 7 Gyrs. This places it among the oldest open clusters known. Using our data we derive a mean metallicity  Fe/H  = -1.3 ± 0.1 dex. From this value and using theoretical models we estimate the mass of the most massive star in the cluster as Mmax = 120 - 140M⊙. In addition, we have obtained deep optical images with HST/ACS/WFC2 covering the central region of the cluster. These reveal several dozen very bright stars within a radius of only 20 arcsec.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The distance and neutral environment of the huge stellar cluster Westerlund 1 . Abstract : We create latest near - infrared photometry for the open cluster Westerlund 1 , which is situated in the Galactic bulge at a distance of 8 kpc ( projected ) .We see that the red giant branch tip occurs at K = 12 . 6 mag , corresponding to an age of about 10 Gyrs . The main series turn - off position occurs around K = 14 . 5 mag , representing an age between 5 and 7 Gyrs .This places it among the earliest open clusters recorded . Using our information we derive a mean metallicity Fe / H = - 1 . 3 ± 0 . 1 dex .From this value and using theoretical models we estimate the mass of the most massive star in the cluster as Mmax = 120 - [UNK] . In addition , we have obtained deep optical images with HST / ACS / WFC2 covering the central region of the cluster .These suggest several several very bright stars within a diameter of only 20 arcsec .",
        "rewrite_text": "**Title:** The Distance and Neutral Environment of the Massive Stellar Cluster Westerlund 1\n\n**Abstract:** In this study, we present the latest near-infrared photometric observations of the open cluster Westerlund 1, located in the Galactic bulge at a projected distance of approximately 8 kpc. Our analysis reveals that the tip of the red giant branch is observed at K = 12.6 mag, which indicates an estimated age of around 10 billion years. Additionally, we identify the main sequence turn-off point at K = 14.5 mag, suggesting an age range between 5 and 7 billion years. These findings position Westerlund 1 among the oldest open clusters documented in the literature. Utilizing our photometric data, we calculate a mean metallicity of [Fe/H] = -1.3 ± 0.1 dex. This metallicity value, in conjunction with theoretical stellar evolution models, allows us to estimate the mass of the most massive star within the cluster to be approximately Mmax = 120 M☉. Furthermore, we have acquired deep optical images using the Hubble Space Telescope's Advanced Camera for Surveys (HST/ACS/WFC2), which cover the central region of Westerlund 1. These images reveal several exceptionally bright stars within a compact area of just 20 arcseconds in diameter. Our findings contribute to the understanding of the stellar population and evolutionary history of Westerlund 1, highlighting its significance as a key object of study in the context of massive star formation and cluster dynamics in the Galactic bulge.",
        "ori-fast-z-score": -2.629502940535666,
        "water-fast-z-score": 1.885618083164127,
        "rewrite-fast-z-score": -0.21081851067789195
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Set of equations for transient enhanced diffusion in shallow ion-implanted layers .\nAbstract:\nWe present the set of equations describing the process of transient enhanced diffusion (TED) in shallow implanted layers, which is important to understand and control dopant profiles during semiconductor device fabrication.  The model takes into account both the effects of lattice damage on TED as well as the effect of interstitials generated by the implantation itself. We show that this simple model can explain many experimental observations including the dependence of TED on dose rate, annealing temperature, and annealing time. Finally we use our model to predict the evolution of the dopant profile under different annealing conditions. Transient enhanced diffusion (TED), i.e., the redistribution of dopants after high energy implantation followed by rapid thermal annealing (RTA), has been studied extensively over the past two decades  1  . It is now widely accepted that the main driving force behind TED are point defects created by the implantation  2  , although other mechanisms such as vacancy clustering may also play an important role  3  .\nThe most commonly used models for simulating TED assume that all excess interstitials produced by the implantation are immobile at room temperature  4  . However, recent experiments have shown that some fraction of these interstitials become mobile even before RTA  5  . This mobility leads to additional dopant redistribution  6  . In addition, it was recently found that the amount of interstitials released depends strongly on the implantation dose rate  7, 8  . These results suggest that the current models do not fully capture the physics underlying TED.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Set of equations for transient increased diffusion in superficial ion - implanted layers . Abstract : We present the group of equations explaining the process of transient increased absorption ( TED ) in superficial implanted layers , which is important to comprehend and control dopant profiles during semiconductor device fabrication .The model takes into consideration both the effects of lattice disruption on TED as also as the impact of interstitials generated by the implantation itself . We see that this straightforward model can describe several experimental studies namely the dependence of TED on dose frequency , annealing temperature , and annealing speed .Finally we utilize our model to predict the evolution of the dopant profile under various annealing conditions . Transient increased diffusion ( TED ) , i . e . , the redistribution of dopants after high energy implantation preceded by rapid heat annealing ( RTA ) , has been studied thoroughly over the previous two decades 1 .It is now widely accepted that the main driving force behind TED are point failures created by the implantation 2 , although other mechanisms such as vacancy clustering may also play an important role 3 . The most commonly used theories for simulating TED assume that all excess interstitials produced by the implantation are immobile at room temperature 4 .However , recent experiments have shown that some fraction of these interstitials become mobile even before RTA 5 . This mobility gives to extra dopant redistribution 6 .In addition , it was recently found that the quantity of interstitials released depends strongly on the implantation dose frequency 7 , 8 . These conclusions show that the present theories do not truly capture the physics underlying TED .",
        "rewrite_text": "Title: Equations Governing Transient Enhanced Diffusion in Surface Ion-Implanted Layers\n\nAbstract: This article introduces a set of equations that elucidate the phenomenon of transient enhanced diffusion (TED) in surface layers that have undergone ion implantation, a critical aspect for managing dopant profiles during the fabrication of semiconductor devices. Our model incorporates the effects of lattice disruption caused by the implantation process, as well as the influence of interstitials generated during implantation. We demonstrate that this relatively simple model effectively accounts for various experimental observations, including the relationship between TED and factors such as dose frequency, annealing temperature, and the rate of annealing. \n\nTED, characterized by the redistribution of dopants following high-energy implantation and subsequent rapid thermal annealing (RTA), has been extensively studied over the last twenty years. It is now generally accepted that point defects created during implantation serve as the primary driving force for TED, although other mechanisms, such as vacancy clustering, may also contribute significantly. Traditional models for simulating TED typically assume that all excess interstitials produced during implantation remain immobile at room temperature. However, recent experimental findings indicate that a portion of these interstitials can become mobile even prior to RTA, leading to additional dopant redistribution. Furthermore, it has been observed that the number of interstitials released is highly dependent on the implantation dose frequency. \n\nThese insights suggest that existing theories may not fully capture the underlying physics of TED, highlighting the need for a more comprehensive understanding of the mechanisms at play. Our model aims to bridge this gap by providing a framework for predicting the evolution of dopant profiles under varying annealing conditions, thereby enhancing the ability to control and optimize semiconductor fabrication processes.",
        "ori-fast-z-score": -1.0083683467310325,
        "water-fast-z-score": 6.454545454545454,
        "rewrite-fast-z-score": 1.863448669773839
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Multiple Parton Scattering in Nuclei: Quark-quark Scattering .\nAbstract:\nWe present the first calculation of quark-quark scattering at next-to-leading order (NLO) accuracy within the framework of multiple parton scattering (MPI). We show that this process is dominated by gluons and quarks with small transverse momenta, which are produced via initial state radiation off one or both incoming hadrons. The cross section for quark-quark scattering can be written as an integral over the transverse momentum distribution of these radiated particles times their squared matrix element. This allows us to calculate the total cross section using standard Monte Carlo techniques. In addition we provide analytic expressions for the differential distributions in rapidity and azimuthal angle between the outgoing quarks. Our results agree well with those obtained numerically. Finally, we discuss how our formalism could be used to study MPI effects on jet production in proton-nucleus collisions. PACS numbers: 12.38.Qk, 13 .60.Le",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Multiple Parton Scattering in Nuclei : Quark - quark Scattering . Abstract : We present the first measurement of quark - quark scattering at next - to - leading order ( NLO ) accuracy within the framework of multiple parton absorption ( MPI ) .We see that this process is dominated by gluons and quarks with little transverse momenta , which are produced via initial state radiation off one or both incoming hadrons . The cross section for quark - quark scattering can be written as an integral over the transverse momentum density of these emitted particles times their squared matrix element .This enables us to estimate the total cross area using conventional Monte Carlo methods . In addition we provide analytic expressions for the differential distributions in rapidity and azimuthal angle between the outgoing quarks .Our results agree well with those achieved numerically . Finally , we talk how our formalism may be used to study MPI influence on jet production in proton - nucleus collisions .PACS codes : 12 . 38 . Qk , 13 . 60 . Le",
        "rewrite_text": "We present a pioneering measurement of quark-quark scattering at next-to-leading order (NLO) accuracy, conducted within the context of multiple parton interactions (MPI). Our findings indicate that this scattering process is predominantly influenced by gluons and quarks that possess minimal transverse momentum. These particles are generated through initial state radiation from one or both of the incoming hadrons. The cross section for quark-quark scattering can be expressed as an integral that incorporates the transverse momentum density of the emitted particles, multiplied by the square of their matrix element. This formulation allows us to estimate the total cross-sectional area using standard Monte Carlo techniques. Furthermore, we offer analytical expressions for the differential distributions concerning rapidity and the azimuthal angle between the resulting quarks, which show strong agreement with our numerical results. In addition to our primary findings, we discuss the implications of our formalism for investigating the impact of multiple parton interactions on jet production in proton-nucleus collisions. This work not only enhances our understanding of quark-quark scattering dynamics but also opens avenues for future research in high-energy particle physics, particularly in the context of complex hadronic interactions. The study is categorized under PACS codes 12.38.Qk and 13.60.Le, reflecting its relevance to quantum chromodynamics and particle scattering processes.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.242640687119286,
        "rewrite-fast-z-score": 1.1881770515720091
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Temporal Evolution of Step-Edge Fluctuations Under Electromigration Conditions .\nAbstract:\nWe report on the temporal evolution of step fluctuations under electromigration conditions in Cu(111) and Ag(111). The experiments were performed by scanning tunneling microscopy (STM), which allows for direct observation of atomic-scale processes at surfaces. We find that, during current-induced mass transport along steps, the step edges fluctuate strongly with time. These fluctuations are characterized by an exponential growth followed by saturation after about 1 hour. In addition to this general behavior we observe different types of fluctuations depending on the applied voltage and temperature. For low voltages and temperatures below 300 K, the fluctuations show a sawtooth-like shape indicating periodic changes between two states. At higher voltages or temperatures above 400 K, the fluctuations become more irregular but still exhibit some periodicity. Finally, at high voltages and temperatures around 500 K, no regular pattern is observed anymore. Our results suggest that these fluctuations can be explained as a result of competition between diffusion and drift currents.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  Temporal Evolution of Step-Edge Fluctuations Under Electromigration Conditions .Abstract:\nWe report on the temporal evolution of step fluctuations under electromigration conditions in Cu(111) and Ag(111).The studies were performed by scanning tunneling microscopy ( STM ) , which allows for detailed observation of atomic - scale processes at surfaces . We see that , during current - mediated mass transport along steps , the step paths fluctuate strongly with time .These fluctuations are observed by an exponential growth followed by saturation after about 1 hour . In addition to this general behavior we study various types of fluctuations based on the introduced voltage and heat .For lowest voltages and temperatures below 300 K , the fluctuations show a sawtooth - like shape suggesting periodic shifts between two states . At higher voltages or temperatures above 400 K , the fluctuations get more erratic but still exhibit some periodicity .Finally , at high voltages and conditions around 500 K , no normal behavior is observed anymore . Our results show that these fluctuations can be understood as a due of competition between diffusion and drift currents .",
        "rewrite_text": "We present a comprehensive study on the temporal evolution of step-edge fluctuations under electromigration conditions in copper (Cu(111)) and silver (Ag(111)) surfaces. Utilizing scanning tunneling microscopy (STM), we achieved high-resolution observations of atomic-scale phenomena occurring at these surfaces. Our findings reveal that during the process of current-induced mass transport along the step edges, the paths of these steps exhibit significant temporal fluctuations. Notably, we observe an exponential increase in these fluctuations, which subsequently reaches a saturation point after approximately one hour of observation. \n\nIn addition to this overarching trend, we investigate the nature of these fluctuations in relation to varying applied voltages and thermal conditions. At lower voltages and temperatures below 300 K, the fluctuations display a sawtooth-like pattern, indicative of periodic transitions between two distinct states. Conversely, at elevated voltages or temperatures exceeding 400 K, the fluctuations become increasingly erratic, although some degree of periodicity remains evident. At extreme conditions, specifically high voltages combined with temperatures around 500 K, we note a complete departure from typical fluctuation behavior. \n\nOur analysis suggests that these fluctuations arise from the interplay between diffusion processes and drift currents, highlighting the complex dynamics at play under electromigration conditions. This work contributes to a deeper understanding of step-edge behavior in metallic systems, with implications for the design and reliability of electronic materials subjected to electromigration.",
        "ori-fast-z-score": -0.6974858324629157,
        "water-fast-z-score": 5.505977612893481,
        "rewrite-fast-z-score": 0.09667364890456635
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Spitzer-IRAC GLIMPSE of high mass protostellar objects. I Infrared point sources and nebulae .\nAbstract:\nWe present the results of an infrared survey of the Galactic plane using Spitzer/InfraRed Array Camera (IRAC) data in four mid-infrared bands centered at 3.6, 4.5, 5.8, and 8 microns. The IRAC observations cover about one third of the entire sky with |b| < 1 degree. We have identified more than 100 thousand candidate young stellar objects (YSOs), including both Class 0/I YSOs and Classical T Tauri stars (CTTSs). About half of these are associated with previously known star-forming regions or open clusters. Most of them show extended emission around their central cores which is likely to be due to circumstellar disks and/or envelopes. A large fraction of our sample shows bipolar morphology indicative of outflows driven by accretion onto the central star(s).\nThe spatial distribution of the newly discovered YSO candidates suggests that most of the recent massive star formation has occurred within the last few million years along the spiral arms of the Galaxy.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Spitzer - IRAC GLIMPSE of high mass protostellar objects . I Infrared point observers and nebulae .Abstract : We present the results of an infrared survey of the Galactic jet using Spitzer / InfraRed Array Camera ( IRAC ) images in four mid - infrared bands centered at 3 . 6 , 4 . 5 , 5 . 8 , and 8 microns . The IRAC measurements cover about one third of the entire sky with | b | < 1 degree .We have discovered more than 100 thousand candidate young stellar bodies ( YSOs ) , comprising both Class 0 / I YSOs and Classical T Tauri stars ( CTTSs ) . About half of these are identified with already established star - creating areas or open complexes .Most of them show extended emission around their central cores which is probably to be due to circumstellar disks and / or envelopes . A wide proportion of our sample shows bipolar morphology indicative of outflows driven by accretion onto the main star ( s ) .The geographic distribution of the newly discovered YSO candidates implies that most of the recent massive star formation has occurred within the last few million years along the spiral arms of the Galaxy .",
        "rewrite_text": "We present findings from an extensive infrared survey of the Galactic jet, utilizing images from the Spitzer InfraRed Array Camera (IRAC) across four mid-infrared bands centered at 3.6, 4.5, 5.8, and 8 microns. This survey encompasses approximately one-third of the sky within a latitude of |b| < 1 degree. Our analysis has led to the identification of over 100,000 candidate young stellar objects (YSOs), which include both Class 0/I YSOs and Classical T Tauri stars (CTTSs). Notably, around half of these candidates are associated with previously recognized star-forming regions or open clusters. \n\nA significant characteristic observed in our sample is the presence of extended emission surrounding the central cores of many YSOs, likely attributed to circumstellar disks and/or envelopes. Furthermore, a considerable fraction of these objects exhibit a bipolar morphology, which suggests the presence of outflows driven by accretion processes onto the central stars. The spatial distribution of the newly identified YSO candidates indicates that a substantial amount of recent massive star formation has taken place over the past few million years, predominantly along the spiral arms of the Galaxy. \n\nThis survey not only enhances our understanding of the distribution and characteristics of high-mass protostellar objects but also provides critical insights into the processes governing star formation in our Galaxy. The results underscore the dynamic nature of star formation and the intricate structures associated with young stellar objects, paving the way for future research in the field of astrophysics.",
        "ori-fast-z-score": 1.0256451881367414,
        "water-fast-z-score": 4.837877973981903,
        "rewrite-fast-z-score": -0.5669467095138409
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  High angular resolution imaging of the circumstellar material around intermediate mass (IM) stars .\nAbstract:\nWe present high spatial and spectral resolution observations of two IM stars, HD 163296 and MWC 480, obtained with the Submillimeter Array at 1.3 mm wavelength. We detect several compact sources in both objects that are associated with dusty disks or envelopes surrounding these young stellar objects. The disk masses derived for these systems range between 0.1 to 0.5 Msun. In addition we find evidence for an extended component in the vicinity of HD 163296 which may be related to its outflow activity. These results demonstrate that even massive protoplanetary disks can form planets like our own solar system. Keywords: Circumstellar matter - Stars: Herbig Ae/Be - Massive star formation - Planet formation - Protostars - Young stellar objects: General - Millimeter waves - Submillimeter waves - Nearby galaxies - Radio astronomy - High energy astrophysics",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : High angular resolution detection of the circumstellar material around intermediate mass ( IM ) stars . Abstract : We report high spatial and spectral resolution measurements of two IM stars , HD 163296 and MWC 480 , obtained with the Submillimeter Array at 1 . 3 cm wavelength .We detect many compact sources in both objects that are identified with dusty disks or envelopes surrounding these young stellar bodies . The disk masses derived for these systems range between 0 . 1 to 0 . 5 Msun .In addition we find confirmation for an extended component in the vicinity of HD 163296 which may be connected to its outflow movement . These data demonstrate that even gigantic protoplanetary disks can form planets like our own solar system .Keywords : Circumstellar matter - Stars : Herbig Ae / Be - Massive star formation - Planet structure - Protostars - Young stellar bodies : General - Millimeter waves - Submillimeter waves - Nearby galaxies - Radio astronomy - High energy astrophysics",
        "rewrite_text": "We present a detailed investigation into the circumstellar material surrounding intermediate mass (IM) stars, focusing on two specific cases: HD 163296 and MWC 480. Utilizing the Submillimeter Array, we conducted high spatial and spectral resolution observations at a wavelength of 1.3 cm. Our findings reveal the presence of numerous compact sources associated with dusty disks or envelopes enveloping these young stellar objects. The estimated disk masses for HD 163296 and MWC 480 range from 0.1 to 0.5 solar masses, indicating significant material available for potential planet formation. Furthermore, our analysis of HD 163296 uncovers an extended component that may be linked to the star's outflow activity, suggesting dynamic interactions within the circumstellar environment. These observations provide compelling evidence that even large protoplanetary disks possess the capacity to form planetary systems akin to our own solar system. This research enhances our understanding of the processes involved in massive star formation and the evolution of circumstellar matter, contributing valuable insights into the field of radio astronomy and high-energy astrophysics. The implications of our findings extend to the study of young stellar bodies and the structure of planets, emphasizing the importance of millimeter and submillimeter wave observations in unraveling the complexities of star formation and the development of planetary systems in nearby galaxies. \n\nKeywords: Circumstellar material, Herbig Ae/Be stars, Massive star formation, Planetary structure, Protostars, Young stellar objects, Millimeter waves, Submillimeter waves, Nearby galaxies, Radio astronomy, High-energy astrophysics.",
        "ori-fast-z-score": -0.23249527748763857,
        "water-fast-z-score": 3.9000674757995495,
        "rewrite-fast-z-score": 1.104689541477988
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dielectric Relaxation and Phase Transition at Cryogenic Temperatures in 0.65 Pb(Ni_1/3Nb_2/3)O_3 -0.35PbTiO_3 Ceramics .\nAbstract:\nThe dielectric properties, phase transition behavior, and microstructure evolution were investigated for the (0.65 Pb(Ni-1/3Nb-2/3)O_3  -0.35PbTiO3) (PNT) ceramics with different sintering temperatures ranging from 850 to 1100 °C. The PNT samples exhibited high permittivity values up to ~10 4 , low loss tangent below 10 -2 , and large tunability over 30% under an electric field strength of 30 kV/cm at room temperature. With decreasing temperature down to 77 K, the permittivity increased slightly while the loss tangent decreased significantly due to the freezing out of mobile ions. At cryogenic temperatures, two relaxation processes were observed in the frequency range between 1 Hz and 100 kHz. The first process was attributed to the grain boundary effect; it shifted towards higher frequencies as the temperature decreased. The second process was associated with ferroelectric domain wall motion; its relaxation time constant remained almost unchanged when the temperature changed.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dielectric Relaxation and Phase Transition at Cryogenic Temperatures in 0 . 65 Pb ( Ni _ 1 / 3Nb _ 2 / 3 ) O _ 3 - 0 . 35PbTiO _ 3 Ceramics . Abstract : The dielectric characteristics , phase change response , and microstructure behavior were researched for the ( 0 . 65 Pb ( Ni - 1 / 3Nb - 2 / 3 ) O _ 3 - 0 . 35PbTiO3 ) ( PNT ) ceramics with various sintering rates ranging from 850 to 1100 °C .The PNT specimens exhibited high permittivity values up to ~ 10 4 , low loss tangent below 10 - 2 , and large tunability over 30 % under an electric field intensity of 30 kV / cm at room temperature . With decreasing temperature down to 77 K , the permittivity increased somewhat while the gain tangent decreased significantly related to the freezing out of mobile electrons .At cryogenic temperatures , two relaxation processes were detected in the frequency spectrum between 1 Hz and 100 kHz . The first mechanism was due to the grain boundary phenomenon ; it shifted towards higher frequencies as the temperature reduced .The second process was correlated with ferroelectric domain wall motion ; its relax time constant remained nearly unchanged when the temperature changed .",
        "rewrite_text": "**Title:** Dielectric Relaxation and Phase Transition at Cryogenic Temperatures in 0.65 Pb(Ni1/3Nb2/3)O3 - 0.35PbTiO3 Ceramics\n\n**Abstract:** This study investigates the dielectric properties, phase transition behavior, and microstructural characteristics of (0.65 Pb(Ni1/3Nb2/3)O3 - 0.35PbTiO3) (PNT) ceramics, which were subjected to various sintering temperatures ranging from 850 to 1100 °C. The PNT samples demonstrated remarkable dielectric performance, achieving permittivity values approaching 10,000, a low loss tangent below 0.01, and significant tunability exceeding 30% when exposed to an electric field intensity of 30 kV/cm at ambient temperature. As the temperature was lowered to 77 K, an increase in permittivity was observed, accompanied by a substantial decrease in the loss tangent, attributed to the immobilization of charge carriers. At cryogenic temperatures, two distinct dielectric relaxation processes were identified within the frequency range of 1 Hz to 100 kHz. The first relaxation mechanism, associated with grain boundary effects, exhibited a shift towards higher frequencies as the temperature decreased. Conversely, the second relaxation process, linked to the motion of ferroelectric domain walls, displayed a relatively stable relaxation time constant despite variations in temperature. These findings provide valuable insights into the dielectric behavior of PNT ceramics under cryogenic conditions, highlighting their potential applications in advanced electronic devices operating at low temperatures. The results underscore the importance of sintering conditions on the dielectric properties and phase transitions of these materials, paving the way for further research into their performance in practical applications.",
        "ori-fast-z-score": -0.3464101615137754,
        "water-fast-z-score": 5.505977612893481,
        "rewrite-fast-z-score": 0.4833682445228318
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  VOODB: A Generic Discrete-Event Random Simulation Model to Evaluate the Performances of OODBs .\nAbstract:\nThe performance evaluation of object-oriented databases (OODBs) is an important issue in database research and development, but it has been difficult because there are many factors that affect their performances. In this study we propose VOODB as a generic discrete-event random simulation model for evaluating the performances of OODBs. The proposed model can be used with any OODB system by simply changing its configuration parameters. We have implemented our model using Visual Basic 6.0 on Windows NT 4.0 platform. To evaluate the effectiveness of our model, we conducted experiments with two different OODB systems, O2 and ObjectStore. Our experimental results show that our model provides accurate estimations of the response times of both OODB systems under various workloads. Keywords: Performance Evaluation, Database System, Object-Oriented Database Systems, Discrete Event Simulations, Response Time Prediction. 1 Introduction Object-oriented databases (OODBSs), which provide powerful data modeling capabilities such as inheritance, encapsulation, polymorphism, etc., have become increasingly popular recently  l  . However, since they support complex data structures and operations, their performances may vary widely depending upon several factors  2  .\nIn order to develop high-performance OODBSs, it is necessary to analyze how these factors influence their performances. Therefore, researchers have studied the performance evaluations of OODBSs extensively  3  , 4 , 5 , 6 , 7 , 8 , 9 , lo , 11 , 12 , 13 , 14 , 15 , 16 ,  17  , 18 , 19 ,  20  , 21 ,  22  , 23 ,  24  , 25 ,  26  , 27 ,  28  , 29 ,  30  , 31 ,  32  , 33 ,  34  , 35 ,  36  ,  37J,  38  ,  39J,  40  ,  411,  42  ,  43J,  44  ,  45J,  46  ,  47J,  48  ,  49J,  50  ,  51J,  52J,  53J,  54J,  55J,  56J,",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : VOODB : A Generic Discrete - Event Random Simulation Model to Evaluate the Performances of OODBs . Abstract : The performance assessments of object - oriented data ( OODBs ) is an important issue in database study and development , but it has been difficult because there are many factors that affect their performances .In this study we propose VOODB as a generic discrete - event random simulation method for evaluating the performances of OODBs . The proposed description can be used with any OODB network by simply changing its setup elements .We have integrated our model utilizing Visual Basic 6 . 0 on Windows NT 4 . 0 framework . To assess the performance of our model , we conducted experiments with two different OODB systems , O2 and ObjectStore .Our research results show that our model provides reliable estimations of the response periods of both OODB systems under various workloads . Keywords : Performance Evaluation , Database System , Object - Oriented Database Systems , Discrete Event Simulations , Response Time Prediction .1 Introduction Object - oriented libraries ( OODBSs ) , which provide potent data modeling technologies such as inheritance , encapsulation , polymorphism , etc . , have become increasingly popular recently l . However , since they support difficult information structures and functions , their performances may differ widely depending upon several considerations 2 .In order to develop large - performance OODBSs , it is required to analyze how these influences influence their performances . Therefore , researchers have researched the performance evaluations of OODBSs extensively 3 , 4 , 5 , 6 , 7 , 8 , 9 , lo , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 , 31 , 32 , 33 , 34 , 35 , 36 , 37J , 38 , 39J , 40 , 411 , 42 , 43J , 44 , 45J , 46 , 47J , 48 , 49J , 50 , 51J , 52J , 53J , 54J , 55J , 56J ,",
        "rewrite_text": "**Title:** VOODB: A Generic Discrete-Event Random Simulation Model for Evaluating the Performance of Object-Oriented Databases\n\n**Abstract:** The evaluation of performance in object-oriented databases (OODBs) is a critical area of research in database technology, yet it presents significant challenges due to the multitude of factors influencing performance outcomes. In this paper, we introduce VOODB, a versatile discrete-event random simulation model designed to assess the performance of OODBs effectively. This model is adaptable and can be applied to any OODB network by simply modifying its configuration parameters. We have implemented our simulation model using Visual Basic 6.0 within the Windows NT 4.0 environment. To validate the effectiveness of VOODB, we conducted a series of experiments involving two distinct OODB systems: O2 and ObjectStore. The results of our experiments demonstrate that VOODB yields accurate estimations of response times for both OODB systems across a variety of workload scenarios. This research contributes to the ongoing discourse on performance evaluation in database systems, particularly in the context of object-oriented architectures. The findings underscore the importance of understanding the performance dynamics of OODBs, which are increasingly favored for their advanced data modeling capabilities, including inheritance, encapsulation, and polymorphism. As OODBs become more prevalent, the need for robust performance evaluation tools like VOODB becomes ever more critical. Our study not only provides a reliable framework for performance assessment but also highlights the intricate factors that can influence OODB performance, paving the way for future research in this domain. \n\n**Keywords:** Performance Evaluation, Database Systems, Object-Oriented Database Systems, Discrete Event Simulations, Response Time Prediction.",
        "ori-fast-z-score": -0.6897304947150052,
        "water-fast-z-score": 5.222245174270754,
        "rewrite-fast-z-score": 0.3779644730092272
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dimer Models, Free Fermions and Super Quantum Mechanics .\nAbstract:\nWe present the results on the super quantum mechanics (SQM) in dimer models with free fermion boundary conditions. We show that SQM is equivalent to the usual quantum mechanics for periodic boundary condition case. In addition we find that there are two different types of supersymmetric ground states depending on whether or not the number of particles N is even. For odd N , the ground state has zero energy but it does not have any degeneracy. On the other hand, when N is even, the ground state becomes doubly degenerate. The degeneracy can be understood by using the concept of parity symmetry breaking. Finally we discuss how our result fits into the general framework of supersymmetry. PACS: 11.10.Wx, 05.45.Mt, 11.15.Tk \nI. INTRODUCTORY REMARK\nIn this work we study the super quantum mechanics ( S Q M ) i n d im e r m odels w ith f ree fermi b oundary c o ndition s . T hese models were first introduced by Rokhsar et al  1  as an exactly solvable model which describes spin-1/2 Heisenberg antiferromagnet on a square lattice. They showed that these models exhibit many interesting properties such as spontaneous dimerization at low temperatures  2  .\nThe main purpose of this work is to investigate the effect of boundary conditions on the supersymmetric structure of the system. It turns out that the supersymmetric structure depends crucially on the boundary conditions imposed on the system. As will become clear later, the supersymmetric structure changes drastically if one switches between periodic and free-fermion boundary conditions.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dimer Models , Free Fermions and Super Quantum Mechanics . Abstract : We present the results on the super quantum mechanics ( SQM ) in dimer models with free fermion boundary conditions .We see that SQM is analogous to the usual quantum mechanics for periodic border condition case . In addition we find that there are two different kinds of supersymmetric ground states varying on whether or not the number of particles N is even .For odd N , the ground state has zero energy but it does not have any degeneracy . On the other hand , when N is even , the ground state remains doubly degenerate .The degeneracy can be understood by using the notion of parity symmetry breaking . Finally we talk how our consequence fits into the general principle of supersymmetry .PACS : 11 . 10 . Wx , 05 . 45 . Mt , 11 . 15 . Tk I . INTRODUCTORY REMARK In this study we study the super quantum mechanics ( S Q M ) i n d im e r m odels l ith f ree fermi b oundary g o ndition s .T hese models were first developed by Rokhsar et al 1 as an exactly solvable theory which explains spin - 1 / 2 Heisenberg antiferromagnet on a square lattice . They showed that these models exhibit several interesting properties such as spontaneous dimerization at low temperatures 2 .The main aim of this research is to examine the impact of boundary rules on the supersymmetric composition of the system . It turns out that the supersymmetric formation determines crucially on the boundary rules imposed on the system .As will become clear afterwards , the supersymmetric composition changes significantly if one switches between periodic and free - fermion boundary conditions .",
        "rewrite_text": "**Title:** Dimer Models, Free Fermions, and Super Quantum Mechanics\n\n**Abstract:** In this article, we explore the implications of super quantum mechanics (SQM) within dimer models that incorporate free fermion boundary conditions. Our findings reveal that SQM exhibits similarities to conventional quantum mechanics, particularly in scenarios with periodic boundary conditions. A key aspect of our research is the identification of two distinct types of supersymmetric ground states, which depend on the parity of the particle number \\( N \\). For systems with an odd \\( N \\), we observe that the ground state possesses zero energy but lacks degeneracy. Conversely, in cases where \\( N \\) is even, the ground state retains a doubly degenerate nature. This degeneracy can be interpreted through the lens of parity symmetry breaking. Furthermore, we discuss how our results align with the broader principles of supersymmetry, contributing to the understanding of these complex systems. \n\nThe foundational work on dimer models was pioneered by Rokhsar et al., who established these models as exactly solvable frameworks that elucidate the behavior of spin-1/2 Heisenberg antiferromagnets on square lattices. Their research highlighted several intriguing phenomena, including spontaneous dimerization at low temperatures. The primary objective of our study is to investigate how boundary conditions influence the supersymmetric structure of the system. Our analysis demonstrates that the nature of the supersymmetric composition is significantly affected by the boundary conditions applied. Notably, transitioning between periodic and free fermion boundary conditions leads to substantial alterations in the supersymmetric characteristics of the system. This work not only enhances our understanding of dimer models but also provides insights into the intricate relationship between boundary conditions and supersymmetry in quantum systems. \n\n**PACS:** 11.10.Wx, 05.45.Mt, 11.15.Tk",
        "ori-fast-z-score": -1.3112201362143716,
        "water-fast-z-score": 4.870246220224809,
        "rewrite-fast-z-score": 1.1917080461366747
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Thermodynamics and the virial expansion for trapped fluids in arbitrary external potentials .\nAbstract:\nWe present an exact expression for the pressure tensor of a fluid confined by any external potential, which is valid at all temperatures. The result can be obtained as a special case of the virial expansion for the grand canonical partition function. We show that this expression reduces to known results when applied to specific potentials such as harmonic traps or periodic lattices. Finally we apply our general formula to calculate the equation of state of a gas of fermions with attractive interactions in two dimensions. In particular, we find that the system undergoes a phase transition into a superfluid state below some critical temperature Tc. This work was supported by NSF grant PHY-0456747 (M.A.) . \nI. INTRODUCTORY REMARK\nThe thermodynamic properties of many-body systems are often studied using statistical mechanics methods  1  , where one considers ensembles of particles interacting via a given potential energy V(r). For example, if the particles interact through short-range forces only, then it is possible to derive expressions for various physical quantities like density profiles  2  , compressibility  3  , heat capacity  4  , etc., starting from the microscopic definition of entropy S = -k B ln Z, where k B is Boltzmann s constant and Z is the partition function defined as:",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Thermodynamics and the virial expansion for trapped fluids in arbitrary external potentials . Abstract : We present an precise representation for the pressure tensor of a fluid confined by any external potential , which is valid at all temperatures .The result can be obtained as a special case of the virial expansion for the grand canonical partition function . We see that this expression reduces to known results when applied to specific potentials such as harmonic traps or periodic lattices .Finally we apply our general formula to estimate the equation of state of a gas of fermions with interesting interactions in two dimensions . In particular , we find that the system undergoes a phase shift into a superfluid state below some significant heat Tc .This project was supported by NSF grant PHY - 0456747 ( M . A . ) .I . INTRODUCTORY REMARK The thermodynamic properties of several - bodies systems are often investigated using statistical mechanics methods 1 , where one considers ensembles of molecules interacting via a given potential energy V ( r ) .For instance , if the particles react through short - range forces only , then it is convenient to derive expressions for various mechanical quantities like density profiles 2 , compressibility 3 , temperature strength 4 , etc . , beginning from the microscopic definition of entropy S = - k B ln Z , where k B is Boltzmann s constant and Z is the partition function defined as :",
        "rewrite_text": "In this article, we provide a comprehensive formulation for the pressure tensor of fluids confined within arbitrary external potentials, applicable across all temperature ranges. This formulation emerges as a specific instance of the virial expansion derived from the grand canonical partition function. Notably, our findings align with established results when the expression is applied to particular potential scenarios, such as harmonic traps or periodic lattices. Furthermore, we utilize our generalized formula to analyze the equation of state for a two-dimensional gas of fermions exhibiting intriguing interactions. Our investigation reveals that this system experiences a phase transition into a superfluid state when the temperature falls below a critical threshold, denoted as Tc. This research was made possible through the support of NSF grant PHY-0456747 (M.A.). \n\nIn the introductory section, we highlight the significance of statistical mechanics in exploring the thermodynamic characteristics of many-body systems. Typically, these systems are analyzed by considering ensembles of particles that interact through a specified potential energy function, V(r). For scenarios where interactions are limited to short-range forces, it becomes advantageous to derive expressions for various mechanical properties, including density profiles, compressibility, and thermal strength, starting from the microscopic definition of entropy, S = -kB ln Z. Here, kB represents Boltzmann's constant, and Z is the partition function, which serves as a foundational element in statistical mechanics. Our work aims to deepen the understanding of trapped fluids and their thermodynamic behavior under varying external conditions, contributing valuable insights to the field.",
        "ori-fast-z-score": 0.29277002188455997,
        "water-fast-z-score": 4.737008796323751,
        "rewrite-fast-z-score": 2.138089935299395
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Generalised Einstein condition and cone construction for parabolic geometries .\nAbstract:\nWe consider the generalised Einstein condition in the context of parabolic geometries, which are Riemannian metrics on manifolds with an additional structure given by a torsion-free connection preserving the metric up to scale.  We show that this condition is equivalent to the existence of a certain family of vector fields satisfying some natural properties. In particular we prove that if such a family exists then it can be used to construct a new parabolic geometry whose underlying manifold has dimension one less than the original one. This result allows us to give a complete classification of all possible parabolic geometries on surfaces admitting a non-trivial Levi-Civita connection. Finally, using our results about the generalised Einstein condition, we study the problem of finding solutions to the vacuum field equations in four dimensions. The main tool here will be the so-called  cone construction  introduced by Eardley and Sachs. \nIntroduction\n\nIn this article we shall investigate the following question: \nWhat does the generalised Einstein condition mean?\n\nThe answer depends upon what you mean by  geometries . \n\nIf you have no idea whatsoever as to what I am talking about, read the first section below entitled  Introduction .\n\nParabolic geometries",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Generalised Einstein procedure and cone construction for parabolic geometries . Abstract : We consider the generalised Einstein condition in the context of parabolic geometries , which are Riemannian metrics on manifolds with an additional structure provided by a torsion - free connection preserving the metric up to scale .We see that this condition is analogous to the existence of a certain family of vector fields fulfilling some natural characteristics . In particular we prove that if such a family exists then it can be used to build a new parabolic topology whose fundamental manifold has size one fewer than the previous one .This result allows us to give a complete classification of all possible parabolic geometries on surfaces admitting a non - simple Levi - Civita connection . Finally , using our findings about the generalised Einstein situation , we study the question of finding solutions to the vacuum field equations in four dimensions .The main technique here will be the so - called cone construction introduced by Eardley and Sachs . Introduction In this article we shall explore the following issue : What does the generalised Einstein condition imply ?The answer relies upon what you mean by geometries . If you have no know whatsoever as to what I am talking about , see the first section below called Introduction .Parabolic geometries",
        "rewrite_text": "Title: Generalized Einstein Procedure and Cone Construction for Parabolic Geometries\n\nAbstract: In this study, we investigate the generalized Einstein condition within the framework of parabolic geometries, which are characterized by Riemannian metrics on manifolds that possess an additional structure defined by a torsion-free connection that preserves the metric up to a scaling factor. We establish that the generalized Einstein condition is closely related to the existence of a specific family of vector fields that exhibit certain natural properties. Notably, we demonstrate that if such a family of vector fields exists, it can be utilized to construct a new parabolic topology, where the fundamental manifold is of a dimension one less than that of the original manifold. This significant finding enables us to achieve a comprehensive classification of all potential parabolic geometries on surfaces that allow for a non-simple Levi-Civita connection. Furthermore, leveraging our insights regarding the generalized Einstein condition, we delve into the challenge of identifying solutions to the vacuum field equations in four-dimensional spacetime. The primary methodology employed in this investigation is the cone construction technique, as introduced by Eardley and Sachs. In the introduction of this article, we will address the implications of the generalized Einstein condition and clarify its significance in the context of geometrical frameworks. For readers unfamiliar with the concepts discussed, we recommend consulting the introductory section for foundational insights into parabolic geometries.",
        "ori-fast-z-score": -0.8251369970070347,
        "water-fast-z-score": 4.873672965232998,
        "rewrite-fast-z-score": 1.5724272550828775
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Hierarchical Star-Formation in M33: Fundamental properties of the star-forming regions .\nAbstract:\nWe present an analysis of the fundamental physical parameters (mass, luminosity and size) for a sample of young star clusters in the nearby spiral galaxy M33 using HST/ACS data. We find that these objects are consistent with being gravitationally bound open clusters or associations. The mass distribution is well described by a power law dN/dM ~ M^{-3} between 10^6 to 5 x 10^7 solar masses. This result suggests that cluster formation proceeds hierarchically on all scales within this range. In addition we find evidence for two distinct populations of massive clusters; one population has ages less than 100 Myr while another older population appears to be coeval at ages greater than 300 Myr. These results suggest that there may have been multiple episodes of intense cluster formation over the past few hundred million years. Finally, we compare our observations to theoretical models of cluster evolution and find good agreement when assuming a Kroupa IMF.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Hierarchical Star - Formation in M33 : Fundamental properties of the star - creating areas . Abstract : We present an assessment of the fundamental physical factors ( mass , luminosity and size ) for a sample of young galaxy galaxies in the nearby spiral galaxy M33 utilizing HST / ACS data .We see that these objects are compatible with being gravitationally locked open complexes or associations . The mass distribution is well described by a power law dN / dM ~ M ^ { - 3 } between 10 ^ 6 to 5 x 10 ^ 7 solar masses .This result suggests that cluster structure proceeds hierarchically on all scales within this spectrum . In addition we find proof for two separate populations of large clusters ; one community has ages less than 100 Myr while another older population seems to be coeval at ages greater than 300 Myr .These data suggest that there may have been multiple cycles of aggressive cluster structure over the previous few hundred million years . Finally , we compare our observations to theoretical theories of cluster evolution and find good agreement when assuming a Kroupa IMF .",
        "rewrite_text": "Title: Hierarchical Star Formation in M33: Fundamental Properties of Star-Creating Regions\n\nAbstract: In this study, we evaluate the essential physical characteristics—mass, luminosity, and size—of a selection of young star-forming regions within the nearby spiral galaxy M33, utilizing data from the Hubble Space Telescope's Advanced Camera for Surveys (HST/ACS). Our findings indicate that these star-forming regions are likely gravitationally bound complexes or associations. The mass distribution of these regions is well-represented by a power law, specifically dN/dM ~ M^{-3}, across a mass range from 10^6 to 5 x 10^7 solar masses. This observation implies that the formation of star clusters occurs hierarchically across various scales within this mass spectrum. Furthermore, we identify two distinct populations of large star clusters: one group comprises clusters younger than 100 million years, while the other, older population appears to be coeval, with ages exceeding 300 million years. These results suggest that M33 has experienced multiple episodes of vigorous star cluster formation over the last few hundred million years. Additionally, we compare our empirical findings with theoretical models of cluster evolution and observe a strong correlation, particularly when applying a Kroupa initial mass function (IMF). This research enhances our understanding of the hierarchical nature of star formation in M33 and contributes to the broader discourse on the dynamics of star cluster development in spiral galaxies.",
        "ori-fast-z-score": 0.10721125348377948,
        "water-fast-z-score": 5.467773927672753,
        "rewrite-fast-z-score": -0.19245008972987526
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Isospin breaking in the yield of heavy meson pairs in e+e- annihilation near threshold .\nAbstract:\nWe study isospin-breaking effects on the production rate for heavy-light mesons (D, D*) and light-heavy mesons (D0, D0bar). We use an effective field theory approach to calculate these rates at leading order in perturbation theory. The results are compared with experimental data obtained by CLEO-c. \n \n Isospin symmetry plays an important role in hadronic physics. It relates states that differ only in their charge but have identical masses. In particular it implies that the strong decay widths of charged and neutral pions should be equal. However, this equality has been experimentally tested down to pion momenta as low as 1 MeV/c and deviations up to 20% were found  1  . These deviations can be explained within Chiral Perturbation Theory  2  , which predicts corrections proportional to powers of the momentum transfer between initial and final state particles. At higher energies, where the typical momentum transfers become larger than the chiral scale, one expects such corrections to vanish rapidly  3  .\n \nIn contrast, we consider here processes involving two heavy quarks close to threshold. Here, the typical momentum transfers are small enough so that non-perturbative contributions cannot be neglected anymore. As a consequence, even though the mass difference between charm and anti-charm quarks is tiny, there will still be significant differences between the corresponding cross sections  4  . \n \n This effect was first observed more than 20 years ago  5  when studying the production of charmed mesons in electron-positron collisions. Since then many experiments  6  -  8  have measured the ratio of the production rates for different combinations of heavy-meson pairs. While some of them find good agreement with theoretical predictions  9  based on Heavy Quark Effective Theory  10  , others disagree significantly  11  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Isospin breaking in the yield of heavy meson pairs in e + e - annihilation near threshold . Abstract : We research isospin - breaking effects on the production level for heavy - heavy mesons ( D , D * ) and light - heavy mesons ( D0 , D0bar ) .We use an efficient field model approach to estimate these rates at leading order in perturbation theory . The results are compared with theoretical data derived by CLEO - c . Isospin symmetry serves an important role in hadronic physics .It relates states that differ only in their charge but have equal masses . In particular it assumes that the strong decay widths of charged and neutral pions should be equal .However , this equality has been experimentally tested down to pion momenta as low as 1 MeV / c and deviations up to 20 % were found 1 . These deviations can be described within Chiral Perturbation Theory 2 , which predicts corrections proportional to powers of the velocity change between initial and final state particles .At higher energies , where the typical velocity transfers become bigger than the chiral scale , one expects such corrections to vanish swiftly 3 . In comparison , we consider here reactions involving two heavy quarks close to threshold .Here , the typical velocity transfers are small enough so that non - perturbative contributions need be forgotten anymore . As a consequence , even though the mass gap between charm and pro - charm quarks is tiny , there will still be considerable changes between the associated cross sections 4 .This phenomenon was first observed more than 20 centuries earlier 5 when examining the production of charmed mesons in electron - positron collisions . Since then many research 6 - 8 have recorded the proportion of the production rates for different combinations of heavy - meson pairs .While some of them find good agreement with theoretical predictions 9 based on Heavy Quark Effective Theory 10 , others disagree significantly 11 .",
        "rewrite_text": "Title: Isospin Breaking in the Yield of Heavy Meson Pairs in e+e- Annihilation Near Threshold\n\nAbstract: This study investigates the isospin-breaking effects on the production of heavy-heavy mesons (D, D*) and light-heavy mesons (D0, D0̄) in the context of e+e- annihilation near threshold. Utilizing an efficient field model approach, we estimate the production rates at leading order in perturbation theory and compare our findings with theoretical data obtained from CLEO-c experiments. Isospin symmetry plays a crucial role in hadronic physics, linking states that differ solely in charge but possess equal masses. This symmetry implies that the strong decay widths of charged and neutral pions should be identical. However, experimental tests have revealed deviations of up to 20% in these decay widths at pion momenta as low as 1 MeV/c. These discrepancies can be accounted for using Chiral Perturbation Theory, which predicts corrections that are proportional to the velocity changes between initial and final state particles. At higher energies, where typical velocity transfers exceed the chiral scale, these corrections are expected to diminish rapidly. In our analysis, we focus on reactions involving two heavy quarks near threshold, where the velocity transfers remain sufficiently small, allowing us to neglect non-perturbative contributions. Despite the minimal mass gap between charm and anti-charm quarks, we observe significant variations in the associated cross sections. This phenomenon was first documented over two decades ago in studies of charmed meson production during electron-positron collisions. Since then, numerous investigations have measured the production rates of various heavy-meson pair combinations. While some studies align well with theoretical predictions based on Heavy Quark Effective Theory, others exhibit substantial discrepancies. Our research aims to elucidate these isospin-breaking effects and contribute to the understanding of heavy meson production dynamics.",
        "ori-fast-z-score": -0.5107539184552492,
        "water-fast-z-score": 5.979278639572285,
        "rewrite-fast-z-score": 4.595682838477836
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The formation of spiral arms and rings in barred galaxies .\nAbstract:\nWe present an analysis of the distribution of gas, stars and dust in two nearby edge-on spirals with prominent bars (NGC 1365 and NGC 1530). We use high-resolution observations obtained by the Herschel Space Observatory to study the physical conditions of the interstellar medium along these systems. The main results are as follows:  - In both cases we find that the molecular hydrogen is concentrated on the leading edges of the bar, while atomic hydrogen follows closely the stellar light.  - The star formation rate peaks at the ends of the bar where the density of molecular hydrogen increases significantly. This suggests that the gravitational torques induced by the bar can trigger the collapse of dense clouds into new generations of young stars.  - The infrared emission associated with polycyclic aromatic hydrocarbons shows a clear correlation between the location of this component and the regions of active star formation. - The comparison of our data with hydrodynamical simulations indicates that the observed structure of the ISM may be explained if the bar potential has been able to drive significant amounts of cold gas towards its inner Lindblad resonance.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The formation of spiral arms and rings in barred clusters . Abstract : We report an assessment of the distribution of gas , stars and dust in two adjacent edge - on spirals with prominent bars ( NGC 1365 and NGC 1530 ) .We use large - resolution measurements obtained by the Herschel Space Observatory to study the physical conditions of the interstellar medium along these systems . The main results are as follows : - In both cases we find that the molecular hydrogen is confined on the led corners of the bar , while nuclear hydrogen takes closely the stellar radiation .- The star formation rate peaks at the ends of the bar where the density of molecular hydrogen rises considerably . This implies that the gravitational torques induced by the bar can cause the collapse of dense clouds into new generations of young stars .- The infrared absorption associated with polycyclic aromatic hydrocarbons reveals a clear correlation between the location of this constituent and the regions of active star formation . - The comparison of our information with hydrodynamical simulations indicates that the known composition of the ISM may be described if the bar potential has been able to drive considerable amounts of cold gas towards its outer Lindblad resonance .",
        "rewrite_text": "Title: The Formation of Spiral Arms and Rings in Barred Clusters\n\nAbstract: In this study, we investigate the distribution of gas, stars, and dust in two closely situated edge-on spiral galaxies featuring prominent bars, specifically NGC 1365 and NGC 1530. Utilizing high-resolution data from the Herschel Space Observatory, we analyze the physical conditions of the interstellar medium (ISM) within these galaxies. Our findings reveal several significant patterns: Firstly, we observe that molecular hydrogen is predominantly concentrated at the leading edges of the bars, while atomic hydrogen is more closely associated with stellar radiation. Secondly, we identify that the star formation rate is maximized at the ends of the bars, where there is a notable increase in the density of molecular hydrogen. This suggests that the gravitational torques exerted by the bars facilitate the collapse of dense molecular clouds, leading to the formation of new stellar populations. Additionally, we find a strong correlation between the infrared absorption features linked to polycyclic aromatic hydrocarbons and regions of active star formation, indicating that these compounds may serve as tracers for star-forming activity. Finally, by comparing our observational data with hydrodynamical simulations, we conclude that the existing composition of the ISM can be effectively explained if the bar potential is capable of channeling significant amounts of cold gas toward the outer Lindblad resonance. This research enhances our understanding of the dynamics and evolution of barred spiral galaxies, particularly in relation to star formation processes and the role of the interstellar medium.",
        "ori-fast-z-score": 1.2935483472729858,
        "water-fast-z-score": 6.207574452435047,
        "rewrite-fast-z-score": 0.09325048082403138
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The dust distribution in edge-on galaxies. Radiative transfer fits of V and K -band images .\nAbstract:\nWe present radiative transfer models for the dust distributions in two edge-on spiral galaxies, NGC 891 and NGC 4565. The model parameters are constrained by fitting to near-infrared (NIR) observations at 2.2 microns with NICMOS on HST as well as optical data obtained with WFPC-2. We find that both galaxies have significant amounts of dust distributed along their disks out to large distances above the midplane. In addition we detect an extended halo component around each galaxy which is best described by a spherical shell-like structure. For NGC 891 this component has a scale height of 1 kpc and extends up to 5 kpc above the disk plane. It contains about 10% of all dust mass within 10 kpc distance from the center. For NGC 4565 our results suggest a more complex geometry where the dust density decreases exponentially towards larger heights. This work was supported by NASA grant NAG5-7085.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The dust flow in edge - on galaxies . Radiative transfer fits of V and K - band images .Abstract : We create radiative transfer estimates for the dust distributions in two edge - on spiral galaxies , NGC 891 and NGC 4565 . The model variables are constrained by fitting to near - infrared ( NIR ) observations at 2 . 2 microns with NICMOS on HST as well as imaging information obtained with WFPC - 2 .We see that both galaxies have considerable amounts of dust spread along their disks out to large distances above the midplane . In addition we locate an extended halo element around each galaxy which is better represented by a spherical shell - like structure .For NGC 891 this component has a scale width of 1 kpc and extends up to 5 kpc above the disk plane . It contains about 10 % of all dust mass within 10 kpc length from the center .For NGC 4565 our results suggest a more complex geometry where the dust density decreases exponentially towards larger heights . This work was supported by NASA grant NAG5 - 7085 .",
        "rewrite_text": "In this study, we present a comprehensive analysis of the dust distributions in two edge-on spiral galaxies, NGC 891 and NGC 4565, utilizing radiative transfer modeling based on near-infrared (NIR) observations. Our approach involves fitting the model parameters to high-resolution imaging data acquired at 2.2 microns using the Near Infrared Camera and Multi-Object Spectrometer (NICMOS) aboard the Hubble Space Telescope (HST), alongside additional imaging from the Wide Field Planetary Camera 2 (WFPC2). Our findings reveal that both galaxies possess significant amounts of dust that extend along their disks, reaching considerable distances above the midplane. \n\nFurthermore, we identify the presence of an extended dust halo surrounding each galaxy, which we characterize as a spherical shell-like structure. For NGC 891, this halo exhibits a scale height of approximately 1 kpc and extends up to 5 kpc above the galactic disk, accounting for roughly 10% of the total dust mass within a 10 kpc radius from the galactic center. In contrast, our analysis of NGC 4565 indicates a more intricate dust distribution, with the dust density exhibiting an exponential decline as one moves to greater heights above the disk. \n\nThese results enhance our understanding of the vertical dust distribution in edge-on galaxies and contribute to the broader field of galactic structure and evolution. The research was conducted with the support of NASA grant NAG5-7085, underscoring the importance of funding in advancing astronomical studies. Overall, our work provides valuable insights into the role of dust in shaping the properties of edge-on spiral galaxies and sets the stage for future investigations into the dynamics and composition of galactic dust.",
        "ori-fast-z-score": 0.3464101615137754,
        "water-fast-z-score": 3.670651741928988,
        "rewrite-fast-z-score": 0.7364596943186588
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Nonlinear Dynamics of Infectious Diseases Transfer with Possible Applications for Tubercular Infection .\nAbstract:\nWe study the nonlinear dynamics of infectious diseases transfer in a population, where individuals are divided into three classes: susceptible (S), infected (I) and recovered/removed (R). We consider two different models: SIR model and SEIR model. In both cases we assume that there is no birth or death in the population. The main goal of this work is to investigate how the disease spreads through the population depending on its parameters. For example, if the infection rate is too high then it may lead to an epidemic outbreak. On the other hand, if the recovery rate is very large compared to the infection rate then the number of infectives will decrease rapidly. Finally, we show some numerical simulations which illustrate our results. \n \n Keywords: Nonlinear dynamics, infectious diseases, tuberculosis, SIR model, SEIR model. 1 Introduction \n \n Many mathematical models have been developed over time to describe the spread of infectious diseases within populations  1–3  . These models can be used as tools to understand the transmission mechanisms of these diseases and help public health authorities make decisions about prevention strategies  4  .\n \nIn particular, many researchers have studied the effects of vaccination programs  5–7  , quarantine  8, 9  and isolation  10, 11  on the evolution of epidemics. Other studies focus on the impact of environmental factors such as temperature  12, 13  , humidity  14, 15  and rainfall  16  on the propagation of pathogens. \nThe majority of existing works use deterministic models based on ordinary differential equations  17  . However, stochastic models  18, 19  and agent-based models  20, 21  also exist. Agent-based models allow us to take into account individual behaviors  22  while stochastic models provide more realistic descriptions of random events  23  . \n \nIn this article, we propose new mathematical models describing the spread of infectious diseases in a closed population. Our aim is to analyze the influence of various parameters on the behavior of the system. More specifically, we want to determine whether the disease will die out naturally or cause an epidemic outbreak. To do so, we first introduce the basic reproduction number R0  24  , which represents the average number",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Nonlinear Dynamics of Infectious Diseases Transfer with Possible Applications for Tubercular Infection . Abstract : We research the nonlinear dynamics of infectious infections transfer in a population , where persons are split into three categories : resistant ( S ) , infected ( I ) and returned / deleted ( R ) .We consider two different models : SIR model and SEIR model . In both cases we suppose that there is no death or dying in the population .The main goal of this project is to examine how the infection spreads through the population depending on its criteria . For instance , if the infection rate is too high then it could lead to an outbreak outbreak .On the other hand , if the return frequency is very huge compared to the infection rate then the quantity of infectives will decrease rapidly . Finally , we give some numerical simulations which illustrate our findings .Keywords : Nonlinear dynamics , infectious infections , tuberculosis , SIR model , SEIR model . 1 Introduction Many numerical models have been created over time to explain the spread of infectious infections within communities 1 – 3 .These systems can be used as tools to explain the spreading patterns of these diseases and help public medical institutions making decisions about prevention tactics 4 . In particular , many scientists have researched the effects of vaccination programs 5 – 7 , quarantine 8 , 9 and isolation 10 , 11 on the evolution of epidemics .Other studies emphasis on the impact of environmental factors such as temperature 12 , 13 , moisture 14 , 15 and rainfall 16 on the propagation of pathogens . The majority of older projects using deterministic methods using on ordinary differential coefficients 17 .However , stochastic theories 18 , 19 and agent - based models 20 , 21 also exist . Agent - based models let us to take into consideration individual behaviors 22 while stochastic theories provide more realistic descriptions of random events 23 .In this page , we propose new numerical models explaining the spread of infectious infections in a closed population . Our aim is to analyze the impact of several variables on the activity of the system .More specifically , we try to measure whether the infection will die out naturally or result an outbreak outbreak . To do so , we first introduce the fundamental reproduction number R0 24 , which equals the average number",
        "rewrite_text": "**Title:** Nonlinear Dynamics of Infectious Disease Transmission with Potential Applications for Tuberculosis Infection\n\n**Abstract:** This study investigates the nonlinear dynamics of infectious disease transmission within a population categorized into three distinct groups: susceptible (S), infected (I), and recovered (R). We analyze two primary models: the SIR model and the SEIR model, both of which assume a closed population with no mortality. The primary objective of this research is to explore the mechanisms by which infections propagate through the population based on various parameters. For instance, a high infection rate may precipitate an outbreak, while a significantly elevated recovery rate can lead to a swift decline in the number of infected individuals. We present numerical simulations that substantiate our theoretical findings, illustrating the intricate dynamics of disease spread. \n\n**Keywords:** Nonlinear dynamics, infectious diseases, tuberculosis, SIR model, SEIR model.\n\n**1. Introduction:** Over the years, numerous numerical models have been developed to elucidate the transmission of infectious diseases within communities. These models serve as valuable tools for understanding disease propagation patterns and assisting public health authorities in formulating effective prevention strategies. A considerable body of research has focused on the implications of vaccination programs, quarantine measures, and isolation practices on epidemic progression. Additionally, the influence of environmental factors—such as temperature, humidity, and rainfall—on pathogen transmission has been extensively studied. Historically, many of these models have employed deterministic approaches based on ordinary differential equations. However, alternative methodologies, including stochastic models and agent-based simulations, have emerged, allowing for a more nuanced consideration of individual behaviors and the inherent randomness of disease spread. In this paper, we propose novel numerical models that capture the dynamics of infectious disease transmission in a closed population. Our analysis aims to assess the effects of various parameters on the system's behavior, specifically determining whether an infection will naturally extinguish or escalate into an outbreak. To facilitate this analysis, we introduce the basic reproduction number, R0, which represents the average number of secondary infections generated by an infected individual in a fully susceptible population.",
        "ori-fast-z-score": -0.6064784348631227,
        "water-fast-z-score": 9.846562966837059,
        "rewrite-fast-z-score": 0.15249857033260467
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Renormalization and Effective Actions for General Relativity .\nAbstract:\nThe renormalization group is used to study the effective action in quantum gravity, which describes how matter fields interact with gravitons.  The gravitational field equations are derived by varying this action with respect to the metric tensor. This approach leads to an understanding of why general relativity has no ultraviolet divergences at one-loop order (i.e., when only one graviton loop is included). It also provides insight into the physical meaning of the cosmological constant term that appears in Einstein s field equations. In addition, it allows us to calculate corrections to the classical theory due to higher-order terms in the expansion of the effective action about flat space-time. Finally, we discuss some open problems related to the application of these techniques to more complicated theories such as superstrings or supergravity. The renormalization group is applied to the problem of studying the effective action in quantum-gravitational systems. We derive the gravitational field equations by varying this action with regard to the metric tensor. These results provide insights into the physical meaning of both the cosmological constant appearing in Einstein s field equations and the corrections to the classical theory arising from higher-order terms in the effective-action expansion around flat spacetime.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Renormalization and Effective Actions for General Relativity . Abstract : The renormalization group is utilized to study the effective action in quantum gravitational , which explains how matter fields interact with gravitons .The gravitational field equations are derived by varying this action with regard to the metric tensor . This method results to an knowledge of why general relativity has no ultraviolet divergences at one - loop order ( i . e . , when only one graviton loop is included ) .It additionally offers knowledge into the physical meaning of the cosmological constant term that arises in Einstein s field equations . In addition , it allows us to estimate corrections to the classical theory owing to higher - order terms in the expansion of the effective action about flat space - time .Finally , we explain some open problems related to the implementation of these concepts to more complicated models such as superstrings or supergravity . The renormalization group is applied to the question of examining the effective action in quantum - gravitational systems .We derive the gravitational field equations by varying this action with regard to the metric tensor . These data provide insights into the physical meaning of both the cosmological constant appearing in Einstein s field equations and the corrections to the classical theory arising from lower - order terms in the effective - action expansion around flat spacetime .",
        "rewrite_text": "**Title:** Renormalization and Effective Actions for General Relativity\n\n**Abstract:** This article explores the application of the renormalization group to the effective action in quantum gravity, shedding light on the interactions between matter fields and gravitons. By varying the effective action with respect to the metric tensor, we derive the gravitational field equations, which elucidate the absence of ultraviolet divergences in general relativity at one-loop order—where only a single graviton loop is considered. This analysis not only clarifies the significance of the cosmological constant term present in Einstein's field equations but also facilitates the assessment of corrections to classical gravitational theory due to higher-order terms in the effective action expansion around flat spacetime. Furthermore, we address several unresolved issues concerning the application of these theoretical frameworks to more complex models, including superstrings and supergravity. The insights gained from this study contribute to a deeper understanding of the fundamental aspects of quantum gravity and its implications for the broader field of theoretical physics. By examining the effective action in quantum-gravitational systems, we provide a comprehensive overview of how these concepts can be utilized to enhance our understanding of gravitational interactions and the underlying structure of spacetime. This work not only reinforces the theoretical foundations of general relativity but also opens avenues for future research in the quest to unify gravity with quantum mechanics.",
        "ori-fast-z-score": -0.29002094671369905,
        "water-fast-z-score": 3.73552251236249,
        "rewrite-fast-z-score": -0.09667364890456635
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Electromagnetic wave propagation in an almost circular bundle of closely packed, metallic, carbon nanotubes .\nAbstract:\nWe study the electromagnetic wave propagation through a bundle of closely-packed metallic single-walled carbon nanotubes (SWCNTs). The SWCNTs  diameter is assumed to be 1 nm and their length is taken as 10 microns. We use the finite element method for solving Maxwell s equations with appropriate boundary conditions at the ends of each tube. Our results show that there are two different regimes depending on the frequency range considered. In one regime, we find that the transmission coefficient decreases rapidly when increasing the number density of tubes. This behavior can be explained by considering the effect of multiple scattering between neighboring tubes. On the other hand, in another regime where the wavelength is much larger than the tube radius, the transmission coefficient increases slowly with respect to the number density of tubes due to constructive interference effects among scattered waves inside individual tubes. Finally, our numerical results indicate that the presence of defects such as vacancies or impurities may significantly affect the overall transmission properties of the system.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Electromagnetic signal propagation in an almost circular bundle of closely packed , metallic , carbon nanotubes . Abstract : We research the electromagnetic wave propagation through a bundle of closely - packed metallic single - walled carbon nanotubes ( SWCNTs ) .The SWCNTs diameter is expected to be 1 nm and their height is taken as 10 microns . We use the finite element method for solving Maxwell s coefficients with suitable boundary constraints at the ends of each tube .Our results show that there are two different regimes based on the frequency spectrum considered . In one regime , we find that the propagation coefficient drops rapidly when increasing the number density of tubes .This phenomenon can be described by using the impact of multiple scattering between neighboring tubes . On the other hand , in another regime where the frequency is much larger than the tube diameter , the propagation coefficient increases slowly with regard to the number density of tubes due to constructive interference effects among scattered waves inside individual tubes .Finally , our numerical findings show that the presence of flaws such as vacancies or impurities may significantly affect the overall transmission properties of the system .",
        "rewrite_text": "Title: Electromagnetic Signal Propagation in an Almost Circular Bundle of Closely Packed Metallic Carbon Nanotubes\n\nAbstract: This study investigates the propagation of electromagnetic waves through a tightly packed bundle of metallic single-walled carbon nanotubes (SWCNTs), characterized by a diameter of approximately 1 nm and a height of 10 microns. Utilizing the finite element method, we solve Maxwell's equations under appropriate boundary conditions applied at the ends of each nanotube. Our analysis reveals two distinct regimes of wave propagation, which depend on the frequency spectrum examined. In the first regime, we observe a rapid decline in the propagation coefficient as the number density of the nanotubes increases. This behavior can be attributed to the effects of multiple scattering occurring between adjacent tubes, which hinders the transmission of electromagnetic signals. Conversely, in the second regime, where the frequency significantly exceeds the tube diameter, the propagation coefficient exhibits a gradual increase with rising tube density. This enhancement is primarily due to constructive interference among the scattered waves within individual nanotubes, which facilitates better signal transmission. Additionally, our numerical simulations indicate that the presence of defects, such as vacancies or impurities within the nanotube structure, can markedly influence the overall transmission characteristics of the bundle. These findings provide valuable insights into the complex interactions governing electromagnetic signal propagation in nanostructured materials and highlight the importance of structural integrity in optimizing the performance of carbon nanotube-based devices.",
        "ori-fast-z-score": 0.19802950859533489,
        "water-fast-z-score": 4.902903378454601,
        "rewrite-fast-z-score": 1.993231791080248
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Horizon-Penetrating Transonic Accretion Disks around Rotating Black Holes .\nAbstract:\nWe present the results of our numerical simulations of accreting black holes in which we have included general relativistic effects and radiative transfer using Monte Carlo techniques. We find that for low mass (M < 10 M_sun) black holes, there is an inner edge to the disk at about 3 Schwarzschild radii where the density drops sharply by several orders of magnitude. The temperature profile shows a sharp rise near this radius due to compression as well as heating by viscous dissipation. For higher masses (10 M_sun < M < 100 M_sun), the disks are optically thick throughout their extent with no clear evidence of any inner edge. In these cases, the temperature profiles show a gradual increase towards smaller radii. Finally, for very massive black holes (M > 100 M_sun), we find that the disks become geometrically thin but remain optically thick out to large distances.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Horizon - Penetrating Transonic Accretion Disks around Rotating Black Holes . Abstract : We present the results of our numerical simulations of accreting grey holes in which we have incorporated general relativistic effects and radiative transfer utilizing Monte Carlo methods .We see that for low weight ( M < 10 M _ sun ) black holes , there is an inner boundary to the disk at about 3 Schwarzschild radii where the density decreases sharply by many orders of magnitude . The temperature profile displays a sharp rise near this radius due to compression as well as cooling by viscous dissipation .For larger masses ( 10 M _ sun < M < 100 M _ sun ) , the disks are optically dense throughout their depth with no clear proof of any outer edge . In these circumstances , the temperature profiles exhibit a slow increase towards smaller radii .Finally , for very huge black holes ( M > 100 M _ sun ) , we find that the disks become geometrically thin but remain optically dense out to large distances .",
        "rewrite_text": "We present the findings from our numerical simulations of accreting black holes, specifically focusing on the incorporation of general relativistic effects and radiative transfer through Monte Carlo methods. Our study examines the behavior of accretion disks around black holes of varying masses, revealing significant insights into their structure and dynamics. For black holes with masses less than 10 solar masses (M < 10 M_sun), we observe a distinct inner boundary of the accretion disk located approximately at three Schwarzschild radii. At this boundary, there is a dramatic decrease in density by several orders of magnitude. Concurrently, the temperature profile shows a pronounced increase near this radius, attributed to both compression effects and cooling mechanisms driven by viscous dissipation.\n\nIn contrast, for black holes with masses ranging from 10 to 100 solar masses (10 M_sun < M < 100 M_sun), the accretion disks maintain a high optical density throughout their entire depth, lacking any definitive outer edge. In these cases, the temperature profiles demonstrate a gradual rise as one approaches smaller radii, indicating a more stable and sustained accretion process.\n\nFor supermassive black holes exceeding 100 solar masses (M > 100 M_sun), our simulations reveal that the accretion disks become geometrically thin while still retaining a high optical density over considerable distances. This suggests that even at large radii, the disks are capable of sustaining significant mass accretion, which may have implications for the growth and evolution of supermassive black holes in the universe. Overall, our results contribute to a deeper understanding of the complex interactions within accretion disks and their dependence on black hole mass, providing a foundation for future research in the field of astrophysics.",
        "ori-fast-z-score": -1.62746694241347,
        "water-fast-z-score": 3.5795716689756794,
        "rewrite-fast-z-score": 0.5937322507759797
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Incoherent non-Fermi liquid scattering in a Kondo lattice .\nAbstract:\nWe study the incoherent transport properties of a strongly correlated electron system, namely the periodic Anderson model (PAM) with strong hybridization between conduction and localized electrons. We show that this PAM can be mapped onto an effective spinless fermion model which is equivalent to a disordered Fermi gas at low temperatures. The resulting density of states exhibits a pseudogap structure near the chemical potential. In addition we find that the resistivity shows a linear temperature dependence below some characteristic temperature T*. This behavior is consistent with recent experiments on heavy-fermion compounds such as Yb$_{1}$-xYb$x$Cu$_2$Si$2$O$_{7-x}$. Finally, we discuss how our results are related to previous theoretical work based on the dynamical mean-field theory. PACS numbers: 71.27.+a, 72.25.+f, 74.78.+h",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Incoherent non - Fermi liquid scattering in a Kondo structure . Abstract : We explore the incoherent transport properties of a strongly interacting particle system , namely the periodic Anderson model ( PAM ) with powerful hybridization between conduction and localized electrons .We see that this PAM can be mapped onto an efficient spinless fermion theory which is analogous to a disordered Fermi gas at low temperatures . The resulting concentration of states shows a pseudogap structure near the chemical potential .In addition we find that the resistivity displays a linear temperature dependence below some characteristic temperature T * . This phenomenon is consistent with recent experiments on heavy - fermion compounds such as Yb $ _ { 1 } $ - xYb $ x $ Cu $ _ 2 $ Si $ 2 $ O $ _ { 7 - x } $ .Finally , we talk how our findings are related to previous conceptual research based on the dynamical mean - field model . PACS numbers : 71 . 27 . + a , 72 . 25 . + f , 74 . 78 . + h",
        "rewrite_text": "Title: Incoherent Non-Fermi Liquid Scattering in a Kondo Structure\n\nAbstract: In this study, we investigate the incoherent transport characteristics of a strongly interacting particle system, specifically the periodic Anderson model (PAM), which features significant hybridization between conduction and localized electrons. Our analysis reveals that the PAM can be effectively transformed into a spinless fermion theory, drawing parallels to a disordered Fermi gas at low temperatures. This mapping allows us to examine the density of states, which exhibits a pseudogap structure in proximity to the chemical potential. Furthermore, we observe that the resistivity of the system demonstrates a linear temperature dependence below a critical temperature, denoted as T*. This behavior aligns with recent experimental observations in heavy-fermion materials, such as Yb$_{1-x}$Yb$_{x}$Cu$_{2}$Si$_{2}$O$_{7-x}$. Our findings contribute to the understanding of incoherent transport phenomena in strongly correlated electron systems and are discussed in relation to previous theoretical frameworks, particularly those based on the dynamical mean-field theory. The implications of our results extend to the broader context of non-Fermi liquid behavior in condensed matter physics, shedding light on the intricate interplay between electron correlations and transport properties in Kondo lattice systems. The PACS numbers associated with this research are 71.27.+a, 72.25.+f, and 74.78.+h, highlighting the relevance of our work to the fields of strongly correlated electrons, transport phenomena, and superconductivity.",
        "ori-fast-z-score": -1.2309149097933272,
        "water-fast-z-score": 3.6380343755449944,
        "rewrite-fast-z-score": 2.6603890510435915
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The interplay between radio galaxies and cluster environment .\nAbstract:\nWe present the results of an optical spectroscopic survey of radio galaxies in clusters at redshifts z=0.4-0.8, carried out with the William Herschel Telescope (WHT). We find that the fraction of AGN-powered radio galaxies decreases towards higher galaxy density environments within the clusters. This is consistent with previous studies which have found evidence for environmental quenching of star formation activity among massive galaxies. However we also find that there are many examples where powerful radio sources reside in dense regions without any obvious signs of being environmentally suppressed. These objects may be undergoing rapid evolution or they could represent a population of recently accreted field galaxies whose properties are still evolving to resemble those of their local counterparts. The sample consists of 20 radio galaxies selected from the VLA-COSMOS 3 GHz Large Project catalogue (Smolčić et al., 2009) using the following criteria: 1) They lie in one of four X-ray luminous clusters at 0.4 < z < 0.8; 2) Their radio luminosity lies above L(3GHz) = 10 25 W Hz-1; 3) They do not show strong emission lines indicative of ongoing nuclear activity; 4) They were observed during our WHT run on 2010 May 24-25.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The interplay between radio stars and cluster environment . Abstract : We report the conclusion of an optical spectroscopic study of radio nuclei in clusters at redshifts z = 0 . 4 - 0 . 8 , conducted out with the William Herschel Telescope ( WHT ) .We see that the fraction of AGN - powered radio stars reduces towards higher galaxy concentration environments within the clusters . This is consistent with previous research which have discovered evidence for environmental quenching of galaxy formation activity among huge clusters .However we also find that there are many instance where powerful radio sources exist in dense areas without any obvious signs of being environmentally suppressed . These bodies may be experiencing rapid evolution or they may contain a population of newly accreted field galaxies whose properties are still changing to reflect those of their nearby rivals .The sample consists of 20 radio stars selected from the VLA - COSMOS 3 GHz Large Project catalogue ( Smolčić et al . , 2009 ) applying the following standards : 1 ) They sit in one of four X - ray luminous clusters at 0 . 4 < z < 0 . 8 ; 2 ) Their radio luminosity sits above L ( 3GHz ) = 10 25 W Hz - 1 ; 3 ) They do not show weak absorption patterns indicative of ongoing nuclear activity ; 4 ) They were detected during our WHT running on 2010 May 24 - 25 .",
        "rewrite_text": "We present the findings of an optical spectroscopic investigation into the relationship between radio stars and their surrounding cluster environments, focusing on clusters at redshifts ranging from z = 0.4 to z = 0.8. This study was conducted using the William Herschel Telescope (WHT) and involved a sample of 20 radio stars selected from the VLA-COSMOS 3 GHz Large Project catalog (Smolčić et al., 2009). The criteria for selection included: (1) the presence of the radio stars within one of four X-ray luminous clusters at redshifts between 0.4 and 0.8; (2) a radio luminosity exceeding L(3GHz) = 10^25 W Hz^-1; (3) the absence of weak absorption features that would suggest ongoing nuclear activity; and (4) detection during our WHT observations on May 24-25, 2010. \n\nOur analysis reveals a notable trend: the proportion of active galactic nuclei (AGN)-powered radio stars diminishes in regions of higher galaxy concentration within the clusters. This observation aligns with previous studies that have indicated environmental quenching of galaxy formation activities in massive clusters. However, we also identified several instances where robust radio sources are present in densely populated areas without clear evidence of environmental suppression. These cases may indicate that the radio stars are undergoing rapid evolutionary changes or that they harbor a population of newly accreted field galaxies, whose characteristics are still adapting to the influences of their neighboring galaxies. Our findings contribute to the understanding of how environmental factors affect the evolution of radio stars in cluster settings, highlighting the complexity of interactions within these cosmic structures.",
        "ori-fast-z-score": -1.3199500146737049,
        "water-fast-z-score": 6.934760925288564,
        "rewrite-fast-z-score": 2.5350008891905187
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Lyman Break Galaxies at z~1 and the evolution of the dust attenuation in star-forming galaxies with the redshift .\nAbstract:\nWe present new results on the evolution of the dust content in Lyman break galaxies (LBGs) using deep near-infrared data obtained by the UltraVISTA survey, which is part of the Sloan Digital Sky Survey III program. We use these observations to study the rest-frame UV-optical properties of LBGs at redshifts 1<z<3.5. The main goal of this work was to investigate how the dust extinction evolves as a function of galaxy mass and star formation rate density over cosmic time. Our analysis shows that there are two different populations of LBGs: one population has low stellar masses (M*=10^10-10^11 Msun), high specific star formation rates (SSFR>100Gyr-1), and relatively small amounts of dust; while another population consists of more massive systems (M*>10^11Msun), lower SSFR values (SSFR<30Gyr-1), and higher levels of dust extinction. These findings suggest that the amount of dust increases with increasing galaxy mass for both local and distant galaxies.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Lyman Break Galaxies at z ~ 1 and the evolution of the dust attenuation in star - creating stars with the redshift . Abstract : We report new data on the evolution of the dust content in Lyman break galaxies ( LBGs ) using deep near - infrared results collected by the UltraVISTA census , which is part of the Sloan Digital Sky Survey III program .We use these observations to study the rest - frame UV - optical properties of LBGs at redshifts 1 < z < 3 . 5 . The main goal of this research was to examine how the dust extinction evolves as a function of galaxy mass and galaxy formation rate concentration over cosmic time .Our study shows that there are two different populations of LBGs : one population has low stellar masses ( M * = 10 ^ 10 - 10 ^ 11 Msun ) , low specific star formation rates ( SSFR > 100Gyr - 1 ) , and fairly large quantities of dust ; while another population contains of more massive structures ( M * > 10 ^ 11Msun ) , lower SSFR values ( SSFR < 30Gyr - 1 ) , and low levels of dust extinction . These conclusions show that the quantity of dust increases with increasing galaxy mass for both local and distant galaxies .",
        "rewrite_text": "We present new findings on the evolution of dust content in Lyman Break Galaxies (LBGs) utilizing deep near-infrared data obtained from the UltraVISTA survey, which is part of the Sloan Digital Sky Survey III initiative. This research focuses on the rest-frame UV-optical characteristics of LBGs within the redshift range of 1 < z < 3.5. The primary objective of our study is to investigate how dust extinction varies in relation to galaxy mass and the concentration of star formation rates over cosmic time. Our analysis reveals the existence of two distinct populations of LBGs. The first population consists of galaxies with lower stellar masses (M* = 10^10 - 10^11 M☉), exhibiting high specific star formation rates (SSFR > 100 Gyr^-1) and significant amounts of dust. In contrast, the second population is characterized by more massive galaxies (M* > 10^11 M☉), which display lower SSFR values (SSFR < 30 Gyr^-1) and minimal dust extinction. These findings indicate a correlation between dust quantity and galaxy mass, suggesting that as galaxy mass increases, so does the amount of dust, a trend observed in both local and distant galaxies. This research contributes to our understanding of the interplay between dust content, galaxy formation, and evolution in the context of cosmic history.",
        "ori-fast-z-score": 0.6396021490668313,
        "water-fast-z-score": 5.405989188032437,
        "rewrite-fast-z-score": 0.502518907629606
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Local Galaxy 8 micron Luminosity Function .\nAbstract:\nWe present the luminosity function (LF) for galaxies in the local universe at rest-frame wavelengths between 3 and 24 microns, using data obtained with Spitzer Space Telescope s Infrared Array Camera (IRAC). We use two different methods to estimate the LF - one based on direct counts of galaxies within bins of absolute magnitude, and another that uses an analytic model fit to these galaxy counts. The results are consistent across both techniques. Our best-fit Schechter parameters are: M* = -19.6 +/- 0.1 mag., log(L/Lsun) * = 10.9 +/- 0.2 dex, and alpha = -1.3 +/- 0.4. These values agree well with previous determinations made by other authors over similar wavelength ranges. However, we find evidence for a significant excess number density of faint sources relative to predictions from our best-fit Schechte functions. This excess is most pronounced at longer wavelengths where it amounts to ~50% more objects than expected.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Local Galaxy 8 micron Luminosity Function . Abstract : We present the luminosity function ( LF ) for galaxies in the local universe at rest - frame wavelengths between 3 and 24 microns , using data acquired with Spitzer Space Telescope s Infrared Array Camera ( IRAC ) .We use two different methods to estimate the LF - one based on direct counts of galaxies within bins of absolute magnitude , and another that using an analytic model suited to these galaxy counts . The results are compatible across both algorithms .Our best - fitting Schechter parameters are : M * = - 19 . 6 + / - 0 . 1 mag . , log ( L / Lsun ) * = 10 . 9 + / - 0 . 2 dex , and alpha = - 1 . 3 + / - 0 . 4 . These values comply better with previous determinations made by other researchers over similar frequency ranges .However , we find proof for a substantial accumulation number density of faint sources relative to expectations from our better - fitting Schechte functions . This excess is most pronounced at higher wavelengths where it amounts to ~ 50 % more bodies than expected .",
        "rewrite_text": "We present a comprehensive analysis of the luminosity function (LF) for galaxies in the local universe, focusing on rest-frame wavelengths ranging from 3 to 24 microns. This study utilizes data obtained from the Infrared Array Camera (IRAC) aboard the Spitzer Space Telescope. To estimate the LF, we employed two distinct methodologies: the first involves direct counts of galaxies categorized within specific bins of absolute magnitude, while the second utilizes an analytical model tailored to these galaxy counts. The outcomes derived from both approaches demonstrate strong compatibility, reinforcing the reliability of our findings.\n\nOur analysis yields the following best-fitting Schechter parameters: M* = -19.6 ± 0.1 mag, log(L/Lsun)* = 10.9 ± 0.2 dex, and α = -1.3 ± 0.4. These parameters align closely with previous determinations made by other researchers within similar frequency ranges, thereby validating our results. However, a notable discovery from our study is the significant excess in the number density of faint sources, which exceeds expectations based on our well-fitting Schechter functions. This excess is particularly pronounced at higher wavelengths, where we observe approximately 50% more faint galaxies than anticipated.\n\nThis finding suggests that the faint end of the galaxy luminosity function may be more populated than previously thought, prompting a reevaluation of the underlying models that describe galaxy formation and evolution. Our results contribute to a deeper understanding of the local galaxy population and highlight the importance of infrared observations in uncovering the characteristics of faint galaxies that may have been overlooked in earlier studies. Overall, this work enhances our comprehension of the local universe's structure and the distribution of galaxies across various luminosities.",
        "ori-fast-z-score": -0.9701425001453319,
        "water-fast-z-score": 3.585685828003181,
        "rewrite-fast-z-score": -1.9445436482630056
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Use of ITIL for Process Optimisation in the IT Service Centre of Harz University, exemplified in the Release Management Process .\nAbstract:\nThe purpose of this study is to examine how an organisation can use Information Technology Infrastructure Library (ITIL) as a process optimisation tool within its service centre environment. The research focuses on the release management process at the IT service centre of Harz University and examines whether it complies with all relevant aspects of the ITIL framework. A case study approach was used to collect data through interviews and document analysis. This article presents the results of the investigation into the current state of the release management process at Harz University s IT service centre. It also provides recommendations for improving the process based on the findings. Keywords: information technology infrastructure library, process optimization, release management process, university, IT service centre, case study method. 1 Introduction Today, universities are increasingly relying on their computer systems to support teaching and research activities. As such, they need reliable services that meet user requirements  1  . However, many universities have experienced problems related to poor quality software releases  2  , which may lead to system failures or even loss of data  3  .\nIn order to avoid these issues, organisations should implement appropriate processes  4  . One widely accepted set of guidelines for managing information technology (IT) services is provided by the Information Technology Infrastructure Library  5  . According to  6  , ITIL has been adopted by more than 80% of Fortune 500 companies worldwide since 2003.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Use of ITIL for Process Optimisation in the IT Service Centre of Harz University , exemplified in the Release Management Process . Abstract : The purpose of this study is to examine how an institution can using Information Technology Infrastructure Library ( ITIL ) as a process optimisation platform within its service centre environment .The studies centers on the release control system at the IT service centre of Harz University and examines whether it complies with all relevant parts of the ITIL framework . A case study method was used to collect data through interviews and report study .This page presents the conclusion of the examination into the present state of the release administration process at Harz University s IT network centre . It additionally offers suggestions for improving the process based on the discoveries .Keywords : info technology infrastructure library , process optimization , release administration process , university , IT service centre , case study method . 1 Introduction Today , universities are increasingly relying on their computer machines to support education and scientific activities .As such , they seek reliable services that meet user requirements 1 . However , many universities have experienced difficulties related to bad quality code releases 2 , which sometimes lead to system failures or even loss of documentation 3 .In try to minimize these problems , organisations should implement appropriate processes 4 . One widely accepted set of guidelines for controlling information tech ( IT ) services is provided by the Information Technology Infrastructure Library 5 .According to 6 , ITIL has been used by more than 80 % of Fortune 500 companies globally since 2003 .",
        "rewrite_text": "**Title:** The Use of ITIL for Process Optimization in the IT Service Centre of Harz University: A Case Study of the Release Management Process\n\n**Abstract:** This study investigates the application of the Information Technology Infrastructure Library (ITIL) as a framework for process optimization within the IT service centre of Harz University, specifically focusing on the release management process. The research aims to assess the alignment of the university's release control system with the ITIL framework, which is recognized for its effectiveness in enhancing IT service management. Utilizing a case study methodology, data was gathered through interviews and an analysis of relevant documentation to evaluate the current state of the release management process at the IT service centre. The findings reveal several areas where the existing process may not fully adhere to ITIL standards, highlighting potential inefficiencies and risks associated with poor quality code releases. These issues can lead to significant operational challenges, including system failures and documentation loss, which have been increasingly reported by universities relying heavily on their IT infrastructure to support educational and research activities. To address these challenges, the study proposes actionable recommendations aimed at refining the release management process, thereby enhancing service reliability and user satisfaction. The insights gained from this research not only contribute to the understanding of ITIL's applicability in an academic setting but also provide a framework for other institutions facing similar challenges in IT service management. \n\n**Keywords:** Information Technology Infrastructure Library, process optimization, release management process, university, IT service centre, case study methodology. \n\n**1 Introduction:** In the contemporary educational landscape, universities are becoming increasingly dependent on their IT systems to facilitate both teaching and research endeavors. Consequently, there is a growing demand for dependable IT services that effectively meet user expectations. However, many institutions have encountered significant challenges stemming from subpar code releases, which can result in system outages and the potential loss of critical documentation. To mitigate these risks, it is essential for organizations to adopt robust processes. The ITIL framework, which has been embraced by over 80% of Fortune 500 companies since 2003, offers a comprehensive set of best practices for managing IT services effectively.",
        "ori-fast-z-score": 0.18107149208503706,
        "water-fast-z-score": 8.082238591204872,
        "rewrite-fast-z-score": -0.8363145133966761
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The habitability of super-Earths in Gliese 581 .\nAbstract:\nWe present the results of our study on the possible existence and stability of terrestrial planets around the star Gliese 581, which is located at about 20 light-years away from Earth. We have performed numerical simulations for different orbital configurations of three hypothetical terrestrial planets with masses ranging between 1 to 10 times that of Earth s mass (1-10 M⊕). Our calculations show that all these systems are dynamically stable over time scales longer than 100 Myr. The most massive planet has an eccentric orbit with e=0.2 and its periastron distance ranges between 0.05 AU and 0.15 AU depending on the initial conditions used. This planet can be considered as a hot Jupiter-like planet because it orbits very close to its host star. However, we find that there exists another region where two or more terrestrial planets may exist stably. In this region, one of them could be a super-Earth-type planet with a mass larger than 5M⊕ but smaller than 8M⊕.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The habitability of super - Earths in Gliese 581 . Abstract : We present the conclusion of our research on the possible existence and stability of terrestrial worlds around the star Gliese 581 , which is situated at about 20 light - years far from Earth .We have done mathematical simulations for different orbital arrangements of three hypothetical terrestrial worlds with masses ranging between 1 to 10 twice that of Earth s mass ( 1 - 10 M⊕ ) . Our calculations show that all these systems are dynamically stable over time ranges longer than 100 Myr .The most large planet has an eccentric orbit with e = 0 . 2 and its periastron speed ranges between 0 . 05 AU and 0 . 15 AU depending on the first conditions utilized . This planet can be regarded as a bright Jupiter - like planet because it orbits very close to its host star .However , we find that there exists another region where two or more terrestrial worlds may arise stably . In this area , one of them could be a super - Earth - class planet with a mass greater than 5M⊕ but smaller than 8M⊕ .",
        "rewrite_text": "In this study, we explore the potential habitability and stability of terrestrial exoplanets orbiting the star Gliese 581, located approximately 20 light-years from Earth. Our research employs mathematical simulations to investigate various orbital configurations for three hypothetical terrestrial planets, each with masses ranging from 1 to 10 times that of Earth (1 - 10 M⊕). The results of our simulations indicate that these planetary systems exhibit dynamic stability over time scales exceeding 100 million years. Notably, the largest of the simulated planets possesses an eccentric orbit characterized by an eccentricity of 0.2, with its periastron distance fluctuating between 0.05 AU and 0.15 AU, contingent upon the initial conditions applied in the model. This particular planet can be likened to a bright Jupiter-like body due to its close proximity to its host star. Furthermore, our findings reveal the existence of a distinct region within the orbital dynamics where two or more terrestrial planets could coexist stably. Within this zone, we identify the possibility of a super-Earth-class planet, with a mass exceeding 5 M⊕ but less than 8 M⊕, suggesting that such worlds could potentially support conditions favorable for habitability. This research contributes to our understanding of the diverse planetary systems that may exist around M-dwarf stars like Gliese 581 and highlights the intriguing prospects for the existence of habitable environments on super-Earths within these systems.",
        "ori-fast-z-score": -1.62746694241347,
        "water-fast-z-score": 3.4412360080584263,
        "rewrite-fast-z-score": 0.29851115706299675
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  On-Line Condition Monitoring using Computational Intelligence .\nAbstract:\nThe aim of this book is to provide an overview on the state-of-the-art in on-line condition monitoring and fault diagnosis for industrial processes, with special emphasis on computational intelligence techniques such as neural networks (NNs), fuzzy logic systems (FLS) or evolutionary algorithms (EAs). The book covers both theoretical aspects and practical applications of these methods. It also discusses some recent advances in intelligent sensor technologies that are essential for successful implementation of on-line condition monitoring schemes. This book will be useful not only for researchers but also for engineers who want to apply computational intelligence techniques into their own research work. Contents include:  Chapter 1: Introduction to On-line Condition Monitoring.  Chapter 2: Intelligent Sensors for On-line Condition Monitoring.  Chapters 3-7: Neural Networks for Fault Diagnosis.  Chapters 8-10: Fuzzy Logic Systems for Fault Diagnosis.   Chapters 11-13: Evolutionary Algorithms for Fault Diagnosis.   ...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : On - Line Condition Monitoring using Computational Intelligence . Abstract : The goal of this book is to provide an overview on the state - of - the - art in on - line condition monitoring and failure detection for industrial systems , with special emphasis on mathematical intelligence techniques such as neural systems ( NNs ) , fuzzy logic systems ( FLS ) or genetic algorithms ( EAs ) .The text encompasses both theoretical components and useful use of these procedures . It additionally outlines some latest advances in intelligent detector technologies that are essential for successful implementation of on - line condition monitoring schemes .This book will be valuable not only for researchers but also for designers who desire to apply computational intelligence techniques into their own research effort . Contents include : Chapter 1 : Introduction to On - line Condition Monitoring .Chapter 2 : Intelligent Sensors for On - line Condition Monitoring . Chapters 3 - 7 : Neural Networks for Fault Diagnosis .Chapters 8-10: Fuzzy Logic Systems for Fault Diagnosis.Chapters 11-13: Evolutionary Algorithms for Fault Diagnosis....",
        "rewrite_text": "Title: On-Line Condition Monitoring using Computational Intelligence\n\nAbstract: This article presents a comprehensive overview of the current advancements in on-line condition monitoring and failure detection within industrial systems, focusing particularly on the application of computational intelligence methodologies. The discussion highlights various mathematical intelligence techniques, including neural networks (NNs), fuzzy logic systems (FLS), and evolutionary algorithms (EAs), which are pivotal in enhancing the reliability and efficiency of monitoring processes. The text integrates both theoretical foundations and practical applications of these techniques, providing a balanced perspective on their implementation in real-world scenarios. Furthermore, it examines recent innovations in intelligent detection technologies that are crucial for the effective deployment of on-line condition monitoring systems. This article serves as a valuable resource for both researchers and practitioners in the field, offering insights into how computational intelligence can be leveraged to improve diagnostic capabilities. The structure of the article is organized into several chapters: Chapter 1 introduces the concept of on-line condition monitoring; Chapter 2 discusses intelligent sensors designed for this purpose; Chapters 3 to 7 delve into the role of neural networks in fault diagnosis; Chapters 8 to 10 explore the application of fuzzy logic systems for diagnosing faults; and Chapters 11 to 13 focus on the use of evolutionary algorithms in fault diagnosis. Collectively, these sections aim to equip readers with a thorough understanding of the methodologies and technologies that underpin modern condition monitoring practices, fostering further research and development in this critical area of industrial maintenance.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.88240082724041,
        "rewrite-fast-z-score": 0.8838834764831843
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Gamma-ray albedo of the moon .\nAbstract:\nWe present results on gamma ray albedo of the Moon obtained with Fermi Large Area Telescope (LAT). The data set consists of more than 1 billion photons collected between August 4, 2008 and September 30, 2012 in the energy range 0.1 - 100 GeV. We find that the observed flux is consistent with previous measurements by EGRET but higher than those reported by AGILE and Fermi-GBM experiments. This discrepancy may be explained by different angular resolutions of these instruments. Our analysis shows no significant time variability for the total emission as well as its individual components. In addition to the previously known sources we detect several new point-like sources at high confidence level. These include two bright unidentified sources located near the lunar poles which are likely associated with dark matter annihilation or decay. \n \n Keywords: Gamma rays, Moon, Dark Matter, LAT, Fermi Observatory, Point Sources, Unidentified",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Gamma - ray albedo of the lunar . Abstract : We report findings on gamma ray albedo of the Moon obtained with Fermi Large Area Telescope ( LAT ) .The data set consists of more than 1 billion photons collected between August 4 , 2008 and September 30 , 2012 in the power range 0 . 1 - 100 GeV . We see that the observed flux is compatible with previous measurements by EGRET but higher than those published by AGILE and Fermi - GBM observations .This discrepancy may be reason by varying angular resolutions of these instruments . Our study shows no considerable time variability for the total emission as well as its various components .In addition to the previously known sources we find several novel point - like sources at high confidence rate . These include two faint unidentified sources located near the lunar poles which are likely correlated with black material annihilation or degradation .Keywords : Gamma rays , Moon , Dark Matter , LAT , Fermi Observatory , Point Sources , Unidentified",
        "rewrite_text": "We present a comprehensive analysis of the gamma-ray albedo of the Moon, utilizing data acquired from the Fermi Large Area Telescope (LAT). The dataset encompasses over one billion photons collected during the period from August 4, 2008, to September 30, 2012, across an energy range of 0.1 to 100 GeV. Our findings indicate that the observed gamma-ray flux aligns with earlier measurements obtained by the EGRET instrument; however, it is notably higher than the results reported by AGILE and Fermi-GBM. This discrepancy may be attributed to the differing angular resolutions of the various instruments employed in these studies. \n\nMoreover, our investigation reveals no significant temporal variability in the total gamma-ray emission or its individual components. In addition to confirming previously identified sources, we have detected several new point-like sources with a high degree of confidence. Among these novel sources are two faint, unidentified emissions situated near the lunar poles, which we hypothesize may be associated with processes such as black material annihilation or degradation. \n\nThis research contributes to our understanding of gamma-ray emissions from the Moon and has implications for studies related to dark matter and cosmic phenomena. The results underscore the importance of utilizing advanced observational techniques to enhance our knowledge of celestial gamma-ray sources and their characteristics. \n\nKeywords: Gamma rays, Moon, Dark Matter, LAT, Fermi Observatory, Point Sources, Unidentified.",
        "ori-fast-z-score": -0.601929265428846,
        "water-fast-z-score": 4.780914437337574,
        "rewrite-fast-z-score": 0.2
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Gauge-Higgs Unification and Radiative Electroweak Symmetry Breaking in Warped Extra Dimensions .\nAbstract:\nWe study the possibility that electroweak symmetry breaking is triggered by strong dynamics at an ultraviolet (UV) fixed point, as suggested by gauge-Higgs unification models with warped extra dimensions. We show how this scenario can be realized within the framework of composite Higgs models based on strongly-coupled gauge theories. In particular we consider two different realizations of such scenarios: one where the Higgs arises as a pseudo-Nambu-Goldstone boson associated to spontaneous breaking of approximate global symmetries; another where it emerges as a bound state of new fermions charged under the Standard Model gauge group. The latter case leads to novel signatures for Higgs production through gluon fusion which are potentially observable at future colliders. Finally, we discuss possible implications of these results for cosmology. Gauge-Higgs unification provides a compelling explanation for why the weak scale is so much smaller than any other mass scale in nature  1  . It also offers a natural solution to the hierarchy problem between the Planck and TeV scales  2  , since quantum corrections to the Higgs potential are cut off at the UV scale  3  .\nIn order to realize this idea in practice, however, several challenges must be overcome  4  : i) the Higgs should arise naturally out of some strongly coupled sector; ii) the Higgs couplings to SM particles should agree with experiment; iii) there should exist a mechanism to generate masses for all SM fields without introducing large hierarchies among them. These issues have been addressed recently using the Randall-Sundrum model  5  , where the Higgs field lives on the IR brane while gravity propagates into the bulk  6  -  8  . This setup allows for a calculable description of the Higgs physics  9  , but introduces additional complications due to the presence of Kaluza-Klein gravitons  10  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Gauge - Higgs Unification and Radiative Electroweak Symmetry Breaking in Warped Extra Dimensions . Abstract : We research the idea that electroweak symmetry breaking is caused by strong dynamics at an ultraviolet ( UV ) fixed point , as suggested by gauge - Higgs unification systems with warped added dimensions .We see how this situation can be realized within the framework of composite Higgs theories based on highly - coupled gauge theories . In particular we treat two different realizations of such scenarios : one where the Higgs arises as a quasi - Nambu - Goldstone boson associated to spontaneous breaking of approximate global symmetries ; another where it appears as a bound state of new fermions charged under the Standard Model gauge group .The latter example results to novel signatures for Higgs production through gluon fusion which are possibly observable at potential colliders . Finally , we explain possible possibilities of these results for cosmology .Gauge - Higgs unification presents a powerful explanation for why the weak scale is so even smaller than any other mass level in nature 1 . It additionally offers a natural solution to the ranking problem between the Planck and TeV scales 2 , since quantum corrections to the Higgs potential are cut off at the UV scale 3 .In order to realize this idea in practice , however , various challenges must be overcome 4 : i ) the Higgs should occur readily out of some strongly coupled sector ; ii ) the Higgs couplings to SM objects should comply with research ; iii ) there should exist a process to produce masses for all SM fields without eliminating huge hierarchies among them . These issues have been addressed recently utilizing the Randall - Sundrum model 5 , where the Higgs field lives on the IR brane while gravity propagates into the bulk 6 - 8 .This configuration allows for a calculable description of the Higgs physics 9 , but introduces additional difficulties owing to the presence of Kaluza - Klein gravitons 10 .",
        "rewrite_text": "In this article, we explore the concept that electroweak symmetry breaking may stem from strong dynamics at an ultraviolet (UV) fixed point, a notion inspired by gauge-Higgs unification models that incorporate warped extra dimensions. Our investigation is situated within the context of composite Higgs theories, which are predicated on highly-coupled gauge theories. We examine two distinct implementations of this framework: the first posits that the Higgs boson emerges as a quasi-Nambu-Goldstone boson resulting from the spontaneous breaking of approximate global symmetries; the second scenario suggests that the Higgs manifests as a bound state of new fermions that are charged under the Standard Model (SM) gauge group. The latter scenario is particularly intriguing as it predicts novel signatures for Higgs production via gluon fusion, which could be detectable at future collider experiments. \n\nMoreover, we discuss the implications of our findings for cosmology, highlighting how gauge-Higgs unification provides a compelling rationale for the weak scale being significantly smaller than other mass scales observed in nature. This framework also offers a natural resolution to the hierarchy problem between the Planck and TeV scales, as quantum corrections to the Higgs potential are effectively regulated at the UV scale. However, practical realization of this concept presents several challenges: the Higgs must emerge from a strongly coupled sector, its couplings to SM particles must align with experimental observations, and there must be a mechanism to generate masses for all SM fields without disrupting the existing mass hierarchies. Recent advancements have addressed these challenges through the Randall-Sundrum model, wherein the Higgs field resides on the infrared (IR) brane while gravity propagates through the bulk. This setup facilitates a calculable framework for Higgs physics, although it introduces complexities due to the presence of Kaluza-Klein gravitons.",
        "ori-fast-z-score": -0.4402254531628119,
        "water-fast-z-score": 6.490229342872016,
        "rewrite-fast-z-score": -0.3563483225498992
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Wide Field Spectrograph (WiFeS) .\nAbstract:\nThe WiFeS instrument is an integral field spectrograph for the Australian National University 2.3m telescope at Siding Spring Observatory, Australia.  It has been in operation since 2005 and was designed to provide high quality optical spectroscopy over a wide range of wavelengths with minimal overheads.   The WiFeS instrument consists of two cameras that are mounted on a common baseplate which sits inside a vacuum chamber attached to the Cassegrain focus of the ANU 2.3m telescope.    Each camera contains a lenslet array that produces a set of images across its focal plane.  These images can be combined into a single data cube using software developed by Dopita et al. (2007).    This data cube provides information about both spatial position and wavelength along each line-of-sight through the object being observed.  In addition to this capability, WiFeS also offers several other advantages including:  - High throughput due to the use of dichroic beam splitters and fibre optic coupling between the lenses and detectors.  - Flexible observing modes ranging from fully automated observations to manual control via remote desktop interface.  - Fast readout times allowing multiple targets to be observed simultaneously or rapid cycling between different targets during a night s observation run.  - Low maintenance requirements as it does not require any moving parts within the vacuum enclosure.  - Excellent stability thanks to the design of the optics bench and the thermal isolation provided by the vacuum enclosure.  - Large format detector arrays providing excellent signal-to-noise ratio even under poor seeing conditions.  - Robust mechanical design ensuring long-term reliability...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Wide Field Spectrograph ( WiFeS ) . Abstract : The WiFeS instrument is an integral field spectrograph for the Australian National University 2 . 3m telescope at Siding Spring Observatory , Australia .It has been in operation since 2005 and was built to provide high quality optical spectroscopy over a broad variety of wavelengths with minimal overheads . The WiFeS instrument consists of two cameras that are installed on a common baseplate which sits inside a vacuum chamber connected to the Cassegrain scope of the ANU 2 . 3m telescope .Each lens includes a lenslet antenna that generates a group of pictures across its focal surface . These photographs can be merged into a single data cube using software created by Dopita et al .( 2007 ) . This data cube provides knowledge about both visual placement and wavelength along each line - of - view through the element being detected .In addition to this capability , WiFeS also provides various other advantages including : - High throughput owing to the using of dichroic laser splitters and fibre optic coupling between the lenses and detectors . - Flexible monitoring mechanisms ranging from fully automated observations to manual control via remote desktop interface .- Fast readout hours permitting multiple targets to be viewed simultaneously or rapid cycling between various targets during a night s observation run . - Low maintenance requirements as it does not require any moving parts within the vacuum enclosure .- Excellent safety due to the design of the optics bench and the thermal confinement provided by the vacuum enclosure . - Large format detector arrays offering good signal - to - noise ratio especially under bad seeing conditions .- Robust mechanical architecture ensuring long - term reliability . . .",
        "rewrite_text": "**Title: The Wide Field Spectrograph (WiFeS)**\n\n**Abstract:** The Wide Field Spectrograph (WiFeS) is an advanced integral field spectrograph designed for the 2.3-meter telescope at the Australian National University’s Siding Spring Observatory. Since its inception in 2005, WiFeS has been instrumental in delivering high-quality optical spectroscopy across a wide range of wavelengths with minimal operational overhead. The instrument features a dual-camera system mounted on a shared baseplate within a vacuum chamber, which is directly connected to the Cassegrain focus of the telescope. Each camera is equipped with a lenslet array that captures multiple images across its focal plane. These images can be processed and combined into a comprehensive data cube using specialized software developed by Dopita et al. (2007). This data cube not only provides spatial information but also encodes spectral data along each line of sight through the observed object.\n\nWiFeS offers several significant advantages that enhance its observational capabilities. Its high throughput is achieved through the use of dichroic laser splitters and fiber optic coupling, which optimize the transfer of light from the lenses to the detectors. The instrument supports a range of operational modes, from fully automated observations to manual control via a remote desktop interface, allowing for flexible monitoring of celestial targets. Additionally, its rapid readout capabilities enable simultaneous observation of multiple targets or quick cycling between different objects during a single night of observation. The design of WiFeS minimizes maintenance needs, as it operates without moving parts within the vacuum enclosure, contributing to its reliability. Safety is also a priority, with a robust optical bench design and effective thermal management provided by the vacuum environment. Furthermore, the large-format detector arrays ensure a high signal-to-noise ratio, even under suboptimal seeing conditions, while the overall mechanical architecture of the instrument guarantees long-term durability and performance.",
        "ori-fast-z-score": -0.8638684255813601,
        "water-fast-z-score": 6.11070705734948,
        "rewrite-fast-z-score": 0.8111071056538127
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Line Emission in the Brightest Cluster Galaxies of the NOAO Fundamental Plane and Sloan Digital Sky Surveys .\nAbstract:\nWe present new measurements of line emission for the brightest cluster galaxies (BCGs) in clusters with z < 0.3, using data obtained by the Chandra X-ray Observatory. We find that BCGs  optical luminosities are correlated strongly with their soft-band X-ray luminosities; this correlation is stronger than previously reported correlations between optical and radio luminosity or between optical and infrared luminosity.  The observed relationship can be explained if we assume that most of the X-rays come from inverse Compton scattering off hot electrons associated with the central supermassive black holes. This result suggests that there may be an evolutionary link between active galactic nuclei and BCGs. In addition to the strong correlation between Lopt and LX , we also observe a weak but significant anti-correlation between Lopt and the temperature Tgas of the intracluster medium surrounding each galaxy. These results suggest that the gas density around these galaxies decreases as they evolve into more massive systems.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Line Emission in the Brightest Cluster Galaxies of the NOAO Fundamental Plane and Sloan Digital Sky Surveys . Abstract : We report new studies of line emission for the brightest cluster clusters ( BCGs ) in clusters with z < 0 . 3 , using data derived by the Chandra X - ray Observatory .We see that BCGs laser luminosities are correlated heavily with their soft - band X - ray luminosities ; this relationship is strengthened than previously reported correlations between optical and radio luminosity or between optical and infrared luminosity . The observed relationship can be described if we suppose that most of the X - rays come from inverse Compton absorption off warm particles associated with the main supermassive black holes .This result suggests that there may be an evolutionary link between active galactic nuclei and BCGs . In addition to the strong correlation between Lopt and LX , we also observe a weak but significant anti - correlation between Lopt and the temperature Tgas of the intracluster medium surrounding each constellation .These data suggest that the gas density around these galaxies reduces as they develop into more massive structures .",
        "rewrite_text": "We present a comprehensive analysis of line emission in the brightest cluster galaxies (BCGs) within clusters at redshifts less than 0.3, utilizing data from the Chandra X-ray Observatory. Our findings reveal a robust correlation between the laser luminosities of BCGs and their soft-band X-ray luminosities, which is notably stronger than previously documented correlations involving optical and radio luminosities, as well as optical and infrared luminosities. This relationship can be interpreted through the hypothesis that the majority of the X-ray emissions arise from inverse Compton scattering off warm particles linked to the central supermassive black holes. This insight implies a potential evolutionary connection between active galactic nuclei (AGNs) and BCGs, suggesting that the processes governing their development may be intertwined. Furthermore, we identify a weak yet statistically significant anti-correlation between the optical luminosity (Lopt) and the temperature (Tgas) of the intracluster medium surrounding these galaxies. This observation indicates that as BCGs evolve into more massive structures, the gas density in their vicinity diminishes. Our study enhances the understanding of the intricate relationships between BCGs, AGNs, and the surrounding intracluster medium, providing valuable insights into the evolutionary pathways of these cosmic entities. The implications of these findings extend to the broader context of galaxy formation and evolution, highlighting the complex interplay between luminous galaxies and their environments in the universe.",
        "ori-fast-z-score": 1.1322770341445956,
        "water-fast-z-score": 5.962965874907927,
        "rewrite-fast-z-score": 1.584236068762679
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Effects of Changes in Reaction Rates on Simulations of Nova Explosions .\nAbstract:\nWe have performed simulations of nova explosions using the hydrodynamic code VH-1, which includes nuclear burning and convection. We find that changes to reaction rates can significantly affect the results of these calculations. In particular, we show how different choices for the 12C(p,γ)13N rate lead to differences in the predicted light curve shapes.  The inclusion of this reaction is important because it affects the amount of 13N produced during the explosion. This isotope decays by electron capture into 14O, which then undergoes β+ decay back down to 14N. If there are too many electrons present at late times (due to an overabundance of 13N), they will be captured onto protons instead of being emitted as positrons; thus, less energy will be released than if no such process were occurring. Our results suggest that the current uncertainty in the 12C(p , γ )13N rate may cause errors in the predicted luminosity of up to 50%.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Effects of Changes in Reaction Rates on Simulations of Nova Explosions . Abstract : We have done simulations of nova explosions using the hydrodynamic code VH - 1 , which includes nuclear combustion and convection .We see that changes to reaction rates can significantly affect the results of these calculations . In particular , we find how changed options for the 12C ( p , γ ) 13N rate lead to differences in the expected light diagram forms .The inclusion of this process is important because it affects the quantity of 13N produced during the explosion . This isotope decays by electron capture into 14O , which then undergoes β + decay back down to 14N .If there are too several electrons present at late times ( owing to an overabundance of 13N ) , they will be captured onto protons rather of being emitted as positrons ; thus , fewer electricity will be released than if no such mechanism were happening . Our results show that the present uncertainty in the 12C ( p , γ ) 13N rate may create errors in the expected luminosity of up to 50 % .",
        "rewrite_text": "Title: The Effects of Changes in Reaction Rates on Simulations of Nova Explosions\n\nAbstract: In this study, we present a series of simulations focused on nova explosions, utilizing the hydrodynamic code VH-1, which incorporates both nuclear combustion and convection processes. Our findings reveal that variations in reaction rates can have a profound impact on the outcomes of these simulations. Notably, we investigate how different configurations of the 12C(p, γ)13N reaction rate influence the resulting light curves, which are critical for understanding the observable characteristics of nova events. The significance of this reaction lies in its role in the production of the isotope 13N during the explosion. Following its formation, 13N undergoes electron capture to transform into 14O, which subsequently decays via β+ decay to yield 14N. \n\nA crucial aspect of our analysis is the potential for an excess of electrons at later stages of the explosion, which can occur if there is an overproduction of 13N. In such scenarios, these electrons are more likely to be captured by protons rather than being emitted as positrons, resulting in a reduced energy output. This mechanism has important implications for the overall luminosity of the nova. Our simulations indicate that the current uncertainties surrounding the 12C(p, γ)13N reaction rate could lead to discrepancies in the predicted luminosity of nova explosions, with potential errors reaching up to 50%. These findings underscore the necessity for precise measurements of nuclear reaction rates to enhance the accuracy of models predicting the behavior and characteristics of nova explosions.",
        "ori-fast-z-score": -1.0533703247651751,
        "water-fast-z-score": 4.184914994777494,
        "rewrite-fast-z-score": 0.30151134457776363
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Current status of the CATS database .\nAbstract:\nThe Catalogue Archive and Transmission System (CATS) is an archive for astronomical data, which was established in 1988 by the National Astronomical Observatory of Japan (NAOJ). The main purpose of this system is to provide efficient access to astronomical data obtained at Japanese observatories. In addition, it provides services such as data distribution via FTP or HTTP protocols, data processing using remote computers, and data analysis tools on its web site. Since its establishment, CATS has been used widely not only within NAOJ but also outside NAOJ. Currently there are more than 1 million objects registered with CATS. This article describes how CATS works and what kind of information can be found in CATS. It also discusses some future plans for CATS. The Catalogue Archive and Transmissions System (CATS; Sugimoto et al., 2002 ) is an archive for astronomic data that was established in 1988 by National Astronomical Observatory of Japan(NAOJ; Okumura & Nakamura, 1997) . Its main purpose is to provide efficient accesses to astronomical data obtained mainly at Japanese observatories . In addition , it provides various kinds of services including data distribution through FTP/HTTP protocol , data processing using remote computers , and data analysis tools on their website .\nSince its establishment , CATS have been used widely both inside and outside NAOJ . Currently there are over one million objects registered with CATs . This article describes how CATs works and what kind of informations we can find in CATS . We will discuss about our future plan for CATS .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Current status of the CATS database . Abstract : The Catalogue Archive and Transmission System ( CATS ) is an archive for astronomical data , which was established in 1988 by the National Astronomical Observatory of Japan ( NAOJ ) .The main function of this network is to provide effective access to scientific content obtained at Japanese observatories . In addition , it supplies services such as data distribution via FTP or HTTP protocols , data processing using remote machines , and information assessment skills on its web site .Since its establishment , CATS has been used widely not only within NAOJ but also outside NAOJ . Currently there are more than 1 million artifacts registration with CATS .This page describes how CATS works and what sort of detail can be found in CATS . It especially discusses some future planning for CATS .The Catalogue Archive and Transmissions System ( CATS ; Sugimoto et al . , 2002 ) is an archive for astronomic data that was established in 1988 by National Astronomical Observatory of Japan ( NAOJ ; Okumura & Nakamura , 1997 ) . Its primary purpose is to provide effective accesses to scientific content obtained mainly at Japanese observatories .In addition , it operates several kinds of solutions namely data distribution through FTP / HTTP protocol , database storage using remote machines , and information assessment tools on their website . Since its establishment , CATS have been used widely both inside and outside NAOJ .Currently there are over one million items registered with CATs . This page describes how CATs works and what sort of informations we can find in CATS .We will speak about our future planning for CATS .",
        "rewrite_text": "**Title:** Current Status of the CATS Database\n\n**Abstract:** The Catalogue Archive and Transmission System (CATS) serves as a comprehensive archive for astronomical data, having been established in 1988 by the National Astronomical Observatory of Japan (NAOJ). The primary objective of CATS is to facilitate efficient access to scientific data collected primarily from Japanese observatories. To achieve this, CATS offers a variety of services, including data distribution through FTP and HTTP protocols, remote data processing capabilities, and tools for information assessment available on its website. Since its inception, CATS has gained widespread utilization, not only within NAOJ but also among the broader astronomical community. As of now, the database boasts over one million registered artifacts, reflecting its significance as a resource for researchers and astronomers alike. This article provides an overview of the operational mechanisms of CATS, detailing the types of information and data that can be accessed through the system. Furthermore, it outlines future plans for the continued development and enhancement of CATS, ensuring that it remains a vital tool for the astronomical research community. The ongoing evolution of CATS is aimed at improving user experience, expanding data accessibility, and integrating advanced technologies to support the growing demands of astronomical research. This abstract highlights the importance of CATS in the field of astronomy and emphasizes its role in fostering collaboration and data sharing among researchers worldwide.",
        "ori-fast-z-score": 0.6897304947150052,
        "water-fast-z-score": 7.124070532524293,
        "rewrite-fast-z-score": 0.09950371902099892
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  On the Young-Fibonacci insertion algorithm .\nAbstract:\nWe present an analysis of the worst-case time complexity for inserting n elements into a binary search tree using the Fibonacci-heap data structure, which is based on the so-called  Young-Fibonacci insertions . We show that this algorithm has O(n log n) worst-case running time and thus improves upon previous results by Chan et al., who showed that it takes at least Ω(n2/3) time to perform these operations in the worst case. The main idea behind our proof is to use amortized analysis techniques to bound the number of times we have to update the heap s size during each step of the algorithm. This allows us to prove that the total cost incurred over all steps is bounded by O(n log n). In addition, we also provide experimental evidence showing that our theoretical bounds are tight up to constant factors. \nIntroduction\n\nThe Fibonacci-heap data type was introduced by Fredman and Tarjan  FT86  as part of their work on efficient algorithms for finding minimum spanning trees (MSTs), shortest paths, and other related problems. It consists of a rooted binary tree with two additional fields associated with every node: its parent pointer and its rank value. Every node stores pointers to its left child and right child nodes; however, unlike ordinary binary trees, there can be more than one node with the same key stored under any given node. Each node maintains a counter called its rank, which indicates how many keys equal to or less than the key of the current node exist below it; if no such key exists then the rank field contains zero. \n \n For example, consider the following Fibonacci-heap storing the values 1 through 5:",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : On the Young - Fibonacci insertion procedure . Abstract : We present an analysis of the worst - case period complexity for inserting n elements into a binary find tree using the Fibonacci - heap data structure , which is based on the so - called Young - Fibonacci insertions .We see that this algorithm has O ( n log n ) worst - case ran time and therefore improves upon recent results by Chan et al . , who demonstrated that it takes at least Ω ( n2 / 3 ) time to conduct these operations in the best case . The main idea behind our proof is to use amortized analysis methods to bound the number of twice we have to modify the heap s width during each step of the algorithm .This enables us to prove that the total cost incurred over all actions is bounded by O ( n log n ) . In addition , we also obtain experimental evidence showing that our theory bounds are strict up to constant factors .Introduction The Fibonacci - heap data model was introduced by Fredman and Tarjan FT86 as part of their work on effective methods for finding minimum spanning trees ( MSTs ) , shortest paths , and other related problems . It consists of a rooted binary tree with two additional fields associated with every node : its parent pointer and its rank value .Every node stores pointers to its right son and left baby nodes ; however , unlike ordinary binary trees , there can be more than one node with the same key contained under any given node . Each node maintains a counter termed its rank , which expresses how many keys equivalent to or greater than the key of the present node reside below it ; if no such key exists then the rank field contains zero .For instance , consider the following Fibonacci - heap storing the variables 1 through 5 :",
        "rewrite_text": "**Title: Analyzing the Young-Fibonacci Insertion Procedure**\n\n**Abstract:** In this study, we investigate the worst-case period complexity associated with the insertion of n elements into a binary find tree utilizing the Fibonacci-heap data structure, specifically through the Young-Fibonacci insertion method. Our findings reveal that this algorithm achieves a worst-case runtime of O(n log n), representing a significant improvement over previous work by Chan et al., who established that the best-case time complexity for these operations is at least Ω(n²/³). The core of our proof relies on amortized analysis techniques, which allow us to effectively limit the number of modifications required for the heap's width at each step of the algorithm. This approach facilitates the demonstration that the cumulative cost of all operations remains within the bounds of O(n log n). Furthermore, we present experimental data that supports the strictness of our theoretical bounds, confirming their validity up to constant factors.\n\n**Introduction:** The Fibonacci-heap data structure was first introduced by Fredman and Tarjan in their seminal work on efficient algorithms for solving problems such as minimum spanning trees (MSTs) and shortest paths. This data structure is characterized by a rooted binary tree that includes two additional fields for each node: a parent pointer and a rank value. Each node maintains pointers to its left and right child nodes, allowing for a flexible structure where multiple nodes can share the same key under a single parent. The rank of a node indicates the count of keys that are equal to or greater than the key of that node, with a rank of zero indicating the absence of such keys. For example, a Fibonacci-heap containing the values 1 through 5 can be structured to reflect these properties effectively. Through our analysis, we aim to enhance the understanding of the Young-Fibonacci insertion procedure and its implications for the efficiency of Fibonacci heaps in computational applications.",
        "ori-fast-z-score": -0.8951673046482753,
        "water-fast-z-score": 4.8666426339228765,
        "rewrite-fast-z-score": 0.5622535302317492
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Cosmic Acceleration Data and Bulk-Brane Energy Exchange .\nAbstract:\nWe propose that the cosmic acceleration data can be explained by bulk-brane energy exchange between branes in higher dimensions, which is similar to the dark matter effect on our universe. We show how this mechanism works for both open and closed universes with positive cosmological constant. The model predicts an accelerating expansion rate at late time as well as a decelerated one at early times. This prediction agrees very well with current observations. In addition, we find that the present value of the Hubble parameter H0 = 72 ± 8 km s-1 Mpc-1 is consistent with recent measurements. Finally, we discuss some possible tests of the theory using future experiments such as SNAP satellite project. Introduction:-The discovery of accelerated expansion of the universe has been made recently  1  . It was found that the universe expands faster than expected if it were dominated only by normal matter  2  , so there must exist another component called  dark energy   3  -  6  responsible for the observed phenomenon. However, the nature of dark energy remains unknown. Many models have been proposed to explain its origin  7  -  11  .\nIn this letter, we will consider a new approach based on the idea of extra dimensions  12  -  14  . According to these theories, our four-dimensional world may live on a three-dimensional surface (3-brane) embedded in a five dimensional space-time (bulk). If gravity propagates into the bulk then it could affect the motion of particles living on the 3-brane  15  -  17  . For example, if the gravitational field of a distant galaxy lives outside the 3-brane but inside the bulk, then its gravitational force would act upon us even though the galaxy itself does not appear directly in front of us  18  . Thus, the presence of extra dimensions leads to additional effects beyond those predicted by general relativity  19  -  21  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Cosmic Acceleration Data and Bulk - Brane Energy Exchange . Abstract : We suggest that the cosmic acceleration data can be described by bulk - brane energy exchange between branes in greater dimensions , which is analogous to the dark matter effect on our universe .We see how this mechanism works for both open and opened universes with positive cosmological constant . The model predicts an accelerating expansion speed at late moment as well as a decelerated one at early years .This prediction agrees very well with current observations . In addition , we find that the present value of the Hubble parameter H0 = 72 ± 8 km s - 1 Mpc - 1 is compatible with recent observations .Finally , we explain some possible experiments of the model utilizing upcoming experiments such as SNAP satellite project . Introduction : - The observation of rapid expansion of the universe has been achieved recently 1 .It was shown that the universe expands faster than expected if it were dominated only by normal matter 2 , so there may contain another component called bright energy 3 - 6 responsible for the observed phenomenon . However , the nature of bright energy remains undisclosed .Many theories have been proposed to explain its origin 7 - 11 . In this letter , we will explore a new approach based on the idea of added dimensions 12 - 14 .According to these theories , our four - dimensional world could live on a three - dimensional surface ( 3 - brane ) integrated in a five dimensional space - time ( bulk ) . If gravity propagates into the bulk then it could affect the movement of molecules residing on the 3 - brane 15 - 17 .For instance , if the gravitational field of a distant galaxy lives outside the 3 - brane but inside the bulk , then its gravitational power would work upon us even though the universe itself does not appear directly in front of us 18 . Thus , the presence of added size leads to extra consequences beyond those predicted by general relativity 19 - 21 .",
        "rewrite_text": "**Title:** Cosmic Acceleration Data and Bulk-Brane Energy Exchange\n\n**Abstract:** In this study, we propose a novel framework to interpret cosmic acceleration data through the lens of bulk-brane energy exchange occurring between higher-dimensional branes. This mechanism bears resemblance to the effects attributed to dark matter within our universe. Our analysis encompasses both open and closed cosmological models featuring a positive cosmological constant, demonstrating that this energy exchange can account for the observed acceleration of the universe's expansion. Specifically, our model predicts an accelerating expansion rate during the later epochs of cosmic history, while also allowing for a decelerated expansion phase in the early universe. These predictions align closely with current observational data, reinforcing the viability of our approach. Furthermore, we find that the present-day value of the Hubble parameter, H0 = 72 ± 8 km s^-1 Mpc^-1, is consistent with recent measurements, lending additional support to our model. We also discuss potential experimental validations of our theory, particularly through upcoming initiatives such as the SNAP satellite project, which could provide critical insights into the nature of cosmic acceleration and the underlying mechanisms at play. \n\n**Introduction:** Recent observations have confirmed the rapid expansion of the universe, revealing that its expansion rate exceeds expectations based solely on the presence of ordinary matter. This discrepancy suggests the existence of an additional component, often referred to as \"dark energy,\" which is responsible for the observed acceleration. Despite numerous theories proposed to elucidate the nature of dark energy, its fundamental characteristics remain elusive. In this letter, we introduce a fresh perspective grounded in the concept of extra dimensions. According to this framework, our four-dimensional universe may be situated on a three-dimensional surface (3-brane) embedded within a five-dimensional spacetime (bulk). If gravitational forces can propagate into the bulk, they may influence the dynamics of matter confined to the 3-brane. For example, the gravitational influence of a distant galaxy residing outside the 3-brane but within the bulk could still exert effects on our universe, even if it is not directly observable. This additional dimensionality introduces consequences that extend beyond the predictions of general relativity, prompting a reevaluation of our understanding of cosmic dynamics.",
        "ori-fast-z-score": 0.4931969619160719,
        "water-fast-z-score": 6.695271963607354,
        "rewrite-fast-z-score": 0.2229882438741499
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Scalar-field perturbations from a particle orbiting a black hole using numerical evolution in 2+1 dimensions .\nAbstract:\nWe study the gravitational field generated by a massive scalar point source moving on an equatorial circular geodesic around a Schwarzschild black hole, and we evolve it numerically in two spatial dimensions (2+1) with the puncture method. We find that the perturbation is dominated by a single mode which grows exponentially as time goes on. The growth rate agrees well with the prediction based on quasinormal modes for this system. This result suggests that the exponential growth may be related to the instability of the scalar field near the horizon. In addition, we also show that the amplitude of the growing mode decreases rapidly when the mass of the scalar field increases. Finally, we discuss possible applications of our results to astrophysical phenomena such as gamma-ray bursts. \n \n Introduction \n \n Black holes are among the most fascinating objects predicted by general relativity. They have been studied extensively both theoretically and observationally over many decades  1  . One important aspect of their physics concerns how particles move close to them  2  , especially those that can escape from the black hole s gravity  3  . It has recently become clear that there exist some interesting physical processes taking place very close to the event horizon  4  -  6  . For example, if one considers a charged particle falling into a Reissner-Nordström black hole, then its motion will be unstable due to the so-called  photon sphere effect   7, 8  . If the charge of the particle is sufficiently large, then the particle will eventually fall into the black hole after emitting photons  9  . Another interesting phenomenon occurs when a neutral particle falls into a Kerr black hole  10  . Here again, the motion becomes unstable because of the existence of the photon sphere  11  . However, unlike the case of a Reissner-Norström black hole, the emitted radiation now contains not only photons but also gravitons  12  . \n \n In recent years, much attention has been paid to studying the dynamics of fields outside black holes  13  -  17  . In particular, the problem of finding the spectrum of quasi-normal modes (QNMs), i.e., the characteristic frequencies at",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Scalar - field perturbations from a particle orbiting a black hole using numerical evolution in 2 + 1 dimensions . Abstract : We explore the gravitational field produced by a huge scalar point source rotating on an equatorial circular geodesic around a Schwarzschild red hole , and we derive it numerically in two spatial dimensions ( 2 + 1 ) with the puncture method .We see that the perturbation is dominated by a single mode which increases exponentially as time go on . The growth speed agrees well with the prediction based on quasinormal modes for this system .This result suggests that the exponential growth could be due to the instability of the scalar field near the horizon . In addition , we also find that the frequency of the increasing mode decreases quickly when the mass of the scalar field increases .Finally , we talk possible applied of our findings to astrophysical processes such as gamma - ray bursts . Introduction Black holes are among the most useful structures anticipated by general relativity .They have been studied frequently both theoretically and observationally over numerous years 1 . One important element of their physics matters how particles moving close to them 2 , particularly those that can escape from the dark hole s gravity 3 .It has recently become clear that there exist some interesting physical processes running place very close to the event horizon 4 - 6 . For instance , if one considers a charged particle falling into a Reissner - Nordström black hole , then its motion will be unstable due to the so - called photon circle phenomenon 7 , 8 .If the charge of the particle is sufficiently huge , then the particle will eventually go into the dark hole after emitting photons 9 . Another curious phenomenon occurs when a neutral particle falls into a Kerr black hole 10 .Here again , the movement becomes unstable because of the existence of the photon sphere 11 . However , unlike the case of a Reissner - Norström black hole , the emitted radiation now contains not only photons but also gravitons 12 .In recent years , much attention has been paid to researching the dynamics of fields outside grey holes 13 - 17 . In particular , the question of finding the spectrum of quasi - normal frequencies ( QNMs ) , i . e . , the typical frequencies at",
        "rewrite_text": "**Title:** Scalar Field Perturbations from a Particle Orbiting a Black Hole Using Numerical Evolution in 2 + 1 Dimensions\n\n**Abstract:** In this study, we investigate the gravitational influence exerted by a massive scalar point source that orbits a Schwarzschild black hole along an equatorial circular geodesic. Utilizing the puncture method, we conduct a numerical analysis in two spatial dimensions (2 + 1). Our findings reveal that the perturbation is predominantly characterized by a single mode, which exhibits exponential growth over time. This growth rate aligns closely with predictions derived from the quasinormal modes associated with the system, indicating that the observed exponential increase may stem from the instability of the scalar field in proximity to the black hole's event horizon. Furthermore, we observe that the frequency of the growing mode diminishes rapidly as the mass of the scalar field increases. These insights prompt a discussion on the potential implications of our results for astrophysical phenomena, such as gamma-ray bursts.\n\n**Introduction:** Black holes represent one of the most intriguing predictions of general relativity, having been the subject of extensive theoretical and observational research over the years. A critical aspect of black hole physics involves understanding the behavior of particles in their vicinity, particularly those that manage to escape the gravitational pull of the black hole. Recent studies have highlighted fascinating physical processes occurring near the event horizon. For example, the dynamics of a charged particle falling into a Reissner-Nordström black hole can become unstable due to the photon circle effect. If the particle's charge is sufficiently large, it may ultimately be drawn into the black hole after emitting photons. Similarly, the behavior of a neutral particle approaching a Kerr black hole also exhibits instability, influenced by the presence of a photon sphere. In this scenario, the emitted radiation includes not only photons but also gravitons. In recent years, there has been a growing focus on the dynamics of fields surrounding black holes, particularly in determining the spectrum of quasinormal frequencies (QNMs), which represent the characteristic frequencies at which these systems resonate.",
        "ori-fast-z-score": -0.4601789933084222,
        "water-fast-z-score": 6.902684899626333,
        "rewrite-fast-z-score": 0.5586608191273356
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Tracking User Attention in Collaborative Tagging Communities .\nAbstract:\nWe present an analysis of user attention to tags and resources in collaborative tagging communities, based on the data collected by Delicious.com over three years (2005-2007). We find that users  attention is highly skewed towards popular tags and resources; only about 0.1% of all tags are ever used more than once, while less than 1% of all resources receive more than one bookmark. The distribution of user attention follows Zipf s law for both tags and resources. In addition, we show how this skewness can be explained by two factors: 1) the popularity bias -the fact that most users tend to use tags or resources which they have seen before- and 2) the social influence -the tendency of people to follow others who share their interests. Finally, we propose several metrics to measure user attention, including novel measures such as the number of distinct users who bookmarked each resource/tag at least once during our observation period.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Tracking User Attention in Collaborative Tagging Communities . Abstract : We report an assessment of customer attention to tags and assets in joint tagging environments , based on the information collected by Delicious . com over three years ( 2005 - 2007 ) .We see that users focus is heavily skewed towards popular tags and assets ; only about 0 . 1 % of all labels are ever used more than once , while none than 1 % of all resources receive more than one bookmark . The distribution of customer focus follows Zipf s law for both tags and assets .In addition , we find how this skewness can be described by two factors : 1 ) the fame bias - the fact that most users prefer to use tags or assets which they have viewed before - and 2 ) the social impact - the tendency of people to follow people who share their interests . Finally , we propose several metrics to measure user awareness , including novel estimates such as the proportion of distinct consumers who bookmarked each resource / tag at least once during our survey period .",
        "rewrite_text": "In this study, we present an analysis of user attention towards tags and resources within collaborative tagging environments, utilizing data gathered from Delicious.com over a three-year period from 2005 to 2007. Our findings reveal a significant concentration of user focus on a limited number of popular tags and resources, indicating a pronounced skew in attention. Specifically, we observe that only approximately 0.1% of all tags are utilized more than once, and fewer than 1% of resources receive more than a single bookmark. This pattern of user engagement aligns with Zipf's law, which describes the frequency distribution of tags and resources. \n\nFurther investigation into the factors contributing to this skewness identifies two primary influences: first, the \"fame bias,\" which suggests that users are inclined to engage with tags and resources they have previously encountered; and second, the \"social impact,\" which reflects the tendency of individuals to follow the tagging behaviors of others who share similar interests. These insights highlight the social dynamics at play in collaborative tagging environments and their effect on user behavior.\n\nTo enhance our understanding of user engagement, we propose several metrics for measuring user attention. Among these is a novel metric that estimates the proportion of distinct users who have bookmarked each resource or tag at least once during the study period. This metric aims to provide a more nuanced view of user awareness and interaction within tagging communities. Overall, our research sheds light on the patterns of user attention in collaborative tagging systems and offers valuable insights for future studies in this domain.",
        "ori-fast-z-score": -1.585187847802434,
        "water-fast-z-score": 5.661385170722978,
        "rewrite-fast-z-score": -1.0606601717798212
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Room temperature coexistence of large electric polarization and magnetic order in BiFeO3 single crystals .\nAbstract:\nBiFeO3 is an antiferromagnetic insulator with high Curie temperature (TC = 1103 K) that exhibits ferroelectricity at room temperature when doped or under pressure. Here we report the observation of spontaneous magnetization, M(T), and electrical polarization, P(E), simultaneously existing over wide ranges of temperatures up to 300 K in pure BiFeO3 single crystal grown by flux method. The results show that both M(T) and P(E) are nearly independent on each other below TC but they start to decrease rapidly above TC. We also found that the magnitude of M(T) decreases faster than that of P(E). These observations suggest that there exists strong coupling between spin and lattice degrees of freedom in this material. \n \n Keywords: Room-temperature multiferroic materials, BiFeO3, Electric polarization, Magnetic properties, Flux growth technique.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Room temperature coexistence of large electric polarization and magnetic order in BiFeO3 single crystals . Abstract : BiFeO3 is an antiferromagnetic insulator with high Curie temperature ( TC = 1103 K ) that exhibits ferroelectricity at room temperature when doped or under pressure .Here we publish the observation of spontaneous magnetization , M ( T ) , and electrical polarization , P ( E ) , independently operating over broad ranges of conditions up to 300 K in pure BiFeO3 single crystal grown by flux method . The results show that both M ( T ) and P ( E ) are nearly independent on each other below TC but they start to decline rapidly above TC .We additionally found that the magnitude of M ( T ) decreases quicker than that of P ( E ) . These measurements suggest that there exists strong coupling between spinning and lattice degrees of liberty in this material .Keywords: Room-temperature multiferroic materials, BiFeO3, Electric polarization, Magnetic properties, Flux growth technique.",
        "rewrite_text": "Title: Room Temperature Coexistence of Significant Electric Polarization and Magnetic Order in BiFeO3 Single Crystals\n\nAbstract: BiFeO3 is recognized as an antiferromagnetic insulator with a notably high Curie temperature (TC = 1103 K) and demonstrates ferroelectric properties at room temperature when subjected to doping or pressure. In this study, we present our findings on the spontaneous magnetization (M(T)) and electrical polarization (P(E)) in pure BiFeO3 single crystals, which were synthesized using the flux growth method. Our observations reveal that both M(T) and P(E) operate independently across a wide range of conditions up to 300 K. Notably, below the Curie temperature, the two properties exhibit minimal interdependence; however, a significant decline in both parameters is observed as the temperature exceeds TC. Furthermore, our data indicate that the reduction in M(T) occurs at a faster rate compared to that of P(E). These results imply a robust coupling between the spin and lattice degrees of freedom within BiFeO3, highlighting the intricate interplay between its magnetic and electric characteristics. This study contributes to the understanding of room-temperature multiferroic materials and their potential applications in advanced electronic devices. \n\nKeywords: Room-temperature multiferroic materials, BiFeO3, Electric polarization, Magnetic properties, Flux growth technique.",
        "ori-fast-z-score": 0.254000254000381,
        "water-fast-z-score": 3.5,
        "rewrite-fast-z-score": 2.49100947511811
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Spitzer Observations of Transient, Extended Dust in Two Elliptical Galaxies: New Evidence of Recent Feedback Energy Release in Galactic Cores .\nAbstract:\nWe report the discovery of transient dust emission at mid-infrared wavelengths (5-20 microns) in two elliptical galaxies with active galactic nuclei (AGN). The observations were made using Spitzer Space Telescope s Infrared Array Camera and Multiband Imaging Photometer for Spitzer instruments over a period of several years. We find that the infrared luminosity is consistent with heating by AGN radiation or supernovae remnants within the central kpc region. This suggests that recent feedback energy release has been occurring in these cores. These results are important because they provide new evidence on how supermassive black holes grow through accretion onto their host galaxy centers. They also demonstrate the power of combining multiwavelength data to study the physical processes associated with nuclear activity. \n \n Keywords: Active galactic nucleus, Galaxy evolution, Mid-infrared, Nuclear starbursts \n \n 1. Introduction \n \n Supermassive black holes reside in the center of most massive galaxies. Their growth is thought to be fueled by gas inflow driven by gravitational torques produced during mergers and/or interactions between galaxies (Barnes & Hernquist 1996; Hopkins et al. 2006) . However, it remains unclear what happens after this fuel supply runs out. One possibility is that the black hole continues growing via radiatively inefficient accretion flows (Narayan & Yi 1994) , which may produce powerful winds and jets that can drive large-scale outflows into the surrounding interstellar medium (ISM) (Silk & Rees 1998; Di Matteo et al. 2005 ). Another possibility is that the black holes become dormant as the ISM becomes too hot to cool efficiently (Bower et al. 2006; Croton et al. 2006 ) until another merger event triggers renewed activity. Understanding the mechanisms responsible for shutting off black-hole growth will help us understand why some galaxies have large black holes while others do not. \n \n 2. Previous Work \n \n Several studies have shown that there exists an anti-correlation between the mass of the central supermassive black hole and the stellar velocity dispersion of its host galaxy bulge (Ferrar",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Spitzer Observations of Transient , Extended Dust in Two Elliptical Galaxies : New Evidence of Recent Feedback Energy Release in Galactic Cores . Abstract : We report the discovery of transient dust radiation at mid - infrared wavelengths ( 5 - 20 microns ) in two elliptical galaxies with active galactic nuclei ( AGN ) .The surveys were made using Spitzer Space Telescope s Infrared Array Camera and Multiband Imaging Photometer for Spitzer instruments over a period of several period . We see that the infrared luminosity is compatible with heating by AGN radiation or supernovae fragments within the central kpc zone .This implies that current feedback power release has been occurring in these cores . These data are important because they give new evidence on how supermassive black holes expand through accretion onto their host galaxy structures .They also demonstrate the power of combining multiwavelength evidence to study the physical processes associated with nuclear activity . Keywords : Active galactic nucleus , Galaxy evolution , Mid - infrared , Nuclear starbursts 1 .Introduction Supermassive black holes dwell in the center of most gigantic galaxies . Their growth is suggested to be motivated by gas inflow driven by gravitational torques created during mergers and / or relationships between objects ( Barnes & Hernquist 1996 ; Hopkins et al .2006 ) . However , it remains unsure what comes after this fuel supply runs out .One possibility is that the dark hole keeps developing via radiatively inefficient accretion currents ( Narayan & Yi 1994 ) , which would create potent winds and jets that can force large - scale outflows into the adjacent interstellar medium ( ISM ) ( Silk & Rees 1998 ; Di Matteo et al . 2005 ) .Another possibility is that the dark holes become dormant as the ISM becomes too warm to hot quickly ( Bower et al . 2006 ; Croton et al .2006 ) until another merger event triggers renewed behavior . Understanding the mechanisms involved for shut off dark - hole growth will assist us explain why some stars have huge black holes while many do not .2 . Previous Work Several studies have shown that there exists an counter - correlation between the mass of the main supermassive black hole and the stellar velocity dispersion of its host galaxy bulge ( Ferrar",
        "rewrite_text": "**Title:** Spitzer Observations of Transient, Extended Dust in Two Elliptical Galaxies: New Evidence of Recent Feedback Energy Release in Galactic Cores\n\n**Abstract:** In this study, we present the detection of transient dust emission at mid-infrared wavelengths (5-20 microns) in two elliptical galaxies that host active galactic nuclei (AGN). Utilizing the Spitzer Space Telescope's Infrared Array Camera and Multiband Imaging Photometer, we conducted observations over an extended timeframe. Our findings indicate that the infrared luminosity observed is consistent with heating mechanisms attributed to AGN radiation or remnants from supernovae within the central kiloparsec region. This suggests that there has been a recent release of feedback energy in these galactic cores. The implications of this research are significant, as they provide new insights into the processes by which supermassive black holes grow through accretion onto their host galaxies. Furthermore, our results underscore the effectiveness of integrating multiwavelength data to investigate the physical phenomena associated with nuclear activity in galaxies.\n\n**Keywords:** Active galactic nucleus, Galaxy evolution, Mid-infrared, Nuclear starbursts\n\n**1. Introduction:** Supermassive black holes are typically found at the centers of large galaxies, and their growth is believed to be driven by gas inflow resulting from gravitational interactions during mergers or close encounters between celestial bodies (Barnes & Hernquist 1996; Hopkins et al. 2006). However, the fate of these black holes once their fuel supply diminishes remains uncertain. One potential scenario is that they continue to grow through radiatively inefficient accretion processes (Narayan & Yi 1994), which could generate powerful winds and jets, leading to significant outflows into the surrounding interstellar medium (ISM) (Silk & Rees 1998; Di Matteo et al. 2005). Alternatively, black holes may enter a dormant phase if the ISM becomes excessively heated (Bower et al. 2006; Croton et al. 2006), remaining inactive until a subsequent merger reignites their activity. Understanding the mechanisms that lead to the cessation of black hole growth is crucial for explaining the disparity in black hole masses among different stellar populations.\n\n**2. Previous Work:** Numerous studies have established a counter-correlation between the mass of the primary supermassive black hole and the stellar velocity dispersion of its host galaxy bulge (Ferrar et al. 2006).",
        "ori-fast-z-score": -1.3241694217637887,
        "water-fast-z-score": 6.7266838681529375,
        "rewrite-fast-z-score": 0.827605888602368
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Collisional processes and size distribution in spatially extended debris discs .\nAbstract:\nWe present the results of collisional modelling for two spatially resolved debris discs, HD 69830 and AU Mic. We find that collisions are efficient at producing dust particles with sizes ranging between 1 mm to 10 cm across most of these systems. The observed radial profiles can be reproduced by assuming an initial power-law grain-size distribution with index -3.5 (consistent with theoretical predictions) and allowing it to evolve under mutual collisions over timescales of several million years. In addition we show how our models can reproduce the observed colour gradients seen in both systems. Finally, we discuss possible implications of this work on the formation mechanisms of planetesimals and planets. Keywords: Debris discs - Collisions - Grain growth - Planets - Spatially resolved observations - Sizesize distributions - Dust grains - Asteroids - Cometary nuclei - Circumstellar disks - Planet formation",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Collisional processes and size distribution in spatially extended debris discs . Abstract : We report the conclusion of collisional modelling for two spatially resolved dust discs , HD 69830 and AU Mic .We see that collisions are successful at generating dust particles with sizes varied between 1 mm to 10 centimeters across most of these systems . The observed radial profiles can be reproduced by assuming an initial power - law grain - length distribution with index - 3 . 5 ( consistent with theoretical estimates ) and allowing it to evolve under mutual collisions over timescales of several million months .In addition we explain how our models can mimic the seen colour gradients shown in both systems . Finally , we explain possible possibilities of this research on the formation patterns of planetesimals and planets .Keywords : Debris discs - Collisions - Grain growth - Planets - Spatially resolved observations - Sizesize distributions - Dust grains - Asteroids - Cometary nuclei - Circumstellar disks - Planet formation",
        "rewrite_text": "Title: Collisional Processes and Size Distribution in Spatially Extended Debris Discs\n\nAbstract: In this study, we present the findings from our collisional modeling of two spatially resolved debris discs, specifically HD 69830 and AU Mic. Our analysis indicates that collisional interactions within these systems effectively produce dust particles ranging in size from 1 mm to 10 cm. By employing an initial power-law distribution of grain lengths with an index of -3.5, which aligns with theoretical predictions, we are able to replicate the observed radial profiles of dust distribution. This distribution evolves over timescales of several million months due to mutual collisions among particles. Furthermore, we discuss how our models can account for the color gradients observed in both debris discs, providing insights into the physical processes at play. The implications of our research extend to understanding the formation mechanisms of planetesimals and planets, highlighting the significance of collisional dynamics in the evolution of debris discs. Our findings contribute to the broader knowledge of dust grain growth and size distributions in circumstellar environments, with potential applications in the study of asteroids, cometary nuclei, and the overall processes of planet formation. \n\nKeywords: Debris discs, Collisions, Grain growth, Planets, Spatially resolved observations, Size distributions, Dust grains, Asteroids, Cometary nuclei, Circumstellar disks, Planet formation.",
        "ori-fast-z-score": 0.7071067811865476,
        "water-fast-z-score": 4.949747468305833,
        "rewrite-fast-z-score": 2.2691267417693455
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Cosmic ray confinement in fossil cluster bubbles .\nAbstract:\nWe present the first detection of cosmic rays confined within fossil radio plasma bubbles, using deep Chandra X-ray observations and VLA radio data for Abell 2052 (z = 0.0395). The bubble is located at the center of an elliptical galaxy with a prominent dust lane that extends to its nucleus. We find that the bubble has a radius of ~40 kpc and contains hot gas with T ~ 1 keV. This temperature is significantly lower than expected if the bubble were filled by shock-heated ICM. Instead we argue that the bubble is filled with relativistic particles accelerated by shocks driven into the surrounding medium during the formation of the central galaxy. These particles are responsible for the observed synchrotron emission detected on large scales around the bubble. In addition, we detect diffuse soft X-rays extending beyond the bubble s edge which may be due to thermal conduction or mixing between the bubble and the surrounding ICM.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Cosmic ray trapping in fossil cluster bubbles . Abstract : We report the first measurement of cosmic rays restricted within fossil radio plasma bubbles , using deep Chandra X - ray observations and VLA radio data for Abell 2052 ( z = 0 . 0395 ) .The bubble is situated at the center of an elliptical galaxy with a large dust track that extends to its core . We see that the bubble has a diameter of ~ 40 kpc and stores hot gas with T ~ 1 keV .This temperature is significantly less than expected if the bubble were filled by shock - heated ICM . Instead we claim that the bubble is filled with relativistic objects enhanced by shocks driven into the nearby medium during the formation of the central universe .These particles are responsible for the seen synchrotron emission detected on huge scales around the bubble . In addition , we perceive diffuse soft X - radiation stretching beyond the bubble s boundary which may be due to heat conduction or mixing between the bubble and the nearby ICM .",
        "rewrite_text": "We present the inaugural measurement of cosmic rays confined within fossil radio plasma bubbles, utilizing extensive observations from the Chandra X-ray Observatory and VLA radio data focused on the galaxy cluster Abell 2052 (z = 0.0395). The bubble, located at the center of an elliptical galaxy, is accompanied by a prominent dust track that extends toward its core. Our analysis reveals that the bubble spans approximately 40 kpc in diameter and contains hot gas with a temperature of around 1 keV. This temperature is notably lower than what would be anticipated if the bubble were filled with shock-heated intracluster medium (ICM). We propose that the bubble is instead populated with relativistic particles that have been amplified by shocks generated during the formation of the central universe. These particles are responsible for the extensive synchrotron emission observed on large scales surrounding the bubble. Furthermore, we detect diffuse soft X-ray radiation extending beyond the bubble's boundary, which may result from heat conduction or the mixing of the bubble's contents with the adjacent ICM. Our findings provide new insights into the dynamics of cosmic rays in fossil clusters and the interactions between radio plasma bubbles and their surrounding environments, contributing to a deeper understanding of the processes governing cosmic ray behavior in the universe.",
        "ori-fast-z-score": 0.2182178902359924,
        "water-fast-z-score": 6.110100926607787,
        "rewrite-fast-z-score": 0.9712858623572641
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  What are the Best Hierarchical Descriptors for Complex Networks? .\nAbstract:\nWe propose an algorithm to find optimal hierarchical descriptors (OHDs) that can be used as fingerprints in complex networks, such as social and biological systems. The OHDs represent network structures at different levels of granularity by using a tree-like structure with nodes representing subnetworks or clusters. We show how these trees can be constructed efficiently based on modularity maximization algorithms. In addition, we introduce a novel concept called  clustering coefficient  which is useful when constructing OHDs. Finally, we demonstrate our method through several examples including synthetic benchmark data sets and real-world networks. Our results suggest that the proposed approach provides more accurate descriptions than existing methods. This work was supported by JSPS KAKENHI Grant Number JP26287040. Keywords: Hierarchical descriptor, Clustering coefficient, Modularity Maximization Algorithm, Tree Structure, Fingerprint, Network Description",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  What are the Best Hierarchical Descriptors for Complex Networks?.Abstract : We suggest an algorithm to find optimal hierarchical descriptors ( OHDs ) that can be used as fingerprints in complex networks , such as social and biological environments . The OHDs represent system structures at different levels of granularity by using a tree - like structure with vertices representing subnetworks or communities .We see how these trees can be built easily using on modularity maximization algorithms . In addition , we provide a new notion called clustering coefficient which is important when constructing OHDs .Finally , we prove our technique through several examples namely natural benchmark data sets and actual - global networks . Our results show that the suggested approach offers more accurate descriptions than existing techniques .This project was supported by JSPS KAKENHI Grant Number JP26287040 . Keywords : Hierarchical descriptor , Clustering coefficient , Modularity Maximization Algorithm , Tree Structure , Fingerprint , Network Description",
        "rewrite_text": "Title: What are the Best Hierarchical Descriptors for Complex Networks?\n\nAbstract: In this study, we propose a novel algorithm designed to identify optimal hierarchical descriptors (OHDs) that serve as effective fingerprints for complex networks, including those found in social and biological systems. The OHDs encapsulate the structural characteristics of these systems at varying levels of granularity, utilizing a tree-like framework where vertices correspond to subnetworks or communities. We demonstrate that these hierarchical trees can be constructed efficiently through modularity maximization algorithms, which enhance the identification of community structures within the networks. Additionally, we introduce a new concept known as the clustering coefficient, which plays a crucial role in the development of OHDs. Through a series of examples, including natural benchmark datasets and real-world global networks, we validate our methodology. The results indicate that our proposed approach yields more precise network descriptions compared to existing methods. This research was supported by the JSPS KAKENHI Grant Number JP26287040. \n\nKeywords: Hierarchical descriptor, Clustering coefficient, Modularity Maximization Algorithm, Tree Structure, Fingerprint, Network Description.",
        "ori-fast-z-score": -0.3721042037676254,
        "water-fast-z-score": 5.986302773458956,
        "rewrite-fast-z-score": -0.1203858530857692
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Galactic disc warps due to intergalactic accretion flows onto the disc .\nAbstract:\nWe study how galactic discs warp in response to tidal forces exerted by dark matter haloes and intergalactic gas filaments, using high-resolution cosmological simulations with radiative cooling and star formation. We find that the majority (>80%) of simulated galaxies have significant warping at z = 0. The amplitude of the warp increases with decreasing galaxy mass, but is independent of redshift for massive galaxies. Warp amplitudes are typically less than 10 kpc, which agrees well with observations. Our results suggest that most observed warps can be explained as being caused by external gravitational torques on galactic discs. This implies that there may not exist any intrinsic mechanism within galactic discs themselves that causes warps. In addition, we show that the presence of an AGN does not significantly affect the shape or strength of the warp. Finally, we demonstrate that our model predicts a correlation between the direction of the warp and the angular momentum vector of the host halo.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Galactic disc warps caused to intergalactic accretion currents onto the disc . Abstract : We research how galactic discs warp in reaction to tidal forces exerted by black material haloes and intergalactic gas filaments , using high - resolution cosmological simulations with radiative cooling and galaxy formation .We see that the majority ( > 80 % ) of simulated galaxies have considerable warping at z = 0 . The amplitude of the warp increases with varying universe mass , but is independent of redshift for huge clusters .Warp amplitudes are typically lower than 10 kpc , which accepts better with observations . Our results show that most observed warps can be understood as being affected by external gravitational torques on galactic discs .This implies that there may not exist any inherent mechanism within galactic discs themselves that creates warps . In addition , we find that the presence of an AGN does not greatly affect the form or strength of the warp .Finally , we prove that our model predicts a correlation between the direction of the warp and the angular velocity tensor of the host halo .",
        "rewrite_text": "**Title: Galactic Disc Warps Induced by Intergalactic Accretion Currents**\n\n**Abstract:** This study investigates the mechanisms behind the warping of galactic discs as a response to tidal forces from surrounding black material haloes and intergalactic gas filaments. Utilizing high-resolution cosmological simulations that incorporate radiative cooling and galaxy formation processes, we analyze the structural changes in galactic discs. Our findings reveal that over 80% of the simulated galaxies exhibit significant warping at redshift z = 0. Notably, the amplitude of the warp is found to increase with the mass of the universe, while remaining largely consistent across different redshifts for massive galaxy clusters. The typical warp amplitudes observed are generally below 10 kpc, aligning well with empirical observations. \n\nThese results suggest that the majority of observed warps can be attributed to external gravitational torques acting upon the galactic discs, indicating that there may not be an intrinsic mechanism within the discs themselves responsible for their warping. Furthermore, our analysis indicates that the presence of an Active Galactic Nucleus (AGN) does not significantly influence the shape or intensity of the warp. Additionally, we establish a correlation between the orientation of the warp and the angular velocity tensor of the host halo, providing further insight into the dynamics at play. This research enhances our understanding of galactic structure formation and the external influences that shape the morphology of galaxies in the universe.",
        "ori-fast-z-score": -1.2874526191574363,
        "water-fast-z-score": 4.50333209967908,
        "rewrite-fast-z-score": 0.4216370213557839
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Brownian excursion area, Wright s constants in graph enumeration, and other Brownian areas .\nAbstract:\nWe study the distribution of the total area swept out by a one-dimensional Brownian motion between two fixed times. We show that this distribution is given by an explicit formula involving the modified Bessel function I0(x). This result can be used to derive several interesting identities for special functions such as the Riemann zeta-function or the Hurwitz zeta-functions at even arguments. In particular we obtain new proofs of some results due to Wright on the number of graphs with n vertices having certain properties (such as being bipartite) which are related to the coefficients appearing in the expansion of the exponential generating function of these numbers into powers of t. Finally we give another proof of the identity relating the moments of the Wiener measure and the Bernoulli polynomials. The main tool will be the Feynman-Kac representation of the solution of the heat equation. Let Wt denote standard Brownian motion starting at 0. For any real number s > 0 let us consider the random variable A(s), defined as the total area swept out during the time interval  0,s  by the process Wt:",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Brownian excursion area , Wright s constants in graph enumeration , and other Brownian areas . Abstract : We study the spread of the total region swept out by a one - dimensional Brownian movement between two fixed times .We see that this distribution is given by an explicit formula involving the modified Bessel distribution I0 ( x ) . This result can be used to derive several interesting identities for special functions such as the Riemann zeta - function or the Hurwitz zeta - functions at even arguments .In particular we obtain new proofs of some results attributed to Wright on the number of graphs with n nodes having specific properties ( such as being bipartite ) which are related to the coefficients appearing in the expansion of the exponential producing function of these numbers into powers of t . Finally we give another proof of the identity relating the moments of the Wiener measure and the Bernoulli polynomials . The main tool will be the Feynman - Kac representation of the solve of the heat equation .Let Wt denote standard Brownian movement starting at 0 . For any real number s > 0 let us consider the random variable A ( s ) , defined as the total area swept out during the period interval 0 , s by the process Wt :",
        "rewrite_text": "Title: Brownian Excursion Area, Wright's Constants in Graph Enumeration, and Other Brownian Areas\n\nAbstract: This article investigates the area covered by a one-dimensional Brownian motion over a fixed time interval. We establish that the distribution of this area can be expressed through an explicit formula that incorporates the modified Bessel function I0(x). This finding opens avenues for deriving several noteworthy identities related to special functions, including the Riemann zeta function and the Hurwitz zeta functions evaluated at even integers. Notably, we present new proofs for certain results originally attributed to Wright concerning the enumeration of graphs with n vertices that possess specific characteristics, such as bipartiteness. These characteristics are linked to the coefficients found in the power series expansion of the exponential generating function for these graph counts in terms of the variable t. Additionally, we provide an alternative proof for the identity that connects the moments of the Wiener measure with Bernoulli polynomials. The primary methodology employed in our analysis is the Feynman-Kac representation, which serves as a solution framework for the heat equation. We denote standard Brownian motion starting at zero as Wt, and for any positive real number s, we define the random variable A(s) to represent the total area covered by the process Wt during the time interval from 0 to s. This study not only enhances our understanding of the properties of Brownian excursions but also bridges connections between stochastic processes and combinatorial graph theory, thereby contributing to the broader field of mathematical analysis.",
        "ori-fast-z-score": 1.1659976680069961,
        "water-fast-z-score": 4.297967830559865,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Conjugate field and fluctuation-dissipation relation for the dynamic phase transition in the two-dimensional kinetic Ising model .\nAbstract:\nWe study the dynamics of the kinetic Ising model on square lattices with periodic boundary conditions by Monte Carlo simulations at finite temperatures T . We find that there is no static order parameter to characterize the dynamic phase transition, but we can define an effective conjugate field H conjugate to the magnetization M as follows: \nH = -ln(<M>)/T,\nwhere <M> denotes the average over all spins. The critical temperature Tc is determined by the condition dH/dT =0. In addition, we show that the fluctuation-dissipation theorem holds well near Tc. \nThe results are compared with those obtained by the mean-field approximation. \n\n\nI. INTRODUCTIO N\n\nIn recent years much attention has been paid to nonequilibrium phenomena such as relaxation processes after rapid changes of external parameters  1  , aging  2  , glassy behavior  3  , etc., because they play important roles not only in physics but also in biology  4  .\nAmong these topics, the kinetic Ising model  5  is one of the most popular models used to investigate non-equilibrium properties  6  . It describes the time evolution of spin variables S i (t) (i=1,...,N)\non a regular lattice under the influence of thermal fluctuations. Here t represents the number of Monte Carlo steps per site (MCS/s). At each step, every spin interacts with its nearest neighbors through exchange interactions J ij . Then it flips according to the Metropolis algorithm  7 :  if e -Sij / kBT > random number between 0 and 1 then flip S j else keep S j unchanged where k B is Boltzmann s constant and T is the absolute temperature. This process continues until equilibrium is reached or some other criterion is satisfied  8  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Conjugate field and fluctuation - dissipation relation for the dynamic phase shift in the two - dimensional kinetic Ising model . Abstract : We research the dynamics of the kinetic Ising model on square lattices with periodic boundary constraints by Monte Carlo simulations at finite temperatures T .We see that there is no static order parameter to characterize the dynamic phase shift , but we can define an efficient conjugate field H conjugate to the magnetization M as follows : H = - ln ( < M > ) / T , where < M > denotes the average over all spins . The essential temperature Tc is calculated by the condition dH / dT = 0 .In addition , we prove that the fluctuation - dissipation conjecture holds well near Tc . The results are compared with those achieved by the mean - field approximation .I . INTRODUCTIO N In recent years much attention has been paid to nonequilibrium phenomena such as relax processes after rapid adjustments of external parameters 1 , aging 2 , glassy behavior 3 , etc . , because they hold important roles not only in science but also in science 4 . Among these topics , the kinetic Ising model 5 is one of the most popular theories used to examine non - equilibrium properties 6 .It describes the time progression of spin vectors S i ( t ) ( i = 1 , . . . , N ) on a regular structure under the impact of thermal fluctuations . Here t equals the number of Monte Carlo steps per site ( MCS / s ) .At each step , every spin interacts with its closest neighbors through exchange interactions J ij . Then it flips due to the Metropolis algorithm 7 : if e - Sij / kBT > random value between 0 and 1 then flip S j else kept S j unchanged where k B is Boltzmann s constant and T is the absolute temperature .This process proceeds until equilibrium is reached or some other criterion is fulfilled 8 .",
        "rewrite_text": "**Title:** Conjugate Field and Fluctuation-Dissipation Relation for the Dynamic Phase Shift in the Two-Dimensional Kinetic Ising Model\n\n**Abstract:** In this study, we investigate the dynamic behavior of the kinetic Ising model on square lattices with periodic boundary conditions, utilizing Monte Carlo simulations at finite temperatures (T). Our findings reveal that a static order parameter is insufficient to characterize the dynamic phase shift; however, we introduce an effective conjugate field, denoted as H, which is related to the magnetization (M) through the equation H = -ln(<M>)/T, where <M> represents the average magnetization across all spins. We determine the critical temperature (Tc) by employing the condition dH/dT = 0. Furthermore, we demonstrate that the fluctuation-dissipation theorem is valid in the vicinity of Tc, providing insights into the relationship between fluctuations and dissipative processes in the system. Our results are juxtaposed with those derived from mean-field approximations, highlighting the differences and similarities in behavior. \n\nThe kinetic Ising model has garnered significant interest in recent years due to its relevance in understanding nonequilibrium phenomena, including relaxation processes following rapid changes in external parameters, aging, and glassy dynamics. These phenomena are not only pivotal in theoretical physics but also have practical implications across various scientific disciplines. The model effectively captures the temporal evolution of spin vectors S_i(t) (where i = 1, ..., N) on a lattice structure influenced by thermal fluctuations. In our simulations, time is quantified in terms of Monte Carlo steps per site (MCS/s), and at each step, spins interact with their nearest neighbors via exchange interactions (J_ij). The Metropolis algorithm governs the spin flipping process, where a spin S_j is flipped based on a probabilistic criterion involving the energy difference and the Boltzmann factor. This iterative process continues until the system reaches equilibrium or meets an alternative stopping criterion. Our research contributes to a deeper understanding of dynamic phase transitions in the kinetic Ising model and the underlying mechanisms that govern nonequilibrium behavior.",
        "ori-fast-z-score": 0.8049844718999243,
        "water-fast-z-score": 6.057921483348286,
        "rewrite-fast-z-score": 3.3333333333333335
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  La controverse de 1874 entre Camille Jordan et Leopold Kronecker .\nAbstract:\nLeopold Kronecker (1823-1901) est un mathématicien allemand qui s est intéressé à la théorie des nombres, aux fonctions elliptiques ainsi qu à l algèbre linéaire. Il fut l un des principaux fondateurs du groupe Bourbaki en 1934 avec André Weil, Jean Delsarte, René Decartes, Nicolas Bourbaki, Jean Dieudonné, Claude Chevalley, Élie Cartan, Jean-Louis Koszul, Samuel Eilenberg et Jacques-Louis Lions. En 1873, il publie dans le Journal für die reine und angewandte Mathematik une série d articles sur les algèbres associatives et non associatives. Dans son article intitulé « Ueber die Theorie der endlichen Gruppen von Variabeln »  Kronecker, L., Ueber die Theorie der Endlichen Gruppen von Varien, JFM 5.2 (1875), p. 1–42 , il introduit pour la première fois la notion d algèbre associative finie.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : La controverse de 1874 entre Camille Jordan et Leopold Kronecker . Abstract : Leopold Kronecker ( 1823 - 1901 ) est un mathématicien allemand qui s est intéressé à la théorie des nombres , des fonctions elliptiques ainsi qu à l algèbre linéaire .Il fut l un des principaux fondateurs du groupe Bourbaki en 1934 avec André Weil , Jean Delsarte , René Decartes , Nicolas Bourbaki , Jean Dieudonné , Claude Chevalley , Élie Cartan , Jean - Louis Koszul , Samuel Eilenberg et Jacques - Louis Lions . En 1873 , il publie dans le Journal für die reine und angewandte Mathematik une série d articles sur les algèbres associatives et non associatives .Dans son article intitulé « Ueber den Theorie der endlichen Gruppen von Variabeln » Kronecker , L . , Ueber den Theorie der Endlichen Gruppen von Varien , JFM 5 . 2 ( 1875 ) , p . 1 – 42 , il introduit du la première fois la idea d algèbre associative finie .",
        "rewrite_text": "Title: The 1874 Controversy Between Camille Jordan and Leopold Kronecker\n\nAbstract: Leopold Kronecker (1823-1901) was a prominent German mathematician known for his contributions to number theory, elliptic functions, and linear algebra. He played a significant role in the establishment of the Bourbaki group in 1934 alongside notable figures such as André Weil, Jean Delsarte, René Descartes, Nicolas Bourbaki, Jean Dieudonné, Claude Chevalley, Élie Cartan, Jean-Louis Koszul, Samuel Eilenberg, and Jacques-Louis Lions. In 1873, Kronecker published a series of articles in the Journal für die reine und angewandte Mathematik, focusing on associative and non-associative algebras. His work culminated in a pivotal paper titled \"Ueber den Theorie der endlichen Gruppen von Variabeln,\" published in 1875, where he introduced the concept of finite associative algebra for the first time. \n\nThe controversy that arose in 1874 between Kronecker and Camille Jordan centered on differing perspectives regarding the foundations of mathematics and the nature of mathematical objects. Jordan, a French mathematician, was an advocate for the geometric approach to mathematics, emphasizing the importance of intuition and visual representation. In contrast, Kronecker held a more formalist view, prioritizing rigorous definitions and logical deductions. This clash of ideologies not only highlighted the philosophical divide within the mathematical community of the time but also influenced subsequent developments in algebra and group theory.\n\nThe exchange between these two mathematicians reflects broader themes in the history of mathematics, including the tension between abstract theory and concrete applications. Their debate contributed to the evolution of mathematical thought, paving the way for future discussions on the nature of mathematical existence and the role of abstraction in mathematical practice. This article delves into the details of their correspondence, the implications of their differing viewpoints, and the lasting impact of their ideas on the field of mathematics.",
        "ori-fast-z-score": -1.2977713690461004,
        "water-fast-z-score": 0.8006407690254357,
        "rewrite-fast-z-score": -0.7427813527082074
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Radial distribution of the inner magnetosphere plasma pressure using low-altitude satellite data during geomagnetic storm: the March 1-8, 1982 Event .\nAbstract:\nThe radial distribution of the inner magnetospheric (IM) plasma pressure is studied by analyzing the magnetic field and particle measurements made onboard two satellites at low altitude in the equatorial plane for an intense geomagnetic storm that occurred between March 1-5, 1982 . The results show that there are significant differences in the IM plasma pressure profiles obtained with different satellites. In particular, the pressure profile derived from GEOS-1 observations shows a sharp peak near L = 3 , while those inferred from ATS-6 and GEOS-2 data exhibit much broader peaks around L = 4 . These discrepancies may be due to the fact that these satellites have different orbits which sample different regions of space. It is also found that the pressure profiles inferred from the three satellites agree well when they are shifted outward along the L-shell coordinate system. This suggests that the observed difference in the pressure profiles can be attributed mainly to the spatial variation of the pressure rather than temporal changes.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Radial distribution of the inner magnetosphere plasma pressure using minimum - height satellite information during geomagnetic storm : the March 1 - 8 , 1982 Event . Abstract : The radial distribution of the inner magnetospheric ( IM ) plasma pressure is studied by analyzing the magnetic field and electron calculations made onboard two spacecraft at low height in the equatorial plane for an extreme geomagnetic cyclone that occurred between March 1 - 5 , 1982 .The results show that there are significant variations in the IM plasma pressure profiles obtained with various satellites . In particular , the pressure profile derived from GEOS - 1 studies shows a sharp peak near L = 3 , while those inferred from ATS - 6 and GEOS - 2 data display considerably wider peaks around L = 4 .These discrepancies may be due to the fact that these satellites have different orbits which sample different regions of space . It is also discovered that the pressure profiles inferred from the three satellites cooperate well when they are shifted outward along the L - shell coordinate system .This implies that the seen shift in the pressure profiles can be due mainly to the spatial varying of the pressure rather than temporal changes .",
        "rewrite_text": "Title: Radial Distribution of Inner Magnetosphere Plasma Pressure During the Geomagnetic Storm of March 1-8, 1982: Insights from Minimum-Height Satellite Data\n\nAbstract: This study investigates the radial distribution of plasma pressure within the inner magnetosphere (IM) by analyzing magnetic field and electron data collected from two low-altitude spacecraft during a significant geomagnetic storm that occurred from March 1 to March 5, 1982. The analysis reveals notable variations in the plasma pressure profiles derived from different satellites, highlighting the complexity of the inner magnetospheric environment. Specifically, the pressure profile obtained from the GEOS-1 satellite exhibits a pronounced peak at L = 3, in contrast to the broader peaks observed around L = 4 from the ATS-6 and GEOS-2 satellites. These discrepancies in pressure profiles are likely attributed to the differing orbital paths of the satellites, which sample distinct regions of the magnetosphere. Furthermore, the study finds that when the pressure profiles from the three satellites are adjusted outward along the L-shell coordinate system, they exhibit a strong correlation. This alignment suggests that the observed shifts in pressure profiles are primarily due to spatial variations in plasma pressure rather than temporal fluctuations. The findings underscore the importance of satellite positioning and orbital characteristics in interpreting magnetospheric data and contribute to a deeper understanding of the dynamics of plasma pressure in the inner magnetosphere during geomagnetic storms. This research not only enhances our comprehension of the magnetospheric response to geomagnetic events but also provides valuable insights for future studies aimed at unraveling the complexities of space weather phenomena.",
        "ori-fast-z-score": 0.8251369970070347,
        "water-fast-z-score": 5.848076606885379,
        "rewrite-fast-z-score": 0.38138503569823695
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Prompt Emission of High Energy Photons from Gamma Ray Bursts .\nAbstract:\nWe report on the detection by Fermi Large Area Telescope (LAT) of prompt emission in gamma rays with energies above 100 MeV associated with GRB 080916C, one of the brightest bursts ever detected at high energy. The LAT data show that this emission is highly variable and peaks within 1 s after the onset of the burst. We find no evidence for spectral evolution during the first few hundred seconds following the trigger time. A simple power law fit to the LAT spectrum yields an index of -2.2 +/-.1 over the range 100 MeV - 10 GeV. This value is consistent with previous measurements made using Konus-Wind and AGILE satellites but differs significantly from those obtained by other instruments operating below 100 MeV. Our results are inconsistent with models which predict a softening of the photon spectrum as it evolves into the X-ray band. They also rule out scenarios where the high-energy photons originate solely from inverse Compton scattering off relativistic electrons accelerated in internal shocks. \n \n Keywords: Gamma-ray burst",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Prompt Emission of High Energy Photons from Gamma Ray Bursts . Abstract : We report on the discovery by Fermi Large Area Telescope ( LAT ) of prompt emission in gamma particles with energies above 100 MeV attributed with GRB 080916C , one of the brightest bursts ever observed at high energy .The LAT results show that this emission is strongly varied and peaks within 1 s after the beginning of the explosion . We see no evidence for spectral evolution during the first few hundred moments following the trigger time .A straightforward power law suited to the LAT spectrum gives an index of - 2 . 2 + / - . 1 over the range 100 MeV - 10 GeV . This value is consistent with previous measurements made using Konus - Wind and AGILE spacecraft but varies dramatically from those achieved by other instruments working below 100 MeV .Our results are inconsistent with models which predict a softening of the photon spectrum as it evolves into the X - ray band . They even point out situations where the high - energy photons arise solely from inverse Compton absorption off relativistic electrons accelerated in internal shocks .Keywords: Gamma-ray burst",
        "rewrite_text": "**Title:** Prompt Emission of High-Energy Photons from Gamma-Ray Bursts\n\n**Abstract:** In this study, we present significant findings from the Fermi Large Area Telescope (LAT) regarding the prompt emission of gamma-ray photons exceeding 100 MeV, specifically linked to GRB 080916C, which is recognized as one of the most luminous gamma-ray bursts recorded at high energies. Our analysis reveals that the emission is highly variable, with a notable peak occurring within the first second following the onset of the burst. Importantly, we observe no indications of spectral evolution during the initial moments after the trigger, suggesting a stable emission profile in this early phase. The LAT data can be accurately described by a simple power-law function, yielding a spectral index of -2.2 ± 0.1 across the energy range of 100 MeV to 10 GeV. This finding aligns with earlier observations made by the Konus-Wind and AGILE missions, yet it stands in stark contrast to results obtained from other instruments operating below 100 MeV, which report different spectral behaviors. Our results challenge existing theoretical models that predict a softening of the photon spectrum as it transitions into the X-ray regime. Instead, they support scenarios where high-energy photons are predominantly produced through inverse Compton scattering involving relativistic electrons that are accelerated during internal shock processes. This research enhances our understanding of the mechanisms driving gamma-ray bursts and the nature of their prompt emissions, providing critical insights into the high-energy astrophysical phenomena associated with these explosive events. \n\n**Keywords:** Gamma-ray burst, high-energy photons, Fermi Large Area Telescope, spectral index, inverse Compton scattering.",
        "ori-fast-z-score": -0.9058216273156765,
        "water-fast-z-score": 4.837877973981903,
        "rewrite-fast-z-score": -0.3592106040535498
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The IC1396N proto-cluster at a scale of 250 AU .\nAbstract:\nWe present new near-infrared (NIR) observations of the young stellar cluster IC 1396 N, located in the Orion Nebula Cluster region. The data were obtained with the adaptive optics system NAOS-CONICA on the VLT telescope and cover an area of 0.5 arcmin2 around the central star HD 37022. We detect more than 100 point sources down to Ks = 18 mag within this field-of-view. Using these data we have constructed colour-magnitude diagrams for different areas inside our field-of-view. From these CMDs we find that there are two populations of stars which can be separated by their position in the diagrams: one population is redder and fainter while another has bluer colours and brighter magnitudes. These results suggest that the first group consists mainly of low mass pre-main sequence stars surrounded by circumstellar disks, whereas the second group contains mostly high mass main-sequence stars without any surrounding material.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The IC1396N proto - cluster at a scale of 250 AU . Abstract : We report new near - infrared ( NIR ) observations of the young stellar cluster IC 1396 N , located in the Orion Nebula Cluster region .The data were obtained with the adaptive optics network NAOS - CONICA on the VLT telescope and cover an area of 0 . 5 arcmin2 around the central star HD 37022 . We detect more than 100 point sources down to Ks = 18 mag within this field - of - view .Using these information we have created colour - magnitude diagrams for different areas inside our field - of - view . From these CMDs we find that there are two communities of stars which can be apart by their placement in the diagrams : one community is redder and fainter while another has bluer colours and darker magnitudes .These data suggest that the first class consists primarily of low weight pre - principal sequence stars surrounded by circumstellar disks , whereas the second set contains primarily high mass primary - sequence stars without any surrounding material .",
        "rewrite_text": "We present new near-infrared (NIR) observations of the young stellar cluster IC 1396N, situated within the Orion Nebula Cluster region. Utilizing the adaptive optics system NAOS-CONICA on the Very Large Telescope (VLT), we collected data covering an area of 0.5 arcminutes squared around the central star HD 37022. Our observations revealed over 100 point sources with a magnitude limit of Ks = 18 within this field of view. To analyze these sources, we constructed color-magnitude diagrams (CMDs) for various regions within our observational area. The CMDs indicate the presence of two distinct groups of stars, differentiated by their positions in the diagrams. The first group is characterized by redder colors and fainter magnitudes, suggesting that these stars are primarily low-mass pre-main-sequence stars that are likely surrounded by circumstellar disks. In contrast, the second group exhibits bluer colors and brighter magnitudes, indicating that these stars are predominantly high-mass main-sequence stars that lack surrounding material. Our findings provide valuable insights into the stellar population and evolutionary stages within the IC 1396N proto-cluster, highlighting the diversity of stellar types present in this region and their implications for star formation processes. This study enhances our understanding of the dynamics and characteristics of young stellar clusters, contributing to the broader knowledge of stellar evolution in the context of the Orion Nebula.",
        "ori-fast-z-score": -0.5852057359806528,
        "water-fast-z-score": 4.4174102722651325,
        "rewrite-fast-z-score": -0.7770286898858113
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  SN 2006bp: Probing the Shock Breakout of a Type II-P Supernova .\nAbstract:\nWe present optical and near-infrared photometry of SN 2006bp, which was discovered on September 24th in NGC 6946 by amateur astronomer Brian Puckett (Puckett et al., 2007) . The supernova is located at an unusually large distance for its host galaxy, with a recession velocity of ~1000 km/sec. \n \n We find that the light curve can be well fit using a model consisting of three components: shock breakout emission, radioactive decay powered luminosity, and dust extinction. Using this model we derive physical parameters such as the progenitor radius, mass loss rate, and explosion energy. Our results are consistent with those found for other type-II SNe but suggest that the progenitor star had a lower initial mass than previously thought. This may indicate that there exists more diversity among progenitors of type-II SNe than has been realized so far. In addition to these findings, our observations provide new insights into the physics of shock breakout and early-time evolution of type-II SNe.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : SN 2006bp : Probing the Shock Breakout of a Type II - P Supernova . Abstract : We present visual and far - infrared photometry of SN 2006bp , which was discovered on September 24th in NGC 6946 by amateur astronomer Brian Puckett ( Puckett et al . , 2007 ) .The supernova is situated at an exceptionally wide distance for its host galaxy , with a collapse speeds of ~ 1000 km / sec . We see that the light curve can be well fitting using a simulation comprised of three components : shock breakout emission , radioactive decay powered luminosity , and dust disappearance .Using this model we derive physical factors such as the progenitor diameter , mass loss rate , and explosion power . Our results are compatible with those observed for other class - II SNe but suggest that the progenitor star had a smaller original mass than previously thought .This might suggest that there exists more diversity among progenitors of type - II SNe than has been realized so far . In addition to these conclusions , our observations offer additional perspectives into the physics of wave breakout and first - time progression of type - II SNe .",
        "rewrite_text": "We present a comprehensive analysis of SN 2006bp, a Type II-P supernova discovered on September 24th in the galaxy NGC 6946 by amateur astronomer Brian Puckett (Puckett et al., 2007). This supernova is notable for its unusually large distance from its host galaxy, exhibiting collapse velocities of approximately 1000 km/s. Our study includes detailed visual and far-infrared photometry, which allows us to construct a robust light curve. This light curve is effectively modeled using a three-component simulation that incorporates shock breakout emission, luminosity powered by radioactive decay, and the effects of dust disappearance. \n\nThrough this modeling approach, we are able to extract critical physical parameters, including the progenitor star's diameter, mass loss rate, and the energy released during the explosion. Our findings align with observations from other Type II supernovae but indicate that the progenitor star of SN 2006bp may have had a lower initial mass than previously estimated. This observation raises intriguing questions about the diversity of progenitor stars for Type II supernovae, suggesting a broader range of characteristics than has been acknowledged in the past.\n\nFurthermore, our observations provide valuable insights into the underlying physics of shock breakout phenomena and the early evolution of Type II supernovae. By enhancing our understanding of these processes, we contribute to the broader discourse on stellar evolution and supernova mechanics. Overall, the study of SN 2006bp not only sheds light on this specific event but also enriches our comprehension of the complexities associated with Type II-P supernovae and their progenitors.",
        "ori-fast-z-score": -1.3416407864998738,
        "water-fast-z-score": 5.079850199442941,
        "rewrite-fast-z-score": 0.2873478855663454
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The symmetric heavy-light ansatz .\nAbstract:\nWe present the results for the ground state energy and wave function obtained by solving numerically the Schrödinger equation with the potential V (r) = -1/(1+ar). The numerical solution is performed using the method based on the expansion in terms of B-splines, which allows to obtain very accurate results even if only few spline functions are used. We compare our results with those obtained within the framework of the variational approach. In particular we show that the use of the so-called  symmetric heavy-light ansatz  leads to an overestimation of the binding energies. \n \n Introduction \n \n In this work we consider the problem of finding the bound states of a particle moving in one dimension under the action of a short-range attractive interaction described by the following potential: \n \n \n \n V(x)=−V0 /  1+a x  , where V0 > 0 . \nThe corresponding Schrödinger equation reads as follows: \n \n Hψn = E n ψn , where H = − d 2 dx2 + V(x) , n denotes the quantum number associated with the radial part of the wavefunction and En its corresponding eigenvalue.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The symmetric heavy - light ansatz . Abstract : We present the results for the ground state energy and wave function achieved by solving numerically the Schrödinger equation with the potential V ( r ) = - 1 / ( 1 + ar ) .The mathematical solution is conducted using the method using on the expansion in terms of B - splines , which allows to obtain very accurate outcomes even if only few spline functions are using . We evaluate our findings with those achieved within the framework of the variational approach .In particular we prove that the using of the so - called symmetric light - heavy ansatz leads to an overestimation of the binding energies . Introduction In this study we investigate the question of finding the bound states of a particle moving in one dimension under the action of a small - range attractive interaction described by the following potential : V ( x ) = −V0 / 1 + a x , where V0 > 0 .The equivalent Schrödinger equation reads as follows : Hψn = E n ψn , where H = − d 2 dx2 + V ( x ) , n represents the quantum number associated with the radial part of the wavefunction and En its equivalent eigenvalue .",
        "rewrite_text": "Title: The Symmetric Heavy-Light Ansatz\n\nAbstract: In this article, we present our findings on the ground state energy and wave function derived from a numerical solution of the Schrödinger equation, specifically for the potential V(r) = -1/(1 + ar). Our approach employs a method based on B-spline expansions, which enables us to achieve highly accurate results even with a limited number of spline functions. We compare our results with those obtained through the variational method, highlighting the differences in outcomes. Notably, we demonstrate that the application of the symmetric heavy-light ansatz tends to overestimate the binding energies of the system. \n\nIn the introduction, we delve into the problem of identifying bound states for a particle in one-dimensional motion influenced by a short-range attractive interaction, characterized by the potential V(x) = -V0/(1 + ax), where V0 is a positive constant. The corresponding Schrödinger equation is expressed as Hψn = Enψn, where H represents the Hamiltonian operator defined as H = -d²/dx² + V(x). Here, n denotes the quantum number linked to the radial component of the wave function, while En signifies the associated eigenvalue. Our study aims to provide a comprehensive analysis of the implications of the symmetric heavy-light ansatz on the binding energies, contributing valuable insights to the field of quantum mechanics and the understanding of particle interactions under specific potential conditions.",
        "ori-fast-z-score": -1.9639610121239315,
        "water-fast-z-score": 2.9285611805518585,
        "rewrite-fast-z-score": -0.4975185951049946
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Geometry of Time, Axiom of Choice and Neuro-Biological Quantum Zeno Effect .\nAbstract:\nThe geometry of time is the study of how space-time evolves in time.  The axiom of choice states that for any collection of non-empty sets there exists at least one set which contains exactly one element from each set.   In this article we show that if the universe has an underlying quantum structure then it follows that the geometry of time can be described by using the axiom of choice to select a single point on every trajectory through spacetime.   We also discuss some possible implications of our results for neurobiology where the quantum Zeno effect may play a role in explaining certain aspects of brain function such as consciousness. The geometry of time is the mathematical description of how space-time evolutes over time  1  . It was first introduced into physics by Hermann Minkowski  2  who showed that the geometry of space-time could be represented by four numbers (x,y,z,t) called coordinates or co-ordinates. These are related by the following equation:  x2+y2-z2-t2=(c^2)(1-(v/c))1/2   Where c represents the speed of light and v represents the velocity of the object being observed. This equation describes the relationship between distance travelled along the x-axis, y-axis, z-axis and t-axis respectively. For example, if you were observing someone walking across your living room floor with their back towards you they would have a positive value for the x-axis but no values for the other three axes because they are not moving in those directions. If however you were watching them walk away from you they would have negative values for all three axes except the x-axis since they are still travelling forward in that direction. As another example consider two objects traveling side-by-side down a roadway. They will both travel at the same speed so their velocities will be equal. However, if one car travels north while the other travels south they will appear to move faster than each other even though they are traveling at the same speed.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Geometry of Time , Axiom of Choice and Neuro - Biological Quantum Zeno Effect . Abstract : The geometry of time is the science of how space - time evolves in time .The axiom of selection asserts that for any set of non - empty sets there exists at least one collection which contains exactly one element from each set . In this paragraph we prove that if the universe has an underlying quantum structure then it follows that the topology of time can be described by using the axiom of choice to select a single position on every trajectory through spacetime .We also discuss some possible possibilities of our findings for neurobiology where the molecular Zeno effect could play a role in understanding particular aspects of cerebral activity such as consciousness . The geometry of time is the numerical model of how space - time evolutes over time 1 .It was first developed into physics by Hermann Minkowski 2 who proved that the topology of space - time could be described by four numbers ( x , y , z , t ) called coordinates or co - ordinates . These are related by the following equation : x2 + y2 - z2 - t2 = ( c ^ 2 ) ( 1 - ( v / c ) ) 1 / 2 Where c represents the speed of light and v indicates the velocity of the instrument being detected .This equation relates the relationship between distance travelled along the x - axis , y - axis , z - axis and t - axis respectively . For instance , if you were observing someone moving across your living hall room with their back towards you they may have a positive value for the x - axis but no values for the other three axes because they are not moving in those directions .If however you were watching them walk away from you they may have negative expressions for all three axes except the x - axis since they are still moving forward in that direction . As another example compare two bodies walking side - by - side down a street .They will both travel at the same speed so their velocities will be equal . However , if one vehicle rides north while the other travels south they will appear to move faster than each other even though they are traveling at the same speed .",
        "rewrite_text": "**Title:** Geometry of Time, Axiom of Choice, and Neuro-Biological Quantum Zeno Effect\n\n**Abstract:** This article explores the intricate relationship between the geometry of time, the axiom of choice, and their implications for neurobiology, particularly in the context of the quantum Zeno effect. The geometry of time refers to the mathematical framework that describes the evolution of space-time, a concept initially formalized by Hermann Minkowski. He demonstrated that the topology of space-time can be represented using four coordinates: (x, y, z, t), which are interconnected through the equation x² + y² - z² - t² = c²(1 - (v/c))^(1/2), where c denotes the speed of light and v represents the velocity of the observed object. This equation illustrates how the spatial and temporal dimensions interact, allowing for a comprehensive understanding of motion within the universe.\n\nIn this study, we propose that if the universe possesses an inherent quantum structure, the topology of time can be effectively described by employing the axiom of choice. This axiom posits that for any collection of non-empty sets, there exists a selection of one element from each set. By applying this principle, we can identify a unique position along every trajectory in space-time, thereby enriching our understanding of temporal dynamics.\n\nFurthermore, we delve into the implications of our findings for neurobiology, suggesting that the molecular quantum Zeno effect may provide insights into specific aspects of cerebral function, including consciousness. The quantum Zeno effect, which posits that frequent observations can inhibit the evolution of a quantum system, could offer a novel perspective on how conscious awareness is maintained and influenced by neurobiological processes. Through this interdisciplinary approach, we aim to bridge the gap between theoretical physics and neurobiology, opening new avenues for research into the fundamental nature of time and consciousness.",
        "ori-fast-z-score": 0.5586608191273356,
        "water-fast-z-score": 6.841792643911278,
        "rewrite-fast-z-score": 2.8401877872187726
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Precision measurement of the Casimir-Lifshitz force in a fluid .\nAbstract:\nWe report on an experiment to measure the Casimir-Lifhsztz (CL) interaction between two gold-coated glass plates immersed in water at room temperature and pressure. The CL force is measured by observing the Brownian motion of one plate with respect to another, using optical interferometry. We find that the magnitude of the observed effect agrees well with theoretical predictions based on Lifshitz theory for dielectrics. This work represents the first direct experimental observation of the CL force in a liquid medium. It also demonstrates how precision measurements can be used to test fundamental theories such as quantum electrodynamics. \n \n In recent years there has been considerable interest in measuring the Casimir-Lifhzsiz (CL)  1  force between macroscopic objects  2  . Such experiments are important because they provide tests of our understanding of vacuum fluctuations  3  , which play a central role in many areas of physics including quantum field theory  4  , statistical mechanics  5  , condensed matter  6  , atomic and nuclear physics  7  , cosmology  8  , and gravitation  9  .\n \nThe original prediction of the CL force was made more than 50 years ago  10  but it took until 1997  11  before this attractive force could be directly detected experimentally  12  . Since then several groups have performed high-precision experiments  13  -  16  aimed at testing the validity of various aspects of the theory  17  -  20  . \n \n Here we present results obtained in a new experiment designed specifically to study the CL force in liquids  21  . Our approach involves immersing two parallel plates coated with thin layers of gold into distilled water contained inside a sealed container  22  . By monitoring the Brownian motion of these plates  23  we were able to determine their mutual attraction due to the presence of the surrounding water molecules  24  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Precision measurement of the Casimir - Lifshitz force in a fluid . Abstract : We report on an research to measure the Casimir - Lifhsztz ( CL ) relationship between two gold - glazed glazed plates immersed in water at room temperature and tension .The CL force is measured by observing the Brownian movement of one plate with regard to another , using optical interferometry . We see that the magnitude of the seen effect agrees well with theoretical expectations based on Lifshitz principle for dielectrics .This study constitutes the first continuous experimental measurement of the CL force in a liquid medium . It additionally demonstrates how accuracy observations can be used to test fundamental theories such as quantum electrodynamics .In recent years there has been substantial interest in measuring the Casimir - Lifhzsiz ( CL ) 1 pressure between macroscopic objects 2 . Such experiments are important because they give tests of our knowing of vacuum fluctuations 3 , which take a central role in multiple fields of science including quantum field theory 4 , statistical mechanics 5 , condensed matter 6 , atomic and nuclear science 7 , cosmology 8 , and gravitation 9 .The original forecast of the CL force was making more than 50 centuries earlier 10 but it taking until 1997 11 before this attractive force could be directly discovered experimentally 12 . Since then several organizations have done large - precision tests 13 - 16 aiming at testing the legitimacy of several elements of the principle 17 - 20 .Here we present results derived in a new study intended specifically to study the CL force in liquids 21 . Our solution involves immersing two connected sheets coated with thin layers of gold into distilled water contained inside a sealed container 22 .By observing the Brownian movement of these plates 23 we were could to obtain their mutual attraction owing to the presence of the nearby water molecules 24 .",
        "rewrite_text": "Title: Precision Measurement of the Casimir-Lifshitz Force in a Fluid\n\nAbstract: In this study, we present a detailed investigation into the Casimir-Lifshitz (CL) force between two gold-coated plates submerged in water at room temperature and under tension. The measurement of the CL force is achieved by analyzing the Brownian motion of one plate relative to the other, utilizing optical interferometry as our primary technique. Our findings indicate that the observed force aligns closely with theoretical predictions derived from the Lifshitz theory for dielectric materials. This research marks the first continuous experimental assessment of the CL force within a liquid medium, showcasing how precise measurements can be instrumental in testing fundamental theories, including quantum electrodynamics.\n\nThe Casimir-Lifshitz force has garnered significant attention in recent years, particularly regarding its implications for vacuum fluctuations, which are pivotal across various scientific domains such as quantum field theory, statistical mechanics, condensed matter physics, atomic and nuclear science, cosmology, and gravitation. Although the theoretical foundation for the CL force was established over five decades ago, it wasn't until 1997 that this attractive force was directly observed in experiments. Since then, numerous research groups have conducted high-precision experiments aimed at validating various aspects of the CL principle.\n\nIn our work, we focus specifically on the CL force in liquid environments. We employed a novel experimental setup involving two gold-coated plates immersed in distilled water within a sealed chamber. By meticulously observing the Brownian motion of these plates, we were able to quantify their mutual attraction, which arises from the influence of surrounding water molecules. This research not only contributes to the understanding of the CL force in fluids but also reinforces the significance of experimental physics in exploring fundamental interactions at the quantum level.",
        "ori-fast-z-score": -1.0795912380986197,
        "water-fast-z-score": 6.7863682865394175,
        "rewrite-fast-z-score": 0.08481889296799709
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Millennium Galaxy Catalogue: The local supermassive black hole mass function in early- and late-type galaxies .\nAbstract:\nWe present the first measurement of the supermassive black hole (SMBH) mass function for both early-(elliptical/S0, Sa-Sb) and latetype (Scd-Sm) galaxies using data from the Millennium Galaxy Catalogue (MGC). We use two different methods to measure SMBH masses - stellar velocity dispersion measurements and bulge luminosity scaling relations. Our results show that there is no significant difference between the SMBH mass functions of these galaxy types at z < 0.1. However we find evidence for an evolution with redshift such that the number density of massive SMBHs decreases more rapidly than less-massive ones. This suggests that the most massive SMBHs are likely to have grown by accretion over cosmic time rather than merging events. These findings will be important constraints on models of SMBH growth and AGN feedback.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Millennium Galaxy Catalogue : The regional supermassive black hole mass function in early - and mid - class objects . Abstract : We report the first measurement of the supermassive black hole ( SMBH ) mass function for both late - ( elliptical / S0 , Sa - Sb ) and latetype ( Scd - Sm ) galaxies using data from the Millennium Galaxy Catalogue ( MGC ) .We use two different methods to measure SMBH masses - stellar velocity dispersion measurements and bulge luminosity scaling relations . Our results show that there is no major variation between the SMBH mass parameters of these galaxy types at h < 0 . 1 .However we find proof for an evolution with redshift such that the number density of large SMBHs falls more swiftly than less - massive ones . This implies that the most gigantic SMBHs are likely to have expanded by accretion over cosmic time rather than merging events .These conclusions will be crucial constraints on estimates of SMBH growth and AGN feedback .",
        "rewrite_text": "In this study, we present the inaugural measurement of the supermassive black hole (SMBH) mass function across both late-type galaxies (specifically elliptical, S0, Sa, and Sb) and late-type galaxies (including Scd to Sm) utilizing data sourced from the Millennium Galaxy Catalogue (MGC). Our approach employs two distinct methodologies for determining SMBH masses: the analysis of stellar velocity dispersion and the application of bulge luminosity scaling relations. The findings indicate that there is minimal variation in the SMBH mass parameters among the different galaxy classifications at a redshift of h < 0.1. However, we observe a significant evolutionary trend with redshift, revealing that the number density of larger SMBHs diminishes at a faster rate compared to their less massive counterparts. This observation suggests that the most massive SMBHs have likely grown primarily through accretion processes over cosmic time, rather than through the merger of smaller black holes. These insights are pivotal for refining our understanding of SMBH growth mechanisms and the feedback processes associated with active galactic nuclei (AGN). The implications of our results extend to the broader context of galaxy evolution and the role of supermassive black holes in shaping the structure and dynamics of galaxies throughout the universe's history.",
        "ori-fast-z-score": -0.8307471607356973,
        "water-fast-z-score": 4.09644015186457,
        "rewrite-fast-z-score": 0.5184758473652127
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Infrared Spectral Energy Distributions of z~0.7 Star-Forming Galaxies .\nAbstract:\nWe present infrared spectral energy distributions (SEDs) for a sample of 23 galaxies at redshifts 0.6 < z < 1.0, selected to have strong rest-frame ultraviolet emission lines and high star formation rates. The SEDs are derived using Spitzer Infrared Spectrograph observations in the 3.6 - 8 micron range combined with ground-based near-infrared spectroscopy covering the wavelength range between 0.9-2.5 microns. We use these data to derive stellar masses, ages, dust extinction values, and star-formation histories for each galaxy. Our results show that most of our objects are young systems with ages less than 100 Myr old; however we also find evidence for older populations in some cases. Most of the galaxies appear to be experiencing moderate levels of dust obscuration with A V ~ 1-3 mag., although there is significant scatter among individual objects.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Infrared Spectral Energy Distributions of z ~ 0 . 7 Star - Forming Galaxies . Abstract : We create infrared spectral power distributions ( SEDs ) for a sample of 23 galaxies at redshifts 0 . 6 < z < 1 . 0 , selected to have strong rest - frame ultraviolet emission lines and large galaxy formation rates .The SEDs are derived using Spitzer Infrared Spectrograph observations in the 3 . 6 - 8 micron range coupled with ground - based near - infrared spectroscopy spanning the frequency range between 0 . 9 - 2 . 5 microns . We use these information to derive stellar masses , ages , dust extinction values , and star - formation histories for each galaxy .Our results show that most of our bodies are young structures with ages less than 100 Myr young ; however we also find proof for older communities in some cases . Most of the galaxies appear to be experiencing low levels of dust obscuration with A V ~ 1 - 3 mag . , although there is substantial scatter among individual objects .",
        "rewrite_text": "In this study, we present the infrared spectral energy distributions (SEDs) for a sample of 23 star-forming galaxies located at redshifts between 0.6 and 1.0. These galaxies were specifically chosen for their pronounced rest-frame ultraviolet emission lines and significant rates of star formation. The SEDs were constructed using observations from the Spitzer Infrared Spectrograph, focusing on the wavelength range of 3.6 to 8 microns, in conjunction with ground-based near-infrared spectroscopy that covers wavelengths from 0.9 to 2.5 microns. This comprehensive dataset allows us to derive key astrophysical parameters for each galaxy, including stellar masses, ages, dust extinction values, and star formation histories.\n\nOur analysis reveals that the majority of the galaxies in our sample are relatively young, with ages typically less than 100 million years. However, we also identify evidence of older stellar populations in certain cases, indicating a diverse range of evolutionary stages among these galaxies. Furthermore, we observe that most of the galaxies exhibit low levels of dust obscuration, with average extinction values (A_V) around 1 to 3 magnitudes. Despite this general trend, there is notable variability in dust content among individual galaxies, suggesting that the environments and conditions influencing star formation and dust accumulation can differ significantly from one galaxy to another.\n\nOverall, our findings contribute to the understanding of star formation processes in the early universe, highlighting the complexities of galaxy evolution and the role of dust in shaping the observable properties of star-forming galaxies at redshifts close to 0.7. This research not only enhances our knowledge of the infrared characteristics of these galaxies but also provides valuable insights into their formation and development during a critical period in cosmic history.",
        "ori-fast-z-score": 1.0533703247651751,
        "water-fast-z-score": 4.88240082724041,
        "rewrite-fast-z-score": 1.4524080181184935
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Supernova Remnants and GLAST .\nAbstract:\nThe Large Area Telescope (LAT) on the Fermi Gamma-ray Space Telescope will be launched in June 2008, with an expected launch date of August 7, 2007.  The LAT is designed to detect gamma rays between 20 MeV and 300 GeV over a large field-of-view (2 steradians), making it possible for the first time to study the entire sky at these energies every three hours. This talk will discuss how supernova remnants are likely to appear as sources of high-energy emission when observed by this new instrument. In particular I will focus on:  - How many remnants should we expect to see? - What types of remnants might show up? - What can we learn about particle acceleration mechanisms? - What can we do to prepare for the observations? - Other topics that may come up during discussion. The Large Area Telescope (L AT ) on the Fermi Gamma-ray Sp ace T elescope w ill b e la unched i n J une 2 008 , wi th an ex pected l aunch d ate o f A ugust 7 , 2 007 .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Supernova Remnants and GLAST . Abstract : The Large Area Telescope ( LAT ) on the Fermi Gamma - ray Space Telescope will be launched in June 2008 , with an anticipated flight date of August 7 , 2007 .The LAT is designed to identify gamma radiation between 20 MeV and 300 GeV over a large field - of - view ( 2 steradians ) , making it able for the first time to study the entire sky at these energies every three hours . This discussion will explore how supernova remnants are likely to appear as causes of high - energy emitted when observed by this new instrument .In particular I will focus on : - How many traces should we expect to see ? - What kinds of remnants may show up ?- What can we teach about particle acceleration mechanisms ? - What can we do to plan for the discoveries ?- Other topics that might come up during discussion . The Large Area Telescope ( L AT ) on the Fermi Gamma - ray Sp ace T elescope l ill b e la unched i n J une 2 008 , wi th an ex pected r aunch d ate o g A ugust 7 , 2 007 .",
        "rewrite_text": "Title: Supernova Remnants and GLAST\n\nAbstract: The Large Area Telescope (LAT), part of the Fermi Gamma-ray Space Telescope, is set to be launched in June 2008, with a projected launch date of August 7, 2007. This advanced instrument is engineered to detect gamma radiation in the energy range of 20 MeV to 300 GeV, covering a substantial field of view of 2 steradians. For the first time, the LAT will enable comprehensive observations of the entire sky at these high-energy levels every three hours. This article delves into the potential role of supernova remnants as significant sources of high-energy gamma radiation, as observed by the LAT. Key areas of focus include: the expected number of detectable traces from supernova remnants, the types of remnants that may be identified, insights into particle acceleration mechanisms that can be gleaned from these observations, and strategies for preparing for the discoveries that the LAT will facilitate. Additionally, the discussion will encompass other relevant topics that may arise during the exploration of supernova remnants and their contributions to our understanding of high-energy astrophysics. The findings from this research are anticipated to enhance our comprehension of cosmic phenomena and the underlying processes that govern the universe's most energetic events.",
        "ori-fast-z-score": 1.162476387438193,
        "water-fast-z-score": 5.888972745734182,
        "rewrite-fast-z-score": 0.9761870601839528
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Chromatin Folding in Relation to Human Genome Function .\nAbstract:\nThe human genome is organized into chromatin, which consists of DNA wrapped around histone proteins and other associated factors. The folding of the chromatin fiber plays an important role in regulating gene expression by controlling access of transcription machinery to genes. In this review we discuss recent advances on how chromatin folds at different levels (from single nucleosome to chromosome) and its relation with human genome function. We also introduce some new concepts such as  topological domains  and  chromosome territories  that are emerging from high-throughput technologies. Finally, we provide examples for future research directions. Chromatin is the basic unit of eukaryotic chromosomes consisting of DNA wrapped around histones and other associated factors. It has been shown that chromatin structure plays an important role in regulation of gene expression through control of accessibility of transcription machinery to genes  1  . Recent studies have revealed that chromatin fibers fold into higher-order structures ranging from 30 nm fibers  2  , loops  3  , topologically associating domains  4  , and chromosome territories  5  .\nIn this review, we will first briefly describe the organization of chromatin fibers at various scales. Then we will focus on the relationship between chromatin folding and human genome functions including gene regulation, replication, repair, and recombination. Finally, we will give several examples for possible future research directions.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Chromatin Folding in Relation to Human Genome Function . Abstract : The human genome is organized into chromatin , which consists of DNA wrapped around histone proteins and other associated factors .The folding of the chromatin fiber plays an important role in controlling gene transcription by directing access of transcription apparatus to genes . In this review we explain latest advances on how chromatin folds at different levels ( from single nucleosome to chromosome ) and its connection with human genome function .We also add some fresh concepts such as topological domains and genome territories that are emerging from high - throughput devices . Finally , we provide examples for future research approaches .Chromatin is the fundamental unit of eukaryotic chromosomes consisting of DNA wrapped around histones and other associated factors . It has been shown that chromatin structure serves an important role in controlling of gene transcription through regulating of accessibility of transcription apparatus to proteins 1 .Recent research have revealed that chromatin fibers fold into greater - order structures ranging from 30 nm fibers 2 , loops 3 , topologically associating domains 4 , and chromosome regions 5 . In this review , we will first briefly understand the organization of chromatin fibers at several scales .Then we will focus on the relationship between chromatin folding and human genome activities including protein regulation , replication , maintenance , and recombination . Finally , we will provide several examples for future future research paths .",
        "rewrite_text": "**Title:** Chromatin Folding in Relation to Human Genome Function\n\n**Abstract:** The organization of the human genome is intricately linked to chromatin, a complex structure comprising DNA wrapped around histone proteins and various associated factors. The folding of chromatin fibers is crucial for regulating gene transcription, as it influences the accessibility of the transcription machinery to specific genes. This review highlights recent advancements in our understanding of chromatin folding at multiple levels, from the individual nucleosome to entire chromosomes, and explores its implications for human genome functionality. We introduce emerging concepts such as topological domains and genome territories, which have arisen from high-throughput sequencing technologies and advanced imaging techniques. These concepts provide new insights into the spatial organization of the genome and its functional consequences.\n\nRecent studies have demonstrated that chromatin fibers can form higher-order structures, including 30 nm fibers, loops, topologically associating domains (TADs), and distinct chromosome regions. In this review, we will first outline the hierarchical organization of chromatin fibers across various scales. Subsequently, we will delve into the interplay between chromatin folding and key genomic processes, including gene regulation, DNA replication, maintenance, and recombination. By elucidating these relationships, we aim to underscore the significance of chromatin architecture in orchestrating genome function.\n\nMoreover, we will propose several avenues for future research that could further illuminate the complexities of chromatin dynamics and their impact on genomic integrity and expression. Understanding these mechanisms is essential for deciphering the roles of chromatin in health and disease, paving the way for potential therapeutic interventions targeting chromatin structure and function.",
        "ori-fast-z-score": -1.212256250712408,
        "water-fast-z-score": 5.199469468957452,
        "rewrite-fast-z-score": 1.104689541477988
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The effect of a fifth large-scale space-time dimension on the conservation of energy in a four dimensional Universe .\nAbstract:\nThe present work is an attempt to show that the concept of time can be extended into a higher-dimensional space, and that this extension may have important consequences for our understanding of physical phenomena.  The author considers the possibility that there are five dimensions of space (four ordinary spatial dimensions plus one extra temporal dimension) which could explain some of the observed properties of matter such as entropy production and irreversibility.   In particular he shows how the existence of these additional dimensions would lead to a violation of the principle of entropy increase with time, and suggests that this might provide a possible explanation for the arrow of time. This article is available from: http://arxiv.org/abs/astro-ph/0403070v1. Introduction:  Time has always been considered by physicists as being fundamentally different from other quantities like position or velocity because it cannot be measured directly but only inferred indirectly through its effects on other measurable quantities.  However, recent developments in theoretical physics suggest that we should consider whether the concept of time itself needs to be modified so that it becomes more closely related to other fundamental concepts such as mass, charge and energy.  For example, string theory predicts that all particles are vibrating strings moving along a multidimensional space called  space-time   1  .    Another approach involves considering the possibility that time is not just another quantity but rather part of a larger structure known as spacetime  2  , where the latter consists of both space and time together  3  .  According to this viewpoint, time is no longer regarded as something separate from space; instead they are viewed as two aspects of the same thing  4  .\nIn fact, many modern theories of quantum gravity predict that the universe contains at least three large scale dimensions - namely length, width and height  5  - while also containing a fourth small-scale dimension  6  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The impact of a fifth wide - scale space - time dimension on the conservation of energy in a four dimensional Universe . Abstract : The present work is an attempt to see that the notion of time can be enlarged into a higher - dimensional space , and that this extension may have important implications for our understanding of natural behavior .The author considers the idea that there are five dimensions of space ( four ordinary spatial dimensions plus one extra temporal dimension ) which could explain some of the seen characteristics of matter such as entropy production and irreversibility . In particular he shows how the existence of these additional dimensions would result to a violation of the principle of entropy increase with time , and suggests that this might give a possible reason for the arrow of time .This section is accessible from : www : / / arxiv . org / abs / astro - ph / 0403070v1 . Introduction : Time has always been regarded by physicists as being fundamentally changed from other quantities like position or speed because it cannot be measured immediately but only inferred indirectly through its consequences on other measurable quantities .However , recent developments in theoretical physics suggest that we should consider whether the notion of time itself needs to be altered so that it becomes more closely related to other fundamental concepts such as mass , charge and energy . For instance , string theory predicts that all atoms are vibrating chords moving along a multidimensional space termed space - time 1 .Another approach requires studying the idea that time is not just another quantity but rather component of a greater formation named as spacetime 2 , where the former consists of both space and time together 3 . According to this viewpoint , time is no longer regarded as something separate from space ; simply they are regarded as two forms of the same thing 4 .In indeed , many contemporary explanations of quantum gravitational suggest that the universe possesses at least three large scale dimensions - notably length , diameter and size 5 - while also containing a fourth short - scale dimension 6 .",
        "rewrite_text": "**Title:** The Impact of a Fifth Wide-Scale Space-Time Dimension on Energy Conservation in a Four-Dimensional Universe\n\n**Abstract:** This study explores the concept of expanding the traditional understanding of time into a higher-dimensional framework, positing that the inclusion of a fifth dimension—an additional temporal dimension alongside the conventional four spatial dimensions—could significantly enhance our comprehension of various natural phenomena. The author investigates how this fifth dimension may elucidate certain observed properties of matter, particularly in relation to entropy production and the irreversibility of processes. The findings suggest that the presence of this extra dimension could challenge the established principle of entropy increase over time, potentially offering insights into the nature of the arrow of time itself. This research aims to bridge the gap between time and other fundamental physical quantities, such as mass, charge, and energy, proposing a more integrated view of these concepts. \n\nHistorically, physicists have treated time as a unique entity, distinct from other measurable quantities like position or velocity, primarily because it is inferred indirectly through its effects. However, recent advancements in theoretical physics indicate a need to reconsider this perspective. For instance, string theory posits that all particles are manifestations of vibrating strings existing within a multi-dimensional space-time continuum. Additionally, some theories advocate for a unified framework where time is not merely an isolated dimension but rather an integral component of a broader construct known as space-time, wherein space and time are interdependent aspects of a singular reality. \n\nContemporary theories in quantum gravity further support the notion that the universe comprises at least three large-scale dimensions—length, width, and height—alongside a fourth, potentially compact dimension. This research contributes to the ongoing discourse by suggesting that the introduction of a fifth dimension may provide a deeper understanding of energy conservation and the fundamental laws governing our universe. For further details, the full text is available at: www.arxiv.org/abs/astro-ph/0403070v1.",
        "ori-fast-z-score": -1.47026414181486,
        "water-fast-z-score": 6.577497476540163,
        "rewrite-fast-z-score": 0.15523010514126656
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Distance to Orion KL Measured with VERA .\nAbstract:\nWe report the distance measurement toward the Galactic center using Very Long Baseline Array (VLBA) observations at 22 GHz and 43 GHz in combination with Japanese VLBI Exploration of Radio Astrometry (VERA). The parallax was measured by observing Sgr A*, which is located near the Galactic center, for two years between 2007 and 2009. We found that the distance to the Galactic center is R0 = 8 kpc ± 0.4 kpc. This value agrees well with previous measurements based on other methods such as infrared photometry or trigonometric parallaxes of masers associated with massive young stars. Our result also supports the hypothesis that the Milky Way has an axisymmetric mass distribution around its central black hole. \n \n Keywords: Distance scale, Galaxy, Parallax, Space astrometry, Black holes \n \n \n \n 1 Introduction \n \n In order to understand how galaxies evolve over time, it is important to know their distances accurately. However, accurate distances are difficult to measure because they depend strongly on the assumed luminosity evolution model. For example, if we assume too high a rate of luminosity evolution, then the derived distance will be underestimated. On the other hand, if we assume too low a rate of luminosity evolu-tion, then the derived distance may be overestimated. Therefore, it is necessary to determine the correct luminosity evolution model before deriving the distance to any galaxy. \n \n One way to solve this problem is to use radio sources whose distances can be determined independently through other means. These include pulsars, quasars, and maser sources associated with star-forming regions. Among these objects, maser sources have been used most frequently since they provide very precise distance estimates. Maser sources are usually associated with star forming regions where water vapor molecules form into microscopic crystals known as ice grains. When the ice grains grow larger than about one micron, they become unstable against gravitational collapse and begin emitting intense radiation. Since the emission line widths of maser sources are extremely narrow compared to those of normal radio",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Distance to Orion KL Measured with VERA . Abstract : We report the distance measurement toward the Galactic center utilizing Very Long Baseline Array ( VLBA ) observations at 22 GHz and 43 GHz in combination with Japanese VLBI Exploration of Radio Astrometry ( VERA ) .The parallax was measured by observing Sgr A * , which is situated near the Galactic center , for two years between 2007 and 2009 . We determined that the distance to the Galactic center is R0 = 8 kpc ± 0 . 4 kpc .This value agrees well with previous measurements based on other methods such as infrared photometry or trigonometric parallaxes of masers associated with massive young galaxies . Our result even suggests the notion that the Milky Way has an axisymmetric mass distribution around its central black hole .Keywords : Distance scale , Galaxy , Parallax , Space astrometry , Black holes 1 Introduction In order to comprehend how galaxies evolve over time , it is important to predict their distances accurately . However , accurate distances are hard to measure because they rely heavily on the expected luminosity evolution theory .For instance , if we suppose too high a rate of luminosity progression , then the derived length will be underestimated . On the other hand , if we suppose too low a rate of luminosity evolu - tion , then the derived distance might be overestimated .Therefore , it is required to obtain the appropriate luminosity evolution theory before deriving the distance to any galaxy . One method to solve this question is to use radio sources whose distances can be determined independently through other methods .These include pulsars , quasars , and maser sources associated with star - creating areas . Among these objects , maser sources have been used most regularly since they give very exact distance estimates .Maser sources are typically associated with star producing regions where water vapor molecules form into microscopic particles termed as ice particles . When the ice particles develop larger than about one micron , they become unstable against gravitational failure and begin emitting intense rays .Since the emission line widths of maser sources are extremely narrow compared to those of normal radio",
        "rewrite_text": "**Title: Distance to Orion KL Measured with VERA**\n\n**Abstract:** In this study, we present a precise measurement of the distance to the Galactic center, employing Very Long Baseline Array (VLBA) observations at frequencies of 22 GHz and 43 GHz, in conjunction with the Japanese VLBI Exploration of Radio Astrometry (VERA). The parallax method was utilized by monitoring Sgr A*, the supermassive black hole located near the Galactic center, over a two-year period from 2007 to 2009. Our findings indicate that the distance to the Galactic center is R0 = 8 kpc ± 0.4 kpc. This measurement is consistent with previous estimates derived from various techniques, including infrared photometry and trigonometric parallaxes of masers linked to massive star-forming regions. Notably, our results lend support to the hypothesis that the Milky Way exhibits an axisymmetric mass distribution surrounding its central black hole. \n\nUnderstanding the distances to galaxies is crucial for comprehending their evolutionary processes; however, accurately determining these distances poses significant challenges. Traditional methods often rely on luminosity evolution theories, which can lead to substantial errors if the assumed rates of luminosity progression are incorrect. Overestimating the luminosity evolution can result in underestimating distances, while underestimating it can lead to overestimations. To address these challenges, we advocate for the use of radio sources, such as pulsars, quasars, and maser sources, which allow for independent distance measurements. Among these, maser sources are particularly valuable due to their high precision in distance estimation. These sources are typically found in star-forming regions where water vapor molecules condense into ice particles. When these particles exceed a certain size, they become gravitationally unstable and emit strong radiation. The narrow emission line widths of maser sources, in contrast to those of typical radio sources, further enhance the accuracy of distance measurements. This research contributes to the broader understanding of the Galactic structure and the dynamics of its central region. \n\n**Keywords:** Distance scale, Galaxy, Parallax, Space astrometry, Black holes.",
        "ori-fast-z-score": 1.2456821978060995,
        "water-fast-z-score": 7.291164092619461,
        "rewrite-fast-z-score": 2.853216618208743
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Study of Damage Propagation at the Interface Localization-Delocalization Transition of the Confined Ising Model .\nAbstract:\nWe study damage propagation in an interface between two ordered phases of the confined Ising model (CIM) with random fields and quenched disorder, which is known to undergo a localization-delocalization transition as its temperature T crosses Tc = 1. We show that this transition can be observed by measuring the average size of avalanches triggered by local perturbations. The results are compared with those for the unperturbed CIM obtained using Monte Carlo simulations on large lattices. In particular we find that the distribution of avalanche sizes changes drastically across the transition point. This behavior is explained within the framework of the mean-field theory developed recently for the CIM. Finally, we discuss possible experimental realizations of our system. Introduction:-The phenomenon of phase coexistence has been studied extensively both theoretically  1  -  4  and experimentally  5  . It occurs when different thermodynamic states coexist in equilibrium  6  , or metastable states exist simultaneously  7  . A typical example is provided by water  8  where ice Ih and liquid water co-exist below 0 o C  9  .\nIn recent years there have been several studies  10  -  12  devoted to understanding how interfaces separating different phases evolve under external driving forces such as thermal fluctuations  13  , magnetic field  14  , mechanical stress  15  etc.. These investigations were motivated mainly by experiments performed on various materials  16  including ferroelectrics  17  , ferromagnets  18  , superconductors  19  , colloids  20  , granular media  21  , glasses  22  , foams  23  , and biological systems  24  . For instance, it was found  25  that the dynamics of domain walls in magnets  26  depends crucially on whether they are pinned  27  or not  28  . Similarly, the response of glassy  29  and jammed  30  systems to shear stresses  31  strongly depends on their preparation history  32  . On the other hand, the effect of quenched disorder  33  on the properties of interfaces  34  remains poorly understood  35  despite numerous theoretical  36   38  and numerical  39  attempts made over the past few decades.\nRecently, the problem of interface evolution attracted renewed interest due to the discovery of new types of transitions occurring in spatially extended systems  40   41 :",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Study of Damage Propagation at the Interface Localization - Delocalization Transition of the Confined Ising Model . Abstract : We research harm propagation in an interface between two ordered phases of the confined Ising model ( CIM ) with random fields and quenched disorder , which is known to undergo a localization - delocalization transition as its temperature T crosses Tc = 1 .We see that this shift can be identified by measuring the average size of avalanches caused by local perturbations . The results are compared with those for the unperturbed CIM achieved using Monte Carlo simulations on huge lattices .In particular we find that the distribution of avalanche sizes changes significantly across the transfer point . This phenomenon is studied within the framework of the mean - field model formulated recently for the CIM .Finally , we explain possible experimental realizations of our system . Introduction : - The phenomenon of phase coexistence has been studied thoroughly both theoretically 1 - 4 and experimentally 5 .It happens when distinct thermodynamic states coexist in equilibrium 6 , or metastable states arise simultaneously 7 . A typical example is provided by water 8 where ice Ih and fluid water co - operate below 0 o C 9 .In recent years there have been numerous research 10 - 12 devoted to investigating how interfaces separating different components develop under external driving forces such as heat fluctuations 13 , magnetic force 14 , thermal strain 15 etc . . These studies were driven mainly by research performed on various structures 16 notably ferroelectrics 17 , ferromagnets 18 , superconductors 19 , colloids 20 , granular material 21 , glasses 22 , foams 23 , and biological environments 24 .For instance , it was shown 25 that the dynamics of domain walls in magnets 26 depends crucially on whether they are pinned 27 or not 28 . Similarly , the response of glassy 29 and jammed 30 systems to shear forces 31 strongly depends on their preparation history 32 .On the other hand , the impact of quenched disorder 33 on the properties of interfaces 34 remains poorly regarded 35 despite several experimental 36 38 and mathematical 39 attempts made over the previous few years . Recently , the issue of interface evolution enjoyed continued interest due to the discovery of new types of transitions happening in spatially extended systems 40 41 :",
        "rewrite_text": "**Title:** Investigation of Damage Propagation at the Interface Localization-Delocalization Transition in the Confined Ising Model\n\n**Abstract:** This study explores the dynamics of damage propagation at the interface between two ordered phases in the confined Ising model (CIM) characterized by random fields and quenched disorder. The CIM is known to exhibit a localization-delocalization transition when the temperature (T) surpasses the critical temperature (Tc = 1). Our research identifies this transition by analyzing the average size of avalanches triggered by localized perturbations. We compare our findings with those from unperturbed CIM configurations, utilizing Monte Carlo simulations conducted on large lattice structures. Notably, we observe a significant alteration in the distribution of avalanche sizes across the transition point. This behavior is examined within the context of a recently developed mean-field model for the CIM. Additionally, we discuss potential experimental implementations of our findings, which could provide insights into the underlying mechanisms of phase transitions in various materials.\n\n**Introduction:** The phenomenon of phase coexistence has been extensively investigated both theoretically and experimentally. It occurs when distinct thermodynamic states exist in equilibrium or when metastable states are present simultaneously. A classic example is the coexistence of ice Ih and liquid water below 0°C. Recent research has focused on how interfaces separating different phases evolve under external influences such as thermal fluctuations, magnetic fields, and mechanical strain. These investigations have primarily targeted various materials, including ferroelectrics, ferromagnets, superconductors, colloids, granular materials, glasses, foams, and biological systems. For instance, studies have demonstrated that the dynamics of domain walls in magnetic systems are significantly influenced by whether they are pinned or free to move. Similarly, the response of glassy and jammed systems to shear forces is heavily dependent on their preparation history. Despite several experimental and theoretical efforts, the effects of quenched disorder on interface properties remain underexplored. Recently, the evolution of interfaces has garnered renewed interest due to the identification of novel transition types in spatially extended systems.",
        "ori-fast-z-score": -1.12089707663561,
        "water-fast-z-score": 6.943355894868313,
        "rewrite-fast-z-score": 0.08137884587711594
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Smooth and Starburst Tidal Tails in the GEMS and GOODS Fields .\nAbstract:\nWe present new results on tidal tails around interacting galaxies using deep optical images taken with HST/ACS for two fields, GEMS (Galaxy Evolution from Morphology and SEDs) and GOODS (Great Observatories Origins Deep Survey). We find that about half of all interacting pairs show clear signs of tidal features such as bridges or tails. The fraction is higher than previous studies because we use much deeper data to detect faint structures. In addition, we identify star forming regions along the tails by measuring their UV-optical colors. About one third of our sample shows strong star formation activity at least 1 kpc away from the galaxy centers. These star bursting regions are found only in systems where both components have similar masses within a factor of 3. This suggests that these interactions trigger star formation over large scales. Finally, we compare the properties of tidal tails between different interaction stages.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Smooth and Starburst Tidal Tails in the GEMS and GOODS Fields . Abstract : We report new data on wave tails around interacting galaxies using deep optical images obtained with HST / ACS for two fields , GEMS ( Galaxy Evolution from Morphology and SEDs ) and GOODS ( Great Observatories Origins Deep Survey ) .We see that about half of all interacting pairs display evident indication of tidal features such as structures or fins . The percentage is higher than prior studies because we using much harder measurements to identify strange structures .In addition , we identify galaxy producing regions along the tails by monitoring their UV - optical colors . About one third of our sample shows intense star formation activity at least 1 kpc apart from the galaxy regions .These star bursting areas are found only in systems where both components have equal masses within a factor of 3 . This implies that these interactions cause star formation over large scales .Finally , we compare the properties of tidal tails between various interaction stages .",
        "rewrite_text": "We present new findings on the tidal tails surrounding interacting galaxies, derived from deep optical imaging conducted with the Hubble Space Telescope's Advanced Camera for Surveys (HST/ACS) in two significant fields: GEMS (Galaxy Evolution from Morphology and SEDs) and GOODS (Great Observatories Origins Deep Survey). Our analysis reveals that approximately 50% of the interacting galaxy pairs exhibit clear signs of tidal features, such as elongated structures or fins. This proportion surpasses that reported in previous studies, which can be attributed to our implementation of more rigorous criteria for identifying these unusual formations. Furthermore, we investigate the regions of star formation along the tidal tails by examining their ultraviolet-optical color profiles. Notably, around one-third of the galaxies in our sample demonstrate vigorous star formation activity occurring at least 1 kiloparsec away from their central galactic regions. These starburst zones are exclusively observed in systems where the interacting galaxies possess comparable masses, within a factor of three. This finding suggests that gravitational interactions between galaxies can trigger star formation across extensive areas. Lastly, we conduct a comparative analysis of the characteristics of tidal tails across different stages of galactic interactions, providing insights into the evolutionary processes at play in these dynamic systems. Our results contribute to a deeper understanding of the role of tidal interactions in galaxy evolution and the mechanisms driving star formation in the universe.",
        "ori-fast-z-score": -0.5423261445466404,
        "water-fast-z-score": 6.539886462510548,
        "rewrite-fast-z-score": 1.873171623163388
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Interacting Tachyon: generic cosmological evolution for a tachyon and a scalar field .\nAbstract:\nWe study the dynamics of an interacting tachyonic field in a spatially flat Friedmann-Robertson-Walker universe with zero spatial curvature, using numerical methods to solve the equations of motion.  We find that there are two distinct phases during which the energy density evolves differently; one phase is dominated by the kinetic energy of the fields while the other is dominated by their potential energies.  The transition between these phases occurs when the Hubble parameter becomes comparable to the mass scale associated with the interaction term.  During this transition period we observe oscillatory behavior in both the Hubble parameter and the energy densities of each individual field.  In addition, we find that the total energy density decreases more slowly than it would if only one component were present (i.e., either a single tachyonic or a single scalar field).  Finally, we show how our results can be used to construct viable inflationary models. We consider a model consisting of a tachyonic field coupled to another scalar field through a quartic self-interaction term.  Using numerical techniques, we examine the time-evolution of various quantities such as the Hubble parameter, the energy densities of each field individually, and the total energy density.  Our analysis reveals several interesting features including the presence of a transition region where the Hubble parameter becomes comparable...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Interacting Tachyon : generic cosmological evolution for a tachyon and a scalar field . Abstract : We research the dynamics of an interacting tachyonic field in a spatially straight Friedmann - Robertson - Walker universe with zero spatial curvature , using numerical methods to correct the equations of movement .We see that there are two different stages during which the power concentration evolves differently ; one phase is dominated by the kinetic power of the fields while the other is dominated by their potential energies . The shift between these stages occurs when the Hubble parameter becomes comparable to the mass scale identified with the interaction term .During this shift period we study oscillatory behavior in both the Hubble parameter and the power densities of each individual field . In addition , we find that the total energy density decreases more slowly than it would if only one element were present ( i . e . , either a single tachyonic or a single scalar field ) .Finally , we prove how our findings can be used to build feasible inflationary theories . We consider a theory consisting of a tachyonic field coupled to another scalar field through a quartic self - interaction term .Using numerical techniques , we investigate the period - evolve of several quantities such as the Hubble parameter , the power densities of each field individually , and the total energy density . Our study reveals numerous interesting features including the presence of a transition region where the Hubble parameter becomes identical . . .",
        "rewrite_text": "**Title**: Interacting Tachyon: Generic Cosmological Evolution for a Tachyon and a Scalar Field\n\n**Abstract**: In this study, we investigate the dynamics of an interacting tachyonic field within a spatially flat Friedmann-Robertson-Walker (FRW) universe characterized by zero spatial curvature. Employing numerical methods, we refine the equations of motion to explore the evolution of the system. Our analysis reveals two distinct phases in the evolution of energy concentration: an initial phase dominated by the kinetic energy of the fields, followed by a subsequent phase where potential energies take precedence. The transition between these phases occurs when the Hubble parameter approaches the mass scale associated with the interaction term. During this transitional period, we observe oscillatory behavior in both the Hubble parameter and the energy densities of the individual fields. Notably, we find that the total energy density decreases at a slower rate compared to scenarios involving a single field, whether it be a tachyonic or scalar field alone. \n\nFurthermore, we demonstrate how our results can contribute to the development of viable inflationary models. Specifically, we consider a theoretical framework that includes a tachyonic field coupled to another scalar field via a quartic self-interaction term. Through numerical simulations, we analyze the time evolution of various quantities, including the Hubble parameter, the energy densities of each field, and the overall energy density of the system. Our findings unveil several intriguing features, including a transition region where the Hubble parameter stabilizes. This research not only enhances our understanding of the interplay between tachyonic and scalar fields in cosmological contexts but also provides insights that could be instrumental in constructing effective inflationary scenarios.",
        "ori-fast-z-score": -0.9534625892455924,
        "water-fast-z-score": 5.2915026221291805,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Mid-infrared laser light nulling experiment using single-mode conductive waveguides .\nAbstract:\nWe report on the first mid-infrared (MIR) laser light nulling experiment with an integrated optics approach based on silicon-on-insulator technology and suspended core fibers. The experimental setup consists of two fiber-coupled MIR lasers, one for each arm of a Michelson interferometer. A pair of suspended-core fibers is used to couple the output beams into a 2x2 multimode interference coupler (MMI). In order to achieve high extinction ratio in this MMI-based beam splitter we use a novel design that incorporates a tapered region at its input port. We demonstrate a maximum extinction ratio of more than 40 dB between both arms of our interferometer by adjusting the phase difference between them. This value corresponds to a minimum intensity contrast of 1e-10 which is sufficient for most applications such as space telescopes or free-space optical communication systems. Finally, we show how the performance can be further improved by optimizing the coupling efficiency through the suspended-core fibers.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Mid - infrared laser light nulling experiment involving single - mode conductive waveguides . Abstract : We report on the first mid - infrared ( MIR ) beam beam nulling experiment with an unified optics methodology relying on silicon - on - insulator technique and suspended core materials .The experimental setup uses of two fiber - coupled MIR lasers , one for each arm of a Michelson interferometer . A couple of suspended - core strands is utilized to couple the output beams into a 2x2 multimode interference coupler ( MMI ) .In order to achieve high extinction ratio in this MMI - based beam splitter we using a novel concept that incorporates a tapered area at its input port . We produce a maximum extinction ratio of more than 40 dB between both arms of our interferometer by adjusting the phase change between them .This value corresponds to a threshold intensity contrast of 1e - 10 which is adequate for most use such as space telescopes or free - space optical transmission systems . Finally , we show how the performance can be further increased by optimizing the coupling performance through the suspended - core strands .",
        "rewrite_text": "We present the inaugural mid-infrared (MIR) beam nulling experiment utilizing a unified optics approach that integrates silicon-on-insulator technology with suspended core materials. The experimental configuration employs two fiber-coupled MIR lasers, each designated for one arm of a Michelson interferometer. To facilitate the coupling of the output beams, we implement a pair of suspended-core fibers that direct the light into a 2x2 multimode interference coupler (MMI). To achieve a high extinction ratio in this MMI-based beam splitter, we introduce an innovative design that features a tapered region at the input port. By fine-tuning the phase difference between the two arms of the interferometer, we successfully attain a maximum extinction ratio exceeding 40 dB. This remarkable performance corresponds to a threshold intensity contrast of 1e-10, which is suitable for various applications, including space telescopes and free-space optical communication systems. Furthermore, we discuss strategies for enhancing performance by optimizing the coupling efficiency through the suspended-core fibers. This work not only demonstrates the feasibility of MIR beam nulling using advanced optical techniques but also opens avenues for future research in high-precision optical systems.",
        "ori-fast-z-score": -0.9649012813540153,
        "water-fast-z-score": 4.610083899802517,
        "rewrite-fast-z-score": -0.30460384954008574
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Tree-Level Stability Without Spacetime Fermions: Novel Examples in String Theory .\nAbstract:\nWe present novel examples of tree-level stable non-BPS D-branes in string theory, which are not associated with spacetime fermion zero modes and thus do not require the presence of orientifolds or other sources for tadpole cancellation. We show that these brane configurations can be constructed by wrapping unstable D-branes on supersymmetric cycles in Calabi-Yau threefolds. The resulting BPS states preserve half of the original supersymmetry but carry no net charge under any gauge group factor. These results provide new insights into the structure of moduli spaces of vacua in string theory. Introduction: In recent years there has been considerable interest in studying non-BPS D-brane (NBD) configurations in type II string theories  1  . NBDs have attracted attention because they may play an important role in understanding various phenomena such as tachyon condensation  2  , open-string pair production  3  , and black hole entropy  4  .\nIn this work we will focus our attention on NBDs whose stability is due to worldsheet instanton effects  5  -  8  rather than spacetime fermion zero-modes  9  . Such NBDs were first studied in  10  where it was shown that certain wrapped D3-branes could become stable at one-loop order without requiring the presence of orientifold planes  11  . Subsequently, several authors  12  -  16  have considered similar constructions involving different types of D-branes and compactifications. However, all of these works required some form of tadpole cancellation  17  so that the total RR-charge carried by the configuration vanishes. Tadpole cancellation conditions place strong constraints on the allowed values of fluxes and charges in the background geometry  18  . It would therefore be interesting if one could find examples of stable NBDs which did not require the presence of additional sources for tadpole cancellations.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Tree - Level Stability Without Spacetime Fermions : Novel Examples in String Theory . Abstract : We introduce novel instances of forest - level stable non - BPS D - branes in string theory , which are not associated with spacetime fermion zero modes and therefore do not require the presence of orientifolds or other sources for tadpole cancellation .We see that these brane configurations can be built by wrapping unstable D - branes on supersymmetric cycles in Calabi - Yau threefolds . The resulting BPS states preserve half of the original supersymmetry but hold no net charge under any gauge group factor .These data provide novel knowledge into the formation of moduli spaces of vacua in string theory . Introduction : In recent years there has been substantial interest in investigating non - BPS D - brane ( NBD ) arrangements in type II string theories 1 .NBDs have garnered attention because they may play an important role in understanding various phenomena such as tachyon condensation 2 , open - string pair production 3 , and dark hole entropy 4 . In this research we will focus our focus on NBDs whose stability is due to worldsheet instanton effects 5 - 8 instead than spacetime fermion zero - modes 9 .Such NBDs were first investigated in 10 where it was shown that particular tangled D3 - branes might remain stable at one - loop order without need the presence of orientifold planes 11 . Subsequently , various scientists 12 - 16 have suggested different constructions concerning diverse kinds of D - branes and compactifications .However , all of these works involved some kind of tadpole cancellation 17 so that the total RR - charge transferred by the configuration vanishes . Tadpole cancellation conditions place powerful restrictions on the allowed values of fluxes and charges in the background geometry 18 .It would therefore be attractive if one could discover examples of stable NBDs which did not require the presence of new sources for tadpole cancellations .",
        "rewrite_text": "We present new examples of tree-level stable non-BPS D-branes in string theory that are not linked to spacetime fermion zero modes, eliminating the need for orientifolds or other mechanisms for tadpole cancellation. These configurations are constructed by wrapping unstable D-branes around supersymmetric cycles within Calabi-Yau threefolds. The resulting BPS states maintain half of the original supersymmetry while exhibiting no net charge under any gauge group factor. This research contributes valuable insights into the formation of moduli spaces of vacua in string theory.\n\nIn recent years, there has been a growing interest in exploring non-BPS D-brane (NBD) configurations within type II string theories. NBDs are significant because they may provide crucial understanding of various phenomena, including tachyon condensation, open-string pair production, and dark hole entropy. Our study specifically examines NBDs whose stability arises from worldsheet instanton effects rather than spacetime fermion zero modes. Previous investigations have highlighted that certain tangled D3-branes can remain stable at one-loop order without the necessity of orientifold planes. Following this, various researchers have proposed different constructions involving diverse types of D-branes and compactifications. However, these prior works have consistently involved some form of tadpole cancellation, ensuring that the total RR-charge associated with the configuration is zero. The conditions for tadpole cancellation impose stringent limitations on the permissible values of fluxes and charges in the background geometry. Therefore, identifying examples of stable NBDs that do not require additional sources for tadpole cancellation would be a significant advancement in the field. Our findings pave the way for further exploration of stable D-brane configurations and their implications for string theory.",
        "ori-fast-z-score": -1.5549631660464482,
        "water-fast-z-score": 5.895067838245651,
        "rewrite-fast-z-score": 1.5787044347526527
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Semi-empirical pressure-volume-temperature equation of state; MgSiO3 perovskite is an example .\nAbstract:\nMgSiO3 perovskite has been studied by first-principles calculations and the semi-empirical P-V-T equation of state (EOS). The calculated results show that MgSiO3 perovskites have high bulk modulus, low compressibility, large volume expansion coefficient at high temperature, and small thermal expansivity in comparison with other oxides. These properties are very important for understanding geophysics phenomena such as earthquakes and volcanic activities.  In addition to these physical properties, we also investigated the electronic structure of MgSiO3 perovskitie using density functional theory (DFT) method. We found that there exists a charge transfer between Si4+ ions and O2-ions which leads to the formation of Si-O bonds. This charge transfer causes the change of bond length and bond angle of Si-O-Si network. Finally, we discussed the effect of oxygen vacancy on the structural stability of MgSiO3 perovksite.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Semi - empirical pressure - volume - temperature equation of state ; MgSiO3 perovskite is an instance . Abstract : MgSiO3 perovskite has been studied by first - principles measurements and the semi - empirical P - V - T equation of state ( EOS ) .The estimated data reveal that MgSiO3 perovskites have high bulk modulus , low compressibility , large volume expansion ratio at high heat , and tiny thermal expansivity in comparison with other oxides . These properties are very important for studying geophysics phenomena such as earthquakes and volcanic movements .In addition to these physical properties , we also examined the electronic structure of MgSiO3 perovskitie using density functional theory ( DFT ) method . We showed that there exists a charge transfer between Si4 + ions and O2 - ions which results to the formation of Si - O bonds .This charge transfer causes the shift of bond length and bond angle of Si - O - Si chain . Finally , we explored the impact of oxygen vacancy on the structural structure of MgSiO3 perovksite .",
        "rewrite_text": "Title: Semi-Empirical Pressure-Volume-Temperature Equation of State: A Case Study of MgSiO3 Perovskite\n\nAbstract: This study investigates the properties of MgSiO3 perovskite through first-principles measurements and the development of a semi-empirical pressure-volume-temperature (P-V-T) equation of state (EOS). Our findings indicate that MgSiO3 perovskite exhibits a notably high bulk modulus and low compressibility, alongside a significant volume expansion ratio under elevated temperatures, while demonstrating minimal thermal expansivity compared to other oxide materials. These characteristics are crucial for understanding geophysical phenomena, including seismic activities and volcanic behavior. Furthermore, we analyzed the electronic structure of MgSiO3 perovskite using density functional theory (DFT). Our results reveal a charge transfer mechanism between Si4+ and O2- ions, which facilitates the formation of Si-O bonds. This charge transfer is responsible for alterations in both the bond length and bond angle within the Si-O-Si chain. Additionally, we investigated the effects of oxygen vacancies on the structural integrity of MgSiO3 perovskite, providing insights into how such vacancies influence the material's properties. Overall, this research enhances our comprehension of the physical and electronic characteristics of MgSiO3 perovskite, contributing valuable knowledge to the field of geophysics and materials science.",
        "ori-fast-z-score": -0.3333333333333333,
        "water-fast-z-score": 3.533808834395089,
        "rewrite-fast-z-score": 0.52999894000318
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dynamics of the giant planets of the solar system in the gaseous proto-planetary disk and relationship to the current orbital architecture .\nAbstract:\nWe present an analysis of the dynamics of the four giant planets of our Solar System during their formation, using numerical simulations with N-body codes that include hydrodynamical effects (gas drag). We show how these interactions can explain some features observed today on the orbits of Jupiter s Trojans asteroids. In particular we find that:  1) The eccentricities of Jupiter s Trojan asteroids are excited by close encounters between Jupiter and Saturn; 2) The inclination distribution is affected by the presence of gas; 3) The semi-major axes of Jupiter s Trojans are shifted towards smaller values due to the effect of gas friction. These results suggest that the dynamical history of Jupiter s Trojans may be related to the evolution of the protoplanetary nebula surrounding the Sun. This work was supported by CONACyT grant No. 164713. We thank J. Laskar for providing us his code used to calculate the secular frequencies of the planetary systems. \n \n Keywords: Giant planet migration, Gas drag",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dynamics of the giant worlds of the planetary system in the gaseous proto - planetary disk and relationship to the present orbital design . Abstract : We present an assessment of the dynamics of the four giant worlds of our Solar System during their formed , using numerical simulations with N - body rules that include hydrodynamical influence ( vapor drag ) .We see how these interactions can describe some features detected nowadays on the orbits of Jupiter s Trojans asteroids . In particular we find that : 1 ) The eccentricities of Jupiter s Trojan asteroids are excited by distant encounters between Jupiter and Saturn ; 2 ) The inclination distribution is affected by the presence of gas ; 3 ) The semi - principal axes of Jupiter s Trojans are shifted towards lesser values due to the impact of gas drag .These data suggest that the dynamical history of Jupiter s Trojans might be connected to the evolution of the protoplanetary nebula surrounding the Sun . This project was supported by CONACyT grant No .164713 . We praise J . Laskar for providing us his code used to estimate the secular frequencies of the planetary components .Keywords : Giant world migration , Gas drag",
        "rewrite_text": "**Title:** Dynamics of the Giant Worlds in the Planetary System within the Gaseous Protoplanetary Disk and Its Relation to Current Orbital Configurations\n\n**Abstract:** This study explores the dynamic evolution of the four giant planets in our Solar System during their formation, utilizing numerical simulations based on N-body dynamics that incorporate hydrodynamic effects, specifically vapor drag. Our findings reveal significant insights into the orbital characteristics of Jupiter's Trojan asteroids, suggesting that their current configurations are influenced by historical interactions within the protoplanetary disk. Notably, we identify three key outcomes from our simulations: first, the eccentricities of Jupiter's Trojan asteroids are notably excited by gravitational encounters with Saturn; second, the distribution of inclinations among these asteroids is significantly shaped by the presence of gas in the protoplanetary environment; and third, the semi-major axes of Jupiter's Trojans are observed to shift towards smaller values as a direct consequence of gas drag effects. These results imply a strong connection between the dynamical evolution of Jupiter's Trojans and the historical development of the protoplanetary nebula that surrounded the early Sun. This research was funded by CONACyT grant No. 164713, and we extend our gratitude to J. Laskar for providing the computational code utilized to calculate the secular frequencies of the planetary bodies involved. \n\n**Keywords:** Migration of giant planets, Gas drag effects, Jupiter's Trojan asteroids, Protoplanetary disk dynamics.",
        "ori-fast-z-score": -1.4269353798659745,
        "water-fast-z-score": 5.666666666666667,
        "rewrite-fast-z-score": -0.7337993857053429
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Quantum simulation of interacting high-dimensional systems: the influence of noise .\nAbstract:\nWe study the effect of decoherence on quantum simulations of strongly correlated systems in one dimension, focusing on the case where the number of particles is large and the Hilbert space grows exponentially with this number. We show that for certain classes of Hamiltonians it is possible to perform efficient numerical calculations using tensor network methods even when the effects of decoherence are taken into account. In particular we consider two different types of errors which can be simulated by adding extra terms to the Hamiltonian describing the evolution of the system. The first type of error corresponds to local fluctuations in the energy levels of individual sites while the second type describes global fluctuations affecting all sites equally. For both cases we find that the resulting effective dynamics can still be efficiently described within the same class of tensor networks as before. This allows us to calculate the time dependence of observables such as correlation functions or entanglement entropy accurately despite the presence of noise.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Quantum simulation of interacting large - dimensional systems : the impact of noise . Abstract : We research the impact of decoherence on particle simulations of highly correlated systems in one dimension , concentrating on the case where the number of particles is huge and the Hilbert space grows exponentially with this amount .We see that for particular groups of Hamiltonians it is easy to conduct efficient numerical analyses using vector network principles even when the effects of decoherence are took into consideration . In particular we treat two different kinds of errors which can be simulated by added extra terms to the Hamiltonian describing the evolution of the process .The first sort of error corresponds to regional fluctuations in the electricity rates of individual sites while the second kind represents global fluctuations involving all locations equally . For both cases we find that the resulting effective dynamics can still be easily characterized within the same category of tensor systems as before .This enables us to estimate the time dependence of observables such as variance distributions or entanglement entropy accurately despite the presence of noise .",
        "rewrite_text": "Title: Quantum Simulation of Interacting Large-Dimensional Systems: The Impact of Noise\n\nAbstract: In this study, we investigate the effects of decoherence on particle simulations of highly correlated one-dimensional systems, particularly focusing on scenarios where the number of particles is significantly large, leading to an exponential growth of the Hilbert space. Our findings reveal that for specific classes of Hamiltonians, it is feasible to perform efficient numerical analyses by employing vector network principles, even when accounting for the influence of decoherence. We specifically examine two distinct types of errors that can be incorporated into the Hamiltonian to model the evolution of the system. The first type of error pertains to localized fluctuations in the energy levels at individual sites, while the second type encompasses global fluctuations that affect all sites uniformly. Notably, we demonstrate that the effective dynamics resulting from both error types can still be characterized within the same framework of tensor systems as previously established. This characteristic allows for accurate estimations of the time evolution of various observables, including variance distributions and entanglement entropy, despite the presence of noise. Our results contribute to a deeper understanding of how decoherence impacts the dynamics of large-dimensional quantum systems and provide a foundation for future research in quantum simulation and error mitigation strategies.",
        "ori-fast-z-score": -2.799769575772148,
        "water-fast-z-score": 4.950821982042208,
        "rewrite-fast-z-score": 1.507556722888818
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Probing polarization states of primordial gravitational waves with CMB anisotropies .\nAbstract:\nWe study the effects on cosmic microwave background (CMB) temperature and polarization anisotropies induced by tensor perturbations in the early universe, which are generated through inflationary processes or other mechanisms. We show that these tensor perturbations can be probed via their imprints on the Stokes parameters Q and U . In particular, we find that the correlation between the two Stokes parameters is proportional to the amplitude of the tensor perturbation at large scales. This effect may provide an important test for models of inflation as well as alternative scenarios such as topological defects. \n \n The recent detection of B-mode polarizations in the CMB  1  has opened up new opportunities to probe physics beyond standard cosmology  2  , including primordial gravitational waves  3  produced during inflation  4  . However, it remains unclear whether this signal arises primarily due to scalar fluctuations  5  or primordial gravitational waves  6  .\n \n \n Tensor modes also induce non-Gaussianities  7, 8  in the primordial curvature perturbation ζ  9  . These non-Gaussianities have been studied extensively  10 - 12  using different approaches  13 - 15  . It was shown  16  that the bispectrum of the primordial curvature perturbation contains information about both the power spectrum Pζ(k) and the spectral index ns  17  of the tensor mode. Recently, Ref.  18  showed that the trispectrum of the primordial curvature perturbations contains additional information about the tensor-to-scalar ratio r = 16PT /PS where PT denotes the power spectrum of the tensor mode and PS denotes the power spectrum of its corresponding scalar counterpart.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Probing polarization states of primordial gravitational waves with CMB anisotropies . Abstract : We research the effects on cosmic microwave background ( CMB ) temperature and polarization anisotropies induced by tensor perturbations in the early universe , which are produced through inflationary processes or other mechanisms .We see that these vector perturbations can be probed via their imprints on the Stokes variables Q and U . In particular , we find that the relationship between the two Stokes variables is proportional to the frequency of the tensor perturbation at large scales .This phenomenon might give an important test for models of inflation as well as additional situations such as topological errors . The recent discovery of B - mode polarizations in the CMB 1 has opened up new opportunities to probe mechanics beyond standard cosmology 2 , notably primordial gravitational waves 3 created during inflation 4 .However , it remains unsure whether this signal exists largely owing to scalar fluctuations 5 or primordial gravitational waves 6 . Tensor modes also induce non - Gaussianities 7 , 8 in the primordial curvature perturbation ζ 9 .These non - Gaussianities have been studied thoroughly 10 - 12 using separate approaches 13 - 15 . It was shown 16 that the bispectrum of the primordial curvature perturbation contains information about both the power spectrum Pζ ( k ) and the spectral index ns 17 of the tensor mode .Recently , Ref . 18 demonstrated that the trispectrum of the primordial curvature perturbations contains additional information about the tensor - to - scalar ratio r = 16PT / PS where PT denotes the power spectrum of the tensor mode and PS refers the power spectrum of its equivalent scalar counterpart .",
        "rewrite_text": "In this study, we investigate the influence of tensor perturbations on the anisotropies of the cosmic microwave background (CMB) temperature and polarization, which arise from early universe phenomena such as inflation. These tensor perturbations can be effectively analyzed through their effects on the Stokes parameters Q and U. Our findings reveal a significant correlation between these two Stokes variables, which is directly proportional to the frequency of the tensor perturbation at large scales. This relationship presents a valuable opportunity to test various inflationary models and explore other scenarios, including the presence of topological defects.\n\nThe recent detection of B-mode polarization in the CMB has opened new avenues for investigating physics beyond the standard cosmological model, particularly concerning primordial gravitational waves generated during the inflationary epoch. However, the origin of these signals remains ambiguous, as it is unclear whether they are predominantly driven by scalar fluctuations or by primordial gravitational waves. Additionally, tensor modes contribute to non-Gaussianities in the primordial curvature perturbation ζ, which have been extensively analyzed through various methodologies.\n\nPrevious research has established that the bispectrum of the primordial curvature perturbation encodes crucial information regarding both the power spectrum Pζ(k) and the spectral index ns of the tensor modes. Furthermore, recent work has shown that the trispectrum of these primordial curvature perturbations provides further insights into the tensor-to-scalar ratio r = 16PT/PS, where PT represents the power spectrum of the tensor modes and PS denotes that of the corresponding scalar modes. This comprehensive analysis aims to deepen our understanding of the interplay between gravitational waves and the CMB, potentially shedding light on the fundamental processes that shaped the early universe.",
        "ori-fast-z-score": -0.6831300510639733,
        "water-fast-z-score": 4.884914054627872,
        "rewrite-fast-z-score": 0.4662524041201569
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Kinetic Sunyaev-Zel dovich Effect Due to the Electrons of Our Galaxy .\nAbstract:\nWe present an analysis of the kinetic Sunyaev-Zeldovich effect (kSZ) due to electrons in our galaxy, using data obtained with the Atacama Cosmology Telescope and the South Pole Telescope. We use two different methods for estimating the kSZ signal; one is based on cross-correlating maps of CMB temperature anisotropy at 150 GHz and 3000 GHz, while the other uses the power spectrum of the CMB temperature fluctuations at 150 GHz. The results are consistent between these two approaches within their respective uncertainties. We find that the amplitude of this signal agrees well with theoretical predictions when we assume a Navarro-Frenk-White profile for dark matter density distribution around galaxies. This measurement provides new constraints on cosmological parameters such as the Hubble constant H0 = 73 +/- 4 km s-1 Mpc-1 , the total mass density parameter Omega_m = 0.27 +/- 0.03 , and the equation-of-state w = -1.06 +/- 0.11 .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Kinetic Sunyaev - Zel dovich Effect Due to the Electrons of Our Galaxy . Abstract : We present an assessment of the kinetic Sunyaev - Zeldovich effect ( kSZ ) related to electrons in our universe , using data acquired with the Atacama Cosmology Telescope and the South Pole Telescope .We use two different methods for estimating the kSZ signal ; one is based on cross - correlating mapping of CMB heat anisotropy at 150 GHz and 3000 GHz , while the other uses the power spectrum of the CMB heat fluctuations at 150 GHz . The results are compatible between these two methodology within their different uncertainties .We see that the amplitude of this signal agrees well with theoretical expectations when we assume a Navarro - Frenk - White model for heavy material concentration distribution around galaxies . This measurement gives additional constraints on cosmological factors such as the Hubble constant H0 = 73 + / - 4 km s - 1 Mpc - 1 , the total mass density variable Omega _ m = 0 . 27 + / - 0 . 03 , and the equation - of - state w = - 1 . 06 + / - 0 . 11 .",
        "rewrite_text": "In this study, we evaluate the kinetic Sunyaev-Zeldovich (kSZ) effect attributed to the electrons present in our galaxy, utilizing observational data from the Atacama Cosmology Telescope and the South Pole Telescope. Our analysis employs two distinct methodologies to estimate the kSZ signal. The first method involves cross-correlating the Cosmic Microwave Background (CMB) heat anisotropy maps at frequencies of 150 GHz and 3000 GHz. The second approach focuses on analyzing the power spectrum of CMB temperature fluctuations at 150 GHz. The findings from both methods demonstrate consistency, falling within their respective uncertainties. \n\nOur results indicate that the amplitude of the kSZ signal aligns closely with theoretical predictions, particularly when we apply the Navarro-Frenk-White (NFW) model to describe the distribution of dark matter surrounding galaxies. This measurement provides valuable constraints on several key cosmological parameters, including the Hubble constant (H0), which we estimate to be 73 ± 4 km s⁻¹ Mpc⁻¹, the total matter density parameter (Ωm) at 0.27 ± 0.03, and the equation of state parameter (w) calculated as -1.06 ± 0.11. These results not only enhance our understanding of the kSZ effect but also contribute to the broader context of cosmological studies, offering insights into the structure and evolution of the universe. The implications of our findings are significant for future research, as they refine existing models and provide a clearer picture of the interplay between cosmic microwave background radiation and the matter content of the universe.",
        "ori-fast-z-score": -0.3841106397986879,
        "water-fast-z-score": 4.826004826007239,
        "rewrite-fast-z-score": 0.7844645405527362
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Mean and Scatter of the Velocity Dispersion-Optical Richness Relation for maxBCG Galaxy Clusters .\nAbstract:\nWe present new measurements of the mean and scatter in the velocity dispersion-optical richness relation (VRR) for galaxy clusters selected by their red-sequence galaxies using data from the Sloan Digital Sky Survey Data Release 7 (SDSS DR7). We use two different methods to measure cluster velocities, one based on spectroscopic observations of member galaxies and another that uses photometric redshift estimates of all galaxies within a fixed aperture centered on each cluster s brightest cluster galaxy (BGG).\nThe results are consistent with previous studies at low-redshift but show significant differences when compared to recent work done at higher redshifts. The discrepancies between our results and those presented in earlier works may be due to systematic effects associated with the measurement techniques used or could indicate evolution in the VRR over time. In either case, these results highlight the need for further investigation into this relationship as well as other scaling relations involving galaxy clusters. \n \n Keywords: galaxy cluster, optical richness",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Mean and Scatter of the Velocity Dispersion - Optical Richness Relation for maxBCG Galaxy Clusters . Abstract : We report new measurements of the mean and scatter in the velocity dispersion - optical richness relation ( VRR ) for galaxy galaxies designated by their red - sequence galaxies using data from the Sloan Digital Sky Survey Data Release 7 ( SDSS DR7 ) .We use two different methods to measure cluster velocities , one based on spectroscopic observations of member galaxies and another that using photometric redshift estimates of all galaxies within a fixed aperture located on each cluster s brightest cluster galaxy ( BGG ) . The results are compatible with previous research at low - redshift but display significant variations when compared to recent work done at higher redshifts .The discrepancies between our findings and those offered in earlier works might be due to systematic effects involved with the monitoring procedures used or could indicate evolution in the VRR over time . In either case , these results emphasize the necessity for further investigation into this relationship as well as other scaling relations involving star clusters .Keywords : galaxy cluster , optical richness",
        "rewrite_text": "Title: The Mean and Scatter of the Velocity Dispersion - Optical Richness Relation for maxBCG Galaxy Clusters\n\nAbstract: In this study, we present new measurements of the mean and scatter in the velocity dispersion-optical richness relation (VRR) for galaxy clusters identified by their red-sequence galaxies, utilizing data from the Sloan Digital Sky Survey Data Release 7 (SDSS DR7). Our analysis employs two distinct methodologies to determine cluster velocities: one method relies on spectroscopic observations of member galaxies, while the other utilizes photometric redshift estimates for all galaxies within a fixed aperture centered on each cluster's brightest cluster galaxy (BCG). Our findings align with previous studies conducted at low redshifts; however, they reveal notable discrepancies when compared to more recent investigations at higher redshifts. These differences may stem from systematic effects associated with the observational techniques employed or could suggest an evolutionary change in the VRR over cosmic time. Regardless of the underlying cause, our results underscore the importance of further exploration into the VRR and other scaling relations pertinent to galaxy clusters. This research contributes to a deeper understanding of the dynamics and formation of galaxy clusters, highlighting the need for continued scrutiny of these relationships in the context of cosmic evolution. \n\nKeywords: galaxy cluster, optical richness",
        "ori-fast-z-score": 1.4269353798659745,
        "water-fast-z-score": 6.32831881684378,
        "rewrite-fast-z-score": 1.0660035817780522
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A multi-wavelength study of z = 3.15 Lyman-alpha emitters in the GOODS South Field .\nAbstract:\nWe have carried out an extensive spectroscopic survey for high redshift (z > 2) galaxies using the VLT/VIMOS instrument on the ESO Very Large Telescope, targeting sources selected by their strong rest-frame UV emission lines and photometric redshifts. We present here our results obtained with this sample at wavelengths ranging from radio to X-ray. The main goal is to investigate how star formation proceeds in these distant objects through detailed studies of their physical properties such as stellar masses, ages, metallicities or dust content. In particular we focus on two samples of Lyman-alpha emitting galaxies which are known to be very young systems undergoing intense bursts of star formation. Our analysis shows that they exhibit large amounts of cold gas but also significant quantities of dust. This suggests that the bulk of the observed infrared luminosity may not come directly from newly formed stars but rather from reprocessed light emitted by hot dust heated by older populations and/or AGN activity.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A multi - wavelength observation of z = 3 . 15 Lyman - alpha emitters in the GOODS South Field . Abstract : We have carried out an extensive spectroscopic study for high redshift ( z > 2 ) clusters using the VLT / VIMOS instrument on the ESO Very Large Telescope , targeting sources chosen by their weak rest - frame UV absorption patterns and photometric redshifts .We present here our findings obtained with this specimen at wavelengths ranging from radio to X - ray . The main goal is to examine how star formation operates in these distant objects through complete studies of their physical properties such as stellar masses , ages , metallicities or dust content .In particular we focus on two specimens of Lyman - alpha emitting galaxies which are known to be very young structures experiencing intense pulses of galaxy formation . Our study shows that they show large quantities of cold energy but also notable quantities of dust .This implies that the majority of the seen infrared luminosity might not come directly from newly established stars but rather from reprocessed light emitted by hot dust cooled by existing populations and / or AGN activity .",
        "rewrite_text": "Title: A Multi-Wavelength Observation of Lyman-alpha Emitters at z = 3.15 in the GOODS South Field\n\nAbstract: In this study, we present a comprehensive spectroscopic analysis of high-redshift clusters (z > 2) utilizing the VLT/VIMOS instrument on the European Southern Observatory's Very Large Telescope. Our research focuses on sources selected based on their faint rest-frame ultraviolet absorption features and photometric redshifts. We report our findings across a broad spectrum of wavelengths, from radio to X-ray, with the primary objective of investigating the mechanisms of star formation in these distant galaxies. By conducting detailed assessments of their physical characteristics—including stellar masses, ages, metallicities, and dust content—we aim to deepen our understanding of these early cosmic structures.\n\nParticularly, we concentrate on two specific Lyman-alpha emitting galaxies, which are recognized as nascent formations undergoing vigorous episodes of star formation. Our analysis reveals that these galaxies possess significant amounts of cold gas and dust, suggesting that a substantial portion of their observed infrared luminosity may not originate solely from newly formed stars. Instead, it appears to be largely attributed to reprocessed light from hot dust, which is cooled by existing stellar populations and/or active galactic nucleus (AGN) activity. This finding highlights the complex interplay between star formation and dust dynamics in the early universe, providing valuable insights into the evolutionary processes of galaxies at high redshifts. Overall, our multi-wavelength approach allows for a more nuanced understanding of the physical conditions and star formation histories of Lyman-alpha emitters, contributing to the broader field of astrophysics and our comprehension of galaxy formation in the early cosmos.",
        "ori-fast-z-score": -0.41256849850351734,
        "water-fast-z-score": 5.9196002117260145,
        "rewrite-fast-z-score": 2.9819972656438996
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  (2+1)-Einstein spacetimes of finite type .\nAbstract:\nWe study the global structure and causal properties of (2 + 1)-dimensional Einstein spaces with nonnegative cosmological constant, which are locally conformally flat but not globally conformal to Minkowski space-time.  We show that such spaces can be classified into three types according to their global structures as follows:  Type I : The universal covering is homeomorphic to R × S 2 . Type II : The universal covering has infinitely many ends. Type III : The universal covering is compact. In particular,  we prove that any spacetime in Types I or II admits closed timelike curves if it contains an incomplete null geodesic segment whose endpoints have different values of the affine parameter along them.   As applications,   we give examples of complete solutions for each type by constructing explicit metrics on open subsets of the hyperbolic plane H 2 , the complex projective line CP1 , and the 3-sphere S3 respectively.    Keywords: Global structure, Causality, Closed timelike curve",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : ( 2 + 1 ) - Einstein spacetimes of finite type . Abstract : We research the global structure and causal properties of ( 2 + 1 ) - dimensional Einstein spaces with nonnegative cosmological constant , which are locally conformally flat but not nationally conformal to Minkowski space - time .We see that such objects can be categorized into three sorts based to their global structures as follows : Type I : The universal covering is homeomorphic to R × S 2 . Type II : The universal covering has infinitely many ends .Type III : The universal covering is compact . In particular , we prove that any spacetime in Types I or II admits closed timelike curves if it contains an incomplete null geodesic segment whose endpoints have different values of the affine vector along them .As applications , we give instance of complete solutions for each type by constructing explicit metrics on open subsets of the hyperbolic plane H 2 , the complex projective line CP1 , and the 3 - sphere S3 respectively . Keywords : Global structure , Causality , Closed timelike curve",
        "rewrite_text": "In this study, we investigate the global structure and causal characteristics of (2 + 1)-dimensional Einstein spacetimes that possess a nonnegative cosmological constant. These spacetimes are locally conformally flat but do not conform to Minkowski space on a global scale. Our analysis categorizes these spacetimes into three distinct types based on their global structures. Type I spacetimes have a universal covering that is homeomorphic to R × S², while Type II spacetimes feature a universal covering with infinitely many ends. Type III spacetimes, on the other hand, are characterized by a compact universal covering. A significant finding of our research is that spacetimes classified as Type I or Type II can exhibit closed timelike curves, particularly if they contain an incomplete null geodesic segment whose endpoints differ in their affine parameter values. To illustrate our findings, we provide explicit examples of complete solutions for each type by constructing specific metrics on open subsets of the hyperbolic plane H², the complex projective line CP¹, and the 3-sphere S³. This work contributes to the understanding of the intricate relationship between the global structure of (2 + 1)-dimensional spacetimes and their causal properties, highlighting the potential for closed timelike curves in certain configurations. Our results have implications for the study of spacetime geometry and the nature of causality in lower-dimensional gravitational theories. \n\nKeywords: Global structure, Causality, Closed timelike curve.",
        "ori-fast-z-score": 0.5933908290969266,
        "water-fast-z-score": 2.7295978138458623,
        "rewrite-fast-z-score": 2.3190036174568114
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Cosmogenic neutrinos as a probe of the transition from Galactic to extragalactic cosmic rays .\nAbstract:\nWe present an analysis of cosmogenic neutrino fluxes expected in IceCube for different models of cosmic ray sources and propagation, including recent results on anisotropy measurements by PAMELA and AMS-02 experiments. We show that these data can be used to discriminate between various scenarios of cosmic ray origin and propagation. In particular we find that:  - The observed anisotropies are consistent with predictions based on conventional astrophysical acceleration mechanisms only if one assumes very hard injection spectra at source (with spectral index below -2). This is inconsistent with observations of other messengers such as gamma-rays or radio waves.  - If one allows for more exotic particle physics processes like decaying dark matter particles then it becomes possible to explain both the observed anisotropies and the spectrum of cosmic rays without violating any observational constraints. However this scenario requires fine-tuning of parameters which makes it less appealing than conventional astrophysics explanations.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Cosmogenic neutrinos as a investigation of the shift from Galactic to extragalactic cosmic rays . Abstract : We report an assessment of cosmogenic neutrino fluxes anticipated in IceCube for different models of gamma ray sources and propagation , notably recent results on anisotropy observations by PAMELA and AMS - 02 experiments .We see that these information can be used to discriminate between various circumstances of cosmic ray origin and propagation . In particular we find that : - The observed anisotropies are compatible with predictions based on conventional astrophysical acceleration mechanisms only if one takes very hard injection spectra at source ( with spectral index below - 2 ) .This is conflicting with observations of other messengers such as gamma - radiation or radio beams . - If one permits for more exotic electron physics processes like decaying heavy material particles then it becomes possible to explain both the known anisotropies and the spectrum of cosmic rays without violating any observational restrictions .However this situation requires fine - tuned of values which makes it less acceptable than conventional astrophysics explanations .",
        "rewrite_text": "Title: Cosmogenic Neutrinos as a Probe of the Transition from Galactic to Extragalactic Cosmic Rays\n\nAbstract: In this study, we evaluate the expected fluxes of cosmogenic neutrinos detectable by the IceCube observatory, focusing on various models of gamma-ray sources and their propagation. Our analysis incorporates recent findings on anisotropy from the PAMELA and AMS-02 experiments, which provide critical insights into the origins and behavior of cosmic rays. We demonstrate that these anisotropy observations can effectively differentiate between different scenarios regarding the sources and propagation mechanisms of cosmic rays. Notably, our findings indicate that the observed anisotropies align with predictions derived from conventional astrophysical acceleration processes only when assuming very steep injection spectra at the source, characterized by a spectral index lower than -2. This assumption, however, presents a contradiction when compared to observations from other cosmic messengers, such as gamma rays and radio emissions. \n\nAlternatively, we explore the implications of allowing for more unconventional processes, such as the decay of heavy particles, which could account for both the observed anisotropies and the cosmic ray spectrum without conflicting with existing observational constraints. While this approach offers a potential resolution, it necessitates a precise tuning of parameters, which raises questions about its viability compared to traditional astrophysical explanations. Our results underscore the complexity of understanding cosmic ray origins and highlight the need for further investigation into both conventional and exotic models to reconcile the discrepancies observed across different cosmic messengers. This work contributes to the ongoing discourse on the nature of cosmic rays and their transition from galactic to extragalactic sources, paving the way for future research in this intriguing area of astrophysics.",
        "ori-fast-z-score": -0.6625891564490792,
        "water-fast-z-score": 6.110100926607787,
        "rewrite-fast-z-score": 1.1818181818181819
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Non-adiabatic transport in a quantum dot turnstile .\nAbstract:\nWe study the non-adiabatic current through an open quantum dot connected to two leads with different chemical potentials, which is driven by time-dependent gate voltages applied on both sides of the dot. We show that this system can be described as a  turnstile  where electrons are pumped between the leads when the driving frequencies match certain resonances. The effect is robust against disorder and dephasing. This work was supported by NSERC (Canada) and CIFAR (Canadian Institute for Advanced Research). In recent years there has been growing interest in studying electron pumps based on semiconductor nanostructures such as quantum dots or carbon nanotubes  1, 2  . These devices have potential applications ranging from metrology  3  , single-electron transistors  4  , and spintronics  5  .\nIn these systems, charge carriers are transported across the device via sequential tunneling processes  6  . A number of theoretical studies  7, 8  have shown that it is possible to achieve high efficiency in these devices even at room temperature  9  . However, most previous works focused only on adiabatic pumping  10  , i.e., the case where the frequency of the external drive is much smaller than all other relevant energy scales  11  . Recently, several experiments  12, 13  reported large currents generated by nonadiabatic pumping  14, 15  . It remains unclear whether these results can be explained within existing theories  16  .\nHere we consider a simple model of a quantum dot connected to two metallic leads  see Fig. 1(a)    17  . The dot level is modulated periodically by applying oscillating gate voltages V L/R = ±V 0 cos ωt on each side of the dot  18  . When the modulation period T ≡ 2π/ω matches one of the dwell times τ n = π / 2(E F − E n )  associated with the discrete levels E n of the isolated dot, electrons will be transferred coherently between the left and right leads  19  . Here E F denotes the Fermi energy of the leads  20  . As illustrated schematically in Figs. 1(b-c), depending on",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Non - adiabatic transport in a quantum dot turnstile . Abstract : We explore the non - adiabatic current through an open quantum dot connected to two leads with varying molecular potentials , which is generated by time - based gate voltages applied on both sides of the dot .We see that this network can be described as a turnstile where electrons are pumped between the leads when the driving frequencies match specified resonances . The phenomenon is robust against disorder and dephasing .This research was supported by NSERC ( Canada ) and CIFAR ( Canadian Institute for Advanced Research ) . In recent years there has been growing interest in investigating electron pumps based on semiconductor nanostructures such as quantum dots or carbon nanotubes 1 , 2 .These systems have potential applications ranging from metrology 3 , single - ion transistors 4 , and spintronics 5 . In these systems , charge carriers are transported across the device via sequential tunneling processes 6 .A couple of theoretical experiments 7 , 8 have shown that it is could to achieve high efficiency in these machines even at room temperature 9 . However , most prior papers focused only on adiabatic pumping 10 , i . e . , the case where the frequency of the external drive is much smaller than all other relevant energy scales 11 .Recently , various tests 12 , 13 revealed huge currents produced by nonadiabatic pumping 14 , 15 . It remains unsure whether these results can be understood within existing models 16 .Here we define a simple model of a quantum dot connected to two metallic leads see Fig . 1 ( a ) 17 .The dot level is modulated periodically by using oscillating gate voltages V L / R = ±V 0 cos ωt on each side of the dot 18 . When the modulation time T ≡ 2π / ω matches one of the dwell times σ n = π / 2 ( E F − E n ) associated with the discrete levels E n of the isolated dot , electrons will be transferred coherently between the leave and left leads 19 .Here E F denotes the Fermi energy of the leads  20  .As illustrated schematically in Figs.1(b-c), depending on",
        "rewrite_text": "**Title:** Non-Adiabatic Transport in a Quantum Dot Turnstile\n\n**Abstract:** This study investigates the non-adiabatic current generated in an open quantum dot connected to two leads with varying molecular potentials, driven by time-dependent gate voltages applied on either side of the dot. We conceptualize this system as a turnstile mechanism, where electrons are effectively pumped between the leads when the driving frequencies align with specific resonances. Notably, this phenomenon demonstrates resilience against disorder and dephasing effects. The research received support from NSERC (Canada) and CIFAR (Canadian Institute for Advanced Research). \n\nIn recent years, there has been an increasing interest in electron pumps utilizing semiconductor nanostructures, such as quantum dots and carbon nanotubes, due to their promising applications in metrology, single-ion transistors, and spintronics. In these systems, charge carriers are transported through sequential tunneling processes. Previous theoretical studies have indicated the potential for achieving high efficiency in these devices, even at room temperature. However, much of the existing literature has primarily focused on adiabatic pumping, where the frequency of the external drive is significantly lower than other relevant energy scales. \n\nRecent experimental findings have highlighted the substantial currents generated through non-adiabatic pumping, raising questions about the applicability of current theoretical models to explain these results. In this work, we propose a straightforward model of a quantum dot linked to two metallic leads. The energy levels of the dot are periodically modulated using oscillating gate voltages on each side. When the modulation period aligns with the dwell times associated with the discrete energy levels of the isolated dot, coherent electron transfer occurs between the leads. This study aims to elucidate the mechanisms underlying non-adiabatic transport and its implications for future quantum dot applications.",
        "ori-fast-z-score": 0.6620847108818944,
        "water-fast-z-score": 5.816546635685188,
        "rewrite-fast-z-score": 0.5980503604017327
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  SBF: multi-wavelength data and models .\nAbstract:\nThe Spitzer Bright Field (SBF) is an all-sky survey at 3.6, 4.5, 5.8 & 8 microns carried out by the Infrared Array Camera on board the Spitzer Space Telescope.  The SBF was designed to provide deep infrared photometry for extragalactic studies in order to complement existing optical surveys such as the Sloan Digital Sky Survey.   This dataset contains images taken with IRAC channel 1 (3.6 micron), channel 2 (4.5 micron), channel 3 (5.8 micron) and channel 4 (8 micron). Each image has been processed using the MOPEX software package developed by the Spitzer Science Center. These images are available through the NASA/IPAC Extragalactic Database (NED).  For more information about this project please see http://irsa.ipac.caltech.edu/data/SPITZER/docs/sbf/sbf-intro.html",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : SBF : multi - wavelength information and models . Abstract : The Spitzer Bright Field ( SBF ) is an all - skies study at 3 . 6 , 4 . 5 , 5 . 8 & 8 microns taken out by the Infrared Array Camera on board the Spitzer Space Telescope .The SBF was built to provide deep infrared photometry for extragalactic studies in order to complement existing imaging observations such as the Sloan Digital Sky Survey . This dataset contains images took with IRAC channel 1 ( 3 . 6 micron ) , channel 2 ( 4 . 5 micron ) , channel 3 ( 5 . 8 micron ) and channel 4 ( 8 micron ) .Each image has been processed utilizing the MOPEX software suite created by the Spitzer Science Center . These photographs are available through the NASA / IPAC Extragalactic Database ( NED ) .For more information about this project please view http : / / irsa . ipac . caltech . edu / data / SPITZER / docs / sbf / sbf - intro . html",
        "rewrite_text": "**Title: SBF: Multi-Wavelength Information and Models**\n\n**Abstract:** The Spitzer Bright Field (SBF) represents a comprehensive all-sky survey conducted at infrared wavelengths of 3.6, 4.5, 5.8, and 8 microns, utilizing the Infrared Array Camera (IRAC) aboard the Spitzer Space Telescope. This initiative was designed to deliver deep infrared photometric data that enhances extragalactic research, serving as a valuable complement to existing optical imaging datasets, such as those provided by the Sloan Digital Sky Survey. The SBF dataset comprises images captured across all four IRAC channels: channel 1 at 3.6 microns, channel 2 at 4.5 microns, channel 3 at 5.8 microns, and channel 4 at 8 microns. Each image has undergone meticulous processing using the MOPEX software suite, developed by the Spitzer Science Center, ensuring high-quality data suitable for scientific analysis. These processed images are accessible through the NASA/IPAC Extragalactic Database (NED), facilitating their use by researchers in the field. The SBF project aims to provide a rich resource for studying the infrared properties of various extragalactic objects, contributing to our understanding of galaxy formation and evolution. For further details regarding the SBF project and its data, interested parties are encouraged to visit the dedicated webpage at http://irsa.ipac.caltech.edu/data/SPITZER/docs/sbf/sbf-intro.html. This extensive dataset not only enhances the existing astronomical databases but also opens new avenues for research in infrared astronomy, enabling scientists to explore the cosmos with unprecedented depth and clarity.",
        "ori-fast-z-score": -0.30151134457776363,
        "water-fast-z-score": 4.024922359499621,
        "rewrite-fast-z-score": 0.20628424925175867
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Origin of which-way information and generalization of the Born rule .\nAbstract:\nWe present an argument for why quantum mechanics is nonlocal, based on the fact that it allows one to predict with certainty whether or not a measurement will be made in any given experiment. We show how this can lead to superluminal communication between two parties sharing entangled particles by using only local operations and classical communication (LOCC). Finally we generalize the Born rule to allow for arbitrary measurements instead of just von Neumann ones. The usual formulation of quantum mechanics assumes that all experiments are performed under ideal conditions where no errors occur during the preparation of states or the execution of measurements. However, in practice there always exist some experimental imperfections such as decoherence due to environmental noise, imprecision in state preparations, and inaccuracy in measurements. In order to account for these effects, several approaches have been proposed including stochastic Schrödinger equations  1  , open systems  2  , and generalized probabilistic theories  3  . Here we consider another approach known as Quantum Bayesianism  4  .\nIn Quantum Bayesianism, the wave function is regarded as representing our knowledge about the system rather than describing its physical properties. This means that when performing a measurement, the outcome is determined by updating our knowledge according to Bayes  theorem  5  . For example, if Alice performs a measurement of spin along the x-axis on her particle, she would update her knowledge accordingly depending on what value was obtained  6  . If Bob also measures his particle s spin along the same axis but obtains different results, then he must perform a new measurement since his knowledge has changed  7, 8  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Origin of which - way information and generalization of the Born rule . Abstract : We present an argument for why quantum mechanics is nonlocal , based on the fact that it allows one to predict with confidence whether or not a measurement will be made in any certain study .We see how this can lead to superluminal transmission between two groups sharing entangled particles by using only local operations and classical communication ( LOCC ) . Finally we generalize the Born rule to allow for arbitrary estimates instead of just von Neumann ones .The typical interpretation of quantum mechanics implies that all experiments are performed under ideal circumstances where no errors occur during the preparation of states or the execution of measurements . However , in practice there always arise some experimental imperfections such as decoherence caused to environmental noise , imprecision in state preparations , and inaccuracy in calculations .In order to explain for these phenomena , various approaches have been proposed namely stochastic Schrödinger equations 1 , open systems 2 , and generalized probabilistic models 3 . Here we choose another technique called as Quantum Bayesianism 4 .In Quantum Bayesianism , the wave function is regarded as representing our information about the process rather than presenting its physical properties . This implies that when performing a measurement , the result is chosen by updating our information according to Bayes principle 5 .For instance , if Alice performs a measurement of spin along the x - axis on her particle , she would update her knowledge appropriately depending on what value was obtained 6 . If Bob also measures his particle s spin along the same axis but obtains different results , then he must perform a new measurement since his knowledge has altered 7 , 8 .",
        "rewrite_text": "**Title:** Origin of Which-Way Information and Generalization of the Born Rule\n\n**Abstract:** In this article, we present a compelling argument supporting the nonlocal nature of quantum mechanics, emphasizing its capacity to predict with high confidence whether a measurement will occur in a given experimental context. This predictive power can facilitate superluminal communication between two parties sharing entangled particles, achieved solely through local operations and classical communication (LOCC). Furthermore, we extend the Born rule to encompass arbitrary estimates, moving beyond the traditional von Neumann framework. \n\nThe conventional interpretation of quantum mechanics assumes that experiments are conducted under ideal conditions, free from errors in state preparation and measurement execution. However, real-world experiments are often plagued by imperfections, including decoherence due to environmental noise, inaccuracies in state preparation, and errors in calculations. To address these challenges, several methodologies have been proposed, such as stochastic Schrödinger equations, open systems, and generalized probabilistic models. In this work, we adopt an alternative approach known as Quantum Bayesianism. \n\nWithin the Quantum Bayesian framework, the wave function is interpreted as a representation of our knowledge regarding the quantum process, rather than a direct depiction of its physical characteristics. This perspective implies that measurement outcomes are determined by updating our information in accordance with Bayes' theorem. For example, when Alice measures the spin of her particle along the x-axis, she updates her knowledge based on the result obtained. If Bob subsequently measures the spin of his particle along the same axis and receives a different outcome, he must conduct a new measurement to adjust his understanding accordingly. This approach not only clarifies the role of information in quantum mechanics but also provides a robust framework for addressing the complexities introduced by experimental imperfections.",
        "ori-fast-z-score": -0.08804509063256238,
        "water-fast-z-score": 6.18718433538229,
        "rewrite-fast-z-score": -1.1748539016153647
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Scaling Laws and Intermittency in Highly Compressible Turbulence .\nAbstract:\nWe study the statistical properties of highly compressible turbulence by means of direct numerical simulations (DNS) at Mach numbers ranging between 10 and 100, using both isothermal and adiabatic equations of state for the gas.  We find that the scaling laws observed in incompressible flows are violated when the Mach number becomes large enough to produce shocks. In particular, we show that the energy spectrum exhibits an exponential decay with wavenumber k instead of the power-law behavior E(k) ~ k-5/3 predicted by Kolmogorov s theory. The intermittent nature of the flow is also investigated through the analysis of probability density functions (PDFs). It turns out that PDF tails become increasingly fat-tailed as M increases, which can be explained by invoking the presence of shock waves. Finally, we discuss how these results may affect our understanding of astrophysical phenomena such as supernova remnants or interstellar clouds.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Scaling Laws and Intermittency in Highly Compressible Turbulence . Abstract : We research the statistical characteristics of highly compressible turbulence by means of direct numerical simulations ( DNS ) at Mach numbers ranging between 10 and 100 , using both isothermal and adiabatic equations of state for the gas .We see that the scaling restrictions experienced in incompressible flows are violated when the Mach number becomes large enough to produce shocks . In particular , we prove that the power spectrum exhibits an exponential decay with wavenumber k instead of the power - law behavior E ( k ) ~ k - 5 / 3 predicted by Kolmogorov s principle .The intermittent nature of the flow is also examined through the examination of likelihood density functions ( PDFs ) . It turns out that PDF tails get increasingly fat - tailed as M rises , which can be described by invoking the presence of shock waves .Finally , we explain how these results may affect our knowing of astrophysical processes such as supernova remnants or interstellar clouds .",
        "rewrite_text": "In this study, we investigate the statistical properties of highly compressible turbulence through direct numerical simulations (DNS) conducted at Mach numbers ranging from 10 to 100. Our analysis employs both isothermal and adiabatic equations of state for the gas to capture the nuances of turbulent behavior under varying compressibility conditions. We find that the traditional scaling laws observed in incompressible flows are significantly altered when the Mach number reaches levels sufficient to generate shock waves. Specifically, our results demonstrate that the power spectrum of the turbulence exhibits an exponential decay with respect to the wavenumber \\( k \\), deviating from the power-law behavior \\( E(k) \\sim k^{-5/3} \\) that is typically predicted by Kolmogorov's theory for incompressible turbulence.\n\nAdditionally, we delve into the intermittent characteristics of the turbulent flow by analyzing probability density functions (PDFs). Our findings reveal that as the Mach number increases, the tails of the PDFs become increasingly fat-tailed, a phenomenon that can be attributed to the influence of shock waves within the turbulent medium. This observation highlights the complex nature of turbulence in highly compressible regimes and suggests that traditional models may not adequately capture the behavior of such flows.\n\nFinally, we discuss the implications of our findings for understanding various astrophysical phenomena, including supernova remnants and interstellar clouds. The insights gained from this research could enhance our comprehension of turbulence in cosmic environments, where compressibility plays a crucial role in the dynamics of gas flows. Overall, our work contributes to the broader understanding of turbulence in highly compressible regimes and its significance in both theoretical and applied contexts.",
        "ori-fast-z-score": -0.35603449745815596,
        "water-fast-z-score": 4.564604740649092,
        "rewrite-fast-z-score": 1.1818181818181819
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Multi-Color Photometry of the Galactic Globular Cluster M75 = NGC 6864. A New Sensitive Metallicity Indicator and the Position of the Horizontal Branch in UV .\nAbstract:\nWe present new multi-color photometric data for the globular cluster M75 (NGC6864) obtained with the Hubble Space Telescope Wide Field Camera 3, which allow us to study its horizontal branch stars in unprecedented detail. We find that the color distribution along the HB is bimodal, indicating two distinct populations of hot and cool HB stars. The blue tail of the observed HB can be explained by assuming an age difference between these two groups of about 1 Gyr. Using our new HST observations we also derive accurate absolute ages for both sub-populations. Our results show that the redder population has an age of 12.6 ± 0.2 Gyr while the bluer one is younger at 11.7 ± 0.1 Gyr. This finding supports previous suggestions that the redder part of the HB may have been formed during a second episode of star formation within this system. \n \n Keywords: Globular clusters; Horizontal branches",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  Multi-Color Photometry of the Galactic Globular Cluster M75 = NGC 6864.A New Sensitive Metallicity Indicator and the Position of the Horizontal Branch in UV .Abstract : We report new multi - color photometric data for the globular cluster M75 ( NGC6864 ) obtained with the Hubble Space Telescope Wide Field Camera 3 , which allow us to study its horizontal branch stars in unprecedented detail . We see that the color variation along the HB is bimodal , showing two different populations of cold and warm HB stars .The blue tail of the observed HB can be described by assuming an age difference between these two groups of about 1 Gyr . Using our new HST observations we also generate reliable absolute ages for both sub - communities .Our results show that the redder population has an age of 12 . 6 ± 0 . 2 Gyr while the bluer one is younger at 11 . 7 ± 0 . 1 Gyr . This conclusion supports earlier suggestions that the redder half of the HB may have been formed during a second episode of star formation within this system .Keywords : Globular clusters ; Horizontal branches",
        "rewrite_text": "Title: Multi-Color Photometry of the Galactic Globular Cluster M75 (NGC 6864): A Novel Sensitive Metallicity Indicator and the Horizontal Branch Position in UV\n\nAbstract: In this study, we present new multi-color photometric observations of the globular cluster M75 (NGC 6864), acquired using the Hubble Space Telescope's Wide Field Camera 3. These observations enable an in-depth analysis of the horizontal branch (HB) stars within the cluster, revealing intricate details that were previously unattainable. Our findings indicate a bimodal distribution in the color variation along the HB, suggesting the presence of two distinct populations of stars: cold and warm HB stars. Notably, the blue tail of the HB can be interpreted by positing an age difference of approximately 1 Gyr between these two stellar groups. Leveraging our HST data, we derive accurate absolute ages for each sub-population. The results indicate that the redder HB stars have an age of 12.6 ± 0.2 Gyr, while the bluer stars are younger, with an age of 11.7 ± 0.1 Gyr. This age disparity supports previous hypotheses that the redder HB population may have originated from a secondary episode of star formation within the cluster. Our research not only enhances the understanding of the evolutionary processes in M75 but also contributes to the broader knowledge of globular cluster dynamics and formation history. The implications of these findings are significant for the study of stellar populations and the chemical evolution of the Milky Way. \n\nKeywords: Globular clusters; Horizontal branches.",
        "ori-fast-z-score": -0.7171371656006361,
        "water-fast-z-score": 3.731961445658845,
        "rewrite-fast-z-score": -0.41702882811414954
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Agujeros de Gusano en Gravedad (2+1) .\nAbstract:\nEn este trabajo se estudia la existencia y estabilidad de agujeros de gusano en el espacio tiempo descrito por una solución exacta del campo gravitacional generado por un cuerpo esférico con simetría axial, que corresponde al caso más simple de agujero negro no rotante. Se muestra que los agujeros de gusano son estables bajo ciertas condiciones sobre las constantes cosmológicas involucradas. Además, se presenta una nueva clase de soluciones exactas para el problema de Einstein-Klein-Gordon en espacios homogéneos e isótropos. Estas soluciones corresponden a ondas escalares estacionarias localizadas alrededor de un punto singular donde existe una densidad infinita de energía escalar. Finalmente, se discute brevemente cómo estas soluciones pueden ser utilizadas como fuentes de radiación gravitatoria coherente. En este trabajo estudiamos la existencia y estabilidad de los llamados agujeros de gusano  en el espacio-tiempo descrito por una solución exacta correspondiente al campo gravitacional generado por una esfera con simetría axial. Esta solución corresponde al caso más sencillo posible de agujero negro sin rotación. Mostramos que estos agujeros de gusano resultan ser estables bajo determinadas condiciones sobre las constantes cosmológica involucradas.  También presentamos una nueva clase de soluciones exáctas para el problema de Klein-Gordon-Einstein en espacios homogéneos isotrópicos. Estas soluciones corresponden a olas escalares estacionarias localizdas alrededor de un punto singular en donde hay una densidad infinita de energía escalara. Por último discutimos brevemente cómo estas soluciones pueden utilizarse como fuentes de radiación gravitacional coherente.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Agujeros de Gusano en Gravedad ( 2 + 1 ) . Abstract : En este trabajo se estudia la existencia y estabilidad de agujeros de gusano en el espacio tiempo descrito por una solución exacta en campo gravitacional generado por un cuerpo esférico con simetría axial , se corresponde un caso más simple de agujero negro no rotante .Se muestra en los agujeros de gusano son estables bajo ciertas condiciones sobre las constantes cosmológicas involucradas . Además , se presenta una nueva clase de soluciones exactas en le problema de Einstein - Klein - Gordon en espacios homogéneos e isótropos .Estas soluciones corresponden a ondas escalares estacionarias localizadas alrededor de un punto singular donde existe una densidad infinita de energía escalar . Finalmente , se discute brevemente cómo estas soluciones pueden ser utilizadas como fuentes de radiación gravitatoria coherente .En este trabajo estudiamos la existencia y estabilidad de los llamados agujeros de gusano en el espacio - tiempo descrito por una solución exacta correspondiente al campo gravitacional generado por una esfera con simetría axial . Esta solución corresponde un caso más sencillo posible de agujero negro sin rotación .Mostramos para estos agujeros de gusano resultan ser estables bajo determinadas condiciones sobre las constantes cosmológica involucradas . También presentamos una nueva clase de soluciones exáctas de el problema de Klein - Gordon - Einstein en espacios homogéneos isotrópicos .Estas soluciones corresponden a olas escalares estacionarias localizdas alrededor de un punto singular en donde hay una densidad infinita de energía escalara . Por último discutimos brevemente cómo estas soluciones pueden utilizarse como fuentes de radiación gravitacional coherente .",
        "rewrite_text": "**Title:** Wormholes in (2 + 1) Gravity\n\n**Abstract:** This study investigates the existence and stability of wormholes within a spacetime framework defined by an exact solution to the gravitational field produced by a spherically symmetric body with axial symmetry. This scenario represents a simpler case of a non-rotating black hole. Our findings indicate that these wormholes can exhibit stability under specific conditions related to the cosmological constants involved. Furthermore, we introduce a novel class of exact solutions to the Einstein-Klein-Gordon problem in homogeneous and isotropic spaces. These solutions correspond to stationary scalar waves that are localized around a singular point characterized by an infinite density of scalar energy. In addition, we briefly discuss the potential applications of these solutions as coherent gravitational radiation sources. This work contributes to the understanding of wormhole dynamics in lower-dimensional gravity and opens avenues for further exploration of their implications in theoretical physics.",
        "ori-fast-z-score": -1.3333333333333333,
        "water-fast-z-score": 1.9148542155126762,
        "rewrite-fast-z-score": 0.23570226039551587
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dynamical Masses for Pre-Main Sequence Stars: A Preliminary Physical Orbit for V773 Tau A .\nAbstract:\nWe present the first dynamical mass measurement for an eclipsing binary system with one component that is still on its pre-main sequence (PMS). The PMS star, V773 Tau A, has been observed to undergo large-amplitude photometric variability and spectroscopic changes indicative of rapid rotation and magnetic activity. We have obtained new high-precision radial velocity measurements using the HIRES spectrograph at Keck Observatory in Hawaii. These data are combined with previously published observations spanning more than 20 years to produce a preliminary orbital solution. Using this orbit we calculate a dynamical mass for V773 Tau A of 0.84 ± 0.04M⊙. This value agrees well with theoretical predictions based upon evolutionary models for young stars. Our results provide strong evidence against previous claims that the components of close binaries evolve independently during their early stages of formation. In addition, our analysis provides further support for the hypothesis that the majority of short-period systems will merge within 10 million years due to tidal interactions between the two components. \n \n Keywords: Mass determination",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dynamical Masses for Pre - Main Sequence Stars : A Preliminary Physical Orbit for V773 Tau A . Abstract : We report the first dynamical mass measurement for an eclipsing binary system with one element that is already on its pre - principal sequence ( PMS ) .The PMS star , V773 Tau A , has been observed to undergo huge - amplitude photometric variability and spectroscopic changes indicative of rapid rotation and magnetic activity . We have achieved new high - precision radial speed measurements using the HIRES spectrograph at Keck Observatory in Hawaii .These data are combined with previously published observations spanning more than 20 years to produce a preliminary orbital solution . Using this orbit we calculate a dynamical mass for V773 Tau A of 0 . 84 ± 0 . [UNK] .This value agrees well with theoretical estimates based upon evolutionary models for young stars . Our results represent strong evidence against prior allegations that the parts of close binaries evolve separately during their early stages of formation .In addition , our analysis provides further evidence for the notion that the majority of short - time systems will merge within 10 million years owing to tidal interactions between the two parts . Keywords : Mass determination",
        "rewrite_text": "**Title:** Dynamical Masses for Pre-Main Sequence Stars: A Preliminary Physical Orbit for V773 Tau A\n\n**Abstract:** In this study, we present the inaugural dynamical mass measurement for an eclipsing binary system featuring a component that is already in the pre-main sequence (PMS) phase. The star V773 Tau A has been observed to exhibit significant photometric variability and spectroscopic alterations, which are indicative of rapid rotation and heightened magnetic activity. Utilizing the HIRES spectrograph at the Keck Observatory in Hawaii, we have obtained new high-precision radial velocity measurements. These measurements are integrated with previously published data collected over a span of more than two decades to derive a preliminary orbital solution for the system. From this orbital analysis, we determine a dynamical mass for V773 Tau A of 0.84 ± 0. [UNK], a finding that aligns closely with theoretical predictions derived from evolutionary models for young stellar objects. Our results provide compelling evidence that challenges earlier claims suggesting that components of close binary systems evolve independently during their formative stages. Furthermore, our findings lend support to the hypothesis that a significant proportion of short-period binary systems are likely to merge within a timeframe of approximately 10 million years due to tidal interactions between the binary components. This research not only enhances our understanding of the mass characteristics of PMS stars but also contributes to the broader discourse on binary star evolution and the dynamics of stellar formation. \n\n**Keywords:** Mass determination, pre-main sequence stars, eclipsing binaries, dynamical mass measurement, stellar evolution.",
        "ori-fast-z-score": -0.5184758473652127,
        "water-fast-z-score": 3.9194007357834146,
        "rewrite-fast-z-score": -0.3651483716701107
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Experimental and theoretical study of light scattering by individual mature red blood cells by use of scanning flow cytometry and discrete dipole approximation .\nAbstract:\nThe optical properties of single human erythrocytes (red blood cells, RBCs) are investigated using the combination of scanning flow cytometry with numerical simulations based on the Discrete Dipole Approximation method. The experimental results show that the forward-scattering intensity is strongly dependent on the refractive index contrast between the cell cytoplasm and surrounding medium as well as on the size distribution of the scatterers inside each cell. In addition to the main peak in the forward direction at small angles, we observe two additional peaks at larger scattering angles corresponding to higher-order multipolar contributions. We find good agreement between our measurements and calculations for both the angular dependence of the scattered intensity and its polarization state. Our results demonstrate that the DDA can be used successfully to model light scattering by biological particles such as RBCs. This work was supported by Russian Science Foundation grant 14-50-00040. \n \n Light scattering by biological objects plays an important role in many biomedical applications including diagnostics  1  , drug delivery  2  , tissue engineering  3  . Scattering characteristics depend not only on the shape but also on the internal structure of the particle  4  . For example, it has been shown recently  5  that the presence of submicron-sized inclusions within the cytoplasm of red blood cells leads to significant changes in their scattering pattern compared to homogeneous spherical models  6  .\nIn this Letter, we report on combined experimental and theoretical investigation of light scattering by individual human erythrocytes. Using scanning flow cytometry  7, 8  , we measure the angular dependence of the forward-scattered intensity I(θ), where θ denotes the angle between incident beam and scattered light. To interpret these data, we perform numerical simulations based on the discrete-dipole-approximation (DDA)  9  technique which allows us to calculate the scattering cross section of arbitrary-shaped dielectric particles  10  . \nWe have studied three types of samples prepared from freshly drawn venous blood taken from healthy donors after informed consent had been obtained according to the protocol approved by the local ethics committee. First sample contained intact RBCs suspended in phosphate-buffered",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Experimental and theoretical investigation of light diffusion by individual mature red blood tissue by using of scanning flow cytometry and discrete dipole approximation . Abstract : The optical properties of single human erythrocytes ( red blood cells , RBCs ) are examined utilizing the combination of scanning flow cytometry with numerical simulations based on the Discrete Dipole Approximation method .The empirical results show that the forward - absorption intensity is strongly dependent on the refractive index contrast between the cell cytoplasm and surrounding medium as also as on the size distribution of the scatterers inside each cell . In addition to the main peak in the forward direction at small angles , we perceive two additional peaks at larger scattering angles similar to higher - order multipolar contributions .We get good agreement between our measurements and predictions for both the angular dependence of the scattered intensity and its polarization state . Our results show that the DDA can be used successfully to model light scattering by biological particles such as RBCs .This project was supported by Russian Science Foundation award 14 - 50 - 00040 . Light scattering by biological objects plays an important role in different biomedical applications notably diagnostics 1 , pharmaceutical production 2 , flesh engineering 3 .Scattering properties depend not only on the form but also on the internal structure of the particle 4 . For instance , it has been shown recently 5 that the presence of submicron - sized inclusions within the cytoplasm of red blood cells leads to significant improvements in their scattering shape compared to homogeneous spherical models 6 .In this Letter , we paper on combined empirical and theoretical investigation of light scattering by individual human erythrocytes . Using scanning flow cytometry 7 , 8 , we measure the angular relationship of the forward - dispersed intensity I ( θ ) , where θ denotes the angle between reflected beam and scattered radiation .To interpret these information , we perform numerical simulations based on the discrete - dipole - equation ( DDA ) 9 method which allows us to estimate the scattering cross section of arbitrary - shaped dielectric molecules 10 . We have researched three sorts of samples made from newly drawn venous blood taken from healthy recipients after informed permission had been sought according to the protocol approved by the local ethics body .First sample contained intact RBCs held in phosphate - buffered",
        "rewrite_text": "**Title:** Experimental and Theoretical Investigation of Light Diffusion by Individual Mature Red Blood Cells Using Scanning Flow Cytometry and Discrete Dipole Approximation\n\n**Abstract:** This study explores the optical characteristics of individual human erythrocytes (red blood cells, RBCs) through a combination of experimental techniques and numerical simulations, specifically employing scanning flow cytometry alongside the Discrete Dipole Approximation (DDA) method. Our empirical findings reveal that the intensity of forward scattering is significantly influenced by the refractive index contrast between the cytoplasm of the cells and the surrounding medium, as well as the size distribution of the internal scatterers within each RBC. Notably, in addition to the primary peak observed at small scattering angles, we identify two additional peaks at larger angles, which are indicative of higher-order multipolar scattering contributions. The correlation between our experimental measurements and theoretical predictions is strong, particularly concerning the angular distribution of scattered intensity and its polarization characteristics. These results affirm the efficacy of the DDA in modeling light scattering phenomena in biological particles such as RBCs. The research was funded by the Russian Science Foundation under award number 14-50-00040. \n\nLight scattering by biological entities is crucial for various biomedical applications, including diagnostics, pharmaceutical manufacturing, and tissue engineering. The scattering properties of these entities are influenced not only by their morphology but also by their internal structures. Recent studies have demonstrated that submicron-sized inclusions within the cytoplasm of RBCs can significantly alter their scattering profiles compared to traditional homogeneous spherical models. In this paper, we present a comprehensive empirical and theoretical analysis of light scattering by individual human erythrocytes. Using scanning flow cytometry, we measured the angular distribution of forward-scattered intensity, denoted as I(θ), where θ represents the angle between the incident beam and the scattered light. To interpret these measurements, we conducted numerical simulations based on the DDA, which enables the estimation of scattering cross-sections for arbitrarily shaped dielectric particles. Our investigation involved three types of samples derived from freshly drawn venous blood from healthy donors, with all procedures conducted in accordance with protocols approved by the local ethics committee. The first sample consisted of intact RBCs suspended in phosphate-buffered saline.",
        "ori-fast-z-score": 0.49872934991536755,
        "water-fast-z-score": 7.480940248730513,
        "rewrite-fast-z-score": 2.147501968772637
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Spectral analysis of Swift long GRBs with known redshift .\nAbstract:\nWe present the results of spectral analysis for all Swift bursts with measured redshifts and durations longer than 2 s, using data obtained by the Burst Alert Telescope (BAT) on board Swift satellite. We find that most of these bursts are best described as blackbody emission in combination with an additional power-law component at higher energies. The temperature of this blackbody component is found to be correlated with the peak energy of the spectrum E p . This correlation can be explained if we assume that the observed blackbody emission comes from photospheric radius expansion during the prompt phase of the burst. In addition, there seems to exist another correlation between the blackbody temperature T bb , the luminosity L iso and the duration t 90 .\nThe existence of such correlations suggests that the physical mechanism responsible for producing the blackbody emission may also play some role in determining other properties of the bursts. \n\n\nIntroduction\n\nGamma-ray bursts (GRB), discovered more than 40 years ago  1  , have been studied extensively since their discovery  2  . However, many questions about them remain unanswered  3  . One important question concerns the origin of the gamma-rays produced in GRBs  4  . It has been suggested that they could come from internal shocks  5  or magnetic reconnection  6  within relativistic jets launched by collapsing massive stars  7, 8  . Alternatively, it was proposed that they might result from external shocks driven into surrounding medium  9  . Another open issue is whether GRBs are standard candles  10  . If so, then one would expect that different bursts should show similar temporal and spectral behaviors  11  . On the contrary, observations suggest that GRBs exhibit large diversity  12  . Finally, the nature of the progenitors of GRBs remains unknown  13  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Spectral study of Swift length GRBs with known redshift . Abstract : We present the conclusion of spectral evaluation for all Swift bursts with recorded redshifts and durations greater than 2 s , using data acquired by the Burst Alert Telescope ( BAT ) on board Swift satellite .We see that most of these bursts are best described as blackbody emission in combination with an additional power - law component at higher energies . The temperature of this blackbody element is found to be correlated with the peak energy of the spectrum E p .This variance can be described if we suppose that the seen blackbody emission arises from photospheric radius expansion during the prompt phase of the explosion . In addition , there seems to exist another interaction between the blackbody altitude T bb , the luminosity L iso and the duration t 90 .The fact of such correlations indicates that the physical process responsible for producing the blackbody emission may even hold some role in determining other properties of the pulses . Introduction Gamma - ray clusters ( GRB ) , detected more than 40 years early 1 , have been studied significantly since their discovery 2 .However , many issues about them remain unanswered 3 . One important question concerns the origin of the gamma - radiation generated in GRBs 4 .It has been proposed that they may come from internal shocks 5 or gravitational reconnection 6 within relativistic jets launched by collapsing large galaxies 7 , 8 . Alternatively , it was suggested that they may come from external shocks driven into surrounding medium 9 .Another open problems is whether GRBs are standard candles 10 . If so , then one would expect that different bursts should exhibit similar temporal and spectral parameters 11 .On the contrary , observations suggest that GRBs exhibit great diversity 12 . Finally , the nature of the progenitors of GRBs remains unidentified 13 .",
        "rewrite_text": "**Title:** Spectral Analysis of Swift Long Gamma-Ray Bursts with Known Redshifts\n\n**Abstract:** In this study, we present a comprehensive spectral analysis of all Swift gamma-ray bursts (GRBs) that have recorded redshifts and durations exceeding 2 seconds, utilizing data from the Burst Alert Telescope (BAT) aboard the Swift satellite. Our findings indicate that the majority of these GRBs can be effectively characterized by a combination of blackbody emission and an additional power-law component at higher energy levels. Notably, we observe a correlation between the temperature of the blackbody component and the peak energy of the spectrum, denoted as E_p. This relationship suggests that the blackbody emission may originate from the expansion of the photospheric radius during the prompt phase of the explosion. Furthermore, we identify an intriguing interaction involving the blackbody temperature (T_bb), the isotropic luminosity (L_iso), and the duration (t_90) of the bursts. The presence of these correlations implies that the physical mechanisms responsible for the blackbody emission could also influence other characteristics of the GRB pulses. \n\nThe phenomenon of gamma-ray bursts has captivated researchers since their discovery over four decades ago, yet numerous questions remain unresolved. A critical inquiry pertains to the source of the gamma radiation emitted during GRBs, with hypotheses ranging from internal shocks and gravitational reconnection within relativistic jets produced by collapsing massive stars to external shocks interacting with the surrounding medium. Another unresolved issue is whether GRBs can be classified as standard candles, which would imply that different bursts should exhibit similar temporal and spectral characteristics. However, observational data reveal significant diversity among GRBs, challenging this notion. Additionally, the exact nature of GRB progenitors continues to elude identification. This study aims to contribute to the understanding of these enigmatic cosmic events by elucidating the spectral properties and underlying physical processes of Swift long GRBs.",
        "ori-fast-z-score": -0.43685202833051895,
        "water-fast-z-score": 7.60122529295103,
        "rewrite-fast-z-score": 1.227881227029841
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Optimal Routing for Decode-and-Forward based Cooperation in Wireless Networks .\nAbstract:\nIn this work, we consider decode-and forward (DF) cooperation among nodes in wireless networks and propose an optimal routing algorithm to maximize the network throughput by jointly optimizing source-to-destination routes as well as cooperative relay paths. We formulate the problem into mixed integer linear programming (MILP), which is NP-hard due to its combinatorial nature. To solve it efficiently, we develop two heuristic algorithms with polynomial time complexity. The first one uses a greedy approach that iteratively selects the best route between each pair of source destination pairs until all flows are routed. In order to further improve the performance, we also design another algorithm using simulated annealing technique. Extensive simulation results show that our proposed algorithms can achieve significant improvement over existing schemes. Index Terms- Cooperative communication, Optimum routing, Simulated annealing, Greedy algorithm. I. INTRODUCTIO N Cooperative communications have been widely studied recently because they provide higher data rates or longer transmission ranges than conventional non-cooperative systems  1  . In particular, decodeand-forward (DF) has attracted much attention since it does not require any extra power consumption at relays  2  .\nHowever, DF-based cooperative transmissions suffer from high end-to-end delay caused by multiple hops involved in forwarding packets  3  , especially when there exist many sources and destinations  4  . Therefore, how to find efficient cooperative relay paths becomes critical to reduce the overall end-to-end delay while maintaining good system performance  5  -  8  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Optimal Routing for Decode - and - Forward based Cooperation in Wireless Networks . Abstract : In this research , we investigate decode - and forward ( DF ) cooperation among nodes in wireless networks and suggest an appropriate routing algorithm to maximize the channel throughput by jointly optimizing source - to - destination lanes as well as cooperative relay routes .We formulate the question into mixed integer linear programming ( MILP ) , which is NP - hard thanks to its combinatorial nature . To solve it easily , we develop two heuristic algorithms with polynomial time complexity .The first one uses a greedy approach that iteratively selects the best path between each couple of source destination pairs until all flows are diverted . In try to further enhance the performance , we also design another optimization use simulated annealing technique .Extensive model results show that our proposed methods can attain valuable improvement over existing plans . Index Terms - Cooperative communication , Optimum routing , Simulated annealing , Greedy method .I . INTRODUCTIO N Cooperative networks have been widely examined lately because they deliver higher data levels or faster transmission ranges than conventional un - cooperative systems 1 . In particular , decodeand - forward ( DF ) has garnered great popularity since it does not require any additional power consumption at relays 2 .However , DF - based cooperative signals suffer from high end - to - end delay resulting by many hops engaged in forwarding packets 3 , particularly when there reside many sources and destinations 4 . Therefore , how to find effective cooperative relay routes seems necessary to reduce the overall end - to - end delay while maintaining good system performance 5 - 8 .",
        "rewrite_text": "**Title:** Optimal Routing for Decode-and-Forward Based Cooperation in Wireless Networks\n\n**Abstract:** This study explores the decode-and-forward (DF) cooperation mechanism among nodes in wireless networks, proposing a routing algorithm aimed at maximizing channel throughput. Our approach involves the joint optimization of both source-to-destination paths and cooperative relay routes. We frame the problem as a mixed integer linear programming (MILP) challenge, which is classified as NP-hard due to its combinatorial characteristics. To facilitate a more manageable solution, we introduce two heuristic algorithms characterized by polynomial time complexity. The first algorithm employs a greedy strategy, which incrementally selects the optimal path for each source-destination pair until all data flows are successfully rerouted. To further enhance performance, we also implement a second optimization method based on simulated annealing techniques. Comprehensive modeling results indicate that our proposed algorithms significantly outperform existing routing strategies. \n\nCooperative networks have gained considerable attention in recent years due to their ability to achieve higher data rates and extended transmission ranges compared to traditional non-cooperative systems. Specifically, the DF method has become increasingly popular as it does not necessitate additional power consumption at relay nodes. However, DF-based cooperative communication often encounters challenges such as increased end-to-end delay, primarily caused by the multiple hops required for packet forwarding, especially in scenarios with numerous sources and destinations. Consequently, identifying effective cooperative relay routes is essential for minimizing overall end-to-end delay while ensuring robust system performance. \n\n**Index Terms:** Cooperative communication, Optimal routing, Simulated annealing, Greedy method.",
        "ori-fast-z-score": -1.110664955031789,
        "water-fast-z-score": 6.700692544471771,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  On the cycling operation in braid groups .\nAbstract:\nWe study the cycle structure of elements in braid groups, and prove that any element can be written as a product of cycles whose lengths are bounded by some constant depending only on the number of strands. This is an extension to higher dimensions of results obtained for free groups by Magnus and Karrass-Solitar. \nTheorem 1 (Cycle decomposition). Let G = B_n(K) be a braid group with n strands over K. Then every g ∈ G has a unique expression as a product of disjoint cycles c_1, ..., c_k such that: \n1. The length of each cycle ci satisfies |ci| ≤ 2^{log_2 n} + 3.\n2. Each cycle ci contains at most one generator t_i which does not commute with its inverse.\n3. If there exists no generator t_i satisfying condition 2 then k = 0.\n4. No two generators appear simultaneously in different cycles.\n5. Every generator appears exactly once in each cycle it belongs to.\n6. For all i, |c_1 · ... · c_(i-1)| < |c_i|.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : On the cycling operation in braid groups . Abstract : We research the cycle composition of elements in braid families , and prove that any element can be written as a product of cycles whose lengths are bounded by some constant depending only on the number of strands .This is an extension to higher dimensions of findings obtained for free groups by Magnus and Karrass - Solitar . Theorem 1 ( Cycle decomposition ) .Let G = B _ n ( K ) be a braid group with n strands over K . Then every g ∈ G has a unique expression as a product of disjoint paths c _ 1 , . . . , b _ k such that : 1 . The span of each cycle ci satisfies | ci | ≤ 2 ^ { log _ 2 n } + 3 .2 . Each cycle ci consists at most one generator t _ i which does not commute with its inverse .3 . If there exists no generator t _ i satisfying condition 2 then k = 0 .4 . No two generators appear separately in different cycles .5 . Every generator appears precisely once in each cycle it belongs to .6 . For all i , | c _ 1 · . . . · c _ ( i - 1 ) | < | c _ i | .",
        "rewrite_text": "Title: On the Cycling Operation in Braid Groups\n\nAbstract: This study delves into the cycle composition of elements within braid families, demonstrating that any element can be expressed as a product of cycles, with the lengths of these cycles constrained by a constant that is solely dependent on the number of strands involved. This research extends previous findings related to free groups, as established by Magnus and Karrass-Solitar, into higher dimensions. We present Theorem 1, which addresses cycle decomposition within the context of braid groups. Specifically, let G = B_n(K) represent a braid group with n strands over a field K. Our findings indicate that for every element g in G, there exists a unique representation as a product of disjoint paths, denoted as c_1, ..., c_k, adhering to several key conditions: \n\n1. The length of each cycle c_i is bounded by |c_i| ≤ 2^{log_2 n} + 3.\n2. Each cycle c_i contains at most one generator t_i that does not commute with its inverse.\n3. If no generator t_i meets the criteria outlined in condition 2, then k equals 0.\n4. No two generators can appear separately in different cycles.\n5. Each generator is represented exactly once in every cycle to which it belongs.\n6. For all indices i, the length of the product of cycles preceding c_i is less than the length of c_i itself, i.e., |c_1 · ... · c_{i-1}| < |c_i|.\n\nThese results provide significant insights into the structure and behavior of braid groups, enhancing our understanding of their algebraic properties and the underlying mechanisms of cycle composition. The implications of this research may extend to various applications in topology and algebra, where braid groups play a crucial role.",
        "ori-fast-z-score": -2.2941573387056176,
        "water-fast-z-score": 1.0256451881367414,
        "rewrite-fast-z-score": -1.9445436482630056
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Star Formation in the Bok Globule CB54 .\nAbstract:\nWe present near-infrared (NIR) imaging and spectroscopy of star formation activity in the Bok globule CB 54, which is located at a distance of about 1 kpc toward the Galactic anti-center direction. We found that there are two young stellar objects (YSOs), one Class I protostar with an infrared luminosity of ~10 Lsun and another embedded YSO candidate with a bolometric temperature of ~1000 K. The former object shows bipolar outflows traced by Herbig-Haro knots and molecular line wings. In addition to these two sources, we detected several other point-like NIR sources within the central region of CB 54. These may be low-mass pre-main-sequence stars or background galaxies. Our results suggest that this cloud core has experienced active star formation over its lifetime. \n \n Keywords: Star formation; Near-infrared spectroscopy; Bipolar outflow; Young stellar object",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Star Formation in the Bok Globule CB54 . Abstract : We present near - infrared ( NIR ) imaging and spectroscopy of star formation activity in the Bok globule CB 54 , which is situated at a distance of about 1 kpc toward the Galactic anti - center position .We showed that there are two young stellar bodies ( YSOs ) , one Class I protostar with an infrared luminosity of ~ 10 Lsun and another embedded YSO candidate with a bolometric temperature of ~ 1000 K . The former object displays bipolar outflows traced by Herbig - Haro knots and molecular line tails . In addition to these two sources , we spotted many other point - like NIR sources within the central region of CB 54 .These may be low - weight pre - principal - sequence stars or background galaxies . Our results show that this storm core has undergone active star formation over its lifetime .Keywords : Star formation ; Near - infrared spectroscopy ; Bipolar outflow ; Young stellar object",
        "rewrite_text": "We present a detailed study of star formation in the Bok globule CB54, located approximately 1 kiloparsec from the Galactic anti-center. Utilizing near-infrared (NIR) imaging and spectroscopy, we identified significant star formation activity within this dense molecular cloud. Our observations revealed the presence of two young stellar objects (YSOs): a Class I protostar exhibiting an infrared luminosity of around 10 Lsun, and another embedded YSO candidate characterized by a bolometric temperature of approximately 1000 K. The Class I protostar is particularly noteworthy as it demonstrates bipolar outflows, which are evidenced by the presence of Herbig-Haro knots and molecular line tails, indicating ongoing dynamic processes associated with its formation. Furthermore, our survey uncovered numerous additional point-like NIR sources in the central region of CB54, which may represent low-mass pre-main-sequence stars or distant background galaxies. The findings suggest that CB54 has experienced a history of active star formation, contributing to our understanding of the processes that govern star formation in such environments. This research highlights the importance of Bok globules as sites of stellar birth and provides valuable insights into the early stages of stellar evolution. The implications of our results extend to the broader context of star formation in molecular clouds, emphasizing the diverse range of stellar objects that can emerge from these regions. \n\nKeywords: Star formation; Near-infrared spectroscopy; Bipolar outflow; Young stellar object.",
        "ori-fast-z-score": -0.5,
        "water-fast-z-score": 3.0,
        "rewrite-fast-z-score": 0.6527533657682196
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Possibility of large lifetime differences in neutral B meson systems .\nAbstract:\nWe study the possibility that there are two different lifetimes for neutral B mesons, one corresponding to the standard model and another to new physics beyond it.  We find that if the decay widths into final states with charm quarks differ by more than about 10% between these two types of B mesons then this can be observed at future experiments such as LHCb or Belle II. If we assume that the ratio of branching fractions is equal to 1 (as predicted within the Standard Model) but allow the total decay widths to vary independently, then we show how the experimental data on the time dependent CP asymmetry parameters SCP and ACP can be used to determine whether the difference in decay widths is due to new physics effects or not. Finally, we discuss possible extensions of our analysis which could lead to further constraints on the allowed parameter space. The results presented here will also have implications for other measurements performed at hadron colliders involving heavy flavour particles.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Possibility of large lifetime variations in neutral B meson systems . Abstract : We research the prospect that there are two different lifetimes for neutral B mesons , one corresponding to the standard description and another to new science beyond it .We see that if the decay widths into last states with charm quarks vary by more than about 10 % between these two kind of B mesons then this can be observed at future research such as LHCb or Belle II . If we suppose that the proportion of branching fractions is equal to 1 ( as predicted within the Standard Model ) but allow the total degradation widths to vary independently , then we tell how the empirical data on the period dependent CP asymmetry characteristics SCP and ACP can be used to predict whether the difference in decay widths is due to novel physics factors or not .Finally , we explain possible extensions of our analysis which potentially contribute to further limitation on the allowed parameter area . The results presented here will also have consequences for other tests accomplished at hadron colliders regarding heavy flavour particles .",
        "rewrite_text": "In this study, we investigate the intriguing possibility of significant variations in the lifetimes of neutral B mesons, proposing that there may exist two distinct lifetimes: one aligned with the conventional understanding of particle physics and another indicative of new physics beyond the Standard Model. Our analysis suggests that if the decay widths into final states containing charm quarks differ by more than approximately 10% between these two types of B mesons, such discrepancies could be detectable in upcoming experiments at facilities like LHCb or Belle II. \n\nWe explore a scenario where the ratio of branching fractions is maintained at unity, as predicted by the Standard Model, while allowing the total decay widths to vary independently. In this context, we demonstrate how empirical data related to time-dependent CP asymmetries, denoted as SCP and ACP, can be utilized to discern whether any observed differences in decay widths stem from novel physics phenomena or are merely artifacts of existing theoretical frameworks. \n\nFurthermore, we outline potential extensions to our analysis that could enhance the constraints on the allowed parameter space, thereby refining our understanding of the underlying physics governing B meson decays. The implications of our findings extend beyond the immediate scope of B meson studies, as they may also influence other experimental tests conducted at hadron colliders concerning heavy flavor particles. This research not only contributes to the ongoing discourse on B meson lifetimes but also opens avenues for future investigations into the fundamental principles of particle physics.",
        "ori-fast-z-score": -2.457864091118742,
        "water-fast-z-score": 5.196152422706631,
        "rewrite-fast-z-score": 1.2567574357593625
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dualheap Selection Algorithm: Efficient, Inherently Parallel and Somewhat Mysterious .\nAbstract:\nThe DualHeap selection algorithm is an efficient method for selecting the kth smallest element in a set S of n elements.  It has been used to solve many problems including finding the minimum spanning tree (MST) in parallel computing environments such as MapReduce.   In this article we present some new results on the DualHeap algorithm that shed light on its efficiency and inherent parallelism.    We show how it can be implemented using only O(logn) bits per element while still maintaining its efficiency guarantees.   This implementation requires no extra space beyond what is needed by the input data itself and thus fits well with modern storage devices like flash memory or hard disks where storing large amounts of data is cheap but accessing individual items may be expensive.   Finally, we prove that the DualHeap algorithm solves the MST problem faster than any other known deterministic algorithm when run on a single processor machine. The DualHeap selection algorithm is one of the most widely used algorithms in computer science today.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dualheap Selection Algorithm : Efficient , Inherently Parallel and Somewhat Mysterious . Abstract : The DualHeap selection method is an efficient algorithm for choose the kth smallest element in a set S of n elements .It has been used to solve many difficulties notably discovering the minimum spanning tree ( MST ) in concurrent processing communities such as MapReduce . In this page we present some new results on the DualHeap algorithm that shed light on its reliability and inherent parallelism .We see how it can be deployed using only O ( logn ) bits per element while nevertheless maintaining its productivity guarantees . This formulation uses no additional room beyond what is required by the input data itself and therefore fits well with modern storage devices like flash memory or hard disks where storing huge amounts of data is cheap but accessing individual items might be expensive .Finally , we prove that the DualHeap algorithm solves the MST problem faster than any other known deterministic procedure when run on a single processor system . The DualHeap selection method is one of the most commonly used algorithms in computer science today .",
        "rewrite_text": "Title: DualHeap Selection Algorithm: Efficient, Inherently Parallel, and Somewhat Mysterious\n\nAbstract: The DualHeap selection algorithm is a highly efficient method for identifying the kth smallest element within a dataset S comprising n elements. This algorithm has proven valuable in addressing various computational challenges, particularly in the realm of concurrent processing frameworks such as MapReduce, where it has been instrumental in tasks like constructing minimum spanning trees (MST). In this article, we present novel findings regarding the DualHeap algorithm that enhance our understanding of its reliability and inherent parallelism. Notably, we demonstrate that the algorithm can be implemented using only O(log n) bits per element, all while preserving its productivity guarantees. This efficient formulation requires no additional memory beyond the input data itself, making it particularly suitable for contemporary storage solutions, such as flash memory and hard drives, where the cost of storing large volumes of data is low, but accessing individual elements can be costly. Furthermore, we establish that the DualHeap algorithm outperforms all known deterministic methods for solving the MST problem when executed on a single-processor system. As one of the most widely utilized algorithms in the field of computer science today, the DualHeap selection method continues to be a subject of interest for researchers and practitioners alike, owing to its efficiency and unique characteristics. Our findings contribute to a deeper comprehension of the algorithm's capabilities and its potential applications in various computational contexts.",
        "ori-fast-z-score": -1.7820842224272613,
        "water-fast-z-score": 4.458892287340829,
        "rewrite-fast-z-score": 0.8392543274162825
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Mott Fermionic  Quantum  Criticality Beyond Ginzburg-Landau-Wilson .\nAbstract:\nWe present an overview of the recent developments in understanding quantum critical phenomena beyond the standard Landau-Ginzburg-Wilson paradigm, focusing on fermion systems with strong electronic correlations and Mott insulating states. We discuss how these new ideas can be applied to understand unconventional superconductivity observed in many strongly correlated materials such as high-Tc cuprates, iron-based compounds, heavy-fermions, organic conductors, etc., which are all characterized by non-Fermi liquid behaviors at low temperatures. In particular we will focus our attention on the concept of  multi-criticality  recently proposed for describing the phase diagram of these complex materials near their quantum critical points (QCPs). The idea is that there exist multiple QCPs along different directions in the parameter space of the system, each associated with its own universality class. These QCPs may also have some common features due to the underlying symmetry group of the system.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Mott Fermionic Quantum Criticality Beyond Ginzburg - Landau - Wilson . Abstract : We bring an overview of the recent developments in understanding quantum critical processes beyond the standard Landau - Ginzburg - Wilson paradigm , concentrating on fermion systems with powerful electronic correlations and Mott insulating states .We discuss how these new concepts can be applied to explain unconventional superconductivity noted in many intensely correlated materials such as high - Tc cuprates , iron - based compounds , heavy - fermions , organic conductors , etc . , which are all characterized by non - Fermi solid behaviors at low temperatures . In particular we will focus our focus on the idea of multi - criticality newly proposed for describing the phase diagram of these complex materials near their quantum vital places ( QCPs ) .The idea is that there lie multiple QCPs along various directions in the parameter space of the system , each associated with its own universality category . These QCPs might additionally have some common features due to the fundamental symmetry class of the system .",
        "rewrite_text": "This article provides a comprehensive overview of recent advancements in the understanding of quantum critical phenomena that extend beyond the traditional Landau-Ginzburg-Wilson framework. The focus is primarily on fermionic systems characterized by strong electronic correlations and Mott insulating states. We explore how these novel concepts can elucidate the mechanisms behind unconventional superconductivity observed in a variety of highly correlated materials, including high-temperature cuprates, iron-based superconductors, heavy-fermion systems, and organic conductors. These materials exhibit non-Fermi liquid behavior at low temperatures, prompting a reevaluation of the existing theoretical models.\n\nA significant emphasis is placed on the concept of multicriticality, which has been recently proposed to describe the intricate phase diagrams of these complex materials in proximity to their quantum critical points (QCPs). The multicriticality framework suggests the existence of multiple QCPs within the parameter space of the system, each corresponding to distinct universality classes. This multiplicity of QCPs allows for a richer understanding of the phase transitions and critical behaviors exhibited by these materials. Furthermore, we discuss the potential commonalities among these QCPs, which may arise from the underlying symmetry properties of the systems in question.\n\nBy integrating these insights, we aim to provide a deeper understanding of the interplay between quantum criticality and electronic correlations, ultimately contributing to the broader field of condensed matter physics. This work not only highlights the limitations of the conventional Ginzburg-Landau-Wilson approach but also paves the way for future research into the complex behaviors of strongly correlated electron systems.",
        "ori-fast-z-score": -0.7875615306482168,
        "water-fast-z-score": 5.142956348249516,
        "rewrite-fast-z-score": 0.7302967433402214
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The First Stellar Cluster .\nAbstract:\nThe first stars in the universe were born out of primordial gas clouds, which collapsed under their own gravity to form dense cores that ignited nuclear fusion and became hot white dwarfs.  The most massive of these early stars are now known as Population III (PopIII) stars.   In this work we present results for PopIII star formation using cosmological hydrodynamic simulations with radiative transfer calculations performed on an adaptive mesh refinement grid.  We find that PopIII stars can be formed by direct collapse of metal-free gas clouds at redshifts z > 20.  These stars have masses M * = 100 - 1000M⊙ and lifetimes t < 10Myr.  They evolve into black holes or pair-instability supernovae after consuming all available fuel within their convective envelopes.  Our results show that PopIII stars may contribute significantly to reionization of the intergalactic medium around redshift z ~ 15.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The First Stellar Cluster . Abstract : The first stars in the universe were born out of primordial plasma storms , which collapsed under their own gravity to form dense cores that sparked nuclear fusion and became hot white dwarfs .The most large of these older stars are now referred as Population III ( PopIII ) stars . In this research we present results for PopIII star formation combining cosmological hydrodynamic simulations with radiative transfer calculations performed on an adaptive mesh refinement grid .We see that PopIII stars can be formed by direct collapse of metal - loose gas clouds at redshifts z > 20 . These stars have masses M * = 100 - [UNK] and lifetimes t < 10Myr .They evolve into black holes or pair - instability supernovae after consuming all available fuel within their convective envelopes . Our results show that PopIII stars likely contribute greatly to reionization of the intergalactic medium around redshift z ~ 15 .",
        "rewrite_text": "**Title: The First Stellar Cluster**\n\n**Abstract:** The formation of the universe's first stars emerged from primordial plasma storms, which collapsed under their own gravitational forces, leading to the creation of dense cores that ignited nuclear fusion, ultimately resulting in the formation of hot white dwarfs. The most massive of these ancient stars are classified as Population III (PopIII) stars. In this study, we present findings on the formation of PopIII stars by integrating cosmological hydrodynamic simulations with radiative transfer calculations conducted on an adaptive mesh refinement grid. Our simulations indicate that PopIII stars can form through the direct collapse of metal-poor gas clouds at redshifts greater than 20. These stars exhibit masses ranging from M* = 100 to an undetermined upper limit and possess lifetimes of less than 10 million years. Following the depletion of their available fuel within their convective envelopes, these stars evolve into either black holes or undergo pair-instability supernovae. Our results suggest that PopIII stars played a significant role in the reionization of the intergalactic medium, particularly around redshift z ~ 15. This research enhances our understanding of the early universe's stellar population and its impact on cosmic evolution, shedding light on the processes that shaped the formation of galaxies and the large-scale structure of the cosmos.",
        "ori-fast-z-score": 1.212678125181665,
        "water-fast-z-score": 4.213504858001922,
        "rewrite-fast-z-score": 1.1406468642034677
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Nonlinear Evolution of Baryon Acoustic Oscillations .\nAbstract:\nWe study the nonlinear evolution of baryonic acoustic oscillations (BAO) in an expanding universe using numerical simulations with N-body particles and hydrodynamic gas dynamics. We find that BAOs are suppressed by nonlinear gravitational clustering, but they can be recovered by applying a Wiener filter to the density field at each redshift. The reconstructed power spectrum agrees well with linear theory predictions on large scales where the effect is negligible. On small scales, however, we observe significant deviations between the two spectra due to nonlinear effects such as shell crossing and virialization. These results suggest that it may be possible to use BAOs as standard rulers for cosmological distance measurements even when the underlying matter distribution becomes highly nonlinear. In this work, we present our main findings below. \n1 Introduction\n\nBaryons play important roles in galaxy formation through their interactions with dark matter. For example, observations show that galaxies form around peaks of the primordial density fluctuations which grow into massive halos via gravitational instability. Therefore, understanding how baryons evolve in time and space is crucial for studying galaxy formation processes.\n\nIn recent years, there has been growing interest in measuring the large-scale structure of the Universe using baryonic tracers like neutral hydrogen or stars. One promising method involves tracing the spatial distribution of these objects back in time using spectroscopic surveys. This technique allows us to measure the statistical properties of the cosmic web, including its geometry and topology, over a wide range of redshifts. \n\nThe most prominent feature observed in the measured correlation functions of various types of baryonic tracers is known as  baryonic acoustic oscillation  (BAO). It refers to periodic wiggles seen in the power spectrum of the tracer population caused by sound waves propagating through the early universe before decoupling  see e.g., 1  . Since the amplitude of the BAO signal depends only weakly on the physical state of the medium, it provides a robust way to probe the expansion history of the universe independent of other cosmological parameters  2  .\nRecently, several groups have reported detections of the BAO signature in the correlation function of Lyman",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Nonlinear Evolution of Baryon Acoustic Oscillations . Abstract : We research the nonlinear development of baryonic sound oscillations ( BAO ) in an increasing universe using numerical simulations with N - bodies particles and hydrodynamic gas mechanics .We see that BAOs are suppressed by nonlinear gravity clustering , but they can be recovered by using a Wiener filter to the density field at each redshift . The reconstructed power spectrum agrees well with continuous theory estimates on huge scales where the impact is negligible .On narrow scales , however , we find considerable deviations between the two spectra due to nonlinear effects such as shell crossing and virialization . These data suggest that it could be possible to use BAOs as conventional rulers for cosmological distance measurements even when the underlying matter distribution appears extremely nonlinear .In this research , we present our major findings below . 1 Introduction Baryons serve important roles in universe formation through their interactions with black material .For instance , observations show that galaxies form around spikes of the primordial density fluctuations which build into huge halos via gravitational instability . Therefore , studying how baryons grow in time and space is crucial for studying galaxy formation cycles .In past times , there has been growing interest in measuring the huge - scale structure of the Universe using baryonic tracers like neutral hydrogen or stars . One promising means includes tracing the spatial distribution of these objects back in time utilizing spectroscopic studies .This method enables us to measure the statistical characteristics of the cosmic web , notably its topology and topology , over a broad variety of redshifts . The most notable feature observed in the measured correlation functions of several kinds of baryonic tracers is known as baryonic sound oscillation ( BAO ) .It refers to periodic wiggles found in the power spectrum of the tracer population induced by sound signals propagating through the early universe before decoupling see e . g . , 1 . Since the frequency of the BAO signal relies only faintly on the physical state of the medium , it gives a reliable way to probe the development period of the universe independent of other cosmological factors 2 .Recently , various groups have reported detections of the BAO signature in the correlation function of Lyman",
        "rewrite_text": "**Title: Nonlinear Evolution of Baryon Acoustic Oscillations**\n\n**Abstract:** This study investigates the nonlinear evolution of baryon acoustic oscillations (BAOs) in an expanding universe through numerical simulations utilizing N-body particles and hydrodynamic gas dynamics. Our findings reveal that BAOs experience suppression due to the effects of nonlinear gravitational clustering; however, they can be effectively recovered by applying a Wiener filter to the density field at each redshift. The reconstructed power spectrum aligns closely with predictions from continuous theory on large scales, where the nonlinear effects are minimal. In contrast, on smaller scales, we observe significant discrepancies between the reconstructed and theoretical spectra, attributed to nonlinear phenomena such as shell crossing and virialization. These results indicate the potential for utilizing BAOs as standard rulers for cosmological distance measurements, even in scenarios where the underlying matter distribution exhibits strong nonlinearity.\n\nIn the introduction, we emphasize the critical role baryons play in the formation of the universe, particularly through their interactions with dark matter. Observational data suggest that galaxies form around peaks in primordial density fluctuations, which evolve into massive halos via gravitational instability. Consequently, understanding the temporal and spatial growth of baryons is essential for comprehending galaxy formation processes. Recent interest has surged in measuring the large-scale structure of the universe using baryonic tracers, such as neutral hydrogen and stars. One promising approach involves tracing the spatial distribution of these tracers over time through spectroscopic studies, allowing for the assessment of the cosmic web's statistical properties, including its topology across various redshifts.\n\nA key feature identified in the correlation functions of diverse baryonic tracers is the baryon acoustic oscillation (BAO), characterized by periodic fluctuations in the power spectrum of the tracer population. These oscillations are induced by sound waves propagating through the early universe prior to decoupling. Since the frequency of the BAO signal is only weakly dependent on the physical state of the medium, it serves as a robust tool for probing the universe's evolution independent of other cosmological influences. Recent studies have reported the detection of the BAO signature in the correlation functions of Lyman-alpha forest data, further underscoring the significance of BAOs in cosmological research.",
        "ori-fast-z-score": -2.1105794120443453,
        "water-fast-z-score": 8.493600116431967,
        "rewrite-fast-z-score": 0.4656903154237997
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Spitzer observations of a 24 micron shadow: Bok Globule CB190 .\nAbstract:\nWe report the detection of an infrared dark cloud (IRDC) in the vicinity of the open cluster NGC 6334, using data obtained with Spitzer Space Telescope s Infrared Array Camera (IRAC). The IRDC is associated with the molecular cloud complex G327.3+0.6 and has been identified as Bok globule CB190 by Clemens & Barvainis (1988) . We find that this object exhibits a prominent 24 micron shadow which may be caused by absorption against bright mid-infrared emission from nearby protostars or young stellar objects. This feature suggests that the cloud contains dense cores at different evolutionary stages. Using near-infrared extinction mapping we identify two candidate starless cores within the cloud. These are located near the center of the cloud where the 24 micron shadow is most pronounced. Our analysis shows that these cores have masses between 0.5 Msun to 1 Msun and radii ranging from 1000 AU to 3000 AU .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Spitzer observations of a 24 micron shadow : Bok Globule CB190 . Abstract : We report the observation of an infrared dark cloud ( IRDC ) in the vicinity of the open cluster NGC 6334 , using data acquired with Spitzer Space Telescope s Infrared Array Camera ( IRAC ) .The IRDC is associated with the molecular dust complex G327 . 3 + 0 . 6 and has been described as Bok globule CB190 by Clemens & Barvainis ( 1988 ) . We see that this body features a distinct 24 micron shadow which may be caused by absorption against bright mid - infrared absorption from nearby protostars or young stellar bodies .This characteristic demonstrates that the cloud contains thick cores at different evolutionary stages . Using near - infrared extinction mapping we identify two proposed starless cores within the cloud .These are situated near the center of the cloud where the 24 micron shadow is most pronounced . Our study shows that these cores have masses between 0 . 5 Msun to 1 Msun and radii ranging from 1000 AU to 3000 AU .",
        "rewrite_text": "We present findings from our observations of an infrared dark cloud (IRDC) located near the open cluster NGC 6334, utilizing data from the Infrared Array Camera (IRAC) aboard the Spitzer Space Telescope. This IRDC, identified as Bok globule CB190 by Clemens & Barvainis (1988), is associated with the molecular dust complex G327.3 + 0.6. Notably, our observations reveal a prominent 24-micron shadow within the cloud, likely resulting from absorption against the bright mid-infrared emissions from nearby protostars or young stellar objects. This shadow indicates the presence of dense cores within the cloud, each at varying stages of evolution. Through near-infrared extinction mapping, we have pinpointed two candidate starless cores located near the center of the cloud, where the 24-micron shadow is most evident. Our analysis indicates that these cores possess masses ranging from 0.5 to 1 solar mass (Msun) and exhibit radii between 1000 AU and 3000 AU. These findings contribute to our understanding of the structure and evolution of Bok globules and the processes involved in star formation within such dense environments. The implications of these observations extend to the broader context of molecular cloud dynamics and the lifecycle of stellar formation, providing valuable insights into the conditions that foster the birth of stars in the universe.",
        "ori-fast-z-score": 0.75,
        "water-fast-z-score": 4.83735464897913,
        "rewrite-fast-z-score": 0.32539568672798425
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Search for Heavy Neutral MSSM Higgs Bosons with CMS: Reach and Higgs-Mass Precision .\nAbstract:\nThe search is performed in the context of the Minimal Supersymmetric Standard Model (MSSM) using data collected by the Compact Muon Solenoid experiment at sqrt(s) = 7 TeV, corresponding to an integrated luminosity of 5 fb-1 . The results are interpreted as limits on the production cross section times branching fraction into two photons of neutral Higgs bosons decaying within the detector acceptance. In addition, upper bounds on the mass difference between the lightest CP-even Higgs boson and its heavier CP-even or CP-odd partner are derived. These results improve upon previous searches conducted by the ATLAS collaboration. \n \n A summary of this work has been presented at: \n \n \n \n \n \n This document contains additional information that may be useful to readers interested in reproducing our analysis or applying it to other datasets. It also includes details about how we have validated our results against those obtained independently by the ATLAS collaboration. \n \nIntroduction\n\nThe discovery of a new particle consistent with the Standard Model (SM) Higgs boson  1–3  has opened up a new era in particle physics. However, many open questions remain regarding the properties of this newly discovered state  4  , including whether it is part of a larger multiplet  5  .\nIn supersymmetry  6  , each SM field has a superpartner differing only in spin statistics  7, 8  . If R-parity  9  is conserved, then all superpartners must be produced in pairs  10  . One consequence of this scenario is that there can exist more than one Higgs doublet  11  . In particular, if the lighter scalar Higgs boson observed at the LHC  12–18  corresponds to the lightest CP-eigenstate h0 of such a model  19, 20  , then the next-to-lightest CP-eigenstates H0 and A0 could both couple strongly to fermions  21  . Such scenarios would lead to enhanced rates for decays of these states into final states containing photons  22  . \n \n In order to explore possible deviations from the SM predictions  23  , precise measurements of the masses and couplings of the Higgs bosons predicted by",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Search for Heavy Neutral MSSM Higgs Bosons with CMS : Reach and Higgs - Mass Precision . Abstract : The hunt is conducted in the context of the Minimal Supersymmetric Standard Model ( MSSM ) using data received by the Compact Muon Solenoid research at sqrt ( s ) = 7 TeV , equivalent to an integrated luminosity of 5 fb - 1 .The results are understood as limits on the production cross area times branching fraction into two photons of neutral Higgs bosons decaying within the detector acceptance . In addition , upper limits on the mass ratio between the lightest CP - even Higgs boson and its lighter CP - even or CP - even partner are derived .These conclusions improve upon recent searches undertaken by the ATLAS collaboration . A description of this research has been presented at : This report contains additional information that might be valuable to readers interested in reproducing our analysis or applying it to other datasets .It also contains details about how we have validated our findings against those acquired independently by the ATLAS collaboration . Introduction The discovery of a new particle compatible with the Standard Model ( SM ) Higgs boson 1 – 3 has opened up a new decade in particle science .However , many open questions remain regarding the properties of this newly discovered state 4 , particularly whether it is part of a greater multiplet 5 . In supersymmetry 6 , each SM field has a superpartner varying only in spin statistics 7 , 8 .If R - parity 9 is conserved , then all superpartners must be made in pairs 10 . One result of this situation is that there can exist more than one Higgs doublet 11 .In particular , if the lighter scalar Higgs boson seen at the LHC 12 – 18 corresponds to the lightest CP - eigenstate h0 of such a theory 19 , 20 , then the second - to - lightest CP - eigenstates H0 and A0 could both bond heavily to fermions 21 . Such scenarios would result to accelerated rates for decays of these states into last states carrying photons 22 .In order to examine possible deviations from the SM predictions 23 , detailed observations of the masses and couplings of the Higgs bosons predicted by",
        "rewrite_text": "**Title:** Search for Heavy Neutral MSSM Higgs Bosons with CMS: Reach and Higgs-Mass Precision\n\n**Abstract:** This study investigates the presence of heavy neutral Higgs bosons within the framework of the Minimal Supersymmetric Standard Model (MSSM), utilizing data collected by the Compact Muon Solenoid (CMS) at a center-of-mass energy of √s = 7 TeV, corresponding to an integrated luminosity of 5 fb⁻¹. The analysis focuses on establishing limits on the production cross-section multiplied by the branching fraction into two photons for neutral Higgs bosons that decay within the detector's acceptance. Furthermore, we derive upper limits on the mass ratios between the lightest CP-even Higgs boson and its lighter CP-even or CP-odd counterparts. These findings enhance the constraints set by recent searches conducted by the ATLAS collaboration. The report includes supplementary information that may assist readers in replicating our analysis or applying it to alternative datasets. Additionally, we provide a comprehensive validation of our results against independent findings from the ATLAS collaboration.\n\nThe discovery of a particle consistent with the Standard Model (SM) Higgs boson has ushered in a new era in particle physics, yet numerous questions persist regarding the characteristics of this newly identified state, particularly its potential association with a larger multiplet. In the context of supersymmetry, each SM field is paired with a superpartner that differs only in spin statistics. If R-parity is conserved, superpartners must be produced in pairs, leading to the possibility of multiple Higgs doublets. Specifically, if the lighter scalar Higgs boson observed at the Large Hadron Collider (LHC) corresponds to the lightest CP-eigenstate (h₀) in this theoretical framework, the second-lightest CP-eigenstates (H₀ and A₀) could couple significantly to fermions. Such interactions would result in enhanced decay rates of these states into final states that include photons. To explore potential deviations from SM predictions, it is crucial to conduct detailed measurements of the masses and couplings of the Higgs bosons anticipated by the MSSM.",
        "ori-fast-z-score": -3.1057477829563984,
        "water-fast-z-score": 5.1946603473211805,
        "rewrite-fast-z-score": -1.4485719366802965
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Detailed study of the GRB 030329 radio afterglow deep into the non-relativistic phase .\nAbstract:\nWe present new observations at 1.4 GHz and 8.5 GHz with the Australia Telescope Compact Array (ATCA) of the radio afterglow associated to the gamma-ray burst GRB030329, which occurred on March 29th 2003. The data were taken between April 2nd and May 8th 2004, i.e., more than one year after the burst event. We find that the source is still brightening in flux density by about 0.5% per day at both frequencies. This behaviour can be explained if we assume that the fireball has not yet reached its final deceleration radius. In this case, the observed emission would come from an expanding shell of relativistically moving material surrounding the central engine. If so, our results suggest that the energy injection process responsible for the late-time flaring activity detected in other GRBs may have continued up to very large distances from the progenitor system. \n \n Keywords: Gamma-Ray Bursts",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Detailed study of the GRB 030329 radio afterglow far into the non - relativistic phase . Abstract : We report new images at 1 . 4 GHz and 8 . 5 GHz with the Australia Telescope Compact Array ( ATCA ) of the radio afterglow related to the alpha - ray burst GRB030329 , which occurred on March 29th 2003 .The data were took between April 2nd and May 8th 2004 , i . e . , more than one year after the explosion event . We see that the source is already brightening in flux concentration by about 0 . 5 % per month at both frequencies .This behaviour can be described if we suppose that the fireball has not already achieved its final deceleration radius . In this situation , the seen emission might come from an evolving shell of relativistically moving material surrounding the main motor .If so , our findings show that the power injection mechanism responsible for the late - time flaring activity detected in other GRBs might have continued up to very significant distances from the progenitor system . Keywords : Gamma - Ray Bursts",
        "rewrite_text": "Title: In-Depth Analysis of the GRB 030329 Radio Afterglow During the Non-Relativistic Phase\n\nAbstract: In this study, we present new radio images captured at frequencies of 1.4 GHz and 8.5 GHz using the Australia Telescope Compact Array (ATCA) to investigate the radio afterglow associated with the gamma-ray burst GRB 030329, which was observed on March 29, 2003. The observations were conducted between April 2 and May 8, 2004, over a year following the initial explosion. Our analysis reveals a notable increase in the flux density of the afterglow, with a monthly brightening rate of approximately 0.5% at both observed frequencies. This gradual increase in brightness suggests that the fireball has not yet reached its final deceleration radius. We propose that the observed emission may originate from a dynamically evolving shell of relativistically moving material that envelops the primary source of the burst. Our findings imply that the power injection mechanism, which has been linked to the late-time flaring activities observed in other gamma-ray bursts, may persist over considerable distances from the progenitor system. This research contributes to the understanding of the long-term behavior of gamma-ray burst afterglows and the mechanisms driving their emissions in the non-relativistic phase. \n\nKeywords: Gamma-Ray Bursts, GRB 030329, Radio Afterglow, Australia Telescope Compact Array, Non-Relativistic Phase.",
        "ori-fast-z-score": -1.4552137502179978,
        "water-fast-z-score": 5.335783750799325,
        "rewrite-fast-z-score": -0.10976425998969035
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Stellar Evolution in NGC 6791: Mass Loss on the Red Giant Branch and the Formation of Low Mass White Dwarfs .\nAbstract:\nWe present new optical photometry for the open cluster NGC 6791, obtained with the Wide Field Camera 3 (WFC3) aboard HST as part of program GO-12775 (PI: A. Dotter). The data cover an area of 0.5 deg2 around the cluster center at high spatial resolution (0.1 arcsec/pixel), allowing us to study individual stars down to V = 20 mag. We use these observations together with archival WFC3/UVIS images taken under programs GO-10775 (PI: J. Kalirai) and GO-11775 (PI: S. Casagrande) to derive accurate stellar parameters for more than 1000 red giant branch (RGB) stars in this cluster. Our analysis shows that RGB mass loss is very efficient among low-mass stars, leading to the formation of white dwarfs with masses below 0.45 M . This result has important implications for our understanding of the evolution of low-mass stars near the end of their lives. \n \n Keywords: Open clusters",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Stellar Evolution in NGC 6791 : Mass Loss on the Red Giant Branch and the Formation of Low Mass White Dwarfs . Abstract : We report new optical photometry for the open cluster NGC 6791 , obtained with the Wide Field Camera 3 ( WFC3 ) aboard HST as part of series GO - 12775 ( PI : A . Dotter ) .The data cover an area of 0 . 5 deg2 around the cluster center at high spatial resolution ( 0 . 1 arcsec / pixel ) , allowing us to study individual stars down to V = 20 mag . We use these observations together with archival WFC3 / UVIS images took under programs GO - 10775 ( PI : J . Kalirai ) and GO - 11775 ( PI : S . Casagrande ) to derive exact stellar characteristics for more than 1000 red dwarf branch ( RGB ) stars in this cluster .Our study shows that RGB mass loss is very efficient among low - mass stars , leading to the formation of white dwarfs with masses below 0 . 45 M . This result has crucial consequences for our understanding of the evolution of lowest - mass stars near the end of their careers .Keywords : Open clusters",
        "rewrite_text": "Title: Stellar Evolution in NGC 6791: Mass Loss on the Red Giant Branch and the Formation of Low Mass White Dwarfs\n\nAbstract: In this study, we present new optical photometry of the open cluster NGC 6791, acquired using the Wide Field Camera 3 (WFC3) on the Hubble Space Telescope (HST) as part of the observing program GO-12775 (Principal Investigator: A. Dotter). The dataset encompasses an area of 0.5 square degrees surrounding the cluster's center, achieving high spatial resolution of 0.1 arcseconds per pixel. This level of detail enables us to analyze individual stars down to a magnitude of V = 20. By integrating these new observations with archival WFC3/UVIS images collected during programs GO-10775 (PI: J. Kalirai) and GO-11775 (PI: S. Casagrande), we have successfully derived precise stellar parameters for over 1,000 red giant branch (RGB) stars within the cluster. Our findings indicate that mass loss during the RGB phase is highly efficient in low-mass stars, resulting in the formation of white dwarfs with masses less than 0.45 solar masses. This discovery has significant implications for our understanding of the evolutionary processes affecting the lowest-mass stars as they approach the end of their life cycles. The results contribute to a deeper comprehension of stellar evolution in open clusters and the mechanisms driving mass loss in red giants, ultimately influencing the characteristics of the resulting white dwarfs. \n\nKeywords: Open clusters, Stellar evolution, Red giant branch, Mass loss, White dwarfs.",
        "ori-fast-z-score": 1.3643820804812932,
        "water-fast-z-score": 4.431293675255978,
        "rewrite-fast-z-score": 1.3065491598369756
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Why there is something rather than nothing (out of everything)? .\nAbstract:\nThe question  why?  has been asked by philosophers for thousands of years, and it remains one of the most important questions in philosophy today. In this article we will consider an answer to that question which was proposed by Sean Carroll in his book The Matter Myth.  We will begin with some background on Sean Carroll s work before considering how he answers the question why?. Finally, we will discuss whether or not Sean Carroll s answer can be considered satisfactory. Philosophy - History of science - Materialism - Materialism - Materialist views - Materialism - Materialists - Materialism - Materialistic materialism - Materialism - Philosophy of space and time - Materialism - Materiality - Materialism - Materialization Materialism - Materialism  - Materialism - Materialisms - Materialism - Materializing Materialism - Materialism Materialism - Materialism   - Materialism - Materializations - Materialism - Materialized Materialism - Materialism",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Why there is something instead than nothing ( out of things ) ? .Abstract : The question why ? has been given by philosophers for thousands of years , and it remains one of the most important questions in philosophy today .In this article we will explore an solution to that question which was suggested by Sean Carroll in his book The Matter Myth . We will start with some details on Sean Carroll s art before evaluating how he addresses the question why ? .Finally , we will explore whether or not Sean Carroll s answer can be regarded acceptable . Philosophy - History of science - Materialism - Materialism - Materialist views - Materialism - Materialists - Materialism - Materialistic materialism - Materialism - Philosophy of space and time - Materialism - Materiality - Materialism - Materialization Materialism - Materialism - Materialism - Materialisms - Materialism - Materializing Materialism - Materialism Materialism - Materialism - Materialism - Materializations - Materialism - Materialized Materialism - Materialism",
        "rewrite_text": "Title: Why is There Something Rather than Nothing?\n\nAbstract: The enduring question of \"why is there something rather than nothing?\" has captivated philosophers for millennia and continues to be a central inquiry in contemporary philosophy. This article delves into a potential resolution to this profound question, as proposed by physicist Sean Carroll in his influential work, \"The Matter Myth.\" We begin by providing an overview of Carroll's arguments and the philosophical context in which he operates. Carroll's approach intertwines concepts from physics and philosophy, suggesting that the existence of matter and the universe can be understood through the lens of materialism. We will critically assess how Carroll articulates his perspective on existence and the implications of his views for our understanding of reality. Furthermore, we will examine the philosophical ramifications of accepting Carroll's explanation, questioning its validity and exploring alternative interpretations. By analyzing the intersections of philosophy, science, and materialism, this article aims to contribute to the ongoing discourse surrounding the fundamental nature of existence and the reasons behind it. Ultimately, we seek to determine whether Carroll's insights provide a satisfactory answer to the age-old question of why there is something rather than nothing, or if further exploration is warranted in the quest for understanding our universe.",
        "ori-fast-z-score": -0.6030226891555273,
        "water-fast-z-score": 4.12837477233712,
        "rewrite-fast-z-score": 0.5360562674188973
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The anticenter old open cluster NGC 1883: radial velocity and metallicity .\nAbstract:\nWe present new spectroscopic observations for the open cluster NGC 1883, which is located at a distance of about 1 kpc in the constellation Cassiopeia (α = 20 h 18 m , δ = +58°). The data were obtained with the 2-m telescope of the Observatorio Astronómico Nacional de San Pedro Mártir (OAN-SPM) on December 16-17, 2009 using the REOSC spectrograph equipped with grism #7 covering the wavelength range 3700-7000 Å. We measured RVs for 23 stars by cross-correlating their spectra against those of template dwarfs observed under similar conditions. Our results show that most of these objects have heliocentric velocities between -40 to -50 km/sec, while only two stars are found outside this interval. These values agree well with previous determinations based on photometric methods. \n \n In addition we derived metallicities  Fe/H  for 14 stars using the calibration of Alonso et al. (1999) . For all but one star our measurements indicate solar or slightly subsolar metallicities ranging from -0.10 dex up to +0.20 dex. Only one object shows an iron abundance significantly higher than solar value (+0.30 dex). \n \n Finally, we compared our results with previously published studies.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The anticenter old open cluster NGC 1883 : radial speed and metallicity . Abstract : We report new spectroscopic observations for the open cluster NGC 1883 , which is situated at a distance of about 1 kpc in the constellation Cassiopeia ( α = 20 h 18 m , δ = + 58° ) .The data were obtained with the 2 - m observatory of the Observatorio Astronómico Nacional de San Pedro Mártir ( OAN - SPM ) on December 16 - 17 , 2009 using the REOSC spectrograph equipped with grism # 7 representing the frequency region 3700 - 7000 Å . We calculated RVs for 23 stars by cross - correlating their spectra against those of template dwarfs observed under corresponding conditions .Our results show that most of these objects have heliocentric velocities between - 40 to - 50 km / sec , while only two stars are found outside this interval . These values comply good with previous determinations based on photometric technique .In addition we derived metallicities Fe / H for 14 stars following the calibration of Alonso et al . ( 1999 ) .For all but one star our measurements indicate solar or slightly subsolar metallicities ranging from - 0 . 10 dex up to + 0 . 20 dex . Only one object displays an metal abundance considerably higher than solar value ( + 0 . 30 dex ) .Finally , we compared our findings with previously written findings .",
        "rewrite_text": "We present new spectroscopic observations of the open cluster NGC 1883, located approximately 1 kpc away in the constellation Cassiopeia (α = 20 h 18 m, δ = +58°). The data were collected using the 2-meter telescope at the Observatorio Astronómico Nacional de San Pedro Mártir (OAN-SPM) on December 16-17, 2009, utilizing the REOSC spectrograph with grism #7, which covers the wavelength range of 3700 to 7000 Å. We determined the radial velocities (RVs) for 23 stars by cross-correlating their spectra with those of template dwarf stars observed under similar conditions. Our analysis reveals that the majority of these stars exhibit heliocentric velocities ranging from -40 to -50 km/s, with only two stars falling outside this velocity range. These findings are consistent with previous measurements obtained through photometric techniques. Additionally, we calculated the metallicities (Fe/H) for 14 stars using the calibration method established by Alonso et al. (1999). The results indicate that, with the exception of one star, the metallicities are either solar or slightly subsolar, varying from -0.10 dex to +0.20 dex. Notably, one star shows a significantly higher metallicity of +0.30 dex, indicating a metal abundance above the solar level. Finally, we compare our results with existing literature to contextualize our findings within the broader scope of research on open clusters. This study enhances our understanding of the properties of NGC 1883 and contributes to the ongoing exploration of stellar populations in the Milky Way.",
        "ori-fast-z-score": -0.11547005383792514,
        "water-fast-z-score": 4.444462481925879,
        "rewrite-fast-z-score": 1.801996396010812
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Constraints on the merging timescale of luminous red galaxies, or, where do all the halos go? .\nAbstract:\nWe present new constraints on the merger rate density and mass ratio distribution for luminous red galaxies (LRGs) using data from the Sloan Digital Sky Survey Data Release 7 (SDSS DR7). We use these results to place limits on the number of massive dark matter haloes that are not associated with LRG hosts in the local universe. The halo occupation model we employ is based upon an extension of the HOD formalism developed by Zheng et al. (2005) , which allows us to simultaneously constrain both the mean occupation function and its scatter at fixed luminosity as well as the fraction of satellite galaxies that have been recently accreted into their host haloes. Our analysis indicates that there exists a significant population of massive haloes that are not occupied by LRGs within our sample volume. This result suggests that either many of the most massive haloes were assembled very early during cosmic history and/or that they contain only low-mass galaxies.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Constraints on the merging timescale of luminous red objects , or , where do all the halos go ? .Abstract : We create additional constraints on the merger rate volume and mass ratio distribution for luminous red objects ( LRGs ) using data from the Sloan Digital Sky Survey Data Release 7 ( SDSS DR7 ) . We use these results to place limits on the quantity of large dark matter haloes that are not associated with LRG hosts in the local universe .The halo occupation model we utilize is based upon an extension of the HOD formalism developed by Zheng et al . ( 2005 ) , which allows us to jointly constrain both the mean occupation function and its scatter at fixed luminosity as well as the fraction of satellite galaxies that have been lately accreted into their host haloes .Our study implies that there exists a substantial population of large haloes that are not populated by LRGs within our sample volume . This result suggests that either several of the most large haloes were assembled very early during cosmic history and / or that they contain only low - mass stars .",
        "rewrite_text": "In this study, we present new constraints on the merger rate, volume, and mass ratio distribution of luminous red objects (LRGs) by analyzing data from the Sloan Digital Sky Survey Data Release 7 (SDSS DR7). Our findings enable us to establish limits on the number of large dark matter halos that are not associated with LRG hosts in the local universe. We employ a halo occupation model that extends the halo occupation distribution (HOD) framework introduced by Zheng et al. (2005). This model allows us to simultaneously constrain both the mean occupation function and its scatter at a fixed luminosity, as well as the proportion of satellite galaxies that have recently been accreted into their respective host halos. Our analysis indicates a significant presence of large halos that remain unoccupied by LRGs within the surveyed volume. This observation raises intriguing questions regarding the evolutionary history of these halos, suggesting that many of the largest halos may have formed early in cosmic history or may predominantly contain low-mass stars. These insights contribute to our understanding of the distribution and evolution of dark matter structures in the universe, highlighting the complex relationship between luminous red objects and their surrounding halos. The implications of our results are critical for refining models of galaxy formation and evolution, as they provide a clearer picture of the dynamics at play in the local universe and the fate of dark matter halos that are not directly linked to luminous galaxies.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.216541329006604,
        "rewrite-fast-z-score": 0.658504607868518
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Electronic spin transport and spin precession in single graphene layers at room temperature .\nAbstract:\nWe report on the observation of electronic spin transport across macroscopic distances (several millimeters) in suspended single-layer graphene devices at room temperature.  The experiments are performed using nonlocal spin-valve measurements with ferromagnetic contacts made by sputtering Co/Ni multilayers onto exfoliated flakes of graphite, followed by annealing to form cobalt silicide contacts. We observe that the resistance-area product RA is strongly dependent upon the contact geometry; for example, we find that it decreases significantly when the length of one of the contacts increases beyond 1 micron. This behavior can be explained by considering the effect of disorder scattering near the interface between the metal and the carbon atoms. In addition, we show that the magnitude of the spin signal depends sensitively on the relative orientation of the magnetization directions of the two ferromagnets. These results demonstrate that the observed spin signals arise primarily due to spin injection into the graphene layer rather than being dominated by proximity effects or other mechanisms associated with the magnetic contacts themselves.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Electronic spin travel and spin precession in single graphene structures at room temperature . Abstract : We report on the observation of electronic spin travel across macroscopic distances ( several millimeters ) in suspended single - layer graphene materials at room temperature .The studies are performed using nonlocal spin - valve measurements with ferromagnetic contacts produced by sputtering Co / Ni multilayers onto exfoliated flakes of graphite , followed by annealing to form cobalt silicide contacts . We see that the tolerance - area product RA is strongly dependent upon the contact shape ; for example , we find that it reduces considerably when the length of one of the contacts increases beyond 1 micron .This phenomenon can be described by using the impact of disorder absorption near the interface between the metal and the carbon atoms . In addition , we find that the magnitude of the spin signal relies sensitively on the relative position of the magnetization directions of the two ferromagnets .These data demonstrate that the known spin signals arise principally due to spinning injection into the graphene substrate rather than being dominated by proximity effects or other mechanisms associated with the magnetic contacts themselves .",
        "rewrite_text": "In this study, we present our findings on the propagation of electronic spin over considerable distances—up to several millimeters—in suspended single-layer graphene at room temperature. Utilizing nonlocal spin-valve measurements, we employed ferromagnetic contacts created by sputtering Co/Ni multilayers onto exfoliated graphite flakes, followed by an annealing process to develop cobalt silicide contacts. Our results indicate that the tolerance-area product (RA) exhibits a significant dependence on the geometry of the contacts. Notably, we observed a marked decrease in RA when the length of one of the contacts exceeded 1 micron. This behavior can be attributed to the effects of disorder absorption occurring at the interface between the metal and carbon atoms. Furthermore, our investigations reveal that the amplitude of the spin signal is highly sensitive to the relative orientations of the magnetization in the two ferromagnetic contacts. These findings suggest that the observed spin signals primarily result from spin injection into the graphene substrate, rather than being predominantly influenced by proximity effects or other mechanisms related to the magnetic contacts themselves. This research enhances our understanding of spin transport in graphene, highlighting its potential for applications in spintronic devices, where efficient spin manipulation and transport are crucial. The implications of these results pave the way for future studies aimed at optimizing spintronic materials and devices based on graphene, particularly in the context of room-temperature operations.",
        "ori-fast-z-score": -0.9434563530497265,
        "water-fast-z-score": 4.873672965232998,
        "rewrite-fast-z-score": 1.4501047335684953
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Hadronization in semi-inclusive deep-inelastic scattering on nuclei .\nAbstract:\nWe present the results for hadron production in semiinclusive DIS off nuclei at large Bjorken x and low Q^2, obtained with the HERMES experiment using data taken between 1997 and 2002. The analysis is performed within the framework of collinear factorisation and the modified perturbative approach to nuclear shadowing developed by Frankfurt et al.. We find that the observed suppression of leading neutron production relative to deuterium can be explained by nuclear effects alone without invoking any additional mechanism such as intrinsic charm or gluon saturation. In addition we observe an enhancement of strange particle production which cannot be described by conventional partonic models but may be attributed to the presence of intrinsic strangeness in the proton wave function. \n \n 1 Introduction \n \n Semi-inclusive deep-inelastic lepton-nucleus scattering (SIDIS) has been studied extensively over many years both experimentally  1 - 6  and theoretically  7  8  9  . This process provides information about the quark structure of the target nucleus through measurements of final state particles produced in association with the scattered lepton. At high values of Bjorken-x, where the struck quarks are highly virtual, SIDIS probes the transition region between the non-perturbative regime governed by confinement physics and the perturbative domain dominated by short-distance interactions  10  . \nIn this kinematic range it becomes possible to study the properties of bound-state systems directly via their interaction with hard probe photons  11  , thereby providing insight into the dynamics underlying the formation of composite states  12  -  14  .\nTheoretical studies have shown that the cross section for SIDIS depends strongly on the transverse momentum k_T of the outgoing hadrons  15  -  17  . It was found that the dependence of the cross sections on k_T could be used to discriminate among different theoretical approaches  18  -  20  . For example, calculations based on the standard DGLAP formalism  21  predict a strong increase of the cross section with increasing k_T  22  while those employing the CCFM evolution equations  23  lead to much weaker dependences  24  . \n \n 2 Experimentally measured quantities",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Hadronization in semi - inclusive depth - inelastic scattering on nuclei . Abstract : We report the results for hadron production in semiinclusive DIS off nuclei at large Bjorken x and low Q ^ 2 , obtained with the HERMES experiment using data taken between 1997 and 2002 .The examination is conducted within the framework of collinear factorisation and the modified perturbative methodology to nuclear shadowing developed by Frankfurt et al . . We see that the reported disruption of leading neutron production relative to deuterium can be described by nuclear effects alone without invoking any additional process such as intrinsic charm or gluon saturation . In addition we encounter an enhancement of odd electron production which cannot be described by traditional partonic models but might be due to the presence of intrinsic strangeness in the proton wave function .1 Introduction Semi - inclusive shallow - inelastic lepton - nucleus scattering ( SIDIS ) has been studied significantly over numerous years both experimentally 1 - 6 and theoretically 7 8 9 . This process provides knowledge about the quark shape of the target nucleus through measurements of last state particles generated in association with the scattered lepton .At high values of Bjorken - x , where the strikes quarks are extremely virtual , SIDIS probes the transfer region between the non - perturbative regime dominated by confinement physics and the perturbative domain dominated by short - distance interactions 10 . In this kinematic range it becomes possible to study the properties of bound - state systems directly via their interaction with hard probe photons 11 , thereby providing information into the dynamics underlying the formation of composite states 12 - 14 .Theoretical experiments have shown that the cross section for SIDIS relies highly on the transverse momentum k _ T of the outgoing hadrons 15 - 17 . It was shown that the dependence of the cross sections on k _ T might be used to discriminate among different conceptual approaches 18 - 20 .For instance , analyses based on the standard DGLAP formalism 21 predict a large rise of the cross area with expanding k _ T 22 while those adopting the CCFM evolution formula 23 lead to considerably weaker dependences 24 . 2 Experimentally recorded quantities",
        "rewrite_text": "**Title: Hadronization in Semi-Inclusive Deep-Inelastic Scattering on Nuclei**\n\n**Abstract:** This article presents findings on hadron production in semi-inclusive deep-inelastic scattering (DIS) off nuclei, specifically at high Bjorken x and low Q², derived from data collected by the HERMES experiment between 1997 and 2002. The analysis is framed within the context of collinear factorization and a modified perturbative approach to nuclear shadowing, as proposed by Frankfurt et al. Our results indicate that the observed suppression of leading neutron production in comparison to deuterium can be attributed solely to nuclear effects, negating the need for additional mechanisms such as intrinsic charm or gluon saturation. Furthermore, we observe an unexpected enhancement in the production of odd electrons, a phenomenon that traditional partonic models fail to explain. This enhancement may be linked to the presence of intrinsic strangeness within the proton's wave function.\n\nThe study of semi-inclusive shallow-inelastic lepton-nucleus scattering (SIDIS) has garnered significant attention over the years, both experimentally and theoretically. This process offers insights into the quark structure of target nuclei by analyzing the final state particles produced alongside the scattered lepton. At elevated Bjorken x values, where the struck quarks exhibit high virtuality, SIDIS serves as a probe into the transition region between non-perturbative confinement physics and perturbative short-distance interactions. This kinematic regime allows for direct investigation of bound-state systems through their interactions with hard probe photons, thereby shedding light on the dynamics involved in the formation of composite states.\n\nTheoretical models suggest that the SIDIS cross section is highly sensitive to the transverse momentum (k_T) of the produced hadrons. Variations in k_T can be instrumental in distinguishing between different theoretical frameworks. For example, analyses grounded in the standard DGLAP formalism predict a significant increase in the cross section with rising k_T, while those employing the CCFM evolution approach suggest a much weaker dependence. This study aims to deepen the understanding of hadronization processes in nuclear environments and contribute to the broader discourse on quark dynamics in composite systems.",
        "ori-fast-z-score": -1.1971303267014333,
        "water-fast-z-score": 5.728012446065256,
        "rewrite-fast-z-score": 0.47733437050543803
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Acyclicity of Preferences, Nash Equilibria, and Subgame Perfect Equilibria: a Formal and Constructive Equivalence .\nAbstract:\nWe show that the set of equilibria in any finite extensive-form game is equivalent to the set of subgame perfect equilibria (SPE) if preferences are acyclic.  We also provide an algorithm for computing SPE when preferences are acyclic.   Our results generalize previous work on equivalence between Nash equilibrium and subgame perfection by showing that this equivalence holds even with incomplete information about payoffs. The main idea behind our proof is to use the concept of dominance paths introduced by Harsanyi and Selten  1988  . \nTheorem 1. Let G be a finite extensive form game where each player has acyclic preferences over his or her actions at every information set. Then there exists one and only one subgame-perfect equilibrium.\n\nCorollary 2. If all players have acyclic preferences then the set of Nash equilibria coincides with the set of subgame-perfect equilibria. \n\nTheorem 3. For any finite extensive form game G = <N, (Ai), T , u> such that each player i ∈ N has acyclic preferences, we can compute the unique subgame-perfect equilibrium using O(n3m) time and space, where n is the number of nodes in T and m is the maximum number of children per node in T.  \n\nIn order to prove Theorem 1, we first introduce some definitions and notation.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Acyclicity of Preferences , Nash Equilibria , and Subgame Perfect Equilibria : a Formal and Constructive Equivalence . Abstract : We see that the set of equilibria in any finite extensive - form game is analogous to the set of subgame perfect equilibria ( SPE ) if preferences are acyclic .We additionally offer an algorithm for computing SPE when preferences are acyclic . Our results generalize past work on equivalence between Nash equilibrium and subgame perfection by showing that this equivalence holds even with insufficient information about payoffs .The main idea behind our proof is to use the notion of superiority paths introduced by Harsanyi and Selten 1988 . Theorem 1 .Let G be a finite extensive form game where each player has acyclic choices over his or her actions at every knowledge set . Then there exists one and only one subgame - perfect equilibrium .Corollary 2 . If all players have acyclic choices then the set of Nash equilibria coincides with the set of subgame - perfect equilibria .Theorem 3 . For any finite extensive form game G = < N , ( Ai ) , T , u > such that each player i ∈ N has acyclic preferences , we can compute the unique subgame - perfect equilibrium using O ( n3m ) time and space , where n is the number of nodes in T and m is the maximum number of children per node in T . In order to prove Theorem 1 , we first introduce some definitions and notation .",
        "rewrite_text": "**Title:** Acyclicity of Preferences, Nash Equilibria, and Subgame Perfect Equilibria: A Formal and Constructive Equivalence\n\n**Abstract:** This article explores the relationship between equilibria in finite extensive-form games and subgame perfect equilibria (SPE) under the condition of acyclic preferences. We demonstrate that when players' preferences are acyclic, the set of Nash equilibria aligns with the set of SPE, establishing a significant equivalence in game theory. Furthermore, we present a constructive algorithm designed to compute the SPE in scenarios where preferences are acyclic, thereby enhancing the practical applicability of our findings. Our research builds upon previous studies that have examined the connection between Nash equilibria and subgame perfection, extending the equivalence to cases where information about payoffs may be limited. The core of our proof leverages the concept of superiority paths, a framework introduced by Harsanyi and Selten in 1988, which facilitates a deeper understanding of strategic interactions in extensive-form games. \n\nWe formally state Theorem 1, which asserts that for any finite extensive-form game G, where each player exhibits acyclic preferences across their action choices at every knowledge set, there exists a unique subgame-perfect equilibrium. Additionally, Corollary 2 confirms that if all players maintain acyclic preferences, the Nash equilibria will coincide with the subgame-perfect equilibria. Theorem 3 further details that for any finite extensive-form game G = < N, (Ai), T, u >, where each player i ∈ N possesses acyclic preferences, the unique SPE can be computed in O(n^3m) time and space complexity, with n representing the number of nodes in the game tree T and m denoting the maximum number of children per node. To substantiate Theorem 1, we begin by establishing essential definitions and notations that will guide our analysis. This work not only contributes to theoretical advancements in game theory but also provides practical tools for computing equilibria in complex strategic environments.",
        "ori-fast-z-score": -0.9761870601839528,
        "water-fast-z-score": 2.372321010475645,
        "rewrite-fast-z-score": -0.09016696346674323
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Un Resultat Gravimetrique pour la Renaissance de la Theorie Corpusculaire  An Experimental Gravimetric Result for the Revival of Corpuscular Theory  .\nAbstract:\nLa gravite est une force fondamentale qui agit sur tous les corps materiels, et dont l action se manifeste par l attraction entre ces derniers. La gravitation newtonienne ne peut expliquer que le phenomene de la chute des corps vers un point unique au centre du systeme solaire. Les theories relativistes d Einstein ont permis d expliquer ce phenomene en introduisant la notion d espace-temps courbe. Cependant, il existe d autres phenomenes physiques tels que l effet Casimir ou encore celui de la pression de radiation electromagnetique qui necessitent l introduction d une nouvelle dimension dans l espace temps. Dans cette these nous proposons d introduire une nouvelle dimension spatiale afin d etendre la geometrie euclidienne classique. Nous montrons alors qu il est possible d interpreter la gravite comme une manifestation macroscopique d un effet microscopique lie aux fluctuations quantiques de l espacetemps. Cette interpretation permet ainsi de retrouver les predictions theoriques de la physique actuelle concernant la dynamique des astres tout en proposant une explication rationnelle a certains phenomenes physiques non expliques jusqu a present.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Un Resultat Gravimetrique pour la Renaissance de la Theorie Corpusculaire An Experimental Gravimetric Result for the Revival of Corpuscular Theory . Abstract : La gravite est le power fondamentale qui agit à tous les corps materiels , et dont l action se manifeste par l attraction entre ces derniers .La gravitation newtonienne ne peut expliquer que le phenomene de la chute des corps vers un place unique au centre le systeme solaire . Les theories relativistes d Einstein ont permis d expliquer ce phenomene en introduisant la conception d espace - temps courbe .Cependant , el existe d autres phenomenes physiques tels se l effet Casimir ou encore celui de la pression de energy electromagnetique qui necessitent l introduction d la nouvelle dimension sur l espace temps . Dans cette these nous proposons d introduire une nouvelle dimension spatiale afin d etendre la geometrie euclidienne classique .Nous montrons alors qu il est possible d interpreter la gravite comme une manifestation macroscopique d un effet microscopique lie à fluctuations quantiques de l espacetemps . Cette interpretation permet ainsi de retrouver les predictions theoriques de la physique actuelle concernant le dynamique du astres tout en proposant une explication rationnelle a certains phenomenes physiques non expliques jusqu a present .",
        "rewrite_text": "**Title:** An Experimental Gravimetric Result for the Revival of Corpuscular Theory\n\n**Abstract:** Gravity is the fundamental force that acts on all material bodies, manifesting itself through the attraction between them. While Newtonian gravity adequately describes the phenomenon of objects falling toward a singular point at the center of the solar system, it falls short in addressing other complex physical phenomena. Einstein's relativistic theories have advanced our understanding by introducing the concept of curved spacetime, yet there remain unresolved phenomena, such as the Casimir effect and electromagnetic energy pressure, that suggest the need for an additional dimension in spacetime. In this paper, we propose the introduction of a new spatial dimension to extend classical Euclidean geometry. We demonstrate that gravity can be interpreted as a macroscopic manifestation of a microscopic effect linked to quantum fluctuations in spacetime. This interpretation not only aligns with current theoretical predictions regarding the dynamics of celestial bodies but also offers a rational explanation for certain previously unexplained physical phenomena. By exploring this novel approach, we aim to bridge gaps in existing theories and provide a more comprehensive understanding of gravitational interactions, potentially leading to significant advancements in both theoretical physics and experimental methodologies. Our findings suggest that revisiting and revising the corpuscular theory of gravity could yield fruitful insights into the nature of the universe and its fundamental forces.",
        "ori-fast-z-score": -0.629940788348712,
        "water-fast-z-score": 2.8977276264040754,
        "rewrite-fast-z-score": 0.29559878344928797
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  An absorption origin for the soft excess in Seyfert 1 AGN .\nAbstract:\nWe present new results on the X-ray spectrum and variability properties of Mrk 509, one of the brightest Seyfert galaxies observed by XMM-Newton. We find that its 0.5-10 keV continuum is well described by an absorbed power law with Γ = 2.1 ± 0.2 (χ2/dof=111/101) plus a reflection component modeled as a PEXRAV model with R=0.7-1.0 and NH=10-23×1022 cm-2. The best-fit parameters are consistent within errors to those found previously using Chandra data alone. No significant spectral changes were detected between different epochs separated by several months apart. However, we do detect strong flux variations at all energies during our observation period. In particular, there was a factor of 3 increase in the hard band count rate over about 20 ks followed by a slower decay back towards the initial level. This behavior can be explained if the source has been caught in a transition state where the accretion disk luminosity increased rapidly due to some instability or perturbation.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : An absorption origin for the soft excess in Seyfert 1 AGN . Abstract : We report new data on the X - ray spectrum and variability properties of Mrk 509 , one of the brightest Seyfert galaxies studied by XMM - Newton .We see that its 0 . 5 - 10 keV continuum is well described by an absorption power law with Γ = 2 . 1 ± 0 . 2 ( χ2 / dof = 111 / 101 ) plus a mirror element modeled as a PEXRAV model with R = 0 . 7 - 1 . 0 and NH = 10 - 23×1022 centimetres - 2 . The best - fitting values are compatible within errors to those identified previously used Chandra data alone .No meaningful spectral changes were detected between various epochs separated by many months separated . However , we do discover powerful flux variations at all energies during our observation term .In particular , there was a factor of 3 gain in the hard band count rate over about 20 ks followed by a slower decay forward towards the first level . This phenomenon can be understood if the source has been caught in a transition state where the accretion disk luminosity increased rapidly due to some instability or perturbation .",
        "rewrite_text": "We present new findings regarding the X-ray spectrum and variability characteristics of Mrk 509, a prominent Seyfert 1 active galactic nucleus (AGN) observed using the XMM-Newton satellite. Our analysis reveals that the 0.5 - 10 keV continuum of Mrk 509 can be accurately modeled by an absorption power law with a photon index of Γ = 2.1 ± 0.2 (χ²/dof = 111/101). Additionally, we incorporate a mirror component represented by a PEXRAV model, with reflection fraction R ranging from 0.7 to 1.0 and hydrogen column density NH estimated at 10 - 23 × 10²² cm⁻². These optimal fitting parameters are consistent with those derived from previous observations using Chandra data, indicating a stable spectral profile over time. Notably, we did not observe significant spectral variations across different epochs, even when separated by several months. However, our observations did reveal substantial flux variability across all energy bands throughout the monitoring period. Specifically, we recorded a threefold increase in the hard band count rate over approximately 20 ks, followed by a gradual decline back to baseline levels. This behavior suggests that Mrk 509 may have been in a transitional state, where the luminosity of the accretion disk experienced a rapid surge due to an instability or perturbation. Our findings contribute to the understanding of the soft excess phenomenon in Seyfert 1 AGN and highlight the dynamic nature of their X-ray emissions.",
        "ori-fast-z-score": -0.44172610429938614,
        "water-fast-z-score": 5.521576303742327,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Fractal analysis for sets of non-differentiability of Minkowski s question mark function .\nAbstract:\nIn this work, we study the fractal dimension and Hausdorff measure of the set of points where  is not differentiable in . We show that these measures are equal to one half times the corresponding values on the unit interval with Lebesgue measure.  This result generalizes previous results by Jarník (for ) and Marstrand (for ). The proof relies on an explicit formula for the Fourier transform of , which was obtained recently by Fejér and Riesz. In addition, we prove that the same holds true if instead of the Lebesgue measure on the unit interval, we consider any other probability measure whose support has positive length. Finally, we give some applications of our main theorem to the theory of lacunary series. Let be the space of all complex-valued functions defined on the real line equipped with the topology of uniform convergence on compact subsets of .\nLet denote the class of all infinitely differentiable functions on such that their derivatives satisfy the Hölder condition with exponent greater than or equal to 1 at every point of the domain. \nFor each integer let us define the operator as follows: \nwhere denotes the th derivative of .  It can easily be seen that is continuous linear functional on .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Fractal analysis for sets of non - differentiability of Minkowski s question mark function . Abstract : In this study , we study the fractal dimension and Hausdorff measure of the group of points where is not differentiable in .We see that these measures are equal to one half times the equivalent values on the unit interval with Lebesgue measure . This result generalizes earlier findings by Jarník ( for ) and Marstrand ( for ) .The proof relies on an explicit formula for the Fourier transform of , which was obtained recently by Fejér and Riesz . In addition , we prove that the same holds true if instead of the Lebesgue measure on the unit interval , we define any other probability measure whose support has positive length .Finally , we give some applications of our major principle to the notion of lacunary series . Let be the space of all complex - valued functions defined on the real line equipped with the topology of uniform convergence on compact subsets of .Let define the class of all infinitely differentiable functions on such that their derivatives satisfy the Hölder condition with exponent greater than or equal to 1 at every point of the domain . For each integer take us define the operator as follows : where denotes the th derivative of .It can easily be saw that is continuous continuous functional on .",
        "rewrite_text": "**Title:** Fractal Analysis of Non-Differentiability Sets in Minkowski's Question Mark Function\n\n**Abstract:** This article investigates the fractal dimension and Hausdorff measure associated with the set of points where Minkowski's question mark function is not differentiable. Our findings reveal that these measures are equivalent to one-half of the corresponding values calculated on the unit interval using Lebesgue measure. This result extends previous work by Jarník and Marstrand, who explored similar concepts in different contexts. The proof of our main result is grounded in a recently derived explicit formula for the Fourier transform of the question mark function, as established by Fejér and Riesz. Furthermore, we demonstrate that our conclusions remain valid when substituting the Lebesgue measure on the unit interval with any other probability measure whose support possesses positive length. \n\nIn addition to these theoretical advancements, we explore practical applications of our primary findings in relation to lacunary series. We define a space of complex-valued functions on the real line, equipped with the topology of uniform convergence on compact subsets. Within this framework, we introduce a class of infinitely differentiable functions whose derivatives adhere to the Hölder condition with an exponent of at least 1 at every point in their domain. For each integer, we define an operator that acts on these functions, where the operator applies to the nth derivative. It can be readily observed that this operator is a continuous functional on the defined space. Our research not only contributes to the understanding of fractal dimensions in the context of non-differentiable functions but also opens avenues for further exploration in related mathematical fields.",
        "ori-fast-z-score": 0.7181848464596079,
        "water-fast-z-score": 3.7567808109943908,
        "rewrite-fast-z-score": 0.4662524041201569
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Spectral Methods for Numerical Relativity .\nAbstract:\nThe book is available at the following URL: http://arxiv.org/abs/gr-qc/0405033 The book contains an introduction to numerical relativity, including basic concepts and techniques in general relativity as well as some advanced topics such as black hole excision methods.  It also includes detailed descriptions on how to implement these algorithms using spectral methods (Fourier or Chebyshev expansions). This book will be useful not only for graduate students but also for researchers who are interested in applying spectral methods to solve problems in astrophysics. Authors:  Yi-Kai Shu, Jian-Xin Liu, Zhi-Hong Zhang, Yu-Ting Wu, Xiao-Liang Zhao, Wen-Jun Ma, Hong-Yuan Chen, Jie-Qiu Shi, Jun-Zhuo Wang, Yong-Sheng Zhou, Ming-Dao Li, Qing-Wei Guo, Xin-Bin Huang",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Spectral Methods for Numerical Relativity . Abstract : The volume is accessible at the following URL : www : / / arxiv . org / abs / gr - qc / 0405033 The text includes an introduction to numerical gravity , comprising fundamental concepts and techniques in general relativity as also as some advanced topics such as black hole excision methods .It additionally contains detailed explanations on how to execute these algorithms utilizing spectral algorithms ( Fourier or Chebyshev expansions ) . This book will be valuable not only for graduate scholars but also for researchers who are concerned in implementing spectral algorithms to solve difficulties in astrophysics .Authors : Yi - Kai Shu , Jian - Xin Liu , Zhi - Hong Zhang , Yu - Ting Wu , Xiao - Liang Zhao , Wen - Jun Ma , Hong - Yuan Chen , Jie - Qiu Shi , Jun - Zhuo Wang , Yong - Sheng Zhou , Ming - Dao Li , Qing - Wei Guo , Xin - Bin Huang",
        "rewrite_text": "Title: Spectral Methods for Numerical Relativity\n\nAbstract: This article, available at www.arxiv.org/abs/gr-qc/0405033, presents a comprehensive overview of numerical relativity, focusing on both foundational concepts and advanced methodologies within the field. The authors delve into essential principles of general relativity, providing a solid grounding for readers new to the subject. In addition to the basics, the text explores sophisticated topics such as black hole excision methods, which are crucial for accurately simulating gravitational phenomena in complex scenarios. \n\nA significant portion of the article is dedicated to the implementation of spectral algorithms, specifically Fourier and Chebyshev expansions, which are powerful tools for solving differential equations that arise in numerical relativity. The authors offer detailed guidance on executing these algorithms, making the content accessible to both graduate students and seasoned researchers. \n\nThe insights provided in this work are particularly beneficial for those interested in applying spectral methods to tackle challenges in astrophysics, as they enhance computational efficiency and accuracy in simulations. The collaborative effort of the authors—Yi-Kai Shu, Jian-Xin Liu, Zhi-Hong Zhang, Yu-Ting Wu, Xiao-Liang Zhao, Wen-Jun Ma, Hong-Yuan Chen, Jie-Qiu Shi, Jun-Zhuo Wang, Yong-Sheng Zhou, Ming-Dao Li, Qing-Wei Guo, and Xin-Bin Huang—ensures a rich and diverse perspective on the subject matter. This article serves as a valuable resource for anyone looking to deepen their understanding of numerical relativity and the application of spectral techniques in this dynamic field.",
        "ori-fast-z-score": -2.2188007849009166,
        "water-fast-z-score": 4.160251471689219,
        "rewrite-fast-z-score": 1.165543034828717
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Privacy - an Issue for eLearning? A Trend Analysis Reflecting the Attitude of European eLearning Users .\nAbstract:\nThe purpose of this study is to investigate how privacy issues are perceived by users in Europe and what their attitude towards these issues is, when using online learning environments (OLE). The research questions are: \nWhat do students think about privacy?\nHow does student s perception change over time?\nWhich factors influence students  attitudes on privacy? How can we measure it? What are the implications for OLE developers? This article presents results from two surveys conducted with university students at different universities across Europe between 2004-2006. It shows that there has been little change in the way students perceive privacy since 2004. Students  concerns have not changed significantly either; they still worry most about personal information being shared without permission or control. However, some differences were found among countries. In addition, the findings show that gender plays a role as well as age. Finally, the authors discuss possible reasons behind the observed trends and provide recommendations for future work.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Privacy - an Issue for eLearning ? A Trend Analysis Reflecting the Attitude of European eLearning Users .Abstract : The purpose of this study is to examine how privacy issues are seen by users in Europe and what their attitude towards these problems is , when using internet learning habitats ( OLE ) . The survey challenges are : What do students think about privacy ?How does student s perception shift over time ? Which variables influence pupils beliefs on privacy ?How can we measure it?What are the implications for OLE developers?This page presents scores from two polls conducted with university students at different schools across Europe between 2004 - 2006 . It demonstrates that there has been less shift in the way pupils understand privacy since 2004 .Students issues have not altered significantly either ; they still concern most about personal data being shared without authorization or authority . However , some differences were found among countries .In addition , the discoveries reveal that gender plays a role as well as aging . Finally , the articles discuss possible reasons behind the reported developments and include recommendations for future projects .",
        "rewrite_text": "**Title:** Privacy Concerns in eLearning: A Trend Analysis of European Users' Attitudes\n\n**Abstract:** This study investigates the perceptions of privacy issues among eLearning users in Europe, focusing on their attitudes towards these challenges within online learning environments (OLE). The research aims to address several key questions: How do students perceive privacy? How has their perception evolved over time? What factors influence their beliefs regarding privacy? How can these perceptions be effectively measured? Additionally, what implications do these findings have for developers of online learning platforms? \n\nTo explore these questions, the study analyzes data from two surveys conducted with university students across various European institutions between 2004 and 2006. The results indicate that there has been minimal change in students' understanding of privacy since 2004. Concerns regarding the unauthorized sharing of personal data remain predominant among students, with little variation in these issues over the years. However, the analysis reveals notable differences in privacy perceptions across different countries. \n\nFurthermore, the findings suggest that demographic factors, such as gender and age, significantly influence students' attitudes towards privacy. The article concludes by discussing potential reasons for the observed trends and offers recommendations for future research and the development of online learning environments. These insights are crucial for OLE developers aiming to address privacy concerns and enhance user trust in digital education platforms.",
        "ori-fast-z-score": -0.52999894000318,
        "water-fast-z-score": 7.378647873726218,
        "rewrite-fast-z-score": -1.299867367239363
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Phonon-mediated decay of an atom in a surface-induced potential .\nAbstract:\nWe study the phonon-assisted relaxation dynamics of an excited atom trapped by a surface-induced optical lattice (SIL). We show that, for sufficiently deep SILs and low temperatures, the atom can be localized within one well of the SIL with high probability. In this regime we find that the lifetime of the atom is determined by its coupling to bulk acoustic phonons via the deformation potential interaction. The resulting lifetimes are found to agree very well with experimental results obtained on cesium atoms trapped at the interface between two dielectric materials. \n \n Introduction \n \n Surface-induced lattices have been used extensively over recent years as a tool for trapping ultracold atoms near surfaces  1–3  . These systems provide a unique opportunity to explore quantum many-body phenomena such as superfluidity  4  , supersolids  5  , and topological insulators  6  using cold-atom experiments  7–9  .\n \nIn these experiments, laser light is focused onto the surface of a transparent material which creates periodic potentials along the direction normal to the surface  10  . This leads to the formation of standing waves known as surface-induced optical lattices (SIL)  11  . Atoms confined inside these lattices experience strong confinement perpendicular to the surface while being weakly coupled to the underlying substrate  12  . As a result, they behave like free particles moving in three dimensions  13  . \n \n While there has been significant progress towards understanding the properties of atoms trapped in SILs  14–18  , relatively little attention has been paid so far to their relaxation dynamics  19, 20  . Here we consider the case where an atom is initially prepared in an excited state |e⟩ above some energy threshold E0. If the initial excitation energy exceeds the depth of the SIL V0 then it will escape into the continuum  21  . However if the initial energy lies below E0 but still exceeds the recoil energy ER = 2 kL2 / 2mL  22  , where mL denotes the mass of the atom and kL is the wavevector associated with the lattice periodicity, then the atom may relax back down to the ground state |g⟩ through emission of a",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Phonon - mediated degradation of an element in a surface - triggered potential . Abstract : We research the phonon - aided relaxation behavior of an excited element trapped by a surface - mediated optical lattice ( SIL ) .We see that , for enough deep SILs and low temperatures , the atom can be localized within one well of the SIL with high probability . In this regime we find that the life of the atom is calculated by its interaction to bulk acoustic phonons via the deformation potential interaction .The resulting lifetimes are found to agree very best with experimental results derived on cesium atoms trapped at the interface between two dielectric materials . Introduction Surface - induced lattices have been used widely over recent months as a technique for trapping ultracold atoms near surfaces 1 – 3 .These systems create a unique opportunity to examine quantum several - bodies phenomena such as superfluidity 4 , supersolids 5 , and topological insulators 6 using cold - atom experiments 7 – 9 . In these experiments , laser light is focused onto the surface of a transparent material which forms periodic potentials along the direction regular to the surface 10 .This leads to the formation of standing currents termed as surface - mediated optical lattices ( SIL ) 11 . Atoms confined inside these lattices experience strong confinement parallel to the surface while being weakly linked to the underlying substrate 12 .As a result , they react like free particles moving in three dimensions 13 . While there has been significant progress towards studying the properties of atoms trapped in SILs 14 – 18 , surprisingly little attention has been paid so far to their relaxation interactions 19 , 20 .Here we imagine the case where an element is initially made in an excited state | e ⟩ above some energy threshold E0 . If the first excitation energy reaches the height of the SIL V0 then it will flee into the continuum 21 .However if the first energy falls below E0 but still exceeds the recoil power ER = 2 kL2 / 2mL 22 , where mL denotes the mass of the atom and kL is the wavevector related with the lattice periodicity , then the atom might cool back down to the ground state | g ⟩ through emitted of a",
        "rewrite_text": "**Title:** Phonon-Mediated Degradation of an Element in a Surface-Triggered Potential\n\n**Abstract:** This study investigates the phonon-assisted relaxation dynamics of an excited atom confined within a surface-induced optical lattice (SIL). Our findings indicate that, under conditions of sufficiently deep SILs and low temperatures, the atom can be localized with a high probability within a single potential well of the SIL. In this localized regime, the atom's lifetime is predominantly determined by its interactions with bulk acoustic phonons through the deformation potential mechanism. Notably, the calculated lifetimes align closely with experimental observations involving cesium atoms situated at the interface of two dielectric materials. \n\nThe introduction of surface-induced lattices has gained traction in recent months as a promising method for trapping ultracold atoms in proximity to surfaces. These innovative systems provide a unique platform for exploring complex quantum phenomena, including superfluidity, supersolids, and topological insulators, through cold-atom experiments. The experimental setup typically involves focusing laser light onto a transparent surface, which generates periodic potentials perpendicular to the surface. This process results in the formation of standing waves known as surface-mediated optical lattices (SILs). Atoms confined within these lattices experience strong confinement parallel to the surface while maintaining a weak coupling to the underlying substrate, allowing them to behave like free particles in three-dimensional space.\n\nDespite significant advancements in understanding the properties of atoms trapped in SILs, there has been a surprising lack of focus on their relaxation interactions. In our analysis, we consider a scenario where an atom is initially excited to a state above a certain energy threshold. If the excitation energy surpasses the height of the SIL, the atom will escape into the continuum. Conversely, if the energy falls below this threshold but remains above the recoil energy, the atom may return to its ground state by emitting a phonon. This research sheds light on the intricate interplay between phonon interactions and atomic relaxation processes in surface-induced optical lattices, paving the way for further exploration of quantum dynamics in these systems.",
        "ori-fast-z-score": -1.0932163332202425,
        "water-fast-z-score": 6.887026769553818,
        "rewrite-fast-z-score": 1.1067971810589328
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Habitat Segregation between Lyman Break Galaxies and Lyman alpha Emitters around a QSO at z~5 .\nAbstract:\nWe present the results on the spatial distribution of galaxies in the vicinity (<5 Mpc) of a bright quasar at redshift 5.2, using deep near-infrared imaging data taken with Subaru/Suprime-Cam. We find that there is an apparent segregation between Lyman break galaxies (LBGs), which are selected by their rest-frame UV colors, and Lyman alpha emitters (LAEs). The LBGs show a clear overdensity toward the quasar position while LAEs do not have such a concentration. This result suggests that the physical conditions for star formation may be different between these two populations. \n \n Keywords: galaxy evolution, quasars, clustering, infrared observations, high-z universe, Lyman break galaxies, Lyman alpha emitters \n \n \n \n 1 Introduction \n \n Quasars play important roles as probes to study the early Universe because they can provide us information about the intergalactic medium through absorption lines observed in their spectra. In addition, quasars themselves emit strong radiation over wide wavelength ranges, so we can use them as background sources to investigate the properties of surrounding objects. For example, it has been suggested that quasars trigger starburst activities in nearby galaxies via intense ultraviolet (UV) radiation and/or gravitational interactions (e.g., Hopkins et al. 2006) . \n \n Recently, several studies have investigated the environments of high-redshift quasars based on multi-wavelength surveys. These include optical/near-infrared spectroscopy (e.g., Adelberger & Steidel 2005; Venemans et al. 2007) , radio continuum emission (e.g., Carilli et al. 2007; Overzier et al. 2008 ) and X-ray emission (e.g,. Brandt et al. 2002; Gilli et al. 2003 ) . However, most previous works focused only on relatively small scales (<1 Mpc) due to limited angular resolution or sensitivity of telescopes used. On larger scales, some authors reported possible evidence for large-scale structures associated with quasars (e.g., Kurk et al. 2000; Pentericci et al",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Habitat Segregation between Lyman Break Galaxies and Lyman alpha Emitters around a QSO at z ~ 5 . Abstract : We report the results on the spatial distribution of stars in the vicinity ( < 5 Mpc ) of a bright quasar at redshift 5 . 2 , using deep near - infrared imaging information taken with Subaru / Suprime - Cam .We see that there is an apparent segregation between Lyman break galaxies ( LBGs ) , which are chosen by their rest - frame UV colors , and Lyman alpha emitters ( LAEs ) . The LBGs exhibit a clear overdensity toward the quasar position while LAEs do not have such a concentration .This result suggests that the physical conditions for star formation might be different between these two communities . Keywords : universe progression , quasars , clustering , infrared observations , low - z galaxy , Lyman break galaxies , Lyman alpha emitters 1 Introduction Quasars serve useful roles as probes to study the early Universe because they can provide us information about the intergalactic medium through absorption patterns observed in their spectra .In addition , quasars themselves emit strong radiation over broad wavelength ranges , so we can using them as background sources to examine the properties of neighbouring items . For instance , it has been proposed that quasars activate starburst activities in nearby galaxies via intense ultraviolet ( UV ) rays and / or gravitational interactions ( e . g . , Hopkins et al .2006 ) . Recently , various studies have researched the conditions of high - redshift quasars based on multi - wavelength polls .These include optical / near - infrared spectroscopy ( e . g . , Adelberger & Steidel 2005 ; Venemans et al . 2007 ) , radio continuum emission ( e . g . , Carilli et al .2007 ; Overzier et al . 2008 ) and X - ray radiation ( e . g , .Brandt et al . 2002 ; Gilli et al .2003 ) . However , most prior studies focused only on relatively small scales ( < 1 Mpc ) resulting to limited radial resolution or sensitivity of telescopes used .On larger scales , some researchers reported proposed proof for large - scale structures involved with quasars ( e . g . , Kurk et al . 2000 ; Pentericci et al",
        "rewrite_text": "**Title:** Habitat Segregation between Lyman Break Galaxies and Lyman Alpha Emitters around a QSO at z ~ 5\n\n**Abstract:** This study presents findings on the spatial distribution of stars in the vicinity of a luminous quasar at redshift 5.2, utilizing deep near-infrared imaging data obtained from Subaru/Suprime-Cam. Our analysis reveals a distinct segregation between Lyman Break Galaxies (LBGs), identified through their rest-frame ultraviolet colors, and Lyman Alpha Emitters (LAEs). Notably, LBGs demonstrate a significant overdensity in proximity to the quasar, whereas LAEs do not exhibit a similar concentration. This observation implies that the conditions conducive to star formation may differ markedly between these two galaxy populations. \n\nQuasars are invaluable for probing the early Universe, as they provide insights into the intergalactic medium through the absorption features in their spectra. Additionally, their intense radiation across a wide range of wavelengths allows them to serve as background sources for investigating the properties of nearby astronomical objects. Previous research has suggested that quasars may trigger starburst activities in surrounding galaxies due to their powerful ultraviolet (UV) emissions and gravitational influences. Recent investigations have explored the characteristics of high-redshift quasars through multi-wavelength approaches, including optical and near-infrared spectroscopy, radio continuum emissions, and X-ray observations. However, much of the existing literature has concentrated on relatively small spatial scales (less than 1 Mpc), which has limited the radial resolution and sensitivity of the telescopes employed. In contrast, some studies have indicated the presence of large-scale structures associated with quasars, highlighting the need for further exploration of these expansive environments. Our findings contribute to this growing body of knowledge by elucidating the spatial dynamics between LBGs and LAEs in the context of quasar influence, thereby enhancing our understanding of star formation processes in the early Universe.\n\n**Keywords:** universe evolution, quasars, clustering, infrared observations, low-redshift galaxies, Lyman Break Galaxies, Lyman Alpha Emitters.",
        "ori-fast-z-score": 0.24413653763134782,
        "water-fast-z-score": 6.5158101808543485,
        "rewrite-fast-z-score": -1.028991510855053
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Catalan s intervals and realizers of triangulations .\nAbstract:\nWe study the Catalan numbers in connection with the number of ways to realize a given triangulation as an ordered sequence of its diagonals, or equivalently, as a set of non-crossing diagonals. We show that this problem is related to counting certain types of Dyck paths. In particular we prove that for any positive integer n there are exactly C(n) different sequences of diagonals which can be realized by a convex quadrilateral having 2n sides. This result generalizes a theorem due to Motzkin and Straus on the number of diagonalizations of a convex polygon. \nIntroduction\n\nThe Catalan numbers count many combinatorial objects such as binary trees, noncrossing partitions, spanning trees, etc., see e.g.   1, 2  . The present work deals with another class of Catalan-like objects: triangulations of polygons (see Figure 1 ). A triangulation T of a simple polygon P is defined as follows: it consists of all edges of P together with some additional diagonals connecting pairs of vertices of P so that each interior angle of P becomes at least 90 degrees after adding these diagonals. It follows immediately that every edge belongs to one and only one diagonal of T .\nIn  3  , Motzkin and Straus  celebrated theorem states that if D denotes the set of diagonals of a convex polygon Q then |D| = 2|Q|. They also proved that the number of diagonalizations d(P ) of a convex polygon P equals the number of diagonals of a triangulation of P . \nIt was shown recently  4  that the number of diagonals in a triangulation of a convex quadrilateral is equal to four times the number of diagonals needed to diagonalize the quadrilateral. Thus, the following question arises naturally: What is the relationship between the number of diagonals required to diagonalize a convex quadrilateral and the number of diagonals used in a triangulation?",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Catalan s intervals and realizers of triangulations . Abstract : We research the Catalan numbers in connection with the number of ways to realize a given triangulation as an ordered sequence of its diagonals , or equivalently , as a group of non - crossing diagonals .We see that this question is related to counting particular kinds of Dyck paths . In particular we prove that for any positive integer n there are exactly C ( n ) different sequences of diagonals which can be realized by a convex quadrilateral having 2n sides .This result generalizes a theorem according to Motzkin and Straus on the number of diagonalizations of a convex polygon . Introduction The Catalan numbers count many combinatorial objects such as binary forests , noncrossing partitions , spanning trees , etc . , see e . g .1 , 2 . The present work deals with another class of Catalan - like structures : triangulations of polygons ( view Figure 1 ) .A triangulation T of a simple polygon P is characterized as follows : it consists of all edges of P together with some additional diagonals connecting pairs of vertices of P so that each interior angle of P becomes at least 90 degrees after adds these diagonals . It follows quickly that every edge belongs to one and only one diagonal of T .In 3 , Motzkin and Straus famous theorem holds that if D denotes the group of diagonals of a convex polygon Q then | D | = 2 | Q | . They also proved that the number of diagonalizations d ( P ) of a convex polygon P equals the number of diagonals of a triangulation of P .It was shown recently 4 that the number of diagonals in a triangulation of a convex quadrilateral is equal to four times the number of diagonals needed to diagonalize the quadrilateral . Thus , the following answer arises readily : What is the relationship between the number of diagonals needed to diagonalize a convex quadrilateral and the number of diagonals used in a triangulation ?",
        "rewrite_text": "**Title:** Catalan's Intervals and Realizers of Triangulations\n\n**Abstract:** This study investigates the relationship between Catalan numbers and the various methods of realizing a specific triangulation through an ordered sequence of its diagonals, or equivalently, as a collection of non-crossing diagonals. We establish a connection between this inquiry and the enumeration of certain types of Dyck paths. Our main result demonstrates that for any positive integer \\( n \\), there are precisely \\( C(n) \\) distinct sequences of diagonals that can be realized by a convex polygon with \\( 2n \\) sides. This finding extends the classical theorem by Motzkin and Straus concerning the number of diagonalizations of a convex polygon. \n\nThe Catalan numbers are known to enumerate a wide array of combinatorial structures, including binary trees, non-crossing partitions, and spanning trees. In this paper, we focus on a specific class of Catalan-like structures: the triangulations of polygons. A triangulation \\( T \\) of a simple polygon \\( P \\) is defined as the inclusion of all edges of \\( P \\) along with additional diagonals that connect pairs of vertices, ensuring that each interior angle of \\( P \\) is at least 90 degrees after the addition of these diagonals. It is important to note that each edge of \\( P \\) corresponds to exactly one diagonal in \\( T \\).\n\nMotzkin and Straus's renowned theorem states that if \\( D \\) represents the set of diagonals of a convex polygon \\( Q \\), then the cardinality \\( |D| \\) is equal to \\( 2|Q| \\). They also established that the number of diagonalizations \\( d(P) \\) of a convex polygon \\( P \\) is equivalent to the number of diagonals in a triangulation of \\( P \\). Recent findings indicate that the number of diagonals in a triangulation of a convex quadrilateral is four times the number of diagonals required to diagonalize that quadrilateral. This leads us to a compelling question: What is the precise relationship between the diagonals necessary for diagonalizing a convex quadrilateral and those utilized in its triangulation?",
        "ori-fast-z-score": 0.5669467095138409,
        "water-fast-z-score": 4.3465914396061125,
        "rewrite-fast-z-score": 0.09853292781642932
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Stokes phenomenon in the confluence of the hypergeometric equation using Riccati equation .\nAbstract:\nIn this article, we study the confluence of the second order linear differential equations with variable coefficients and their solutions by applying the method of successive approximations to find the solution of the confluent hypergeometric equation (CHGE). We also use the concept of the Stokes phenomenon for finding the singularities of the CHGE s solutions at infinity. The results are illustrated numerically through examples. Keywords: Confluence of differential equations; Stokes phenomenon; Hypergeometric functions. 1 Introduction In mathematics, the confluent hypergeometic function is defined as the sum of all possible products of n terms taken from an infinite series of complex numbers  1  . It has been used extensively in many fields such as physics  2  , engineering  3  , computer science  4  , statistics  5  , etc., because it can be applied to solve various types of problems  6  .\nThe confluent hypergeometric function is denoted by ${}_1F_1$ or ${_2F_1}$ depending on whether the first argument is equal to one or not  7, 8  . For example, if the first argument equals one then the function is called the Kummer confluent hypergeometric function:",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Stokes effect in the confluence of the hypergeometric equation using Riccati equation . Abstract : In this article , we study the confluence of the second order continuous differential equations with variable equations and their solutions by using the method of successive approximations to find the solve of the confluent hypergeometric equation ( CHGE ) .We additionally using the idea of the Stokes effect for finding the singularities of the CHGE s solutions at infinity . The results are shown numerically through examples .Keywords : Confluence of differential equations ; Stokes effect ; Hypergeometric functions . 1 Introduction In mathematics , the confluent hypergeometic function is characterized as the sum of all possible products of n terms obtained from an infinite sequence of complex numbers 1 .It has been used heavily in different fields such as science 2 , engineering 3 , computer science 4 , statistics 5 , etc . , because it can be applied to solve different kinds of problems 6 . The confluent hypergeometric function is denoted by $ { } _ 1F _ 1 $ or $ { _ 2F _ 1 } $ depending on whether the first argument is equal to one or not 7 , 8 .For instance , if the first argument equals one then the function is dubbed the Kummer confluent hypergeometric function :",
        "rewrite_text": "**Title:** The Stokes Effect in the Confluence of the Hypergeometric Equation Using the Riccati Equation\n\n**Abstract:** This article investigates the confluence of second-order continuous differential equations with variable coefficients and their corresponding solutions, employing the method of successive approximations to address the confluent hypergeometric equation (CHGE). We explore the application of the Stokes effect to identify the singularities of the solutions to the CHGE at infinity. The Stokes effect, which describes the behavior of differential equations near their singular points, plays a crucial role in understanding the asymptotic properties of these solutions. Through a series of numerical examples, we illustrate our findings and demonstrate the efficacy of our approach. The confluent hypergeometric function, a fundamental mathematical construct, is characterized as the sum of all possible products of n terms derived from an infinite sequence of complex numbers. Its versatility has led to widespread use across various disciplines, including science, engineering, computer science, and statistics, due to its ability to address a diverse range of problems. The confluent hypergeometric function is typically denoted as $_1F_1$ or $_2F_1$, depending on whether the first argument is equal to one. Notably, when the first argument equals one, the function is referred to as the Kummer confluent hypergeometric function. Our study not only enhances the understanding of the confluence of differential equations but also provides insights into the application of the Stokes effect in analyzing the behavior of hypergeometric functions. The results presented herein contribute to the broader mathematical discourse surrounding these functions and their applications, paving the way for future research in this area.\n\n**Keywords:** Confluence of differential equations; Stokes effect; Hypergeometric functions.",
        "ori-fast-z-score": -0.329292779969071,
        "water-fast-z-score": 4.1461399144838555,
        "rewrite-fast-z-score": 1.1917080461366747
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Analytical evaluation of the X-ray scattering contribution to imaging degradation in grazing-incidence X-ray telescopes .\nAbstract:\nWe present an analytical model for evaluating the effect of X-ray scattering on image quality in grazing incidence X-ray telescopes (GIXTs). The model is based on ray tracing and takes into account both surface roughness and bulk density fluctuations as sources of scattering. We show that, depending on the telescope parameters, either one or both effects can dominate over the other. In addition we demonstrate how our results compare with those obtained by Monte Carlo simulations. Finally, using this model, we evaluate the impact of different levels of surface roughness on the performance of the proposed GIXT mission Athena+. Our analysis shows that even relatively small values of surface roughness may significantly degrade the angular resolution of the instrument. This work was supported by NASA under grant NNX10AD65G issued through the Astrophysics Theory Program. Keywords: X-ray optics, Ray tracing, Surface roughness, Scattering",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Analytical evaluation of the X - ray scattering contribution to visual degradation in grazing - incidence X - ray telescopes . Abstract : We present an analytical model for evaluating the impact of X - ray scattering on image quality in grazing incidence X - ray telescopes ( GIXTs ) .The model is based on ray tracing and takes into consideration both exterior roughness and bulk density fluctuations as sources of scattering . We see that , depending on the telescope parameters , either one or both impacts can dominate over the other .In addition we prove how our findings compare with those achieved by Monte Carlo simulations . Finally , using this simulation , we assess the impact of different amounts of terrain roughness on the performance of the suggested GIXT mission Athena + .Our study shows that even surprisingly low measures of exterior roughness might considerably degrade the angular resolution of the instrument . This effort was supported by NASA under grant NNX10AD65G granted through the Astrophysics Theory Program .Keywords : X - ray optics , Ray tracing , Surface roughness , Scattering",
        "rewrite_text": "Title: Analytical Evaluation of the X-ray Scattering Contribution to Visual Degradation in Grazing-Incidence X-ray Telescopes\n\nAbstract: In this study, we introduce a comprehensive analytical model designed to assess the influence of X-ray scattering on the image quality of grazing incidence X-ray telescopes (GIXTs). Our model employs ray tracing techniques and incorporates factors such as surface roughness and fluctuations in bulk density as potential sources of scattering. The analysis reveals that the significance of these factors can vary based on the specific parameters of the telescope, with one or both effects potentially dominating the scattering contribution. Furthermore, we validate our analytical results by comparing them with outcomes derived from Monte Carlo simulations, demonstrating a strong correlation between the two approaches. To further illustrate the practical implications of our findings, we evaluate the effects of varying levels of terrain roughness on the performance of the proposed GIXT mission, Athena+. Our results indicate that even minimal levels of surface roughness can lead to substantial degradation in the angular resolution of the telescope. This research was conducted with the support of NASA under grant NNX10AD65G, awarded through the Astrophysics Theory Program. The keywords associated with this study include X-ray optics, ray tracing, surface roughness, and scattering, which encapsulate the core themes of our investigation into the challenges faced by GIXTs in achieving optimal image quality.",
        "ori-fast-z-score": -0.562543950463012,
        "water-fast-z-score": 4.024922359499621,
        "rewrite-fast-z-score": 0.09950371902099892
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Towards a New Standard Model for Black Hole Accretion .\nAbstract:\nWe present the first results on black hole accretion using our new numerical scheme, which is based on an implicit-explicit time integration method and uses adaptive mesh refinement (AMR). We study two different models of accretion flows onto Kerr black holes in order to test the robustness of our code against various physical effects such as viscosity, magnetic fields, radiative cooling/heating processes, etc.. In particular we focus on the properties of the flow at large distances from the central object where it becomes supersonic and forms shocks. Our main goal here was to check whether these features are correctly captured by our AMR code. The results show that our code reproduces all known analytical solutions very well. \n \n Keywords: Black holes - General relativity - Numerical methods - Shocks - Supersonic turbulence - Time-dependent simulations \n \n \n \n 1 Introduction \n \n It has been more than 30 years since the discovery of quasars  1  . Since then there have been many theoretical studies trying to explain how supermassive black holes grow so rapidly  2  , but only recently were the first observational data available  3  . These observations suggest that most galaxies contain massive black holes with masses ranging between 10^6 M_sol < M_blackhole < 10^9 M_sol  4  . This poses serious challenges for current theories of galaxy formation because they predict much smaller values for the mass of the central black hole  5  . \n \n One possible solution to this problem could be provided by so-called active galactic nuclei (AGN), i.e., systems containing a supermassive black hole surrounded by an accretion disk  6  . If the gas density in the disk is high enough, the gravitational field of the black hole can cause the infalling matter to lose angular momentum through viscous stresses  7, 8  . As a result, the gas falls towards the center of the system forming a geometrically thin accretion disk  9  . However, if the gas density drops below some critical value, the disk may become unstable  10  or even fragment into clumps  11  . Such instabilities lead to the development of large-scale",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Towards a New Standard Model for Black Hole Accretion . Abstract : We present the first findings on dark hole accretion use our new numerical system , which is based on an implicit - explicit time integration scheme and using adaptive mesh refinement ( AMR ) .We research two different models of accretion flows onto Kerr black holes in order to test the robustness of our code against several physical effects such as viscosity , magnetic fields , radiative cooling / cooling mechanisms , etc . . In particular we focus on the properties of the flow at large distances from the main object where it becomes supersonic and shapes shocks .Our main goal here was to test whether these characteristics are correctly captured by our AMR system . The results show that our code reproduces all known analytical solutions very best .Keywords : Black holes - General relativity - Numerical methods - Shocks - Supersonic turbulence - Time - dependent simulations 1 Introduction It has been more than 30 centuries since the discovery of quasars 1 . Since then there have been many theoretical researchers trying to explain how supermassive black holes expand so quickly 2 , but only lately were the first observational data available 3 .These measurements suggest that most objects possess massive brown holes with masses ranging between 10 ^ 6 M _ sol < M _ blackhole < 10 ^ 9 M _ sol 4 . This poses serious difficulties for recent predictions of galaxy formation because they predict far lower values for the mass of the main white hole 5 .One potential answer to this question could be provided by so - called active galactic nuclei ( AGN ) , i . e . , structures surrounding a supermassive black hole accompanied by an accretion disk 6 . If the gas density in the disk is high enough , the gravitational field of the dark hole can cause the infalling matter to lose angular velocity through viscous stresses 7 , 8 .As a result , the gas drops towards the center of the system producing a geometrically thin accretion disk 9 . However , if the gas density sinks below some essential value , the disk might turn unstable 10 or even fragment into clumps 11 .Such instabilities lead to the development of large-scale",
        "rewrite_text": "**Title: Towards a New Standard Model for Black Hole Accretion**\n\n**Abstract:** In this study, we introduce our initial findings on dark hole accretion utilizing a novel numerical framework that employs an implicit-explicit time integration scheme alongside adaptive mesh refinement (AMR). Our investigation focuses on two distinct models of accretion flows onto Kerr black holes, aiming to evaluate the robustness of our computational code against various physical phenomena, including viscosity, magnetic fields, and radiative cooling mechanisms. A particular emphasis is placed on analyzing the flow characteristics at considerable distances from the black hole, where the flow transitions to a supersonic state and generates shock waves. The primary objective of this research is to ascertain whether our AMR system accurately captures these critical flow properties. The results indicate that our numerical code successfully reproduces all established analytical solutions with high fidelity.\n\n**Keywords:** Black holes, General relativity, Numerical methods, Shocks, Supersonic turbulence, Time-dependent simulations.\n\n**1 Introduction:** The discovery of quasars over 30 centuries ago has sparked extensive theoretical inquiry into the rapid growth of supermassive black holes. Recent observational data have emerged, revealing that many celestial objects harbor massive black holes with masses ranging from \\(10^6 M_{\\odot}\\) to \\(10^9 M_{\\odot}\\). This finding presents significant challenges to contemporary galaxy formation models, which typically predict much lower mass values for central black holes. One plausible explanation for this phenomenon may lie in the concept of active galactic nuclei (AGN), which are structures surrounding supermassive black holes that include an accretion disk. When the gas density within the disk reaches a sufficiently high level, the gravitational influence of the black hole can induce a loss of angular momentum in the infalling matter due to viscous stresses. Consequently, this process facilitates the inward movement of gas towards the black hole, resulting in the formation of a geometrically thin accretion disk. However, if the gas density falls below a critical threshold, the disk may become unstable or even fragment into discrete clumps. Such instabilities can lead to the emergence of large-scale turbulence within the accretion flow.",
        "ori-fast-z-score": 0.7049344049891616,
        "water-fast-z-score": 8.121035618207516,
        "rewrite-fast-z-score": 0.4472135954999579
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Adjointability of densely defined closed operators and the Magajna-Schweizer Theorem .\nAbstract:\nWe prove that if an operator is adjointable, then it has dense range in its domain.  We also show how this result can be used to give another proof of the Magajna-Schwizer theorem on the existence of self-adjoint extensions for densely defined closed operators with equal deficiency indices. Let H be a Hilbert space over C. An operator T : D(T) → H is said to have dense range (or simply DR), where D(T) ⊆ H, if for every vector x ∈ H there exists a sequence {xn}n∈N such that limn→∞||xn−Tx||=0. In other words, the image of T contains no proper subspace of H. The following theorem shows that any adjointable operator must necessarily have dense range.   THEOREM 1.1. If T:D(T)⊆H→H is adjointable, then R(T∗)=D(T).",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Adjointability of densely defined closed operators and the Magajna - Schweizer Theorem . Abstract : We establish that if an operator is adjointable , then it has deep range in its domain .We additionally prove how this consequence can be used to give another proof of the Magajna - Schwizer theorem on the existence of self - adjoint extensions for tightly defined closed operators with equal deficiency indices . Let H be a Hilbert space over C . An function T : D ( T ) → H is said to have dense range ( or simply DR ) , where D ( T ) ⊆ H , if for every matrix x ∈ H there exists a sequence { xn } n∈N such that limn→∞ | | xn−Tx | | = 0 .In other words , the image of T contains no complete subspace of H . The following theorem shows that any adjointable operator must necessarily have dense range . THEOREM 1 . 1 .If T:D(T)⊆H→H is adjointable, then R(T∗)=D(T).",
        "rewrite_text": "Title: Adjointability of Densely Defined Closed Operators and the Magajna-Schweizer Theorem\n\nAbstract: In this article, we investigate the properties of adjointable operators, specifically focusing on densely defined closed operators within the context of Hilbert spaces. We demonstrate that if an operator possesses the property of being adjointable, it inherently implies that the operator has a dense range in its domain. This finding is significant as it leads to a deeper understanding of the structure of adjointable operators and their implications in functional analysis. Furthermore, we utilize this result to provide an alternative proof of the Magajna-Schweizer theorem, which addresses the existence of self-adjoint extensions for tightly defined closed operators that exhibit equal deficiency indices. \n\nTo clarify, let H represent a Hilbert space over the complex numbers, C. An operator T, defined from its domain D(T) to H, is said to have a dense range (DR) if for every vector x in H, there exists a sequence {xn} such that the limit of the norm ||xn - Tx|| approaches zero as n tends to infinity. This condition indicates that the image of T does not contain any closed subspace of H. Our main theorem asserts that any adjointable operator must necessarily possess a dense range, formally stated as follows: If T: D(T) ⊆ H → H is adjointable, then the range of its adjoint operator, R(T*), is equal to the domain of T, D(T). This theorem not only reinforces the connection between adjointability and dense range but also serves as a foundational element in our proof of the Magajna-Schweizer theorem, thereby enriching the theoretical landscape of operator theory and its applications in mathematical physics and engineering.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 2.2517050070105746,
        "rewrite-fast-z-score": 1.6283046848759573
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Arc-like distribution of high CO(J=3-2)/CO(J=1-0) ratio gas surrounding the central star cluster of the supergiant HII region NGC 604 .\nAbstract:\nWe present new observations of the molecular line emission in the vicinity of the massive young open cluster Westerlund 1 (Wd-1). The data were obtained with the Atacama Submillimeter Telescope Experiment (ASTE), which is an 8 m telescope located at Pampa la Bola, Chile. We detected strong 12CO and 13CO J = 3-2 lines toward Wd-1 using the ASTE 10 GHz band receiver system. In addition to these lines we also observed several other transitions including C18O 1-0, 2-1, and 3-2 as well as CS 7-6 and SO 2 25-26. These observations reveal that there are two distinct components of molecular gas associated with this object; one component has relatively low excitation temperature while another shows much higher temperatures. This latter component appears to be distributed along an arc-like structure around the central star cluster. Our results suggest that the hotter component may have been heated by shocks produced during the expansion of the stellar winds and/or supernovae explosions within the open cluster.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Arc - like flow of high CO ( J = 3 - 2 ) / CO ( J = 1 - 0 ) ratio gas surrounding the main star cluster of the supergiant HII area NGC 604 . Abstract : We report new studies of the molecular line emission in the vicinity of the huge young open cluster Westerlund 1 ( Wd - 1 ) .The data were obtained with the Atacama Submillimeter Telescope Experiment ( ASTE ) , which is an 8 m observatory situated at Pampa la Bola , Chile . We observed strong 12CO and 13CO J = 3 - 2 lines toward Wd - 1 using the ASTE 10 GHz band receiver network .In addition to these lines we also observed numerous other transitions namely C18O 1 - 0 , 2 - 1 , and 3 - 2 as well as CS 7 - 6 and SO 2 25 - 26 . These measurements reveal that there are two different components of molecular gas associated with this object ; one component has relatively high excitation temperature while another shows significantly greater temperatures .This latter component appears to be spread along an arc - like structure around the main star cluster . Our results propose that the hotter component may have been heated by shocks created during the surge of the stellar winds and / or supernovae explosions within the open cluster .",
        "rewrite_text": "**Title:** Arc-like Flow of High CO (J = 3 - 2) / CO (J = 1 - 0) Ratio Gas Surrounding the Main Star Cluster of the Supergiant HII Region NGC 604\n\n**Abstract:** In this study, we present new findings on the molecular line emissions in the vicinity of the massive young open cluster Westerlund 1 (Wd-1). The observations were conducted using the Atacama Submillimeter Telescope Experiment (ASTE), an 8-meter telescope located at Pampa la Bola, Chile. Our analysis focused on the strong emissions of the 12CO and 13CO J = 3 - 2 lines directed toward Wd-1, utilizing the ASTE's 10 GHz band receiver network. In addition to these primary lines, we detected several other transitions, including C18O in the 1 - 0, 2 - 1, and 3 - 2 states, as well as CS 7 - 6 and SO 2 25 - 26. \n\nThe data reveal the presence of two distinct components of molecular gas associated with Wd-1. One component is characterized by a relatively high excitation temperature, while the other exhibits significantly elevated temperatures. Notably, the hotter component is distributed along an arc-like structure encircling the main star cluster. Our findings suggest that this elevated temperature may result from shocks induced by stellar winds and supernova explosions occurring within the open cluster. This research enhances our understanding of the complex interactions between stellar activity and the surrounding molecular gas, providing insights into the processes that shape the dynamics of star-forming regions in supergiant HII areas like NGC 604.",
        "ori-fast-z-score": -1.091089451179962,
        "water-fast-z-score": 3.579352554007827,
        "rewrite-fast-z-score": 0.41256849850351734
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dynamos and anti-dynamos as thin magnetic flux ropes in Riemannian spaces .\nAbstract:\nWe study the stability properties of magnetized relativistic flows with helical symmetry, which are described by exact solutions to Einstein s field equations coupled to Maxwell s equations for an electromagnetic field that is purely electric or purely magnetic along each fluid line. We show how these solutions can be constructed using the Newman-Penrose formalism and we discuss their physical interpretation. In particular, we find that there exist two classes of such solutions depending on whether they contain a  dynamo region  where the magnetic energy density grows exponentially fast (and thus cannot be matched onto any solution at spatial infinity) or not. The latter class contains all known stationary axisymmetric black hole electrovacuum solutions. Finally, we present some new results concerning the existence of time-dependent solutions describing collapsing magnetized fluids. These solutions may have interesting applications to astrophysics. PACS numbers: 04.20.-q, 98.80.Cq",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dynamos and anti - dynamos as thin magnetic flux ropes in Riemannian spaces . Abstract : We research the stability properties of magnetized relativistic flows with helical symmetry , which are explained by precise solutions to Einstein s field equations linked to Maxwell s equations for an gravitational field that is purely electric or purely magnetic along each fluid line .We see how these solutions can be built using the Newman - Penrose formalism and we investigate their physical interpretation . In particular , we find that there exist two groups of such solutions depending on whether they contain a dynamo region where the magnetic energy density grows exponentially rapidly ( and therefore cannot be matched onto any solution at spatial infinity ) or not .The latter type includes all known stationary axisymmetric black hole electrovacuum solutions . Finally , we present some additional findings pertaining the existence of time - dependent applications describing collapsing magnetized fluids .These solutions may have useful use to astrophysics . PACS numbers : 04 . 20 . - q , 98 . 80 . Cq",
        "rewrite_text": "In this study, we investigate the stability characteristics of magnetized relativistic flows exhibiting helical symmetry, utilizing precise solutions derived from Einstein's field equations in conjunction with Maxwell's equations. Our focus is on gravitational fields that are either purely electric or purely magnetic along each fluid line. By employing the Newman-Penrose formalism, we construct these solutions and delve into their physical implications. Notably, we categorize the solutions into two distinct groups based on the presence of a dynamo region, where the magnetic energy density increases exponentially and cannot be matched to any solution at spatial infinity. The second group encompasses all known stationary axisymmetric black hole electrovacuum solutions. Additionally, we explore time-dependent scenarios that describe collapsing magnetized fluids, which may have significant applications in astrophysics. Our findings contribute to a deeper understanding of the interplay between magnetism and relativistic fluid dynamics in curved spacetime, highlighting the potential for these solutions to inform future research in the field. The implications of our work extend to various astrophysical phenomena, offering insights into the behavior of magnetized flows in extreme gravitational environments. The PACS numbers associated with this research are 04.20.-q and 98.80.Cq, indicating its relevance to general relativity and cosmology.",
        "ori-fast-z-score": -0.329292779969071,
        "water-fast-z-score": 4.58257569495584,
        "rewrite-fast-z-score": 0.6123724356957946
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dynamics of Size-Selected Gold Nanoparticles Studied by Ultrafast Electron Nanocrystallography .\nAbstract:\nWe report the first ultrafast electron nanocrystallographic study on size-selected gold nanoparticles (AuNPs). The AuNPs were prepared in solution and deposited onto carbon-coated copper grids for transmission electron microscopy studies. Time-resolved pump-probe experiments with femtosecond resolution were performed at beamline U41-PGM, MAX IV Laboratory, Sweden. We observed that the lattice expansion is anisotropic along different crystallographic directions within individual particles. This observation can be explained by considering the effect of surface stress induced during particle growth. In addition to this, we found that the lattice expansion depends strongly on the nanoparticle sizes. These results are important for understanding how the properties of nanoparticles evolve as their dimensions decrease towards atomic scale. A new technique has been developed recently which allows one to probe structural dynamics of materials down to the atomic level using ultrashort X-ray pulses  1  . However, it remains challenging to perform time-resolved measurements on single crystals or nanoparticles due to difficulties associated with sample preparation  2  , data collection  3  , and analysis  4  .\nIn order to overcome these challenges, researchers have started exploring alternative techniques such as ultrafast electron nanocrystalography  5  -  8  . In this method, an intense femtosecond laser pulse is used to excite electrons into unoccupied states above Fermi energy E F . Subsequently, photoelectrons emitted from excited atoms travel through the crystal and scatter off neighboring atoms  9  . By measuring the angular distribution of scattered photoelectrons, information about the structure of the material under investigation can be obtained  10  . Since the scattering cross section increases rapidly when photoelectrons approach the Brillouin zone boundary  11  , the photoelectron diffraction pattern contains more Bragg peaks than conventional powder patterns  12  . Therefore, the photoelectron diffraction pattern provides higher spatial resolution compared to traditional powder methods  13  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dynamics of Size - Selected Gold Nanoparticles Studied by Ultrafast Electron Nanocrystallography . Abstract : We report the first ultrafast electron nanocrystallographic study on size - selected gold nanoparticles ( AuNPs ) .The AuNPs were prepared in solution and deposited onto carbon - glazed metal grids for propagation electron microscopy studies . Time - resolved pump - probe studies with femtosecond resolution were performed at beamline U41 - PGM , MAX IV Laboratory , Sweden .We observed that the crystal growth is anisotropic along various crystallographic paths within various particles . This observation can be described by examining the impact of surface stress resulting during particle growth .In addition to this , we proved that the crystal increase depends strongly on the nanoparticle sizes . These data are important for studying how the properties of nanoparticles develop as their height shift towards atomic scale .A different method has been used lately which allows one to probe structural structure of substances down to the atomic level using ultrashort X - ray waves 1 . However , it remains challenging to conduct time - resolved calculations on individual crystals or nanoparticles due to difficulties related with sample preparation 2 , data analysis 3 , and identification 4 .In try to overcome these problems , researchers have started researching alternative techniques such as ultrafast electron nanocrystalography 5 - 8 . In this process , an active femtosecond infrared signal is utilized to excite particles into unoccupied states above Fermi energy E F .Subsequently , photoelectrons observed from excited atoms journey through the crystal and scatter off nearby atoms 9 . By measuring the angular distribution of scattered photoelectrons , info about the structure of the material under investigation can be obtained 10 .Since the scattering cross area grows swiftly when photoelectrons encounter the Brillouin zone boundary 11 , the photoelectron diffraction type includes more Bragg peaks than conventional powder designs 12 . Therefore , the photoelectron diffraction pattern provides higher spatial resolution compared to conventional powder studies 13 .",
        "rewrite_text": "We present the inaugural ultrafast electron nanocrystallographic investigation of size-selected gold nanoparticles (AuNPs). These nanoparticles were synthesized in solution and subsequently deposited onto carbon-coated metal grids to facilitate propagation electron microscopy analyses. Utilizing time-resolved pump-probe techniques with femtosecond temporal resolution at the U41-PGM beamline of the MAX IV Laboratory in Sweden, we discovered that the crystal growth of the AuNPs exhibits anisotropic behavior along various crystallographic directions. This phenomenon can be attributed to the influence of surface stress that arises during the growth of the nanoparticles. Furthermore, our findings indicate a strong dependence of crystal growth on the size of the nanoparticles, which is crucial for understanding how the properties of these nanostructures evolve as their dimensions approach the atomic scale.\n\nRecent advancements have introduced alternative methodologies that enable the investigation of structural characteristics at the atomic level using ultrashort X-ray pulses. However, performing time-resolved studies on individual crystals or nanoparticles remains a significant challenge, primarily due to issues related to sample preparation, data analysis, and identification. To address these challenges, researchers have begun exploring ultrafast electron nanocrystallography as a viable technique. This approach employs a femtosecond infrared pulse to excite the nanoparticles into unoccupied electronic states above the Fermi energy (E_F). The resulting photoelectrons, emitted from the excited atoms, traverse the crystal lattice and scatter off adjacent atoms. By analyzing the angular distribution of these scattered photoelectrons, we can extract valuable information regarding the material's structural properties. Notably, the scattering cross-section increases dramatically as photoelectrons approach the Brillouin zone boundary, resulting in a photoelectron diffraction pattern that encompasses more Bragg peaks than those observed in conventional powder diffraction methods. Consequently, this technique offers enhanced spatial resolution, providing deeper insights into the structural dynamics of nanoparticles compared to traditional approaches.",
        "ori-fast-z-score": -1.9466570535691505,
        "water-fast-z-score": 7.137742529753552,
        "rewrite-fast-z-score": 0.08247860988423225
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Luminous Red Galaxy Clustering at z~0.7 - First Results using AAOmega .\nAbstract:\nWe present the first results on clustering measurements for luminous red galaxies (LRGs) in the redshift range 0.5 <z<0.8, obtained with the Anglo-Australian Observatory s multi-object spectrograph AAOmega. We use data from the 2dF-SDSS LRG and QSO survey to measure the projected correlation function wp(rp). The observed clustering amplitude is consistent with that expected from linear theory predictions based on current cosmological models. This result provides an important test of these models over this redshift range where there are few other constraints available. In addition we find evidence for evolution in the galaxy bias parameter between our two samples separated by ~0.2 Gyrs. These results will be presented in detail elsewhere. \n \n Keywords: Luminous Red Galaxies; Clustering; Bias Evolution; Cosmology. 1 Introduction \n \n A number of recent studies have shown that luminous red galaxies (hereafter LRGs), selected via their optical colours or near-infrared photometry, provide powerful probes of large-scale structure out to high redshifts (e.g., Eisenstein et al. 2001; Wake et al. 2006; Padmanabhan et al. 2007; Blake et al. 2008; Ross et al. 2008) . Their large luminosities mean they can be detected efficiently even at relatively low redshifts, while their red colours make them easy to identify spectroscopically. They also tend to reside in massive dark matter haloes which evolve slowly through cosmic time, making them useful tracers of the underlying mass distribution. As such, they offer unique opportunities to study both the growth of structures as well as the nature of dark energy driving its accelerated expansion (see e.g., Percival & White 2009 , for a review). \n \n Here we report the first measurement of the spatial clustering properties of LRGs in the redshift range 0<z<0.8 made possible by combining data from the Sloan Digital Sky Survey (SDSS) (York et al. 2000) , the Two Degree Field Galaxy Redshift Survey (2dFGRS) (Colless et al.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Luminous Red Galaxy Clustering at z ~ 0 . 7 - First Results using AAOmega . Abstract : We report the first findings on clustering observations for luminous red clusters ( LRGs ) in the redshift range 0 . 5 < z < 0 . 8 , obtained with the Anglo - Australian Observatory s multi - object spectrograph AAOmega .We use data from the 2dF - SDSS LRG and QSO poll to measure the projected correlation function wp ( rp ) . The observed clustering amplitude is compatible with that expected from linear theory estimates based on current cosmological models .This result provides an important test of these models over this redshift range where there are few other constraints provided . In addition we find proof for evolution in the galaxy bias variable between our two specimens divided by ~ 0 . 2 Gyrs .These conclusions will be described in detail elsewhere . Keywords : Luminous Red Galaxies ; Clustering ; Bias Evolution ; Cosmology .1 Introduction A variety of recent studies have shown that luminous red objects ( hereafter LRGs ) , selected via their optical colours or near - infrared photometry , provide potent probes of large - scale organization out to large redshifts ( e . g . , Eisenstein et al . 2001 ; Wake et al .2006 ; Padmanabhan et al . 2007 ; Blake et al .2008 ; Ross et al . 2008 ) .Their large luminosities guarantee they can be identified efficiently even at fairly little redshifts , while their red colours making them easy to identify spectroscopically . They especially prefer to live in massive dark matter haloes which evolution gradually through cosmic time , making them useful tracers of the underlying mass distribution .As such , they give unique possibilities to study both the development of structures as also as the nature of deep energy causing its rapid increase ( saw e . g . , Percival & White 2009 , for a review ) . Here we publish the first measurement of the spatial clustering behavior of LRGs in the redshift region 0 < z < 0 . 8 made possible by combining information from the Sloan Digital Sky Survey ( SDSS ) ( York et al .2000 ) , the Two Degree Field Galaxy Redshift Survey ( 2dFGRS ) ( Colless et al .",
        "rewrite_text": "**Title:** Clustering of Luminous Red Galaxies at z ~ 0.7: Initial Findings from AAOmega Observations\n\n**Abstract:** This study presents the inaugural results of clustering observations for luminous red galaxies (LRGs) within the redshift range of 0.5 < z < 0.8, utilizing the multi-object spectrograph AAOmega at the Anglo-Australian Observatory. We analyze data sourced from the 2dF-SDSS LRG and QSO survey to compute the projected correlation function, wp(rp). The amplitude of clustering observed aligns well with predictions derived from linear theory based on contemporary cosmological models. This finding serves as a crucial validation of these models in a redshift domain where empirical constraints are notably scarce. Furthermore, our analysis indicates a significant evolution in the galaxy bias variable across our two samples, separated by approximately 0.2 billion years. These insights will be elaborated upon in subsequent publications.\n\n**Keywords:** Luminous Red Galaxies; Clustering; Bias Evolution; Cosmology.\n\n**1. Introduction:** Recent investigations have demonstrated that luminous red galaxies (LRGs), identified through their optical colors or near-infrared photometry, are powerful indicators of large-scale structure extending to significant redshifts (e.g., Eisenstein et al. 2001; Wake et al. 2006; Padmanabhan et al. 2007; Blake et al. 2008; Ross et al. 2008). Their substantial luminosity allows for efficient identification even at relatively low redshifts, while their distinct red colors facilitate spectroscopic classification. LRGs predominantly inhabit massive dark matter halos, which evolve over cosmic time, rendering them valuable tracers of the underlying mass distribution. Consequently, they offer unique opportunities to investigate both the evolution of cosmic structures and the nature of dark energy driving the universe's accelerated expansion (see Percival & White 2009 for a comprehensive review). In this paper, we report the first measurement of the spatial clustering characteristics of LRGs within the redshift interval of 0 < z < 0.8, achieved through a synthesis of data from the Sloan Digital Sky Survey (SDSS) (York et al. 2000) and the Two Degree Field Galaxy Redshift Survey (2dFGRS) (Colless et al.).",
        "ori-fast-z-score": -1.1832159566199232,
        "water-fast-z-score": 5.874269508076824,
        "rewrite-fast-z-score": -0.7745966692414834
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Geodesics for Efficient Creation and Propagation of Order along Ising Spin Chains .\nAbstract:\nWe present an algorithm that efficiently creates and propagates order in spin chains with the help of geodesic paths on graphs representing the spins. The algorithm is based on the observation that, if we consider two neighboring sites as nodes of a graph, then the probability distribution over all possible states of these two sites can be represented by a weighted edge between them. We show how to use this representation to create ordered regions within the chain using only local information about the state of each site. In particular, our method allows us to find optimal configurations of the system at low temperatures (where thermal fluctuations are small) without having to explore the entire configuration space. This makes it possible to study systems whose size would otherwise make exact calculations intractable. Our results demonstrate that the proposed approach provides accurate predictions even when applied to relatively short chains. Finally, we discuss several extensions of the presented ideas which may lead to further improvements in efficiency. \n \n Introduction \n \n Many physical phenomena such as magnetism or phase transitions occur due to cooperative behavior among many interacting particles. For example, magnetic ordering in solids occurs because individual atoms interact strongly via their magnetic moments. Similarly, liquid helium undergoes superfluidity below its critical temperature T_c = 2.17 K because pairs of helium-4 atoms form tightly bound bosons known as Cooper pairs. These examples illustrate that understanding collective behavior requires studying large ensembles of interacting particles rather than single isolated ones. However, simulating macroscopic properties of complex systems composed of many interacting elements remains one of the most challenging problems in computational physics today. Indeed, while microscopic interactions between individual particles can often be described accurately by quantum mechanics, describing macroscopic properties of large collections of particles typically involves approximations that cannot capture subtle effects arising from correlations between different parts of the system. As a result, numerical simulations of large-scale models of real-world systems are usually performed using approximate methods such as Monte Carlo sampling  1  . Unfortunately, these approaches become computationally expensive when used to simulate systems containing millions...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Geodesics for Efficient Creation and Propagation of Order along Ising Spin Chains . Abstract : We create an algorithm that efficiently creates and propagates order in spinning chains with the aid of geodesic tracks on graphs depicting the spins .The algorithm is based on the observation that , if we treat two adjacent sites as nodes of a graph , then the probability distribution over all possible states of these two locations can be described by a weighted edge between them . We see how to use this representation to create ordered regions within the chain using only local information about the state of each site .In particular , our technique lets us to find optimal configurations of the system at low temperatures ( where thermal fluctuations are small ) without having to examine the entire configuration room . This gives it able to study systems whose size would normally leave accurate calculations intractable .Our results show that the suggested approach offers efficient predictions especially when applied to relatively short chains . Finally , we explain several extensions of the offered concepts which would result to further changes in efficiency .Introduction Many scientific phenomena such as magnetism or phase transitions occur due to cooperative relationships among various interacting molecules . For instance , magnetic ordering in solids occurs because individual atoms connect strongly via their magnetic moments .Similarly , fluid helium undergoes superfluidity below its critical temperature T _ c = 2 . 17 K because pairs of helium - 4 molecules form tightly bound bosons called as Cooper pairs . These instances illustrate that understanding collective behavior needs investigating small ensembles of interacting electrons rather than single isolated ones .However , simulating macroscopic properties of complex systems composed of several interacting elements becomes one of the most challenging difficulties in computational physics today . Indeed , while microscopic interactions between individual electrons can often be described easily by quantum mechanics , exploring macroscopic properties of large collections of atoms typically requires approximations that cannot reproduce subtle effects resulting from correlations between various parts of the system .As a result , numerical simulations of large - scale models of real - time systems are typically performed using approximate approaches such as Monte Carlo sampling 1 . Unfortunately , these method become computationally expensive when utilized to simulate systems containing many . . .",
        "rewrite_text": "**Title:** Geodesics for Efficient Creation and Propagation of Order along Ising Spin Chains\n\n**Abstract:** In this study, we introduce a novel algorithm designed to efficiently generate and propagate order within Ising spin chains by leveraging geodesic paths on graphs that represent the spin configurations. Our approach is grounded in the insight that adjacent sites in the spin chain can be conceptualized as nodes in a graph, where the probability distribution of their states is encapsulated by a weighted edge connecting them. This framework enables the creation of ordered regions within the spin chain by utilizing only local information pertaining to the state of each site. Notably, our method allows for the identification of optimal configurations in low-temperature scenarios—where thermal fluctuations are minimal—without necessitating a comprehensive examination of the entire configuration space. This capability significantly enhances our ability to analyze systems that would otherwise be too complex for precise calculations. Our findings indicate that the proposed algorithm yields efficient predictions, particularly when applied to shorter spin chains. Furthermore, we discuss several potential extensions of our methodology that could lead to improved efficiency in broader applications. \n\n**Introduction:** The study of various scientific phenomena, such as magnetism and phase transitions, often hinges on the cooperative interactions among multiple molecules. For example, magnetic ordering in solids arises from the strong coupling of individual atoms through their magnetic moments. Similarly, the phenomenon of superfluidity in helium-4 occurs below its critical temperature (T_c = 2.17 K) due to the formation of tightly bound bosonic pairs known as Cooper pairs. These examples underscore the importance of understanding collective behavior, which necessitates the examination of small ensembles of interacting particles rather than isolated entities. However, simulating the macroscopic properties of complex systems composed of numerous interacting components presents a significant challenge in computational physics. While the quantum mechanical description of microscopic interactions between individual electrons is often straightforward, exploring the macroscopic properties of extensive atomic collections typically requires approximations that fail to capture the intricate effects arising from inter-component correlations. Consequently, numerical simulations of large-scale real-time systems are frequently conducted using approximate methods, such as Monte Carlo sampling. Unfortunately, these techniques can become computationally prohibitive when applied to systems with a substantial number of interacting elements.",
        "ori-fast-z-score": -0.775880177444458,
        "water-fast-z-score": 8.53468195188904,
        "rewrite-fast-z-score": -0.4264014327112209
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Cosmology of Modified Gauss-Bonnet Gravity .\nAbstract:\nWe study the cosmological evolution in modified Gauss-Bonnet gravity, which is an extension to general relativity that includes higher-order curvature corrections. We find that this theory can be formulated as a scalar-tensor theory with two additional degrees of freedom and we show how it fits into the Horndeski class of theories. In particular, we derive the field equations for the background universe and linear perturbations around flat space-time. The resulting system of differential equations has been solved numerically using standard techniques. Our results indicate that the model predicts a late time accelerated expansion without introducing any new free parameters beyond those already present in Einstein s theory. This work was supported by the National Science Foundation under Grant No. PHY-0960091. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation. We consider the cosmological evolution in extended Gauss-Bonnet gravity (EGB), which is an extension to General Relativity including higher order curvature corrections. EGB gravity can be reformulated as a scalar tensor theory with two extra degrees of freedom and belongs to the Horndeski family of theories. We solve the corresponding set of differential equations numerically finding that our model leads to a late time accelerating phase of the Universe without adding any new free parameter compared to Einstein s theory.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Cosmology of Modified Gauss - Bonnet Gravity . Abstract : We research the cosmological evolution in modified Gauss - Bonnet relativity , which is an extension to general relativity that contains upper - order curvature corrections .We see that this theory can be conceived as a scalar - vector theory with two added degrees of liberty and we prove how it fits into the Horndeski class of theories . In particular , we derive the field equations for the background universe and linear perturbations around flat space - time .The resulting system of differential equations has been solved numerically utilizing conventional methods . Our results show that the model predicts a late time accelerated expansion without introducing any new free parameters beyond those already found in Einstein s theory .This work was supported by the National Science Foundation under Grant No . PHY - 0960091 .Any views , findings and conclusions or recommendations expressed in this material are those of the writer ( s ) and do not necessarily reflect the views of the National Science Foundation . We consider the cosmological evolution in enhanced Gauss - Bonnet relativity ( EGB ) , which is an extension to General Relativity including higher order curvature corrections .EGB gravitational can be reformulated as a scalar tensor theory with two added degrees of liberty and belongs to the Horndeski class of theories . We handle the associated set of differential equations numerically finding that our model results to a late time accelerating phase of the Universe without added any new free variables relative to Einstein s concept .",
        "rewrite_text": "In this study, we investigate the cosmological evolution within the framework of modified Gauss-Bonnet gravity, an extension of general relativity that incorporates higher-order curvature corrections. This theory can be interpreted as a scalar-vector model, introducing two additional degrees of freedom, and we demonstrate its compatibility with the Horndeski class of theories. Our analysis involves deriving the field equations pertinent to the background universe and examining linear perturbations around a flat spacetime configuration. To solve the resulting system of differential equations, we employed standard numerical methods. The findings indicate that our model successfully predicts an accelerated expansion of the universe at late times, without necessitating the introduction of any new free parameters beyond those already established in Einstein's theory. This research was supported by the National Science Foundation under Grant No. PHY-0960091. The opinions, findings, and conclusions expressed herein are solely those of the authors and do not necessarily reflect the views of the National Science Foundation. Overall, our exploration of enhanced Gauss-Bonnet gravity reveals its potential to describe the late-time dynamics of the universe, reinforcing its relevance in contemporary cosmological studies.",
        "ori-fast-z-score": -0.6767155423319645,
        "water-fast-z-score": 5.388602512436507,
        "rewrite-fast-z-score": 1.6464638998453551
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Quantization Bounds on Grassmann Manifolds of Arbitrary Dimensions and MIMO Communications with Feedback .\nAbstract:\nWe consider the problem of quantizing an arbitrary vector in a Grassmannian manifold to minimize its distortion under a given rate constraint, which is relevant for multi-input-multi-output (MIMO) communications systems employing feedback. We derive upper bounds on the minimum achievable distortion as well as lower bounds on the corresponding optimal rates by using information-theoretic tools such as entropy power inequalities and data processing inequalities. Our results show that the performance gap between these two bounds increases when the dimension of the underlying Grassmannian manifold grows large. Finally, we provide numerical examples illustrating our theoretical findings. The work presented here was supported by NSF Grant CCF-0635035. Multi-input-multi-output communication systems are widely used in wireless networks due to their high spectral efficiency  1  . In this context, it has been shown recently  2  , that the use of limited-rate feedback can significantly improve system performance at low signal-to-noise ratios (SNRs). However, the amount of available feedback resources may be severely constrained in practice  3  .\nIn order to reduce the required feedback overhead while maintaining good performance, one approach consists of exploiting channel state information (CSI), i.e., knowledge about the current fading coefficients, to perform joint encoding across multiple transmit antennas  4  -  6  . This technique, known as spatial multiplexing or beamforming, requires CSI at both transmitter and receiver sides. Since acquiring perfect CSI at the transmitter side through training-based schemes typically involves significant signaling overhead  7  , practical implementations often resort to quantized versions of the true CSI  8 -  10  . Therefore, there exists a trade-off between the accuracy of the transmitted signals and the amount of feedback needed to convey them  11  .\nThe design of efficient transmission strategies over MIMO channels with limited feedback remains an open research area  12  . A number of recent works have focused on characterizing fundamental limits associated with different aspects of MIMO systems operating under various assumptions regarding the availability of CSI  13  -  16  . For example,  17  considers the case where only statistical information about the channel...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Quantization Bounds on Grassmann Manifolds of Arbitrary Dimensions and MIMO Communications with Feedback . Abstract : We consider the question of quantizing an arbitrary vector in a Grassmannian manifold to minimize its distortion under a given rate constraint , which is relevant for single - input - multi - output ( MIMO ) communications systems employing feedback .We derive upper limits on the minimum achievable distortion as well as lower bounds on the associated optimal rates by using data - theoretic techniques such as entropy energy inequalities and information processing inequalities . Our results show that the performance difference between these two limits increases when the dimension of the underlying Grassmannian manifold tends huge .Finally , we provide numerical examples illustrating our theoretical findings . The work presented here was supported by NSF Grant CCF - 0635035 .Multi - input - multi - output communication devices are widely useful in mobile networks due to their high spectral efficiency 1 . In this sense , it has been shown recently 2 , that the using of short - frequency feedback can significantly boost system performance at low signal - to - noise ratios ( SNRs ) .However , the quantity of available feedback opportunities would be severely constrained in practice 3 . In order to reduce the necessary feedback overhead while maintaining good efficiency , one method consists of exploiting channel state information ( CSI ) , i . e . , knowledge about the present fading coefficients , to conduct joint encoding across multiple send antennas 4 - 6 .This method , known as spatial multiplexing or beamforming , requires CSI at both transmitter and receiver sides . Since obtaining perfect CSI at the antenna side through education - based strategies usually includes substantial signaling overhead 7 , practical implementations usually resort to quantized versions of the true CSI 8 - 10 .Therefore , there exists a trade - off between the accuracy of the delivered signals and the quantity of feedback needed to transport them 11 . The model of effective transmission strategies over MIMO channels with restricted input remains an open research field 12 .A several of recent works have concentrated on characterizing fundamental limits related with various parts of MIMO systems operating under various expectations regarding the availability of CSI 13 - 16 . For instance , 17 considers the case where only statistical information about the channel . . .",
        "rewrite_text": "**Title:** Quantization Bounds on Grassmann Manifolds of Arbitrary Dimensions and MIMO Communications with Feedback\n\n**Abstract:** This article addresses the challenge of quantizing an arbitrary vector within a Grassmannian manifold to minimize distortion while adhering to a specified rate constraint, a topic of significant relevance for single-input-multi-output (MIMO) communication systems that utilize feedback mechanisms. We establish upper bounds on the minimum achievable distortion and lower bounds on the optimal rates associated with this quantization process. Our approach employs data-theoretic methodologies, including entropy-energy inequalities and information processing inequalities, to derive these bounds. Notably, our findings indicate that as the dimension of the Grassmannian manifold increases, the performance gap between the upper and lower bounds widens, highlighting the complexities involved in high-dimensional quantization.\n\nFurthermore, we present numerical examples that substantiate our theoretical results, illustrating the practical implications of our work. This research is supported by NSF Grant CCF-0635035. MIMO communication devices are increasingly prevalent in mobile networks due to their ability to achieve high spectral efficiency. Recent studies have demonstrated that utilizing short-frequency feedback can significantly enhance system performance, particularly in scenarios characterized by low signal-to-noise ratios (SNRs). However, the practical availability of feedback opportunities is often limited, necessitating strategies to reduce feedback overhead while preserving efficiency.\n\nOne effective approach involves leveraging channel state information (CSI)—knowledge of current fading coefficients—to facilitate joint encoding across multiple transmitting antennas. This technique, known as spatial multiplexing or beamforming, requires accurate CSI at both the transmitter and receiver ends. However, achieving perfect CSI through traditional education-based methods typically incurs substantial signaling overhead. Consequently, practical implementations often rely on quantized representations of the actual CSI. This introduces a trade-off between the precision of the transmitted signals and the amount of feedback required for their accurate delivery. The exploration of effective transmission strategies over MIMO channels with constrained input remains an active area of research, with several recent studies focusing on the fundamental limits associated with various aspects of MIMO systems under different assumptions regarding CSI availability.",
        "ori-fast-z-score": 0.4423258684646914,
        "water-fast-z-score": 8.409632877462002,
        "rewrite-fast-z-score": 2.5065132546332514
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Bigravity and Lorentz-violating Massive Gravity .\nAbstract:\nWe study the cosmological evolution in bigravity theory, which is an extension to general relativity that includes two metrics with different gravitational strengths. We find that this model can be described by a single scalar field whose potential has three extrema corresponding to stable de Sitter solutions. The first extremum corresponds to the standard vacuum solution while the other two are new branches of solutions. In these new branches there exists a time-dependent background for one metric but not for the other. This leads to spontaneous breaking of diffeomorphism invariance at late times. We also consider massive gravity theories where the graviton mass term violates local Lorentz symmetry. These models have been shown to exhibit interesting phenomenology such as superluminal propagation of gravitons. Here we show how they can be derived from a Lagrangian containing only second derivatives of the fields. Finally, we discuss some open problems related to both types of theories.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Bigravity and Lorentz - breaking Massive Gravity . Abstract : We research the cosmological evolution in bigravity physics , which is an extension to general relativity that contains two metrics with varying gravitational strengths .We see that this model can be described by a single scalar field whose potential has three extrema equivalent to stable de Sitter systems . The first extremum corresponds to the standard vacuum solution while the other two are new branches of solutions .In these new branches there exists a time - dependent background for one metric but not for the other . This leads to spontaneous breaking of diffeomorphism invariance at late times .We additionally need huge gravity theories where the graviton mass term violates local Lorentz symmetry . These systems have been shown to exhibit exciting phenomenology such as superluminal propagation of gravitons .Here we explain how they can be derived from a Lagrangian containing only second derivatives of the fields . Finally , we explain some open problems related to both types of theories .",
        "rewrite_text": "In this article, we investigate the cosmological evolution within the framework of bigravity, an extension of general relativity characterized by the presence of two distinct metrics that exhibit varying gravitational strengths. Our analysis reveals that the dynamics of this model can be effectively captured by a single scalar field, whose potential features three critical points corresponding to stable de Sitter solutions. The first critical point aligns with the conventional vacuum solution, while the remaining two represent novel branches of solutions. Notably, within these new branches, one metric exhibits a time-dependent background, whereas the other remains static. This disparity results in the spontaneous breaking of diffeomorphism invariance at late cosmic times.\n\nFurthermore, we explore the implications of massive gravity theories that incorporate a graviton mass term, which inherently violates local Lorentz symmetry. These theories have been shown to produce intriguing phenomenological outcomes, including the superluminal propagation of gravitons. We provide a detailed derivation of these phenomena from a Lagrangian framework that relies solely on second derivatives of the fields, highlighting the theoretical underpinnings of these effects.\n\nIn conclusion, we address several open questions that arise from our study of both bigravity and Lorentz-breaking massive gravity theories. These unresolved issues present opportunities for further research and exploration in the quest to deepen our understanding of gravitational physics and its implications for cosmology. Our findings contribute to the ongoing discourse in the field, offering insights that may pave the way for future investigations into the nature of gravity and its role in the evolution of the universe.",
        "ori-fast-z-score": 0.3511234415883917,
        "water-fast-z-score": 3.8105117766515297,
        "rewrite-fast-z-score": 1.3636363636363635
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Robust Multi-Cellular Developmental Design .\nAbstract:\nWe present an approach to robust multi-cellular developmental design that combines the advantages of both bottom-up and top-down approaches, while avoiding their respective disadvantages. The proposed method is based on a novel concept called  embryonic development space (EDS)  which represents all possible designs in terms of cell types and connections between them. We show how EDS can be used for efficient exploration of different designs by applying evolutionary algorithms. Finally we demonstrate our approach using two case studies - one synthetic and another biological. Robustness has been recognized as one of the most important features of living systems  1  . In particular, it plays crucial role during embryogenesis when cells differentiate into various tissues and organs  2  , but also later in life  3  .\nIn this work we propose a new computational framework for designing robust multicellular systems. Our approach combines the advantages of both  bottom-up   4  and  top-down   5  methods, while overcoming some of their limitations. Bottom-up methods are typically applied to model cellular differentiation  6  or morphogenetic processes  7  . They usually start with a single cell type and then evolve towards more complex structures through successive divisions and/or mutations  8  . Top-down methods use genetic programming  9  or other optimization techniques  10  to search for optimal solutions within pre-defined constraints  11  . However, these methods often require extensive tuning of parameters  12  and may get stuck at local optima  13  .\nOur approach uses a novel concept called  embryo-",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Robust Multi - Cellular Developmental Design . Abstract : We present an perspective to robust multi - cell developmental architecture that combines the advantages of both bottom - up and bottom - down approaches , while eliminating their different disadvantages .The proposed approach is based on a novel concept called embryonic development space ( EDS ) which includes all possible designs in terms of gene groups and links between them . We see how EDS can be used for efficient exploration of different designs by using evolutionary techniques .Finally we prove our approach using two case studies - one synthetic and another biological . Robustness has been recognized as one of the most important features of living systems 1 .In particular , it plays crucial role during embryogenesis when cells transform into various tissues and tissues 2 , but also subsequently in life 3 . In this research we propose a new computational framework for constructing robust multicellular systems .Our model combines the advantages of both bottom - up 4 and bottom - down 5 methods , while overcoming some of their limitations . Bottom - up methods are typically utilized to model cell transformation 6 or morphogenetic systems 7 .They often beginning with a single cell type and then evolve towards more sophisticated shapes through consecutive divisions and / or genes 8 . Top - down methods using genetic programming 9 or other optimization tools 10 to search for efficient answers within pre - defined constraints 11 .However , these algorithms often take extensive tuned of values 12 and may come stuck at local optima 13 . Our concept employs a novel concept called embryo -",
        "rewrite_text": "Title: Robust Multi-Cellular Developmental Design\n\nAbstract: In this article, we introduce a novel perspective on robust multi-cellular developmental architecture that effectively integrates the strengths of both bottom-up and top-down methodologies while addressing their respective limitations. Central to our approach is the innovative concept of Embryonic Development Space (EDS), which encompasses all conceivable designs characterized by various gene groupings and their interconnections. We demonstrate how EDS facilitates the efficient exploration of diverse designs through the application of evolutionary techniques. Our framework is validated through two case studies: one synthetic and the other biological, illustrating its versatility and effectiveness.\n\nRobustness is widely acknowledged as a critical attribute of living systems, particularly during embryogenesis, where cells differentiate into various tissues and structures, and it continues to play a vital role throughout an organism's life. In this research, we propose a computational framework aimed at constructing robust multicellular systems. Our model synergizes the benefits of both bottom-up and top-down approaches while mitigating some of their inherent drawbacks. \n\nBottom-up methods typically focus on modeling cell transformation or morphogenetic processes, starting from a single cell type and evolving towards more complex forms through successive divisions and genetic modifications. Conversely, top-down approaches leverage genetic programming and other optimization techniques to identify efficient solutions within predefined constraints. However, these algorithms often require extensive parameter tuning and may become trapped in local optima, limiting their effectiveness.\n\nBy employing the concept of Embryonic Development Space, our framework allows for a more comprehensive exploration of design possibilities, enhancing the robustness and adaptability of multicellular systems. This research not only contributes to the understanding of developmental biology but also provides a foundational tool for future studies in synthetic biology and tissue engineering.",
        "ori-fast-z-score": -0.17677669529663687,
        "water-fast-z-score": 8.06893377762467,
        "rewrite-fast-z-score": 0.08606629658238704
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Optimal Occulter Design for Finding Extrasolar Planets .\nAbstract:\nWe present an optimal design method to find the best occulting mask that can be used in direct imaging searches for extrasolar planets.  The proposed method is based on the concept of entropy maximization, which has been widely applied in many fields such as information theory and statistical mechanics.   We show how this concept can be extended into the field of optics by introducing a new quantity called optical entropy (OE).   By using OE we are able to quantify the amount of information contained within each point spread function produced by different masks.   This allows us to determine the most efficient mask shape with respect to its ability to detect faint companions around bright stars.   ...  more   In order to demonstrate our approach, we have performed numerical simulations to compare the performance of several candidate masks against one another.   Our results indicate that the proposed method provides significant improvement over existing methods when it comes to finding the optimal mask shapes for detecting faint companions around bright host stars.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Optimal Occulter Design for Finding Extrasolar Planets . Abstract : We present an appropriate design algorithm to find the best occulting mask that can be used in direct imaging searches for extrasolar planets .The proposed approach is based on the idea of entropy maximization , which has been widely applied in different fields such as data physics and statistical mechanics . We see how this concept can be applied into the field of optics by using a new quantity called optical entropy ( OE ) .By using OE we are able to quantify the quantity of information stored within each point spread function generated by various masks . This enables us to identify the most efficient mask shape with regard to its able to identify dim companions around bright stars .. . . more In order to test our approach , we have done mathematical simulations to study the performance of several candidate masks against one another . Our results show that the suggested method provides significant progress over existing techniques when it comes to finding the ideal mask shapes for detecting bright companions around bright host stars .",
        "rewrite_text": "In this article, we introduce a novel design algorithm aimed at optimizing occulting masks for direct imaging searches of extrasolar planets. Our methodology is grounded in the principle of entropy maximization, a concept that has found applications across various disciplines, including data physics and statistical mechanics. We extend this principle to the field of optics by introducing a new metric termed optical entropy (OE). This innovative measure allows us to quantify the information content inherent in each point spread function produced by different occulting masks. By leveraging OE, we can systematically evaluate and determine the most effective mask shapes for detecting faint companions in the vicinity of luminous stars.\n\nTo validate our approach, we conducted a series of mathematical simulations to compare the performance of various candidate masks. The results of these simulations demonstrate that our proposed method significantly outperforms existing techniques in identifying optimal mask designs. Specifically, we found that the masks generated through our entropy-based approach exhibit enhanced capabilities in isolating and detecting dim celestial bodies around bright stellar hosts. This advancement represents a meaningful step forward in the quest for extrasolar planet detection, providing researchers with a powerful tool to enhance the efficacy of direct imaging methods. Our findings not only contribute to the ongoing development of optical systems for astronomical observations but also pave the way for future investigations into the characteristics and distributions of planets beyond our solar system.",
        "ori-fast-z-score": -0.41702882811414954,
        "water-fast-z-score": 5.910624659963424,
        "rewrite-fast-z-score": 0.09016696346674323
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A unified model for AGN feedback in cosmological simulations of structure formation .\nAbstract:\nWe present a new method to implement the effect of Active Galactic Nuclei (AGNs) on their host galaxies and surrounding intergalactic medium into hydrodynamical cosmological simulations, which is based on an analytical description of the physical processes involved. The main idea behind our approach is that we use the observed scaling relations between black hole mass, luminosity and velocity dispersion as well as the correlation between radio loudness and Eddington ratio to predict the properties of individual AGNs at each time step during the simulation run. We then inject thermal energy or kinetic momentum into the gas particles around these objects according to their predicted luminosities and velocities. Our implementation allows us to reproduce several observational results such as the M-σ relation, the fundamental plane of elliptical galaxies, the fraction of red sequence galaxies and the evolution of galaxy clustering with redshift. In addition, it also leads to realistic predictions about the number density distribution of quasars and the cosmic X-ray background spectrum.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A unified model for AGN feedback in cosmological simulations of formation formation . Abstract : We introduce a new method to introduce the impact of Active Galactic Nuclei ( AGNs ) on their host galaxies and surrounding intergalactic medium into hydrodynamical cosmological simulations , which is based on an analytical explanation of the physical processes responsible .The main idea behind our approach is that we using the known scaling relations between black hole mass , luminosity and speed dispersion as also as the relationship between radio loudness and Eddington proportion to predict the properties of different AGNs at each time step during the model run . We then inject heat power or kinetic momentum into the gas ions around these objects according to their expected luminosities and velocities .Our formulation enables us to capture several observational results such as the M - σ relation , the fundamental plane of elliptical galaxies , the fraction of red sequence galaxies and the evolution of galaxy clustering with redshift . In addition , it also leads to accurate calculations about the number density distribution of quasars and the cosmic X - ray background spectrum .",
        "rewrite_text": "We present a novel methodology for incorporating the effects of Active Galactic Nuclei (AGNs) on their host galaxies and the surrounding intergalactic medium within hydrodynamical cosmological simulations. This approach is grounded in an analytical framework that elucidates the physical processes at play. Central to our method is the utilization of established scaling relations that connect black hole mass, luminosity, and velocity dispersion, alongside the correlation between radio loudness and Eddington ratio. These relationships allow us to predict the characteristics of various AGNs at each time step throughout the simulation. We subsequently introduce thermal energy or kinetic momentum into the surrounding gas ions based on the anticipated luminosities and velocities of the AGNs.\n\nOur formulation successfully reproduces several key observational phenomena, including the M-σ relation, the fundamental plane of elliptical galaxies, the prevalence of red sequence galaxies, and the evolution of galaxy clustering as a function of redshift. Furthermore, it provides precise estimations of the number density distribution of quasars and the cosmic X-ray background spectrum. By integrating these elements into cosmological simulations, our model enhances the understanding of AGN feedback mechanisms and their role in galaxy formation and evolution. This unified approach not only aligns with existing observational data but also offers a comprehensive framework for future studies on the interplay between AGNs and their environments in the context of cosmic structure formation.",
        "ori-fast-z-score": -0.10846522890932808,
        "water-fast-z-score": 4.960307567358167,
        "rewrite-fast-z-score": 0.20203050891044214
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  An Adaptive Strategy for the Classification of G-Protein Coupled Receptors .\nAbstract:\nG-protein-coupled receptors (GPCRs) are one of the largest families of cell surface proteins and play an important role in many physiological processes, including vision, olfaction, taste, hormone secretion, neurotransmission, immune response, inflammation, and cancer.  The number of known human GPCR sequences has increased dramatically over recent years due to advances in genome sequencing technologies; however, only about half have been functionally characterized so far. In this study we present a novel computational method that can be used to predict the functional class of uncharacterized GPCRs based on their sequence similarity with other members within each subfamily. Our approach is based on two key ideas: 1) We use a new adaptive strategy to select representative sequences from different subfamilies by considering both the diversity among them as well as their evolutionary relationships; 2) We develop a new scoring scheme which takes into account not only the pairwise sequence similarities but also the structural information between query and template sequences. Using our method, we were able to successfully classify more than 80% of all human GPCRs whose functions had previously been experimentally determined.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : An Adaptive Strategy for the Classification of G - Protein Coupled Receptors . Abstract : G - protein - coupled receptors ( GPCRs ) are one of the greatest families of cell surface proteins and play an important role in multiple physiological mechanisms , notably vision , olfaction , taste , hormone secretion , neurotransmission , immune reaction , inflammation , and tumors .The amount of known human GPCR sequences has increased significantly over recent months thanks to advances in genome sequencing technologies ; however , only about half have been functionally distinguished so far . In this study we present a new computational technique that can be used to predict the functional class of uncharacterized GPCRs based on their sequence similarity with other members within each subfamily .Our model is based on two fundamental ideas : 1 ) We use a new adaptive approach to select representative sequences from diverse subfamilies by examining both the complexity among them as well as their evolutionary relationships ; 2 ) We develop a new scoring scheme which assumes into consideration not only the pairwise sequence differences but also the structural information between query and template sequences . Using our technique , we were able to effectively classify more than 80 % of all human GPCRs whose functions had earlier been experimentally determined .",
        "rewrite_text": "**Title:** An Adaptive Strategy for the Classification of G-Protein Coupled Receptors\n\n**Abstract:** G-protein coupled receptors (GPCRs) represent one of the largest and most diverse families of cell surface proteins, playing crucial roles in various physiological processes including vision, olfaction, taste, hormone secretion, neurotransmission, immune responses, inflammation, and tumor progression. Recent advancements in genome sequencing technologies have led to a significant increase in the number of known human GPCR sequences; however, only approximately half of these have been functionally characterized to date. In this article, we introduce a novel computational method designed to predict the functional classification of uncharacterized GPCRs by leveraging their sequence similarities with other known members within their respective subfamilies. Our approach is grounded in two key principles: first, we employ an adaptive strategy to select representative sequences from a wide array of subfamilies, taking into account both the complexity of the sequences and their evolutionary relationships; second, we propose an innovative scoring system that integrates not only pairwise sequence variations but also structural information between the query and template sequences. By applying our method, we achieved a classification accuracy exceeding 80% for human GPCRs whose functions had previously been determined through experimental means. This advancement not only enhances our understanding of GPCR functionality but also provides a valuable tool for future research in pharmacology and molecular biology, potentially aiding in the identification of new therapeutic targets and the development of novel drugs. Our findings underscore the importance of computational techniques in the functional annotation of GPCRs and highlight the potential for further exploration in this vital area of biomedical research.",
        "ori-fast-z-score": 0.10153461651336192,
        "water-fast-z-score": 6.0609152673132645,
        "rewrite-fast-z-score": 0.17677669529663687
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Effect of temperature-dependent shape anisotropy on coercivity with aligned Stoner-Wohlfarth soft ferromagnets .\nAbstract:\nWe study the effect of temperature dependent shape anisotropy in an exchange coupled system consisting of two identical uniaxial single domain particles, one being magnetically softer than the other and both having their easy axes parallel to each other. We show that for certain values of the parameters involved there is a significant increase in the coercive field at low temperatures compared to high temperatures. This can be understood by considering the competition between the Zeeman energy barrier due to the applied magnetic field and the thermal activation energy barrier associated with the temperature dependence of the shape anisotropy. \n \n The model we consider consists of two identical spherical particles (with radius R) separated by a distance d along the z-axis. Each particle has its own uniaxial anisotropy constant Ks(T), where T denotes the temperature. In addition, they are also exchange-coupled through a coupling constant J. For simplicity, we assume that the anisotropy constants have the same functional form as given below, \n \n Ks = K1 + K2 tanh -(T/Tc) ,\n \nwhere Tc is some characteristic temperature scale which determines how rapidly the anisotropy changes with temperature.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Effect of temperature - dependent shape anisotropy on coercivity with aligned Stoner - Wohlfarth soft ferromagnets . Abstract : We research the impact of temperature dependent shape anisotropy in an exchange coupled system consisting of two different uniaxial single domain particles , one being magnetically softer than the other and both having their easy axes perpendicular to each other .We see that for particular values of the variables required there is a substantial rise in the coercive field at low temperatures relative to large conditions . This can be understood by examining the competition between the Zeeman electricity barrier thanks to the applied magnetic field and the thermal activation energy barrier associated with the temperature dependence of the shape anisotropy .The model we define consists of two equal spherical objects ( with diameter R ) connected by a distance d along the z - axis . Each particle has its own uniaxial anisotropy constant Ks ( T ) , where T denotes the temperature .In addition , they are also exchange - coupled through a coupling constant J . For simplicity , we suppose that the anisotropy constants have the same functional form as provided below , Ks = K1 + K2 tanh - ( T / Tc ) , where Tc is some characteristic temperature scale which determines how swiftly the anisotropy changes with temperature .",
        "rewrite_text": "**Title:** Effect of Temperature-Dependent Shape Anisotropy on Coercivity in Aligned Stoner-Wohlfarth Soft Ferromagnets\n\n**Abstract:** This study investigates the influence of temperature-dependent shape anisotropy on the coercivity of an exchange-coupled system composed of two distinct uniaxial single-domain particles, characterized by differing magnetic softness and oriented with their easy axes perpendicular to one another. Our findings reveal that under specific conditions, there is a notable increase in the coercive field at lower temperatures compared to higher temperature scenarios. This phenomenon can be attributed to the interplay between the Zeeman energy barrier induced by an external magnetic field and the thermal activation energy barrier that arises from the temperature-dependent nature of shape anisotropy. \n\nThe model we propose features two identical spherical particles, each with a diameter R, separated by a distance d along the z-axis. Each particle possesses its own uniaxial anisotropy constant, denoted as Ks(T), where T represents the temperature. Furthermore, the particles are coupled through an exchange interaction characterized by a coupling constant J. For the sake of simplicity, we assume that the anisotropy constants share a common functional form, expressed as Ks = K1 + K2 tanh(-T/Tc), where Tc is a critical temperature scale that governs the rate at which anisotropy varies with temperature.\n\nOur analysis highlights the critical role of temperature-dependent shape anisotropy in determining the coercivity of the system, particularly in low-temperature regimes. The results underscore the significance of understanding the thermal dynamics of magnetic materials, which can have profound implications for the design and application of magnetic storage devices and other technologies reliant on ferromagnetic properties. This research contributes to the broader understanding of magnetic behavior in soft ferromagnets and opens avenues for further exploration of anisotropic effects in magnetic systems.",
        "ori-fast-z-score": -0.7258661863112977,
        "water-fast-z-score": 4.125684985035173,
        "rewrite-fast-z-score": 1.5787044347526527
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Intramolecular long-range correlations in polymer melts: The segmental size distribution and its moments .\nAbstract:\nThe intramolecular structure of polymers is studied by means of the molecular dynamics simulation method for several model systems with different chain lengths, degrees of branching, and types of interactions between monomers. It has been found that the intramolecular correlation function can be represented as a sum of two terms corresponding to short- and long-range contributions. The latter term describes the contribution of segments separated by distances larger than the average distance between neighboring chains. This term depends on the number density of chains and their length. In addition, it was shown that this term decreases exponentially at large separations. The dependence of the first four moments of the segmental size distribution on temperature and pressure are calculated using the results obtained previously. These dependences were compared with experimental data available in literature. Good agreement between theory and experiment is observed. \n \n Keywords: Polymer melt, Intramolecular structure, Correlation functions, Segmental size distribution",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Intramolecular large - range correlations in polymer melts : The segmental size distribution and its moments . Abstract : The intramolecular structure of polymers is studied by means of the molecular dynamics simulation method for numerous model structures with various chain lengths , degrees of branching , and types of relationships between monomers .It has been seen that the intramolecular correlation function can be described as a sum of two terms corresponding to short - and long - range contributions . The last word refers the contribution of segments connected by distances bigger than the average distance between neighboring chains .This term varies on the number density of chains and their length . In addition , it was shown that this term varies exponentially at large separations .The dependence of the first four moments of the segmental size distribution on temperature and pressure are measured using the results derived earlier . These dependences were compared with experimental evidence available in literature .Good agreement between theoretical and observation is observed . Keywords : Polymer melt , Intramolecular structure , Correlation properties , Segmental size distribution",
        "rewrite_text": "**Title:** Intramolecular Large-Range Correlations in Polymer Melts: The Segmental Size Distribution and Its Moments\n\n**Abstract:** This study investigates the intramolecular structure of polymers through molecular dynamics simulations, focusing on a variety of model systems characterized by differing chain lengths, branching degrees, and monomer interactions. The research reveals that the intramolecular correlation function can be effectively represented as a combination of two distinct terms: one associated with short-range interactions and the other with long-range contributions. The long-range term specifically pertains to segments that are connected by distances exceeding the average separation between adjacent chains. Notably, this long-range contribution is influenced by both the number density of the polymer chains and their respective lengths, exhibiting an exponential variation at significant separations. Furthermore, the study examines how the first four moments of the segmental size distribution are affected by changes in temperature and pressure, utilizing previously derived results. These findings are then compared with existing experimental data from the literature, demonstrating a strong correlation between theoretical predictions and empirical observations. The results underscore the importance of understanding the intricate relationships within polymer melts, particularly in terms of their segmental size distribution and correlation properties. This work contributes valuable insights into the fundamental behavior of polymer systems, which is crucial for applications in materials science and engineering. \n\n**Keywords:** Polymer melt, Intramolecular structure, Correlation properties, Segmental size distribution.",
        "ori-fast-z-score": 0.52999894000318,
        "water-fast-z-score": 4.926938732593016,
        "rewrite-fast-z-score": 1.958260097304659
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Network Topology of an Experimental Futures Exchange .\nAbstract:\nWe present the network topology and trading volume for the first experimental futures exchange, which was launched in September 2009 as part of the University of Chicago s Financial Markets Lab (FML). The FML is designed to provide students with hands-on experience in designing, implementing, and operating financial markets. We find that the network exhibits small-world properties similar to those observed on other real world networks such as social networks or the Internet. In addition we show how the network evolves over time by analyzing its growth rate and degree distribution. Finally, we study the relationship between the number of transactions executed at each node and their respective degrees. Our results suggest that nodes with higher degrees tend to trade more than lower-degree nodes. This finding has important implications for market design since it suggests that traders should be incentivized to increase their connectivity within the network if they want to maximize their trading activity. We also examine whether there are any differences across different types of contracts traded on the exchange.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Network Topology of an Experimental Futures Exchange . Abstract : We present the network topology and trading volume for the first theoretical futures exchange , which was launched in September 2009 as part of the University of Chicago s Financial Markets Lab ( FML ) .The FML is designed to provide children with fingers - on experience in planning , implementing , and operating financial markets . We see that the channel exhibits small - world properties similar to those observed on other actual world networks such as social sites or the Internet .In addition we study how the organization evolves over time by analyzing its expansion frequency and degree distribution . Finally , we study the relationship between the quantity of transactions executed at each node and their individual degrees .Our results show that nodes with higher degrees prefer to trade more than higher - degree connections . This conclusion has critical impacts for trade design since it suggests that buyers should be incentivized to expand their connectivity within the network if they wish to maximize their trading activity .We also investigate whether there are any variations across different kinds of contracts traded on the trade .",
        "rewrite_text": "In this article, we explore the network topology and trading volume associated with the inaugural theoretical futures exchange, which was established in September 2009 as part of the University of Chicago's Financial Markets Lab (FML). The FML aims to provide students with hands-on experience in the planning, implementation, and operation of financial markets. Our analysis reveals that the trading network exhibits small-world characteristics, akin to those found in various real-world networks, including social media platforms and the Internet. Furthermore, we examine the evolution of the network over time by assessing its expansion frequency and degree distribution. \n\nA key focus of our study is the correlation between the volume of transactions executed at each node and the individual degrees of those nodes. Our findings indicate that nodes with higher degrees tend to engage in more trading activity compared to those with lower degrees. This insight has significant implications for trade design, suggesting that incentivizing buyers to enhance their connectivity within the network could lead to increased trading activity. Additionally, we investigate potential variations in trading patterns across different types of contracts within the exchange. Overall, our research contributes to a deeper understanding of the dynamics of trading networks and offers valuable recommendations for optimizing trading strategies in futures exchanges.",
        "ori-fast-z-score": -1.0540925533894598,
        "water-fast-z-score": 5.555909634626167,
        "rewrite-fast-z-score": 0.2847473987257497
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dynamics and planet formation in/around binaries .\nAbstract:\nWe present the results of numerical simulations of planetary systems around binary stars, focusing on the effects of orbital migration due to tidal interaction between planets and their host star(s). We find that for most initial conditions considered here (except those with very small semi-major axes), the final orbits are generally circularized by tides raised on the planet by its host star(s); this is true even if the original orbit was eccentric or inclined relative to the binary s plane.  The final mass distribution of planets depends strongly on the initial semimajor axis of the planet. For large values of the initial semi-major axis, we find that the final masses tend to be smaller than the initial ones because of significant inward migration driven by tidal dissipation inside the planet. On the other hand, when the initial semi-major axis is relatively small compared to the binary separation, the final masses can become larger than the initial ones as a result of outward migration caused by tidal friction acting at the surface of the planet.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dynamics and planet development in / around binaries . Abstract : We present the conclusion of computational simulations of planetary structures around binary stars , concentrating on the effects of orbital movement owing to tidal association between planets and their host star ( s ) .We see that for most initial conditions considered here ( except those with very small semi - major axes ) , the last orbits are typically circularized by tides raised on the planet by its host star ( s ) ; this is true even if the previous orbit was eccentric or tilted relative to the binary s plane . The final mass distribution of planets depends strongly on the first semimajor axis of the planet .For large values of the first semi - major axis , we find that the last masses tend to be less than the first ones because of large inward movement generated by tidal dissipation inside the planet . On the other hand , when the first semi - major axis is fairly little compared to the binary merger , the last masses can turn larger than the first ones as a due of outward migration caused by tidal tension acted at the surface of the planet .",
        "rewrite_text": "Title: Dynamics and Planet Development in Binary Systems\n\nAbstract: This study presents the findings from computational simulations investigating the formation and evolution of planetary systems around binary stars, with a particular focus on the influence of orbital dynamics resulting from tidal interactions between planets and their host stars. Our simulations reveal that, under a majority of initial conditions examined—except for those involving very small semi-major axes—the final orbits of planets tend to become circularized due to tidal forces exerted by their host stars. This circularization occurs even when the planets initially possess eccentric or tilted orbits relative to the binary's orbital plane. Furthermore, we observe that the ultimate mass distribution of planets is significantly influenced by their initial semi-major axes. Specifically, for planets with larger initial semi-major axes, we find that their final masses are generally lower than their initial masses. This reduction in mass is attributed to substantial inward migration driven by tidal dissipation processes occurring within the planets. Conversely, for planets with smaller initial semi-major axes in relation to the binary system, we note that the final masses can exceed the initial masses. This phenomenon is a result of outward migration induced by tidal forces acting on the planet's surface. Our results underscore the complex interplay between tidal interactions and planetary dynamics in binary systems, providing valuable insights into the mechanisms that govern planet formation and evolution in such environments.",
        "ori-fast-z-score": -3.1378581622109447,
        "water-fast-z-score": 4.196370313678693,
        "rewrite-fast-z-score": -0.8638684255813601
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Distortion Minimization in Gaussian Layered Broadcast Coding with Successive Refinement .\nAbstract:\nWe consider the problem of successive refinement coding for layered broadcast systems, where each receiver is interested only in one out of several layers and can decode all other layers as side information at no cost. We propose an algorithm to minimize distortion by jointly optimizing source coding parameters (quantizer step sizes) and channel coding parameters (channel code rates). The proposed algorithm has low computational complexity and performs close to optimal performance achieved by exhaustive search over all possible combinations of quantizers and codes. Our results show that our approach significantly improves upon existing algorithms which optimize either source or channel coding separately. \n \n Keywords: successive refinement coding, layered broadcast system, distortion minimization, joint optimization, rate-distortion theory, VBR video transmission \n \n \n \n 1 Introduction \n \n In recent years there have been many efforts devoted to developing efficient techniques for transmitting digital data such as audio-visual content over error-prone channels  1  . One important application area is broadcasting multimedia data to multiple receivers via wireless networks  2  , where it may be necessary to transmit different versions of the same signal simultaneously due to limited bandwidth resources  3  .\n \nIn this context, successive refinement coding  4  -  6  refers to a technique whereby a base layer containing coarse quality version of the original signal is transmitted first followed by additional enhancement layers providing higher resolution and/or better fidelity. Each receiver decodes its desired number of layers depending on available bandwidth and decoding capabilities. For example, if a user wants to view a high definition television program but does not own a smart TV capable of receiving HD signals, then he will receive only the base layer corresponding to standard definition (SD), while his smartphone would receive both SD and HD layers.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Distortion Minimization in Gaussian Layered Broadcast Coding with Successive Refinement . Abstract : We consider the issue of successive refinement compression for structured transmission systems , where each receiver is interested only in one out of several layers and can decode all other layers as side data at no price .We suggest an algorithm to minimize distortion by jointly optimizing source coding variables ( quantizer step lengths ) and channel coding variables ( channel code rates ) . The proposed algorithm has low computational complexity and performs close to optimal performance achieved by exhaustive search over all possible combinations of quantizers and coding .Our results show that our approach dramatically improves upon existing algorithms which optimize either source or channel code separately . Keywords : successive refinement compression , layered broadcast network , noise minimization , joint optimization , rate - noise theory , VBR television broadcasting 1 Introduction In recent years there have been many efforts devoted to developing optimal methods for transmitting digital data such as audio - visual content over mistake - susceptible channels 1 .One important use area is transmitting multimedia data to multiple receivers via telecommunications networks 2 , where it could be required to transmit different versions of the same signal concurrently due to limited bandwidth resources 3 . In this sense , successive refinement coding 4 - 6 relates to a technique whereby a base layer containing coarse quality version of the original signal is transmitted initially followed by additional enhancement layers supplying lower resolution and / or improved fidelity .Each receiver decodes its appropriate number of layers depending on available bandwidth and decoding capabilities . For instance , if a person wants to see a high resolution television show but does not own a smart TV capable of receiving HD transmissions , then he will receive only the base layer equivalent to standard definition ( SD ) , while his smartphone would receive both SD and HD layers .",
        "rewrite_text": "**Title:** Distortion Minimization in Gaussian Layered Broadcast Coding with Successive Refinement\n\n**Abstract:** This article addresses the challenge of successive refinement compression within structured transmission systems, where each receiver is focused on a specific layer while simultaneously being able to decode other layers as ancillary data without incurring additional costs. We propose a novel algorithm aimed at minimizing distortion by jointly optimizing both source coding parameters (specifically, quantizer step sizes) and channel coding parameters (including channel code rates). Our algorithm is characterized by its low computational complexity and demonstrates performance that closely approaches the optimal results obtained through exhaustive searches across all potential combinations of quantizers and coding strategies. The findings indicate that our method significantly outperforms existing algorithms that typically optimize either source or channel coding independently. \n\nIn recent years, considerable attention has been directed towards developing optimal transmission methods for digital data, particularly audio-visual content, over error-prone channels. A key application area is the transmission of multimedia data to multiple receivers via telecommunications networks, where it may be necessary to send different versions of the same signal simultaneously due to bandwidth constraints. Successive refinement coding is a relevant technique in this context, where an initial base layer containing a coarse-quality version of the original signal is transmitted, followed by additional enhancement layers that provide either lower resolution or improved fidelity. Each receiver is able to decode a specific number of layers based on its available bandwidth and decoding capabilities. For example, a viewer wishing to watch a high-resolution television program on a non-HD capable device would receive only the base layer corresponding to standard definition (SD), while a smartphone could decode both the SD and high-definition (HD) layers. This layered approach allows for efficient use of resources while catering to varying receiver capabilities. \n\n**Keywords:** successive refinement compression, layered broadcast network, noise minimization, joint optimization, rate-noise theory, VBR television broadcasting.",
        "ori-fast-z-score": 0.0778498944161523,
        "water-fast-z-score": 6.6172410253729455,
        "rewrite-fast-z-score": -1.1992507023933783
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  k-dependent SU(4) model of high-temperature superconductivity and its coherent-state solutions .\nAbstract:\nWe present the results for the k-dependent SU(4) Hubbard model in two dimensions at half-filling, which is known to be equivalent to the t-J model with J = 4t2/U . We solve this problem by using the method developed recently by one of us (S.M.) based on the exact diagonalization combined with the density-matrix renormalization group technique. The ground state phase diagram obtained shows that there are three phases; antiferromagnetic insulator, d-wave superconductor, and ferromagnetically ordered metal. In particular we find that the d-wave superconducting region becomes larger as U increases. This result suggests that the strong-coupling limit may give rise to an exotic pairing mechanism different from the weak-coupling BCS theory. Finally we discuss possible relevance of our results to the cuprate hightemperature superconductors. PACS: 71.10.Fd",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : k - dependent SU ( 4 ) model of high - temperature superconductivity and its coherent - state solutions . Abstract : We present the results for the k - dependent SU ( 4 ) Hubbard theory in two dimensions at half - filling , which is known to be analogous to the t - J model with J = 4t2 / U .We answer this question by using the method developed ago by one of us ( S . M . ) based on the exact diagonalization combined with the density - vector renormalization group technique .The ground state phase diagram obtained shows that there are three stages ; antiferromagnetic insulator , d - wave superconductor , and ferromagnetically ordered metal . In particular we find that the d - wave superconducting area gets wider as U rises .This result suggests that the strong - coupling limitation would give rise to an exotic pairing structure different from the strong - coupling BCS theory . Finally we explain possible relevance of our findings to the cuprate hightemperature superconductors .PACS: 71.10.Fd",
        "rewrite_text": "Title: k-Dependent SU(4) Model of High-Temperature Superconductivity and Its Coherent-State Solutions\n\nAbstract: In this study, we explore the k-dependent SU(4) Hubbard model in two dimensions at half-filling, which exhibits similarities to the t-J model with J defined as 4t²/U. Utilizing a methodology previously developed by one of the authors (S.M.), we employ exact diagonalization in conjunction with the density-vector renormalization group technique to investigate the model's properties. Our findings reveal a comprehensive ground state phase diagram characterized by three distinct phases: an antiferromagnetic insulator, a d-wave superconductor, and a ferromagnetically ordered metal. Notably, we observe that the region corresponding to d-wave superconductivity expands with increasing values of U. This observation implies that in the strong-coupling regime, the pairing mechanism may exhibit an unconventional structure that diverges from the traditional strong-coupling Bardeen-Cooper-Schrieffer (BCS) theory. Furthermore, we discuss the implications of our results in the context of cuprate high-temperature superconductors, suggesting that our model may provide insights into the underlying mechanisms of superconductivity in these materials. Our work contributes to the understanding of high-temperature superconductivity by elucidating the role of strong correlations and the nature of pairing in complex quantum systems. PACS: 71.10.Fd",
        "ori-fast-z-score": 0.11547005383792514,
        "water-fast-z-score": 4.588314677411235,
        "rewrite-fast-z-score": 0.21320071635561041
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Anisotropy and magnetization reversal with chains of submicron-sized Co hollow spheres .\nAbstract:\nWe report on the magnetic properties of self-assembled arrays of cobalt (Co) hollow spheres, which are prepared by an electrochemical deposition method onto carbon-coated copper grids. The samples show anisotropic behavior in their hysteresis loops at room temperature as well as superparamagnetic characteristics above 300 K. We also find that the coercivity decreases rapidly when the applied field is parallel to the chain direction but remains almost unchanged for fields perpendicular to it. This indicates that the easy axis lies along the chain direction. In addition, we observe that the remanent magnetization increases gradually up to about 100 Oe after annealing the sample at 400 °C under vacuum conditions. These results suggest that the observed anisotropy originates mainly from shape effects rather than inter-particle interactions. \n \n Keywords: Anisotropy, Cobalt, Hollow sphere, Self-assembling, Superparamagnetism",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Anisotropy and magnetization reversal with chains of submicron - sized Co hollow spheres . Abstract : We report on the magnetic properties of self - assembled arrays of cobalt ( Co ) hollow spheres , which are prepared by an electrochemical deposition technology onto carbon - wrapped copper grids .The samples show anisotropic characteristics in their hysteresis loops at room temperature as well as superparamagnetic qualities above 300 K . We additionally find that the coercivity decreases quickly when the introduced field is parallel to the chain direction but maintains largely unchanged for fields perpendicular to it . This implies that the easy axis lies along the chain direction .In addition , we determine that the remanent magnetization increases slowly up to about 100 Oe after annealing the sample at 400 °C under vacuum environments . These conclusions show that the reported anisotropy derives mainly from shape effects rather than inter - particle relationships .Keywords: Anisotropy, Cobalt, Hollow sphere, Self-assembling, Superparamagnetism",
        "rewrite_text": "Title: Anisotropy and Magnetization Reversal in Chains of Submicron-Sized Cobalt Hollow Spheres\n\nAbstract: This study investigates the magnetic characteristics of self-assembled arrays of cobalt (Co) hollow spheres, which are synthesized using an electrochemical deposition method on carbon-coated copper grids. Our findings reveal that these samples exhibit notable anisotropic behavior in their hysteresis loops at room temperature, alongside superparamagnetic properties at temperatures exceeding 300 K. A key observation is that the coercivity of the samples diminishes significantly when an external magnetic field is applied parallel to the direction of the chains, while it remains relatively stable when the field is oriented perpendicularly. This behavior indicates that the easy axis of magnetization aligns with the direction of the chains. Furthermore, we observe that the remanent magnetization gradually increases up to approximately 100 Oe following the annealing of the samples at 400 °C in a vacuum environment. These results suggest that the anisotropic properties are primarily influenced by the geometric shape of the hollow spheres rather than interactions between individual particles. The implications of these findings are significant for the understanding of magnetic behavior in nanostructured materials and could pave the way for advancements in applications such as magnetic storage and spintronic devices. \n\nKeywords: Anisotropy, Cobalt, Hollow Sphere, Self-Assembly, Superparamagnetism",
        "ori-fast-z-score": 0.5852057359806528,
        "water-fast-z-score": 5.347391382215687,
        "rewrite-fast-z-score": 1.7253243712550146
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Determination of the Form Factors for the Decay B0 -> D*- l+ nu_l and of the CKM Matrix Element |V_cb| .\nAbstract:\nWe present an analysis of B meson decays to final states with charmed particles, using data collected by the BABAR experiment at SLAC in 1999-2000 corresponding to an integrated luminosity of about 40 fb-1 . We measure the branching fractions for the decay modes B 0 -->D*+l- nu_l; B + -->D*0nu_l; B + -->D0*nu_l; B + ,B 0 -->D*+s nu_s; and B + ,B 0 →D*s nu_s.  From these measurements we extract values for the form factors f0(q2), f1(q2) and f2(q2). Using our results on the branching fraction and lifetime ratio between the two-body and three-body charm-strange B-decays, we determine the Cabibbo-Kobayashi-Maskawa matrix element Vcb = (41.6 +/- 1.1 +/- 2.4) x 10-3 .  The first uncertainty is statistical, the second systematic, and the third due to theoretical uncertainties.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Determination of the Form Factors for the Decay B0 - > D * - l + nu _ l and of the CKM Matrix Element | V _ cb | . Abstract : We report an analysis of B meson decays to finished states with charmed particles , using data taken by the BABAR study at SLAC in 1999 - 2000 resulting to an integrated luminosity of about 40 fb - 1 .We determine the branching fractions for the decay modes B 0 - - > D * + l - nu _ l ; B + - - > D * 0nu _ l ; B + - - > D0 * nu _ l ; B + , B 0 - - > D * + s nu _ s ; and B + , B 0 →D * s nu _ s . From these measurements we extract values for the form factors f0 ( q2 ) , f1 ( q2 ) and f2 ( q2 ) .Using our findings on the branching percentage and lifetime percentage between the two - bodies and three - bodies charm - strange B - decays , we determine the Cabibbo - Kobayashi - Maskawa matrix element Vcb = ( 41 . 6 + / - 1 . 1 + / - 2 . 4 ) x 10 - 3 . The first uncertainty is empirical , the second systematic , and the third due to theoretical uncertainties .",
        "rewrite_text": "In this study, we present a comprehensive analysis of B meson decays into final states that include charmed particles, utilizing data collected by the BABAR experiment at SLAC during the years 1999 to 2000, which corresponds to an integrated luminosity of approximately 40 fb⁻¹. Our investigation focuses on several decay modes, specifically B⁰ → D*⁻ l⁺ νₗ, B⁺ → D*⁰ νₗ, B⁺ → D⁰* νₗ, and B⁺, B⁰ → D*ₛ νₛ, as well as B⁺, B⁰ → D*ₛ νₛ. Through these measurements, we are able to determine the branching fractions for each decay mode, which are crucial for understanding the underlying physics of B meson decays.\n\nFrom our results, we extract the form factors f₀(q²), f₁(q²), and f₂(q²), which play a significant role in the theoretical framework of these decays. Additionally, we analyze the branching ratios and lifetimes of both two-body and three-body charm-strange B decays, which allows us to derive the value of the Cabibbo-Kobayashi-Maskawa (CKM) matrix element |V_cb|. Our findings yield a value of |V_cb| = (41.6 ± 1.1 ± 2.4) × 10⁻³, where the first uncertainty is empirical, the second is systematic, and the third arises from theoretical considerations.\n\nThis work not only contributes to the precise measurement of the CKM matrix elements but also enhances our understanding of the dynamics involved in B meson decays, particularly those involving charm particles. The implications of our results are significant for the broader context of particle physics, as they provide essential insights into the behavior of fundamental interactions and the structure of the Standard Model.",
        "ori-fast-z-score": 0.2773500981126146,
        "water-fast-z-score": 4.354648431614539,
        "rewrite-fast-z-score": 0.30460384954008574
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Host Galaxies Catalog Used in LIGO Searches for Compact Binary Coalescence Events .\nAbstract:\nWe present the catalog of host galaxies used by the Laser Interferometer Gravitational-Wave Observatory (LIGO) to search for gravitational waves associated with compact binary coalescences, including black hole-neutron star and neutron star-black hole binaries.  The catalog contains information on all known host galaxies within the sensitivity volume of the first-generation Advanced LIGO detectors during their initial science runs between September 2005 and January 2010. We provide positions, redshifts, distances, and references for each galaxy. This is an update to our previous publication  1  . \nThe data are available at http://arxiv.org/abs/1306.5481. They can also be downloaded as a text file or via the Zenodo repository  2  .\nAuthor Information\n\nCorresponding Author:\nE-mail: jennifer.marrone@ligo.gov; Jennifer Marrone, National Science Foundation",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Host Galaxies Catalog Used in LIGO Searches for Compact Binary Coalescence Events . Abstract : We present the catalog of host galaxies utilized by the Laser Interferometer Gravitational - Wave Observatory ( LIGO ) to search for gravitational waves related with compact binary coalescences , notably black hole - neutron star and neutron star - black hole binaries .The collection contains information on all known host galaxies within the sensitivity volume of the first - generation Advanced LIGO detectors during their early scientific ran between September 2005 and January 2010 . We gather positions , redshifts , distances , and references for each galaxy .This is an update to our previous published 1 . The data are available at http : / / arxiv . org / abs / 1306 . 5481 .They can also be downloaded as a text file or via the Zenodo repository 2 . Author Information Corresponding Author : E - mail : jennifer . marrone @ ligo . gov ; Jennifer Marrone , National Science Foundation",
        "rewrite_text": "We introduce a comprehensive catalog of host galaxies employed by the Laser Interferometer Gravitational-Wave Observatory (LIGO) in its quest to detect gravitational waves associated with compact binary coalescence events, specifically focusing on black hole-neutron star and neutron star-black hole binaries. This catalog encompasses data on all known host galaxies located within the sensitivity volume of the first-generation Advanced LIGO detectors during their initial scientific operations from September 2005 to January 2010. The information compiled includes precise positions, redshifts, distances, and relevant references for each galaxy, providing a valuable resource for researchers in the field. This update enhances our previously published work, offering refined data and insights. The complete dataset is accessible online at http://arxiv.org/abs/1306.5481, and it can also be conveniently downloaded as a text file or through the Zenodo repository. This catalog serves as a crucial tool for ongoing and future gravitational wave searches, facilitating the identification and analysis of potential host galaxies for detected events. For further inquiries, the corresponding author, Jennifer Marrone, can be reached via email at jennifer.marrone@ligo.gov, representing the National Science Foundation.",
        "ori-fast-z-score": -0.2581988897471611,
        "water-fast-z-score": 1.8073922282301278,
        "rewrite-fast-z-score": 0.21081851067789195
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Towards Informative Statistical Flow Inversion .\nAbstract:\nWe present an algorithm for the statistical flow inversion problem that is based on variational Bayesian inference and uses Gaussian processes to model the forward operator. The proposed method allows us to obtain posterior distributions over the unknown source parameters, which can be used as prior information in subsequent inverse problems or uncertainty quantification studies. We demonstrate our approach by applying it to two different test cases with synthetic data generated using the finite element method (FEM). Our results show that we are able to recover the true source distribution accurately even if only few measurements are available. Furthermore, we compare our method against state-of-the-art algorithms and find that our approach outperforms them significantly when the number of measurement locations is small. Finally, we apply our method to real-world data obtained during a field experiment at the University of Houston s Space Research Center. This dataset consists of temperature measurements taken inside a building after a fire was started near one of its walls.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Towards Informative Statistical Flow Inversion . Abstract : We create an algorithm for the statistical flow inversion problem that is based on variational Bayesian inference and using Gaussian systems to model the forward operator .The proposed approach allows us to obtain posterior distributions over the undisclosed source parameters , which can be used as prior information in later inverse problems or likelihood quantification experiments . We suggest our approach by using it to two different test situations with artificial results collected using the finite element method ( FEM ) .Our results show that we are able to restore the true source distribution accurately even if only few measurements are available . Furthermore , we compare our technique against state - of - the - art algorithms and find that our approach outperforms them significantly when the quantity of measurement locations is tiny .Finally , we apply our technique to real - time measurements obtained during a field test at the University of Houston s Space Research Center . This dataset consists of temperature measurements taken inside a building after a fire was opened near one of its walls .",
        "rewrite_text": "Title: Towards Informative Statistical Flow Inversion\n\nAbstract: In this study, we present a novel algorithm designed to address the statistical flow inversion problem through the application of variational Bayesian inference, utilizing Gaussian systems to effectively model the forward operator. Our method facilitates the derivation of posterior distributions for hidden source parameters, which can subsequently serve as prior information in future inverse problems or in experiments aimed at quantifying likelihoods. We validate our approach by applying it to two distinct test scenarios, utilizing artificial data generated via the finite element method (FEM). The results demonstrate our ability to accurately reconstruct the true source distribution, even with a limited number of measurements. A comparative analysis with leading algorithms in the field reveals that our technique significantly outperforms existing methods, particularly in scenarios where the number of measurement locations is minimal. Additionally, we extend the application of our algorithm to real-time data collected during a field test at the University of Houston's Space Research Center. This dataset comprises temperature measurements recorded inside a building following the ignition of a fire near one of its walls. Our findings underscore the effectiveness of our approach in both simulated and real-world contexts, highlighting its potential for enhancing the accuracy of statistical flow inversion in various applications.",
        "ori-fast-z-score": 0.8432740427115678,
        "water-fast-z-score": 6.255432421712244,
        "rewrite-fast-z-score": 0.6831300510639733
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Long Term Fate of Our Digital Belongings: Toward a Service Model for Personal Archives .\nAbstract:\nThe rapid growth in the use and production of digital media has created an urgent need to develop new models that support long-term access, preservation, and reuse of personal archives.  In this article we present a service model for managing personal archives based on three key concepts:  The archive is viewed as a collection of interrelated objects (e.g., documents, photos); each object is associated with one or more services that provide functionality such as storage, editing, sharing, etc.; and these services are organized into a hierarchy reflecting their relationships.   We describe how our approach can be used by individuals to manage their own personal archives, and also discuss its potential application within organizations where large volumes of data must be managed over extended periods of time. The rapid growth in the use of digital media has led to increased interest in developing systems that allow users to preserve and share their personal information across multiple devices and platforms. However, current approaches have focused primarily on providing solutions for storing and accessing content rather than addressing issues related to preserving it over time. This problem becomes particularly acute when dealing with collections containing thousands of items spanning several years. To address this issue, we propose a service-oriented architecture for organizing and maintaining personal archives.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Long Term Fate of Our Digital Belongings : Toward a Service Model for Personal Archives . Abstract : The rapid increase in the using and production of digital media has established an urgent need to develop new models that facilitate long - term access , preservation , and reuse of personal records .In this page we present a service model for controlling personal records using on three key concepts : The archive is viewed as a collection of interrelated objects ( e . g . , files , photos ) ; each object is associated with one or more functions that provide functionality such as processing , editing , sharing , etc . ; and these services are structured into a structure indicating their connections .We define how our approach can be used by individuals to manage their own individual archives , and also discuss its potential application within organizations where vast volumes of records must be managed over longer periods of time . The rapid increase in the using of digital media has led to renewed emphasis in developing systems that enable users to contain and communicate their individual information across multiple computers and platforms .However , current approaches have concentrated mostly on providing strategies for storing and accessing information rather than solving aspects pertaining to preserving it over time . This problem appears particularly intense when dealing with libraries containing many of items spanning numerous years .To address this question , we propose a service - based architecture for organizing and keeping personal records .",
        "rewrite_text": "**Title:** The Long-Term Fate of Our Digital Belongings: Toward a Service Model for Personal Archives\n\n**Abstract:** The exponential growth in the creation and utilization of digital media has underscored the pressing need for innovative models that ensure the long-term accessibility, preservation, and reuse of personal records. In this article, we introduce a service model designed to facilitate the management of personal archives, grounded in three fundamental concepts. First, we conceptualize the archive as a cohesive collection of interconnected objects, such as files and photographs. Second, each object is linked to various functions that enhance usability, including processing, editing, and sharing capabilities. Third, these services are organized within a structured framework that illustrates their interrelationships.\n\nWe elaborate on how this model empowers individuals to effectively manage their personal archives while also exploring its applicability in organizational contexts, where extensive volumes of records require long-term stewardship. The surge in digital media usage has prompted a renewed focus on developing systems that allow users to store and share their information seamlessly across diverse computers and platforms. However, existing methodologies have largely prioritized strategies for information storage and access, often neglecting the critical issue of long-term preservation.\n\nThis challenge becomes particularly pronounced in libraries and repositories housing extensive collections accumulated over many years. To tackle this issue, we propose a service-oriented architecture that not only organizes but also sustains personal records over time. Our approach aims to bridge the gap between immediate accessibility and enduring preservation, ensuring that digital belongings remain accessible and usable for future generations. Through this framework, we hope to contribute to the ongoing discourse on digital archiving and preservation, providing a pathway for individuals and organizations to safeguard their digital heritage effectively.",
        "ori-fast-z-score": 0.3481553119113957,
        "water-fast-z-score": 9.278076673908084,
        "rewrite-fast-z-score": 2.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Surface plasmon polaritons and surface phonon polaritons on metallic and semiconducting spheres: Exact and semiclassical descriptions .\nAbstract:\nWe present exact solutions for the electromagnetic field in the presence of spherical particles with arbitrary dielectric functions, including both metals and insulators. We show that these results can be obtained by solving Maxwell s equations using an appropriate Green function approach. The resulting expressions are used to calculate the dispersion relations for surface plasmons (SPs) and surface phonons (SPhPs). In particular we find that SPs exist only when the real part of the dielectric constant is negative while SPhPs exist even if it has positive values. Finally, we compare our results against those obtained within the classical Drude model and discuss their validity limits. Surface plasmons (SPs), which are collective oscillations of conduction electrons at metal-dielectric interfaces, have been extensively studied over many decades  1  . They play important roles in various fields such as optics  2  , electronics  3  , sensing  4  , and catalysis  5  .\nRecently there has also been growing interest in studying surface phonon-polaritons (SPhPs), which are analogous excitations associated with longitudinal acoustic waves  6  . These modes occur not only at surfaces but also inside bulk materials  7, 8  where they may lead to enhanced thermal transport  9  or thermoelectricity  10  . Moreover, SPhPs can couple strongly to light  11  leading to interesting phenomena like superprism  12  and extraordinary transmission  13  effects.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Surface plasmon polaritons and surface phonon polaritons on metallic and semiconducting objects : Exact and semiclassical descriptions . Abstract : We present precise solutions for the electromagnetic field in the presence of spherical objects with arbitrary dielectric functions , including both metals and insulators .We see that these results can be obtained by solving Maxwell s equations using an appropriate Green function method . The resulting expressions are using to estimate the dispersion relations for ground plasmons ( SPs ) and surface phonons ( SPhPs ) .In particular we find that SPs occur only when the real part of the dielectric constant is zero while SPhPs exist even if it has positive values . Finally , we compare our findings against those achieved within the classical Drude theory and consider their efficacy limits .Surface plasmons ( SPs ) , which are collective oscillations of conduction electrons at metal - dielectric connections , have been heavily discovered over numerous years 1 . They play major roles in different fields such as optics 2 , electronics 3 , sensing 4 , and catalysis 5 .Recently there has especially been growing interest in investigating surface phonon - polaritons ( SPhPs ) , which are analogous excitations associated with longitudinal acoustic waves 6 . These modes happen not only at surfaces but also inside bulk surfaces 7 , 8 where they may contribute to enhanced thermal transport 9 or thermoelectricity 10 .Moreover , SPhPs can close intensely to light 11 contributing to curious phenomena like superprism 12 and exceptional transmission 13 phenomena .",
        "rewrite_text": "**Title:** Surface Plasmon Polaritons and Surface Phonon Polaritons on Metallic and Semiconducting Objects: Exact and Semiclassical Descriptions\n\n**Abstract:** This article presents comprehensive solutions for the electromagnetic field surrounding spherical objects characterized by arbitrary dielectric functions, encompassing both metallic and insulating materials. By employing an appropriate Green's function method to solve Maxwell's equations, we derive precise expressions that facilitate the estimation of dispersion relations for surface plasmons (SPs) and surface phonon polaritons (SPhPs). Our findings reveal that SPs emerge exclusively when the real part of the dielectric constant is zero, whereas SPhPs can exist even when this value is positive. We further compare our results with those derived from classical Drude theory, discussing the limitations of this traditional approach.\n\nSurface plasmons, which represent collective oscillations of conduction electrons at metal-dielectric interfaces, have been extensively studied over the years due to their significant implications in various domains such as optics, electronics, sensing, and catalysis. Recently, there has been an increasing interest in surface phonon polaritons, which are analogous excitations linked to longitudinal acoustic waves. These modes are not confined to surfaces; they can also propagate within the bulk of materials, potentially enhancing thermal transport and thermoelectric effects. Additionally, SPhPs can couple strongly with light, leading to intriguing phenomena such as superprism effects and exceptional transmission. This study aims to deepen the understanding of both SPs and SPhPs, highlighting their unique properties and applications in modern technology.",
        "ori-fast-z-score": -0.9712858623572641,
        "water-fast-z-score": 5.581052602166382,
        "rewrite-fast-z-score": 0.7844645405527362
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Angular momentum conservation and torsional oscillations in the Sun and solar-like stars .\nAbstract:\nWe present an analysis of helioseismic data for the angular velocity profile of the Sun, which is based on the assumption that the total angular momentum of the Sun is conserved during its evolution. We find that this hypothesis leads to a good agreement between theoretical predictions and observations at all latitudes except near the poles where we observe significant deviations. The discrepancies are explained by taking into account the effects of torsional oscillations in the convective envelope of the Sun. Our results show that these oscillations can be excited by internal magnetic fields generated by differential rotation in the radiative interior of the Sun. This mechanism may also operate in other solar-type stars with strong surface magnetic activity. In our study we use high-precision measurements of the frequency splitting of acoustic modes obtained using space-based instruments (GOLF and MDI) aboard the Solar and Heliospheric Observatory satellite. \n \n Keywords: Angular momentum, Torsional oscillations",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Angular momentum conservation and torsional oscillations in the Sun and solar - like stars . Abstract : We report an assessment of helioseismic data for the angular velocity profile of the Sun , which is based on the assumption that the total angular velocity of the Sun is conserved during its evolve .We see that this hypothesis leads to a better agreement between theoretical estimates and observations at all latitudes except near the poles where we perceive considerable deviations . The discrepancies are explained by take into consideration the effects of torsional oscillations in the convective envelope of the Sun .Our results show that these oscillations can be excited by inner magnetic waves generated by differential rotation in the radiative interior of the Sun . This mechanism may even exist in other solar - class stars with powerful surface magnetic activity .In our research we using high - precision observations of the frequency separation of acoustic modes obtained using space - based instruments ( GOLF and MDI ) aboard the Solar and Heliospheric Observatory satellite . Keywords : Angular momentum , Torsional oscillations",
        "rewrite_text": "Title: Angular Momentum Conservation and Torsional Oscillations in the Sun and Solar-like Stars\n\nAbstract: In this study, we evaluate helioseismic data to analyze the angular velocity profile of the Sun, grounded in the premise that the Sun's total angular momentum remains conserved throughout its evolutionary process. Our findings indicate that this conservation hypothesis significantly enhances the alignment between theoretical predictions and empirical observations across various latitudes, with the exception of regions near the poles, where notable discrepancies arise. We attribute these deviations to the influence of torsional oscillations occurring within the Sun's convective envelope. Our analysis reveals that these oscillations can be triggered by magnetic waves originating from differential rotation in the Sun's radiative interior. This excitation mechanism is likely applicable to other solar-type stars exhibiting strong surface magnetic activity. To conduct our research, we utilized high-precision measurements of acoustic mode frequency separations, which were acquired through space-based instruments such as GOLF and MDI aboard the Solar and Heliospheric Observatory (SOHO) satellite. Our results contribute to a deeper understanding of angular momentum dynamics and torsional oscillations in the Sun, providing insights that may extend to the behavior of similar stars in the universe. \n\nKeywords: Angular momentum, Torsional oscillations",
        "ori-fast-z-score": -0.11396057645963795,
        "water-fast-z-score": 4.755563543407302,
        "rewrite-fast-z-score": -0.21566554640687682
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Testing the Accuracy of Synthetic Stellar Libraries .\nAbstract:\nWe present an analysis of synthetic stellar libraries used to calibrate photometric surveys, such as Gaia and LSST. We show that these libraries are not accurate enough for this purpose because they do not include all relevant physical processes in their models (e.g., convection). This leads to systematic errors when using them to calibrate photometry or derive distances. We demonstrate how we can use observations of open clusters with known ages and metallicities to test the accuracy of different synthetic libraries by comparing observed and predicted cluster properties. Finally, we discuss possible improvements on current synthetic libraries. The next generation of space-based telescopes will provide unprecedented amounts of data about our Galaxy. These new datasets require large efforts to be analyzed properly. One important aspect is the calibration of photometric surveys like Gaia and LSST which will deliver precise astrometry and multi-color photometry for billions of stars across the sky. To achieve high precision results it is crucial to understand potential sources of error and biases introduced during the reduction process. In particular, one has to ensure that the derived absolute magnitudes M_(V) are correct within 0.01 mag over most of the color range covered by the survey. \n \n For example, if the distance modulus DM = 5log10(d/d_sun), where d is the true distance between us and the star and d_sun is the Sun’s distance from Earth, then a difference of 0.01 mag corresponds to a factor of 1.1 in distance. Thus, even small uncertainties in the absolute magnitude scale translate into significant errors in inferred distances. Therefore, it is essential to have reliable methods to determine the absolute magnitudes of individual stars accurately before deriving distances.  \n \n Currently there exist several approaches to estimate absolute magnitudes based on theoretical model atmospheres. However, these models often fail to reproduce observational constraints at low temperatures and/or high surface gravities. As a result, the resulting absolute magnitudes may deviate significantly from those obtained through other techniques, e.g., eclipsing binaries. Moreover, some of these models also suffer from incomplete",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Testing the Accuracy of Synthetic Stellar Libraries . Abstract : We report an assessment of synthetic stellar databases employed to calibrate photometric surveys , such as Gaia and LSST .We suggest that these books are not authoritative enough for this objective because they do not include all relevant physical processes in their models ( e . g . , convection ) . This leads to systematic errors when using them to calibrate photometry or calculate distances .We suggest how we can using observations of open clusters with established periods and metallicities to test the accuracy of different synthetic databases by comparing observed and anticipated cluster properties . Finally , we explain possible advances on current artificial libraries .The future generation of space - based telescopes will provide immense sums of evidence about our Galaxy . These new datasets require large efforts to be analyzed correctly .One important element is the calibration of photometric surveys like Gaia and LSST which will provide accurate astrometry and multi - color photometry for billions of stars across the sky . To achieve high precision outcomes it is crucial to realize potential sources of mistake and biases created during the reduction step .In particular , one has to ensure that the derived absolute magnitudes M _ ( V ) are correct within 0 . 01 mag over most of the color range covered by the survey . For example , if the distance modulus DM = 5log10 ( d / d _ sun ) , where d is the true distance between us and the star and d _ sun is the Sun ’ s distance from Earth , then a difference of 0 . 01 mag corresponds to a factor of 1 . 1 in distance .Thus , even narrow uncertainties in the absolute magnitude range result into considerable errors in inferred distances . Therefore , it is important to have reliable techniques to identify the absolute magnitudes of individual stars accurately before deriving distances .Currently there remain many approaches to estimate absolute magnitudes based on theoretical model atmospheres . However , these models often fail to capture observational parameters at low temperatures and / or low exterior gravities .As a result , the resulting absolute magnitudes might deviate greatly from those achieved through other techniques , e . g . , eclipsing binaries . Moreover , some of these models even suffer from incomplete",
        "rewrite_text": "**Title: Evaluating the Precision of Synthetic Stellar Libraries**\n\n**Abstract:** In this study, we conduct a thorough evaluation of synthetic stellar libraries utilized for the calibration of photometric surveys, including prominent projects like Gaia and LSST. Our findings indicate that these libraries may lack the necessary authority for accurate calibration due to their exclusion of critical physical processes, such as convection, in their modeling. This omission can lead to systematic inaccuracies when these libraries are employed for photometric calibration or distance calculations. To address this issue, we propose a methodology that leverages observations of open clusters with well-established periods and metallicities to assess the accuracy of various synthetic databases. By comparing the observed properties of these clusters with the predictions made by synthetic models, we can identify discrepancies and improve the reliability of these databases.\n\nFurthermore, we discuss potential advancements in the development of synthetic stellar libraries. The upcoming generation of space-based telescopes is expected to yield vast amounts of data regarding our Galaxy, necessitating significant analytical efforts to ensure accurate interpretation. A critical aspect of this analysis is the calibration of photometric surveys like Gaia and LSST, which aim to deliver precise astrometry and multi-color photometry for billions of stars. Achieving high precision in these measurements is contingent upon recognizing and mitigating potential sources of error and bias during the data reduction process. \n\nSpecifically, it is essential to ensure that the derived absolute magnitudes, M_(V), are accurate to within 0.01 magnitudes across the majority of the color spectrum covered by the survey. For instance, a difference of 0.01 magnitudes in the distance modulus (DM = 5log10(d/d_sun), where d represents the actual distance to the star and d_sun is the distance from the Sun to Earth) translates to a significant variation in distance, approximately a factor of 1.1. Consequently, even minor uncertainties in absolute magnitude can lead to substantial errors in distance estimations. Therefore, establishing reliable methods for accurately determining the absolute magnitudes of individual stars is crucial prior to deriving distances. \n\nCurrently, various approaches exist for estimating absolute magnitudes based on theoretical model atmospheres; however, these models frequently struggle to accurately represent observational parameters, particularly at lower temperatures and gravities. This limitation can result in significant deviations in absolute magnitudes compared to those obtained through alternative methods, such as eclipsing binaries. Additionally, some models are hindered by incomplete data, further complicating the calibration process.",
        "ori-fast-z-score": -1.6448469449747105,
        "water-fast-z-score": 6.383694290536715,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Spontaneous Symmetry Breaking as a Basis of Particle Mass .\nAbstract:\nThe Standard Model (SM) is the most successful theory in particle physics, but it has some problems such as hierarchy problem and naturalness problem. In this article we will discuss spontaneous symmetry breaking which can solve these problems. We will also introduce Higgs mechanism to explain how particles get their masses. Finally, we will talk about electroweak precision tests that are used to test SM predictions at high energy experiments. The Standard Model (SM) describes all known elementary particles and interactions between them very well up to now. However there are still some open questions like why do quarks have different masses? Why does the weak interaction become strong when energies increase? These questions lead us to consider new physics beyond the Standard Model. \n \n Spontaneous symmetry breaking occurs if a system with many symmetries becomes unstable against small perturbations. This means that even though the original Lagrangian describing the system looks symmetric under certain transformations, the ground state of the system breaks those symmetries spontaneously. For example, in quantum electrodynamics (QED), photons interact with electrons through the exchange of virtual electron-positron pairs. If you look closely into QED, you find out that the photon-electron vertex violates parity because the electric charge of an electron is negative while its spin is positive. But since the vacuum expectation value of the field operator for the electron vanishes, the ground state of QED preserves parity.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Spontaneous Symmetry Breaking as a Basis of Particle Mass . Abstract : The Standard Model ( SM ) is the most popular concept in particle theory , but it has some problems such as hierarchy problem and naturalness problem .In this page we will explore spontaneous symmetry breaking which can answer these problems . We will also add Higgs mechanism to explain how atoms get their masses .Finally , we will speak about electroweak accuracy tests that are using to test SM predictions at high energy experiments . The Standard Model ( SM ) contains all known elementary particles and interactions between them very well up to now .However there are still some open questions like why do quarks have different masses ? Why does the weak interaction become strong when energies rise ?These questions drive us to consider new theory beyond the Standard Model . Spontaneous symmetry breaking occurs if a system with many symmetries grows unstable against small perturbations .This implies that even though the original Lagrangian describing the system seems invariant under certain transformations , the ground state of the system breaks those symmetries spontaneously . For instance , in quantum electrodynamics ( QED ) , photons react with electrons through the transfer of virtual electron - positron couples .If you look closely into QED , you find out that the photon - electron vertex violates parity because the electric current of an electron is zero while its spin is positive . But since the vacuum expectation value of the field operator for the electron vanishes , the ground state of QED preserves parity .",
        "rewrite_text": "**Title: Spontaneous Symmetry Breaking as a Basis of Particle Mass**\n\n**Abstract:** The Standard Model (SM) of particle physics is widely regarded as the most comprehensive framework for understanding elementary particles and their interactions. However, it faces significant challenges, including the hierarchy and naturalness problems. This article delves into the concept of spontaneous symmetry breaking as a potential solution to these issues. We will discuss the Higgs mechanism, which provides a compelling explanation for how particles acquire mass, thereby addressing one of the fundamental questions in particle physics. Additionally, we will examine electroweak precision tests that serve to validate the predictions of the SM through high-energy experimental data.\n\nDespite the SM's success in describing known particles and their interactions, several unresolved questions remain, such as the disparity in quark masses and the behavior of weak interactions at elevated energy levels. These inquiries prompt the exploration of theories that extend beyond the SM. Spontaneous symmetry breaking occurs in systems characterized by multiple symmetries that become unstable under minor disturbances. This phenomenon indicates that while the governing Lagrangian of the system may exhibit invariance under specific transformations, the actual ground state can spontaneously break these symmetries.\n\nA pertinent example can be found in quantum electrodynamics (QED), where photons interact with electrons through the exchange of virtual electron-positron pairs. A closer examination of QED reveals that the photon-electron interaction vertex violates parity, as the electric current associated with an electron vanishes despite its positive spin. Nevertheless, due to the vacuum expectation value of the electron field operator being zero, the ground state of QED maintains parity symmetry. This exploration of spontaneous symmetry breaking not only enhances our understanding of particle mass generation but also opens avenues for addressing the limitations of the Standard Model, paving the way for future theoretical advancements in particle physics.",
        "ori-fast-z-score": 0.6704783996548059,
        "water-fast-z-score": 5.789863774090244,
        "rewrite-fast-z-score": -1.2815364865751413
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The design of a very high-Q superconductor electromechanical clock .\nAbstract:\nWe present the design and performance of an extremely low-noise, high-accuracy cryogenic mechanical oscillator based on a suspended silicon nitride membrane with integrated Josephson junctions. The device is designed to operate at temperatures below 1 K in a dilution refrigerator environment. We demonstrate that this system can be used as both a highly stable reference frequency source for microwave electronics or as a sensitive probe of quantum mechanics by measuring the vacuum fluctuations of its own motion. \n \n A key requirement for many applications of quantum information science is the ability to generate and detect single photons. In order to achieve these goals it will be necessary to develop new technologies capable of generating and detecting individual quanta of light. One promising approach involves coupling semiconductor nanocrystals (quantum dots) to optical cavities such as Fabry-Perot resonators1-5. These devices are expected to have important applications ranging from quantum optics6-8 to solid-state quantum computing9-11. However, one major challenge facing their development has been achieving sufficiently large Purcell factors12-14 so that spontaneous emission rates into the cavity mode become comparable to those observed in atomic systems15-17. This problem may be overcome using photonic crystal cavities18-20 which allow for strong confinement of electromagnetic fields within small volumes21-23.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The construction of a very high - Q superconductor electromechanical clock . Abstract : We present the development and performance of an incredibly small - noise , large - accuracy cryogenic mechanical oscillator based on a suspended silicon nitride cell with integrated Josephson junctions .The system is designed to run at pressures below 1 K in a dilution fridge climate . We indicate that this device can be used as both a highly stable reference wavelength source for microwave electronics or as a sensitive probe of quantum mechanics by monitoring the vacuum fluctuations of its own movement .A key requirement for numerous uses of quantum information research is the ability to produce and locate single photons . In order to achieve these objectives it will be required to develop new inventions capable of collecting and detecting individual quanta of light .One promising solution involves coupling semiconductor nanocrystals ( quantum dots ) to optical cavities such as Fabry - Perot resonators1 - 5 . These systems are expected to have important use ranging from quantum optics6 - 8 to liquid - state quantum computing9 - 11 .However , one major challenge facing their development has been achieving sufficiently huge Purcell factors12 - 14 so that spontaneous emission rates into the cavity mode become similar to those observed in atomic systems15 - 17 . This problem could be overcome using photonic crystal cavities18 - 20 which allow for strong confinement of electromagnetic fields within tiny volumes21 - 23 .",
        "rewrite_text": "We present the development and evaluation of an ultra-low noise, high-precision cryogenic mechanical oscillator, which is based on a suspended silicon nitride structure integrated with Josephson junctions. This innovative system is engineered to operate at temperatures below 1 K within a dilution refrigerator environment. The oscillator serves dual purposes: it functions as a highly stable reference wavelength source for microwave electronics and acts as a sensitive detector of quantum mechanics by observing the vacuum fluctuations of its own motion. \n\nA critical requirement for advancing quantum information research is the capability to generate and manipulate single photons. To meet this demand, there is a pressing need for novel technologies that can efficiently collect and detect individual light quanta. One promising approach involves the integration of semiconductor nanocrystals, known as quantum dots, with optical cavities such as Fabry-Perot resonators. These hybrid systems are anticipated to play a significant role in various applications, extending from quantum optics to liquid-state quantum computing.\n\nHowever, a significant hurdle in the development of these systems is the challenge of achieving sufficiently high Purcell factors, which are essential for ensuring that the spontaneous emission rates into the cavity mode are comparable to those observed in atomic systems. This challenge can potentially be addressed through the use of photonic crystal cavities, which facilitate the strong confinement of electromagnetic fields within minuscule volumes. By overcoming these obstacles, we can enhance the performance and applicability of quantum dot-cavity systems, paving the way for advancements in quantum technologies.",
        "ori-fast-z-score": 0.3779644730092272,
        "water-fast-z-score": 6.743417843388197,
        "rewrite-fast-z-score": 1.3525044520011484
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Massive Yang-Mills Theory based on the Nonlinearly Realized Gauge Group .\nAbstract:\nWe present an explicit construction for a massive gauge theory with spontaneous symmetry breaking, which is based on the nonlinearly realized gauge group. The model contains two scalar fields and one vector field in addition to the usual Higgs doublet. We show that this model can be obtained by gauging the global SU(2)LxSU(2)R chiral symmetry of the Nambu-Jona-Lasinio (NJL) model. In order to obtain the correct vacuum structure we introduce additional interactions between the scalars and fermions. These are motivated by the requirement that they should not change the physical spectrum but only modify the effective potential. This leads us to consider four different types of interaction terms. By performing a mean-field analysis at zero temperature we find that our model has three phases depending on the values of the parameters. For small coupling constants there exists a phase where both symmetries remain unbroken. If the couplings become larger then either the left-handed or right-handed symmetry breaks spontaneously while the other remains intact. Finally if all coupling constants are large enough then both symmetries break simultaneously.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Massive Yang - Mills Theory based on the Nonlinearly Realized Gauge Group . Abstract : We introduce an explicit construction for a huge gauge theory with spontaneous symmetry breaking , which is based on the nonlinearly realized gauge group .The model includes two scalar fields and one vector field in addition to the usual Higgs doublet . We see that this model can be obtained by gauging the global SU ( 2 ) LxSU ( 2 ) R chiral symmetry of the Nambu - Jona - Lasinio ( NJL ) model .In order to obtain the appropriate vacuum structure we apply additional interactions between the scalars and fermions . These are motivated by the requirement that they should not change the physical spectrum but only alter the effective potential .This leads us to consider four different kinds of relationship terms . By conducting a mean - field evaluation at zero temperature we find that our model has three stages depending on the values of the variables .For small coupling constants there exists a phase where both symmetries stay unbroken . If the couplings get bigger then either the left - handed or left - handed symmetry broke spontaneously while the other remains alive .Finally if all correlation constants are big enough then both symmetries broke simultaneously .",
        "rewrite_text": "We present a detailed construction of a substantial gauge theory characterized by spontaneous symmetry breaking, grounded in a nonlinearly realized gauge group framework. This theoretical model incorporates two scalar fields and one vector field, alongside the conventional Higgs doublet. Notably, the model emerges from gauging the global chiral symmetry SU(2)L × SU(2)R of the Nambu-Jona-Lasinio (NJL) model. To establish the desired vacuum structure, we introduce additional interactions between the scalar and fermionic fields. These interactions are carefully designed to preserve the physical spectrum while modifying the effective potential. Consequently, we explore four distinct types of relationship terms within the model. \n\nThrough a mean-field analysis conducted at zero temperature, we identify three distinct phases of the model, which depend on the coupling constants involved. In the regime of small coupling constants, we observe a phase where both symmetries remain unbroken. As the coupling constants increase, we find that one of the symmetries—either the left-handed or right-handed—undergoes spontaneous breaking, while the other symmetry persists. In the final phase, when all coupling constants reach sufficiently large values, both symmetries break down simultaneously. This comprehensive analysis not only elucidates the behavior of the model under varying coupling conditions but also contributes to the broader understanding of gauge theories with nonlinearly realized symmetries and their implications for particle physics.",
        "ori-fast-z-score": -0.10721125348377948,
        "water-fast-z-score": 5.193989612031165,
        "rewrite-fast-z-score": 0.9712858623572641
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Network statistics on early English Syntax: Structural criteria .\nAbstract:\nThe present study investigates the structural properties of Early Modern English syntax by applying network analysis to data obtained through corpus linguistics techniques. The results show that, in general terms, syntactic networks are characterized by high clustering coefficients and low average path lengths. In addition, it is shown how these two parameters can be used as indicators for identifying different types of syntactic structures. Finally, some possible applications of this approach are discussed. Keywords: Network Analysis; Corpus Linguistics; Syntactic Structures; Clustering Coefficients; Average Path Lengths. 1 Introduction A growing number of studies have been carried out recently using network theory (Watts & Strogatz 1998 , Newman 2003a ) to investigate various aspects of language structure (Ferrer-i-Cancho 2004 , Ferrer-i-Cancho et al 2005 . These investigations have mainly focused on phonological systems (e.g., Ferrer-i-Cancho 2002 , Ferrer-i-Cancho & Solé 2007 or lexical-semantic relations (e.g., Steyvers & Tenenbaum 2005 ) . However, there has also been interest in exploring other linguistic levels such as morphosyntax (Ferrer-i-Canchos 2006 ) , prosody (Ferrer-i-Canchós 2008) , pragmatics (Ferrer-i-Canchis 2009) or even discourse (Ferrer-i-Canchi 2010) . This article focuses on one particular aspect of syntax -namely, word order-using network analysis to explore its structural characteristics. More specifically, we will use network theory to analyze data collected with corpus-linguistic methods. We believe that this type of investigation could provide new insights into the way in which syntactic structures emerge during language acquisition.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Network statistics on early English Syntax : Structural criteria . Abstract : The present research investigates the structural properties of Early Modern English syntax by using network theory to data acquired through corpus linguistics tools .The results show that , in general terms , syntactic circuits are marked by high clustering parameters and low average route distances . In addition , it is demonstrated how these two parameters can be used as indicators for determining specific kinds of syntactic structures .Finally , some possible users of this methodology are discussed . Keywords : Network Analysis ; Corpus Linguistics ; Syntactic Structures ; Clustering Coefficients ; Average Path Lengths .1 Introduction A growing number of studies have been carried out recently utilizing network theory ( Watts & Strogatz 1998 , Newman 2003a ) to examine different components of language system ( Ferrer - i - Cancho 2004 , Ferrer - i - Cancho et al 2005 . These studies have chiefly centered on phonological systems ( e . g . , Ferrer - i - Cancho 2002 , Ferrer - i - Cancho & Solé 2007 or lexical - linguistic relations ( e . g . , Steyvers & Tenenbaum 2005 ) .However , there has additionally been discussion in investigating other linguistic levels such as morphosyntax ( Ferrer - i - Canchos 2006 ) , prosody ( Ferrer - i - Canchós 2008 ) , pragmatics ( Ferrer - i - Canchis 2009 ) or also discourse ( Ferrer - i - Canchi 2010 ) . This page focuses on one special aspect of syntax - namely , word order - using network analysis to examine its structural traits .More specifically , we will use system theory to analyze information collected with corpus - linguistic methods . We believe that this form of investigation could give novel discoveries into the way in which syntactic processes emerge during language acquire .",
        "rewrite_text": "**Title:** Network Statistics on Early English Syntax: Structural Criteria\n\n**Abstract:** This study explores the structural characteristics of Early Modern English syntax through the lens of network theory, utilizing data obtained from corpus linguistics methodologies. The findings indicate that syntactic networks generally exhibit high clustering coefficients alongside low average path lengths. These two metrics serve as valuable indicators for identifying distinct types of syntactic structures within the language. Furthermore, the research discusses potential applications of this analytical framework, highlighting its relevance for various linguistic inquiries. Recent advancements in the application of network theory to language systems (e.g., Watts & Strogatz, 1998; Newman, 2003a) have predominantly focused on phonological and lexical aspects (Ferrer-i-Cancho, 2004; Ferrer-i-Cancho et al., 2005). However, there is an emerging interest in extending this approach to other linguistic dimensions, including morphosyntax (Ferrer-i-Cancho, 2006), prosody (Ferrer-i-Cancho, 2008), pragmatics (Ferrer-i-Cancho, 2009), and discourse (Ferrer-i-Cancho, 2010). This paper specifically addresses the structural properties of word order within syntax, employing network analysis to uncover its underlying traits. By leveraging system theory in conjunction with corpus-linguistic data, we aim to provide fresh insights into the emergence of syntactic processes during language acquisition. The implications of this research extend to various fields, offering a novel perspective on the interplay between syntax and network dynamics. \n\n**Keywords:** Network Analysis; Corpus Linguistics; Syntactic Structures; Clustering Coefficients; Average Path Lengths.",
        "ori-fast-z-score": -1.1917080461366747,
        "water-fast-z-score": 6.6996452071463715,
        "rewrite-fast-z-score": -1.165543034828717
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Are Advanced Potentials Anomalous? .\nAbstract:\nWe present the results of an analysis of data on advanced potentials in hadronic collisions at high energies, obtained by the TOTEM experiment at LHC and by the UA7 collaboration at SppS collider. We show that these data are consistent with predictions based on Regge phenomenology for elastic scattering amplitudes. The observed behavior is also compatible with expectations from perturbative QCD calculations within the framework of the BFKL approach to high-energy evolution. \n \n Keywords: High energy physics, Elastic scattering amplitude, Perturbative QCD, BFKL equation, LHC, SppS, TOTEM, UA7 experiments \n \n 1 Introduction \n \n In recent years there has been considerable interest in studying the properties of elastic scattering amplitudes at very high energies (see e.g.,  1  ). This interest was triggered mainly by the discovery of new phenomena in this area made possible by the advent of accelerators operating at TeV scale such as the Large Hadron Collider (LHC)  2  . These discoveries include the observation of rapid growth of total cross sections  3  , dip-bump structure  4  , forward-backward asymmetry  5  , etc.. It should be noted however that many important questions remain open concerning the nature of the underlying dynamics responsible for all these effects  6  .\n \nIn particular, it remains unclear whether they can be explained within the conventional Regge theory  7, 8  or require more complicated approaches like those involving unitarization  9  and/or saturation  10  mechanisms. Another interesting question concerns the role played by higher-order corrections in perturbative Quantum Chromodynamics (QCD). Indeed, while the leading order BFKL  11  and DGLAP  12  equations provide reasonable description of experimental data  13  , their next-to-leading order extensions  14, 15  lead to significant deviations  16  which may indicate the need for resummation techniques  17  . \n \n 2 Data Analysis \n \n To shed some light on these issues we have performed detailed study of available data on elastic scattering processes collected recently by two dedicated experiments -the TOTEM  18  and UA7  19  collaborations. Both groups measured differential distributions dσ/d",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  Are Advanced Potentials Anomalous?.Abstract : We present the results of an assessment of evidence on advanced potentials in hadronic collisions at high energies , obtained by the TOTEM study at LHC and by the UA7 collaboration at SppS collider . We see that these information are compatible with predictions based on Regge phenomenology for elastic scattering amplitudes .The observed behavior is also consistent with predictions from perturbative QCD calculations within the framework of the BFKL approach to large - energy evolution . Keywords : High energy physics , Elastic scattering amplitude , Perturbative QCD , BFKL equation , LHC , SppS , TOTEM , UA7 experiments 1 Introduction In recent years there has been substantial interest in investigating the properties of elastic scattering amplitudes at very high energies ( saw e . g . , 1 ) .This activity was sparked mainly by the discovery of new concepts in this area made possible by the advent of accelerators active at TeV scale such as the Large Hadron Collider ( LHC ) 2 . These finds feature the observation of rapid increase of complete cross sections 3 , dip - bump formation 4 , backwards - backward asymmetry 5 , etc . .It should be mentioned however that several important questions remain open concerning the nature of the fundamental interactions involved for all these influences 6 . In particular , it remains unsure whether they can be described within the standard Regge principle 7 , 8 or use more complicated approaches like those concerning unitarization 9 and / or saturation 10 mechanisms .Another important dispute concerns the importance played by higher - order corrections in perturbative Quantum Chromodynamics ( QCD ) . Indeed , while the led order BFKL 11 and DGLAP 12 equations offer reasonable explanation of theoretical data 13 , their next - to - leading order additions 14 , 15 lead to significant deviations 16 which would indicate the necessity for resummation methods 17 .2 Data Analysis To shed some light on these problems we have done thorough study of available data on elastic scattering systems collected lately by two dedicated studies - the TOTEM 18 and UA7 19 collaborations . Both groups recorded differential functions dσ / d",
        "rewrite_text": "**Title: Are Advanced Potentials Anomalous?**\n\n**Abstract:** This article presents a comprehensive evaluation of the evidence surrounding advanced potentials in high-energy hadronic collisions, drawing on data from the TOTEM experiment at the Large Hadron Collider (LHC) and the UA7 collaboration at the Super Proton Synchrotron (SppS) collider. Our findings indicate that the observed data aligns well with predictions derived from Regge phenomenology concerning elastic scattering amplitudes. Furthermore, the behavior noted in these experiments is consistent with theoretical predictions from perturbative Quantum Chromodynamics (QCD), particularly within the BFKL framework, which addresses large-energy evolution. The investigation into elastic scattering amplitudes has gained momentum in recent years, largely due to the emergence of new concepts facilitated by the operation of TeV-scale accelerators like the LHC. Significant phenomena such as the rapid increase in total cross sections, the formation of dip-bumps, and backward-backward asymmetry have been observed, yet several critical questions regarding the fundamental interactions at play remain unresolved. It is still uncertain whether these phenomena can be adequately described by the conventional Regge principle or if more complex frameworks, such as unitarization and saturation mechanisms, are required. Additionally, the role of higher-order corrections in perturbative QCD is a topic of ongoing debate. While the leading-order BFKL and DGLAP equations provide a reasonable fit to theoretical data, their next-to-leading order corrections introduce notable discrepancies, suggesting a need for resummation techniques. To address these issues, we conducted an extensive analysis of the elastic scattering data recently gathered by the TOTEM and UA7 collaborations, focusing on differential cross-sections and their implications for our understanding of high-energy physics.\n\n**Keywords:** High energy physics, Elastic scattering amplitude, Perturbative QCD, BFKL equation, LHC, SppS, TOTEM, UA7 experiments.",
        "ori-fast-z-score": -0.40961596025952024,
        "water-fast-z-score": 8.981462390204987,
        "rewrite-fast-z-score": 0.3508232077228117
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Highly turbulent solutions of LANS-alpha and their LES potential .\nAbstract:\nWe present highly resolved numerical simulations of the incompressible Navier-Stokes equations with the LANS-alpha model, which is known to produce good results for wall-bounded flows at low Reynolds numbers. We show that this method can also be used in high-Reynolds number situations where it produces accurate results even though its underlying assumptions are not valid anymore. The main advantage over standard LES methods lies in the fact that no explicit subgrid-scale models have to be introduced. This makes the approach very attractive since there is no need to tune any parameters or coefficients as required by other LES approaches. In addition we demonstrate how the LANS-alpha method can be combined with an implicit LES scheme based on the variational multiscale formulation (VMS-LES) to obtain more efficient computations. Finally, we discuss some open issues related to the use of these schemes in practical applications. Turbulence plays a crucial role in many physical phenomena ranging from weather prediction to oceanic circulation and combustion processes. However, despite decades of research turbulence still remains one of the most challenging problems in computational fluid dynamics. One reason for this difficulty is due to the wide range of length scales involved in turbulent flows. While large eddies contain most of the kinetic energy they only occupy a small fraction of the total volume. On the other hand smaller eddies fill up almost all space but contribute little to the overall kinetic energy. Therefore, if one wants to resolve all relevant flow structures accurately enough then extremely fine grids would be needed leading to prohibitively expensive calculations. To overcome this problem so-called Large Eddy Simulations (LESs) were developed during the last two decades  1, 2  . These techniques aim at resolving only those large-scale motions responsible for the bulk of the kinetic energy while modeling the effect of unresolved small-scale fluctuations using suitable closure relations. Although LES has been successfully applied to various engineering problems  3–5  , it suffers from several drawbacks such as the lack of universality of the employed sub-grid scale models  6  .\nIn recent years new classes of LES-like methods have emerged  7–10  . They are based",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Highly chaotic solutions of LANS - alpha and their LES potential . Abstract : We create highly resolved numerical simulations of the incompressible Navier - Stokes equations with the LANS - alpha model , which is known to produce excellent performance for floor - defined flows at low Reynolds numbers .We see that this method can also be used in high - Reynolds number circumstances where it generates accurate conclusions even though its core assumptions are not valid anymore . The main advantage over traditional LES methods lies in the fact that no explicit subgrid - scale models have to be adopted .This gives the approach very appealing since there is no require to tune any values or coefficients as required by other LES approaches . In addition we prove how the LANS - alpha method can be merged with an implicit LES system using on the variational multiscale formulation ( VMS - LES ) to obtain more efficient computations .Finally , we explain some open problems related to the using of these schemes in effective applications . Turbulence plays a crucial role in many natural observations ranging from weather prediction to oceanic circulation and combustion systems .However , despite decades of research turbulence nonetheless appears one of the most challenging difficulties in computational liquid mechanics . One reason for this challenge is due to the broad variety of length scales implicated in turbulent flows .While big eddies collect most of the kinetic power they only comprise a small fraction of the total quantity . On the other hand tiny eddies fill up nearly all space but add little to the overall kinetic power .Therefore , if one wants to resolve all relevant stream structures accurately enough then extremely good grids might be needed leading to prohibitively expensive calculations . To solve this situation so - called Large Eddy Simulations ( LESs ) were developed during the last two decades 1 , 2 .These methods aim at resolving only those huge - scale motions responsible for the bulk of the kinetic power while solving the impact of unresolved small - scale fluctuations using appropriate completion relations . Although LES has been successfully application to numerous technical problems 3 – 5 , it suffers from several drawbacks such as the lack of universality of the involved sub - grid scale models 6 .In recent years new classes of LES - like methods have developed 7 – 10 . They are based",
        "rewrite_text": "Title: Highly Chaotic Solutions of LANS-α and Their LES Potential\n\nAbstract: In this study, we present highly resolved numerical simulations of the incompressible Navier-Stokes equations utilizing the LANS-α model, which has demonstrated exceptional performance for floor-defined flows at low Reynolds numbers. Our findings indicate that this model is also applicable in high-Reynolds number scenarios, yielding accurate results despite the breakdown of its fundamental assumptions. A significant advantage of the LANS-α approach over conventional Large Eddy Simulation (LES) methods is the absence of the need for explicit subgrid-scale models. This characteristic enhances its appeal, as it eliminates the necessity for tuning parameters or coefficients typically required in other LES frameworks. Furthermore, we demonstrate the potential for integrating the LANS-α method with an implicit LES system through the variational multiscale formulation (VMS-LES), leading to more efficient computational processes.\n\nTurbulence is a critical factor in various natural phenomena, including weather forecasting, oceanic circulation, and combustion systems. Despite extensive research over the past decades, turbulence remains one of the most formidable challenges in computational fluid dynamics. This complexity arises from the wide range of length scales involved in turbulent flows. While large eddies account for the majority of kinetic energy, they represent only a small fraction of the total flow structure. Conversely, smaller eddies occupy nearly all available space but contribute minimally to the overall kinetic energy. Consequently, accurately resolving all relevant flow structures necessitates highly refined grids, which can result in prohibitively expensive computational costs.\n\nTo address this issue, Large Eddy Simulations (LES) were developed over the last two decades, focusing on resolving the large-scale motions that dominate kinetic energy while modeling the effects of unresolved small-scale fluctuations through appropriate closure relations. Although LES has been successfully applied to a variety of engineering problems, it faces limitations, particularly regarding the universality of the subgrid-scale models employed. Recent advancements have led to the emergence of new classes of LES-like methods, which aim to overcome these challenges and enhance the accuracy and efficiency of turbulence simulations.",
        "ori-fast-z-score": -1.9639610121239315,
        "water-fast-z-score": 7.558065382861293,
        "rewrite-fast-z-score": -0.15523010514126656
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Watermark Embedding and Detection .\nAbstract:\nIn this work, we propose an efficient watermarking scheme for digital images based on the discrete wavelet transform (DWT). The proposed method embeds the watermark into the high-frequency sub-bands by modifying their coefficients using a quantization index modulation technique. In order to detect the embedded watermark in the attacked image, we use a correlation detector with a threshold value that is determined adaptively according to the local statistics of each block. Experimental results show that our algorithm has good performance against various attacks such as JPEG compression, salt & pepper noise addition, Gaussian filtering, cropping, rotation, scaling, blurring, sharpening, histogram equalization, gamma correction, median filter, and mean filter. Keywords: Watermarking; Discrete Wavelet Transform; Quantization Index Modulation; Adaptive Threshold Value. Digital watermarking technology can be used to protect copyrights or prevent illegal distribution of digital content. However, it may also cause some problems if the watermarks are not detected correctly due to intentional or unintentional attacks. Therefore, robustness should always be considered when designing any watermarking system.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Watermark Embedding and Detection . Abstract : In this study , we propose an efficient watermarking scheme for digital photographs relying on the discrete wavelet transition ( DWT ) .The proposed approach embeds the watermark into the high - frequency sub - bands by modifying their parameters using a quantization index modulation method . In order to identify the embedded watermark in the captured image , we utilize a correlation detector with a threshold value that is calculated adaptively based to the local statistics of each block .Experimental findings show that our technique has good success against several attacks such as JPEG compression , salt & pepper noise addition , Gaussian filtering , cropping , rotation , scaling , blurring , sharpening , histogram equalization , alpha reduction , median filter , and mean filter . Keywords : Watermarking ; Discrete Wavelet Transform ; Quantization Index Modulation ; Adaptive Threshold Value .Digital watermarking technology can be used to shield copyrights or avoid illegal collection of digital content . However , it could also create some problems if the watermarks are not observed correctly due to deliberate or unintentional attempts .Therefore , robustness should always be regarded when designing any watermarking scheme .",
        "rewrite_text": "Title: Watermark Embedding and Detection\n\nAbstract: This article presents a novel and efficient watermarking technique specifically designed for digital photographs, utilizing the discrete wavelet transform (DWT). Our method focuses on embedding watermarks within the high-frequency sub-bands of the image by adjusting their parameters through a quantization index modulation approach. To facilitate the detection of the embedded watermark in the processed image, we implement a correlation detector that employs an adaptive threshold value, which is calculated based on the local statistical characteristics of each image block. The results from our experiments demonstrate that this watermarking technique exhibits significant resilience against a variety of common image processing attacks, including JPEG compression, the addition of salt and pepper noise, Gaussian filtering, cropping, rotation, scaling, blurring, sharpening, histogram equalization, alpha reduction, median filtering, and mean filtering. \n\nThe importance of digital watermarking technology lies in its ability to protect copyrights and prevent unauthorized use of digital content. However, challenges arise when watermarks are not accurately detected due to either intentional or unintentional alterations. Consequently, ensuring robustness is a critical consideration in the design of any watermarking scheme. Our findings contribute to the ongoing development of watermarking methodologies, emphasizing the balance between effective watermark embedding and the preservation of image quality, while also addressing the need for reliable detection mechanisms. This research not only enhances the understanding of watermarking techniques but also provides a practical framework for future applications in digital rights management and content protection.\n\nKeywords: Watermarking; Discrete Wavelet Transform; Quantization Index Modulation; Adaptive Threshold Value.",
        "ori-fast-z-score": -1.3251783128981585,
        "water-fast-z-score": 5.673665146135802,
        "rewrite-fast-z-score": 1.5583874449479593
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  On over-reflection and generation of Gravito-Alfven waves in solar-type stars .\nAbstract:\nWe study the reflection of Alfven waves at the surface of a rotating star with an inclined magnetic field, which is relevant to the problem of coronal heating by MHD turbulence generated near the stellar surface. We show that if the angle between the wave vector k and the normal n to the reflecting surface satisfies cos(k·n) > 1 (over-reflection), then there exists a critical frequency ωcr = √ 2Ω above which no Alfven waves can propagate into the interior region of the star. This leads to the formation of standing waves on the surface of the star whose amplitude grows exponentially as time goes on. The growth rate depends only weakly on the inclination angle θB of the magnetic field lines but strongly on the density contrast across the surface layer. For typical parameters of the Sun we find that the exponential growth starts after about one day for θB = 60 degrees and after about ten days for θB = 90 degrees.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : On over - reflection and generation of Gravito - Alfven currents in solar - class stars . Abstract : We research the reflection of Alfven waves at the surface of a rotating star with an inclined magnetic force , which is relevant to the issue of coronal heating by MHD turbulence generated near the stellar surface .We see that if the angle between the wave vector k and the normal k to the reflecting surface satisfies cos ( k · n ) > 1 ( over - reflection ) , then there exists a critical frequency ωcr = √ 2Ω above which no Alfven currents can propagate into the interior region of the star . This leads to the formation of standing currents on the surface of the star whose intensity increases exponentially as time go on .The growth speed relies only faintly on the inclination angle θB of the magnetic field lines but heavily on the density contrast across the surface layer . For characteristic parameters of the Sun we find that the exponential growth starts after about one night for θB = 60 degrees and after about fifteen days for θB = 90 degrees .",
        "rewrite_text": "In this study, we investigate the phenomenon of Alfven wave reflection at the surface of a rotating star that possesses an inclined magnetic field, a topic of significant relevance to understanding coronal heating mechanisms driven by magnetohydrodynamic (MHD) turbulence occurring near the stellar surface. Our analysis reveals that when the angle between the wave vector \\( \\mathbf{k} \\) and the normal vector \\( \\mathbf{n} \\) to the reflecting surface satisfies the condition \\( \\cos(\\mathbf{k} \\cdot \\mathbf{n}) > 1 \\), a scenario termed \"over-reflection\" arises. Under these circumstances, we identify a critical frequency \\( \\omega_{cr} = \\sqrt{2\\Omega} \\) beyond which Alfven currents are unable to penetrate into the star's interior. This phenomenon results in the establishment of standing currents at the star's surface, whose intensity exhibits exponential growth over time. Notably, the rate of this growth is only weakly dependent on the inclination angle \\( \\theta_B \\) of the magnetic field lines, while it is significantly influenced by the density contrast present across the surface layer. For solar-like stars, our findings indicate that exponential growth of these currents initiates approximately one night after the onset of the conditions for \\( \\theta_B = 60 \\) degrees, and approximately fifteen days for \\( \\theta_B = 90 \\) degrees. This research contributes to the broader understanding of how Alfven wave dynamics can influence stellar behavior and may provide insights into the mechanisms underlying coronal heating in solar-class stars.",
        "ori-fast-z-score": -0.44172610429938614,
        "water-fast-z-score": 4.061277619618543,
        "rewrite-fast-z-score": 1.0536089137432665
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Multicriticality of the (2+1)-dimensional gonihedric model: A realization of the (d,m)=(3,2) Lifshitz point .\nAbstract:\nWe study the multicritical behavior in the two-dimensional gonihedric model with nearest-neighbor interactions on an anisotropic triangular lattice. We show that this system realizes the (d,m ) = ( 3 , 2 ) Lifshitz point and exhibits three different phases at zero temperature as functions of two parameters characterizing the anisotropy of the lattice structure. The phase diagram is obtained by means of Monte Carlo simulations combined with finite-size scaling analysis. In addition to the conventional ordered state and disordered state, we find another novel phase which has neither translational nor orientational order but shows algebraic decaying spin-spin correlations. This new phase can be regarded as a kind of spin-liquid-like state. Our results are also compared with those for other models such as the Ashkin-Teller model and the Blume-Capel model. \nI n t r o d u c t i o n :\nThe concept of Lifshitz points was originally introduced into condensed matter physics more than half a century ago  1  . It describes a critical point where several distinct phases meet each other simultaneously  2  . Recently, it attracted renewed interest because of its possible relevance to high-temperature superconductivity  3  .\nIn particular, the so-called (d, m) = (3, 2) Lifshitz point  4  , where d denotes spatial dimension and m represents number of components of order parameter fields, has been studied extensively both theoretically  5  -  8  and experimentally  9  -  11  . However, most studies have focused only on systems with short-range interactions  12  or purely magnetic systems  13  -  16  . On the other hand, there exist few theoretical investigations  17  -  20  concerning the effects of longer-ranged interactions  21  and/or competing orders  22  on the Lifshitz point.\nIn this Letter, we investigate the multicritical behavior of the two-dimensional gonihedrickson-Lee (GL) model  23  with nearestneighbor interactions on an anisotopic triangular lattice  see Fig.  1  . Although the GL model itself does not exhibit any ordering transition  24  , our previous work  25  showed that the introduction of anisotropy leads to",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Multicriticality of the ( 2 + 1 ) - dimensional gonihedric model : A realization of the ( d , m ) = ( 3 , 2 ) Lifshitz point . Abstract : We explore the multicritical behavior in the two - dimensional gonihedric model with nearest - neighbor interactions on an anisotropic triangular lattice .We see that this scheme assumes the ( d , m ) = ( 3 , 2 ) Lifshitz point and exhibits three different stages at zero temperature as functions of two parameters characterizing the anisotropy of the lattice structure . The phase diagram is achieved by means of Monte Carlo simulations combined with discrete - length scaling processing .In addition to the usual ordered state and disordered state , we find another novel phase which has neither translational nor orientational order but exhibits algebraic decaying spin - spinning correlations . This new phase can be regarded as a kind of spin - fluid - like state .Our results are also compared with those for other models such as the Ashkin - Teller model and the Blume - Capel theory . I n t r o d v c t i o n : The concept of Lifshitz points was originally adopted into condensed matter science more than half a millennium later 1 .It depicts a critical position where many unique stages encounter each other simultaneously 2 . Recently , it garnered renewed popularity because of its potential significance to large - temperature superconductivity 3 .In particular , the so - called ( d , m ) = ( 3 , 2 ) Lifshitz point 4 , where d indicates temporal dimension and m means number of components of order parameter fields , has been studied thoroughly both theoretically 5 - 8 and experimentally 9 - 11 . However , most studies have concentrated only on systems with short - range coupling 12 or purely magnetic systems 13 - 16 .On the other hand , there remain few theoretical investigations 17 - 20 concerning the effects of extended - ranged interactions 21 and / or competing orders 22 on the Lifshitz point . In this Letter , we investigate the multicritical behavior of the two - dimensional gonihedrickson - Lee ( GL ) model 23 with nearestneighbor interactions on an anisotopic triangular lattice seeing Fig .1 . Although the GL model itself does not show any ordering transition 24 , our previous research 25 showed that the introduction of anisotropy leads to",
        "rewrite_text": "**Title:** Multicriticality of the (2 + 1)-Dimensional Gonihedric Model: A Realization of the (d, m) = (3, 2) Lifshitz Point\n\n**Abstract:** In this study, we investigate the multicritical behavior of the two-dimensional gonihedric model characterized by nearest-neighbor interactions on an anisotropic triangular lattice. Our findings indicate that this model corresponds to the (d, m) = (3, 2) Lifshitz point, revealing three distinct phases at zero temperature, which depend on two parameters that define the lattice's anisotropy. We construct the phase diagram through a combination of Monte Carlo simulations and discrete-length scaling techniques. Notably, in addition to the conventional ordered and disordered states, we identify a novel phase that lacks both translational and orientational order, yet displays algebraic decay in spin-spin correlations. This phase can be interpreted as a spin-fluid-like state, contributing to the complexity of the system's behavior. Our results are contextualized by comparisons with other theoretical frameworks, including the Ashkin-Teller model and the Blume-Capel theory.\n\nThe concept of Lifshitz points, which emerged in condensed matter physics over five centuries ago, describes critical conditions where multiple unique phases converge. Recently, interest in Lifshitz points has surged due to their relevance to high-temperature superconductivity. Specifically, the (d, m) = (3, 2) Lifshitz point, where 'd' denotes the temporal dimension and 'm' represents the number of components in the order parameter fields, has been extensively explored both theoretically and experimentally. However, most existing research has focused on systems exhibiting short-range interactions or purely magnetic properties. There remains a gap in the theoretical understanding of how extended-range interactions and competing orders influence the Lifshitz point. In this letter, we delve into the multicritical behavior of the gonihedric model, which, despite not exhibiting any ordering transitions in its original form, demonstrates significant changes when anisotropy is introduced. Our work aims to shed light on these complex interactions and their implications for the broader understanding of multicritical phenomena in condensed matter systems.",
        "ori-fast-z-score": 0.24576957615571215,
        "water-fast-z-score": 6.144239403892804,
        "rewrite-fast-z-score": -0.2526455763199557
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Clusters AgeS Experiment (CASE). I. V209 omega Cen - An Eclipsing Post-Common Envelope Binary in the Globular Cluster omega Cen .\nAbstract:\nWe report on our discovery and analysis of an eclipsing binary system, designated as V209 omega Cen, located at the center of globular cluster Omega Centari. The primary star is a red giant with T eff = 5200 K and log g = 3.9 while its companion has a mass M 2 sin i = 0.33M ⊙ . We find that this system is likely to be a post common envelope binary consisting of two helium white dwarfs orbiting each other every 1.3 hours. This finding supports previous suggestions that such systems are responsible for producing Type Ia supernovae through their merger into one single degenerate object. In addition we have found evidence for a third component which may be another low-mass star or brown dwarf. Our results show that the age of Omega Cen must be greater than 12 Gyrs if it formed like most open clusters but less than 13 Gyrs if it was born like the oldest Galactic globulars.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  The Clusters AgeS Experiment (CASE).I. V209 omega Cen - An Eclipsing Post-Common Envelope Binary in the Globular Cluster omega Cen .Abstract : We report on our discovery and evaluation of an eclipsing binary system , designated as V209 omega Cen , located at the center of globular cluster Omega Centari . The main star is a red dwarf with T eff = 5200 K and log f = 3 . 9 while its companion has a mass M 2 sin i = 0 . 33M [UNK] .We see that this system is probably to be a post common envelope binary composed of two helium white dwarfs orbiting each other every 1 . 3 hours . This found supports earlier suggestions that such systems are responsible for producing Type Ia supernovae through their merger into one single degenerate object .In addition we have discovered evidence for a third element which may be another low - mass star or brown dwarf . Our results show that the age of Omega Cen must be greater than 12 Gyrs if it created like most open complexes but less than 13 Gyrs if it was founded like the youngest Galactic globulars .",
        "rewrite_text": "We present our findings on the eclipsing binary system V209 omega Cen, situated at the core of the globular cluster Omega Centauri. Our investigation reveals that the primary component of this binary is a red dwarf star with an effective temperature (T_eff) of 5200 K and a logarithmic surface gravity (log g) of 3.9. The secondary component, on the other hand, has a mass of M_2 sin i = 0.33 M_☉. Our analysis suggests that V209 omega Cen is likely a post-common envelope binary system, consisting of two helium white dwarfs that orbit each other with a period of approximately 1.3 hours. This discovery aligns with previous hypotheses proposing that such binary systems could be significant contributors to the formation of Type Ia supernovae through the eventual merger of their components into a single degenerate star.\n\nFurthermore, we have identified potential evidence for a third body in the system, which may either be another low-mass star or a brown dwarf. Our findings indicate that the age of Omega Centauri must exceed 12 billion years if it formed similarly to most open clusters, yet it is constrained to be less than 13 billion years if its formation parallels that of the youngest Galactic globular clusters. This research not only enhances our understanding of the stellar population within Omega Centauri but also contributes to the broader discourse on the evolutionary pathways of binary systems and their role in cosmic events such as supernovae. The implications of our work extend to the study of stellar evolution and the dynamics of globular clusters, providing valuable insights into the age and formation history of one of the Milky Way's most intriguing stellar systems.",
        "ori-fast-z-score": -1.212678125181665,
        "water-fast-z-score": 3.00964632714423,
        "rewrite-fast-z-score": -0.3713906763541037
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Hamiltonian Unification of General Relativity and Standard Model .\nAbstract:\nThe standard model is the most successful theory in particle physics, but it fails to explain gravity. In this article we present an alternative approach that unifies general relativity with quantum mechanics by using a new concept called  quantum potential energy density  (QPD). We show how QPD can be used as a source for gravitational field equations which are derived from Hamilton s principle of least action. The resulting field equations have solutions similar to those obtained from Einstein s field equations. However, unlike Einstein s field equations, our proposed field equations do not contain any free parameters such as cosmological constant or dark matter. This means that all physical phenomena predicted by these two theories should agree exactly if they are based on the same underlying principles. Finally, we discuss some possible experimental tests of our proposal. The standard model is the most succesful theory in particle physics, however it fails to explain gravity. \n \n In this article we present another approach that unifies general relavity with quantum mechanics by introducing a new concept called “quantum potental energy density”(QPD). \n \n Quantum potential energy density has been introduced previously by several authors  1-5 , but its role was limited only to explaining certain aspects of quantum mechanics like uncertainty relations  6 , tunneling effect  7-9 , wave-particle duality  10-12  etc.. \n \n Here we propose a novel interpretation of QPD where it plays a central role in deriving gravitational field equations. These field equations are then derived from Hamilton’s principle of least action. \n \n Our results suggest that QPD may play a fundamental role in understanding both gravity and quantum mechanics at their deepest level.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Hamiltonian Unification of General Relativity and Standard Model . Abstract : The standard theory is the most popular concept in particle theory , but it fails to explain gravity .In this article we present an additional method that unifies general relativity with quantum mechanics by using a new definition called quantum potential energy density ( QPD ) . We see how QPD can be used as a source for gravitational field equations which are derived from Hamilton s principle of least action .The resulting field equations have solutions identical to those achieved from Einstein s field equations . However , unlike Einstein s field equations , our proposed field equations do not include any free parameters such as cosmological factor or black material .This implies that all physical phenomena predicted by these two explanations should agree precisely if they are based on the same underlying principles . Finally , we talk some possible experimental tests of our proposal .The basic model is the most succesful model in particle science , however it fails to explain gravity . In this article we present another methodology that unifies general relavity with quantum mechanics by offering a new notion called “ particle potental energy density ” ( QPD ) .Quantum potential energy density has been proposed earlier by various literature 1 - 5 , but its significance was confined only to describing different areas of quantum mechanics like uncertainty relations 6 , tunneling effect 7 - 9 , wave - particle duality 10 - 12 etc . . Here we undertake a novel interpretation of QPD where it takes a central role in deriving gravitational field equations .These field equations are then constructed from Hamilton ’ s principle of least action . Our results propose that QPD may play a basic part in understanding both gravity and quantum mechanics at their deepest level .",
        "rewrite_text": "**Title: Hamiltonian Unification of General Relativity and the Standard Model**\n\n**Abstract:** The prevailing framework in particle physics, known as the Standard Model, has proven to be immensely successful in explaining a wide range of phenomena; however, it falls short in addressing the fundamental nature of gravity. In this article, we introduce a novel approach that seeks to unify general relativity with quantum mechanics through the introduction of a new concept termed quantum potential energy density (QPD). We demonstrate how QPD can serve as a source for gravitational field equations, which are derived from Hamilton's principle of least action. The field equations we propose yield solutions that are consistent with those derived from Einstein's field equations. A significant distinction of our approach is that it does not rely on any free parameters, such as the cosmological constant or dark matter, which are typically present in Einstein's formulations. This absence of free parameters suggests that the physical phenomena predicted by both our equations and Einstein's should align perfectly when grounded in the same foundational principles.\n\nMoreover, we explore the historical context of QPD, noting its previous applications in various aspects of quantum mechanics, including uncertainty relations, the tunneling effect, and wave-particle duality. Our work offers a fresh perspective on QPD, positioning it as a crucial element in the derivation of gravitational field equations. By doing so, we propose that QPD may play an essential role in bridging the gap between gravity and quantum mechanics, potentially leading to a deeper understanding of both domains. Finally, we discuss potential experimental tests that could validate our theoretical framework, paving the way for future research in the quest to unify these two pillars of modern physics.",
        "ori-fast-z-score": 1.9727878476642875,
        "water-fast-z-score": 7.89115139065715,
        "rewrite-fast-z-score": 0.780398972571708
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Coherent Lattice Vibrations in Superconductors .\nAbstract:\nThe phonon spectrum is calculated for the anisotropic triangular lattice with nearest-neighbor hopping and next-nearest neighbor repulsion using exact diagonalization method. The results are compared to those obtained by density functional theory (DFT) calculations, which show that DFT underestimates the energy gap between acoustic and optical branches as well as overestimating the bandwidths. We find that the lowest-lying branch has an almost linear dispersion relation at small wave vectors, while higher branches have quadratic dispersions. In addition we observe flat bands near the Fermi level arising due to strong electron-phonon coupling. These features can be observed experimentally through angle resolved photoemission spectroscopy measurements. \n \n Introduction: \n \n One of the most important properties of high-temperature superconducting materials is their ability to carry current without resistance below certain critical temperature T_c. This phenomenon arises because these materials undergoes a phase transition into a state where electrons pair up to form bosonic quasiparticles known as Cooper pairs. However, it was not until recently when the microscopic origin of this pairing mechanism became clear after the discovery of unconventional d-wave symmetry of the order parameter  1  . It turns out that the key ingredient responsible for such behavior is the presence of strongly correlated electronic states on the Fermi surface  2  , which leads to the formation of collective excitations called phonons  3  . Therefore, understanding how phonons behave in different types of lattices may provide valuable information about the nature of the underlying interactions among charge carriers  4  .\n \nIn recent years there has been growing interest in studying the effects of phonons on the physical properties of various classes of compounds  5  -  8  . For example, one of the simplest models used to describe the physics of cuprates is based on the two-dimensional square lattice  9  -  11  . On the other hand, another class of compounds known as iron-based pnictides  12  -  14  also exhibits similar characteristics but they are described by more complicated three-dimensional structures  15  -  17  . Moreover, some theoretical studies suggest that the role played by phonons in determining the ground-state properties of these systems cannot be ignored  18  -",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Coherent Lattice Vibrations in Superconductors . Abstract : The phonon spectrum is calculated for the anisotropic triangular lattice with nearest - neighbor hopping and last - nearest neighbor repulsion use accurate diagonalization technique .The results are compared to those achieved by density functional theory ( DFT ) estimates , which show that DFT underestimates the power gap between acoustic and optical branches as well as overestimating the bandwidths . We see that the smallest - lying branch has an relatively linear dispersion relation at small wave vectors , while greater branches have quadratic dispersions .In addition we study flat bands near the Fermi level arising owing to powerful atom - phonon coupling . These features can be identified experimentally through angle resolution photoemission spectroscopy observations .Introduction : One of the most important characteristics of high - temperature superconducting materials is their ability to carry current without tolerance below certain critical temperature T _ c . This phenomenon arises because these structures undergoes a phase shift into a state where electrons pair up to form bosonic quasiparticles known as Cooper pairs .However , it was not until recently when the microscopic origin of this pairing structure became clear after the discovery of unconventional d - wave symmetry of the order parameter 1 . It turns out that the key ingredient involved for such behavior is the presence of highly correlated electronic states on the Fermi surface 2 , which results to the formation of collective excitations called phonons 3 .Therefore , studying how phonons interact in different kinds of lattices might give valuable info about the nature of the fundamental interactions among charge carriers 4 . In recent years there has been growing interest in investigating the effects of phonons on the physical properties of several classes of compounds 5 - 8 .For instance , one of the simplest models used to explain the physics of cuprates is based on the two - dimensional square lattice 9 - 11 . On the other hand , another class of compounds known as iron - based pnictides 12 - 14 already exhibits similar characteristics but they are explained by more complicated three - dimensional forms 15 - 17 .Moreover , some theoretical researchers indicate that the importance played by phonons in determining the ground - state properties of these systems cannot be excluded 18 -",
        "rewrite_text": "**Title: Coherent Lattice Vibrations in Superconductors**\n\n**Abstract:** This study presents a detailed calculation of the phonon spectrum for an anisotropic triangular lattice, incorporating nearest-neighbor hopping and next-nearest-neighbor repulsion, utilizing an accurate diagonalization technique. The findings are juxtaposed with results derived from density functional theory (DFT), revealing that DFT tends to underestimate the energy gap between acoustic and optical phonon branches while overestimating their bandwidths. Notably, the lowest-lying phonon branch exhibits a relatively linear dispersion relation at small wave vectors, whereas higher branches demonstrate quadratic dispersion characteristics. Additionally, we investigate the emergence of flat bands near the Fermi level, which arise due to strong atom-phonon coupling. These distinctive features can potentially be observed experimentally through angle-resolved photoemission spectroscopy (ARPES).\n\n**Introduction:** High-temperature superconductors are characterized by their remarkable ability to conduct electricity without resistance below a critical temperature (T_c). This phenomenon is attributed to a phase transition in which electrons pair up to form bosonic quasiparticles known as Cooper pairs. Recent advancements have shed light on the microscopic origins of this pairing mechanism, particularly following the identification of an unconventional d-wave symmetry in the order parameter. Central to this behavior is the presence of highly correlated electronic states on the Fermi surface, which facilitate the formation of collective excitations termed phonons. Consequently, exploring the interactions of phonons within various lattice structures may yield critical insights into the fundamental interactions among charge carriers. In recent years, there has been an increasing interest in examining the influence of phonons on the physical properties of diverse material classes. For instance, the two-dimensional square lattice model has been widely employed to elucidate the physics of cuprate superconductors. Conversely, iron-based pnictides exhibit analogous characteristics but are described by more complex three-dimensional lattice structures. Furthermore, theoretical investigations suggest that the role of phonons in shaping the ground-state properties of these systems is significant and warrants further exploration.",
        "ori-fast-z-score": 0.3823595564509363,
        "water-fast-z-score": 7.990050631310736,
        "rewrite-fast-z-score": 2.846049894151541
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Forbidden territories in the string landscape .\nAbstract:\nWe study the distribution of low-lying states on random surfaces with boundaries, which are obtained by gluing together closed Riemann surfaces along their boundary components. We show that this problem is equivalent to counting the number of distinct ways one can tile a given surface using tiles of different shapes and sizes. This leads us to consider the statistical mechanics of an ensemble of interacting particles living on such surfaces. The partition function for these systems turns out to be related to the generating functions of certain classes of Feynman diagrams. In particular we find exact expressions for the free energy density as well as the entropy density at large volume (large genus) limit. These results provide new insights into the structure of the so-called  string landscape . Introduction:-In recent years there has been considerable interest in studying the properties of quantum gravity theories defined over spaces with non-trivial topology  1, 2  . One of the most interesting questions in this context concerns the existence of stable ground state configurations corresponding to classical geometries  3, 4  .\nThe simplest example of such a theory corresponds to the case where space-time is taken to have the topology of a torus T 2 = S 1 ×S 1 , or more generally any orientable two-dimensional manifold M g of genus g > 0  5  . It was shown recently  6  that in this case it is possible to define a consistent set of physical observables associated with the vacuum expectation values of local operators O(x), x ∈ M g . Moreover, it follows from general arguments  7, 8  that all correlation functions of these operators factorize into products of two-point correlators < O i (x)O j (y) >= C ij δ(x − y). Here C ij denotes some constant depending only on the positions of points x and y within the underlying geometry  9  . For instance, if we take M g to be a sphere then C ij will depend only on the geodesic distance between x and y  10  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Forbidden territories in the string landscape . Abstract : We research the spread of low - lying states on random surfaces with borders , which are derived by gluing together closed Riemann surfaces along their boundary parts .We see that this question is equal to counting the proportion of distinct ways one can mosaic a given surface using tiles of different shapes and shapes . This leads us to consider the statistical mechanics of an ensemble of interacting particles living on such surfaces .The partition function for these systems seems out to be connected to the generating functions of certain classes of Feynman diagrams . In particular we find exact expressions for the free energy density as well as the entropy density at large volume ( large genera ) limit .These data provide fresh insights into the formation of the so - called string landscape . Introduction : - In recent years there has been substantial interest in examining the properties of quantum gravitational theories constructed over spaces with non - trivial topology 1 , 2 .One of the most exciting questions in this situation concerns the existence of stable ground state structures corresponding to classical geometries 3 , 4 . The simplest example of such a theory refers to the case where space - time is taken to have the topology of a torus T 2 = S 1 ×S 1 , or more generally any orientable two - dimensional manifold M g of genus g > 0 5 .It was shown recently 6 that in this situation it is possible to define a consistent set of physical observables associated with the vacuum expectation values of local operators O ( x ) , x ∈ M g . Moreover , it appears from general statements 7 , 8 that all correlation functions of these operators factorize into products of two - point correlators < O i ( x ) O j ( y ) > = C ij δ ( x − y ) .Here C ij represents some constant depending only on the places of points x and y within the underlying geometry 9 . For instance , if we took M g to be a sphere then C ij will depend only on the geodesic length between x and y 10 .",
        "rewrite_text": "**Title:** Forbidden Territories in the String Landscape\n\n**Abstract:** This study investigates the distribution of low-lying states on random surfaces with boundaries, which are constructed by joining closed Riemann surfaces along their edges. We establish that this inquiry is analogous to determining the various distinct methods for tiling a specified surface using tiles of diverse shapes and configurations. This exploration leads us to delve into the statistical mechanics of a system comprising interacting particles residing on these surfaces. Notably, we find that the partition function for such systems is intricately linked to the generating functions of specific classes of Feynman diagrams. Our analysis yields precise formulations for both the free energy density and the entropy density in the limit of large volumes, corresponding to high genera. These findings offer novel perspectives on the emergence of the so-called string landscape.\n\n**Introduction:** In recent years, there has been a growing interest in the characteristics of quantum gravitational theories formulated over spaces with non-trivial topologies. A particularly intriguing question in this context pertains to the existence of stable ground state structures that correspond to classical geometries. The simplest instance of such a theory involves scenarios where spacetime is modeled with the topology of a torus (T² = S¹ × S¹) or, more generally, any orientable two-dimensional manifold (M_g) with genus g > 0. Recent research has demonstrated that it is feasible to define a coherent set of physical observables linked to the vacuum expectation values of local operators O(x), where x belongs to M_g. Furthermore, it appears that all correlation functions of these operators can be expressed as products of two-point correlators, <O_i(x) O_j(y)> = C_ij δ(x − y). Here, C_ij is a constant that depends solely on the positions of points x and y within the underlying geometry. For example, if M_g is a sphere, C_ij will be contingent on the geodesic distance between points x and y.",
        "ori-fast-z-score": -1.9581511249698935,
        "water-fast-z-score": 3.928371006591931,
        "rewrite-fast-z-score": -1.5670935878004129
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Phenomenological theory of spin excitations in La- and Y-based cuprates .\nAbstract:\nWe present the results of our study on the phenomenology of spin excitations in high-Tc superconductors based on lanthanum (La) or yttrium (Y). We show that the observed magnetic response can be described by two distinct contributions, one coming from the low-energy collective modes with an energy scale of about 10 meV and another one originating from high-energy incoherent fluctuations at energies up to 100-200 meV. The latter contribution is found to dominate over the former for temperatures above T* ~ 150 K. In addition we find that the temperature dependence of the intensity of these fluctuations follows closely the behavior expected for fermionic quasiparticles. Finally, we discuss how this picture compares with other experimental findings as well as theoretical predictions. \n \n High-temperature superconductivity has been discovered more than twenty years ago  1-3  but its microscopic origin remains unknown. One of the most important open questions concerns the nature of elementary excitations responsible for pairing  4  . A number of experiments have shown that the normal state properties are dominated by strong electronic correlations which lead to a variety of interesting phenomena such as pseudogap  5-7  , non-Fermi liquid  8  , charge density wave  9  , stripe formation  10  etc.. These effects are believed to play also an essential role in determining the physical properties of the superconducting phase  11  .\nIn recent years there has been considerable progress in understanding the physics of strongly correlated systems using angle resolved photoemission spectroscopy (ARPES)  12  . This technique allows direct access to the single-particle spectral function A(k,ω), where k denotes momentum and ω frequency. ARPES studies revealed several remarkable features of the electronic structure of hightemperature superconductors: Fermi surface reconstruction  13  , presence of flat bands  14  , van Hove singularities  15  , d-wave symmetry of the order parameter  16  , existence of nodal  17  and antinodal  18  regions, and so forth. However, despite all efforts made during last decade it still seems impossible to explain many aspects of the data within any existing theoretical framework  19  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Phenomenological study of spin excitations in La - and Y - based cuprates . Abstract : We present the conclusion of our research on the phenomenology of spin excitations in high - Tc superconductors based on lanthanum ( La ) or yttrium ( Y ) .We see that the seen magnetic response can be described by two different contributions , one coming from the high - energy collective modes with an energy scale of about 10 meV and another one originating from high - energy incoherent fluctuations at energies up to 100 - 200 meV . The latter contribution is found to dominate over the former for temperatures above T * ~ 150 K . In addition we find that the temperature dependence of the strength of these fluctuations follows carefully the dynamics anticipated for fermionic quasiparticles .Finally , we talk how this picture compares with other experimental discoveries as well as conceptual predictions . High - temperature superconductivity has been detected more than twenty years previously 1 - 3 but its microscopic source remains obscure .One of the most important open questions concerns the nature of primary excitations responsible for pairing 4 . A variety of studies have shown that the ordinary state properties are dominated by stable electronic correlations which lead to a variety of interesting phenomena such as pseudogap 5 - 7 , non - Fermi solid 8 , charge density frequency 9 , stripe formation 10 etc . .These effects are considered to hold also an essential part in establishing the physical properties of the superconducting phase 11 . In recent years there has been substantial advances in understanding the physics of highly correlated systems use angle resolution photoemission spectroscopy ( ARPES ) 12 .This method enables direct access to the single - particle spectral relation A ( h , ω ) , where k denotes momentum and ω frequency . ARPES studies showed several notable features of the electronic structure of hightemperature superconductors : Fermi surface reconstruction 13 , presence of flat bands 14 , van Hove singularities 15 , d - wave symmetry of the order parameter 16 , existence of nodal 17 and antinodal 18 regions , and so forth .However , despite all efforts made during final decade it still seems difficult to explain much aspects of the information within any established conceptual foundation 19 .",
        "rewrite_text": "**Title:** Phenomenological Study of Spin Excitations in La- and Y-Based Cuprates\n\n**Abstract:** This article presents the findings of our investigation into the phenomenology of spin excitations in high-temperature superconductors, specifically those based on lanthanum (La) and yttrium (Y). Our research reveals that the magnetic response observed in these materials can be attributed to two distinct contributions. The first is associated with high-energy collective modes, characterized by an energy scale of approximately 10 meV. The second contribution arises from high-energy incoherent fluctuations, which extend to energies between 100 and 200 meV. Notably, we find that the latter contribution predominates at temperatures exceeding T* ~ 150 K. Furthermore, we observe that the temperature dependence of the strength of these fluctuations aligns closely with the dynamics predicted for fermionic quasiparticles. \n\nIn addition to these findings, we discuss how our results correlate with other experimental observations and theoretical predictions in the field. Despite the discovery of high-temperature superconductivity over two decades ago, the underlying microscopic mechanisms remain elusive. A critical unresolved issue pertains to the nature of the primary excitations that facilitate pairing in these materials. Numerous studies have indicated that the properties of the normal state are significantly influenced by stable electronic correlations, which give rise to intriguing phenomena such as the pseudogap, non-Fermi liquid behavior, charge density waves, and stripe formation. These effects are believed to play a crucial role in shaping the physical characteristics of the superconducting phase.\n\nRecent advancements in the understanding of highly correlated systems have been made possible through angle-resolved photoemission spectroscopy (ARPES), which provides direct insights into the single-particle spectral function A(k, ω). ARPES studies have uncovered several remarkable features of the electronic structure in high-temperature superconductors, including Fermi surface reconstruction, the presence of flat bands, van Hove singularities, and the d-wave symmetry of the order parameter, along with the identification of nodal and antinodal regions. However, despite significant progress over the past decade, many aspects of these phenomena remain challenging to explain within any established theoretical framework.",
        "ori-fast-z-score": -0.07881104062391006,
        "water-fast-z-score": 7.652514332541697,
        "rewrite-fast-z-score": 0.48349377841522817
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Mesoscopic effective material parameters for thin layers modeled as single and double grids of interacting loaded wires .\nAbstract:\nWe present an approach to calculate the effective material properties of thin layered structures, which are composed by two or more different materials with periodic microstructure. The method is based on homogenization theory combined with finite element analysis (FEA) in order to account for local interactions between neighboring unit cells. We consider three types of unit cell geometries that can be used to model various composite materials such as: wire grid composites, fiber reinforced polymeric matrix composites, and metal foams. In particular we focus our attention on wire grid composites made up of periodically arranged parallel wires embedded into a homogeneous medium. For this type of structure it has been shown experimentally that the macroscopic elastic moduli depend strongly on the volume fraction occupied by the wires. However, these experimental results cannot be explained using classical homogenization theories because they do not take into account the interaction effects among adjacent wires. Therefore, we propose here a new theoretical framework to study the mechanical behavior of wire grid composites at both micro-and meso-scales.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Mesoscopic effective material parameters for thin layers modeled as single and double grids of interacting loaded wires . Abstract : We present an approach to estimate the effective material properties of dense layered systems , which are composed by two or more separate materials with periodic microstructure .The method is based on homogenization theory combined with finite element assessment ( FEA ) in order to account for local interactions between neighboring unit cells . We consider three categories of unit cell geometries that can be used to model numerous composite materials such as : wire grid composites , fiber strengthened polymeric matrix composites , and steel foams .In particular we focus our focus on wire grid composites made up of regularly arranged connected wires embedded into a homogeneous medium . For this form of form it has been shown experimentally that the macroscopic elastic moduli rely highly on the volume fraction occupied by the wires .However , these theoretical results cannot be described using traditional homogenization assumptions because they do not take into consideration the interaction influences among adjoining fibers . Therefore , we propose here a new theoretical framework to study the mechanical behavior of wire grid composites at both micro - and meso - scales .",
        "rewrite_text": "We introduce a novel methodology for estimating the effective material properties of dense layered systems composed of two or more distinct materials with periodic microstructures. This approach integrates homogenization theory with finite element analysis (FEA) to effectively capture the local interactions between adjacent unit cells. Our study categorizes three types of unit cell geometries that can represent a variety of composite materials, including wire grid composites, fiber-reinforced polymeric matrix composites, and steel foams. We specifically emphasize wire grid composites, which consist of regularly arranged interconnected wires embedded within a homogeneous medium. Experimental findings indicate that the macroscopic elastic moduli of these composites are significantly influenced by the volume fraction of the wires. However, traditional homogenization techniques fall short in accurately describing these theoretical results, as they overlook the interaction effects among neighboring fibers. To address this gap, we propose a new theoretical framework that facilitates the investigation of the mechanical behavior of wire grid composites at both micro and meso scales. This framework not only enhances our understanding of the material properties of such composites but also provides a robust tool for predicting their performance in practical applications. By incorporating the effects of local interactions, our approach offers a more comprehensive perspective on the mechanics of layered systems, paving the way for the design of advanced materials with tailored properties.",
        "ori-fast-z-score": -0.5827715174143585,
        "water-fast-z-score": 4.543661498514618,
        "rewrite-fast-z-score": 1.8034001919297482
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Gravity Dual and LHC Study of Single-Sector Supersymmetry Breaking .\nAbstract:\nWe study the phenomenology of single-sector supersymmetric models in which all superpartners are degenerate at tree level, but receive mass splittings through radiative corrections. We show that such models can be described by an effective field theory with only two parameters, one of which is fixed by requiring that the lightest neutralino have the correct relic density to explain dark matter. The other parameter controls the masses of the remaining sparticles; we find that these masses are typically within reach of current searches for supersymmetry at the Large Hadron Collider (LHC). In particular, we consider the case where the gravitino has a mass mG = O(10) GeV, as motivated by gauge mediation scenarios. For this range of gravitino masses, we demonstrate how our results may be used to interpret existing LHC data on gluinos and squarks. Finally, we discuss prospects for future LHC experiments.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Gravity Dual and LHC Study of Single - Sector Supersymmetry Breaking . Abstract : We research the phenomenology of single - sector supersymmetric theories in which all superpartners are degenerate at tree level , but accept mass splittings through radiative corrections .We see that such theories can be described by an efficient field model with only two parameters , one of which is fixed by requiring that the lightest neutralino have the appropriate relic volume to explain dark matter . The other parameter controls the masses of the remaining sparticles ; we find that these masses are typically within reach of recent searches for supersymmetry at the Large Hadron Collider ( LHC ) .In particular , we study the case where the gravitino has a mass mG = O ( 10 ) GeV , as motivated by gauge mediation strategies . For this range of gravitino masses , we prove how our findings may be used to predict older LHC evidence on gluinos and squarks .Finally , we discuss prospects for future LHC research .",
        "rewrite_text": "In this article, we investigate the phenomenological aspects of single-sector supersymmetric theories characterized by the degeneracy of all superpartners at the tree level, while allowing for mass differences to emerge through radiative corrections. Our analysis reveals that these theories can be effectively modeled using a simplified field framework that incorporates only two key parameters. One of these parameters is determined by the requirement that the lightest neutralino possesses the correct relic density to account for dark matter. The second parameter governs the mass spectrum of the remaining superparticles, which we find to generally fall within the detection capabilities of recent supersymmetry searches conducted at the Large Hadron Collider (LHC).\n\nFocusing specifically on scenarios where the gravitino mass is approximately on the order of 10 GeV, a value motivated by gauge mediation mechanisms, we demonstrate how our results can be leveraged to make predictions regarding previously observed LHC data concerning gluinos and squarks. This connection not only enhances our understanding of the underlying physics but also provides a framework for interpreting existing experimental results in the context of single-sector supersymmetry breaking.\n\nFurthermore, we explore the implications of our findings for future LHC experiments, highlighting potential avenues for further investigation into the nature of supersymmetry and its manifestations in high-energy collisions. Our work contributes to the ongoing discourse on supersymmetry, offering insights that may guide experimental efforts in the search for new physics beyond the Standard Model. Through this study, we aim to bridge theoretical predictions with experimental observations, thereby enriching the landscape of particle physics research.",
        "ori-fast-z-score": -0.9113223768657671,
        "water-fast-z-score": 3.9691432779197755,
        "rewrite-fast-z-score": -0.09325048082403138
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Can We Detect the Anisotropic Shapes of Quasar HII Regions During Reionization Through The Small-Scale Redshifted 21cm Power Spectrum? .\nAbstract:\nWe investigate whether we can detect anisotropy in quasar H II regions during reionization through their small-scale redshifted 21 cm power spectrum (21-cm PS). In our model, quasars are assumed to be located at peaks of dark matter density fluctuations and ionize surrounding gas with an anisotropic Strömgren sphere whose shape is determined by the local tidal field. By performing numerical simulations for different values of the spin temperature T S , we find that the 21-cm PS has a characteristic peak structure which reflects the shapes of individual H II regions. This peak structure becomes more prominent as T S decreases because the number of neutral hydrogen atoms increases due to the decrease in the brightness temperature difference between the CMB and the 21-cm emission line. Our results suggest that it may be possible to use this peak structure to constrain the value of T S . However, since there exist many other factors affecting the 21-cm PS besides T S , further studies will be needed before drawing any conclusions on its detectability.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Can We Detect the Anisotropic Shapes of Quasar HII Regions During Reionization Through The Small - Scale Redshifted 21cm Power Spectrum ? .Abstract : We explore whether we can locate anisotropy in quasar H II regions during reionization through their tiny - scale redshifted 21 cm power spectrum ( 21 - cm PS ) . In our model , quasars are expected to be found at peaks of dark matter density fluctuations and ionize neighboring gas with an anisotropic Strömgren sphere whose shape is chosen by the local tidal field .By conducting numerical simulations for different values of the spin temperature T S , we find that the 21 - cm PS has a peculiar peak structure which reflects the shapes of different H II regions . This peak structure becomes more prominent as T S drops because the proportion of neutral hydrogen atoms increases owing to the decrease in the brightness temperature difference between the CMB and the 21 - cm absorption line .Our results propose that it could be possible to use this peak structure to constrain the value of T S . However , since there remain many other influences involving the 21 - cm PS besides T S , further studies will be needed before drew any findings on its detectability .",
        "rewrite_text": "In this study, we investigate the potential for detecting anisotropic shapes of quasar H II regions during the epoch of reionization by analyzing the small-scale redshifted 21 cm power spectrum (21-cm PS). Our theoretical framework posits that quasars are predominantly located at the peaks of dark matter density fluctuations, where they ionize the surrounding gas, resulting in anisotropic Strömgren spheres shaped by the local tidal field. Through a series of numerical simulations, we examine the impact of varying spin temperature (T_S) on the 21-cm PS. Our findings reveal a distinctive peak structure in the power spectrum that correlates with the geometries of the H II regions. Notably, this peak structure becomes increasingly pronounced as T_S decreases, attributed to a rise in the fraction of neutral hydrogen atoms, which occurs due to a reduction in the brightness temperature difference between the cosmic microwave background (CMB) and the 21-cm absorption line. These results suggest that the observed peak structure in the 21-cm PS could serve as a valuable tool for constraining the spin temperature. However, we acknowledge that the 21-cm PS is influenced by various factors beyond T_S, indicating that additional research is necessary to fully understand the detectability of these anisotropic features. This work lays the groundwork for future investigations into the relationship between quasar activity, ionization processes, and the characteristics of the 21-cm PS during the critical period of cosmic reionization.",
        "ori-fast-z-score": 1.3093073414159544,
        "water-fast-z-score": 5.45544725589981,
        "rewrite-fast-z-score": 2.1572774865200244
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Entangled games are hard to approximate .\nAbstract:\nWe show that the problem of approximating Nash equilibria in two-player zero-sum entangled games is NP-hard, even if one restricts attention to pure strategies and exact equilibria.  We also prove an analogous result for mixed-strategy equilibria. These results hold under standard complexity-theoretic assumptions such as P = NP or RP = NEXP. The proof uses a reduction from the maximum cut problem on graphs with bounded treewidth. This shows that finding Nash equilibria exactly (even only among pure strategies) can be intractable when players have correlated information about each other s payoffs. Our results suggest that it may not always be possible to find good solutions by using local search algorithms like best-response dynamics. In this work we study the computational difficulty of computing Nash equilibria in two-person zero-sum games where players  utilities depend on their joint actions but they do not know these actions  outcomes before making decisions. Such games are called entangled because the outcome depends on both players  choices; see Figure 1 .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Entangled players are hard to approximate . Abstract : We see that the question of approximating Nash equilibria in two - player zero - sum entangled players is NP - hard , even if one restricts focus to pure strategies and exact equilibria .We additionally prove an analogous result for mixed - strategy equilibria . These results hold under standard complexity - theoretic assumptions such as P = NP or RP = NEXP .The proof uses a reduction from the maximum cut question on graphs with bounded treewidth . This shows that finding Nash equilibria precisely ( even only among true schemes ) can be intractable when participants have correlated information about each other s payoffs .Our results show that it could not always be possible to find good answers by using local search methods like best - response dynamics . In this research we study the computational difficulty of computing Nash equilibria in two - person zero - sum games where players utilities depend on their joint actions but they do not understand these actions outcomes before deciding choice .Such matches are called entangled because the result varies on both participants selection ; look Figure 1 .",
        "rewrite_text": "Title: Entangled Players Are Hard to Approximate\n\nAbstract: In this study, we investigate the computational complexity associated with approximating Nash equilibria in two-player zero-sum games involving entangled players. Our findings reveal that the problem of approximating these equilibria is NP-hard, even when we limit our analysis to pure strategies and seek exact equilibria. Furthermore, we extend our results to mixed-strategy equilibria, demonstrating that the same complexity challenges apply. These conclusions are drawn under widely accepted complexity-theoretic assumptions, including P = NP and RP = NEXP. \n\nThe proof of our main results employs a reduction from the maximum cut problem in graphs with bounded treewidth, illustrating that the task of precisely identifying Nash equilibria—particularly in scenarios where players possess correlated information regarding each other's payoffs—can be computationally intractable. This research highlights the limitations of local search techniques, such as best-response dynamics, in yielding satisfactory solutions in these contexts. \n\nWe focus on the unique characteristics of entangled matches, where the outcomes are contingent upon the joint actions of the players, and the players lack prior knowledge of the consequences of their choices. This interdependence complicates the decision-making process, as illustrated in Figure 1. Our work contributes to the understanding of the inherent difficulties in computing Nash equilibria in entangled scenarios, emphasizing the need for further exploration of alternative approaches to tackle these complex problems in game theory.",
        "ori-fast-z-score": 0.52999894000318,
        "water-fast-z-score": 5.346252667281783,
        "rewrite-fast-z-score": -0.6767155423319645
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Detection of two-sided alternatives in a Brownian motion model .\nAbstract:\nWe consider the problem of detecting whether an unknown parameter belongs to a given closed interval or not, when observations are generated by a one-dimensional diffusion process with drift and volatility functions depending on this parameter. We propose a test statistic based on the supremum norm between the empirical distribution function (EDF) of the sample path and its theoretical counterpart under the null hypothesis. The asymptotic behavior is studied for both fixed and diverging time horizons. In particular we show that our test statistic converges weakly towards a functional of a standard Brownian bridge as the horizon increases. This result allows us to derive the exact level-alpha critical value of the proposed test at any finite horizon using Monte Carlo simulations. Finally, numerical experiments illustrate the performance of the proposed procedure compared to existing tests. Keywords: Detection problems; Diffusion processes; Empirical distribution function; Two-sided alternative hypotheses; Critical values; Asymptotic normality; Finite-time detection",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Detection of two - sided alternatives in a Brownian movement model . Abstract : We consider the issue of detecting whether an unknown parameter belongs to a given closed interval or not , when observations are produced by a one - dimensional diffusion system with drift and volatility functions depending on this parameter .We suggest a test statistic using on the supremum norm between the empirical distribution function ( EDF ) of the sample path and its theoretical counterpart under the null theory . The asymptotic behavior is studied for both fixed and diverging time horizons .In particular we prove that our test statistic converges weakly towards a functional of a traditional Brownian cross as the horizon rises . This result allows us to derive the exact level - alpha critical value of the suggested test at any finite horizon utilizing Monte Carlo simulations .Finally , numerical studies exhibit the performance of the suggested method compared to existing experiments . Keywords : Detection problems ; Diffusion processes ; Empirical distribution relation ; Two - sided alternative hypotheses ; Critical values ; Asymptotic normality ; Finite - time detection",
        "rewrite_text": "Title: Detection of Two-Sided Alternatives in a Brownian Movement Model\n\nAbstract: This article addresses the challenge of determining whether an unknown parameter lies within a specified closed interval when observations are generated by a one-dimensional diffusion process characterized by drift and volatility functions that are dependent on this parameter. We propose a novel test statistic that utilizes the supremum norm to compare the empirical distribution function (EDF) derived from the sample path with its theoretical counterpart under the null hypothesis. The study explores the asymptotic properties of this test statistic for both fixed and diverging time horizons. Notably, we demonstrate that as the time horizon increases, our test statistic converges weakly to a functional associated with a classical Brownian bridge. This convergence result is pivotal as it enables us to compute the exact level-alpha critical value for the proposed test at any finite time horizon through Monte Carlo simulations. Furthermore, we present numerical experiments that illustrate the efficacy of our method in comparison to existing approaches in the literature. The findings indicate that our proposed detection method offers significant advantages in terms of performance, particularly in scenarios involving two-sided alternative hypotheses. This research contributes to the broader field of detection problems in diffusion processes, providing valuable insights into empirical distribution relations and the determination of critical values, while also addressing the challenges of asymptotic normality and finite-time detection. \n\nKeywords: Detection problems; Diffusion processes; Empirical distribution relations; Two-sided alternative hypotheses; Critical values; Asymptotic normality; Finite-time detection.",
        "ori-fast-z-score": -0.10846522890932808,
        "water-fast-z-score": 4.824506406770077,
        "rewrite-fast-z-score": 0.3592106040535498
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Cosmic string formation by flux trapping .\nAbstract:\nWe study the evolution of cosmic strings in an expanding universe, focusing on their formation mechanism and subsequent growth. We show that cosmic strings can form when magnetic fields are trapped inside overdense regions during inflation. The resulting network consists of many small loops which evolve into larger ones through gravitational radiation emission. This process is similar to the one proposed for electroweak strings formed at phase transitions after inflation. However, we find that the loop distribution function has a different shape than previously assumed. In particular, it contains more large loops with sizes comparable to the Hubble radius today. These loops may be detectable as stochastic backgrounds of gravitational waves or gamma rays. Cosmic strings have been predicted to exist since the early 1980s  1, 2  . They could arise naturally if there were extra dimensions beyond those observed so far  3  , or they might be produced at symmetry breaking phase transitions  4  .\nCosmic strings would produce observable effects such as gravitational lensing  5  , CMB anisotropies  6  , and primordial black holes  7, 8  . Despite this interest, no direct detection of cosmic strings has yet been made  9  . One reason why cosmic strings remain elusive is because they are expected to be very light (with masses less than $10^{-16}eV$)  10  . Another problem is that cosmic strings are not stable objects but rather decay rapidly via gravitational radiation  11  . Therefore, any observational evidence must come indirectly from the products of cosmic string decays  12  .\nIn order to make predictions about possible observations, cosmological simulations need to be performed  13  . A number of groups have studied cosmic string networks using N-body codes  14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Cosmic string formation by flux trapping . Abstract : We research the evolution of universe strings in an increasing universe , concentrating on their structure process and subsequent expansion .We see that cosmic strings can form when magnetic fields are locked inside overdense regions during inflation . The resulting system consists of several small loops which evolve into larger ones through gravity radiation emission .This process is related to the one proposed for electroweak strings generated at phase transitions after inflation . However , we find that the curve distribution map has a different shape than previously predicted .In particular , it contains more wide loops with sizes comparable to the Hubble diameter today . These rings may be detectable as stochastic backgrounds of gravitational waves or gamma radiation .Cosmic strings have been predicted to arise since the early 1980s 1 , 2 . They could occur readily if there were extra dimensions beyond those observed so far 3 , or they may be formed at symmetry breaking phase transitions 4 .Cosmic strings would create observable effects such as gravity lensing 5 , CMB anisotropies 6 , and primordial black holes 7 , 8 . Despite this interest , no close observation of universe strings has yet been achieved 9 .One reason why cosmic strings remain elusive is because they are expected to be very light ( with masses fewer than $ 10 ^ { - 16 } eV $ ) 10 . Another question is that cosmic strings are not stable objects but rather decay rapidly via gravitational rays 11 .Therefore , any observational evidence needs go indirectly from the products of cosmic string decays 12 . In order to make predictions about likely observations , cosmological simulations need to be performed 13 .A several of groups have researched cosmic string systems use N - body symbols 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 , 31 , 32 , 33 , 34 , 35 , 36 , 37 , 38 , 39 , 40 , 41 , 42 , 43 , 44 , 45 , 46 , 47 , 48 , 49 , 50 , 51 , 52 , 53 , 54 , 55 , 56 , 57 , 58 , 59 , 60 , 61 , 62 , 63 , 64",
        "rewrite_text": "**Title: Cosmic String Formation through Flux Trapping**\n\n**Abstract:** This study investigates the evolution of cosmic strings in an expanding universe, focusing on their structural formation and subsequent growth. We propose that cosmic strings can emerge when magnetic fields become trapped within overdense regions during the inflationary phase of the universe. The initial configuration consists of numerous small loops that gradually evolve into larger structures through the emission of gravitational radiation. This mechanism bears resemblance to the formation of electroweak strings during phase transitions following inflation. However, our findings reveal a distinct difference in the distribution of loop sizes compared to previous predictions, particularly indicating a prevalence of wider loops that approach sizes comparable to the current Hubble diameter. These larger loops may be observable as stochastic backgrounds of gravitational waves or gamma radiation.\n\nCosmic strings have been theorized since the early 1980s and could potentially arise from the existence of extra dimensions or during symmetry-breaking phase transitions. Their presence would lead to observable phenomena such as gravitational lensing, cosmic microwave background (CMB) anisotropies, and the formation of primordial black holes. Despite significant theoretical interest, direct observations of cosmic strings remain elusive. One contributing factor is their expected low mass, typically less than \\(10^{-16} \\, \\text{eV}\\), which complicates detection efforts. Additionally, cosmic strings are not stable entities; they tend to decay rapidly through the emission of gravitational radiation, necessitating indirect observational strategies that focus on the decay products.\n\nTo enhance our understanding and make predictions regarding potential observations, cosmological simulations are essential. Several research groups have employed N-body simulations to explore the dynamics of cosmic string systems, contributing to a deeper comprehension of their properties and implications for cosmology. This work aims to bridge theoretical predictions with observational strategies, thereby advancing the search for evidence of cosmic strings in the universe.",
        "ori-fast-z-score": 0.4931969619160719,
        "water-fast-z-score": 7.8383671769061705,
        "rewrite-fast-z-score": 2.5847594849602595
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A mathematical model for a copolymer in an emulsion .\nAbstract:\nThe present work is concerned with the development and analysis of a mathematical model describing the dynamics of a copolymerization reaction occurring inside droplets dispersed into water, which are stabilized by surfactants.  The system consists of three partial differential equations (PDEs) coupled through nonlinear boundary conditions at the interface between two phases. In addition to the usual convection-diffusion-reaction terms appearing in such models, we also include diffusion due to Marangoni stresses induced by surface tension gradients across the interface. We prove existence of global weak solutions using Galerkin approximations combined with compactness arguments. Finally, we perform numerical simulations that illustrate our theoretical results. Keywords: Copolymerization; Emulsions; Surface tension gradients; Mathematical modelling. 1 Introduction Polymeric materials have been widely used as coatings on solid surfaces or as additives in many industrial processes including paints, cosmetics, pharmaceuticals, food processing etc., see e.g.  21, 22  . A common way to produce these materials involves polymerizing monomers within small droplets suspended in water, called emulsions. This process can be achieved either chemically or physically depending upon whether the droplets contain chemical species necessary for initiating the polymerization reactions or not. For example, if the droplets do not contain any chemicals then they must first be prepared separately before being added to the main mixture containing all other ingredients. Once this has been done, the droplets will begin to grow until their size becomes comparable to the wavelength of light passing through them. At this point, the droplets become optically opaque and the growth continues unabated leading eventually to the formation of large particles known as micelles. These particles may then be separated out from the rest of the solution by centrifugation or filtration techniques. Alternatively, one could use a different type of emulsifier so that the droplets remain transparent throughout the entire process. However, it should be noted that this approach requires additional control over the amount of energy supplied during the process since too much heat generated by the exothermic nature of",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A mathematical description for a copolymer in an emulsion . Abstract : The present work is concerned with the development and assessment of a mathematical description explaining the dynamics of a copolymerization process happening inside droplets dispersed into water , which are stabilized by surfactants .The system consists of three partial differential equations ( PDEs ) coupled through nonlinear boundary conditions at the interface between two phases . In addition to the usual convection - diffusion - reaction conditions present in such models , we also cover convection due to Marangoni stresses imposed by surface friction gradients across the interface .We establish existence of global weak solutions use Galerkin approximations combined with compactness arguments . Finally , we perform numerical simulations that highlight our theoretical results .Keywords : Copolymerization ; Emulsions ; Surface tension gradients ; Mathematical modelling . 1 Introduction Polymeric materials have been widely useful as coatings on solid surfaces or as additives in different industrial systems including paints , cosmetics , pharmaceuticals , dairy packaging etc . , see e . g .21 , 22 . A typical way to produce these structures involves polymerizing monomers within tiny droplets dropped in water , called emulsions .This process can be obtained either chemically or physically depending upon whether the droplets comprise chemical species needed for initiating the polymerization reactions or not . For instance , if the droplets do not include any chemicals then they must first be made individually before being added to the main mix using all other products .Once this has been performed , the droplets will start to expand until their height appears similar to the frequency of light traveling through them . At this time , the droplets become optically opaque and the development begins unabated leading ultimately to the formation of large particles named as micelles .These particles may then be isolated out from the remainder of the solution by centrifugation or filtration methods . Alternatively , one might using a unique kind of emulsifier so that the droplets stay clear throughout the entire process .However , it should be mentioned that this methodology demands extra control over the quantity of power provided during the process since too much heat generated by the exothermic nature of",
        "rewrite_text": "**Title:** A Mathematical Framework for Copolymer Dynamics in Emulsions\n\n**Abstract:** This study focuses on the formulation and evaluation of a mathematical model that elucidates the dynamics of copolymerization occurring within droplets dispersed in an aqueous medium, stabilized by surfactants. The model is represented by a system of three coupled partial differential equations (PDEs), which incorporate nonlinear boundary conditions at the interface between the two distinct phases. In addition to the conventional convection-diffusion-reaction mechanisms typically observed in such systems, our model also accounts for convection driven by Marangoni stresses, which arise from surface tension gradients across the interface. We demonstrate the existence of global weak solutions to the proposed system by employing Galerkin approximations alongside compactness arguments. Furthermore, we conduct numerical simulations that validate our theoretical findings, illustrating the intricate dynamics of the copolymerization process. \n\n**Keywords:** Copolymerization; Emulsions; Surface tension gradients; Mathematical modeling.\n\n**1 Introduction:** Polymeric materials are extensively utilized as coatings on solid surfaces and as additives across various industrial applications, including paints, cosmetics, pharmaceuticals, and dairy packaging. A common method for producing these materials involves the polymerization of monomers within minuscule droplets suspended in water, known as emulsions. This polymerization can occur either chemically or physically, depending on whether the droplets contain the necessary chemical species to initiate the reactions. In cases where the droplets lack these chemicals, they must be individually synthesized before being incorporated into the main mixture with other components. As the droplets expand, they reach a size comparable to the wavelength of light, at which point they become optically opaque, initiating a continuous development process that ultimately leads to the formation of larger aggregates known as micelles. These micelles can subsequently be separated from the solution through centrifugation or filtration techniques. Alternatively, a specialized emulsifier can be employed to maintain the clarity of the droplets throughout the process. However, it is important to note that this approach necessitates careful control over the energy input during the process, as excessive heat generated by the exothermic reactions can adversely affect the outcome.",
        "ori-fast-z-score": -2.227560395692044,
        "water-fast-z-score": 8.168431332896885,
        "rewrite-fast-z-score": 0.16783627165933782
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  High-energy threshold reaction rates on 0.8 GeV proton-irradiated thick Pb-target .\nAbstract:\nThe high-energy threshold reaction rates for the reactions induced by protons with energies up to 800 MeV are calculated using the statistical model code TALYS-1.6 and compared with experimental data obtained at JINR-Dubna in the framework of the project  Study of nuclear processes under extreme conditions . The results show that the theoretical predictions agree well with experiment within uncertainties, except for some cases where there is an overestimation or underestimation of the measured cross sections. \n \n Keywords: Reaction rate, Thick target, Proton beam, Statistical model, Nuclear physics, Cross section measurement, High energy threshold reaction rate calculation, Extreme condition study, Projectile fragmentation, Heavy ion collision, Energy deposition, Radiation damage, Lead target, Inverse kinematics, Fragmentation function, Neutron emission probability distribution, Excitation functions, Stopping power, Energy loss straggling, Energy transfer",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : High - energy threshold reaction rates on 0 . 8 GeV proton - irradiated thick Pb - target . Abstract : The high - energy threshold reaction rates for the reactions generated by protons with energies up to 800 MeV are measured using the statistical model code TALYS - 1 . 6 and compared with research data acquired at JINR - Dubna in the framework of the project Study of nuclear systems under extreme circumstances .The results show that the theoretical estimates agree well with experiment within uncertainties , except for some cases where there is an overestimation or underestimation of the measured cross sections . Keywords : Reaction rate , Thick target , Proton beam , Statistical model , Nuclear physics , Cross section measurement , High energy threshold reaction level calculation , Extreme condition measurement , Projectile fragmentation , Heavy ion crash , Energy deposition , Radiation injury , Lead target , Inverse kinematics , Fragmentation function , Neutron radiation probability distribution , Excitation functions , Stopping energy , Energy gain straggling , Energy transfer",
        "rewrite_text": "**Title:** High-Energy Threshold Reaction Rates in a 0.8 GeV Proton-Irradiated Thick Lead Target\n\n**Abstract:** This study investigates the high-energy threshold reaction rates resulting from protons with energies reaching up to 800 MeV, utilizing the statistical model code TALYS-1.6. The research focuses on reactions occurring in a thick lead target, with findings compared against experimental data obtained from the Joint Institute for Nuclear Research (JINR) in Dubna, as part of the project aimed at exploring nuclear systems under extreme conditions. The analysis reveals a strong correlation between theoretical predictions and experimental results, with most estimates falling within the expected uncertainties. However, certain instances demonstrate discrepancies, where the measured cross sections are either overestimated or underestimated. This research contributes to a deeper understanding of reaction rates in high-energy nuclear physics, particularly in the context of projectile fragmentation and energy deposition phenomena. The implications of these findings extend to various applications, including radiation injury assessments and the development of inverse kinematics techniques. The study also addresses critical aspects such as neutron radiation probability distributions, excitation functions, and energy transfer mechanisms, which are essential for advancing knowledge in the field of nuclear reactions under extreme conditions. Overall, this work enhances the comprehension of high-energy interactions in thick targets and provides valuable insights for future experimental and theoretical investigations in nuclear physics.\n\n**Keywords:** Reaction rate, Thick target, Proton beam, Statistical model, Nuclear physics, Cross section measurement, High-energy threshold reaction level calculation, Extreme condition measurement, Projectile fragmentation, Heavy ion collisions, Energy deposition, Radiation injury, Lead target, Inverse kinematics, Fragmentation function, Neutron radiation probability distribution, Excitation functions, Stopping energy, Energy gain straggling, Energy transfer.",
        "ori-fast-z-score": 1.162476387438193,
        "water-fast-z-score": 4.965212315030781,
        "rewrite-fast-z-score": 2.685380346549405
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Optimizing Scrip Systems: Efficiency, Crashes, Hoarders, and Altruists .\nAbstract:\nWe study the efficiency of scrip systems in which users can buy goods with their own currency or by exchanging it for other people s currency (scrip). We show that when there are many buyers and sellers who use such a system to trade items, then the system is efficient if most people have an incentive to be fair traders. However, we also find that this may not always happen because some users may try to hoard money while others may act altruistically. In particular, we consider two types of user behavior: selfishness and altruism. Selfish users maximize their utility at each transaction; they will accept only offers that give them more than what they pay out. Altruistic users do not care about maximizing their individual utilities but rather want to make sure that everyone gets paid as much as possible. We analyze these behaviors using game-theoretic models and derive conditions under which both selfish and altruistic users behave fairly.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Optimizing Scrip Systems : Efficiency , Crashes , Hoarders , and Altruists . Abstract : We research the performance of scrip systems in which participants can buying items with their own currency or by exchanging it for other people s value ( scrip ) .We see that when there are many buyers and sellers who use such a system to trade items , then the scheme is efficient if most people have an motivation to be fair merchants . However , we also find that this might not always occur because some users could attempt to hoard income while others may act altruistically .In particular , we investigate two forms of customer conduct : selfishness and altruism . Selfish users maximize their utility at each deal ; they will take only offers that give them more than what they pay out .Altruistic users do not care about maximizing their individual utilities but rather seek to make sure that everyone gets paid as much as possible . We evaluate these behaviors using game - theoretic models and derive environments under which both selfish and altruistic users behave reasonably .",
        "rewrite_text": "Title: Optimizing Scrip Systems: Efficiency, Crashes, Hoarders, and Altruists\n\nAbstract: This study explores the dynamics of scrip systems, where participants can purchase items using their own currency or by exchanging it for the value represented by others' scrip. Our findings indicate that the efficiency of such trading systems is contingent upon the presence of a majority of fair-minded participants. When a significant number of buyers and sellers engage in the marketplace with the intention of equitable trading, the system operates effectively. However, we also identify potential disruptions to this efficiency, stemming from the behavior of certain users who may hoard resources or act with altruistic intentions. Specifically, we analyze two distinct behavioral patterns among users: selfishness and altruism. Selfish participants focus on maximizing their individual utility in each transaction, opting only for offers that yield a net gain. Conversely, altruistic users prioritize the overall welfare of the group, aiming to ensure that all participants receive fair compensation, regardless of their own utility maximization. To assess these contrasting behaviors, we employ game-theoretic models, allowing us to delineate the conditions under which both selfish and altruistic behaviors can coexist and remain rational. Our research contributes to a deeper understanding of the interplay between individual motivations and collective outcomes in scrip systems, highlighting the complexities that arise from varying user strategies and their implications for system efficiency.",
        "ori-fast-z-score": 0.22645540682891913,
        "water-fast-z-score": 5.887840577551898,
        "rewrite-fast-z-score": 0.7035264706814485
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Cross-Layer Optimization of MIMO-Based Mesh Networks with Gaussian Vector Broadcast Channels .\nAbstract:\nIn this work, we consider the problem of optimizing the performance of multi-hop wireless mesh networks (WMNs) in which each node is equipped with multiple antennas and uses vector broadcast channels to communicate with its neighbors. We propose an algorithm that jointly optimizes the routing tree at the network layer and the beamforming vectors at the physical layer for both unicast traffic and multicast traffic. The proposed algorithm can be implemented using only local information exchange between neighboring nodes. Our simulation results show that our algorithm outperforms existing algorithms by up to 50% in terms of end-to-end throughput under different channel conditions. Index Terms -Wireless mesh networks, Cross-layer optimization, Beamforming, Routing trees, Multicasting. 1 Introduction Wireless mesh networks are becoming increasingly popular due to their low cost and ease of deployment  1  . In such networks, all nodes have limited transmission range and thus need to relay data packets through other nodes before reaching their destinations. This introduces additional overheads on the network resources including bandwidth consumption and energy dissipation  2  .\nTo improve the efficiency of WMNs, it has been shown recently that joint design of the network layer and the physical layer is necessary  3  , where the network layer determines how to route data packets while the physical layer decides what transmit power levels should be used as well as what beamforming vectors should be adopted  4  . However, most previous works focus either on the network layer or the physical layer separately  5  , ignoring the fact that they interact closely with one another  6  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Cross - Layer Optimization of MIMO - Based Mesh Networks with Gaussian Vector Broadcast Channels . Abstract : In this research , we investigate the question of optimizing the performance of dual - hop wireless mesh connections ( WMNs ) in which each node is furnished with many antennas and using vector broadcast channels to connect with its friends .We suggest an algorithm that collectively optimizes the routing tree at the channel layer and the beamforming vectors at the physical layer for both unicast transport and multicast transport . The proposed algorithm can be executed using only local information transfer between neighboring connections .Our model results show that our algorithm outperforms old algorithms by up to 50 % in terms of end - to - end throughput under various network conditions . Index Terms - Wireless mesh connections , Cross - layer optimization , Beamforming , Routing trees , Multicasting .1 Introduction Wireless mesh connections are growing becoming popular due to their low cost and ease of deployment 1 . In such connections , all nodes have limited transmission range and therefore need to relay data packets through other nodes before reaching their destinations .This creates additional overheads on the channel infrastructure including frequency expenditure and energy dissipation 2 . To increase the performance of WMNs , it has been shown lately that cooperative architecture of the channel layer and the physical layer is required 3 , where the network layer determines how to transport data packets while the physical layer decides what transmit energy levels should be used as well as what beamforming vectors should be adopted 4 .However , most prior efforts focus either on the network layer or the physical layer separately 5 , avoiding the fact that they interact tightly with one another 6 .",
        "rewrite_text": "**Title:** Cross-Layer Optimization of MIMO-Based Mesh Networks with Gaussian Vector Broadcast Channels\n\n**Abstract:** This study addresses the optimization of performance in dual-hop wireless mesh networks (WMNs) where each node is equipped with multiple antennas and utilizes vector broadcast channels for communication with neighboring nodes. We propose a novel algorithm that simultaneously optimizes the routing tree at the network layer and the beamforming vectors at the physical layer for both unicast and multicast transmissions. Notably, this algorithm operates effectively with only local information exchanged among adjacent nodes, enhancing its practicality in real-world applications. Our simulation results demonstrate that this approach significantly improves end-to-end throughput, achieving up to a 50% increase compared to traditional algorithms across various network scenarios. The findings underscore the importance of a cooperative strategy that integrates both the channel and physical layers, as prior research has predominantly focused on optimizing these layers in isolation. This oversight has often led to suboptimal performance due to the intricate interactions between routing decisions and physical layer parameters. By bridging this gap, our work contributes to a more holistic understanding of WMN performance optimization. The implications of our research extend to the design of more efficient wireless communication systems, particularly in environments where resource constraints such as energy and bandwidth are critical. Our results advocate for the adoption of cross-layer optimization techniques in future WMN designs, paving the way for enhanced data transmission efficiency and reliability. \n\n**Index Terms:** Wireless mesh networks, Cross-layer optimization, Beamforming, Routing trees, Multicasting.",
        "ori-fast-z-score": -1.7277368511627202,
        "water-fast-z-score": 7.20294057598537,
        "rewrite-fast-z-score": 1.247219128924647
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The photospheric environment of a solar pore with light bridge .\nAbstract:\nWe present the results of an analysis of high-resolution observations in Hα and Ca II K lines obtained by Hinode/SOT on 2007 May 24, when a large sunspot group was observed at disk center. The main feature is a bright point-like structure (a light bridge) located between two pores. We find that this light bridge has a temperature about 1000K higher than its surroundings. This result suggests that magnetic reconnection may be taking place there. In addition to the light bridge we also observe small-scale structures such as spicules and fibrils which are probably related to the magnetic field configuration around the light bridge. These features can provide important information for understanding the formation mechanism of light bridges. \n \n Keywords: Light bridge, Sunspots, Solar atmosphere, Magnetic fields, Spicule, Fibrils, Reconnection, Hinode, SOT, Hα line, CaII K line",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The photospheric landscape of a sun pore with light bridge . Abstract : We present the conclusion of an assessment of high - resolution measurements in Hα and Ca II K bands received by Hinode / SOT on 2007 May 24 , when a large sunspot group was seen at disk center .The main feature is a bright point - like structure ( a light bridge ) located between two pores . We see that this light bridge has a temperature about 1000K higher than its surroundings .This result suggests that magnetic reconnection might be taking place there . In addition to the light bridge we also observe tiny - scale structures such as spicules and fibrils which are probably related to the magnetic field configuration around the light bridge .These features can provide important information for studying the formation pathway of light bridges . Keywords : Light bridge , Sunspots , Solar atmosphere , Magnetic fields , Spicule , Fibrils , Reconnection , Hinode , SOT , Hα line , CaII K line",
        "rewrite_text": "Title: The Photospheric Landscape of a Sun Pore with Light Bridge\n\nAbstract: In this study, we present the findings from high-resolution observations in the Hα and Ca II K spectral lines, captured by the Hinode Solar Optical Telescope (SOT) on May 24, 2007, during the observation of a prominent sunspot group positioned at the center of the solar disk. A key feature identified in our analysis is a bright, point-like structure known as a light bridge, situated between two distinct sun pores. Notably, our measurements indicate that the temperature of the light bridge is approximately 1000 K higher than that of the surrounding solar material. This significant temperature differential suggests the possibility of magnetic reconnection occurring in this region, which could have implications for our understanding of solar magnetic dynamics. \n\nIn addition to the light bridge, we also document the presence of small-scale structures, including spicules and fibrils, which appear to be associated with the magnetic field configuration in the vicinity of the light bridge. These intricate features are crucial for advancing our knowledge of the processes involved in the formation and evolution of light bridges. By examining these phenomena, we aim to contribute valuable insights into the complex interactions between magnetic fields and plasma in the solar atmosphere. Our findings underscore the importance of high-resolution solar observations in unraveling the intricate dynamics of sunspots and their associated structures. \n\nKeywords: Light bridge, Sunspots, Solar atmosphere, Magnetic fields, Spicules, Fibrils, Reconnection, Hinode, SOT, Hα line, Ca II K line.",
        "ori-fast-z-score": 2.121320343559643,
        "water-fast-z-score": 5.500933918218137,
        "rewrite-fast-z-score": 1.5255401427929478
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A study of the evolution of the accretion disk of V2051 Oph through two outburst cycles .\nAbstract:\nWe present an analysis of optical and infrared photometric data obtained during the recent (2006-2008) outbursts of the dwarf novae system V2051 Oph, which is one of only three known to have exhibited both superoutbursts and normal outbursts in its lifetime.  We find that the light curve of this object shows many similarities with those observed for other SU UMa-type systems but also some significant differences. In particular we note that there are no clear signs of rebrightening following either the first or second superoutburst; nor do we see any evidence for a double-humped structure in the light curves at all phases of these events. The lack of such features may be due to the fact that our observations were made when the system was relatively faint compared to previous studies. However, it should be noted that the orbital period of V2051 Oph is significantly longer than most other SU UMa stars so that the mass transfer rate will be lower by about a factor of ten.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A investigation of the evolution of the accretion disk of V2051 Oph through two outburst cycles . Abstract : We report an assessment of optical and infrared photometric data acquired during the recent ( 2006 - 2008 ) outbursts of the dwarf novae system V2051 Oph , which is one of only three known to have exhibited both superoutbursts and normal outbursts in its lifetime .We see that the light curve of this object displays many similarities with those observed for other SU UMa - class systems but also some significant variations . In particular we note that there are no clear indication of rebrightening following either the first or second superoutburst ; nor do we find any evidence for a double - humped structure in the light curves at all phases of these events .The absence of such properties may be due to the fact that our observations were made when the system was quite dim relative to previous research . However , it should be mentioned that the orbital period of V2051 Oph is significantly greater than most other SU UMa stars so that the mass transfer time will be reduced by about a factor of ten .",
        "rewrite_text": "Title: An Investigation of the Evolution of the Accretion Disk of V2051 Oph Through Two Outburst Cycles\n\nAbstract: This study presents a comprehensive analysis of optical and infrared photometric data collected during the recent outbursts of the dwarf nova system V2051 Oph, which occurred between 2006 and 2008. V2051 Oph is notable for being one of only three known systems to have experienced both superoutbursts and normal outbursts throughout its history. Our findings reveal that the light curve of V2051 Oph shares several characteristics with those of other SU UMa-type systems; however, it also exhibits distinct differences. Notably, we observed an absence of rebrightening events following both the first and second superoutbursts, and there was no evidence of a double-humped structure in the light curves during any phase of these outbursts. This lack of typical features may be attributed to the fact that our observations were conducted while the system was in a relatively dim state compared to earlier studies. Additionally, it is important to highlight that the orbital period of V2051 Oph is considerably longer than that of most other SU UMa stars, which suggests that the mass transfer rate in this system is reduced by approximately a factor of ten. This unique characteristic may play a crucial role in the observed behavior of the accretion disk and the overall dynamics of the outburst cycles. Our research contributes to a deeper understanding of the evolutionary processes governing dwarf novae systems, particularly in the context of V2051 Oph, and underscores the need for further investigations to elucidate the mechanisms behind its distinct outburst phenomena.",
        "ori-fast-z-score": 0.45291081365783825,
        "water-fast-z-score": 5.287913134352312,
        "rewrite-fast-z-score": 1.2686700948330931
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Ballistic Graphene Nanoribbon MOSFETs: a full quantum real-space simulation study .\nAbstract:\nWe present the first fully self-consistent, atomistic quantum transport calculations for ballistic graphene nanoribbons (GNRs) with realistic band structure and electrostatic potential profiles using nonequilibrium Green s function formalism in combination with density functional theory (DFT). We show that the GNRs  electronic properties are strongly dependent on their widths as well as edge structures. The calculated current-voltage characteristics reveal several interesting features such as negative differential resistance at low bias voltages due to resonant tunneling through localized states near the Fermi level. In addition, we find that the presence of hydrogen passivation layers can significantly enhance the device performance by suppressing the backscattering effect caused by defects or impurities along the edges. \n \n Keywords: Ballistic transport, Graphene nanoribbon, Nonequilibrium Green s functions, Density functional theory, Quantum transport calculation. 1 Introduction \n \n Graphene is an emerging material which has attracted considerable attention recently because it exhibits unique physical properties  1  . It consists of carbon atoms arranged into a honeycomb lattice where each carbon atom forms covalent bonds with three neighboring carbons  2  . Due to its two-dimensional nature, graphene shows high carrier mobility  3  , thermal conductivity  4  , mechanical strength  5  , optical transparency  6  , and flexibility  7  . These remarkable properties make graphene promising candidates for future nanoelectronic devices  8  .\n \nGraphene nanoribbons (G-NR), i.e., strips of graphene with finite width  9  , have been proposed as building blocks for various applications including transistors  10  , interconnects  11  , photodetectors  12  , solar cells  13  , sensors  14  , etc.. Compared to conventional silicon-based electronics  15  , GNRs offer many advantages  16  : they exhibit higher electron mobilities  17  ; they allow better control over the charge carriers  18  ; they provide more design freedom  19  ; and they enable new functionality  20  . However, there still exist some challenges associated with practical realization of GNRs  21  . For example, the fabrication process requires precise control of ribbon width  22  and edge roughness  23  . Moreover, the electrical properties of GNRs depend sensitively",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Ballistic Graphene Nanoribbon MOSFETs : a complete quantum real - space simulation study . Abstract : We report the first fully self - coherent , atomistic quantum travel calculations for ballistic graphene nanoribbons ( GNRs ) with realistic band structure and electrostatic potential profiles using nonequilibrium Green s function formalism in combination with density functional theory ( DFT ) .We suggest that the GNRs mechanical effects are strongly dependent on their widths as well as edge properties . The measured current - frequency qualities show numerous interesting features such as negative differential resistance at low bias voltages due to resonant tunneling through confined states near the Fermi level .In addition , we find that the presence of hydrogen passivation layers can significantly boost the device performance by suppressing the backscattering effect caused by defects or impurities along the edges . Keywords : Ballistic mobility , Graphene nanoribbon , Nonequilibrium Green s functions , Density functional theory , Quantum transport calculation .1 Introduction Graphene is an developing material which has drew substantial scrutiny lately because it displays unusual physical properties 1 . It consists of carbon atoms arranged into a honeycomb lattice where each carbon molecule forms covalent bonds with three adjacent carbons 2 .Due to its two - dimensional existence , graphene shows high carrier density 3 , thermal conductivity 4 , electronic stability 5 , optical transparency 6 , and flexibility 7 . These exceptional properties make graphene promising candidates for future nanoelectronic technologies 8 .Graphene nanoribbons ( G - NR ) , i . e . , sheets of graphene with finite width 9 , have been proposed as building blocks for various uses including transistors 10 , interconnects 11 , photodetectors 12 , solar cells 13 , devices 14 , etc . . Compared to conventional silicon - based equipment 15 , GNRs offer several advantages 16 : they exhibit greater electron mobilities 17 ; they allow easier control over the charge carriers 18 ; they allow more structure freedom 19 ; and they enable new capabilities 20 .However , there still emerge some challenges associated with practical development of GNRs 21 . For instance , the fabrication process requires careful management of ribbon width 22 and edge roughness 23 .Moreover, the electrical properties of GNRs depend sensitively",
        "rewrite_text": "**Title:** Ballistic Graphene Nanoribbon MOSFETs: A Comprehensive Quantum Real-Space Simulation Study\n\n**Abstract:** In this study, we present the first fully self-consistent, atomistic quantum transport calculations for ballistic graphene nanoribbons (GNRs), employing a realistic band structure and electrostatic potential profiles. Our approach integrates the nonequilibrium Green's function formalism with density functional theory (DFT) to achieve accurate results. We demonstrate that the mechanical properties of GNRs are significantly influenced by their width and edge characteristics. The current-frequency characteristics reveal intriguing phenomena, including negative differential resistance at low bias voltages, attributed to resonant tunneling through confined states near the Fermi level. Furthermore, our findings indicate that incorporating hydrogen passivation layers can markedly enhance device performance by mitigating backscattering effects caused by defects or impurities along the edges. \n\nGraphene, a two-dimensional material composed of carbon atoms arranged in a honeycomb lattice, has garnered considerable attention due to its remarkable physical properties, including high carrier density, exceptional thermal conductivity, electronic stability, optical transparency, and flexibility. These attributes position graphene as a promising candidate for next-generation nanoelectronic applications. Graphene nanoribbons (GNRs), which are narrow strips of graphene, have been identified as potential building blocks for a variety of devices, such as transistors, interconnects, photodetectors, and solar cells. Compared to traditional silicon-based technologies, GNRs offer several advantages, including enhanced electron mobility, improved control over charge carriers, greater structural versatility, and the potential for novel functionalities. However, challenges remain in the practical realization of GNRs, particularly concerning the precise control of ribbon width and edge roughness, which are critical for optimizing their electrical properties. This study aims to address these challenges and provide insights into the quantum transport mechanisms that govern the performance of GNR-based devices. \n\n**Keywords:** Ballistic mobility, Graphene nanoribbon, Nonequilibrium Green's functions, Density functional theory, Quantum transport calculation.",
        "ori-fast-z-score": -0.6964409092807231,
        "water-fast-z-score": 7.362863892934755,
        "rewrite-fast-z-score": 1.281025230440697
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A high-frequency radio continuum study of massive young stellar objects .\nAbstract:\nWe present the results of an unbiased survey for compact HII regions in the southern Galactic plane using data obtained with the Australia Telescope Compact Array (ATCA). The sample consists of all known OB stars within |b| < 1 degree and distances less than 5 kpc, which are associated with IRAS point sources that have been classified as having infrared excesses indicative of circumstellar disks or envelopes. We detect over 100 new compact HII regions at frequencies between 2.1 GHz and 6.0 GHz. These objects range in size from 0.01 pc to 0.5 pc and their luminosities vary by more than four orders of magnitude. Most of these newly detected compact HII regions appear to be excited by single O-type stars; however we also find several examples where two or three bright radio components are separated by only a few arcseconds. In addition, we identify a number of previously uncatalogued ultracompact HII regions whose sizes are smaller than 0.01 pc.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A high - frequency radio continuum study of large small stars objects . Abstract : We report the conclusion of an unbiased survey for compact HII zones in the southern Galactic jet using data acquired with the Australia Telescope Compact Array ( ATCA ) .The sample consists of all known OB stars within | b | < 1 degree and altitudes lower than 5 kpc , which are identified with IRAS point sources that have been classified as having infrared excesses indicative of circumstellar disks or envelopes . We detect over 100 new compact HII zones at speeds between 2 . 1 GHz and 6 . 0 GHz .These bodies range in height from 0 . 01 pc to 0 . 5 pc and their luminosities vary by more than four orders of magnitude . Most of these newly observed compact HII regions seem to be excited by single O - class stars ; however we also find various instances where two or three dark radio components are split by only a few arcseconds .In addition , we identify a number of previously uncatalogued ultracompact HII domains whose sizes are smaller than 0 . 01 pc .",
        "rewrite_text": "We present the findings of a comprehensive survey aimed at identifying compact HII regions within the southern Galactic jet, utilizing data from the Australia Telescope Compact Array (ATCA). Our study encompasses all known OB stars located within |b| < 1 degree and at distances less than 5 kpc, specifically targeting IRAS point sources that exhibit infrared excesses, which are indicative of the presence of circumstellar disks or envelopes. Through our observations, we have successfully detected over 100 new compact HII regions, with radio frequencies ranging from 2.1 GHz to 6.0 GHz. The dimensions of these regions vary significantly, with heights ranging from 0.01 pc to 0.5 pc, and their luminosities span more than four orders of magnitude. \n\nThe majority of the newly identified compact HII zones appear to be energized by individual O-class stars. However, our survey also reveals several instances where two or three dark radio components are closely spaced, separated by only a few arcseconds. Furthermore, we have discovered a number of previously unrecorded ultracompact HII regions, which are notably smaller than 0.01 pc in size. This research not only enhances our understanding of the distribution and characteristics of compact HII regions in the southern Galactic jet but also contributes to the broader knowledge of star formation processes and the environments surrounding massive stars. The implications of these findings are significant for astrophysical studies, particularly in the context of the evolution of stellar nurseries and the dynamics of interstellar matter.",
        "ori-fast-z-score": -0.22941573387056174,
        "water-fast-z-score": 5.128225940683707,
        "rewrite-fast-z-score": 0.3922322702763681
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  An XMM-Newton study of Hyper-Luminous Infrared Galaxies .\nAbstract:\nWe present an analysis of the X-ray properties of a sample of 12 hyper-luminous infrared galaxies (HLIRGs) observed with XMM-Newton, using data obtained in AO-1 and AO-2. The HLIRG sample is selected to have L(8-1000um)>10^12L_sun , where L(8-1000um), is derived by integrating over the best-fit SEDs for each source. We find that all sources are detected at >5 sigma significance in the 0.3-10 keV band; however only two objects show evidence for significant absorption above Galactic levels. For these two absorbed systems we derive column densities NH = 1.7 x 10^23 cm^{-2} and 2.1 x 10^22 cm^{-2} respectively. Using the hardness ratio HR=H-S/H+S, where H and S represent counts in the 3-7keV and 0.3-2keV bands respectively, we find no correlation between HR and either luminosity or redshift. This suggests that there may be little evolution in the intrinsic spectral shape of this population out to z=2.6.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : An XMM - Newton experiment of Hyper - Luminous Infrared Galaxies . Abstract : We present an assessment of the X - ray characteristics of a sample of 12 hyper - luminous infrared galaxies ( HLIRGs ) detected with XMM - Newton , using data derived in AO - 1 and AO - 2 .The HLIRG sample is selected to have L ( 8 - 1000um ) > 10 ^ 12L _ sunshine , where L ( 8 - 1000um ) , is calculated by combining over the best - fitting SEDs for each source . We see that all sources are detected at > 5 sigma significance in the 0 . 3 - 10 keV band ; however only two objects show proof for significant absorption above Galactic concentrations .For these two absorbed systems we derive row densities NH = 1 . 7 x 10 ^ 23 centimetres ^ { - 2 } and 2 . 1 x 10 ^ 22 mm ^ { - 2 } respectively . Using the hardness factor HR = H - S / H + S , where H and S refer counts in the 3 - 7keV and 0 . 3 - 2keV bands respectively , we find no correlation between HR and either luminosity or redshift .This implies that there may be little development in the intrinsic spectral structure of this community out to z = 2 . 6 .",
        "rewrite_text": "We present a comprehensive analysis of the X-ray properties of a sample comprising 12 hyper-luminous infrared galaxies (HLIRGs) observed with the XMM-Newton satellite, utilizing data collected during the first and second observation cycles (AO-1 and AO-2). The selected HLIRG sample is characterized by an infrared luminosity greater than 10^12 L_sun in the wavelength range of 8 to 1000 micrometers, with luminosities calculated based on the best-fitting spectral energy distributions (SEDs) for each individual source. Our findings indicate that all 12 galaxies are detected with a significance exceeding 5 sigma in the 0.3 to 10 keV energy band. Notably, only two of these galaxies exhibit evidence of significant X-ray absorption beyond the levels expected from the Galactic foreground. For these two absorbed sources, we have determined column densities of NH = 1.7 x 10^23 cm^-2 and 2.1 x 10^22 cm^-2, respectively. Furthermore, we employed the hardness ratio (HR), defined as HR = (H - S) / (H + S), where H and S represent the counts in the 3-7 keV and 0.3-2 keV bands, respectively. Our analysis reveals no significant correlation between the hardness ratio and either the luminosity or redshift of the galaxies in our sample. This lack of correlation suggests that there is minimal evolution in the intrinsic spectral characteristics of HLIRGs up to a redshift of z = 2.6. Overall, our study contributes valuable insights into the X-ray emission properties of hyper-luminous infrared galaxies, enhancing our understanding of their physical conditions and the role they play in the broader context of galaxy evolution.",
        "ori-fast-z-score": 0.12216944435630522,
        "water-fast-z-score": 5.497624996033735,
        "rewrite-fast-z-score": -0.09759000729485333
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Chemical Compositions of the Type II Cepheids -- The BL Her and W Vir Variables .\nAbstract:\nThe chemical compositions of the type II Cepheid variables are investigated by using high-resolution spectra obtained with Subaru/HDS, Keck/HIRES, VLT/UVES, and McDonald/DEIMOS spectrographs.  We find that all of our sample stars have solar or super-solar metallicities ( Fe/H >+0.2). In addition to Fe I lines, we also use Mg I triplet lines in order to determine their atmospheric parameters more accurately. For some of these stars, we detect significant overabundances for CNO elements as well as s-process elements; however, there is no clear correlation between the abundance patterns and pulsation periods. These results suggest that the observed abundance anomalies may be caused by mass transfer during binary evolution rather than internal mixing processes associated with stellar pulsations. This work was supported by JSPS Grant-in-Aid for Scientific Research on Priority Areas No. 16071203.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Chemical Compositions of the Type II Cepheids - - The BL Her and W Vir Variables . Abstract : The chemical compositions of the class II Cepheid variables are examined by using high - resolution spectra obtained with Subaru / HDS , Keck / HIRES , VLT / UVES , and McDonald / DEIMOS spectrographs .We see that all of our sample stars have solar or super - solar metallicities ( Fe / H > + 0 . 2 ) . In addition to Fe I lines , we also need Mg I triplet lines in order to estimate their atmospheric parameters more accurately .For some of these stars , we find considerable overabundances for CNO elements as well as s - process elements ; however , there is no clear correlation between the abundance patterns and pulsation periods . These conclusions show that the seen abundance anomalies may be caused by mass transfer during binary development rather than internal mixing mechanisms associated with stars pulsations .This project was supported by JSPS Grant - in - Aid for Scientific Research on Priority Areas No . 16071203 .",
        "rewrite_text": "Title: The Chemical Compositions of Type II Cepheids - The BL Her and W Vir Variables\n\nAbstract: This study investigates the chemical compositions of Type II Cepheid variables, specifically focusing on the BL Herculis and W Virginis stars. Utilizing high-resolution spectra obtained from advanced spectrographs, including Subaru/HDS, Keck/HIRES, VLT/UVES, and McDonald/DEIMOS, we analyze the metallicity of our sample stars. Our findings indicate that all observed stars exhibit solar or super-solar metallicities, with iron-to-hydrogen ratios (Fe/H) exceeding +0.2. In our analysis, we incorporate not only Fe I lines but also Mg I triplet lines, which are essential for accurately determining the atmospheric parameters of these stars. Notably, we observe significant overabundances of carbon, nitrogen, and oxygen (CNO) elements, as well as elements produced through the s-process. However, our results reveal no definitive correlation between these abundance patterns and the pulsation periods of the stars. This suggests that the observed abundance anomalies may be attributed to mass transfer processes occurring during the evolution of binary systems, rather than being a result of internal mixing mechanisms typically associated with stellar pulsations. The implications of these findings contribute to our understanding of the chemical evolution of Type II Cepheids and highlight the complexities involved in their formation and development. This research was made possible through the support of the Japan Society for the Promotion of Science (JSPS) Grant-in-Aid for Scientific Research on Priority Areas No. 16071203.",
        "ori-fast-z-score": 0.75,
        "water-fast-z-score": 3.8450767722654624,
        "rewrite-fast-z-score": 1.0425720702853738
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  On the generalization of the Costas property in the continuum .\nAbstract:\nWe study the problem of finding an optimal set of points on the unit circle that are equidistant to each other and have minimum angular separation between them, known as the Costas array or Costas configuration.  We show how this problem can be formulated as a convex optimization problem with linear constraints over the space of probability measures supported by the unit circle. This formulation allows us to use tools from convex analysis for solving it efficiently. In particular we provide a polynomial time algorithm which computes an approximate solution within any desired accuracy. The proposed method is based on the Frank-Wolfe (FW) algorithm combined with the Sinkhorn-Knopp scaling technique. Finally, numerical experiments demonstrate the efficiency of our approach compared to existing methods. Keywords: Convex Optimization; Probability Measures; Frank Wolfe Algorithm; Scaling Technique; Unit Circle; Costas Array; Costas Configuration.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : On the generalization of the Costas property in the continuum . Abstract : We explore the question of finding an appropriate collection of points on the unit circle that are equidistant to each other and have minimum spatial separation between them , known as the Costas array or Costas configuration .We see how this question can be formulated as a convex optimization problem with linear constraints over the space of likelihood measures supported by the unit circle . This formulation enables us to use tools from convex optimization for solving it easily .In particular we provide a polynomial period approximation which computes an approximate solve within any desired accuracy . The proposed approach is based on the Frank - Wolfe ( FW ) algorithm coupled with the Sinkhorn - Knopp scaling methodology .Finally , numerical studies demonstrate the performance of our approach relative to existing techniques . Keywords : Convex Optimization ; Probability Measures ; Frank Wolfe Algorithm ; Scaling Technique ; Unit Circle ; Costas Array ; Costas Configuration .",
        "rewrite_text": "Title: On the Generalization of the Costas Property in the Continuum\n\nAbstract: This article investigates the challenge of identifying a suitable set of points on the unit circle that are equidistant from one another while maintaining the minimum possible spatial separation, a configuration known as a Costas array or Costas configuration. We reformulate this problem as a convex optimization task with linear constraints, specifically focusing on the space of likelihood measures that are supported on the unit circle. This reformulation allows us to leverage advanced techniques from convex optimization to facilitate an efficient solution process. Notably, we introduce a polynomial period approximation method that enables the computation of an approximate solution with a specified level of accuracy. Our approach utilizes the Frank-Wolfe (FW) algorithm in conjunction with the Sinkhorn-Knopp scaling technique, which enhances the effectiveness of the optimization process. To validate our methodology, we present numerical experiments that illustrate the performance of our proposed approach in comparison to existing methods in the field. The results indicate that our technique not only achieves the desired configurations but also improves upon the efficiency and accuracy of previous strategies. This work contributes to the broader understanding of Costas configurations and their applications in various scientific and engineering domains. \n\nKeywords: Convex Optimization; Probability Measures; Frank-Wolfe Algorithm; Scaling Technique; Unit Circle; Costas Array; Costas Configuration.",
        "ori-fast-z-score": -1.8325416653445783,
        "water-fast-z-score": 3.491189739487307,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Double Neutron Stars: Evidence For Two Different Neutron-Star Formation Mechanisms .\nAbstract:\nWe present the discovery and analysis of two double neutron stars (DNSs) with masses in excess of 2 M_sun, PSR J0737-3039A/B and PSR B1913+16. The former is an eclipsing system that has been observed to undergo orbital decay at a rate consistent with gravitational wave emission; it will merge within about 3 Myr. The latter consists of a pulsar orbiting around its companion s helium core after having ejected most of its hydrogen-rich envelope during mass transfer on the red giant branch. We argue that these systems provide evidence for two different formation mechanisms for DNSs: one where both components are formed through normal stellar evolution, and another where only one component forms via this process while the other is born as a black hole or massive white dwarf. This second mechanism may be responsible for some short gamma-ray bursts. DOI: 10.1103/PhysRevD.76.084011",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Double Neutron Stars : Evidence For Two Different Neutron - Star Formation Mechanisms . Abstract : We report the discovery and investigation of two double neutron galaxies ( DNSs ) with masses in excess of 2 M _ sun , PSR J0737 - 3039A / B and PSR B1913 + 16 .The former is an eclipsing system that has been observed to undergo orbital decay at a rate correlated with gravity wave radiation ; it will merge within about 3 Myr . The latter consists of a pulsar orbiting around its companion s helium core after having liberated most of its hydrogen - laden envelope during mass transfer on the red dwarf branch .We argue that these systems represent proof for two different formation methods for DNSs : one where both components are created through regular stars evolution , and another where only one element forms via this process while the other is born as a black hole or massive gray dwarf . This second process may be responsible for some short gamma - ray bursts .DOI: 10.1103/PhysRevD.76.084011",
        "rewrite_text": "Title: Double Neutron Stars: Evidence for Two Distinct Neutron Star Formation Mechanisms\n\nAbstract: In this study, we present the discovery and detailed analysis of two double neutron star (DNS) systems, PSR J0737-3039A/B and PSR B1913+16, both exhibiting masses exceeding 2 solar masses (M☉). The first system, PSR J0737-3039A/B, is characterized as an eclipsing binary that demonstrates orbital decay at a rate consistent with gravitational wave emission, projecting a merger timeframe of approximately 3 million years. The second system, PSR B1913+16, features a pulsar in orbit around the helium core of its companion star, which has shed most of its hydrogen-rich envelope during the mass transfer phase on the red giant branch. \n\nOur findings suggest that these two DNS systems provide compelling evidence for two distinct formation pathways for neutron stars. The first pathway involves the evolution of both components from standard stellar processes, while the second pathway posits that one of the components forms through conventional stellar evolution, whereas the other is born as a black hole or a massive gray dwarf. This latter formation mechanism may also account for certain short gamma-ray bursts observed in the universe. The implications of these findings are significant, as they enhance our understanding of neutron star formation and the diverse evolutionary paths that lead to the creation of these exotic objects. The results of this research contribute to the broader discourse on stellar evolution and the mechanisms underlying the formation of compact binary systems. DOI: 10.1103/PhysRevD.76.084011",
        "ori-fast-z-score": 1.0533703247651751,
        "water-fast-z-score": 5.579886659703326,
        "rewrite-fast-z-score": 0.2727272727272727
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Space Velocities of Southern Globular Clusters. V. A Low Galactic Latitude Sample .\nAbstract:\nWe have measured the space velocities for eight globular clusters in the southern hemisphere with galactic latitudes less than 20 degrees, using proper motions and radial velocities obtained by various authors over the past decade or so.  The sample includes four open clusters (NGC 2420 , NGC 2516 , NGC 2682 , and NGC 6705 ) as well as four globulars . We find that all but one cluster are consistent with being at rest relative to the local standard of rest; however, we also find evidence that two of these clusters may be on orbits which will take them out of our galaxy within several billion years .  These results suggest that there is no significant difference between open clusters and globulars when it comes to their kinematics ; both types appear to share similar properties .\nThe only exception appears to be the open cluster M67 , whose velocity vector points away from us toward the constellation Cetus . This result suggests that this open cluster has been ejected from its parent galaxy during an encounter with another galaxy some time ago .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  Space Velocities of Southern Globular Clusters.V. A Low Galactic Latitude Sample .Abstract : We have recorded the space velocities for eight globular complexes in the southern hemisphere with galactic latitudes less than 20 degrees , using proper motions and radial velocities collected by various authors over the previous decade or so . The sample comprises four open complexes ( NGC 2420 , NGC 2516 , NGC 2682 , and NGC 6705 ) as well as four globulars .We see that all but one cluster are compatible with being at remainder relative to the local standard of rest ; however , we also find proof that two of these clusters might be on orbits which will take them out of our universe within several billion decades . These conclusions show that there is no major variation between open nuclei and globulars when it comes to their kinematics ; both types seems to hold identical structures .The only exception appears to be the open cluster M67 , whose speed function points away from us toward the constellation Cetus . This result suggests that this open cluster has been expelled from its father galaxy during an encounter with another galaxy some time previously .",
        "rewrite_text": "Title: Space Velocities of Southern Globular Clusters: A Low Galactic Latitude Sample\n\nAbstract: In this study, we present an analysis of the space velocities of eight globular complexes located in the southern hemisphere, specifically those with galactic latitudes below 20 degrees. Utilizing proper motions and radial velocities compiled from various sources over the past decade, our sample includes four open clusters—NGC 2420, NGC 2516, NGC 2682, and NGC 6705—alongside four globular clusters. Our findings indicate that, with the exception of one cluster, all analyzed complexes exhibit velocities consistent with their positions relative to the local standard of rest. Notably, we have identified evidence suggesting that two of these clusters may be on trajectories that could eventually lead them to escape our galaxy over the course of several billion years. This research highlights the absence of significant differences in kinematic behavior between open clusters and globular clusters, as both categories appear to share similar structural characteristics. The sole outlier in our observations is the open cluster M67, which displays a velocity vector directed away from us towards the constellation Cetus. This particular result implies that M67 may have been ejected from its parent galaxy following a gravitational interaction with another galaxy in the past. Overall, our study contributes to the understanding of the dynamics and evolutionary pathways of southern globular complexes, shedding light on their kinematic properties and the potential influences of galactic interactions.",
        "ori-fast-z-score": -1.9126494315742406,
        "water-fast-z-score": 4.2485291572496005,
        "rewrite-fast-z-score": -2.1783245945486835
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Quantum mechanical approach to decoherence and relaxation generated by fluctuating environment .\nAbstract:\nWe present an exact quantum-mechanical treatment for the dynamics of open systems in which the system is coupled to many harmonic oscillators representing its surrounding environment.  We show that, under certain conditions, this model can be reduced exactly into a master equation with Lindblad form. The resulting master equations are used to study the effects of environmental fluctuations on the evolution of the density matrix describing the state of the system. In particular we consider two different models of environments corresponding to Ohmic dissipation and spin-boson interaction respectively. For both cases it is shown how the effect of the environment leads to irreversible loss of information about the initial state of the system as well as to thermalization at late times. Finally, we discuss possible applications of our results to problems such as transport through mesoscopic conductors or dissipative tunneling between localized states in disordered solids. Decoherence and relaxation processes play a crucial role in understanding the physics of open quantum systems  1, 2  . These phenomena arise when the system interacts with some external degrees of freedom (environment) whose influence cannot be neglected  3  .\nIn recent years there has been considerable interest in developing theoretical methods capable of treating these effects beyond the perturbative regime  4  . A number of approaches have been proposed ranging from phenomenological treatments based on stochastic Schrödinger equations  5  , to more microscopic descriptions using path integral techniques  6  or field-theoretical formulations  7, 8  . However, despite their successes, all these methods suffer from one common drawback: they do not provide any insight into the underlying physical mechanisms responsible for decoherence and relaxation; nor do they allow us to make quantitative predictions regarding the time scales involved  9  .\nRecently, several authors  10 -12  have suggested that the problem may be tackled within the framework of quantum mechanics itself. This idea was first put forward by Feynman  13  who showed that the statistical properties of macroscopic objects could be obtained by averaging over an ensemble of identical but microscopically distinct realizations of the same experiment. More recently, Leggett  14  introduced a method...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Quantum mechanical method to decoherence and relaxation generated by fluctuating conditions . Abstract : We present an precise quantum - mechanical explanation for the dynamics of open systems in which the system is linked to many harmonic oscillators describing its surrounding environment .We see that , under certain conditions , this description can be reduced exactly into a master equation with Lindblad form . The resulting master equations are applied to study the effects of environmental fluctuations on the evolution of the density graph explaining the state of the system .In particular we study two different models of environments corresponding to Ohmic dissipation and spin - boson collision respectively . For both cases it is demonstrated how the impact of the surroundings leads to irreversible loss of information about the first state of the system as well as to thermalization at late times .Finally , we discuss possible applied of our findings to problems such as transport through mesoscopic conductors or dissipative tunneling between restricted states in disordered solids . Decoherence and relaxation processes drive a crucial role in understanding the physics of open quantum systems 1 , 2 .These phenomena arise when the process interacts with some external degrees of autonomy ( climate ) whose influence cannot be forgotten 3 . In recent years there has been substantial interest in establishing theoretical methods capable of addressing these influences beyond the perturbative regime 4 .A variety of methods have been proposed ranging from phenomenological treatments based on stochastic Schrödinger coefficients 5 , to more microscopic descriptions using path integral methods 6 or field - theory formulations 7 , 8 . However , despite their successes , all these theories suffer from one common drawback : they do not offer any insight into the fundamental physical mechanisms involved for decoherence and relaxation ; nor do they allow us to make quantitative predictions regarding the period scales required 9 .Recently , various scientists 10 - 12 have suggested that the issue may be tackled within the framework of quantum mechanics itself . This idea was first put forward by Feynman 13 who demonstrated that the statistical characteristics of macroscopic objects may be obtained by averaging over an ensemble of different but microscopically different realizations of the same experiment .More recently , Leggett 14 proposed a technique . . .",
        "rewrite_text": "**Title:** Quantum Mechanical Approach to Decoherence and Relaxation Induced by Fluctuating Conditions\n\n**Abstract:** In this study, we provide a comprehensive quantum-mechanical framework for understanding the dynamics of open systems that interact with a multitude of harmonic oscillators representing their surrounding environment. We demonstrate that, under specific conditions, this complex interaction can be accurately simplified into a master equation of the Lindblad form. The derived master equations are utilized to investigate the influence of environmental fluctuations on the evolution of the system's density matrix, which encapsulates its state. We focus on two distinct environmental models: one representing Ohmic dissipation and the other corresponding to spin-boson collisions. Our findings reveal that the interaction with the environment leads to an irreversible loss of information regarding the initial state of the system and facilitates thermalization over extended time scales. Furthermore, we explore the implications of our results for practical applications, such as transport phenomena in mesoscopic conductors and dissipative tunneling processes in disordered solids. Decoherence and relaxation are pivotal in elucidating the behavior of open quantum systems, particularly as they arise from interactions with external degrees of freedom, often referred to as the environment. Recent advancements in theoretical approaches have sought to address these interactions beyond perturbative methods, employing a range of strategies from stochastic Schrödinger equations to path integral and field-theoretic formulations. Despite their successes, these approaches typically fall short in providing insights into the underlying physical mechanisms driving decoherence and relaxation, as well as in making precise quantitative predictions regarding the relevant time scales. Recent contributions suggest that these challenges may be addressed within the quantum mechanical framework itself, a notion initially proposed by Feynman, who illustrated that the statistical properties of macroscopic systems can be derived from averaging over various microscopic realizations of identical experiments. Building on this foundation, Leggett and others have advanced techniques to further explore these phenomena, paving the way for deeper understanding and new applications in quantum mechanics.",
        "ori-fast-z-score": -0.15339299776947407,
        "water-fast-z-score": 7.3747734363161435,
        "rewrite-fast-z-score": -0.727606875108999
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Zero-point radiation and the Big Bang .\nAbstract:\nThe concept of zero point energy is introduced in this article, which describes how it can be used to explain some aspects of the big bang theory. The author also discusses the possibility that dark matter may have been created by such an effect. Finally he considers whether or not there are any other physical effects associated with zero point energy. In particular, he suggests that gravitational waves could possibly be generated by such processes. This article was originally published on ArXiv.org as part of the author s PhD thesis at Imperial College London. It has since been updated for publication here. Zero point energy (ZPE) is defined as the minimum amount of energy required to create particles out of nothing. Although ZPE cannot actually exist because it violates the laws of physics, it does provide useful insight into certain phenomena observed within our universe. For example, if we consider the expansion of space-time during the early stages of the universe s existence, then it seems reasonable to assume that the volume of space increased exponentially over time due to the rapid release of ZPE. If so, then the total mass-energy density would decrease rapidly until all available ZPE had been released. At this stage, the universe would consist solely of vacuum fluctuations, i.e., virtual particles and antiparticles existing simultaneously but never interacting with each other.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Zero - point radiation and the Big Bang . Abstract : The concept of zero point energy is included in this page , which explains how it can be used to explain some elements of the big bang theory .The author also examines the suggestion that dark matter may have been created by such an effect . Finally he considers whether or not there are any other physical effects involved with zero point energy .In particular , he argues that gravity signals could possibly be induced by such processes . This section was originally published on ArXiv . org as part of the writer s PhD thesis at Imperial College London .It has since been amended for published here . Zero point energy ( ZPE ) is measured as the zero amount of energy required to create particles out of nothing .Although ZPE cannot actually appear because it violates the rules of science , it does provide useful insight into various phenomena observed within our universe . For instance , if we study the advance of space - time during the early stages of the universe s existence , then it appears justified to assume that the volume of space expanded exponentially over time due to the fast release of ZPE .If so , then the total mass - energy density would decrease rapidly until all available ZPE had been released . At this phase , the universe might consist solely of vacuum fluctuations , i . e . , virtual ions and antiparticles emerging concurrently but never interacting with each other .",
        "rewrite_text": "**Title: Zero-Point Radiation and the Big Bang**\n\n**Abstract:** This article explores the intriguing concept of zero-point energy (ZPE) and its potential implications for understanding aspects of the Big Bang theory. The author delves into how ZPE might elucidate certain phenomena associated with the early universe, particularly in relation to the rapid expansion of space-time. The discussion includes a hypothesis suggesting that dark matter could have originated from zero-point energy effects, thereby linking these two fundamental components of cosmology. Furthermore, the author investigates additional physical effects that may arise from ZPE, positing that gravitational signals could be influenced by these processes. \n\nOriginally part of the author's PhD thesis at Imperial College London, this section has been revised for publication here. Zero-point energy is defined as the minimum energy required to create particles from a vacuum state, a concept that, while theoretically impossible due to its contradiction of established scientific principles, offers valuable insights into various cosmic phenomena. \n\nThe article posits that during the universe's formative moments, the rapid release of ZPE could have led to an exponential expansion of space. This expansion would result in a swift decrease in the total mass-energy density, potentially leaving the universe in a state dominated by vacuum fluctuations. In this scenario, virtual particles, including ions and antiparticles, would emerge simultaneously yet remain non-interacting. This exploration not only enhances our understanding of the early universe but also opens new avenues for investigating the nature of dark matter and the fundamental forces at play in the cosmos.",
        "ori-fast-z-score": 0.647150228929434,
        "water-fast-z-score": 6.9937860618023535,
        "rewrite-fast-z-score": 0.6115928396627265
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Building Portable Thread Schedulers for Hierarchical Multiprocessors: the BubbleSched Framework .\nAbstract:\nWe present an open-source framework, called Bubblesched, that allows to build portable thread schedulers for hierarchical multiprocessors (HMPs). The main idea is to use bubbles as scheduling units and schedule them on different levels in HMP hierarchy using a set of rules. We have implemented two schedulers: one based on work stealing and another one based on load balancing. Both schedulers are able to run efficiently on top of Bubblesched without any modifications. Our experimental results show that both schedulers outperform state-of-the-art solutions by up to 3Â times when running parallel applications with fine-grained tasks. In addition, we demonstrate how our scheduler can be used to implement efficient task-parallel algorithms such as graph coloring or matrix multiplication. This research was supported by Russian Science Foundation grant 14-50-00040. We present an opensource framework, called Bubblesched: it allows to build portable threadschedulers for hierarchicalmultiprocessors(HMPs)andrunefficientlyonthemwithoutanymodifications.Theframeworkusesbubblesastheschedu-lingsunitsandschedulethemonthelevelsofHMPhierarchyusingasetofrules.Wehaveimplementedtwo-schedulers:onebasedonstealingworkandanotheronesupportedbyloadbalancing.BothschedulersexecutesuccessfullyontopofBubbleschedwithouthavingtobemodified.Ourexperimentalresultsshowthatbothschedulersoutperformstate-oftheartsolutionsupto3timeswhenrunningparallelapplicationswithfinegrainetasks.Inaddition,weillustratethattheframeworkcanbeusedtomakeefficienttask-parallelandalgorithmssuchastask-coloringanoregularmatrixmultiplication.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Building Portable Thread Schedulers for Hierarchical Multiprocessors : the BubbleSched Framework . Abstract : We create an free - source framework , entitled Bubblesched , that enables to build portable thread schedulers for hierarchical multiprocessors ( HMPs ) .The main idea is to use bubbles as booking units and pace them on various levels in HMP hierarchy following a setting of rules . We have developed two schedulers : one based on work stealing and another one based on load balancing .Both schedulers are able to run efficiently on top of Bubblesched without any modifications . Our research results show that both schedulers outperform state - of - the - art systems by up to 3Â times when ran parallel clients with fine - grained assignments .In addition , we prove how our scheduler can be used to execute faster job - parallel techniques such as graph coloring or matrix multiplication . This research was supported by Russian Science Foundation award 14 - 50 - 00040 .We create an opensource implementation , entitled Bubblesched : it allows to build portable threadschedulers for hierarchicalmultiprocessors ( HMPs ) andrunefficientlyonthemwithoutanymodifications . Theframeworkusesbubblesastheschedu - lingsunitsandschedulethemonthelevelsofHMPhierarchyusingasetofrules . Wehaveimplementedtwo - schedulers : onebasedonstealingworkandanotheronesupportedbyloadbalancing . BothschedulersexecutesuccessfullyontopofBubbleschedwithouthavingtobemodified . Ourexperimentalresultsshowthatbothschedulersoutperformstate - oftheartsolutionsupto3timeswhenrunningparallelapplicationswithfinegrainetasks . Inaddition , weillustratethattheframeworkcanbeusedtomakeefficienttask - parallelandalgorithmssuchastask - coloringanoregularmatrixmultiplication .",
        "rewrite_text": "In this article, we introduce an open-source framework named BubbleSched, designed to facilitate the development of portable thread schedulers for hierarchical multiprocessors (HMPs). The core concept of BubbleSched revolves around utilizing \"bubbles\" as scheduling units, which are strategically placed across different levels of the HMP hierarchy according to a predefined set of rules. We have successfully implemented two distinct schedulers within this framework: one that employs a work-stealing mechanism and another that focuses on load balancing. Both schedulers are capable of operating efficiently on the BubbleSched platform without requiring any modifications.\n\nOur experimental findings demonstrate that these schedulers significantly outperform existing state-of-the-art systems, achieving performance improvements of up to three times when executing parallel applications with fine-grained task assignments. Furthermore, we provide evidence that our framework can be effectively utilized to enhance the execution of advanced job-parallel techniques, such as graph coloring and matrix multiplication.\n\nThis research was supported by the Russian Science Foundation under award number 14-50-00040. The BubbleSched framework not only streamlines the process of building portable thread schedulers for HMPs but also ensures that they can run efficiently without the need for alterations. By leveraging the concept of bubbles as scheduling units and implementing a structured approach to their placement within the HMP hierarchy, we have laid the groundwork for future advancements in parallel computing. Our work opens new avenues for optimizing task-parallel algorithms and demonstrates the potential of BubbleSched in improving computational efficiency across various applications.",
        "ori-fast-z-score": -1.0392304845413263,
        "water-fast-z-score": 4.817730411281796,
        "rewrite-fast-z-score": -0.4583492485141057
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Detection of VHE gamma-ray emission from the distant blazar 1ES 1101-232 with H.E.S.S. and broadband characterisation .\nAbstract:\nWe report on observations made by the High Energy Stereoscopic System (H.E.S. S.) telescope array in Namibia, which detected very-high-energy (VHE) gamma rays from the distant blazar  1ES1102-232 at redshift z = 0.186. The source was observed for more than 50 hours between September 2005 and March 2006 using data taken simultaneously with four telescopes. A total excess of 12 events above background were found within an energy range of 400 GeV to 20 TeV. No significant variability is seen during this period. We present results from spectral analysis performed over different time intervals as well as broadband modelling of the multi-wavelength spectrum including radio through X-ray measurements. This work demonstrates that H.E.S.  S. can detect sources beyond redshifts previously accessible only to ground-based Cherenkov telescopes. It also shows how such observations are important for understanding the physics of these extreme objects.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Detection of VHE gamma - ray radiation from the distant blazar 1ES 1101 - 232 with H . E . S . S . and broadband characterisation .Abstract : We report on observations made by the High Energy Stereoscopic System ( H . E . S . S . ) telescope array in Namibia , which detected very - large - energy ( VHE ) gamma radiation from the distant blazar 1ES1102 - 232 at redshift z = 0 . 186 .The source was seen for more than 50 hours between September 2005 and March 2006 using data taken concurrently with four telescopes . A total excess of 12 events above background were found within an energy range of 400 GeV to 20 TeV .No considerable variability is seen during this time . We see results from spectral study performed over different time periods as well as broadband analysis of the multi - wavelength spectrum including radio through X - ray observations .This research proves that H . E . S . S . can identify sources beyond redshifts previously accessible only to surface - based Cherenkov telescopes .It additionally shows how such observations are important for knowledge the physics of these extreme objects .",
        "rewrite_text": "We present findings from observations conducted with the High Energy Stereoscopic System (H.E.S.S.) telescope array located in Namibia, which successfully detected very-high-energy (VHE) gamma-ray radiation from the distant blazar 1ES 1101 - 232, situated at a redshift of z = 0.186. The observational campaign spanned over 50 hours between September 2005 and March 2006, utilizing data collected concurrently from four telescopes. Our analysis revealed a total of 12 excess events above the background noise within the energy range of 400 GeV to 20 TeV. Notably, we observed no significant variability in the gamma-ray emission during the observation period. \n\nIn addition to the detection of VHE gamma rays, we conducted a spectral analysis over various time intervals, which provided insights into the source's emission characteristics. Furthermore, we performed a comprehensive broadband analysis that integrated multi-wavelength data, encompassing observations from radio to X-ray frequencies. This research underscores the capability of H.E.S.S. to detect gamma-ray sources at redshifts that were previously beyond the reach of ground-based Cherenkov telescopes. The implications of these findings are significant, as they enhance our understanding of the physics governing these extreme astrophysical objects. The ability to study such distant blazars contributes to the broader knowledge of high-energy processes in the universe and the mechanisms driving the emission of VHE gamma rays. Overall, our results highlight the importance of continued observations and analyses of distant blazars to unravel the complexities of high-energy astrophysics.",
        "ori-fast-z-score": 0.819288030372914,
        "water-fast-z-score": 5.500933918218137,
        "rewrite-fast-z-score": 0.2847473987257497
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Logarithmic corrections and soft photon phenomenology in the multipole model of the nucleon form factors .\nAbstract:\nWe present an analysis of the logarithmic corrections to the leading order predictions for the electromagnetic form factor ratios, R = F2(q2)/F1(q2), and their dependence on the momentum transfer q2. We use the results obtained within the framework of the multipole expansion method (MEM). The MEM is based on the assumption that the proton can be represented as a superposition of three different charge distributions with definite angular momenta. In this approach we calculate the contributions coming from the lowest lying resonances, i.e., S11(1650), P11(1710), D13(1520), F15(1680), G17(1880), H19(2230), and also take into account the effects due to the pion cloud surrounding these states. Our numerical calculations show that at low values of the momentum transfer, q2 < 1 GeV 2 , the contribution of the higher-order terms becomes significant.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Logarithmic corrections and soft photon phenomenology in the multipole model of the nucleon form factors . Abstract : We present an assessment of the logarithmic corrections to the main order predictions for the electromagnetic form factor ratios , R = F2 ( q2 ) / F1 ( q2 ) , and their dependence on the velocity transition q2 .We use the results derived within the framework of the multipole expansion method ( MEM ) . The MEM is based on the assumption that the proton can be described as a superposition of three different charge distributions with definite angular momenta .In this methods we determine the contributions come from the lowest lying resonances , i . e . , S11 ( 1650 ) , P11 ( 1710 ) , D13 ( 1520 ) , F15 ( 1680 ) , G17 ( 1880 ) , H19 ( 2230 ) , and also took into consideration the effects due to the pion bubble surrounding these states . Our numerical measurements show that at low values of the velocity transfer , q2 < 1 GeV 2 , the contribution of the higher - order terms appears substantial .",
        "rewrite_text": "In this article, we evaluate the logarithmic corrections to the leading-order predictions for the ratios of electromagnetic form factors, specifically R = F2(q²) / F1(q²), and analyze their dependence on the momentum transfer q². Our investigation is grounded in the multipole expansion method (MEM), which posits that the proton can be modeled as a combination of three distinct charge distributions, each characterized by specific angular momenta. Through this approach, we identify contributions from the lowest-lying resonances, including S11(1650), P11(1710), D13(1520), F15(1680), G17(1880), and H19(2230). Additionally, we account for the influence of the pion cloud surrounding these resonances. Our numerical results indicate that at low momentum transfer values, specifically for q² < 1 GeV², the impact of higher-order terms becomes significant. This finding suggests that logarithmic corrections play a crucial role in accurately describing the electromagnetic form factor ratios at lower energy scales. The implications of these corrections are essential for a deeper understanding of nucleon structure and the underlying dynamics of strong interactions. Overall, our study contributes to the ongoing discourse in soft photon phenomenology and enhances the theoretical framework surrounding nucleon form factors within the multipole model.",
        "ori-fast-z-score": -1.61245154965971,
        "water-fast-z-score": 2.4618298195866544,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Is diversity good? .\nAbstract:\nThe question is not new, but it has been recently revived by the publication of two papers in Nature and Science that claim to show that biodiversity loss leads to ecosystem collapse.  The authors argue that this finding should be taken seriously because ecosystems are essential for human well-being.   They also point out that there have been many previous studies showing that biodiversity loss can lead to declines in ecosystem functioning (e.g., productivity) without necessarily causing an abrupt change in state or collapse.    In this article we review these recent findings on biodiversity-ecosystem function relationships as well as some earlier results suggesting that biodiversity may sometimes enhance rather than reduce ecosystem stability.  We conclude with a discussion about how our understanding of biodiversity-ecosystem function interactions could be improved through further research. Biodiversity loss is one of humanity s greatest challenges today. It threatens the sustainability of natural resources used directly by humans such as food production systems and water supply, and indirectly via changes in climate regulation and disease transmission pathways. There is growing concern over the rate at which species extinction rates are increasing globally due to anthropogenic activities including habitat destruction, pollution, overexploitation, and invasive alien species1–3. This situation has led to calls for urgent action to conserve biological diversity4–6. However, despite widespread recognition of the importance of conserving biodiversity7–10, there remains considerable uncertainty regarding its role in maintaining ecosystem functions11–13. A number of theoretical models suggest that biodiversity loss will cause reductions in ecosystem functioning14–16. For example, Tilman et al. (1997)17 showed theoretically that reducing plant species richness would decrease primary productivity in grassland communities. Similarly, Naeem & Li (1998)18 found experimentally that removing species from soil microcosms reduced decomposition rates. These predictions were supported by numerous subsequent empirical studies19–22.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Is diversity great ? .Abstract : The question is not current , but it has been lately revived by the publication of two papers in Nature and Science that argue to find that ecosystem failure leads to biodiversity disaster . The authors argue that this finding should be taken seriously because ecosystems are essential for human well - being .They especially note out that there have been many earlier findings indicating that fauna loss can lead to declines in ecological functioning ( e . g . , output ) without necessarily creating an unexpected change in state or failure . In this page we review these recent results on biodiversity - ecological structure interactions as well as some earlier findings indicating that conservation may sometimes enhance rather than limit ecological stability .We end with a debate about how our appreciation of wildlife - ecological structure interactions might be improved through further studies . Biodiversity loss is one of humanity s worst problems currently .It damages the sustainability of natural assets used directly by humans such as feed production systems and water supply , and indirectly via alterations in climate control and illness transmission pathways . There is growing awareness over the pace at which species extinction frequencies are growing globally due to anthropogenic efforts including habitat damage , contamination , overexploitation , and invasive alien species1 – 3 .This problem has led to calls for urgent action to conserve biological diversity4 – 6 . However , despite widespread appreciation of the importance of conserving biodiversity7 – 10 , there exists considerable uncertainty regarding its function in maintaining ecosystem functions11 – 13 .A variety of theoretical theories indicate that biodiversity losing will cause reductions in ecological functioning14 – 16 . For instance , Tilman et al .( 1997 ) 17 showed theoretically that decreasing plant population richness would affect basic efficiency in prairie systems . Similarly , Naeem & Li ( 1998 ) 18 found experimentally that removing species from soil microcosms lowered oxidation rates .These predictions were endorsed by various subsequent empirical studies19 – 22 .",
        "rewrite_text": "**Title: Is Diversity Beneficial?**\n\n**Abstract:** The inquiry into the value of biodiversity has resurfaced recently, prompted by two significant publications in *Nature* and *Science* that highlight the correlation between ecosystem failure and biodiversity loss. The authors of these studies emphasize the critical role ecosystems play in supporting human well-being, arguing that the implications of their findings warrant serious consideration. They reference numerous prior studies that suggest a decline in fauna can adversely affect ecological functions, such as productivity, without necessarily resulting in a dramatic shift in ecosystem state or outright failure. This article reviews recent research on the interactions between biodiversity and ecological structure, alongside earlier findings that suggest conservation efforts may sometimes bolster rather than hinder ecological stability. \n\nThe ongoing loss of biodiversity represents one of the most pressing challenges facing humanity today. It threatens the sustainability of vital natural resources that humans rely on directly, such as food production systems and water supplies, while also impacting climate regulation and disease transmission pathways. There is an increasing recognition of the alarming rate at which species are becoming extinct globally, driven by human activities such as habitat destruction, pollution, overexploitation, and the introduction of invasive species. This crisis has sparked urgent calls for action to preserve biological diversity. \n\nDespite a widespread acknowledgment of the importance of biodiversity conservation, significant uncertainties remain regarding its role in sustaining ecosystem functions. Various theoretical frameworks suggest that biodiversity loss may lead to diminished ecological performance. For example, Tilman et al. (1997) theoretically demonstrated that a reduction in plant species richness could impair efficiency in prairie ecosystems. Similarly, Naeem and Li (1998) provided experimental evidence showing that the removal of species from soil microcosms resulted in decreased oxidation rates. These theoretical predictions have been supported by a range of empirical studies, underscoring the need for further investigation into the complex relationships between biodiversity and ecological integrity.",
        "ori-fast-z-score": -2.3301026868087695,
        "water-fast-z-score": 8.994380267950337,
        "rewrite-fast-z-score": -0.07516460280028289
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Co-orbital Oligarchy .\nAbstract:\nWe study the orbital evolution and stability properties of oligarchic co-orbitals in the Solar System, i.e., bodies with masses comparable to that of Jupiter which are trapped on orbits close to those of Neptune or Uranus for billions of years.  We show how these objects can be identified by their long-term dynamical behavior as well as by their current positions relative to Neptune s orbit. The existence of such bodies is confirmed by numerical integrations over timescales up to 10 billion years using the symplectic N-body code SyMBA. In addition we find that there exist at least two other stable regions where oligarchs may reside. These results suggest that the Solar System contains several dozen oligarchic co-orbitals:  - At least four known trans-Neptunian objects (Pluto, Charon, Haumea, Makemake) have been found to exhibit this type of dynamics; - There exists another region around 30 AU containing three additional bodies (Sedna, 2000 CR 105 , 2003 SQ 317 ); - Finally, our simulations indicate that there might also be an additional group of oligarchs located between 50-60 AU.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Co - orbital Oligarchy . Abstract : We research the orbital evolution and stability properties of oligarchic co - orbitals in the Solar System , i . e . , bodies with masses similar to that of Jupiter which are locked on orbits close to those of Neptune or Uranus for billions of years .We see how these objects can be identified by their long - term dynamical behavior as well as by their current positions relative to Neptune s orbit . The existence of such objects is discovered by numerical integrations over timescales up to 10 billion decades utilizing the symplectic N - bodies code SyMBA .In addition we find that there exist at least two other stable locations where oligarchs might live . These data suggest that the Solar System includes several hundred oligarchic co - orbitals : - At least four known trans - Neptunian objects ( Pluto , Charon , Haumea , Makemake ) have been seen to contain this form of dynamics ; - There exists another region around 30 AU holding three extra bodies ( Sedna , 2000 CR 105 , 2003 SQ 317 ) ; - Finally , our simulations confirm that there might additionally be an additional family of oligarchs located between 50 - 60 AU .",
        "rewrite_text": "In this study, we investigate the orbital dynamics and stability characteristics of oligarchic co-orbitals within the Solar System—specifically, celestial bodies with masses comparable to that of Jupiter that have maintained orbits in close proximity to those of Neptune or Uranus for billions of years. Our research focuses on identifying these objects through their long-term dynamical behaviors and their current spatial positions relative to Neptune's orbit. The existence of such co-orbital bodies has been revealed through extensive numerical simulations conducted over timescales reaching up to 10 billion years, utilizing the symplectic N-body code SyMBA. Our findings indicate that there are at least two additional stable regions where oligarchic bodies may reside. The evidence suggests that the Solar System harbors several hundred oligarchic co-orbitals. Notably, we have observed that at least four known trans-Neptunian objects—Pluto, Charon, Haumea, and Makemake—exhibit this oligarchic dynamical behavior. Furthermore, we have identified another region around 30 AU that contains three additional bodies: Sedna, 2000 CR 105, and 2003 SQ 317. Our simulations also imply the potential existence of another family of oligarchs located between 50 and 60 AU. These results contribute to our understanding of the complex dynamical landscape of the Solar System and highlight the significance of oligarchic co-orbitals in its evolution.",
        "ori-fast-z-score": -1.5215349135496974,
        "water-fast-z-score": 4.478342947514801,
        "rewrite-fast-z-score": -1.462614271203831
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Production of a sterile species: quantum kinetics .\nAbstract:\nWe study the production of a sterile species in an open system with two stable particles and one unstable particle, where the decay products are not observed.  We show that if the initial state is pure then there exists no final mixed state which can be reached by unitary evolution. This result implies that the production of a sterility cannot occur under any circumstances for such systems. If we allow the possibility to prepare arbitrary states as input, however, it turns out that the production of a certain kind of sterility may still take place. In this case, the output state will always contain some amount of entanglement between the subsystems corresponding to the different types of particles involved. The results presented here have been obtained within the framework of Quantum Kinetic Theory (QKT). QKT provides a description of non-equilibrium phenomena at mesoscopic scales based on the concept of entropy production rate. It has recently attracted considerable attention due to its potential applications in many areas ranging from physics to biology. \nI. INTRODUCTORY REMARK\nThe phenomenon of spontaneous emission plays a crucial role in modern physics. For example, it is responsible for the cooling process in laser-cooling experiments  1  . On the other hand, spontaneous emission also leads to decoherence effects  2  , which limit the performance of quantum information processing devices  3  .\nIn recent years, several authors  4  -  8  studied the problem of producing a particular type of  sterility  in open quantum systems. A state is called  sterile  when it does not interact with itself or another given set of states  9  . More specifically, let us consider a bipartite Hilbert space H = H 1 ⊗H 2 , where dim(H i ) = N i . Then, a density matrix ρ ∈ B(H) is said to be  sterile  wrt. a subset S ⊆ H iff Tr ρσ  = 0 for all σ ∈ S. Here, Tr denotes the trace operation over either H 1 or H 2 depending on whether σ belongs to H 1 or H 2 respectively. Note that the notion of  ster",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Production of a sterile species : quantum kinetics . Abstract : We research the production of a sterile species in an open system with two stable particles and one unstable particle , where the decay products are not observed .We see that if the first system is pure then there exists no final mixed system which can be reached by unitary evolution . This result means that the production of a sterility cannot occur under any situations for such systems .If we allow the option to develop arbitrary states as input , however , it turns out that the production of a certain sort of sterility may always hold place . In this instance , the output state will always contain some degree of entanglement between the subsystems corresponding to the different kinds of molecules concerned .The results presented here have been achieved within the framework of Quantum Kinetic Theory ( QKT ) . QKT provides a description of non - equilibrium phenomena at mesoscopic scales based on the idea of entropy production rate .It has recently attracted considerable scrutiny due to its potential applications in multiple fields ranging from biology to biology . I .INTRODUCTORY REMARK The phenomenon of spontaneous emission takes a crucial role in modern physics . For instance , it is responsible for the freezing process in laser - cooling experiments 1 .On the other hand , spontaneous emission additionally results to decoherence effects 2 , which reduce the performance of quantum information processing equipment 3 . In recent years , various scientists 4 - 8 studied the question of creating a certain type of sterility in open quantum systems .A state is dubbed sterile when it does not interact with itself or another particular set of states 9 . More specifically , let us consider a bipartite Hilbert field H = H 1 [UNK] 2 , where dim ( H i ) = N i .Then , a density matrix ρ ∈ B ( H ) is said to be sterile wrt . a subset S ⊆ H iff Tr ρσ = 0 for all σ ∈ S . Here , Tr denotes the trace operation over either H 1 or H 2 depending on whether ρ corresponds to H 1 or H 2 respectively .Note that the notion of  ster",
        "rewrite_text": "**Title: Production of a Sterile Species: Quantum Kinetics**\n\n**Abstract:** This study investigates the production of a sterile species within an open quantum system characterized by two stable particles and one unstable particle, where the decay products remain unobserved. Our findings indicate that if the initial system is in a pure state, it is impossible to achieve a final mixed state through unitary evolution. This implies that the generation of sterility is unattainable under such conditions. However, when we permit the introduction of arbitrary input states, we discover that the emergence of a specific type of sterility becomes feasible. In this scenario, the resulting output state consistently exhibits a degree of entanglement among the subsystems associated with the various particle types involved. The results presented in this paper are derived from the principles of Quantum Kinetic Theory (QKT), which offers a framework for understanding non-equilibrium phenomena at mesoscopic scales, emphasizing the concept of entropy production rate. QKT has garnered significant attention recently due to its wide-ranging applications across diverse fields, including biology and materials science. \n\nIn the introductory remarks, we highlight the pivotal role of spontaneous emission in contemporary physics, noting its critical involvement in processes such as laser cooling, which can lead to decoherence effects that hinder the efficiency of quantum information processing systems. Recent research has focused on the creation of sterility in open quantum systems, where a state is classified as sterile if it does not interact with itself or a designated set of states. Specifically, we consider a bipartite Hilbert space \\( H = H_1 \\otimes H_2 \\), where the dimensions of \\( H_i \\) are denoted as \\( N_i \\). A density matrix \\( \\rho \\in B(H) \\) is defined as sterile with respect to a subset \\( S \\subseteq H \\) if the trace \\( \\text{Tr}(\\rho \\sigma) = 0 \\) for all \\( \\sigma \\in S \\). This exploration into the nature of sterility in quantum systems opens new avenues for understanding quantum interactions and their implications for future technological advancements.",
        "ori-fast-z-score": 0.8778955729143844,
        "water-fast-z-score": 7.6373499280870085,
        "rewrite-fast-z-score": 1.5888598190134724
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The discovery of a massive supercluster at z=0.9 in the UKIDSS DXS .\nAbstract:\nWe report on the detection and characterization of a massive galaxy cluster, Abell S1063 (z = 0.90), using data obtained with the Wide Field Camera 3 (WFC3) aboard Hubble Space Telescope (HST). The cluster was discovered as part of an ongoing survey for distant clusters carried out by our team within the framework of the UKIRT Infrared Deep Sky Survey Data Release 8 (UKIDSS DR8). We use photometric redshifts to select galaxies that are likely members of this structure over a large area around its center. Using these candidates we identify two brightest cluster galaxies (BCGs) separated by about 1 arcmin along the line-of-sight. These BCGs have magnitudes mF160B = 20.6 ± 0.1 mag and mF140W = 21.0 ± 0.2 mag respectively. Their colors suggest they are both early-type galaxies.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The observation of a huge supercluster at z = 0 . 9 in the UKIDSS DXS . Abstract : We report on the discovery and characterization of a huge galaxy cluster , Abell S1063 ( z = 0 . 90 ) , using data acquired with the Wide Field Camera 3 ( WFC3 ) aboard Hubble Space Telescope ( HST ) .The cluster was discovered as part of an ongoing search for distant galaxies carried out by our team within the framework of the UKIRT Infrared Deep Sky Survey Data Release 8 ( UKIDSS DR8 ) . We use photometric redshifts to select clusters that are likely elements of this formation over a large area around its core .Using these candidates we identify two brightest cluster clusters ( BCGs ) separated by about 1 arcmin along the line - of - view . These BCGs have magnitudes mF160B = 20 . 6 ± 0 . 1 mag and mF140W = 21 . 0 ± 0 . 2 mag respectively .Their colors indicate they are both earliest - class galaxies .",
        "rewrite_text": "We present the discovery and detailed analysis of a significant galaxy cluster, designated Abell S1063, located at a redshift of z = 0.90. This finding is based on observations obtained with the Wide Field Camera 3 (WFC3) on the Hubble Space Telescope (HST). Our research is part of a broader initiative aimed at identifying distant galaxies, conducted within the framework of the UKIRT Infrared Deep Sky Survey Data Release 8 (UKIDSS DR8). By employing photometric redshift techniques, we have successfully selected clusters that are likely constituents of this extensive formation, covering a substantial area surrounding the cluster's core. Among the identified candidates, we pinpointed two brightest cluster galaxies (BCGs) that are positioned approximately 1 arcminute apart along the line of sight. The BCGs exhibit magnitudes of mF160B = 20.6 ± 0.1 mag and mF140W = 21.0 ± 0.2 mag, respectively. Their color profiles suggest that both galaxies belong to the earliest class of galaxies, indicating their significant role in the evolution of the cluster. This discovery not only enhances our understanding of galaxy formation and clustering at high redshifts but also provides valuable insights into the large-scale structure of the universe during this epoch. The implications of our findings contribute to the ongoing discourse on the nature of galaxy clusters and their evolution over cosmic time.",
        "ori-fast-z-score": -1.386750490563073,
        "water-fast-z-score": 3.5381518506868126,
        "rewrite-fast-z-score": 0.6396021490668313
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Bayesian analysis of the low-resolution polarized 3-year WMAP sky maps .\nAbstract:\nWe present an update on our previous work (Jarosik et al., 2006) where we used Bayesian methods to analyze the polarization data obtained by Wilkinson Microwave Anisotropy Probe (WMAP). We use the same likelihood function as in Jarosik et al. (2006), but now include also information about the noise covariance matrix and its uncertainty, which was not available at that time. The main goal is to test whether the observed large-scale temperature-polarization correlation can be explained within the standard cosmological model with adiabatic initial conditions and Gaussian primordial fluctuations or it requires some additional physics beyond this simple picture. In particular, we are interested in testing predictions made recently by several authors who suggested that such correlations could arise due to gravitational lensing effects induced by large scale structure along the line-of-sight. Our results show no significant evidence for any deviations from the simplest theoretical models considered here.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Bayesian analysis of the small - resolution polarized 3 - year WMAP sky mapping . Abstract : We present an update on our previous research ( Jarosik et al . , 2006 ) where we using Bayesian methods to analyze the polarization evidence derived by Wilkinson Microwave Anisotropy Probe ( WMAP ) .We use the same likelihood function as in Jarosik et al . ( 2006 ) , but now include also knowledge about the noise covariance matrix and its uncertainty , which was not accessible at that point .The main goal is to test whether the observed large - scale temperature - polarization correlation can be described within the standard cosmological model with adiabatic initial conditions and Gaussian primordial fluctuations or it costs some additional physics beyond this straightforward picture . In particular , we are concerned in proving predictions provided recently by various scientists who proposed that such correlations might arise due to gravitational lensing effects induced by large scale structure along the line - of - view .Our results show no considerable evidence for any deviations from the simplest theoretical theories treated here .",
        "rewrite_text": "In this article, we provide an updated analysis of the polarization data obtained from the Wilkinson Microwave Anisotropy Probe (WMAP), building upon our previous work (Jarosik et al., 2006). Utilizing Bayesian statistical methods, we re-examine the polarization evidence while incorporating new insights regarding the noise covariance matrix and its associated uncertainties, which were not available in our earlier study. Our primary objective is to evaluate whether the observed large-scale correlations between temperature and polarization can be adequately explained within the framework of the standard cosmological model, which assumes adiabatic initial conditions and Gaussian primordial fluctuations. Alternatively, we investigate whether these correlations necessitate additional physical explanations beyond this conventional model. A particular focus of our research is to assess recent hypotheses suggesting that such correlations may be influenced by gravitational lensing effects caused by large-scale structures along the line of sight. Our findings indicate that there is no significant evidence to support deviations from the simplest theoretical models considered in this analysis. This suggests that the observed temperature-polarization correlations can be accounted for by the standard cosmological framework without invoking additional complexities. Overall, our study reinforces the robustness of the standard cosmological model in explaining the polarization data from WMAP, while also highlighting the importance of incorporating noise characteristics in Bayesian analyses to enhance the reliability of cosmological interpretations.",
        "ori-fast-z-score": -1.0,
        "water-fast-z-score": 4.638124095143555,
        "rewrite-fast-z-score": 0.10259783520851541
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Unitary Representations of Wavelet Groups and Encoding of Iterated Function Systems in Solenoids .\nAbstract:\nWe study the encoding problem for iterated function systems (IFS) on solenoids by unitary representations of wavelet groups, which are infinite-dimensional Lie groups generated by affine transformations of the real line. We show that if an IFS satisfies certain conditions then it can be encoded into a unitary representation of its associated wavelet group. This result is applied to prove that every self-similar fractal set with finite local complexity has a unique invariant measure up to scaling factors. The proof relies on the fact that any such fractal set can be approximated by a sequence of compact sets whose boundaries have zero Lebesgue measures. In particular, we obtain a new characterization of self-affine carpets as those fractals satisfying this condition. Finally, we give examples showing how our results apply to some well-known fractals including Sierpinski gasket, Koch snowflake, von Koch curve, and Menger sponge.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Unitary Representations of Wavelet Groups and Encoding of Iterated Function Systems in Solenoids . Abstract : We research the encoding problem for iterated function schemes ( IFS ) on solenoids by unitary representations of wavelet groups , which are infinite - dimensional Lie groups produced by affine transformations of the real line .We see that if an IFS satisfies certain conditions then it can be interpreted into a unitary representation of its identified wavelet group . This result is applied to prove that every self - similar fractal set with finite local complexity has a unique invariant measure up to scaling factors .The proof relies on the fact that any such fractal set can be approximated by a sequence of compact sets whose limits have zero Lebesgue measures . In particular , we obtain a new definition of self - affine carpets as those fractals satisfying this condition .Finally , we give instance demonstrating how our findings apply to some well - famous fractals including Sierpinski gasket , Koch snowflake , von Koch curve , and Menger sponge .",
        "rewrite_text": "In this article, we investigate the encoding problem associated with iterated function systems (IFS) on solenoids through the lens of unitary representations of wavelet groups. These wavelet groups are characterized as infinite-dimensional Lie groups generated by affine transformations of the real line. Our research reveals that under specific conditions, an IFS can be effectively represented as a unitary representation of its corresponding wavelet group. This significant finding allows us to establish that every self-similar fractal set exhibiting finite local complexity possesses a unique invariant measure, modulo scaling factors. The proof of this assertion is grounded in the observation that such fractal sets can be approximated by a sequence of compact sets, with the limits of these sequences yielding zero Lebesgue measures. Notably, we introduce a novel definition of self-affine carpets, identifying them as fractals that meet this criterion. To illustrate the practical implications of our results, we provide examples that demonstrate how our theoretical framework applies to several well-known fractals, including the Sierpinski gasket, Koch snowflake, von Koch curve, and Menger sponge. Through this work, we contribute to a deeper understanding of the relationship between wavelet groups and fractal geometry, paving the way for further exploration of the encoding mechanisms inherent in complex dynamical systems.",
        "ori-fast-z-score": 0.601929265428846,
        "water-fast-z-score": 4.00693842672377,
        "rewrite-fast-z-score": 2.3597502097958545
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dynamic Effects Increasing Network Vulnerability to Cascading Failures .\nAbstract:\nWe study the vulnerability of interdependent networks under cascading failures, where nodes are subject to random attacks and their neighbors can fail as well due to loss of connectivity. We show that dynamic effects increase network vulnerability by enhancing the spreading of failures in time. In particular, we find that increasing the number of attackers or decreasing the recovery rate increases the probability for global cascade failure. Our results suggest that it is important to consider both static and dynamic aspects when studying the robustness of real-world systems against cascading failures. Interdependence between different components of complex systems has been shown to be crucial for understanding many phenomena such as epidemic outbreaks  1  , traffic jams  2  , financial crashes  3  , and blackouts  4  . The recent 2008 power grid crisis caused by an unprecedented series of cascading failures  5  highlighted the importance of considering interdependence among system elements  6  .\nIn this work, we focus on interdependent networks  7, 8  , which consist of two types of nodes: source (S) and target (T). Source nodes provide services to other nodes while target nodes depend on these services. For example, in the case of the power grid, generators supply electricity to substations; if one generator fails then its neighboring substations will also lose power  9  . Similarly, in social networks people may rely on each other s opinions  10  ; if someone becomes ill  11  or loses her job  12  she might affect others  health status  13  or income  14  respectively. Recent studies have shown that interdependency plays an important role in determining the resilience of interconnected systems  15, 16  . However, most previous works focused only on static properties  17  , i.e., they assumed that all links remain stable over time  18  . This assumption does not hold true in practice since links often break down  19  and new ones form  20  . Therefore, it is necessary to take into account the dynamics of interactions  21  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dynamic Effects Increasing Network Vulnerability to Cascading Failures . Abstract : We research the vulnerability of interdependent networks under cascading crashes , where nodes are subject to random attacks and their relatives can fail as well due to lack of communication .We see that dynamic effects improve network vulnerability by increased the spreading of failures in time . In particular , we find that increasing the number of attackers or decreasing the recovery rate raises the probability for global cascade failure .Our results show that it is important to consider both static and dynamic elements when examining the robustness of real - global networks against cascading crashes . Interdependence between various components of complex systems has been shown to be crucial for studying many phenomena such as plague outbreaks 1 , road jams 2 , financial crashes 3 , and blackouts 4 .The recent 2008 power system disaster caused by an unprecedented series of cascading problems 5 indicated the importance of considering interdependence among system components 6 . In this project , we focus on interdependent networks 7 , 8 , which consist of two kind of nodes : source ( S ) and goal ( T ) .Source networks provide operations to other nodes while target nodes depend on these services . For instance , in the case of the power system , generators supply energy to substations ; if one generator fails then its adjacent substations will also lose power 9 .Similarly , in social organizations everyone would rely on each other s views 10 ; if someone causes illness 11 or losing her employment 12 she might impact people health status 13 or income 14 respectively . Recent research have shown that interdependency takes an important role in measuring the resilience of interconnected networks 15 , 16 .However , most prior papers focused only on static properties 17 , i . e . , they expected that all links remain stable over time 18 . This assumption does not stand true in practice since links sometimes break down 19 and new ones create 20 .Therefore , it is required to take into consideration the dynamics of relationships 21 .",
        "rewrite_text": "**Title:** Dynamic Effects Increasing Network Vulnerability to Cascading Failures\n\n**Abstract:** This study investigates the susceptibility of interdependent networks to cascading failures, particularly in scenarios where nodes experience random attacks, leading to potential failures in connected nodes due to disrupted communication. Our findings indicate that dynamic factors significantly enhance network vulnerability by facilitating the temporal spread of failures. Specifically, we demonstrate that an increase in the number of attackers or a reduction in the recovery rate heightens the likelihood of a global cascade failure. This research underscores the necessity of incorporating both static and dynamic components when assessing the resilience of real-world networks against cascading failures. The interdependence among various elements of complex systems is critical for understanding a range of phenomena, including epidemic outbreaks, traffic congestion, financial crises, and power outages. The catastrophic power system failure in 2008, which stemmed from a series of cascading issues, highlighted the importance of recognizing the interconnections among system components. Our focus is on interdependent networks, which comprise two types of nodes: source nodes (S) that provide essential services and target nodes (T) that rely on these services. For example, in a power grid, generators (source nodes) supply electricity to substations (target nodes); a failure in one generator can lead to power loss in adjacent substations. Similarly, in social networks, individuals depend on each other's perspectives; if one person faces health issues or job loss, it can adversely affect the well-being or income of others. Recent studies have emphasized the pivotal role of interdependency in evaluating the resilience of interconnected networks. However, most existing literature has predominantly concentrated on static properties, assuming that all connections remain constant over time. This assumption is often unrealistic, as links can deteriorate or new connections can emerge. Therefore, it is essential to account for the dynamic nature of these relationships to gain a comprehensive understanding of network vulnerabilities.",
        "ori-fast-z-score": -0.15713484026367722,
        "water-fast-z-score": 9.108437646689818,
        "rewrite-fast-z-score": 0.48666426339228763
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Multiwavelength Study of X-ray Selected Star Forming Galaxies within the Chandra Deep Field South .\nAbstract:\nWe present multiwavelength observations for a sample of X-ray selected star forming galaxies (SFGs) in the Chandra Deep Field-South (CDF-S). The sample consists of 16 SFGs with spectroscopic redshifts between 1 and 3, which were detected by both the soft-band (0.5-2 keV) and hard-band (2-8 keV) surveys conducted by the Advanced CCD Imaging Spectrometer on board XMM-Newton. We have obtained optical spectroscopy using the Keck telescope to measure their stellar masses and SFRs as well as near-infrared photometry taken with the Infrared Array Camera aboard Spitzer Space Telescope to estimate dust extinction. Our results show that these SFGs are massive systems with M* = 1013 -1014M⊙ at z ~ 2 -3. They also exhibit high specific star-formation rates ranging from 10^(-3) yr-1 to 10^(1) yr-1, indicating intense ongoing star formation activity.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Multiwavelength Study of X - ray Selected Star Forming Galaxies within the Chandra Deep Field South . Abstract : We report multiwavelength studies for a sample of X - ray selected star producing galaxies ( SFGs ) in the Chandra Deep Field - South ( CDF - S ) .The sample consists of 16 SFGs with spectroscopic redshifts between 1 and 3 , which were detected by both the hard - band ( 0 . 5 - 2 keV ) and hard - band ( 2 - 8 keV ) observations conducted by the Advanced CCD Imaging Spectrometer on board XMM - Newton . We have achieved optical spectroscopy utilizing the Keck telescope to measure their stellar masses and SFRs as also as near - infrared photometry made with the Infrared Array Camera aboard Spitzer Space Telescope to estimate dust extinction .Our results show that these SFGs are vast systems with M * = 1013 - [UNK] at z ~ 2 - 3 . They even exhibit strong specific star - formation rates ranging from 10 ^ ( - 3 ) yr - 1 to 10 ^ ( 1 ) yr - 1 , indicating intense ongoing star formation activity .",
        "rewrite_text": "We present a comprehensive multiwavelength analysis of a sample of X-ray selected star-forming galaxies (SFGs) located in the Chandra Deep Field South (CDF-S). Our study focuses on 16 SFGs, all of which have spectroscopic redshifts ranging from 1 to 3. These galaxies were identified through observations in both the hard X-ray band (0.5 - 2 keV) and the harder band (2 - 8 keV) using the Advanced CCD Imaging Spectrometer aboard the XMM-Newton satellite. To further investigate their properties, we conducted optical spectroscopy with the Keck telescope, allowing us to derive their stellar masses and star formation rates (SFRs). Additionally, we utilized near-infrared photometry from the Infrared Array Camera on the Spitzer Space Telescope to assess dust extinction levels in these galaxies.\n\nOur findings reveal that these SFGs are substantial systems, with stellar masses estimated at M* = 10^13 M☉ at redshifts around 2 to 3. Notably, they exhibit remarkably high specific star formation rates, ranging from 10^(-3) yr^(-1) to 10^(1) yr^(-1), which underscores the vigorous star formation activity occurring within these galaxies. This study contributes to our understanding of the evolution of star-forming galaxies in the early universe, highlighting the significant role that X-ray observations play in identifying and characterizing these dynamic systems. The combination of multiwavelength data allows us to paint a more detailed picture of the physical processes driving star formation in these distant galaxies, providing valuable insights into their formation and evolution during a critical period in cosmic history.",
        "ori-fast-z-score": -0.9113223768657671,
        "water-fast-z-score": 2.840187787218772,
        "rewrite-fast-z-score": 0.19611613513818404
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Origin of the Galaxy Mass-Metallicity Relation and Implications for Galactic Outflows .\nAbstract:\nWe present an analysis of the mass-metallicity relation (MMR) in galaxies, using data from the Sloan Digital Sky Survey Data Release 7 (SDSS DR7). We use two different methods to estimate galaxy masses -the stellar-mass-to-light ratio method and the dynamical mass method-which are both calibrated against direct measurements of galaxy masses obtained by gravitational lensing. The MMR is found to be well described by a power law with index -0.25 ± 0.01 dex/log(M*), where M* denotes the total stellar mass within one effective radius Re. This slope agrees very well with previous studies based on smaller samples or less accurate estimates of galaxy masses. However, we find that our results differ significantly from those reported recently by Tremonti et al. (2004) , who claim a shallower slope of -0.16 ± 0.02 dex/log(M*) (a factor of 2 difference at high masses).\nThe origin of this discrepancy can be traced back to differences between the sample selection criteria used in these works. In particular, Tremonti et al. select their sample according to the presence/absence of emission lines in the SDSS spectra, which introduces significant biases into the resulting distribution of metallicities. Our results show no evidence for such biases: when restricting ourselves to only star-forming galaxies, as done by Tremonti et",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Origin of the Galaxy Mass - Metallicity Relation and Implications for Galactic Outflows . Abstract : We present an assessment of the mass - metallicity relation ( MMR ) in galaxies , using data from the Sloan Digital Sky Survey Data Release 7 ( SDSS DR7 ) .We use two different methods to estimate galaxy masses - the stellar - mass - to - light ratio technique and the dynamical mass method - which are both calibrated against direct measurements of galaxy masses obtained by gravitational lensing . The MMR is found to be well described by a power law with index - 0 . 25 ± 0 . 01 dex / log ( M * ) , where M * denotes the total stellar mass within one effective radius Re .This slope agrees very best with previous investigations based on smaller specimens or smaller accurate calculations of galaxy ages . However , we find that our findings differ significantly from those published recently by Tremonti et al .( 2004 ) , who estimate a shallower curve of - 0 . 16 ± 0 . 02 dex / log ( M * ) ( a factor of 2 difference at high masses ) . The origin of this discrepancy can be traced back to differences between the sample selection criteria applied in these works .In particular , Tremonti et al . select their sample according to the presence / lack of emission lines in the SDSS spectra , which puts significant biases into the resulting distribution of metallicities .Our results show no evidence for such biases : when restricting ourselves to only galaxy - creating objects , as done by Tremonti et",
        "rewrite_text": "We provide a comprehensive analysis of the mass-metallicity relation (MMR) in galaxies, utilizing data from the Sloan Digital Sky Survey Data Release 7 (SDSS DR7). Our study employs two distinct methodologies for estimating galaxy masses: the stellar-mass-to-light ratio technique and the dynamical mass method. Both approaches are calibrated against direct measurements of galaxy masses obtained through gravitational lensing, ensuring accuracy in our results. Our findings reveal that the MMR can be effectively characterized by a power law with an index of -0.25 ± 0.01 dex/log(M*), where M* represents the total stellar mass within one effective radius (Re). This slope aligns closely with previous studies that utilized smaller samples or less precise calculations of galaxy ages. However, we observe a notable divergence from the results reported by Tremonti et al. (2004), who estimated a shallower slope of -0.16 ± 0.02 dex/log(M*), indicating a significant discrepancy of a factor of two at higher masses. We attribute this difference to variations in sample selection criteria between the studies. Specifically, Tremonti et al. based their sample on the presence or absence of emission lines in the SDSS spectra, which introduces substantial biases in the resulting metallicity distribution. In contrast, our analysis shows no evidence of such biases; when we focus exclusively on galaxy-forming objects, as Tremonti et al. did, our results remain consistent. This research not only clarifies the MMR but also has implications for understanding galactic outflows and their role in the evolution of galaxies. By addressing the discrepancies in previous studies, we contribute to a more nuanced understanding of the relationship between galaxy mass and metallicity, which is crucial for unraveling the processes that govern galaxy formation and evolution.",
        "ori-fast-z-score": 0.8783100656536799,
        "water-fast-z-score": 5.172270386627226,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Magnetorotational Collapse of Population III Stars .\nAbstract:\nWe present the results of our numerical simulations of magnetorotational collapse (MRC) in primordial stars with initial masses between 100 and 1000 M⊙, which are formed at redshifts z = 20 − 30. We find that for all models considered here, magnetic fields play an important role during the formation process of black holes. The final mass of the central object is determined by the strength of the magnetic field. For weak magnetic fields (B < 10^10 G), we obtain stellar-mass black holes; while for stronger fields (B > 10^{10}G), supermassive black holes form. In addition to this effect on the final mass, magnetic fields also affect the angular momentum distribution inside the collapsing star. This leads to different spin parameters of the resulting black hole depending on its progenitor s initial mass. \n \n Keywords: Black Hole, Primordial Star Formation, Magnetohydrodynamics",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Magnetorotational Collapse of Population III Stars . Abstract : We report the conclusion of our numerical simulations of magnetorotational collapse ( MRC ) in primordial stars with initial masses between 100 and 1000 [UNK] , which are formed at redshifts z = 20 − 30 .We see that for all models discussed here , magnetic fields take an important role during the formation period of black holes . The final mass of the main object is chosen by the strength of the magnetic force .For weak magnetic fields ( B < 10 ^ 10 G ) , we obtain stellar - mass black holes ; while for heavier fields ( B > 10 ^ { 10 } G ) , supermassive black holes create . In addition to this effect on the finished mass , magnetic fields still affect the angular velocity distribution inside the falling star .This leads to different spinning variables of the resulting black hole depending on its progenitor s initial mass . Keywords : Black Hole , Primordial Star Formation , Magnetohydrodynamics",
        "rewrite_text": "We present the findings from our comprehensive numerical simulations investigating the magnetorotational collapse (MRC) of Population III stars, which possess initial masses ranging from 100 to 1000 solar masses and formed at redshifts between z = 20 and 30. Our study reveals that magnetic fields play a crucial role during the black hole formation process across all models analyzed. The strength of the magnetic field significantly influences the final mass of the resulting black hole. Specifically, in scenarios with weak magnetic fields (B < 10^10 G), we observe the formation of stellar-mass black holes. Conversely, in environments with stronger magnetic fields (B > 10^10 G), the simulations indicate the creation of supermassive black holes. Beyond their impact on the final mass, magnetic fields also influence the distribution of angular velocity within the collapsing star. This variation results in differing spin characteristics of the resultant black holes, which are contingent upon the initial mass of their progenitor stars. Our findings underscore the importance of magnetic forces in the early universe's black hole formation processes and provide insights into the diverse outcomes of primordial star evolution. This research contributes to the broader understanding of black hole formation mechanisms and the role of magnetohydrodynamics in astrophysical phenomena. \n\nKeywords: Black Hole, Primordial Star Formation, Magnetohydrodynamics.",
        "ori-fast-z-score": -0.3511234415883917,
        "water-fast-z-score": 3.719924439802217,
        "rewrite-fast-z-score": -0.8700628401410971
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Retaining Black Holes with Very Large Recoil Velocities .\nAbstract:\nWe study the possibility that black holes can be retained in binary systems by gravitational radiation reaction even if their initial velocities are very large, and we find that this is possible for certain ranges of parameters.  We consider two types of binaries:  those consisting of one black hole and another compact object (such as neutron star or white dwarf), and those consisting of two black holes.   In both cases, we show how to calculate the final velocity after emission of gravitational waves using post-Newtonian approximations up to third order.  For binaries containing at least one black hole, we also use numerical relativity simulations to check our results.  Finally, we discuss some astrophysical implications of these findings. The discovery of gravitational waves has opened an exciting new window on the universe  1  . One of its most surprising features was the detection of merging black holes  2  , which were found to have masses ranging between about 4M☉ and 36M☉  3  . This raises the question whether there exist other ways besides mergers through which black holes may form  4  .\nIn particular, it would be interesting to know what happens when a black hole moves into a binary system composed of either another black hole or a nonblack-hole companion  5  . If the black hole s initial speed is too high, then it will escape the system before emitting enough energy via gravitational waves  6  . However, if the black hole starts out slowly but still faster than the orbital speed of the binary components, then it could potentially be captured  7, 8  .  Here, we investigate this scenario further and determine under which conditions such capture is indeed possible.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Retaining Black Holes with Very Large Recoil Velocities . Abstract : We research the prospect that black holes can be retained in binary systems by gravitational wave reaction even if their early velocities are very huge , and we find that this is possible for particular ranges of constraints .We consider two forms of binaries : those consisting of one dark hole and another compact particle ( such as neutron star or black dwarf ) , and those consisting of two black holes . In both cases , we show how to estimate the last speed after absorption of gravitational waves using post - Newtonian approximations up to third order .For binaries bearing at least one dark hole , we also apply numerical relativity simulations to test our findings . Finally , we explain some astrophysical consequences of these conclusions .The observation of gravitational waves has opened an exciting new window on the universe 1 . One of its most surprising properties was the observation of fused black holes 2 , which were found to have masses ranging between about 4M☉ and 36M☉ 3 .This opens the question whether there exist other ways besides mergers through which black holes might form 4 . In particular , it would be interesting to say what happens when a black hole moves into a binary system composed of either another dark hole or a nonblack - hole companion 5 .If the dark hole s initial speed is too high , then it will flee the system before emitting sufficient energy via gravitational waves 6 . However , if the dark hole starts out slowly but still faster than the orbital speed of the binary components , then it could potentially be captured 7 , 8 .Here , we investigate this situation further and establish under which conditions such capture is indeed possible .",
        "rewrite_text": "In this study, we explore the intriguing possibility of retaining black holes within binary systems, even when they possess significantly high initial velocities, through the mechanism of gravitational wave interactions. Our findings indicate that retention is feasible under specific constraints. We examine two types of binary configurations: one comprising a black hole and a compact object, such as a neutron star or a black dwarf, and the other consisting of two black holes. For both configurations, we develop a method to estimate the final velocity after the absorption of gravitational waves, utilizing post-Newtonian approximations up to the third order. Additionally, for binaries that include at least one black hole, we employ numerical relativity simulations to validate our theoretical predictions.\n\nThe advent of gravitational wave detection has revolutionized our understanding of the universe, revealing unexpected phenomena such as the merger of black holes with masses ranging from approximately 4M☉ to 36M☉. This discovery prompts further inquiry into alternative formation mechanisms for black holes beyond traditional mergers. A particularly compelling scenario arises when a black hole enters a binary system that includes either another black hole or a non-black hole companion. If the incoming black hole's initial velocity exceeds a critical threshold, it risks escaping the system before it can emit enough energy through gravitational waves to facilitate its capture. Conversely, if the black hole begins with a lower velocity, albeit still greater than the orbital speed of the binary components, there exists a potential for it to be captured.\n\nIn this paper, we delve deeper into these dynamics, delineating the conditions under which the capture of a high-velocity black hole into a binary system is achievable. Our results not only enhance the theoretical framework surrounding black hole interactions but also have significant implications for our understanding of binary evolution and the broader astrophysical landscape.",
        "ori-fast-z-score": -1.2815364865751413,
        "water-fast-z-score": 5.788544409159491,
        "rewrite-fast-z-score": 0.07881104062391006
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Forced accretion in stochastically fed AGN and quasars .\nAbstract:\nWe present the results of cosmological simulations that follow the growth of supermassive black holes (SMBHs) by stochastic gas inflow, including radiative feedback effects on their surroundings. We find that SMBHs grow primarily through mergers with other BHs rather than gas accretion at high redshifts z > 6. At lower redshift we observe an increase in the fraction of mass gained via gas accretion relative to merger events. The resulting luminosity function is consistent with observations for both active galactic nuclei (AGNs) and quasars out to z = 7.5. Our model predicts a large number of low-luminosity AGNs which are not observed yet but may be detectable with future surveys such as LSST or Euclid. In addition, our model produces a population of obscured quasars whose properties agree well with recent observational constraints. Finally, we show that the predicted quasar lifetime distribution agrees very well with current estimates based on SDSS data.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Forced accretion in stochastically driven AGN and quasars . Abstract : We report the results of cosmological simulations that track the development of supermassive black holes ( SMBHs ) by stochastic gas inflow , particularly radiative feedback effects on their environment .We see that SMBHs grow primarily through mergers with other BHs rather than gas accretion at high redshifts z > 6 . At lower redshift we study an increase in the fraction of mass gained via gas accretion compared to merger events .The resulting luminosity function is compatible with observations for both active galactic nuclei ( AGNs ) and quasars out to z = 7 . 5 . Our model predicts a large number of low - luminosity AGNs which are not observed yet but might be detectable with current surveys such as LSST or Euclid .In addition , our model generates a population of distorted quasars whose characteristics comply better with recent observational restrictions . Finally , we prove that the expected quasar lifetime distribution agrees very best with current estimates based on SDSS information .",
        "rewrite_text": "We present findings from our cosmological simulations that investigate the evolution of supermassive black holes (SMBHs) influenced by stochastic gas inflow and the associated radiative feedback on their surroundings. Our analysis reveals that at high redshifts (z > 6), the growth of SMBHs is predominantly driven by mergers with other black holes rather than by gas accretion. As we examine lower redshift scenarios, we observe a notable increase in the proportion of mass acquired through gas accretion relative to merger events. The luminosity function derived from our simulations aligns well with observational data for both active galactic nuclei (AGNs) and quasars up to z = 7.5. Furthermore, our model forecasts a significant population of low-luminosity AGNs that have yet to be detected, which could potentially be identified in ongoing surveys such as the Large Synoptic Survey Telescope (LSST) and the Euclid mission. Additionally, we find that our simulations produce a set of distorted quasars whose properties are more consistent with recent observational constraints. Lastly, we demonstrate that the predicted distribution of quasar lifetimes closely matches current estimates derived from Sloan Digital Sky Survey (SDSS) data. These results enhance our understanding of the mechanisms driving SMBH growth and the implications for the evolution of the universe's structure.",
        "ori-fast-z-score": 0.47140452079103173,
        "water-fast-z-score": 5.500933918218137,
        "rewrite-fast-z-score": 0.44172610429938614
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Neutrino Decays and Neutrino Electron Elastic Scattering in Unparticle Physics .\nAbstract:\nWe study the decay modes of neutrinos into unparticles, as well as their elastic scattering off electrons mediated by virtual unparticles. We find that these processes are strongly suppressed for small values of the coupling constant between ordinary particles and unparticles. For larger values of this coupling we obtain branching ratios which can be tested at future experiments such as T2K or NOVA. The results presented here may also have implications on other models with extra dimensions beyond those considered so far. \n \n Introduction \n \n In recent years there has been an increasing interest in theories where new physics is described by fields whose excitations do not carry standard model (SM) charges  1  . These so-called  unparticles   2  , if they exist, could manifest themselves through various experimental signatures  3  .\n \nIn particular, it was shown  4  that decays of SM particles to pairs of unparticles would lead to deviations from the expected exponential behavior of the corresponding lifetimes. This effect should be observable experimentally  5  . Furthermore, it was suggested  6  that unparticles might play a role in explaining some puzzling features observed recently in cosmic ray data  7, 8  . \n \n Another interesting possibility is that unparticles couple directly to SM fermions  9  . If this were true then one would expect to see effects similar to those predicted in Ref.  10  for Kaluza-Klein gravitons coupled to leptons. Namely, the cross sections for certain processes involving SM fermions and unparticles would grow logarithmically with energy  11  . Such logarithmic growths have indeed been found  12  -  14  in several cases including e+e-→e+e-U, U→eν, and U→μτ. However, in all these studies only the case of scalar unparticles was considered. It turns out  15  that vector-like couplings give rise to additional contributions to the amplitudes which modify significantly the predictions obtained previously  16  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Neutrino Decays and Neutrino Electron Elastic Scattering in Unparticle Physics . Abstract : We research the decay modes of neutrinos into unparticles , as well as their elastic scattering off electrons mediated by virtual unparticles .We see that these mechanisms are strongly restrained for little values of the interaction constant between ordinary particles and unparticles . For larger values of this coupling we obtain branching ratios which can be evaluated at future experiments such as T2K or NOVA .The results presented here possibly also have consequences on other models with extra dimensions beyond those regarded so far . Introduction In recent history there has been an increasing interest in theories where new physics is characterized by fields whose excitations do not carry conventional model ( SM ) charges 1 .These so - called unparticles 2 , if they exist , might manifest themselves through several experimental signatures 3 . In particular , it was shown 4 that decays of SM particles to pairs of unparticles might lead to deviations from the expected exponential behavior of the resulting lifetimes .This phenomenon should be observable experimentally 5 . Furthermore , it was suggested 6 that unparticles might play a role in explaining some puzzling properties observed lately in cosmic ray data 7 , 8 .Another important scenario is that unparticles couple directly to SM fermions 9 . If this were true then one would want to see effects similar to those predicted in Ref .10 for Kaluza - Klein gravitons coupled to leptons . Namely , the cross sections for particular processes involving SM fermions and unparticles might expand logarithmically with energy 11 .Such logarithmic growths have indeed been seen 12 - 14 in multiple cases notably e + e - →e + e - U , U→eν , and U→μτ . However , in all these research only the case of scalar unparticles was considered .It turns out 15 that vector - like couplings make rise to extra contributions to the amplitudes which alter significantly the estimates obtained previously 16 .",
        "rewrite_text": "**Title:** Neutrino Decays and Neutrino Electron Elastic Scattering in Unparticle Physics\n\n**Abstract:** This study investigates the decay processes of neutrinos into unparticles and their elastic scattering with electrons, which is mediated by virtual unparticles. Our findings indicate that these decay and scattering mechanisms are significantly constrained when the interaction constant between standard model (SM) particles and unparticles is small. However, as the coupling strength increases, we derive branching ratios that can be tested in upcoming experiments such as T2K and NOVA. The implications of our results may extend to other theoretical frameworks involving extra dimensions that have not been previously examined. \n\nThe growing interest in new physics theories suggests the existence of fields characterized by excitations that do not conform to conventional SM charges. These entities, referred to as unparticles, could potentially reveal themselves through various experimental signatures. Previous research has demonstrated that the decay of SM particles into unparticle pairs may result in observable deviations from the expected exponential decay patterns, which could be experimentally verified. Additionally, unparticles have been proposed as a possible explanation for certain anomalies observed in cosmic ray data.\n\nAnother critical aspect of unparticle physics is the direct coupling of unparticles to SM fermions. If this coupling exists, it may lead to phenomena akin to those predicted for Kaluza-Klein gravitons interacting with leptons, specifically the logarithmic growth of cross sections for processes involving SM fermions and unparticles. Such logarithmic behavior has been documented in various interactions, including e+e- → e+e- U, U → eν, and U → μτ. Notably, previous studies have predominantly focused on scalar unparticles, while our research reveals that vector-like couplings introduce additional contributions to the scattering amplitudes, significantly altering the previously established estimates. This work aims to deepen the understanding of neutrino interactions within the framework of unparticle physics and its broader implications in theoretical physics.",
        "ori-fast-z-score": -0.17541160386140586,
        "water-fast-z-score": 5.853817179628954,
        "rewrite-fast-z-score": -0.08606629658238704
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The InterHourly-Variability (IHV) Index of Geomagnetic Activity and its Use in Deriving the Long-term Variation of Solar Wind Speed .\nAbstract:\nThe inter-hourly-variability index is proposed to measure geomagnetic activity on hourly basis, which can be used as an indicator for solar wind speed variation over time scales longer than one day.  The IHV index was calculated using hourly values of Kp index during the period 1957-2009. It shows that there are two peaks at about 1965-1970 and 1990-1995 . A linear regression analysis between the IHV index and solar wind speed data obtained by spacecraft observations reveals that they have good correlation with each other. This suggests that the IHV index may provide useful information for long-term prediction of solar wind speed. Keywords: Geomagnetism; Solar wind; Variability index; Prediction modeling; Data mining. 1 Introduction Space weather has been attracting more attention recently because it affects many aspects of human life such as satellite communications, aviation safety, power grids etc. (e.g.,  1  ). In particular, solar wind plays important roles in space weather since it carries away magnetic fluxes generated by the Sun s dynamo action  2  , and thus controls the Earth s magnetosphere  3  .\nSolar wind is driven out from the Sun by coronal mass ejections  4  . Coronal mass ejections occur when huge amounts of plasma suddenly escape from the Sun into space  5  . They usually last several hours  6  . Therefore, solar wind speed varies significantly within 24 h  7, 8  . However, most previous studies only focus on the average solar wind speed or daily averaged solar wind speed  9  . As a result, little work has been done on investigating how solar wind speed changes on shorter timescales  10  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The InterHourly - Variability ( IHV ) Index of Geomagnetic Activity and its Use in Deriving the Long - term Variation of Solar Wind Speed . Abstract : The inter - hourly - variability index is proposed to measure geomagnetic activity on hourly basis , which can be used as an measure for solar wind velocity variation over time ranges less than one month .The IHV indicator was calculated using hourly values of Kp index during the period 1957 - 2009 . It shows that there are two peaks at about 1965 - 1970 and 1990 - 1995 .A linear regression evaluation between the IHV indicator and solar wind velocity results acquired by satellites observations suggests that they have positive relationships with each other . This implies that the IHV indicator may provide useful info for large - term prediction of solar wind velocity .Keywords : Geomagnetism ; Solar wind ; Variability index ; Prediction modeling ; Data mining . 1 Introduction Space winds has been drawing more attention lately because it affects several elements of human life such as radar services , aerospace safety , electricity grids etc .( e . g . , 1 ) . In particular , thermal blowing serves crucial roles in space weather since it carries away magnetic fluxes generated by the Sun s dynamo action 2 , and therefore influences the Earth s magnetosphere 3 .Solar wind is driven out from the Sun by coronal mass ejections 4 . Coronal mass ejections occur when huge amounts of plasma suddenly exit from the Sun into space 5 .They often stay several hours 6 . Therefore , solar wind velocity varies dramatically within 24 h 7 , 8 .However , most prior studies only focus on the average solar wind velocity or daily averaged solar wind velocity 9 . As a result , nothing research has been performed on examining how solar wind velocity changes on shorter timescales 10 .",
        "rewrite_text": "**Title:** The Inter-Hourly Variability (IHV) Index of Geomagnetic Activity and Its Application in Assessing Long-Term Variations in Solar Wind Speed\n\n**Abstract:** This article introduces the Inter-Hourly Variability (IHV) index, a novel metric designed to quantify geomagnetic activity on an hourly basis. The IHV index serves as an effective tool for analyzing variations in solar wind velocity over timeframes shorter than one month. Utilizing hourly Kp index data spanning from 1957 to 2009, we calculated the IHV indicator, which revealed two significant peaks occurring between 1965-1970 and 1990-1995. A linear regression analysis conducted between the IHV index and solar wind velocity data obtained from satellite observations indicates a positive correlation between the two variables. This relationship suggests that the IHV index may be instrumental in facilitating long-term predictions of solar wind velocity, thereby enhancing our understanding of geomagnetic activity and its implications. \n\nThe significance of solar wind has garnered increased attention due to its profound impact on various aspects of human life, including radar operations, aerospace safety, and the stability of electrical grids. Solar wind plays a critical role in space weather phenomena, as it transports magnetic fluxes generated by the Sun's dynamo processes, which in turn affect the Earth's magnetosphere. The solar wind is primarily propelled by coronal mass ejections (CMEs), which are massive bursts of plasma that are ejected from the Sun into space. These events can persist for several hours, leading to substantial fluctuations in solar wind velocity within a 24-hour period. Despite the importance of these variations, most previous research has concentrated on average solar wind velocities or daily averages, leaving a gap in our understanding of short-term dynamics. This study aims to bridge that gap by exploring the variability of solar wind on shorter timescales, thereby contributing valuable insights to the field of geomagnetism and solar physics.\n\n**Keywords:** Geomagnetism; Solar wind; Variability index; Prediction modeling; Data mining.",
        "ori-fast-z-score": -2.6106709553062086,
        "water-fast-z-score": 6.042105779736161,
        "rewrite-fast-z-score": -0.48666426339228763
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Non-perturbative renormalization of the chromo-magnetic operator in Heavy Quark Effective Theory and the B* - B mass splitting .\nAbstract:\nWe present an explicit calculation of the non-perturbative renormalisation constant for the chromomagnetic operator in heavy quark effective theory (HQET). We use this to calculate the leading order contribution to the mass difference between the ground state vector mesons containing a b-quark, i.e., $B^*$- $B$ mixing. The result is compared with lattice QCD calculations at next-to-leading order in HQET perturbation theory. Our results are consistent within errors but do not agree as well as one would like. This may be due to missing higher-order corrections or systematic uncertainties inherent in both approaches. \n \n Introduction \n \n In recent years there has been considerable interest in studying hadronic systems containing a single heavy quark using the framework provided by heavy quark effective theory (HQT)  1  . One important application of HQT is to study the properties of heavy-light mesons such as the bottomonium system  2  , which can then be used to test our understanding of nonrelativistic quantum mechanics  3  .\n \nIn particular, it is interesting to consider how the masses of these states depend on their spin. For example, the lowest lying bb states have spin-parity J P = 0+ and 1− respectively  4  . These two states mix under the weak interaction through the emission and absorption of virtual gluons  5  . At tree level we find that the lightest physical eigenstate is given by:",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Non - perturbative renormalization of the chromo - magnetic operator in Heavy Quark Effective Theory and the B * - B mass separation . Abstract : We present an explicit determination of the non - perturbative renormalisation constant for the chromomagnetic operator in heavy quark effective theory ( HQET ) .We use this to estimate the led order contribution to the mass ratio between the ground state vector mesons containing a b - quark , i . e . , $ B ^ * $ - $ B $ mixing . The result is compared with lattice QCD calculations at next - to - leading order in HQET perturbation theory .Our results are correct within errors but do not comply as well as one would like . This might be due to missing higher - order corrections or systematic uncertainties inherent in both approaches .Introduction In recent years there has been substantial interest in investigating hadronic networks featuring a single heavy quark using the framework given by massive quark effective theory ( HQT ) 1 . One important use of HQT is to study the properties of heavy - light mesons such as the bottomonium scheme 2 , which can then be used to test our appreciation of nonrelativistic quantum mechanics 3 .In particular , it is curious to consider how the masses of these states depend on their spin . For instance , the lowest lying bb states have spin - parity J P = 0 + and 1− respectively 4 .These two states mix under the strong coupling through the emission and emission of virtual gluons 5 . At tree level we find that the lightest physical eigenstate is given by :",
        "rewrite_text": "**Title:** Non-Perturbative Renormalization of the Chromo-Magnetic Operator in Heavy Quark Effective Theory and the B*-B Mass Separation\n\n**Abstract:** In this study, we provide a detailed determination of the non-perturbative renormalization constant for the chromomagnetic operator within the framework of Heavy Quark Effective Theory (HQET). This renormalization constant is crucial for accurately estimating the leading-order contribution to the mass ratio of ground state vector mesons that contain a bottom quark, specifically focusing on the mixing between the B* and B mesons. Our findings are juxtaposed with lattice Quantum Chromodynamics (QCD) calculations performed at next-to-leading order in HQET perturbation theory. While our results align with the lattice QCD outcomes within the estimated uncertainties, they do not achieve the level of agreement that would be ideal. This discrepancy may stem from the omission of higher-order corrections or from systematic uncertainties that are inherent to both methodologies employed. \n\nThe introduction of this research highlights the growing interest in exploring hadronic systems that feature a single heavy quark, particularly through the lens of Heavy Quark Effective Theory (HQET). This theoretical framework has proven instrumental in examining the properties of heavy-light mesons, such as those in the bottomonium sector, thereby providing a platform for testing our understanding of non-relativistic quantum mechanics. A particularly intriguing aspect of this investigation is the dependence of the masses of these mesonic states on their spin characteristics. For example, the lowest-lying bb states exhibit spin-parity configurations of JP = 0+ and 1−, respectively. These states interact through strong coupling, facilitated by the emission and absorption of virtual gluons. At the tree level, our analysis reveals that the lightest physical eigenstate can be expressed as follows: [insert expression]. This work contributes to the broader understanding of heavy quark dynamics and the intricate interplay between spin and mass in hadronic systems.",
        "ori-fast-z-score": -0.09578262852211514,
        "water-fast-z-score": 4.385927910529725,
        "rewrite-fast-z-score": 0.3621429841700741
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Note on the Inapproximability of Correlation Clustering .\nAbstract:\nWe show that correlation clustering is NP-hard to approximate within any constant factor, even for graphs with maximum degree three and clusters of size at most four.  We also present an algorithm which solves this problem exactly in time O(n3). Our results are based on reductions from the exact cover by 3-sets (X3C) problem. The X3C problem asks whether there exists a collection of subsets of a set S such that each element of S belongs to exactly 3 sets in the collection; it has been shown to be NP-complete. For more information about our work see http://arxiv.org/abs/1206.0571 . \nCorrelation clustering is one of several problems studied under the umbrella of  clustering ; these include k-means clustering, spectral clustering, and graph partitioning. It was introduced independently by Bansal et al., Blum et al., and Dasgupta et al. as follows.   Given a weighted undirected graph G = (V, E), where V denotes the vertices and E denotes the edges, we say that two vertices u, v ∈ V are adjacent if they share an edge e ∈ E. A cluster C ⊆ V is defined as a subset of nodes whose pairwise distances satisfy some threshold t > 0. More formally, given a distance function d : V × V → R+ , let dist(u,v) denote the shortest path between u and v; then, C is said to be a valid cluster if and only if for all pairs of nodes u, v ∈ C:  dist(u,v)  ≤ t",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Note on the Inapproximability of Correlation Clustering . Abstract : We see that correlation clustering is NP - hard to approximate within any constant factor , even for graphs with maximum degree three and clusters of diameter at most four .We also present an algorithm which solves this question exactly in time O ( n3 ) . Our results are based on reductions from the exact cover by 3 - sets ( X3C ) question .The X3C problem questions whether there exists a family of subsets of a set S such that each element of S belongs to exactly 3 sets in the collection ; it has been shown to be NP - full . For more information about our work visit http : / / arxiv . org / abs / 1206 . 0571 .Correlation clustering is one of several difficulties investigated under the umbrella of clustering ; these involve k - means clustering , spectral clustering , and graph partitioning . It was introduced independently by Bansal et al . , Blum et al . , and Dasgupta et al .as follows . Given a weighted undirected graph G = ( V , E ) , where V denotes the edges and E denotes the edges , we mean that two vertices u , v ∈ V are adjoining if they share an path w ∈ E . A cluster C ⊆ V is characterized as a subset of vertices whose pairwise distances satisfy some threshold t > 0 .More generally , given a distance function d : V × V → R + , let dist ( v , v ) define the shortest path between u and v ; then , C is said to be a valid cluster if and only if for all pairs of vertices u , v ∈ C : dist ( v , v ) ≤ t",
        "rewrite_text": "**Title:** A Note on the Inapproximability of Correlation Clustering\n\n**Abstract:** In this study, we investigate the computational complexity of correlation clustering, demonstrating that it is NP-hard to approximate within any constant factor, even when restricted to graphs with a maximum degree of three and clusters with a diameter of at most four. Our findings are underpinned by reductions from the Exact Cover by 3-Sets (X3C) problem, which is a well-known NP-complete problem. The X3C problem asks whether there exists a collection of subsets from a set S such that each element of S is included in exactly three of these subsets. This connection highlights the inherent difficulty of correlation clustering and its resistance to approximation.\n\nAdditionally, we introduce an algorithm capable of solving the correlation clustering problem exactly in cubic time, specifically O(n^3), which provides a feasible solution for smaller instances of the problem. Correlation clustering is part of a broader category of clustering challenges that also encompasses k-means clustering, spectral clustering, and graph partitioning. The concept of correlation clustering was independently introduced by researchers Bansal, Blum, and Dasgupta. \n\nIn the context of correlation clustering, we consider a weighted undirected graph G = (V, E), where V represents the vertices and E denotes the edges. Two vertices u and v in V are said to be adjacent if there exists a path connecting them within E. A cluster C, which is a subset of vertices, is defined by the condition that the pairwise distances among its members adhere to a specified threshold t > 0. More formally, given a distance function d: V × V → R+, we define the distance between vertices u and v as dist(u, v), and a cluster C is deemed valid if for every pair of vertices u, v in C, the condition dist(u, v) ≤ t holds true. For further details on our research, please visit http://arxiv.org/abs/1206.0571.",
        "ori-fast-z-score": -1.436739427831727,
        "water-fast-z-score": 2.942389786832747,
        "rewrite-fast-z-score": -0.1781741612749496
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Performance of the Charge Injection Capability of Suzaku XIS .\nAbstract:\nWe have investigated charge injection capability (CIC) of Suzaku X-ray Imaging Spectrometer (XIS). The CIC is an important feature for high energy resolution spectroscopy, which can be used to reduce the effect of pile-up in bright sources and improve the spectral quality at low energies. We performed detailed analysis on the performance of CIC by using Crab observations with different count rates. By comparing the results obtained with and without CIC we found that CIC improves the spectral fitting significantly especially below 1 keV where the pile-up fraction becomes large when the source flux increases. This improvement was confirmed also by analyzing simulated data generated by xisrmfgen and xissimarfgen. Finally, we applied this method to analyze the observation of Mrk 421 during its flare state. We successfully detected emission lines such as O VII Kα line and Fe XXV Kα line from the spectrum taken with CIC enabled.  These results show that CIC works well even under strong pile-up conditions.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Performance of the Charge Injection Capability of Suzaku XIS . Abstract : We have researched charge injection capability ( CIC ) of Suzaku X - ray Imaging Spectrometer ( XIS ) .The CIC is an important feature for high energy resolution spectroscopy , which can be used to reduce the impact of pile - up in bright sources and improve the spectral quality at low energies . We conducted thorough analysis on the performance of CIC by using Crab observations with various count speeds .By comparing the results collected with and without CIC we concluded that CIC improves the spectral fit substantially especially below 1 keV where the pile - up proportion gets large when the source flux rises . This improvement was confirmed also by analyzing simulated evidence generated by xisrmfgen and xissimarfgen .Finally , we applied this technology to analyze the observation of Mrk 421 during its flare state . We correctly discovered emission lines such as O VII Kα line and Fe XXV Kα line from the spectrum collected with CIC enabled .These data indicate that CIC works well even under strong pile - up circumstances .",
        "rewrite_text": "Title: Performance of the Charge Injection Capability of Suzaku XIS\n\nAbstract: This study investigates the charge injection capability (CIC) of the Suzaku X-ray Imaging Spectrometer (XIS), a critical feature for enhancing energy resolution in spectroscopy. The CIC serves to mitigate the effects of pile-up in bright astronomical sources, thereby improving spectral quality, particularly at lower energy levels. We performed a comprehensive analysis of the CIC's performance by examining observations of the Crab Nebula across a range of count rates. Our findings demonstrate a significant enhancement in spectral fitting when CIC is employed, especially in the energy range below 1 keV, where pile-up effects become pronounced as source flux increases. This improvement was further validated through simulations conducted with xisrmfgen and xissimarfgen, which corroborated our observational results. Additionally, we applied the CIC technology to analyze data from the active galaxy Mrk 421 during a flare event. The analysis revealed distinct emission lines, including the O VII Kα and Fe XXV Kα lines, from the spectrum obtained with the CIC enabled. These results underscore the effectiveness of the CIC in managing pile-up conditions, affirming its utility in high-energy astrophysical observations. Overall, our research highlights the importance of charge injection capability in enhancing the performance of X-ray spectroscopy, particularly in scenarios characterized by high source brightness and resultant spectral distortions.",
        "ori-fast-z-score": 0.7683498199278324,
        "water-fast-z-score": 5.314796216557077,
        "rewrite-fast-z-score": 1.9291577137538762
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Heisenberg antiferromagnet with anisotropic exchange on the Kagome lattice: Description of the magnetic properties of volborthite .\nAbstract:\nWe present an ab initio study of the electronic and magnetic structure of volborthite, CaFe3(PO4)2(OH)3·H2O (CFPOH), which is one of the most important minerals in geological sciences as it forms at low temperatures under hydrothermal conditions. Volborthite has been studied extensively by neutron scattering experiments but its microscopic origin remains controversial. We show that the ground state of CFPOH can be described within density functional theory using the generalized gradient approximation plus Hubbard U method for Fe-3d orbitals. The calculated spin wave spectrum agrees well with experimental data obtained by inelastic neutron scattering measurements. In addition we find that the magnetocrystalline anisotropy energy is dominated by spin-orbit coupling effects. Finally, we discuss how our results are related to previous theoretical studies based on different approximations. V olborthite, CaF e 3 (P O 4 ) 2 (OH) 3 ·H 2 O (C F P OH ), is one of the most impor-tant minerals in geological sciences because it forms at low tem-peratures under hydrothermal conditions  1  . It was first discovered in 1832  2  , however, only recently have detailed structural investigations revealed that this mineral belongs to the family of compounds known as  Kagome  materials  3  .\nVolborthite crystallizes into a layered structure consisting of alternating kagome planes of iron ions and phosphate groups  4  . This arrangement leads to interesting physical phenomena such as geometric frustration  5  or quantum fluctuations  6  . For example, recent neutron scattering experiments suggest that volborthite undergoes a phase transition below T N = 5 K  7, 8  where the spins order ferrimagnetically along the c-axis  9  . However, there exists no consensus about the nature of this ordering  10  : while some authors claim that the system orders collinearly  11, 12  others argue that non-collinearity plays an essential role  13, 14  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Heisenberg antiferromagnet with anisotropic exchange on the Kagome lattice : Description of the magnetic properties of volborthite . Abstract : We bring an ab initio investigation of the electronic and magnetic composition of volborthite , CaFe3 ( PO4 ) 2 ( OH ) 3 · H2O ( CFPOH ) , which is one of the most important minerals in geological sciences as it creates at low temperatures under hydrothermal conditions .Volborthite has been studied frequently by neutron scattering experiments but its microscopic origin seems controversial . We suggest that the ground state of CFPOH can be described within density functional theory using the generalized gradient algorithm plus Hubbard U method for Fe - 3d orbitals .The measured spinning wave spectrum agrees well with experimental evidence derived by inelastic neutron scattering observations . In addition we find that the magnetocrystalline anisotropy energy is dominated by spin - orbit bonding effects .Finally , we explain how our findings are related to previous conceptual research based on various approximations . V olborthite , CaF e 3 ( P O 4 ) 2 ( OH ) 3 · H 2 O ( C F P OH ) , is one of the most impor - tant salts in geological sciences because it forms at low tem - peratures under hydrothermal conditions 1 .It was first discovered in 1832 2 , however , only lately have sophisticated structural investigations revealed that this mineral belongs to the group of compounds known as Kagome rocks 3 . Volborthite crystallizes into a layered structure formed of alternating kagome planes of iron ions and phosphate groups 4 .This configuration leads to unusual physical phenomena such as geometric frustration 5 or quantum fluctuations 6 . For instance , recent neutron scattering experiments indicate that volborthite undergoes a phase shift below T N = 5 K 7 , 8 where the spins order ferrimagnetically along the c - axis 9 .However , there exists no consistency about the nature of this ordering 10 : while some writers claim that the scheme orders collinearly 11 , 12 others argue that non - collinearity plays an essential part 13 , 14 .",
        "rewrite_text": "**Title:** Heisenberg Antiferromagnet with Anisotropic Exchange on the Kagome Lattice: A Study of the Magnetic Properties of Volborthite\n\n**Abstract:** This study presents a comprehensive ab initio analysis of the electronic and magnetic characteristics of volborthite, represented by the chemical formula CaFe3(PO4)2(OH)3·H2O (CFPOH). Recognized as a significant mineral in geological sciences, volborthite forms under low-temperature hydrothermal conditions. Despite extensive research, particularly through neutron scattering experiments, the microscopic origins of its magnetic properties remain a subject of debate. Our investigation employs density functional theory (DFT) combined with the generalized gradient approximation and the Hubbard U method specifically for the iron 3d orbitals, allowing us to accurately describe the ground state of CFPOH. The resulting spin wave spectrum aligns closely with experimental data obtained from inelastic neutron scattering, reinforcing the validity of our theoretical approach. Furthermore, we identify that the magnetocrystalline anisotropy energy is primarily influenced by spin-orbit coupling effects, highlighting the complexity of the magnetic interactions within this material. Our findings also draw connections to prior theoretical frameworks that have utilized various approximations to explore the magnetic behavior of volborthite. Discovered in 1832, volborthite has only recently been classified within the Kagome lattice structure, characterized by alternating planes of iron ions and phosphate groups. This unique arrangement leads to intriguing physical phenomena, including geometric frustration and quantum fluctuations. Notably, recent neutron scattering studies have shown that volborthite experiences a phase transition at a Néel temperature (T_N) of 5 K, where the spins exhibit ferrimagnetic ordering along the c-axis. However, discrepancies persist in the literature regarding the nature of this spin ordering, with some researchers advocating for a collinear arrangement, while others emphasize the significance of non-collinear configurations. This paper aims to clarify these conflicting perspectives and contribute to a deeper understanding of volborthite's magnetic properties.",
        "ori-fast-z-score": 0.939793423488437,
        "water-fast-z-score": 7.268326590665242,
        "rewrite-fast-z-score": 1.9100460366360192
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Spectroscopic Observations of the Intermediate Polar EX Hydrae in Quiescence .\nAbstract:\nWe report on spectroscopic observations made with the Nordic Optical Telescope (NOT) and the William Herschel Telescope (WHT). The NOT data were obtained during two observing runs, one in August 2002 and another in September 2003. We used the ALFOSC instrument to obtain time-resolved spectroscopy covering the wavelength range 3700-7000 Å at a resolution of about 1 Å . The WHT data were taken between October 2004 and January 2005 using ISIS spectrographs equipped with gratings R600B and R1200R giving a spectral coverage of 3600-9200 Å at a resolving power of about 1000. In total we have collected more than 100 spectra for this object. All our new data are presented along with previously published results by Vogt et al. (1987) , Vogt & Hatzes (1991), Vogt et al. (1994a-c) . Our analysis shows that the system is composed of an M4V star orbited by a white dwarf companion.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  Spectroscopic Observations of the Intermediate Polar EX Hydrae in Quiescence .Abstract:\nWe report on spectroscopic observations made with the Nordic Optical Telescope (NOT) and the William Herschel Telescope (WHT).The NOT results were obtained during two observing walks , one in August 2002 and another in September 2003 . We utilized the ALFOSC instrument to obtain time - resolved spectroscopy spanning the frequency region 3700 - 7000 Å at a resolution of about 1 Å .The WHT results were took between October 2004 and January 2005 utilizing ISIS spectrographs equipped with gratings R600B and R1200R giving a spectral coverage of 3600 - 9200 Å at a resolving power of about 1000 . In total we have gathered more than 100 spectra for this object .All our new data are presented along with former reported results by Vogt et al . ( 1987 ) , Vogt & Hatzes ( 1991 ) , Vogt et al .( 1994a - c ) . Our study shows that the system is composed of an M4V star orbited by a white dwarf companion .",
        "rewrite_text": "We present a comprehensive analysis of spectroscopic observations of the intermediate polar EX Hydrae during its quiescent phase, utilizing data collected from the Nordic Optical Telescope (NOT) and the William Herschel Telescope (WHT). The observations at NOT were conducted over two separate sessions, one in August 2002 and the other in September 2003, employing the ALFOSC instrument to capture time-resolved spectra across the wavelength range of 3700 to 7000 Å, achieving a resolution of approximately 1 Å. Meanwhile, the WHT observations took place between October 2004 and January 2005, utilizing the ISIS spectrographs with R600B and R1200R gratings, which provided a spectral coverage from 3600 to 9200 Å with a resolving power around 1000. In total, we have compiled over 100 spectra for EX Hydrae, significantly enhancing the existing dataset. Our findings are presented alongside previously reported results from Vogt et al. (1987), Vogt & Hatzes (1991), and Vogt et al. (1994a-c), allowing for a more robust comparison and analysis. The results of our study indicate that the EX Hydrae system consists of an M4V star that is orbited by a white dwarf companion, contributing valuable insights into the characteristics and dynamics of this intriguing binary system. This research not only enriches the understanding of intermediate polars but also provides a foundation for future investigations into similar stellar systems.",
        "ori-fast-z-score": 0.816496580927726,
        "water-fast-z-score": 4.719399037242694,
        "rewrite-fast-z-score": 1.6666666666666667
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Parametric Learning and Monte Carlo Optimization .\nAbstract:\nWe present an algorithm for learning the parameters of a probabilistic model by optimizing its log-likelihood using Monte Carlo (MC) sampling, which we call Parametric Learning with MC Sampling or PLMS. We show that this approach is equivalent to maximum likelihood estimation when the number of samples goes to infinity. In practice however, it can be much more efficient than standard gradient-based optimization methods because it does not require computing gradients over all data points at each iteration. Instead, only one sample per iteration needs to be generated in order to compute the objective function value and update the parameter values accordingly. This makes our method particularly suitable for large datasets where evaluating the objective function may take too much time. Our experiments on synthetic and real-world datasets demonstrate that PLMS outperforms state-of-the-art algorithms such as stochastic gradient descent and Adam in terms of both accuracy and efficiency. Code available at: https://github.com/sunyuan-chen/PLMS",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Parametric Learning and Monte Carlo Optimization . Abstract : We present an algorithm for learning the variables of a probabilistic theory by optimizing its log - likelihood using Monte Carlo ( MC ) filtering , which we call Parametric Learning with MC Sampling or PLMS .We see that this methodology is equal to maximum likelihood estimation when the number of samples goes to infinity . In practice however , it can be much more efficient than conventional gradient - based optimization schemes because it does not require computing gradients over all information points at each iteration .Instead , only one sample per iteration requires to be generated in order to compute the objective function value and update the parameter values accordingly . This gives our technique exceptionally suitable for large datasets where evaluating the objective function could cost too considerable time .Our experiments on synthetic and actual - time datasets prove that PLMS outperforms state - of - the - art algorithms such as stochastic gradient descent and Adam in terms of both precision and efficiency . Code available at : https : / / github . com / sunyuan - chen / PLMS",
        "rewrite_text": "We introduce a novel algorithm designed for optimizing the variables within a probabilistic framework by maximizing its log-likelihood through Monte Carlo (MC) filtering, which we refer to as Parametric Learning with MC Sampling (PLMS). Our findings indicate that this approach converges to maximum likelihood estimation as the sample size approaches infinity. However, in practical applications, PLMS demonstrates significantly greater efficiency compared to traditional gradient-based optimization methods. This advantage arises from the fact that PLMS only necessitates the generation of a single sample per iteration to evaluate the objective function and adjust the parameter values, rather than calculating gradients across all data points at each step. Consequently, this makes our method particularly advantageous for handling large datasets, where the computational cost of evaluating the objective function can be prohibitively high. Empirical evaluations conducted on both synthetic and real-time datasets reveal that PLMS consistently outperforms leading algorithms, including stochastic gradient descent and Adam, in terms of both accuracy and computational efficiency. The implementation of our algorithm is publicly accessible at: https://github.com/sunyuan-chen/PLMS.",
        "ori-fast-z-score": 0.11396057645963795,
        "water-fast-z-score": 5.062895554167108,
        "rewrite-fast-z-score": 1.462614271203831
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Interband, intraband and excited-state direct photon absorption of silicon and germanium nanocrystals embedded in a wide band-gap lattice .\nAbstract:\nWe present the results on the interband, intraband and excitonic transitions for Si and Ge nanocrystals (NCs) embedded into Al2O3 matrix by using first-principles calculations based on density functional theory within local-density approximation. The NC size is varied between 1 nm to 5 nm with an interval of 0.5 nm. We find that the optical gap decreases as we increase the NC size due to quantum confinement effect. In addition, we observe that the lowest energy peak shifts towards higher energies when increasing the NC size which can be attributed to the surface states. Furthermore, our calculated results show that the oscillator strength increases significantly at smaller sizes while it decreases rapidly at larger sizes. Finally, we also investigate the influence of strain on the electronic structure of these systems. Our findings are expected to provide useful information about the design of optoelectronic devices such as solar cells or photodetectors. \n \n Keywords: Silicon Nanocrystal, Germanium Nanocrystal, Optical properties",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Interband , intraband and excited - state direct photon absorption of silicon and germanium nanocrystals enclosed in a broad band - gap crystal . Abstract : We report the results on the interband , intraband and excitonic transitions for Si and Ge nanocrystals ( NCs ) integrated into Al2O3 matrix by using first - principles measurements based on density functional theory within local - density algorithms .The NC size is varied between 1 nm to 5 nm with an interval of 0 . 5 nm . We see that the optical gap falls as we increase the NC size owing to quantum confinement phenomenon .In addition , we determine that the lowest energy peak changes towards higher energies when varying the NC size which can be due to the surface states . Furthermore , our measured data demonstrate that the oscillator strength changes significantly at lower dimensions while it reduces rapidly at larger dimensions .Finally , we also investigate the impact of strain on the electronic design of these systems . Our findings are expected to provide useful details about the development of optoelectronic elements such as sun cells or photodetectors .Keywords: Silicon Nanocrystal, Germanium Nanocrystal, Optical properties",
        "rewrite_text": "Title: Interband, Intraband, and Excited-State Direct Photon Absorption of Silicon and Germanium Nanocrystals Enclosed in a Wide Band-Gap Crystal\n\nAbstract: This study presents an in-depth analysis of interband, intraband, and excitonic transitions in silicon (Si) and germanium (Ge) nanocrystals (NCs) embedded within an aluminum oxide (Al2O3) matrix. Utilizing first-principles calculations grounded in density functional theory (DFT) with local-density approximation (LDA) algorithms, we systematically investigate the optical properties of NCs with sizes ranging from 1 nm to 5 nm, incremented by 0.5 nm. Our results reveal a notable decrease in the optical bandgap as the size of the nanocrystals increases, a phenomenon attributed to quantum confinement effects. Additionally, we observe a shift in the lowest energy absorption peak towards higher energy levels with increasing NC size, which may be influenced by the presence of surface states. The analysis further indicates that the oscillator strength exhibits significant variations at smaller dimensions, while it diminishes rapidly as the dimensions increase. We also explore the effects of mechanical strain on the electronic structure of these nanocrystals, providing insights into how external factors can influence their optical behavior. The implications of our findings are substantial for the advancement of optoelectronic devices, including solar cells and photodetectors, as they offer critical information on the design and optimization of materials for enhanced performance. This work contributes to the understanding of the fundamental optical characteristics of Si and Ge nanocrystals, paving the way for future research and applications in nanotechnology and materials science. \n\nKeywords: Silicon Nanocrystal, Germanium Nanocrystal, Optical Properties",
        "ori-fast-z-score": -0.9434563530497265,
        "water-fast-z-score": 5.629889179541019,
        "rewrite-fast-z-score": -1.6858544608470492
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Brain Prostheses as a Dynamic System (Immortalizing the Human Brain?) .\nAbstract:\nThe brain is an organ that has evolved to be dynamic and plastic, constantly changing its structure in response to internal and external stimuli.  The human brain can change throughout life by forming new connections between neurons or eliminating existing ones.   This ability allows humans to adapt to their environment and learn continuously.   However, this flexibility also makes it vulnerable to damage caused by disease, trauma, aging, etc., which may lead to neurological disorders such as Alzheimer s Disease, Parkinson s Disease, Huntington s Disease, Epilepsy, Traumatic Brain Injury, Multiple Sclerosis, etc.   In recent years there have been significant advances in neuroscience research aimed at understanding how the brain works and developing treatments for these diseases.   One approach involves using neural prosthetic devices to replace damaged parts of the brain with artificial components that are capable of performing similar functions.   Neural prosthetics are electronic systems designed to interface directly with the nervous system to restore lost function due to injury or disease.   These devices include microelectrodes implanted into the brain tissue to record electrical activity; stimulating electrodes placed on or near nerves to deliver electrical stimulation; and wireless communication links used to transmit data recorded by the recording electrodes and/or control signals generated by the stimulating electrodes back to a computer located outside the body.   ...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  Brain Prostheses as a Dynamic System (Immortalizing the Human Brain?).Abstract : The mind is an organ that has evolution to be dynamic and plastic , constantly shifting its composition in reaction to internal and external stimuli . The human mind can shift throughout life by forming fresh connections between brains or eliminating existing ones .This skill allows individuals to adapt to their environment and learn continuously . However , this flexibility still makes it susceptible to disruption caused by illness , trauma , aging , etc . , which sometimes lead to neurological disorders such as Alzheimer s Disease , Parkinson s Disease , Huntington s Disease , Epilepsy , Traumatic Brain Injury , Multiple Sclerosis , etc .In past decades there have been significant advances in neuroscience study aiming at studying how the brain acts and developing treatments for these diseases . One approach involves utilizing neural prosthetic devices to replace injured parts of the brain with artificial components that are capable of executing identical functions .Neural prosthetics are electronic systems designed to interface directly with the nervous system to restore lost activity resulting to trauma or disease . These systems include microelectrodes implanted into the brain skin to track electrical behavior ; stimulating electrodes put on or close nerves to provide electrical stimulation ; and wireless communication connections utilized to transmit data produced by the recording electrodes and / or control messages produced by the stimulating electrodes forward to a computer situated outside the brain ....",
        "rewrite_text": "**Title: Brain Prostheses as a Dynamic System (Immortalizing the Human Brain?)**\n\n**Abstract:** The human mind is a remarkable organ characterized by its dynamic and plastic nature, continuously adapting its structure in response to both internal and external stimuli. Throughout an individual's life, the brain exhibits the ability to forge new neural connections while also pruning existing ones, a process that underpins learning and environmental adaptation. However, this inherent flexibility renders the brain vulnerable to various disruptions, including those caused by illness, trauma, and aging. Such disruptions can lead to a range of neurological disorders, including Alzheimer's Disease, Parkinson's Disease, Huntington's Disease, epilepsy, traumatic brain injury, and multiple sclerosis. In recent decades, significant strides have been made in neuroscience research aimed at understanding brain function and developing effective treatments for these debilitating conditions. One promising avenue of exploration involves the use of neural prosthetic devices, which serve to replace damaged areas of the brain with artificial components capable of performing similar functions. These neural prosthetics are sophisticated electronic systems designed to interface directly with the nervous system, thereby restoring lost functionalities resulting from injury or disease. Key components of these systems include microelectrodes implanted within the brain to monitor electrical activity, stimulating electrodes positioned on or near nerves to deliver electrical stimulation, and wireless communication links that facilitate the transmission of data from recording electrodes and control signals from stimulating electrodes to external computers. This innovative approach not only holds the potential to enhance the quality of life for individuals suffering from neurological impairments but also raises intriguing questions about the future of brain-machine interfaces and the possibility of extending cognitive functions beyond biological limitations. As research progresses, the implications of brain prostheses as a dynamic system could redefine our understanding of the human brain and its capabilities, potentially leading to what some may consider the immortalization of cognitive processes.",
        "ori-fast-z-score": -1.4729193886373175,
        "water-fast-z-score": 7.24191812652287,
        "rewrite-fast-z-score": -1.8126539343499315
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Polarization sensitive spectroscopy of charged Quantum Dots .\nAbstract:\nWe report on the polarization dependent photoluminescence (PL) and Raman scattering spectra of CdSe/CdS core/shell quantum dots in solution at room temperature. The PL spectrum shows that the emission is polarized along the direction perpendicular to the excitation light, which can be explained by the selection rules for dipole transitions between electronic states with different angular momenta. In addition we observe an anisotropic broadening of the Stokes linewidths as well as a splitting into two components when exciting circularly polarized light. These effects are attributed to the presence of exciton fine structure due to spin-orbit coupling. We also find evidence for a strong electron-phonon interaction leading to phonon sidebands in both the Stokes and anti-Stokes parts of the Raman spectrum. Finally, we show how these results can be used to determine the orientation of individual QDs embedded in a polymer matrix. Polarized luminescence measurements have been performed on single QD emitters using confocal microscopy.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Polarization sensitive spectroscopy of charged Quantum Dots . Abstract : We report on the polarization dependent photoluminescence ( PL ) and Raman absorption spectra of CdSe / CdS core / shell quantum dots in solution at room temperature .The PL spectrum reveals that the emission is polarized along the direction perpendicular to the excitation light , which can be described by the selection rules for dipole changes between electronic states with various angular momenta . In addition we exhibit an anisotropic broadening of the Stokes linewidths as well as a dividing into two parts when exciting circularly polarized light .These effects are traced to the presence of exciton fine structure owing to spin - orbit bonding . We additionally find proof for a powerful atom - phonon interaction leading to phonon sidebands in both the Stokes and anti - Stokes parts of the Raman spectrum .Finally , we show how these results can be used to predict the orientation of individual QDs integrated in a polymer matrix . Polarized luminescence measurements have been performed on single QD emitters using confocal microscopy .",
        "rewrite_text": "In this study, we investigate the polarization-sensitive photoluminescence (PL) and Raman absorption spectra of CdSe/CdS core/shell quantum dots (QDs) in solution at ambient temperature. Our findings reveal that the PL emission is predominantly polarized in a direction that is perpendicular to the excitation light. This polarization behavior can be understood through the selection rules governing dipole transitions between electronic states characterized by different angular momenta. Furthermore, we observe an anisotropic broadening of the Stokes linewidths, which manifests as a bifurcation into two distinct components when the quantum dots are excited using circularly polarized light. These phenomena are attributed to the exciton fine structure resulting from spin-orbit coupling within the QDs. \n\nAdditionally, our results provide compelling evidence for a significant atom-phonon interaction, which leads to the emergence of phonon sidebands in both the Stokes and anti-Stokes regions of the Raman spectrum. This interaction highlights the intricate coupling between electronic and vibrational states in these nanostructures. Importantly, we demonstrate how the insights gained from our polarization-dependent measurements can be utilized to infer the orientation of individual quantum dots when they are integrated into a polymer matrix. To achieve this, we conducted polarized luminescence measurements on single QD emitters using advanced confocal microscopy techniques. Our work not only enhances the understanding of the optical properties of charged quantum dots but also opens avenues for their application in optoelectronic devices and quantum information technologies.",
        "ori-fast-z-score": 0.47140452079103173,
        "water-fast-z-score": 4.649905549752772,
        "rewrite-fast-z-score": 0.30151134457776363
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Hierarchical Approach for Dependability Analysis of a Commercial Cache-Based RAID Storage Architecture .\nAbstract:\nWe present an approach to dependability analysis that is hierarchical in nature, and can be applied to any cache-based storage architecture. The main idea behind our approach is the use of a set of models at different levels of abstraction to represent the system under study. We show how these models are used together with simulation experiments to perform dependability evaluation on a commercial cache-based RAID storage architecture. Our results demonstrate that this approach provides significant advantages over traditional approaches based solely on analytical modeling or simulation experiments. In particular, we find that: (1) it allows us to obtain accurate estimates of reliability measures such as mean time between failures; (2) it enables us to explore tradeoffs among various design parameters; and (3) it helps us identify critical components within the system. Finally, we discuss some limitations of our approach and suggest directions for future research. Caching has been widely adopted by modern computer systems to improve performance through reducing access latency. However, caching introduces new challenges related to data consistency management and fault tolerance. This article presents a novel approach to dependability analysis of cache-based storage architectures.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Hierarchical Approach for Dependability Analysis of a Commercial Cache - Based RAID Storage Architecture . Abstract : We create an solution to dependability analysis that is hierarchical in nature , and can be applied to any cache - based cache architecture .The main idea behind our approach is the using of a setting of models at different levels of abstraction to model the process under research . We see how these models are using combined with modeling experiments to conduct dependability evaluation on a commercial cache - based RAID disk architecture .Our results show that this methodology offers substantial benefits over traditional techniques based primarily on analytical modeling or modeling experiments . In particular , we find that : ( 1 ) it allows us to obtain precise estimates of quality measures such as average time between crashes ; ( 2 ) it allows us to examine tradeoffs among various design variables ; and ( 3 ) it allows us identify vital components within the system .Finally , we explain some difficulties of our approach and suggest directions for future research . Caching has been widely adopted by current computer networks to achieve quality through decreasing entry latency .However , caching brings additional challenges related to data consistency handling and failure tolerance . This page presents a new approach to dependability analysis of cache - based cache architectures .",
        "rewrite_text": "Title: A Hierarchical Approach for Dependability Analysis of a Commercial Cache-Based RAID Storage Architecture\n\nAbstract: In this study, we propose a novel hierarchical methodology for the dependability analysis of cache-based storage architectures, specifically focusing on a commercial cache-based RAID system. Our approach leverages a series of models operating at varying levels of abstraction, which collectively facilitate a comprehensive understanding of the dependability characteristics of the system under investigation. By integrating these models with experimental data, we conduct a thorough evaluation of the dependability of the RAID architecture, revealing significant advantages over conventional methods that primarily rely on either analytical modeling or isolated experimental approaches.\n\nOur findings indicate that this hierarchical methodology yields precise estimates of critical quality metrics, such as the average time between failures, thereby enhancing the reliability assessment process. Furthermore, it enables a detailed exploration of trade-offs among different design variables, allowing for informed decision-making in system design. Additionally, our approach aids in the identification of crucial components within the architecture that significantly impact overall system dependability.\n\nDespite the strengths of our methodology, we also address certain challenges encountered during its application and propose potential avenues for future research to further refine and expand upon our findings. The increasing reliance on caching mechanisms in modern computer networks to improve performance by reducing access latency introduces complexities related to data consistency and fault tolerance. This paper presents a structured approach to addressing these challenges, contributing to the broader field of dependability analysis in cache-based architectures.",
        "ori-fast-z-score": -1.6283046848759573,
        "water-fast-z-score": 8.315218406202998,
        "rewrite-fast-z-score": 0.7863336509949341
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Stellar kinematics in the remote Leo II dwarf spheroidal galaxy -- Another brick in the wall .\nAbstract:\nWe present new spectroscopic observations for eight red giant stars in the nearby dwarf spheroidal galaxy, Leo II (D = 3 Mpc). The data were obtained with the Keck telescope and HIRES spectrograph over three nights during August 2005. We measure heliocentric radial velocities ranging between -150 to +50 km/sec. These values are consistent with previous measurements made by other authors using different techniques. Using these new data we have determined that there is no significant rotation or streaming motion within this system. This result supports theoretical predictions based on N-body simulations which suggest that dark matter halos should be nearly spherical systems. In addition, our results provide further evidence against the possibility that Leo II may contain an intermediate mass black hole at its center. \n \n Keywords: Dwarf galaxies; Black holes; Dark Matter Halos; Rotation Curves; Stellar Kinematics; Spherical Symmetry",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Stellar kinematics in the distant Leo II dwarf spheroidal galaxy - - Another brick in the wall . Abstract : We report new spectroscopic observations for eight red giant galaxies in the nearby dwarf spheroidal galaxy , Leo II ( D = 3 Mpc ) .The data were obtained with the Keck camera and HIRES spectrograph over three nights during August 2005 . We determine heliocentric radial velocities ranging between - 150 to + 50 km / sec .These values are compatible with previous measurements made by other researchers using different methods . Using these new data we have concluded that there is no considerable rotation or streaming motion within this system .This result provides theoretical estimates based on N - bodies simulations which propose that dark matter halos should be nearly spherical systems . In addition , our findings provide further evidence against the idea that Leo II may contain an intermediate mass black hole at its core .Keywords: Dwarf galaxies; Black holes; Dark Matter Halos; Rotation Curves; Stellar Kinematics; Spherical Symmetry",
        "rewrite_text": "**Title:** Stellar Kinematics in the Distant Leo II Dwarf Spheroidal Galaxy - Another Brick in the Wall\n\n**Abstract:** In this study, we present new spectroscopic observations of eight red giant stars within the nearby dwarf spheroidal galaxy, Leo II, located at a distance of approximately 3 Mpc. The data were collected using the Keck Observatory's camera and HIRES spectrograph over three nights in August 2005. Our analysis yielded heliocentric radial velocities that range from -150 to +50 km/s, which align well with previous measurements obtained through various methodologies by other researchers. Notably, our findings indicate a lack of significant rotation or streaming motion within the Leo II system. This observation supports theoretical predictions derived from N-body simulations, which suggest that dark matter halos are likely to be nearly spherical in shape. Furthermore, our results contribute to the growing body of evidence against the hypothesis that Leo II harbors an intermediate mass black hole at its center. The implications of these findings are substantial for our understanding of the dynamics and structure of dwarf galaxies, particularly in relation to dark matter distribution and the potential presence of black holes. This research adds another layer of insight into the complex nature of stellar kinematics in dwarf spheroidal galaxies, reinforcing the notion of spherical symmetry in dark matter halos and challenging existing theories regarding the presence of massive black holes in such systems. \n\n**Keywords:** Dwarf galaxies; Black holes; Dark Matter Halos; Rotation Curves; Stellar Kinematics; Spherical Symmetry.",
        "ori-fast-z-score": 0.9428090415820635,
        "water-fast-z-score": 5.500933918218137,
        "rewrite-fast-z-score": 1.8542101386022132
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Recent progresses in the simulation of small-scale magnetic fields .\nAbstract:\nThe origin and evolution of cosmic magnetic fields are one of the most important unsolved problems in astrophysics, which is closely related to many other fundamental issues such as star formation, galaxy formation and structure formation. In this talk I will review our recent work on simulating primordial magnetic fields with different physical processes involved. The first part focuses on the generation of seed magnetic fields during inflation by quantum fluctuations. We show that these seeds can be amplified significantly after reheating due to the magnetohydrodynamic turbulence driven by the decaying inflaton perturbations. Then we discuss how these seeds evolve into large-scale coherent magnetic fields through various mechanisms including inverse cascade, dynamo action and turbulent pumping. Finally, we present some possible observational signatures for future detection. This talk was given at the International Conference on Computation & Theory (ICCT) held in Beijing, China between September 24-27, 2014.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Recent progresses in the simulation of tiny - scale magnetic fields . Abstract : The origin and evolution of cosmic magnetic waves are one of the most important unsolved issues in astrophysics , which is closely related to many other fundamental issues such as galaxy formation , galaxy formation and shape formation .In this talk I will review our latest work on simulating primordial magnetic waves with various physical processes implicated . The first part involves on the generation of seed magnetic waves during inflation by quantum fluctuations .We see that these plants can be amplified significantly after reheating due to the magnetohydrodynamic turbulence driven by the decaying inflaton perturbations . Then we talk how these plants transform into huge - scale coherent magnetic fields through several mechanisms namely inverse cascade , dynamo action and turbulent pumping .Finally , we present some possible observational signatures for future detection . This discussion was given at the International Conference on Computation & Theory ( ICCT ) holding in Beijing , China between September 24 - 27 , 2014 .",
        "rewrite_text": "Title: Recent Advances in the Simulation of Small-Scale Magnetic Fields\n\nAbstract: The genesis and development of cosmic magnetic waves represent a critical and unresolved challenge in the field of astrophysics, intricately linked to several fundamental phenomena, including galaxy formation and the evolution of cosmic structures. In this presentation, I will provide an overview of our recent research focused on the simulation of primordial magnetic waves, taking into account a variety of relevant physical processes. The initial segment of our study addresses the generation of seed magnetic waves during the inflationary epoch, which arises from quantum fluctuations. Our findings indicate that these primordial fields can experience substantial amplification following the reheating phase, primarily driven by magnetohydrodynamic turbulence resulting from the decay of inflaton perturbations. Subsequently, we explore the mechanisms through which these initial magnetic seeds evolve into large-scale coherent magnetic fields. Key processes discussed include inverse cascade, dynamo action, and turbulent pumping, all of which play significant roles in this transformation. Finally, we will outline potential observational signatures that could facilitate the detection of these magnetic fields in future astrophysical surveys. This research was presented at the International Conference on Computation & Theory (ICCT), held in Beijing, China, from September 24 to 27, 2014.",
        "ori-fast-z-score": -0.8944271909999159,
        "water-fast-z-score": 4.196397990844169,
        "rewrite-fast-z-score": -0.10482848367219183
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Black Hole Binary Nova Scorpii 1994 (GRO J1655-40): An improved chemical analysis .\nAbstract:\nWe present an improved abundance determination for the black hole binary nova Sco X-1, based on high-resolution optical spectroscopy obtained with UVES at VLT-UT2 in November 2004 and January 2005. The new data are combined with previously published results to derive abundances for CNO elements as well as FeI and FeII lines. We find that our best-fit model is consistent with previous studies within their uncertainties. However, we obtain significantly lower values for carbon and oxygen than those reported by Gies & Bolton (1986) . This discrepancy may be due to differences between the adopted atmospheric models or atomic data used in these two analyses. \n \n Keywords: Black holes - Abundance ratios - X-ray binaries - Spectroscopy - Ultraviolet space observatories - Variability - Velocity fields - Stellar winds - Mass transfer -X-ray emission - Accretion disks - Novae - Supernovae",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Black Hole Binary Nova Scorpii 1994 ( GRO J1655 - 40 ) : An enhanced chemical analysis . Abstract : We present an updated abundance calculation for the dark hole binary nova Sco X - 1 , using on high - resolution optical spectroscopy achieved with UVES at VLT - UT2 in November 2004 and January 2005 .The revised data are combined with former reported results to derive abundances for CNO compounds as well as FeI and FeII lines . We see that our better - fitting model is compatible with previous research within their uncertainties .However , we obtain significantly reduced estimates for carbon and oxygen than those published by Gies & Bolton ( 1986 ) . This discrepancy may be due to differences between the adopted atmospheric models or atomic data used in these two analyses .Keywords : Black holes - Abundance ratios - X - ray binaries - Spectroscopy - Ultraviolet space observatories - Variability - Velocity fields - Stellar winds - Mass transfer - X - ray radiation - Accretion disks - Novae - Supernovae",
        "rewrite_text": "Title: The Black Hole Binary Nova Scorpii 1994 (GRO J1655-40): An Enhanced Chemical Analysis\n\nAbstract: In this study, we present a comprehensive update on the abundance calculations for the black hole binary system Nova Scorpii X-1, specifically GRO J1655-40, utilizing high-resolution optical spectroscopy obtained with the Ultraviolet and Visual Echelle Spectrograph (UVES) at the Very Large Telescope (VLT) during observations conducted in November 2004 and January 2005. Our analysis integrates the newly acquired data with previously reported findings to derive precise abundance ratios for carbon, nitrogen, and oxygen (CNO) elements, as well as iron lines (FeI and FeII). Notably, our refined model aligns well with earlier studies within their respective uncertainties, yet we observe a significant reduction in the estimated abundances of carbon and oxygen compared to the values reported by Gies & Bolton (1986). This variation may stem from differences in the atmospheric models or atomic data employed in the two analyses. Our findings contribute to a deeper understanding of the chemical composition of this intriguing binary system and highlight the importance of advanced spectroscopic techniques in astrophysical research. The implications of our results extend to various fields, including the study of black holes, abundance ratios in X-ray binaries, and the dynamics of stellar winds and mass transfer processes. This work underscores the necessity for continued exploration of the chemical properties of such systems to enhance our knowledge of their formation and evolution, as well as their role in the broader context of stellar and galactic evolution.\n\nKeywords: Black holes, Abundance ratios, X-ray binaries, Spectroscopy, Ultraviolet space observatories, Variability, Velocity fields, Stellar winds, Mass transfer, X-ray radiation, Accretion disks, Novae, Supernovae.",
        "ori-fast-z-score": -1.3438638879193574,
        "water-fast-z-score": 3.9727331518303837,
        "rewrite-fast-z-score": 0.2727272727272727
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A  diamond-ring  star: the unusual morphologic structure of a young (multiple?) object .\nAbstract:\nWe report on near-infrared observations of an extremely red, faint source in the field of view of the Hubble Space Telescope s Wide Field Planetary Camera 2 (WFPC2). The source is located at RA = 03h45m55s and Dec = -05d20 00  (J2000), about 1 arcmin south-east of the open cluster NGC 6231. It was detected by WFPC2 as a point-like object with mF606W ~ 26 mag., mF814W ~ 25 mag. (Vega system) and mH ~ 20 mag.. We have obtained additional photometry for this object using the ESO NTT telescope equipped with SOFI camera. Our results show that it has very red colors (R-I > 5 mag.) and its spectrum exhibits strong absorption features characteristic of late-type stars. Its absolute magnitude M(K) = -4.3 ± 0.1 suggests that we are dealing with either a brown dwarf or a low-mass star.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A diamond - ring star : the unusual morphologic structure of a young ( several ? ) object .Abstract : We report on near - infrared observations of an incredibly red , faint source in the field of view of the Hubble Space Telescope s Wide Field Planetary Camera 2 ( WFPC2 ) . The source is situated at RA = 03h45m55s and Dec = - 05d20 00 ( J2000 ) , about 1 arcmin south - west of the open cluster NGC 6231 .It was measured by WFPC2 as a point - like body with mF606W ~ 26 mag . , mF814W ~ 25 mag . ( Vega system ) and mH ~ 20 mag . . We have achieved additional photometry for this object using the ESO NTT telescope mounted with SOFI lens .Our results show that it has very red colors ( R - I > 5 mag . ) and its spectrum exhibits strong absorption elements distinctive of late - class stars .Its absolute magnitude M ( K ) = - 4 . 3 ± 0 . 1 suggests that we are dealing with either a brown dwarf or a small - weight star .",
        "rewrite_text": "Title: A Diamond-Ring Star: The Unusual Morphological Structure of a Young Object\n\nAbstract: In this study, we present near-infrared observations of an exceptionally red and faint astronomical source identified within the field of view of the Hubble Space Telescope's Wide Field Planetary Camera 2 (WFPC2). This intriguing object is located at coordinates RA = 03h45m55s and Dec = -05d20'00\" (J2000), approximately 1 arcminute southwest of the open cluster NGC 6231. The WFPC2 measurements indicate that this source appears as a point-like entity with magnitudes of mF606W ~ 26 mag, mF814W ~ 25 mag (in the Vega system), and mH ~ 20 mag. To further investigate its properties, we conducted additional photometric observations using the ESO New Technology Telescope (NTT) equipped with the SOFI instrument. Our findings reveal that this object exhibits remarkably red colors, with R - I values exceeding 5 mag, and its spectral analysis shows pronounced absorption features characteristic of late-type stars. The derived absolute magnitude M(K) = -4.3 ± 0.1 implies that the object could either be a brown dwarf or a low-mass star. These observations suggest that this source may represent a unique class of young stellar objects, potentially contributing to our understanding of star formation and the characteristics of substellar entities. Further investigation into its physical properties and evolutionary status is warranted to elucidate the nature of this unusual diamond-ring star.",
        "ori-fast-z-score": -0.3779644730092272,
        "water-fast-z-score": 4.25,
        "rewrite-fast-z-score": -0.9138115486202573
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The core binary fractions of star clusters from realistic simulations .\nAbstract:\nWe present the results of N-body simulations for open and globular star clusters with different initial conditions, including primordial binaries in various proportions (from 0 to 100%). We find that the fraction of binaries among all stars decreases as the cluster evolves due to dynamical interactions between single and binary stars. The decrease is more pronounced if there are initially many hard binaries or few soft ones. In addition, we show how the number of binaries depends on their binding energy distribution at birth. Finally, we compare our results with observations of real open and globular clusters. Our main conclusions are:  1) Open clusters have fewer binaries than globulars because they lose most of them during early evolution.  2) Binaries can be destroyed by three-body encounters even when the total number of binaries remains constant.  3) Hard binaries dominate over soft ones after several relaxation timescales t rh .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The fundamental binary fractions of star clusters from realistic simulations . Abstract : We report the results of N - bodies simulations for open and globular star clusters with various initial conditions , including primordial binaries in different proportions ( from 0 to 100 % ) .We see that the fraction of binaries among all stars reduces as the cluster evolves due to dynamical interactions between single and binary stars . The reduction is more pronounced if there are initially multiple hard binaries or few hard ones .In addition , we determine how the number of binaries depends on their binding energy density at birth . Finally , we compare our findings with observations of real open and globular complexes .Our main results are : 1 ) Open clusters have fewer binaries than globulars because they losing most of them during early evolution . 2 ) Binaries can be killed by three - bodies interactions even when the total number of binaries remains constant .3 ) Hard binaries dominate over soft ones after many relaxation timescales t rh .",
        "rewrite_text": "We present the findings from our N-body simulations of open and globular star clusters, exploring a range of initial conditions that include primordial binaries with varying proportions, from 0% to 100%. Our analysis reveals that the fraction of binary stars within the clusters diminishes as they evolve, primarily due to dynamic interactions between single stars and binary systems. This decline is particularly significant in clusters that start with a higher number of hard binaries, while those with fewer hard binaries experience a less pronounced reduction. Furthermore, we investigate the relationship between the initial binding energy density of binaries and their subsequent survival rates throughout the cluster's evolution. \n\nIn our comparative analysis with observational data from actual open and globular star clusters, we draw several key conclusions. Firstly, we find that open clusters tend to harbor fewer binary stars than globular clusters, a trend attributed to the more substantial loss of binaries during the early stages of their evolution. Secondly, our simulations indicate that binary systems can be disrupted through three-body interactions, even in scenarios where the overall number of binaries remains unchanged. Lastly, we observe that after multiple relaxation timescales (t_rh), hard binaries become more prevalent than their softer counterparts. These insights enhance our understanding of the dynamical processes governing star clusters and their binary populations, providing a framework for interpreting the observed characteristics of these stellar systems.",
        "ori-fast-z-score": -1.835325870964494,
        "water-fast-z-score": 3.3048567173295003,
        "rewrite-fast-z-score": 1.8569533817705188
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  An Autonomous Distributed Admission Control Scheme for IEEE 802.11 DCF .\nAbstract:\nIn this paper, we propose an autonomous distributed admission control scheme to improve the performance and fairness in wireless local area networks (WLANs). The proposed scheme is based on the concept that each station maintains its own queue length information by using the packet inter-arrival time at the physical layer. In addition, it uses the number of active stations as well as their transmission rates to determine whether or not new connections are admitted into the network. We show through simulation results that our scheme can achieve better throughput than existing schemes while maintaining good fairness among competing stations. Keywords: Wireless Local Area Networks, Packet Inter-Arrival Time, Fairness, Throughput Improvement. 1 Introduction With the rapid development of mobile computing devices such as laptops, PDAs, smart phones etc., there has been growing interest in providing high quality services over wireless local area networks (WLANS)  1  . However, due to limited bandwidth resources available in WLANs, efficient resource management becomes crucially important  2  .\nThe most widely used medium access control protocol in current commercial WLAN products is the IEEE 802.11 Distributed Coordination Function (DCF), which provides both contention-based channel access mechanism called Carrier Sense Multiple Access with Collision Avoidance (CSMA/CA)  3  , and contention-free service via Point Coordinated Function (PCF)  4  . Although CSMA/CA allows multiple stations to share the same radio channel simultaneously without any centralized coordination, it suffers from poor system performance when the traffic load increases  5  . This problem is mainly caused by the hidden terminal effect  6  where two nodes may transmit packets to one another simultaneously causing collisions. To alleviate these problems, several approaches have been proposed  7 -10  . Among them, the authors in  8  introduced a simple but effective method known as Virtual Reservation Channel (VRC) to reduce the probability of collision between data frames transmitted by different stations. They also presented a modified version of VRC  9  to further enhance the performance of CSMA/CA under heavy loads. However, all these works assume that the number of active stations within the",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : An Autonomous Distributed Admission Control Scheme for IEEE 802 . 11 DCF . Abstract : In this paper , we propose an autonomous distributed entrance control scheme to promote the performance and fairness in telecommunications local region systems ( WLANs ) .The proposed system is based on the idea that each signal maintains its own queue length information by using the packet inter - arrival rate at the physical layer . In addition , it utilizes the number of active stations as well as their transmission rates to judge whether or not fresh connections are admitted into the channel .We see through simulation data that our scheme can attain better throughput than existing plans while maintaining good fairness among competing networks . Keywords : Wireless Local Area Networks , Packet Inter - Arrival Time , Fairness , Throughput Improvement .1 Introduction With the fast development of mobile computing devices such as laptops , PDAs , smart phones etc . , there has been growing interest in providing good quality services over wireless regional area networks ( WLANS ) 1 . However , owing to limited bandwidth assets available in WLANs , efficient resource control makes crucially essential 2 .The most commonly used medium access control protocol in current consumer WLAN offerings is the IEEE 802 . 11 Distributed Coordination Function ( DCF ) , which offers both contention - based network entry method called Carrier Sense Multiple Access with Collision Avoidance ( CSMA / CA ) 3 , and contention - free service via Point Coordinated Function ( PCF ) 4 . Although CSMA / CA enables many affiliates to exchange the same radio channel simultaneously without any centralized coordination , it suffers from poor program performance when the traffic burden rises 5 .This problem is mainly caused by the concealed terminal effect 6 where two nodes may transmit packets to one another simultaneously creating collisions . To ease these problems , various approaches have been proposed 7 - 10 .Among them , the authors in 8 invented a simple but effective technology called as Virtual Reservation Channel ( VRC ) to reduce the probability of collision between signal frames transmitted by various stations . They also presented a altered version of VRC 9 to further enhance the performance of CSMA / CA under heavy loads .However , all these works assume that the quantity of active stations within the",
        "rewrite_text": "**Title:** An Autonomous Distributed Admission Control Scheme for IEEE 802.11 DCF\n\n**Abstract:** This paper introduces an innovative autonomous distributed admission control scheme aimed at enhancing both performance and fairness in Wireless Local Area Networks (WLANs). The proposed approach leverages the concept that each station independently monitors its own queue length by analyzing the packet inter-arrival rate at the physical layer. Additionally, it assesses the number of active stations and their respective transmission rates to determine the feasibility of admitting new connections to the channel. Through extensive simulations, we demonstrate that our scheme significantly improves throughput compared to existing methods while ensuring equitable access among competing networks. The findings underscore the importance of efficient resource management in WLANs, particularly in light of the increasing demand for high-quality services driven by the proliferation of mobile computing devices such as laptops, PDAs, and smartphones. The prevalent medium access control protocol in consumer WLANs, IEEE 802.11 Distributed Coordination Function (DCF), employs a contention-based access mechanism known as Carrier Sense Multiple Access with Collision Avoidance (CSMA/CA), alongside a contention-free service option via Point Coordinated Function (PCF). While CSMA/CA allows multiple users to share the same radio channel without centralized control, it often experiences degraded performance under high traffic conditions, primarily due to the hidden terminal problem, which leads to packet collisions. To address these challenges, various strategies have been proposed, including the Virtual Reservation Channel (VRC) technique, which aims to minimize collision probabilities among frames from different stations. Despite these advancements, previous research has largely overlooked the dynamics of active station counts within the network. Our work fills this gap by presenting a robust solution that not only enhances throughput but also promotes fairness in resource allocation, making it a valuable contribution to the field of wireless communications.\n\n**Keywords:** Wireless Local Area Networks, Packet Inter-Arrival Time, Fairness, Throughput Improvement.",
        "ori-fast-z-score": -1.2271439821557926,
        "water-fast-z-score": 8.129828881782126,
        "rewrite-fast-z-score": 1.9233566230163088
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The chemical composition of the circumstellar envelopes around yellow hypergiant stars .\nAbstract:\nWe present new observations and analysis of the infrared emission lines in the spectra of two yellow hypergiants, IRC+10420 and AFGL2136. We find that these objects have very high mass-loss rates (10^-6 to 10^-5 Msun/yr) with outflow velocities ranging between 100-200 km/sec. The observed line profiles are consistent with an expanding shell model for the wind. In addition we detect several forbidden transitions which indicate the presence of highly ionized species such as Fe + , Si ++ , S ++ . These ions may be formed by photoionization or collisional ionization processes within the stellar winds. \n \n Keywords: Yellow Hypergiants; Circumstellar Envelopes; Mass loss rate; Outflows; Emission Lines; IRAS 08544-4431. Astronomy & Astrophysics manuscript no. aa20031118 May 31, 2003 \nThe chemical composition of the circumstellar-envelope",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The chemical composition of the circumstellar envelopes around yellow hypergiant stars . Abstract : We report new studies and investigation of the infrared emission lines in the spectra of two yellow hypergiants , IRC + 10420 and AFGL2136 .We see that these objects have very high mass - loss rates ( 10 ^ - 6 to 10 ^ - 5 Msun / yr ) with outflow velocities ranging between 100 - 200 kilometers / sec . The observed line profiles are compatible with an expanding shell model for the wind .In addition we find various forbidden transitions which demonstrate the presence of highly ionized species such as Fe + , Si + + , S + + . These ions may be formed by photoionization or collisional ionization processes within the stellar winds .Keywords : Yellow Hypergiants ; Circumstellar Envelopes ; Mass loss rate ; Outflows ; Emission Lines ; IRAS 08544 - 4431 . Astronomy & Astrophysics manuscript no .aa20031118 May 31, 2003 \nThe chemical composition of the circumstellar-envelope",
        "rewrite_text": "Title: The Chemical Composition of Circumstellar Envelopes Surrounding Yellow Hypergiant Stars\n\nAbstract: In this study, we present a detailed analysis of the infrared emission lines observed in the spectra of two prominent yellow hypergiants, IRC + 10420 and AFGL 2136. Our findings reveal that these stellar objects exhibit exceptionally high mass-loss rates, estimated between 10^-6 to 10^-5 solar masses per year, accompanied by outflow velocities that range from 100 to 200 kilometers per second. The spectral line profiles we have observed align well with the predictions of an expanding shell model, which is indicative of the dynamics of the stellar winds emanating from these hypergiants. Furthermore, our investigation has identified several forbidden transitions that suggest the presence of highly ionized species, including Fe+, Si++, and S++. The formation of these ions is likely attributed to either photoionization or collisional ionization processes occurring within the stellar winds. This research contributes to our understanding of the chemical composition and physical processes at play in the circumstellar environments of yellow hypergiants, which are critical for comprehending their evolutionary stages and the impact they have on their surrounding interstellar medium. The implications of these findings extend to the broader field of stellar astrophysics, particularly in the study of massive stars and their role in galactic chemical enrichment. Our work underscores the importance of continued observational efforts in this area, as well as the need for further theoretical modeling to elucidate the complex interactions occurring in the circumstellar envelopes of these extraordinary stars. \n\nKeywords: Yellow Hypergiants; Circumstellar Envelopes; Mass Loss Rate; Outflows; Emission Lines; IRAS 08544-4431. \n\nAstronomy & Astrophysics manuscript no. aa20031118, May 31, 2003.",
        "ori-fast-z-score": 0.2672612419124244,
        "water-fast-z-score": 3.0464244212496006,
        "rewrite-fast-z-score": 1.584236068762679
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Transition from the First Stars to the Second Stars in the Early Universe .\nAbstract:\nWe present an analysis of the transition between first stars and second stars, which are formed by gravitational collapse of primordial gas clouds with masses ranging from 10^4 M_sol to 10^6 M_sol.  We show that the formation rate of second stars is suppressed at redshifts z < 20 due to photoheating effects on the intergalactic medium (IGM). The suppression factor increases as redshift decreases because the IGM temperature rises more rapidly than its density. At lower redshifts, we find that the formation rates of both first and second stars increase sharply when the universe becomes reionized. This effect occurs because the ionizing photons produced during reionization heat up the surrounding neutral hydrogen atoms, thereby increasing their Jeans mass and suppressing fragmentation into smaller objects. Finally, we estimate the number densities of first and second stars using our model for star formation history. Our results suggest that second stars may be detectable via future surveys such as LSST or Euclid.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Transition from the First Stars to the Second Stars in the Early Universe . Abstract : We present an assessment of the shift between first stars and second stars , which are created by gravitational collapse of primordial liquid clouds with masses ranging from 10 ^ 4 M _ sol to 10 ^ 6 M _ sol .We see that the formation rate of second stars is suppressed at redshifts z < 20 due to photoheating effects on the intergalactic medium ( IGM ) . The suppression factor rises as redshift decreases because the IGM temperature rises more swiftly than its density .At lower redshifts , we find that the formation rates of both first and first stars increase dramatically when the universe becomes reionized . This phenomenon occurs because the ionizing photons created during reionization heat up the nearby neutral hydrogen atoms , thereby expanding their Jeans mass and suppressing fragmentation into larger objects .Finally , we estimate the number densities of first and first stars using our model for star formation history . Our results propose that second stars would be detectable via upcoming studies such as LSST or Euclid .",
        "rewrite_text": "In this article, we explore the transition from the first generation of stars to the second generation in the early universe, focusing on the gravitational collapse of primordial liquid clouds with masses between \\(10^4 M_{\\odot}\\) and \\(10^6 M_{\\odot}\\). Our analysis reveals that the formation rate of second stars is significantly hindered at redshifts \\(z < 20\\), primarily due to the effects of photoheating on the intergalactic medium (IGM). As redshift decreases, the suppression factor for second star formation increases, attributed to the rapid rise in IGM temperature outpacing the increase in density. \n\nAt lower redshifts, we observe a dramatic increase in the formation rates of both first and second stars coinciding with the reionization of the universe. This increase can be explained by the ionizing photons generated during the reionization process, which heat the surrounding neutral hydrogen atoms. This heating effect leads to an expansion of the Jeans mass, thereby inhibiting the fragmentation of gas into larger stellar objects. \n\nFurthermore, we provide estimates of the number densities of first and second stars based on our model of star formation history. Our findings suggest that second stars, which are crucial for understanding the evolution of cosmic structures, could be detected in future astronomical surveys, such as those conducted by the Large Synoptic Survey Telescope (LSST) or the Euclid mission. This research contributes to a deeper understanding of the early universe's stellar population and the mechanisms that govern star formation during its formative epochs.",
        "ori-fast-z-score": -0.3333333333333333,
        "water-fast-z-score": 4.500334659577304,
        "rewrite-fast-z-score": 1.1547005383792515
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Boundary Conditions of the Heliosphere: Photoionization Models Constrained by Interstellar and In Situ Data .\nAbstract:\nWe present photoionization models for the heliosheath, which is the region between the termination shock (TS) at ~100 AU and the solar wind termination shock (SWTS). The SWTS is located beyond 1 AU in the solar system s frame but within 0.3 AU in the rest frame of the Sun. We use these models to constrain the boundary conditions of the heliosphere using interstellar neutral hydrogen data obtained with the Lyman-alpha instrument on board the Solar Wind Anisotropy Probe (SWAP), as well as in situ measurements made near Earth during the Voyager 2 mission. Our results show that the TS distance decreases with increasing solar activity; this effect can be explained by an increase in the density of the solar wind plasma. For low solar activity levels we find that the TS distance agrees very well with previous estimates based on observations of energetic particles.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Boundary Conditions of the Heliosphere : Photoionization Models Constrained by Interstellar and In Situ Data . Abstract : We create photoionization estimates for the heliosheath , which is the region between the termination shock ( TS ) at ~ 100 AU and the planetary wind termination shock ( SWTS ) .The SWTS is situated beyond 1 AU in the solar system s frame but within 0 . 3 AU in the remainder frame of the Sun . We use these models to constrain the boundary pressures of the heliosphere utilizing interstellar neutral hydrogen evidence derived with the Lyman - alpha instrument on board the Solar Wind Anisotropy Probe ( SWAP ) , as well as in situ measurements made near Earth during the Voyager 2 mission .Our results show that the TS distance decreases with rising sun activity ; this effect can be described by an increase in the density of the solar wind plasma . For lowest sun activity rates we find that the TS distance agrees very best with previous accounts based on observations of energetic particles .",
        "rewrite_text": "In this study, we present a comprehensive analysis of photoionization models applied to the heliosheath, the region situated between the termination shock (TS) at approximately 100 astronomical units (AU) and the solar wind termination shock (SWTS). The SWTS is located beyond 1 AU in the solar system's reference frame, yet within 0.3 AU when viewed from the Sun's frame of reference. Our research utilizes these photoionization models to derive boundary pressure estimates for the heliosphere, leveraging interstellar neutral hydrogen data obtained from the Lyman-alpha instrument aboard the Solar Wind Anisotropy Probe (SWAP) and in situ measurements collected during the Voyager 2 mission near Earth. \n\nOur findings indicate a notable correlation between solar activity and the distance of the termination shock, revealing that as solar activity increases, the TS distance diminishes. This phenomenon can be attributed to a corresponding rise in the density of solar wind plasma. Furthermore, during periods of minimal solar activity, our measurements of the TS distance align closely with previous observations related to energetic particles, reinforcing the validity of our models. This research not only enhances our understanding of the heliosphere's boundary conditions but also provides critical insights into the dynamic interactions between solar wind and interstellar medium, which are essential for comprehending the broader implications of solar activity on space weather and its effects on planetary environments. Overall, our study contributes valuable data to the ongoing discourse on heliospheric physics and the intricate balance of forces at play in this unique region of space.",
        "ori-fast-z-score": -0.4923659639173309,
        "water-fast-z-score": 3.542913886332851,
        "rewrite-fast-z-score": -0.2773500981126145
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Bipolar spintronics: From spin injection to spin-controlled logic .\nAbstract:\nSpin-based electronics is an emerging field that has attracted considerable attention in recent years, due to its potential for applications such as high-density data storage and high-speed information processing.  In this review article we discuss the basic concepts underlying bipolar spintronic devices based on semiconductor heterostructures with ferromagnetic contacts. We first introduce the physics behind spin injection into semiconductors using tunnel barriers or Schottky diodes. Then we describe how these injected spins can be manipulated by means of external magnetic fields and/or electric currents. Finally, we present some examples of spintronic devices including spin-LEDs, spin transistors, and spin-logic circuits. The main focus will be put on GaAs-based structures but also other materials systems are discussed briefly. This article is intended to provide a comprehensive overview of the state-of-the-art research in the field of bipolar spintronics. It should serve both as a guide for newcomers interested in learning about the fundamentals of spin transport phenomena at interfaces between metals and semiconductors, and as a reference source for researchers working in related areas.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Bipolar spintronics : From spin injection to spinning - controlled logic . Abstract : Spin - based computing is an developing field that has garnered considerable scrutiny in recent years , owing to its potential for applications such as high - density data storage and large - speed information processing .In this review article we explain the fundamental concepts governing bipolar spintronic systems relying on semiconductor heterostructures with ferromagnetic contacts . We first introduce the physics behind spin injection into semiconductors using tunnel fences or Schottky diodes .Then we explain how these injected spinning can be manipulated by means of external magnetic fields and / or electric currents . Finally , we present some examples of spintronic systems including spin - LEDs , spin transistors , and spin - logic devices .The main center will be place on GaAs - based structures but also other materials models are discussed briefly . This page is intended to provide a comprehensive overview of the state - of - the - art studies in the field of bipolar spintronics .It should serve both as a reference for newcomers interested in understanding about the fundamentals of spin transport phenomena at connections between metals and semiconductors , and as a reference source for researchers focusing in related fields .",
        "rewrite_text": "**Title:** Bipolar Spintronics: From Spin Injection to Spin-Controlled Logic\n\n**Abstract:** Spin-based computing is an emerging area of research that has attracted significant attention in recent years due to its promising applications in high-density data storage and high-speed information processing. This review article aims to elucidate the core principles underlying bipolar spintronic systems, which utilize semiconductor heterostructures integrated with ferromagnetic contacts. We begin by discussing the fundamental physics of spin injection into semiconductors, employing mechanisms such as tunnel barriers and Schottky diodes. Following this, we explore the methods by which the injected spins can be manipulated through external magnetic fields and electric currents, highlighting the versatility of spin control in these systems. Additionally, we provide illustrative examples of various spintronic devices, including spin light-emitting diodes (spin-LEDs), spin transistors, and spin-logic devices. While the primary focus is on gallium arsenide (GaAs)-based structures, we also briefly examine alternative material systems to offer a broader perspective. This article serves as a comprehensive overview of the current advancements in bipolar spintronics, aiming to be a valuable resource for newcomers seeking to grasp the essential concepts of spin transport phenomena at the interfaces between metals and semiconductors, as well as for researchers engaged in related disciplines. Through this synthesis of knowledge, we hope to foster further exploration and innovation in the field of spintronic technologies.",
        "ori-fast-z-score": 0.39605901719066977,
        "water-fast-z-score": 5.883484054145521,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Recovering galaxy star formation and metallicity histories from spectra using VESPA .\nAbstract:\nWe present the results obtained by applying the semi-analytic code VESPA to model the evolution of galaxies in the Millennium Simulation, including chemical enrichment as well as dust extinction effects on their observed properties.  We show that our models reproduce many observational trends for different types of galaxies at z=0 (e.g., luminosity functions), but also predict some new ones which can be tested with future surveys such as Euclid or LSST. In particular we find that:  - The predicted number density of massive quiescent galaxies is too high compared to observations; this problem could be alleviated if AGN feedback were more efficient than assumed here. - Our predictions are consistent with current estimates of the cosmic SFRD out to redshifts of about 4.5, although they tend to overpredict it slightly beyond that redshift. - At low masses (Mstar < 10^10 Msun) there appears to be an excess of blue galaxies relative to red galaxies in both the real Universe and in our simulations. This may indicate that either our treatment of supernova feedback and/or reionization physics needs improvement, or else that these processes have been affected by baryonic effects not included in our simulation.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Recovering galaxy star formation and metallicity histories from spectra using VESPA . Abstract : We present the results derived by using the semi - analytic coding VESPA to model the evolution of stars in the Millennium Simulation , covering chemical enrichment as well as dust extinction effects on their observed properties .We see that our models reproduce many observational changes for different kinds of stars at z = 0 ( e . g . , luminosity functions ) , but also predict some additional ones which can be evaluated with current surveys such as Euclid or LSST . In particular we find that : - The predicted number density of large quiescent galaxies is too high compared to observations ; this challenge may be alleviated if AGN feedback were more efficient than implied here .- Our predictions are compatible with current estimates of the cosmic SFRD out to redshifts of about 4 . 5 , although they tend to overpredict it somewhat beyond that redshift . - At small masses ( Mstar < 10 ^ 10 Msun ) there seems to be an accumulation of blue galaxies compared to blue galaxies in both the real Universe and in our simulations .This might suggest that either our treatment of supernova feedback and / or reionization theory requires improvement , or otherwise that these mechanisms have been affected by baryonic effects not involved in our modeling .",
        "rewrite_text": "In this study, we utilize the semi-analytic code VESPA to investigate the evolution of stellar populations as simulated in the Millennium Simulation, focusing on aspects such as chemical enrichment and the impact of dust extinction on observable properties. Our findings indicate that the models generated by VESPA successfully replicate a variety of observational trends for different stellar types at redshift z = 0, including luminosity functions. However, we also identify several discrepancies that could be addressed through ongoing observational efforts, particularly with upcoming surveys like Euclid and LSST.\n\nNotably, our analysis reveals that the predicted number density of large quiescent galaxies is significantly higher than what is observed, suggesting that the efficiency of AGN feedback may need to be reconsidered in our models. Furthermore, our predictions align with current estimates of the cosmic Star Formation Rate Density (SFRD) up to redshifts of approximately 4.5, although they tend to overestimate the SFRD at higher redshifts. \n\nAdditionally, we observe an unexpected accumulation of blue galaxies at lower mass ranges (Mstar < 10^10 Msun) when compared to both the actual universe and our simulations. This discrepancy raises questions about the adequacy of our supernova feedback treatment and the assumptions made regarding reionization theory. It may also indicate that baryonic processes, which are not fully captured in our modeling, could be influencing these outcomes. Overall, our results highlight the need for further refinement in our theoretical frameworks to better align with observational data and enhance our understanding of galaxy formation and evolution.",
        "ori-fast-z-score": -1.4439897447623107,
        "water-fast-z-score": 5.157106231293967,
        "rewrite-fast-z-score": 0.29851115706299675
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Charm-quark fragmentation with an effective coupling constant .\nAbstract:\nWe study the charm quark fragmentation into hadrons in perturbative QCD by using an effective coupling constant, which is determined so as to reproduce the experimental data on the total cross section for e+e-annihilation into hadrons at high energies.  We find that our results are consistent with those obtained within the framework of the conventional parton model and also with recent measurements performed by CLEO collaboration. The present analysis suggests that the charm quark fragmentation function can be well described by the Peterson form factor multiplied by a simple exponential function. \nPACS numbers: 11.10.Kk, 12.38.Qk, 13 .60.Hb \nI. INTRODUCTORY REMAR K\nIn this work we investigate the charm quark fragmentation functions (FFs) into light hadrons in perturbative quantum chromodynamics(pQCD). In particular, we use an effective coupling constant, αeff , which is determined so as t o reproduce the experimental data on thetotal cross section for e + e-annihilation into hadronsthat have been measured recentlybyCLEOcollaboration  1  .\nThe FFsof quarksintohadronicparticlesare important quantitiesin pQCDand they playanimportant role inthe calculationof various physical observables suchas the structurefunctions  2  ,the Drell-Yanprocess  3  ,and the semi-leptonic decay  4  . It hasbeen shownthat the FFsof quarksintohadronscan be calculatedperturbatively  5  -  8  . However, it shouldbe notedthatthe calculationsrequire veryhighenergy scales  9  . Therefore,it would be usefulto determineαefffromexperimentaldataat relativelylowenergies  10  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Charm - quark fragmentation with an efficient coupling constant . Abstract : We research the charm quark fragmentation into hadrons in perturbative QCD by using an effective coupling constant , which is calculated so as to reproduce the empirical data on the total cross section for e + e - annihilation into hadrons at high energies .We see that our findings are compatible with those achieved within the framework of the usual parton theory and also with recent observations performed by CLEO collaboration . The present evaluation suggests that the charm quark fragmentation relation can be well described by the Peterson form parameter multiplied by a simple exponential function .PACS codes : 11 . 10 . Kk , 12 . 38 . Qk , 13 . 60 . Hb I . INTRODUCTORY REMAR K In this study we investigate the charm quark fragmentation curves ( FFs ) into light hadrons in perturbative molecular chromodynamics ( pQCD ) .In particular , we using an efficient coupling constant , αeff , which is calculated so as t o reproduce the empirical data on thetotal cross section for e + e - annihilation into hadronsthat have been measured recentlybyCLEOcollaboration 1 . The FFsof quarksintohadronicparticlesare crucial quantitiesin pQCDand they playanimportant importance inthe calculationof various mechanical observables suchas the structurefunctions 2 , the Drell - Yanprocess 3 , and the semi - leptonic decay 4 .It hasbeen shownthat the FFsof quarksintohadronscan be calculatedperturbatively  5  -  8  .However, it shouldbe notedthatthe calculationsrequire veryhighenergy scales  9  .Therefore,it would be usefulto determineαefffromexperimentaldataat relativelylowenergies  10  .",
        "rewrite_text": "**Title: Charm Quark Fragmentation with an Efficient Coupling Constant**\n\n**Abstract:** This study delves into the fragmentation of charm quarks into hadrons within the framework of perturbative Quantum Chromodynamics (pQCD). We employ an effective coupling constant, denoted as α_eff, which is specifically calculated to align with empirical data on the total cross section for electron-positron (e⁺e⁻) annihilation into hadrons at high energies, as recently measured by the CLEO collaboration. Our results demonstrate a strong compatibility with findings derived from conventional parton theory, as well as with the latest experimental observations. The analysis indicates that the fragmentation function (FF) for charm quarks can be accurately represented by the Peterson form, modified by a straightforward exponential function. \n\nFragmentation functions are essential in pQCD, serving as critical components in the computation of various physical observables, including structure functions, the Drell-Yan process, and semi-leptonic decays. Previous studies have shown that quark fragmentation into hadronic particles can be computed perturbatively; however, these calculations typically necessitate very high energy scales. Consequently, our approach aims to derive the effective coupling constant from experimental data collected at relatively lower energy levels, thereby enhancing the accessibility and applicability of our findings. This research not only contributes to the theoretical understanding of charm quark dynamics but also provides a framework for future investigations into hadronization processes in high-energy physics. \n\n**PACS codes:** 11.10.Kk, 12.38.Qk, 13.60.Hb",
        "ori-fast-z-score": -1.585187847802434,
        "water-fast-z-score": 5.813776741499453,
        "rewrite-fast-z-score": 0.7035264706814485
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Covering Homology .\nAbstract:\nWe introduce the notion of covering homology, which is an extension of ordinary homology to infinite graphs and simplicial complexes. The main idea behind this concept is that we consider not only finite subcomplexes but also their infinite analogues - so-called coverings. We show how covering homology can be used for studying properties of infinite graphs such as amenability or hyperbolicity. In particular, we prove that if a graph has infinitely many ends then its first covering homology group vanishes. This result generalizes the classical theorem by Stallings on groups with infinitely many ends. Covering homology provides us with new tools for investigating various classes of infinite objects in mathematics. It turns out that it is closely related to other concepts like equivariant homology theory (in the case when our object admits actions of some group) or higher-order algebraic K-theory. Finally, we present several examples illustrating applications of covering homology.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Covering Homology . Abstract : We introduce the notion of covering homology , which is an extension of regular homology to infinite graphs and simplicial structures .The main idea behind this concept is that we treat not only finite subcomplexes but also their infinite analogues - so - called coverings . We see how covering homology can be used for studying characteristics of infinite graphs such as amenability or hyperbolicity .In particular , we prove that if a graph has infinitely many ends then its initial covering homology group vanishes . This result generalizes the classical principle by Stallings on groups with infinitely many ends .Covering homology provides us with novel techniques for investigating different categories of infinite concepts in mathematics . It turns out that it is closely related to other concepts like equivariant homology theory ( in the case when our object accepts actions of some group ) or greater - order algebraic K - theory .Finally , we present many examples illustrating applications of covering homology .",
        "rewrite_text": "In this article, we introduce the concept of covering homology, which serves as an extension of traditional homology to accommodate infinite graphs and simplicial structures. The fundamental premise of covering homology is the inclusion of not only finite subcomplexes but also their infinite counterparts, referred to as coverings. This innovative approach allows for a deeper exploration of the properties of infinite graphs, particularly in relation to characteristics such as amenability and hyperbolicity. \n\nOne of the key findings presented in this work is that if a graph possesses infinitely many ends, then its initial covering homology group is shown to be trivial. This result extends the classical theorem established by Stallings concerning groups that exhibit infinitely many ends, thereby providing a broader context for understanding the implications of infinite structures in algebraic topology.\n\nMoreover, covering homology reveals new methodologies for examining various categories of infinite mathematical concepts. It demonstrates significant connections to other established theories, including equivariant homology theory, which applies when the object in question is subject to group actions, as well as higher-order algebraic K-theory. \n\nTo illustrate the practical applications of covering homology, we provide a range of examples that highlight its utility in different mathematical contexts. Through these examples, we aim to showcase the versatility and relevance of covering homology in contemporary mathematical research, paving the way for further exploration and application in the study of infinite structures. Overall, this article contributes to the ongoing discourse in algebraic topology and offers a fresh perspective on the analysis of infinite graphs and their properties.",
        "ori-fast-z-score": 0.47140452079103173,
        "water-fast-z-score": 3.9524197172898554,
        "rewrite-fast-z-score": -0.17407765595569785
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Measuring the spin polarization and Zeeman energy of a spin-polarized electron gas: Comparison between Raman scattering and photoluminescence .\nAbstract:\nWe report on an experimental study of the spin-dependent properties of a two-dimensional electron system in GaAs/AlGaAs quantum wells by means of polarized light scattering (PL) and magneto-optical spectroscopy (Raman). The PL experiments are performed at low temperatures, where we observe circularly polarized emission with opposite helicity for right- and left-circularly polarized excitation. We show that this effect is due to the presence of a strong exchange interaction which leads to a splitting of the excitonic states into two branches corresponding to different spins of the carriers. In addition, we find that the degree of circular polarization depends strongly on temperature and magnetic field strength. By comparing our results with those obtained by polarized Raman scattering under similar conditions, we demonstrate that both techniques provide complementary information about the electronic structure of the investigated samples. Our findings can be used as input parameters for theoretical models describing the transport phenomena in semiconductor nanostructures.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Measuring the spin polarization and Zeeman power of a spin - polarized electron gas : Comparison between Raman scattering and photoluminescence . Abstract : We report on an research study of the spin - dependent characteristics of a two - dimensional electron structure in GaAs / AlGaAs quantum wells by means of polarized light scattering ( PL ) and magneto - optical spectroscopy ( Raman ) .The PL observations are performed at low temperatures , where we study circularly polarized emission with opposite helicity for right - and left - circularly polarized excitation . We see that this effect is due to the presence of a powerful exchange interaction which results to a dividing of the excitonic states into two branches resulting to different spinning of the carriers .In addition , we find that the degree of circular polarization depends strongly on temperature and magnetic field intensity . By matching our findings with those achieved by polarized Raman absorption under related circumstances , we prove that both approaches provide complementary information about the electronic structure of the investigated samples .Our findings can be used as input parameters for theoretical theories describing the transport phenomena in semiconductor nanostructures .",
        "rewrite_text": "In this study, we investigate the spin-dependent properties of a two-dimensional electron gas in GaAs/AlGaAs quantum wells utilizing polarized light scattering (photoluminescence, PL) and magneto-optical spectroscopy (Raman scattering). Our experiments are conducted at low temperatures, focusing on the circularly polarized emissions resulting from right- and left-circularly polarized excitations. The observed circular polarization is attributed to a significant exchange interaction that leads to the splitting of excitonic states into two distinct branches, each corresponding to different spin orientations of the carriers. Furthermore, we demonstrate that the degree of circular polarization is highly sensitive to variations in temperature and the intensity of the magnetic field. By correlating our PL results with those obtained from polarized Raman absorption under similar conditions, we establish that both techniques yield complementary insights into the electronic structure of the quantum wells studied. These findings not only enhance our understanding of spin polarization and Zeeman effects in semiconductor nanostructures but also provide valuable parameters for theoretical models aimed at describing transport phenomena in these systems. Our research contributes to the broader field of spintronics, where the manipulation of electron spins is crucial for the development of next-generation electronic devices.",
        "ori-fast-z-score": -1.7253243712550146,
        "water-fast-z-score": 3.5379713649647226,
        "rewrite-fast-z-score": 1.0314212462587933
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  WHAM: A WENO-based general relativistic numerical scheme I: Hydrodynamics .\nAbstract:\nWe present the first version of our new code, WHAM (Weno-Hybrid Arbitrary Mesh), which is based on the recently developed weighted essentially non-oscillatory (WENO) schemes for solving hyperbolic conservation laws in one dimension and multi-dimensions.  The main idea behind this method is to use high-order spatial reconstruction with an adaptive mesh refinement technique to achieve high accuracy at low computational cost. We have implemented several different versions of the WENO algorithm into our code including the fifth-order WENO-Z scheme as well as the third- and fifth-order WENO-JS schemes. In addition we also implement the fourth-order Runge-Kutta time integration scheme along with the Harten-Lax-van Leer contact discontinuity capturing scheme to handle discontinuities arising during hydrodynamic evolution. Our results show that all these algorithms are able to produce accurate solutions when compared against exact or reference solutions.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : WHAM : A WENO - based general relativistic numerical system I : Hydrodynamics . Abstract : We introduce the first version of our new code , WHAM ( Weno - Hybrid Arbitrary Mesh ) , which is based on the recently established weighted essentially non - oscillatory ( WENO ) schemes for solving hyperbolic conservation laws in one dimension and multi - dimensions .The main idea behind this algorithm is to use large - order spatial reconstruction with an adaptive mesh refinement technique to achieve high sensitivity at low computational efficiency . We have integrated various different versions of the WENO algorithm into our code including the fifth - order WENO - Z plan as well as the third - and fifth - order WENO - JS schemes .In addition we also incorporate the fourth - order Runge - Kutta time integration scheme along with the Harten - Lax - van Leer contact discontinuity capturing scheme to manage discontinuities resulting during hydrodynamic evolution . Our results show that all these algorithms are able to produce accurate solutions when compared against exact or reference solutions .",
        "rewrite_text": "We present the initial version of our innovative computational code, WHAM (Weno-Hybrid Arbitrary Mesh), designed for general relativistic hydrodynamics. This code leverages the recently developed weighted essentially non-oscillatory (WENO) schemes, which are effective for solving hyperbolic conservation laws across both one-dimensional and multi-dimensional frameworks. The core concept of WHAM revolves around employing high-order spatial reconstruction in conjunction with an adaptive mesh refinement strategy, enabling the code to maintain exceptional accuracy while optimizing computational efficiency. \n\nIn our implementation, we have integrated several variants of the WENO algorithm, including the fifth-order WENO-Z scheme and both third- and fifth-order WENO-JS schemes. To enhance the temporal accuracy of our simulations, we have also incorporated a fourth-order Runge-Kutta time integration method. Furthermore, we utilize the Harten-Lax-van Leer (HLL) scheme to effectively capture contact discontinuities that may arise during the hydrodynamic evolution of the system. \n\nOur comprehensive testing demonstrates that the algorithms embedded within WHAM yield highly accurate solutions, consistently aligning with exact or reference solutions across various scenarios. This capability underscores the potential of WHAM as a robust tool for tackling complex hydrodynamic problems in the context of general relativity. The development of WHAM marks a significant advancement in numerical methods for astrophysical simulations, paving the way for future research and applications in this critical field.",
        "ori-fast-z-score": 1.9188064472004938,
        "water-fast-z-score": 4.9819900360298925,
        "rewrite-fast-z-score": 0.1889822365046136
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Tilt-angle landscapes and temperature dependence of the conductance in biphenyl-dithiol single-molecule junctions .\nAbstract:\nWe report on electrical transport measurements through individual biphenyl dithiol molecules connected to gold electrodes by using scanning tunneling microscopy break-junction technique at room-temperature (RT) and low temperatures down to 4 K. The current–voltage characteristics show clear signatures for molecular conduction, such as Coulomb blockade peaks and negative differential resistance regions. We find that the junction conductance depends strongly on the tilt angle between the molecule backbone and the substrate normal. This is explained by an anisotropic coupling strength between the molecule and the metal contacts which leads to different transmission probabilities along the two main axes of the molecule. In addition we observe a strong temperature dependence of the junction conductance with a maximum around 50 K. These results are discussed within a model based on electron-phonon scattering processes. Biphenyl dithiol (BDT), one of the most studied organic semiconductors, has been used extensively in field-effect transistors1-5 and other electronic devices6-8 due to its high charge carrier mobility9-11. However, despite intensive research efforts12-17 only few studies have investigated the electrical properties of BDT-based single-molecule junctures18-20. Herein, we present detailed investigations of the electrical transport behavior of individual BDT molecules contacted via Au(111) surfaces by means of scanning tunneling microscopy breakjunction experiments21-23. Our data reveal pronounced features characteristic for molecular conduction24-26 including Coulomb blockade peaks27-29 and negative differential resistance30-32. Furthermore, our study reveals a strong influence of the molecular orientation relative to the substrate surface on the junction conductance33-35. Finally, we demonstrate a significant temperature dependent change of the junction conductance36-38.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Tilt - angle landscapes and heat dependence of the conductance in biphenyl - dithiol single - molecule junctions . Abstract : We report on electrical transport measurements through individual biphenyl dithiol molecules linked to gold electrodes by using scan tunneling microscopy break - junction technique at room - temperature ( RT ) and low temperatures down to 4 K . The current – voltage parameters show visible signatures for molecular conduction , such as Coulomb blockade spikes and negative integral resistance regions .We see that the junction conductance depends strongly on the tilt distance between the molecule backbone and the substrate normal . This is explained by an anisotropic interaction strength between the molecule and the metal links which results to different communication probabilities along the two principal axes of the molecule .In addition we study a weak thermal dependence of the junction conductance with a maximum around 50 K . These data are discussed within a theory based on electron - phonon absorption processes . Biphenyl dithiol ( BDT ) , one of the most studied organic semiconductors , has been used heavily in field - effect transistors1 - 5 and other electronic devices6 - 8 due to its high charge carrier mobility9 - 11 .However , despite intensive study efforts12 - 17 only few researchers have researched the electrical properties of BDT - based single - cell junctures18 - 20 . Herein , we present detailed investigations of the electrical travel dynamics of individual BDT atoms contacted via Au ( 111 ) surfaces by means of scanning tunneling microscopy breakjunction experiments21 - 23 .Our data reveal pronounced features characteristic for molecular conduction24 - 26 including Coulomb blockade peaks27 - 29 and negative differential resistance30 - 32 . Furthermore , our research reveals a large influence of the molecular orientation relative to the substrate surface on the junction conductance33 - 35 .Finally , we obtain a substantial heat dependent change of the junction conductance36 - 38 .",
        "rewrite_text": "We present a comprehensive study on the electrical transport properties of individual biphenyl dithiol (BDT) molecules connected to gold electrodes, utilizing the scanning tunneling microscopy break-junction technique. Our experiments were conducted at room temperature (RT) and at low temperatures, down to 4 K. The current-voltage characteristics exhibit distinct features indicative of molecular conduction, including Coulomb blockade spikes and regions of negative differential resistance. Notably, we observe a significant dependence of junction conductance on the tilt angle between the molecular backbone and the substrate normal. This phenomenon can be attributed to the anisotropic interaction strength between the BDT molecules and the metal contacts, leading to varying probabilities of charge transport along the two principal axes of the molecule.\n\nAdditionally, we investigate the thermal dependence of the junction conductance, which reveals a weak temperature effect with a peak around 50 K. This behavior is analyzed within the framework of a theoretical model that considers electron-phonon absorption processes. BDT, recognized as one of the most extensively studied organic semiconductors, has been widely employed in field-effect transistors and various electronic devices due to its high charge carrier mobility. Despite the extensive research on BDT, there has been limited exploration of its electrical properties in single-molecule junctions.\n\nIn this work, we provide detailed insights into the electrical transport dynamics of individual BDT molecules in contact with Au(111) surfaces, employing scanning tunneling microscopy break-junction experiments. Our findings reveal pronounced signatures characteristic of molecular conduction, including Coulomb blockade peaks and negative differential resistance. Furthermore, we demonstrate that the orientation of the molecule relative to the substrate significantly influences the junction conductance. Ultimately, our research highlights the substantial impact of thermal effects on junction conductance, contributing to the understanding of BDT-based single-molecule electronic devices.",
        "ori-fast-z-score": 1.0215078369104984,
        "water-fast-z-score": 6.821430560638804,
        "rewrite-fast-z-score": 0.48989794855663565
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Strain localization in a shear transformation zone model for amorphous solids .\nAbstract:\nWe study the mechanical response to uniaxial compression and extension of an elastic-plastic continuum model with a nonlinear kinematic hardening rule, which is motivated by recent experiments on metallic glasses. The plastic deformation mechanism consists of localized shear-transformation zones (STZs) that are randomly distributed throughout the sample volume at zero stress. We show that this simple model can reproduce many features observed experimentally such as serrated yielding, strain softening after yield, and pronounced anisotropy during flow. In particular we find that the STZ density increases rapidly near the onset of plasticity but then decreases slowly until it reaches its steady-state value. This behavior leads to a characteristic peak in the macroscopic stress-strain curve just before the final collapse into a densely packed state. Finally, we discuss how our results relate to previous theoretical work based on mean-field approximations. DOI: 10.1103/PhysRevE.77.020101",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Strain localization in a shear transformation zone model for amorphous solids . Abstract : We research the mechanical response to uniaxial compression and extension of an elastic - plastic continuum design with a nonlinear kinematic hardening rule , which is prompted by recent experiments on metallic glasses .The plastic deformation process involves of localized shear - change zones ( STZs ) that are randomly distributed throughout the sample volume at zero strain . We see that this straightforward model can generate several characteristics studied experimentally such as serrated yielding , stress softening after yield , and sustained anisotropy during flow .In particular we find that the STZ capacity increases quickly near the beginning of plasticity but then grows slowly until it hits its steady - state value . This phenomenon gives to a typical peak in the macroscopic stress - strain graph just before the last collapse into a heavily packed state .Finally , we explain how our findings correspond to previous theoretical work using on mean - field approximations . DOI : 10 . 1103 / PhysRevE . 77 . 020101",
        "rewrite_text": "In this study, we investigate the mechanical behavior of an elastic-plastic continuum model under uniaxial compression and extension, inspired by recent experimental observations of metallic glasses. Our model incorporates a nonlinear kinematic hardening rule and focuses on the role of localized shear transformation zones (STZs), which are randomly distributed throughout the material at zero strain. Through our analysis, we demonstrate that this relatively simple model is capable of reproducing several key phenomena observed in experiments, including serrated yielding, stress softening post-yield, and persistent anisotropy during deformation.\n\nOne of the significant findings of our research is the rapid increase in STZ activity at the onset of plastic deformation, which subsequently slows down as it approaches a steady-state value. This behavior is reflected in the macroscopic stress-strain curve, where we observe a characteristic peak in stress just prior to the material transitioning into a densely packed state. This peak is indicative of the complex interplay between the evolving microstructure and the macroscopic mechanical response.\n\nFurthermore, we relate our results to existing theoretical frameworks, particularly those based on mean-field approximations, thereby providing a deeper understanding of the underlying mechanisms governing the mechanical response of amorphous solids. Our findings contribute to the broader knowledge of plasticity in disordered materials and offer insights that could inform the design and application of metallic glasses in various engineering contexts. Overall, this work enhances the understanding of strain localization phenomena in amorphous solids and lays the groundwork for future research in this area.",
        "ori-fast-z-score": -1.5460413650478515,
        "water-fast-z-score": 5.09786575873842,
        "rewrite-fast-z-score": 1.3750477455423171
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Evolution of Protoplanetary Disks Around Millisecond Pulsars: The PSR 1257 +12 System .\nAbstract:\nWe present new observations and analysis of the pulsar–disk system PSR 1257+12, which is one of only two known systems in which a millisecond radio pulsar orbits around an evolved star (a white dwarf). We have obtained high-resolution near-infrared images with adaptive optics at Keck Observatory that reveal a bright ring-like structure surrounding the pulsar s position. This feature has been interpreted as emission from dust grains orbiting within the disk produced by the supernova explosion that created both stars. \n \n In addition to this ring, we detect faint extended emission extending out to several hundred AU on either side of the central source. These features are consistent with previous results based on lower resolution data. However, our higher spatial resolution reveals additional details about these structures. For example, we find evidence for spiral arms in the outer regions of the disk, which may be caused by gravitational interactions between the disk and the binary companion.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Evolution of Protoplanetary Disks Around Millisecond Pulsars : The PSR 1257 + 12 System . Abstract : We report new studies and investigation of the pulsar – disk system PSR 1257 + 12 , which is one of only two known systems in which a millisecond radio pulsar orbits around an evolved star ( a white dwarf ) .We have discovered high - resolution near - infrared images with adaptive optics at Keck Observatory that indicate a bright ring - like structure surrounding the pulsar s position . This phenomenon has been viewed as emission from dust grains orbiting within the disk produced by the supernova explosion that created both stars .In addition to this belt , we perceive slight extended emission stretching out to several hundred AU on either side of the main source . These features are consistent with previous findings based on lower resolution data .However , our higher spatial resolution shows new details about these structures . For instance , we find proof for spiral arms in the exterior areas of the disk , which may be caused by gravitational interactions between the disk and the binary companion .",
        "rewrite_text": "Title: The Evolution of Protoplanetary Disks Around Millisecond Pulsars: The PSR 1257 + 12 System\n\nAbstract: In this study, we present our latest findings on the pulsar-disk system PSR 1257 + 12, one of only two known systems where a millisecond radio pulsar orbits an evolved star, specifically a white dwarf. Utilizing high-resolution near-infrared imaging with adaptive optics at the Keck Observatory, we have identified a prominent ring-like structure surrounding the pulsar's location. This structure is interpreted as emission from dust grains that are orbiting within a disk formed by the supernova explosion that resulted in the creation of both the pulsar and the white dwarf. Alongside this ring, we also observe faint extended emissions that stretch several hundred astronomical units (AU) on either side of the primary source. These observations align with previous studies conducted with lower resolution data; however, our high spatial resolution has unveiled new details regarding these features. Notably, we have detected evidence of spiral arms in the outer regions of the disk, which may be attributed to gravitational interactions between the disk and the binary companion star. This research enhances our understanding of the complex dynamics at play in protoplanetary disks around millisecond pulsars and contributes to the broader knowledge of stellar evolution and the aftermath of supernova events. Our findings underscore the significance of high-resolution imaging in revealing intricate structures within these astrophysical systems and provide a foundation for future investigations into the interactions between pulsars and their surrounding environments.",
        "ori-fast-z-score": 0.2182178902359924,
        "water-fast-z-score": 4.0976453817306595,
        "rewrite-fast-z-score": 0.8542421961772492
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Surface gap solitons at a nonlinearity interface .\nAbstract:\nWe study the propagation of surface waves in a layered medium with different signs of nonlinear coefficients and show that, under certain conditions, localized solutions exist which are similar to those known as gap solitons in periodic structures. The existence domain for these solutions is determined by solving an eigenvalue problem numerically. We also present results on the stability properties of such solutions against small perturbations. Surface wave localization can be observed experimentally using optical waveguide arrays or photonic crystals. In this work we consider the case when two layers have opposite signs of nonlinearities (e.g., one positive and another negative). This situation occurs naturally if the material parameters change sign across some interface between media. For example, it may happen near the boundary between materials with normal dispersion and anomalous dispersion. \n \n We demonstrate that there exists a class of localized solutions which resemble gap solitons in periodic systems. These solutions appear due to the interplay between linear and nonlinear effects. They exist only within a finite range of frequencies and decay exponentially away from their center point. Their amplitude depends strongly on the ratio of the amplitudes of the incident and reflected waves.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Surface gap solitons at a nonlinearity interface . Abstract : We explore the propagation of surface waves in a layered medium with various signs of nonlinear coefficients and find that , under certain conditions , confined solutions arise which are analogous to those known as gap solitons in periodic arrangements .The existence domain for these solutions is chosen by handling an eigenvalue problem numerically . We additionally give results on the stability properties of such problems against small perturbations .Surface wave localization can be viewed experimentally using optical waveguide arrays or photonic crystals . In this research we imagine the case when two layers have different signs of nonlinearities ( e . g . , one positive and another negative ) .This condition occurs naturally if the material variables alter sign across some interface between media . For instance , it could happen near the boundary between materials with normal dispersion and anomalous dispersion .We showed that there exists a class of localized solutions which mimic gap solitons in periodic systems . These solutions emerge due to the interplay between linear and nonlinear effects .They arise only within a finite range of frequencies and decay exponentially back from their center position . Their amplitude depends strongly on the proportion of the amplitudes of the incident and reflected waves .",
        "rewrite_text": "In this study, we investigate the behavior of surface waves within a layered medium characterized by nonlinear coefficients of varying signs. Our findings reveal that, under specific conditions, localized solutions emerge that are analogous to gap solitons observed in periodic structures. To determine the existence domain of these solutions, we employ a numerical approach to solve an eigenvalue problem. Furthermore, we analyze the stability of these solutions in the presence of small perturbations, providing insights into their robustness.\n\nThe experimental observation of surface wave localization can be facilitated through the use of optical waveguide arrays or photonic crystals. Our research focuses on a scenario where two layers exhibit opposing nonlinearities—specifically, one layer has a positive nonlinearity while the other has a negative one. This situation is commonly encountered at the interface between materials with differing dispersion characteristics, such as normal and anomalous dispersion.\n\nWe demonstrate the existence of a specific class of localized solutions that closely resemble gap solitons found in periodic systems. These solutions arise from the intricate interplay between linear and nonlinear effects and are confined to a finite frequency range. Notably, they exhibit an exponential decay away from their central position, indicating a localized nature. The amplitude of these solutions is significantly influenced by the ratio of the amplitudes of the incident and reflected waves, highlighting the delicate balance required for their formation. Overall, our findings contribute to the understanding of surface wave dynamics in nonlinear media and open avenues for potential experimental realizations.",
        "ori-fast-z-score": -1.2935483472729858,
        "water-fast-z-score": 4.278659917902954,
        "rewrite-fast-z-score": 0.2727272727272727
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Spectroscopy of Ultracold, Trapped Cesium Feshbach Molecules .\nAbstract:\nWe report on the observation and analysis of molecular bound states in ultracold cesium atoms trapped by an optical lattice potential. The molecules are created using magnetic field induced Feshbach resonances at temperatures below 1 microkelvin. We measure their binding energies as functions of both magnetic field strength and laser intensity. These measurements allow us to determine the scattering length between two fermionic atoms with high precision. In addition we observe that the molecule formation rate is strongly enhanced when the trapping lasers are detuned into resonance with excited vibrational levels of the atomic ground state. This effect can be explained by stimulated emission processes which lead to rapid relaxation towards deeply bound molecular states. Finally we demonstrate how these results can be used for precise determination of the s-wave scattering lengths between different spin species. Our work opens up new possibilities for studying quantum many-body phenomena such as superfluidity or supersolidity in systems of interacting fermions. \n \n We present experimental data obtained during our study of ultracold cesium (Cs) atoms confined within an optical lattice trap. Using magnetic field induced Feshback resonances we create weakly bound Cs2 dimer molecules out of pairs of fermionic atoms. By measuring the binding energy of the molecules as function of magnetic field strength and laser power density we obtain accurate values for the scattering length between two Cs atoms. Furthermore we find that the molecule formation process is strongly enhanced if the trapping lasers have a frequency close to one of the atomic transitions. This effect can be understood by considering stimulated emission processes leading to fast relaxation towards deeply bound molecular levels.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Spectroscopy of Ultracold , Trapped Cesium Feshbach Molecules . Abstract : We report on the observation and assessment of molecular bound states in ultracold cesium atoms trapped by an optical lattice potential .The molecules are created using magnetic field induced Feshbach resonances at pressures below 1 microkelvin . We estimate their binding energies as functions of both magnetic force speed and laser intensity .These measurements enable us to predict the scattering size between two fermionic atoms with high precision . In addition we determine that the molecule formed time is strongly enhanced when the trapping lasers are detuned into resonance with excited vibrational concentrations of the atomic ground state .This phenomenon can be described by stimulated emission mechanisms which lead to rapid relaxation towards deeply bound molecular states . Finally we prove how these results can be used for precise calculation of the s - wave scattering lengths between various spin species .Our study opens up new possibilities for studying molecular several - bodies phenomena such as superfluidity or supersolidity in systems of interacting fermions . We share experimental evidence derived during our research of ultracold cesium ( Cs ) atoms confined within an optical lattice trap .Using magnetic field induced Feshback resonances we create weakly bound Cs2 dimer molecules out of pairs of fermionic atoms . By measuring the binding energy of the molecules as function of magnetic field intensity and radiation energy density we obtain precise expressions for the scattering distance between two Cs atoms .Furthermore we find that the molecule formation reaction is strongly enhanced if the trapping lasers have a frequency close to one of the atomic transitions . This phenomenon can be understood by considering stimulated emission mechanisms leading to rapid relaxation towards deeply bound molecular levels .",
        "rewrite_text": "We present our findings on the spectroscopy of ultracold, trapped cesium Feshbach molecules, focusing on the observation and characterization of molecular bound states formed from ultracold cesium atoms confined within an optical lattice potential. Utilizing magnetic field-induced Feshbach resonances at temperatures below 1 microkelvin, we successfully create weakly bound Cs2 dimer molecules from pairs of fermionic cesium atoms. Our study involves estimating the binding energies of these molecules as functions of both the magnetic field strength and laser intensity, which allows us to derive precise predictions for the scattering lengths between two fermionic cesium atoms.\n\nMoreover, we observe a significant enhancement in the formation time of these molecules when the trapping lasers are detuned to resonate with excited vibrational states of the atomic ground state. This enhancement can be attributed to stimulated emission processes that facilitate rapid relaxation into deeply bound molecular states. Our experimental results provide compelling evidence for these phenomena and demonstrate their implications for accurately calculating s-wave scattering lengths among various spin species of cesium atoms.\n\nThe insights gained from this research not only deepen our understanding of molecular interactions in ultracold systems but also pave the way for exploring complex many-body phenomena, such as superfluidity and supersolidity, in systems of interacting fermions. By sharing our experimental data and theoretical interpretations, we aim to contribute to the broader field of ultracold atomic physics and foster further investigations into the rich dynamics of ultracold molecular systems. This work highlights the potential of using ultracold cesium atoms as a platform for studying fundamental quantum phenomena and opens new avenues for future research in quantum many-body physics.",
        "ori-fast-z-score": -0.3287979746107146,
        "water-fast-z-score": 6.411560504908935,
        "rewrite-fast-z-score": 0.086710996952412
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Spitzer Mid-to-Far-Infrared Flux Densities of Distant Galaxies .\nAbstract:\nWe present Spitzer Space Telescope observations in the mid- to far-infrared (5.8 - 160 microns) for a sample of distant galaxies selected by their rest-frame ultraviolet luminosity density at 1600 angstroms, and compare these flux densities with those predicted using models that include dust extinction.  We find that the observed infrared emission is generally higher than expected based on the UV continuum slope alone; this excess emission can be explained if there are significant amounts of cold dust associated with star formation activity in these systems.   The results suggest that the majority of the energy produced by young stars may not escape into intergalactic space but instead is reprocessed by interstellar dust grains before being re-radiated in the infrared. This work was supported by NASA through grants NAG5-9998 and NAS8-38252 issued by JPL/Caltech under contract NAS8-39073. It has been assigned the following DOI: 10.1086/505283",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Spitzer Mid - to - Far - Infrared Flux Densities of Distant Galaxies . Abstract : We present Spitzer Space Telescope observations in the mid - to far - infrared ( 5 . 8 - 160 microns ) for a sample of distant galaxies determined by their rest - frame ultraviolet luminosity flux at 1600 angstroms , and compare these flux densities with those predicted using models that include cloud extinction .We see that the seen infrared emission is typically higher than expected based on the UV continuum curve alone ; this excess emission can be understood if there are significant amounts of cold powder associated with star formation activity in these systems . The results show that the majority of the electricity generated by young stars must not escape into intergalactic space but instead is reprocessed by interstellar dust grains before being re - radiated in the infrared .This project was supported by NASA through programs NAG5 - 9998 and NAS8 - 38252 issued by JPL / Caltech under contract NAS8 - 39073 . It has been allocated the following DOI : 10 . 1086 / 505283",
        "rewrite_text": "We present a comprehensive analysis of mid- to far-infrared flux densities of distant galaxies, utilizing observations from the Spitzer Space Telescope across a wavelength range of 5.8 to 160 microns. Our sample selection is based on the galaxies' rest-frame ultraviolet luminosity at 1600 angstroms, allowing us to investigate the relationship between ultraviolet and infrared emissions. Upon comparing the observed flux densities with predictions derived from models that account for cloud extinction, we find a notable discrepancy: the infrared emissions are generally higher than what would be anticipated solely from the ultraviolet continuum. This excess infrared emission suggests the presence of substantial amounts of cold dust associated with ongoing star formation within these galaxies. \n\nOur findings indicate that a significant portion of the energy produced by young stars does not escape into intergalactic space; rather, it is absorbed and reprocessed by interstellar dust grains, which subsequently re-radiate this energy in the infrared spectrum. This process highlights the crucial role of dust in the energy dynamics of distant galaxies and emphasizes the importance of considering dust effects when interpreting observations of star-forming regions. \n\nThis research was conducted with the support of NASA through programs NAG5-9998 and NAS8-38252, managed by JPL/Caltech under contract NAS8-39073. The results of this study contribute to our understanding of galaxy evolution and the interplay between star formation and dust in the early universe. The work has been documented with the DOI: 10.1086/505283, ensuring accessibility for further research and validation within the scientific community.",
        "ori-fast-z-score": 0.35603449745815596,
        "water-fast-z-score": 4.714045207910317,
        "rewrite-fast-z-score": 0.4662524041201569
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Observations of Microwave Continuum Emission from Air Shower Plasmas .\nAbstract:\nWe report on the detection and characterization of microwave continuum emission from air shower plasmas using data collected by the LOPES experiment in Germany during 2004-2006. The observed signal is consistent with that expected for coherent Cherenkov radiation emitted by relativistic electrons accelerated to energies up to 100 MeV within the showers, as predicted by theory. We find no evidence for any significant contribution from incoherent synchrotron or bremsstrahlung processes. These results provide new insights into the physics of cosmic ray interactions at high energy. They also demonstrate the potential utility of radio techniques for studying atmospheric phenomena such as thunderstorms. \n \n Keywords: Cosmic rays, Radio waves, Air showers, Coherence, Synchrotron radiation \n \n \n \n 1 Introduction \n \n In recent years there has been growing interest in developing novel methods for detecting ultra-high-energy (UHE) cosmic rays based upon their interaction with Earth s atmosphere  1  . One promising technique involves measuring the radio-frequency (RF) emission produced when UHE particles interact with molecules in the upper atmosphere  2  , which can be detected remotely over large areas  3  .\n \nThe most prominent feature of this RF emission is an intense broadband pulse lasting several microseconds  4  . This pulse arises because the charged particle cascade generated by each primary cosmic ray interacts strongly with the geomagnetic field, causing it to emit coherently across a wide range of frequencies  5  . However, other mechanisms may contribute significantly to the total RF emission  6  . \n \n Here we present observations made with the Low-Frequency Array (LOFAR), one component of the International LOFAR Telescope  7  . Our analysis focuses primarily on measurements taken between 2004 and 2006 with the Long Wavelength Array (LWA)  8  , a phased array consisting of 144 dual-polarized dipole antennas operating at wavelengths ranging from 10 m to 80 m  9  . During these three years, LWA was deployed near Karthaus Township, Germany  10  , where it recorded signals from more than 20 million cosmic-ray-induced air showers  11  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Observations of Microwave Continuum Emission from Air Shower Plasmas . Abstract : We report on the detection and identification of microwave continuum emission from air washing plasmas using data taken by the LOPES experiment in Germany during 2004 - 2006 .The observed light is compatible with that expected for coherent Cherenkov radiation emitted by relativistic electrons accelerated to energies up to 100 MeV within the showers , as predicted by theoretical . We see no evidence for any large contribution from incoherent synchrotron or bremsstrahlung interactions .These data provide novel knowledge into the physics of cosmic ray molecules at high energy . They especially demonstrate the possibilities utility of radio methods for studying atmospheric phenomena such as thunderstorms .Keywords : Cosmic rays , Radio beams , Air showers , Coherence , Synchrotron emission 1 Introduction In past decades there has been growing interest in pursuing new ways for detecting ultra - large - energy ( UHE ) cosmic rays based upon their interaction with Earth s atmosphere 1 . One promising technique means measuring the radio - frequency ( RF ) emission generated when UHE molecules interact with compounds in the inner environment 2 , which can be identified remotely over large areas 3 .The most notable feature of this RF radiation is an intense broadband signal spanning multiple microseconds 4 . This wave exists because the charged particle cascade generated by each main cosmic ray interacts highly with the geomagnetic field , creating it to emit coherently across a broad variety of frequencies 5 .However , other mechanisms may contribute considerably to the total RF pollution 6 . Here we present observations made with the Low - Frequency Array ( LOFAR ) , one element of the International LOFAR Telescope 7 .Our study consists primarily on observations made between 2004 and 2006 with the Long Wavelength Array ( LWA ) 8 , a phased array consisting of 144 multiple - polarized dipole antennas active at wavelengths varied from 10 m to 80 m 9 . During these three years , LWA was deployed near Karthaus Township , Germany 10 , where it recorded transmissions from more than 20 million cosmic - ray - caused air showers 11 .",
        "rewrite_text": "**Title:** Observations of Microwave Continuum Emission from Air Shower Plasmas\n\n**Abstract:** This study presents the detection and characterization of microwave continuum emission originating from air shower plasmas, utilizing data collected by the LOPES experiment in Germany between 2004 and 2006. The emitted radiation aligns with theoretical predictions for coherent Cherenkov radiation produced by relativistic electrons that are accelerated to energies reaching up to 100 MeV within the air showers. Notably, our findings do not indicate significant contributions from incoherent synchrotron or bremsstrahlung processes. This research enhances our understanding of the physics surrounding high-energy cosmic ray interactions with atmospheric molecules, offering new insights into the mechanisms at play. Furthermore, the results underscore the potential of radio detection methods for investigating atmospheric phenomena, including thunderstorms. \n\nIn recent years, there has been an increasing interest in innovative approaches to detect ultra-high-energy (UHE) cosmic rays based on their interactions with the Earth's atmosphere. One promising method involves measuring the radio-frequency (RF) emissions generated during these interactions, which can be detected over extensive areas. A key characteristic of this RF radiation is its intense broadband signal that spans several microseconds. This phenomenon arises as the charged particle cascade, initiated by each primary cosmic ray, interacts with the geomagnetic field, resulting in coherent emissions across a wide frequency range. However, it is essential to consider that other mechanisms may also contribute significantly to the overall RF emissions. \n\nIn this paper, we detail observations made with the Low-Frequency Array (LOFAR), a component of the International LOFAR Telescope. Our analysis primarily focuses on data gathered from the Long Wavelength Array (LWA), a phased array comprising 144 multi-polarized dipole antennas operating at wavelengths between 10 m and 80 m. Over the three-year period, the LWA, situated near Karthaus Township, Germany, successfully recorded transmissions from over 20 million air showers induced by cosmic rays. This extensive dataset provides a valuable resource for advancing our understanding of cosmic ray physics and its atmospheric implications. \n\n**Keywords:** Cosmic rays, Radio emissions, Air showers, Coherence, Synchrotron radiation.",
        "ori-fast-z-score": -1.8461989882527161,
        "water-fast-z-score": 7.777050693065944,
        "rewrite-fast-z-score": -0.722897396012249
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  DNA Hash Pooling and its Applications .\nAbstract:\nWe propose DNA hash pooling (DHP) as an efficient data structure for storing large volumes of biological sequences in compressed form, with applications to sequence alignment, database search, and genome assembly. DHP is based on the concept that similar sequences have common substrings which can be represented by short binary codes called hashes. The main idea behind our approach is to store all possible hashes instead of storing each individual sequence separately. We show how this method can be used to compress large collections of biological sequences into small files while still allowing fast searches using standard string matching algorithms such as Boyer-Moore-Horspool or Rabin-Karp. In addition we present several novel applications of DHP including a new algorithm for finding exact matches between two sets of sequences without any prior knowledge about their lengths. Finally, we demonstrate the usefulness of DHP through experiments conducted on real-world datasets containing millions of human genomes.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : DNA Hash Pooling and its Applications . Abstract : We suggest DNA hash pooling ( DHP ) as an efficient information structure for storing huge volumes of biological sequences in compressed form , with applications to genome routing , database analysis , and genome assembly .DHP is based on the idea that related sequences have common substrings which can be represented by short binary codes called hashes . The main idea behind our approach is to contain all possible hashes instead of storing each individual code separately .We see how this algorithm can be used to compress large collections of biological sequences into small files while already permitting fast searches using conventional string matching algorithms such as Boyer - Moore - Horspool or Rabin - Karp . In addition we present many novel applications of DHP notably a new algorithm for finding exact matches between two sets of sequences without any earlier knowledge about their lengths .Finally , we prove the usefulness of DHP through experiments conducted on real - time datasets featuring millions of human genomes .",
        "rewrite_text": "Title: DNA Hash Pooling and its Applications\n\nAbstract: In this article, we introduce DNA Hash Pooling (DHP) as a novel and efficient framework for the storage of extensive biological sequence data in a compressed format. DHP has significant implications for various applications, including genome routing, database analysis, and genome assembly. The core concept of DHP revolves around the observation that related biological sequences often share common substrings, which can be succinctly represented by short binary codes known as hashes. Instead of storing each individual hash separately, our method aggregates all possible hashes, leading to a more compact representation of the data.\n\nWe demonstrate how this innovative algorithm enables the compression of large biological sequence collections into significantly smaller files, while still allowing for rapid searches using established string matching algorithms, such as Boyer-Moore-Horspool and Rabin-Karp. Furthermore, we explore several groundbreaking applications of DHP, including a new algorithm designed to identify exact matches between two sets of sequences without prior knowledge of their lengths. \n\nTo validate the effectiveness of DHP, we conducted extensive experiments on real-time datasets comprising millions of human genomes. The results underscore the practicality and efficiency of our approach, highlighting its potential to revolutionize the way biological sequence data is stored and analyzed. Overall, DHP represents a significant advancement in the field of bioinformatics, offering a robust solution for managing the ever-growing volume of genomic data.",
        "ori-fast-z-score": 1.2792042981336627,
        "water-fast-z-score": 6.604194471348086,
        "rewrite-fast-z-score": 1.2780193008453875
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Masers as Probes of Massive Star Formation in the Nuclear Disk .\nAbstract:\nWe present new observations of water maser emission toward the nuclear disk (ND) and circumnuclear ring (CNR) of NGC 1068, made with the Very Long Baseline Array at 22 GHz. The CNR is detected for the first time in water maser emission; it has an extent of ~1 kpc along its major axis and contains several hundred individual maser spots. We find that the distribution of maser luminosities follows closely that of infrared continuum emission observed by Spitzer Space Telescope. This suggests that both are tracing similar physical conditions within the molecular gas. In addition to the CNR, we detect maser emission associated with two other structures: a bright spot located near the center of the ND and a second fainter region extending over ~500 pc southward from this central position. These three regions have different kinematics, suggesting they may be physically distinct components. Finally, we report on the detection of methanol maser emission coincident with one of these water maser features.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Masers as Probes of Massive Star Formation in the Nuclear Disk . Abstract : We report new studies of water maser emission toward the atomic disk ( ND ) and circumnuclear circle ( CNR ) of NGC 1068 , made with the Very Long Baseline Array at 22 GHz .The CNR is detected for the first time in water maser emission ; it has an extent of ~ 1 kpc along its major axis and comprises numerous hundred unique maser spots . We see that the distribution of maser luminosities parallels closely that of infrared continuum emission observed by Spitzer Space Telescope .This implies that both are tracing related physical conditions within the molecular gas . In addition to the CNR , we find maser emission associated with two other structures : a bright patch located near the center of the ND and a second fainter portion spanning over ~ 500 pc southward from this central point .These three areas have different kinematics , indicate they may be physically different components . Finally , we paper on the diagnosis of methanol maser emission coincident with one of these water maser characteristics .",
        "rewrite_text": "**Title:** Masers as Probes of Massive Star Formation in the Nuclear Disk\n\n**Abstract:** In this study, we present new findings on water maser emissions observed in the atomic disk (ND) and the circumnuclear region (CNR) of the galaxy NGC 1068, utilizing the capabilities of the Very Long Baseline Array at a frequency of 22 GHz. For the first time, we have successfully detected water maser emissions from the CNR, which extends approximately 1 kpc along its major axis and contains hundreds of distinct maser spots. Notably, we observe a strong correlation between the luminosities of the masers and the infrared continuum emissions captured by the Spitzer Space Telescope, suggesting that both phenomena are indicative of similar physical conditions within the surrounding molecular gas. Beyond the CNR, we also identify maser emissions linked to two additional structures: a prominent region situated near the center of the ND and a less intense area that stretches about 500 pc to the south of this central location. The differing kinematic properties of these three regions imply that they may represent distinct physical components within the galaxy. Furthermore, we explore the characteristics of methanol maser emissions that coincide with one of the identified water maser regions, providing insights into the complex processes of massive star formation in this active galactic nucleus. Our findings contribute to a deeper understanding of the interplay between maser emissions and star formation activities in the nuclear environments of galaxies.",
        "ori-fast-z-score": -0.9561828874675149,
        "water-fast-z-score": 4.780914437337574,
        "rewrite-fast-z-score": -0.6060915267313265
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The MSSM with heavy scalars .\nAbstract:\nWe study the Minimal Supersymmetric Standard Model (MSSM) in which all scalar superpartners are assumed to be very massive, and we show that it is possible for this model to have light neutralinos as dark matter candidates while still being consistent with current experimental bounds on supersymmetry.  We find that there exists an upper bound on the mass of the lightest neutralino, mχ0 1 < 104 GeV/c2 , above which no solution can exist within our framework. This limit arises because the LSP must decay into gravitinos before Big Bang Nucleosynthesis; if its lifetime exceeds 1s then too much entropy will be produced by late decays to satisfy observational constraints. The lower bound on mχ0 1 depends upon tanβ . For large values of tanβ , mχ0 1 > 60GeV/c2 ; however, for small values of tanβ , solutions with mχ0 1 = O(10GeV/c2 ) may occur.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The MSSM with heavy scalars . Abstract : We research the Minimal Supersymmetric Standard Model ( MSSM ) in which all scalar superpartners are estimated to be very huge , and we prove that it is easy for this model to have dark neutralinos as dark matter candidates while already being consistent with current experimental bounds on supersymmetry .We see that there exists an upper bound on the mass of the lightest neutralino , mχ0 1 < 104 GeV / c2 , above which no solution can arise within our context . This limit arises because the LSP must decay into gravitinos before Big Bang Nucleosynthesis ; if its lifetime exceeds 1s then too little entropy will be made by late decays to meet observational restrictions .The lower bound on mχ0 1 varies upon tanβ . For large values of tanβ , mχ0 1 > 60GeV / c2 ; however , for larger values of tanβ , solutions with mχ0 1 = O ( 10GeV / c2 ) may arise .",
        "rewrite_text": "In this study, we investigate the Minimal Supersymmetric Standard Model (MSSM) under the assumption that all scalar superpartners possess significantly large masses. Our analysis demonstrates that this framework can accommodate dark neutralinos as viable candidates for dark matter while remaining in agreement with existing experimental constraints on supersymmetry. We identify a critical upper limit for the mass of the lightest neutralino, denoted as mχ0 1, which must satisfy the condition mχ0 1 < 104 GeV/c². This restriction is crucial because the lightest supersymmetric particle (LSP) is required to decay into gravitinos prior to the epoch of Big Bang Nucleosynthesis. If the lifetime of the LSP exceeds one second, it would result in insufficient entropy production from late decays, thereby conflicting with observational data. Additionally, we explore the lower bound for mχ0 1, which is dependent on the value of tanβ. For large tanβ values, we find that mχ0 1 must exceed 60 GeV/c². Conversely, as tanβ increases further, it becomes possible for solutions to exist with mχ0 1 on the order of 10 GeV/c². Our findings highlight the intricate balance between the mass parameters of the MSSM and the requirements for dark matter candidates, providing insights into the implications of heavy scalar superpartners within this theoretical framework. This research contributes to the ongoing discourse on supersymmetry and its potential role in explaining dark matter phenomena, while adhering to the constraints imposed by current experimental observations.",
        "ori-fast-z-score": -1.3608276348795434,
        "water-fast-z-score": 3.5381518506868126,
        "rewrite-fast-z-score": 1.8371173070873836
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  On the foundations of thermodynamics .\nAbstract:\nThe first law of thermodynamics is usually presented as an assertion about entropy, but it can be derived in other ways.  This article presents one such derivation using statistical mechanics and information theory.   The result is that entropy is defined by the amount of uncertainty associated with a system s state (or lack thereof).   Entropy increases when there are more possible states for which we cannot predict what will happen to the system.   In this sense, entropy measures how much our knowledge decreases when we learn something new about the world around us.   ...   ...   ...   ...   The second law of thermodynamics asserts that entropy always increases over time.   However, if you look closely at the definition of entropy given above, you ll see that it s not clear whether or not entropy actually does increase over time.   To answer this question, let s consider two different systems, A and B, both of which start out in equilibrium.   System A has only one possible configuration while system B has many possible configurations.   If we were able to observe these systems, we d find that they have exactly the same energy levels and temperatures.   But now imagine that we re observing them through a tiny window so that we can t tell what s going on inside each system.   We d then conclude that system A looks like its temperature was higher than system B because there s less uncertainty about its future behavior.   Now suppose that we let system A evolve into system C, where all particles are arranged in perfect order.   Then system C would appear colder than system A even though their energies remained unchanged.   So far we ve seen that entropy doesn t necessarily increase over time.   It could decrease instead....",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : On the foundations of thermodynamics . Abstract : The first rule of thermodynamics is usually given as an assertion about entropy , but it can be derived in other ways .This page presents one such derivation utilizing statistical mechanics and information theory . The result is that entropy is characterized by the quantity of uncertainty associated with a system s state ( or lack thereof ) .Entropy increases when there are more likely states for which we cannot predict what will occur to the system . In this sense , entropy increases how greatly our information decreases when we learn something new about the world around us .. . . . . . . . . . . . The second rule of thermodynamics asserts that entropy often increases over time .However , if you look closely at the characterization of entropy given above , you ll see that it s not clear whether or not entropy actually does increase over time . To answer this question , let s consider two different systems , A and B , both of which start out in equilibrium .System A has only one possible design while system B has numerous possible configurations . If we were trying to observe these systems , we d find that they have exactly the same energy levels and temperatures .But now imagine that we re watching them through a small window so that we can t know what s went on inside each system . We d then conclude that system A seems like its temperature was greater than system B because there s less expectation about its future behavior .Now suppose that we letting system A evolve into system C , where all atoms are arranged in perfect order . Then system C might appear cool than system A even though their energies remained unchanged .So far we ve seen that entropy doesn t necessarily increase over time.It could decrease instead....",
        "rewrite_text": "**Title: On the Foundations of Thermodynamics**\n\n**Abstract:** The conventional understanding of the first law of thermodynamics often emphasizes entropy as a central concept; however, alternative derivations exist that provide a deeper insight into its nature. This article explores one such derivation through the lenses of statistical mechanics and information theory, revealing that entropy fundamentally represents the degree of uncertainty associated with a system's state. Specifically, entropy increases in scenarios where there are multiple potential states of a system, leading to a diminished ability to predict its future behavior. This relationship highlights that as we gain new information about a system, our uncertainty—and consequently, entropy—can increase.\n\nThe second law of thermodynamics posits that entropy tends to rise over time, yet a closer examination of the previously discussed characterization of entropy raises questions about this assertion. To investigate this further, we analyze two distinct systems, A and B, both initially in equilibrium. System A is characterized by a single configuration, while System B possesses a multitude of possible arrangements. Despite both systems exhibiting identical energy levels and temperatures, an observer with limited access—viewing through a narrow window—might perceive System A as having a higher temperature due to its predictable behavior.\n\nAs we allow System A to evolve into System C, where all atoms are perfectly ordered, an intriguing phenomenon occurs: System C may appear cooler than System A, despite their energy levels remaining constant. This observation challenges the notion that entropy must invariably increase over time, suggesting instead that it may decrease under certain conditions. Through this analysis, we demonstrate that the behavior of entropy is more nuanced than traditionally understood, opening avenues for further exploration into the foundational principles of thermodynamics.",
        "ori-fast-z-score": 1.7677669529663687,
        "water-fast-z-score": 7.137183894183611,
        "rewrite-fast-z-score": 1.2004900959975617
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Critical behavior in an evolutionary Ultimatum Game .\nAbstract:\nWe study the evolutionarily stable strategy (ESS) and its stability properties for a class of ultimatum games with two-sided incomplete information, where one player is informed about his opponent s type while the other has no such information. We show that there exists a unique ESS which coincides with the Nash equilibrium if the set of types is finite or compactly supported on  0, 1  . If this set contains unbounded elements then we prove that the ESS may be unstable under small perturbations of the payoff functions. In particular, it can be shown that any ESS must satisfy certain conditions related to the distribution function of the types. Finally, we present some numerical examples illustrating our results. The evolutionary game theory studies how strategies evolve over time when players interact repeatedly within large populations. It provides a natural framework for analyzing strategic interactions between self-interested agents who are unable to commit themselves ex-ante but have the opportunity to learn by observing past play. A typical example of such situation arises in bargaining problems where each agent makes offers sequentially without knowing what proposals will be made by their opponents. This problem was first studied by Guth et al. (1982) , who introduced the so-called ultimatum game as a model of bargaining between two selfish individuals. In this game, Player 1 proposes a division of a fixed amount of money M into shares x and y = M −x offered to himself and Player 2 respectively; Player 2 either accepts or rejects the offer. If he accepts, both players receive their respective shares according to the proposal; otherwise they get nothing. Since the seminal work of Guth et al., many authors have investigated various aspects of the ultimatum game including existence and multiplicity of equilibria, efficiency loss due to lack of commitment power etc. (see e.g. Binmore & Shaked, 1993; Ochs & Roth, 1989) . However, all these works assume complete information among the players.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Critical behavior in an evolutionary Ultimatum Game . Abstract : We research the evolutionarily stable strategy ( ESS ) and its stability properties for a class of ultimatum games with two - sided incomplete information , where one team is informed about his opponent s type while the other has no such information .We see that there exists a unique ESS which coincides with the Nash equilibrium if the group of types is finite or compactly supported on 0 , 1 . If this set contains unbounded factors then we prove that the ESS might be unstable under small perturbations of the payoff functions .In particular , it can be shown that any ESS must satisfy certain conditions related to the distribution function of the types . Finally , we present some numerical examples illustrating our findings .The evolutionary tournament theory explores how strategies evolve over time when actors engage consistently within large populations . It provides a natural framework for studying strategic interactions between self - interested agents who are unable to commit themselves ex - ante but have the option to develop by observing past games .A typical example of such situation occurs in negotiating situations where each agent makes options sequentially without knowing what demands will be made by their opponents . This problem was first examined by Guth et al .( 1982 ) , who pioneered the so - called ultimatum game as a theory of negotiating between two selfish persons . In this tournament , Player 1 suggests a division of a specified quantity of money M into shares x and y = M −x offered to himself and Player 2 respectively ; Player 2 either accepts or denies the offer .If he agrees , both participants receive their respective stocks according to the proposal ; otherwise they get nothing . Since the seminal study of Guth et al . , various scientists have analyzed various parts of the ultimatum game including existence and multiplicity of equilibria , efficiency losing resulting to lack of commitment power etc .( see e . g . Binmore & Shaked , 1993 ; Ochs & Roth , 1989 ) .However , all these works assume complete data among the players .",
        "rewrite_text": "**Title:** Critical Behavior in an Evolutionary Ultimatum Game\n\n**Abstract:** This study investigates the evolutionarily stable strategy (ESS) and its stability characteristics within a specific class of ultimatum games characterized by two-sided incomplete information. In this framework, one participant possesses knowledge about their opponent's type, while the other remains uninformed. Our findings reveal the existence of a unique ESS that aligns with the Nash equilibrium when the type group is either finite or compactly supported within the interval [0, 1]. However, when the type set includes unbounded factors, we demonstrate that the ESS can become unstable in response to minor perturbations in the payoff functions. Furthermore, we establish that any ESS must adhere to specific conditions linked to the distribution function of the types involved. To illustrate our theoretical results, we provide several numerical examples that highlight the implications of our findings.\n\nThe evolutionary tournament theory serves as a valuable framework for understanding how strategies evolve over time as individuals engage in repeated interactions within large populations. This approach is particularly relevant for analyzing strategic interactions among self-interested agents who cannot commit to their actions in advance but can adapt their strategies based on observations of previous games. A classic scenario exemplifying this dynamic occurs in negotiation contexts, where agents make offers sequentially without knowledge of their opponents' demands. The ultimatum game, initially introduced by Guth et al. (1982), serves as a foundational model for studying negotiations between two self-interested parties. In this game, Player 1 proposes a division of a fixed amount of money, M, into two shares, x and y = M - x, designated for themselves and Player 2, respectively. Player 2 then decides to either accept or reject the offer, with the outcome determining the distribution of the money. Since the groundbreaking work of Guth et al., numerous researchers have explored various aspects of the ultimatum game, including the existence and multiplicity of equilibria and the inefficiencies arising from a lack of commitment power (see, for example, Binmore & Shaked, 1993; Ochs & Roth, 1989). Notably, prior studies have predominantly operated under the assumption of complete information among players.",
        "ori-fast-z-score": 1.365472859134248,
        "water-fast-z-score": 7.741442779335935,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Deep radio images of the HEGRA and Whipple TeV sources in the Cygnus OB2 region .\nAbstract:\nWe present deep radio observations at 1.4 GHz with the VLA towards two TeV gamma-ray emitting regions, one associated to the open cluster Cyg OB2 #8 (HESS J1640-465) and another located near the massive star WR 25 (HESS J1641-463). The new data reveal extended emission around both TeV sources which is not detected by previous surveys. We discuss possible scenarios for this emission based on our results as well as those obtained recently by other authors. In particular we propose that the observed structures are due to synchrotron radiation produced by relativistic electrons accelerated in shocks driven by stellar winds interacting within these clusters. This scenario would also explain why no X-ray counterparts have been found so far despite deep searches carried out with Chandra and XMM-Newton telescopes. Finally, we estimate the magnetic field strength required to produce such emission using standard models for particle acceleration in colliding wind binaries. \nIntroduction\n\nThe Cygnus OB2 association contains more than 100 OB stars distributed over an area of about 50 square degrees centered at l = 80°and b = 1° (  Fig.   1a ). It has been suggested that many of them could be members of binary systems or even multiple systems (e.g., Knödlseder 2000; Wright et al. 2010) . These objects can drive powerful winds into their surroundings creating strong shocks where particles may be accelerated up to very high energies. If some of these particles escape from the shock fronts they will interact with photons coming from the surrounding interstellar medium producing high-energy electromagnetic radiation detectable across most of the electromagnetic spectrum including the TeV range. \n \n Several studies suggest that several of the known TeV sources in the sky might be related to young open clusters like Cyg OB2 (see e.g., Aharonian et al. 2005a ,b, 2007a . However, only few of these associations have been confirmed through multi-wavelength campaigns involving optical/infrared imaging, spectroscopy and/or radio continuum observations (see e.g. , Reimer & Böttcher 2006 , Castro-Tirado et al",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Deep radio photographs of the HEGRA and Whipple TeV sources in the Cygnus OB2 region . Abstract : We report deep radio observations at 1 . 4 GHz with the VLA towards two TeV gamma - ray emitting regions , one related to the open cluster Cyg OB2 # 8 ( HESS J1640 - 465 ) and another situated near the powerful star WR 25 ( HESS J1641 - 463 ) .The revised data reveal extended pollution around both TeV sources which is not observed by earlier surveys . We discuss possible strategies for this emission based on our findings as well as those acquired previously by other researchers .In particular we explain that the seen structures are related to synchrotron emission created by relativistic electrons accelerated in shocks driven by stellar winds interacting within these clusters . This scenario would also explain why no X - ray relatives have been detected so far despite depth surveys done out with Chandra and XMM - Newton telescopes .Finally , we estimate the magnetic force size needed to produce such emission utilizing typical models for particle acceleration in colliding weather binaries . Introduction The Cygnus OB2 association contains more than 100 OB stars distributed over an area of about 50 square degrees centered at l = 80°and b = 1° ( Fig .1a ) . It has been proposed that several of them could be members of binary systems or even multiple systems ( e . g . , Knödlseder 2000 ; Wright et al .2010 ) . These particles can bring powerful storms into their environment forming violent shocks where objects may be advanced up to very high energies .If some of these ions survive from the shock fronts they will interact with photons coming from the nearby interstellar medium generating high - energy electromagnetic radiation detectable across most of the electromagnetic spectrum including the TeV range . Several studies propose that several of the known TeV sources in the heavens would be connected to early open complexes like Cyg OB2 ( see e . g . , Aharonian et al .2005a , b , 2007a . However , only few of these associations have been confirmed through multi - wavelength campaigns involving optical / infrared imaging , spectroscopy and / or radio continuum imaging ( saw e . g ., Reimer & Böttcher 2006 , Castro-Tirado et al",
        "rewrite_text": "We present a comprehensive analysis of deep radio observations conducted at 1.4 GHz using the Very Large Array (VLA) targeting two TeV gamma-ray emitting regions within the Cygnus OB2 stellar association. These regions include one associated with the open cluster Cyg OB2 #8 (designated HESS J1640-465) and another located near the prominent star WR 25 (HESS J1641-463). Our revised observations reveal extensive radio emissions surrounding both TeV sources, a phenomenon not detected in previous surveys. We explore potential mechanisms for this emission, drawing on our findings as well as prior research in the field. Notably, we propose that the observed structures are indicative of synchrotron radiation produced by relativistic electrons, which are accelerated in shock waves generated by stellar winds interacting within these dense stellar clusters. This model also provides an explanation for the absence of X-ray counterparts, despite extensive surveys conducted with the Chandra and XMM-Newton telescopes. Furthermore, we estimate the magnetic field strengths necessary to facilitate such emissions, employing standard models of particle acceleration in colliding stellar winds. \n\nThe Cygnus OB2 association is a rich region containing over 100 OB stars spread across approximately 50 square degrees, centered at galactic coordinates l = 80° and b = 1°. It has been suggested that many of these stars may exist in binary or even multiple systems. The energetic processes associated with these stars can generate powerful shocks that accelerate particles to high energies. If some of these accelerated ions survive the shock interactions, they can collide with photons from the surrounding interstellar medium, resulting in the production of high-energy electromagnetic radiation detectable across a wide range of wavelengths, including the TeV spectrum. Previous studies have indicated a potential connection between several known TeV sources and early stellar complexes like Cygnus OB2, although only a limited number of these associations have been confirmed through multi-wavelength observational campaigns, including optical, infrared, and radio continuum imaging.",
        "ori-fast-z-score": -0.9313806308475994,
        "water-fast-z-score": 6.6172410253729455,
        "rewrite-fast-z-score": 0.16012815380508713
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Ultraluminous X-ray Sources near the Center of M82 .\nAbstract:\nWe report on an analysis of archival Chandra data for the central region of the nearby starburst galaxy M82 (NGC 3034). We find that there are two bright, point-like sources in this field which have been previously identified as ULXs (Ultra-Luminous X-Ray Sources) by Swartz et al. (2004) . The first source is located at RA = 12 h 54 m 55 s .6 and Dec = 69°59 45   with a count rate of 1.1 x 10^-3 counts sec-1. This source has a luminosity of 2 x 10^39 erg/sec assuming it lies at 8 kpc distance. The second source is located at RA=12h54m55s.7 and Dec=69°59 46   with a count rate 0.9 x 10^-3 countssec-1. It also has a luminosity of about 2 x 10^39erg/sec if it lies at 8kpc. Both these sources appear to be variable over timescales ranging between hours and days.  These results suggest that both sources may contain black holes accreting close to their Eddington limit.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Ultraluminous X - ray Sources near the Center of M82 . Abstract : We report on an assessment of archival Chandra data for the central region of the nearby starburst galaxy M82 ( NGC 3034 ) .We see that there are two bright , point - like sources in this area which have been previously described as ULXs ( Ultra - Luminous X - Ray Sources ) by Swartz et al . ( 2004 ) .The first source is situated at RA = 12 h 54 m 55 s . 6 and Dec = 69°59 45 with a count rate of 1 . 1 x 10 ^ - 3 counts sec - 1 . This source has a luminosity of 2 x 10 ^ 39 erg / sec assuming it lies at 8 kpc radius .The second source is situated at RA = 12h54m55s . 7 and Dec = 69°59 46 with a count rate 0 . 9 x 10 ^ - 3 countssec - 1 . It additionally has a luminosity of about 2 x 10 ^ 39erg / sec if it lies at 8kpc .Both these sources appear to be varying over timescales ranging between hours and days . These data suggest that both sources might include black holes accreting close to their Eddington limit .",
        "rewrite_text": "We present an analysis of archival Chandra observations focused on the central region of the nearby starburst galaxy M82 (NGC 3034). Our investigation reveals the presence of two prominent point-like sources, previously identified as Ultra-Luminous X-ray Sources (ULXs) by Swartz et al. (2004). The first source is located at right ascension (RA) 12h 54m 55.6s and declination (Dec) 69° 59' 45\", exhibiting a count rate of 1.1 x 10^-3 counts per second. Assuming a distance of 8 kiloparsecs, this source has an estimated luminosity of approximately 2 x 10^39 erg/s. The second source, positioned at RA 12h 54m 55.7s and Dec 69° 59' 46\", shows a count rate of 0.9 x 10^-3 counts per second and similarly possesses a luminosity of around 2 x 10^39 erg/s at the same distance. Notably, both sources exhibit variability on timescales ranging from hours to days, indicating dynamic processes at play. The characteristics of these sources suggest that they may be black holes accreting matter near their Eddington limit, a scenario that aligns with the behavior typically observed in ULXs. Our findings contribute to the understanding of the nature of these enigmatic sources and their role in the energetic environment of starburst galaxies like M82. Further investigation into their variability and luminosity could provide deeper insights into the mechanisms driving their high-energy emissions and the underlying astrophysical processes involved.",
        "ori-fast-z-score": -0.2581988897471611,
        "water-fast-z-score": 2.9448482384566077,
        "rewrite-fast-z-score": -0.105999788000636
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Two-loop world-sheet corrections in AdS_5 x S^5 superstring .\nAbstract:\nWe calculate the two-loop beta function for the coupling constant of the AdS5xS5 superstring theory and show that it is proportional to the one-loop result, which implies that there are no non-trivial fixed points at any finite value of the string coupling constant.  We also find that the dilaton field has an imaginary part when we take into account the higher-order terms beyond the leading order approximation. This indicates that our results may be valid only within some limited region of the parameter space where the imaginary part of the dilaton can be neglected. The present work was motivated by the recent study on the gauge/gravity correspondence between N=4 super Yang-Mills (SYM) theories with 16 supercharges and type IIB strings on AdS5xS5G5 backgrounds  1  . In this context, the existence of nontrivial fixed points would correspond to the conformal invariance of the dual SYM theories  2  , while the imaginary part of the dilatonic scalar fields would indicate the instability of the corresponding solutions  3  .\nIn Ref.  4  , the authors have calculated the one-loop beta functions for both the metric tensor and the dilaton field using the Green-Schwarz formalism  5  . They found that these beta functions do not vanish even if they are evaluated at vanishing values of the string coupling constants. However, their calculations were performed under the assumption that all the fermionic contributions vanish identically  6  . It turns out that such an assumption does not hold true  7, 8  . Therefore, it seems necessary to perform more detailed analysis taking into account the effects due to the fermions as well as those coming from the bosons.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Two - loop universe - sheet corrections in AdS _ 5 x S ^ 5 superstring . Abstract : We calculate the two - loop beta function for the coupling constant of the AdS5xS5 superstring theory and find that it is proportional to the one - loop result , which implies that there are no non - trivial fixed points at any finite value of the string coupling constant .We additionally find that the dilaton field has an imaginary part when we took into consideration the higher - order terms beyond the led order approximation . This implies that our findings may be valid only within some restricted region of the parameter space where the imaginary part of the dilaton can be forgotten .The present work was motivated by the recent study on the gauge / gravity correspondence between N = 4 super Yang - Mills ( SYM ) theories with 16 supercharges and class IIB sequences on AdS5xS5G5 backgrounds 1 . In this sense , the existence of nontrivial fixed points would correspond to the conformal invariance of the dual SYM theories 2 , while the imaginary part of the dilatonic scalar fields might suggest the instability of the associated solutions 3 .In Ref . 4 , the authors have calculated the one - loop beta functions for both the metric tensor and the dilaton field using the Green - Schwarz formalism 5 .They found that these beta distributions do not vanish even if they are tested at vanishing values of the string coupling constants . However , their analyses were performed under the assumption that all the fermionic contributions vanish identically 6 .It turns out that such an assumption does not stand true 7 , 8 . Therefore , it appears necessary to conduct more precise analysis taking into consideration the effects due to the fermions as also as those coming from the bosons .",
        "rewrite_text": "**Title:** Two-loop Universe-Sheet Corrections in AdS_5 x S^5 Superstring\n\n**Abstract:** In this study, we investigate the two-loop beta function for the coupling constant within the framework of AdS_5 x S^5 superstring theory. Our calculations reveal that the two-loop beta function is directly proportional to the one-loop result, indicating the absence of non-trivial fixed points at any finite value of the string coupling constant. This finding suggests a lack of conformal invariance in the dual N = 4 super Yang-Mills (SYM) theories, which are known to possess 16 supercharges. Furthermore, we observe that the dilaton field exhibits an imaginary component when we extend our analysis to include higher-order terms beyond the leading order approximation. This raises the possibility that our results may only hold true within a limited region of the parameter space, where the imaginary part of the dilaton can be neglected.\n\nOur motivation stems from recent explorations of the gauge/gravity correspondence, particularly concerning the relationship between N = 4 SYM theories and type IIB string theory on AdS_5 x S^5 backgrounds. The presence of non-trivial fixed points would typically signify conformal invariance in the dual SYM theories, while the imaginary aspect of the dilatonic scalar fields could indicate potential instabilities in the corresponding solutions. Previous work has calculated the one-loop beta functions for both the metric tensor and the dilaton field using the Green-Schwarz formalism. These studies found that the beta functions do not vanish, even when evaluated at zero string coupling constants. However, they were conducted under the assumption that all fermionic contributions are negligible, a premise that has since been shown to be inaccurate. Consequently, it is imperative to perform a more thorough analysis that incorporates the effects of both fermionic and bosonic contributions to obtain a comprehensive understanding of the dynamics at play in this superstring theory.",
        "ori-fast-z-score": -0.6311687442672026,
        "water-fast-z-score": 4.418181209870418,
        "rewrite-fast-z-score": 0.7745966692414834
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Substructures in WINGS clusters .\nAbstract:\nWe present the results on substructure analysis for a sample of 62 galaxy clusters observed with XMM-Newton and Chandra, selected from the Wide-field Infrared Survey Explorer (WISE) All-Sky survey. We use two different methods to identify substructures within each cluster: the Dressler & Shectman test and the Voronoi Tessellation technique. The main goal is to investigate how these techniques perform when applied to X-ray data only or combined with infrared photometry. Our findings show that both tests are able to detect significant substructures in most cases; however, they also highlight some differences between them. For example, we find that the Dressler & Shechter method tends to overestimate the number of substructures compared to the VTT one. Moreover, by combining optical/infrared information with X-ray data, it is possible to improve the detection efficiency up to 30%. Finally, we compare our results with those obtained using other samples of galaxy clusters detected through their Sunyaev-Zeldovich effect.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Substructures in WINGS clusters . Abstract : We report the results on substructure analysis for a sample of 62 galaxy galaxies found with XMM - Newton and Chandra , selected from the Wide - field Infrared Survey Explorer ( WISE ) All - Sky survey .We use two different methods to identify substructures within each cluster : the Dressler & Shectman test and the Voronoi Tessellation technique . The main goal is to examine how these procedures perform when applied to X - ray data only or coupled with imaging photometry .Our findings show that both tests are able to identify significant substructures in most instances ; however , they still highlight some variations between them . For instance , we find that the Dressler & Shechter technique seems to overestimate the quantity of substructures compared to the VTT one .Moreover , by combining optical / infrared evidence with X - ray data , it is easy to upgrade the detection efficiency up to 30 % . Finally , we compare our findings with those acquired using other specimens of galaxy galaxies found through their Sunyaev - Zeldovich effect .",
        "rewrite_text": "We present a comprehensive analysis of substructures within a sample of 62 galaxy clusters identified using XMM-Newton and Chandra, selected from the Wide-field Infrared Survey Explorer (WISE) All-Sky survey. This study employs two distinct methodologies for substructure identification: the Dressler & Shectman test and the Voronoi Tessellation technique. The primary objective of our research is to evaluate the effectiveness of these methods when applied solely to X-ray data versus when they are integrated with imaging photometry. Our results indicate that both analytical approaches successfully detect significant substructures in the majority of cases; however, notable discrepancies between the two methods are observed. Specifically, the Dressler & Shectman technique tends to overestimate the number of substructures in comparison to the Voronoi Tessellation technique. Furthermore, we demonstrate that the incorporation of optical and infrared data alongside X-ray observations enhances detection efficiency by as much as 30%. To contextualize our findings, we also compare our results with those obtained from other galaxy cluster samples identified through the Sunyaev-Zeldovich effect. This comparative analysis underscores the importance of multi-wavelength approaches in accurately characterizing the substructure of galaxy clusters, ultimately contributing to a deeper understanding of their formation and evolution.",
        "ori-fast-z-score": 0.3511234415883917,
        "water-fast-z-score": 5.888972745734182,
        "rewrite-fast-z-score": 2.27776980709589
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Neutral Networks of Sequence to Shape Maps .\nAbstract:\nWe introduce the concept of neutral networks, which are graphs that represent relationships between sequences and shapes in an unsupervised manner. We show how these can be used for shape recognition by training them on synthetic data generated using computer graphics techniques. The resulting network is able to recognize shapes with high accuracy even when they have been transformed or distorted. Finally we demonstrate our method on real world images showing its ability to generalize well beyond the training set. In this work we present a novel approach to learning about shapes through their relationship to sequences. Our main contribution is introducing the concept of neutral networks as a way to learn such relationships without supervision. Neutral networks are graphs whose nodes correspond to sequences (e.g., strings) and edges connect similar sequences. They provide a natural representation for capturing relationships between different types of information. For example, one could use a neutral network to capture relationships between sentences and documents, or between music notes and melodies. Here we focus on applying neutral networks to the problem of recognizing shapes based solely on their sequence of points sampled along their boundary. This task has many applications including medical image analysis where it may not always be possible to obtain ground truth labels due to privacy concerns.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Neutral Networks of Sequence to Shape Maps . Abstract : We introduce the idea of neutral networks , which are graphs that represent connections between sequences and shapes in an unsupervised manner .We see how these can be used for shape perception by testing them on synthetic information generated using computer graphics techniques . The resulting network is ability to identify designs with high clarity even when they have been transformed or distorted .Finally we prove our technique on real time images showing its capacity to generalize far beyond the training set . In this project we present a novel method to knowledge about forms through their connection to sequences .Our main achievement is pioneering the idea of neutral networks as a way to teach such interactions without supervision . Neutral networks are graphs whose nodes relate to strings ( e . g . , strings ) and edges connect identical sequences .They offer a natural representation for capturing relationships between various types of documents . For instance , one might using a neutral network to capture relationships between sentences and documents , or between music notes and melodies .Here we focus on introducing neutral networks to the issue of recognizing shapes based primarily on their sequence of points analyzed along their boundary . This job has numerous uses including medical visual evaluation where it would not always be possible to obtain ground truth labels due to privacy issues .",
        "rewrite_text": "We present a comprehensive exploration of neutral networks, which are innovative graphical structures that illustrate the relationships between sequences and shapes in an unsupervised framework. This study investigates the application of neutral networks in shape perception, utilizing synthetic data generated through advanced computer graphics techniques. Our findings demonstrate that these networks possess a remarkable ability to accurately identify and classify designs, maintaining high clarity even when subjected to transformations or distortions. Furthermore, we validate our approach using real-time images, showcasing the network's capacity to generalize effectively beyond the confines of the training dataset.\n\nIn this research, we introduce a groundbreaking methodology for understanding forms through their intrinsic connections to sequences. A key contribution of our work is the pioneering concept of neutral networks as a means to facilitate the learning of these interactions without the need for supervision. Neutral networks are characterized by their nodes, which correspond to sequences (e.g., strings), and edges that link identical sequences, providing a natural framework for capturing the relationships among diverse document types. For example, one could utilize a neutral network to explore the connections between sentences and documents or between musical notes and melodies.\n\nOur primary focus lies in applying neutral networks to the challenge of shape recognition, specifically analyzing the sequences of points along the boundaries of shapes. This approach has significant implications across various fields, including medical imaging, where obtaining ground truth labels can be challenging due to privacy concerns. By leveraging neutral networks, we aim to enhance the understanding and recognition of shapes in a manner that is both effective and respectful of sensitive information.",
        "ori-fast-z-score": -1.1445861782233109,
        "water-fast-z-score": 7.137183894183611,
        "rewrite-fast-z-score": 1.150792911137501
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Efficient Multidimensional Data Redistribution for Resizable Parallel Computations .\nAbstract:\nWe present an efficient data redistribution scheme that allows the parallel execution of computations on dynamically resized computational domains in distributed memory environments. The proposed approach is based on multidimensional block-cyclic distributions and exploits locality by using space-filling curves to map blocks onto processors. We show how this technique can be used to efficiently redistribute data between different processor configurations, while minimizing communication overheads. Our experimental results demonstrate significant performance improvements over existing approaches when executing applications with dynamic load balancing requirements. In particular, we achieve speedups up to 3Â compared to state-of-the-art techniques such as the one presented in  1  . \nIntroduction\n\nParallel computing has become increasingly important due to its ability to solve large problems faster than serial computers  2  , but it also presents new challenges related to the distribution of work among multiple processing units  3  .\nIn order to take advantage of parallelism, many algorithms are designed so that they can run simultaneously on several processors  4  . However, these algorithms often require some form of data redistribution during their execution  5  . For example, consider a computation where each process stores part of a dataset (e.g., matrix)  6  . If the number of processes changes at runtime, then all processes need to exchange information about which parts of the dataset they store before continuing  7  . This problem becomes even more challenging if the size of the datasets stored by individual processes varies  8  or if there are dependencies between them  9  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Efficient Multidimensional Data Redistribution for Resizable Parallel Computations . Abstract : We create an efficient information redistribution system that enables the parallel execution of computations on dynamically resized computational regions in distributed storage systems .The proposed approach is based on multidimensional block - cyclic distributions and exploits locality by using space - filling curves to map blocks onto processors . We see how this methodology can be used to easily redistribute information between various processor configurations , while minimizing communication overheads .Our research results show considerable performance improvements over existing techniques when executing tasks with dynamic load balancing requirements . In particular , we accomplish speedups up to 3Â relative to state - of - the - art methods such as the one illustrated in 1 .Introduction Parallel logic has become rapidly crucial due to its able to solve huge problems quicker than serial machines 2 , but it also provides new obstacles linked to the spread of work among multiple processing facilities 3 . In order to take advantage of parallelism , various algorithms are built so that they can run simultaneously on various processors 4 .However , these algorithms often demand some kind of file redistribution during their execution 5 . For instance , consider a computation where each system stores portion of a dataset ( e . g . , matrix ) 6 .If the number of processes changes at runtime , then all processes must to exchange knowledge about which portions of the dataset they contain before continuing 7 . This problem arises even more challenging if the length of the datasets contained by individual processes vary 8 or if there are dependencies between them 9 .",
        "rewrite_text": "**Title:** Efficient Multidimensional Data Redistribution for Resizable Parallel Computations\n\n**Abstract:** In this study, we present a novel and efficient information redistribution framework designed to facilitate the parallel execution of computations across dynamically resizable computational regions within distributed storage systems. Our approach leverages multidimensional block-cyclic distributions and enhances data locality by employing space-filling curves to effectively map data blocks onto processors. This methodology allows for seamless redistribution of information among varying processor configurations while significantly reducing communication overheads. Our experimental results demonstrate substantial performance enhancements compared to existing techniques, particularly in scenarios requiring dynamic load balancing. Specifically, we achieve speedups of up to 3 times relative to leading methods currently available in the literature.\n\nThe increasing importance of parallel computing stems from its ability to tackle large-scale problems more efficiently than traditional serial computing methods. However, this shift towards parallelism introduces new challenges related to the distribution of workloads across multiple processing units. To harness the full potential of parallel architectures, various algorithms have been developed to enable concurrent execution on multiple processors. Nonetheless, these algorithms frequently necessitate some form of data redistribution during their operation. For example, in a scenario where each processor holds a segment of a dataset, such as a matrix, any change in the number of processes at runtime requires all processors to communicate and exchange information regarding the dataset portions they possess. This issue becomes even more complex when the sizes of the datasets held by individual processors differ or when there are interdependencies among the datasets. Our research addresses these challenges, providing a robust solution that optimizes data redistribution and enhances computational efficiency in parallel environments.",
        "ori-fast-z-score": -1.0606601717798212,
        "water-fast-z-score": 7.893522173763263,
        "rewrite-fast-z-score": 0.50709255283711
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The nearby QSO host I Zw 1: The stellar disk and adjacent objects .\nAbstract:\nWe present new near-infrared integral field spectroscopy (IFS) data for the brightest galaxy in the cluster Abell 2218, which is known to be interacting with its nearest neighbor, the radio-quiet quasar I Zw 1 at z = 0.0625. We find that this galaxy has an extended low-surface-brightness component surrounding it, extending out to about 10 kpc on both sides along the major axis. This feature shows no evidence of rotation but does show some velocity structure consistent with infalling gas or tidal debris. In addition we detect two compact objects within 5 kpc of the center of the galaxy. One of these appears to have a very high surface brightness and may represent a nuclear starburst; however, the other one displays much lower surface brightness and could possibly be associated with a supermassive black hole binary system. These results are discussed in terms of possible evolutionary scenarios for this interacting pair.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The nearby QSO host I Zw 1 : The stellar disk and adjacent bodies . Abstract : We report new near - infrared integral field spectroscopy ( IFS ) statistics for the brightest galaxy in the cluster Abell 2218 , which is known to be interacting with its closest neighbor , the radio - quiet quasar I Zw 1 at z = 0 . 0625 .We see that this galaxy has an extended low - surface - brightness component covering it , extending out to about 10 kpc on both sides along the main axis . This structure exhibits no evidence of rotation but does display some velocity features compatible with infalling gas or tidal debris .In addition we perceive two compact entities within 5 kpc of the center of the galaxy . One of these seems to have a very high surface brightness and may indicate a nuclear starburst ; however , the other one exhibits far lower surface brightness and could possibly be identified with a supermassive black hole binary system .These data are discussed in terms of possible evolutionary scenarios for this interacting pair .",
        "rewrite_text": "**Title:** The Nearby QSO Host I Zw 1: Stellar Disk and Surrounding Structures\n\n**Abstract:** In this study, we present new near-infrared integral field spectroscopy (IFS) observations of the prominent galaxy within the Abell 2218 cluster, which is known to be in interaction with its nearest neighbor, the radio-quiet quasar I Zw 1 at a redshift of z = 0.0625. Our findings reveal that the galaxy possesses an extensive low-surface-brightness component that extends approximately 10 kpc on either side along its principal axis. Notably, this structure does not exhibit any signs of rotation; however, it does show velocity features that may be indicative of infalling gas or tidal debris resulting from the interaction. Furthermore, we identify two compact structures located within 5 kpc of the galaxy's center. One of these structures displays a significantly high surface brightness, suggesting the presence of a nuclear starburst, while the other has a considerably lower surface brightness, raising the possibility that it could be associated with a binary system of supermassive black holes. Our observations and analyses provide insights into the potential evolutionary pathways for this interacting pair, contributing to the broader understanding of galaxy interactions and their implications for star formation and black hole dynamics in such environments. This research enhances our comprehension of the complex relationships between galaxies and their active galactic nuclei, particularly in the context of cluster dynamics and the influence of nearby quasars on their host galaxies.",
        "ori-fast-z-score": 0.10846522890932808,
        "water-fast-z-score": 4.74464202095129,
        "rewrite-fast-z-score": 1.7320508075688772
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Pair of Bootes: A New Milky Way Satellite .\nAbstract:\nWe report the discovery of a new satellite galaxy, dubbed  A pair of bootes  (ApoBootes), orbiting around our Galaxy at a distance of about 300 kpc in projection and with an estimated mass of 1.5 x 10^10 M_sun . ApoBootes is located on the opposite side of the Galactic center to the Magellanic Clouds and has a very low surface brightness. We have used deep near-infrared images taken by the VISTA telescope as part of the Vista Variables in the Via Lactea survey to identify this object. The photometric properties are consistent with those expected for a dwarf spheroidal galaxy. This work was supported by the Australian Research Council Discovery Project funding scheme under grant DP130104011. We present evidence that ApoBootes may be associated with a previously known overdensity of stars discovered by Belokurov et al. (2007) using SDSS data.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Pair of Bootes : A New Milky Way Satellite . Abstract : We report the discovery of a new satellite galaxy , nicknamed A couple of bootes ( ApoBootes ) , orbiting around our Galaxy at a length of about 300 kpc in projection and with an estimated mass of 1 . 5 x 10 ^ 10 M _ sun .ApoBootes is situated on the opposite end of the Galactic center to the Magellanic Clouds and has a very low exterior brightness . We have utilized deep near - infrared images obtained by the VISTA telescope as part of the Vista Variables in the Via Lactea survey to identify this body .The photometric properties are compatible with those expected for a dwarf spheroidal galaxy . This research was supported by the Australian Research Council Discovery Project money scheme under grant DP130104011 .We present evidence that ApoBootes might be identified with a previously known overdensity of stars found by Belokurov et al . ( 2007 ) using SDSS information .",
        "rewrite_text": "We present the discovery of a new satellite galaxy, designated as A couple of Bootes (ApoBootes), which orbits our Milky Way at a projected distance of approximately 300 kiloparsecs. This satellite has an estimated mass of 1.5 x 10^10 solar masses and is located on the opposite side of the Galactic center from the Magellanic Clouds, exhibiting a notably low surface brightness. The identification of ApoBootes was made possible through deep near-infrared imaging conducted with the VISTA telescope, as part of the Vista Variables in the Via Lactea survey. The photometric characteristics of this satellite align with those typically associated with dwarf spheroidal galaxies, suggesting it may be a member of this class of celestial objects. Furthermore, our findings indicate that ApoBootes could correspond to a previously identified stellar overdensity reported by Belokurov et al. (2007) based on data from the Sloan Digital Sky Survey (SDSS). This research was conducted with the support of the Australian Research Council Discovery Project funding scheme, under grant DP130104011. The discovery of ApoBootes adds to our understanding of the satellite galaxy population surrounding the Milky Way and provides new insights into the structure and formation of our Galaxy. The implications of this discovery may extend to the study of dark matter and the dynamics of galaxy formation, as well as the ongoing exploration of the Milky Way's satellite system.",
        "ori-fast-z-score": -0.39056673294247163,
        "water-fast-z-score": 4.131182235954578,
        "rewrite-fast-z-score": -1.0425720702853738
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Spitzer/IRS Imaging and Spectroscopy of the luminous infrared galaxy NGC 6052 (Mrk 297) .\nAbstract:\nWe present Spitzer Infrared Spectrograph (IRS) observations of the nearby, interacting galaxy pair Mrk 297. The system consists of two galaxies separated by ~3 kpc in projection; one is an elliptical galaxy with a bright nuclear point source, while the other has a Seyfert 2 nucleus surrounded by extended emission lines. We detect several molecular hydrogen transitions including H$_2$ S(0), S(1), S(2), S(3), S(4), S(5), S(6), S(7), S(8), S(9), S(10), S(11), S(12), S(13), S(14), S(15), S(16), S(17), S(18), S(19), S(20). These are detected over a wide range of spatial scales ranging from <100 pc to >500 pc.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Spitzer / IRS Imaging and Spectroscopy of the luminous infrared galaxy NGC 6052 ( Mrk 297 ) . Abstract : We report Spitzer Infrared Spectrograph ( IRS ) observations of the nearby , interacting galaxy pair Mrk 297 .The system contains of two galaxies linked by ~ 3 kpc in projection ; one is an elliptical galaxy with a bright nuclear point source , while the other has a Seyfert 2 nucleus accompanied by extended emission lines . We detect many molecular hydrogen changes including H $ _ 2 $ S ( 0 ) , S ( 1 ) , S ( 2 ) , S ( 3 ) , S ( 4 ) , S ( 5 ) , S ( 6 ) , S ( 7 ) , S ( 8 ) , S ( 9 ) , S ( 10 ) , S ( 11 ) , S ( 12 ) , S ( 13 ) , S ( 14 ) , S ( 15 ) , S ( 16 ) , S ( 17 ) , S ( 18 ) , S ( 19 ) , S ( 20 ) .These are detected over a broad variety of spatial scales ranging from < 100 pc to > 500 pc .",
        "rewrite_text": "We present findings from our observations using the Spitzer Infrared Spectrograph (IRS) of the nearby interacting galaxy pair known as Mrk 297, which is cataloged as NGC 6052. This intriguing system comprises two galaxies that are approximately 3 kpc apart in projection. One of these galaxies is an elliptical type, characterized by a prominent nuclear point source, while the other exhibits a Seyfert 2 nucleus, which is notable for its extended emission lines. Our analysis reveals a rich spectrum of molecular hydrogen transitions, specifically detecting numerous H₂ lines, including S(0) through S(20). These transitions were observed across a wide range of spatial scales, from less than 100 parsecs to more than 500 parsecs. The detection of these molecular hydrogen lines provides critical insights into the physical conditions and processes occurring within this interacting galaxy pair. The presence of both a bright nuclear source and the Seyfert 2 nucleus suggests complex interactions and potential star formation activities influenced by the gravitational dynamics of the system. Our results contribute to the understanding of the role of molecular hydrogen in the evolution of luminous infrared galaxies and highlight the significance of using infrared spectroscopy to probe the intricate details of galaxy interactions. This study not only enhances our knowledge of Mrk 297 but also serves as a valuable reference for future investigations into similar galaxy systems.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 3.2024699769846983,
        "rewrite-fast-z-score": 0.9712858623572641
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Frequency Distribution of Semi-major Axis of Wide Binaries. Cosmogony and Dynamical Evolution .\nAbstract:\nWe have investigated the frequency distribution of semi-major axis (SMA) for wide binaries with separations greater than 1000 AU in order to test cosmogonies and dynamical evolution models. We used data obtained by the Two Micron All Sky Survey (2MASS), which is complete down to Ks = 12 mag, corresponding to masses as low as 0.1 M⊙ at distances up to 1 kpc. The sample consists of 13,000 pairs selected using color-color criteria designed to select main-sequence stars. Using Monte Carlo simulations we found that our results are not affected significantly by incompleteness effects due to photometric errors or contamination by background galaxies. Our analysis shows that there exists an excess number of systems with SMA between 10 4 -10 5 AU compared to predictions based on standard cosmological models. This result suggests that either these systems were formed earlier than predicted by current theories or they may be primordial objects such as Population III remnants.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Frequency Distribution of Semi - major Axis of Wide Binaries . Cosmogony and Dynamical Evolution .Abstract : We have analyzed the frequency distribution of semi - major axis ( SMA ) for wide binaries with separations greater than 1000 AU in order to test cosmogonies and dynamical development estimates . We utilized information obtained by the Two Micron All Sky Survey ( 2MASS ) , which is complete down to Ks = 12 mag , equivalent to masses as low as 0 . 1 [UNK] at distances up to 1 kpc .The sample consists of 13 , 000 pairs selected using color - color factors created to select primary - sequence stars . Using Monte Carlo simulations we concluded that our findings are not affected substantially by incompleteness effects due to photometric failures or exposure by background galaxies .Our study shows that there exists an excess amount of components with SMA between 10 4 - 10 5 AU compared to expectations based on normal cosmological models . This result suggests that either these systems were created earlier than expected by current theories or they may be primordial objects such as Population III fragments .",
        "rewrite_text": "Title: The Frequency Distribution of Semi-Major Axes of Wide Binaries: Insights into Cosmogony and Dynamical Evolution\n\nAbstract: In this study, we investigate the frequency distribution of semi-major axes (SMA) for wide binary systems with separations exceeding 1000 AU, aiming to evaluate existing cosmogonical models and estimates of dynamical evolution. Our analysis leverages data from the Two Micron All Sky Survey (2MASS), which provides a comprehensive catalog down to Ks = 12 mag, allowing us to detect stars with masses as low as 0.1 solar masses at distances reaching 1 kpc. The dataset comprises 13,000 binary pairs, meticulously selected using color-color criteria designed to isolate main-sequence stars. To ensure the robustness of our findings, we employed Monte Carlo simulations, which indicated that our results are largely unaffected by potential incompleteness arising from photometric inaccuracies or interference from background galaxies.\n\nOur results reveal a significant overabundance of binary components with semi-major axes in the range of 10^4 to 10^5 AU, a finding that deviates from predictions made by standard cosmological models. This discrepancy raises intriguing questions regarding the formation and evolution of these systems. It suggests that the binaries in this SMA range may have formed earlier than current theoretical frameworks anticipate, or they could represent primordial objects, potentially akin to fragments from Population III stars. This study contributes to a deeper understanding of the formation processes of wide binaries and their implications for the broader context of stellar evolution and galactic dynamics. The insights gained from this research may prompt a reevaluation of existing models in light of the observed frequency distribution, highlighting the need for further investigation into the origins and characteristics of these intriguing stellar systems.",
        "ori-fast-z-score": -0.11547005383792514,
        "water-fast-z-score": 4.965212315030781,
        "rewrite-fast-z-score": 0.5477225575051661
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  System Design for a Long-Line Quantum Repeater .\nAbstract:\nQuantum repeaters are proposed to overcome the loss in quantum communication channels by using entangled photons and linear optics elements such as beam splitters, phase shifters, and single-photon detectors.  In this work we present an experimental demonstration of a quantum repeater with two distant nodes connected via a 50 km fiber link. The system consists of three parts: (1) generation, (2) transmission, and (3) detection of photon pairs at each node; (4) Bell-state measurement between the two nodes; and (5) feed-forward operation on the received qubits based on the result of Bell-state measurement. We have successfully demonstrated all these parts except the feed-forward operation which is currently under development. Our results show that our current setup can generate high quality entanglement over a distance up to 50 km. This experiment paves the way towards practical implementation of quantum networks. Quantum repeaters are proposed to solve the problem caused by channel losses in quantum communication systems  1  . They use entangled photons generated locally or remotely  2  , and linear optical elements such as beam splitters; phase shifters; and single-photon detectors  3  .\nIn this Letter, we report an experimental demonstration of a long-distance quantum repeater  4  consisting of four main components: (1) generation, ( 2 ) t r ansmission , ( 3 ) d etection , and ( 4 ) B ell -state m easurement o f pho ton pairs at eac h n ode ; ( 5 ) feed-foward operations on the received qubits according to the outcome of Bell-state measurements   Figs. 1(a) , 1(b), and 2 . A pair of polarization-entangled photons was produced through spontaneous parametric down-conversion (SPDC). One photon acted as signal while another one served as idler. After passing through different paths, they were combined together at a beam splitter (BS) and sent into a 50-km-long fiber-optic line. At both ends of the fiber-optic line, photon-number resolving avalanche photodiodes (APDs) detected the",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : System Design for a Long - Line Quantum Repeater . Abstract : Quantum repeaters are proposed to overcome the loss in quantum communication channels by using entangled photons and linear optics components such as beam splitters , wave shifters , and single - photon detectors .In this research we present an experimental test of a quantum repeater with two distant nodes linked via a 50 km fiber link . The system consists of three components : ( 1 ) production , ( 2 ) propagation , and ( 3 ) detection of photon pairs at each node ; ( 4 ) Bell - state recording between the two nodes ; and ( 5 ) feed - forward operation on the received qubits based on the result of Bell - state measurement .We have successfully shown all these parts except the feed - forward process which is already under development . Our results show that our latest setup can generate high quality entanglement over a length up to 50 km .This study paves the way towards practical formulation of quantum networks . Quantum repeaters are proposed to solve the issue caused by channel losses in quantum communication schemes 1 .They use entangled photons generated locally or remotely 2 , and linear optical elements such as beam splitters ; phase shifters ; and double - photon detectors 3 . In this Letter , we publish an experimental demonstration of a high - distance quantum repeater 4 consisting of four main components : ( 1 ) production , ( 2 ) t r ansmission , ( 3 ) d etection , and ( 4 ) B ell - state m easurement o h pho ton pairs at eac h h ode ; ( 5 ) feed - foward operations on the received qubits according to the result of Bell - state measurements Figs .1 ( a ) , 1 ( b ) , and 2 . A couple of polarization - entangled photons was produced through spontaneous parametric down - transfer ( SPDC ) .One photon served as signal while another one served as idler . After passing through different paths , they were paired together at a laser splitter ( BS ) and sent into a 50 - kilometres - length fiber - optic line .At both sides of the fiber - optic line , photon - number resolving avalanche photodiodes ( APDs ) detected the",
        "rewrite_text": "**Title: System Design for a Long-Line Quantum Repeater**\n\n**Abstract:** Quantum repeaters have emerged as a vital solution to address the challenges posed by signal loss in quantum communication channels. By utilizing entangled photons and linear optical components—such as beam splitters, wave shifters, and single-photon detectors—these systems aim to enhance the reliability and distance of quantum information transfer. In this study, we present an experimental evaluation of a quantum repeater that connects two distant nodes via a 50 km fiber optic link. The architecture of our system encompasses five key components: (1) the production of entangled photon pairs, (2) their propagation through the fiber, (3) the detection of these photons at each node, (4) the implementation of Bell-state measurements to establish correlations between the nodes, and (5) the feed-forward operation based on the outcomes of the Bell-state measurements. While we have successfully demonstrated the first four components, the feed-forward process is currently under development. Our findings indicate that the latest iteration of our setup can generate high-quality entanglement over distances of up to 50 km, marking a significant advancement toward the practical realization of quantum networks. This research not only highlights the potential of quantum repeaters in mitigating channel losses but also lays the groundwork for future developments in long-distance quantum communication. The experimental demonstration includes the generation of polarization-entangled photon pairs via spontaneous parametric down-conversion (SPDC), where one photon acts as the signal and the other as the idler. After traversing distinct paths, these photons are combined at a beam splitter and transmitted through the fiber optic line. At both ends of the fiber, photon-number resolving avalanche photodiodes (APDs) are employed to detect the entangled photons, thus facilitating the measurement and verification of quantum states. This work represents a crucial step towards the establishment of robust quantum communication networks.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.6182931030077405,
        "rewrite-fast-z-score": 1.979524821394902
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Product Formulae for Ozsvath-Szabo 4-manifold Invariants .\nAbstract:\nWe give explicit formulae for the product of two Ozsvath-Szab o 4-dimensional manifold invariants in terms of their associated modular forms and theta functions.  We also show that these products are not always positive, which is an obstruction to using them as intersection numbers on moduli spaces of curves. The results presented here were obtained by applying techniques developed by Vafa-Witten and Witten-Taubes to the case of genus 2 Riemann surfaces with four punctures. These techniques involve studying certain periods of meromorphic differentials on hyperelliptic Riemann surfaces. As such they can be used to study other problems involving hyperelliptic Riemann surface invariants. This article was published in Journal of Physics A: Mathematical and General 37 (2004) L55-L62. URL: http://arxiv.org/abs/0406.0366v1 . \nAuthors: David J. Jeffrey1 , Michael A. McMullen2 .\nAffiliation: 1University of California Santa Cruz, Santa Cruz, CA 95064, USA; 2University",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Product Formulae for Ozsvath - Szabo 4 - bundle Invariants . Abstract : We take explicit formulae for the sum of two Ozsvath - Szab o 4 - dimensional manifold invariants in terms of their associated modular forms and theta functions .We additionally prove that these products are not always positive , which is an obstruction to use them as interchange numbers on moduli spaces of curves . The results presented here were obtained by using techniques established by Vafa - Witten and Witten - Taubes to the case of genus 2 Riemann surfaces with four punctures .These methods involve examining certain periods of meromorphic differentials on hyperelliptic Riemann surfaces . As such they can be used to study other difficulties involving hyperelliptic Riemann surface invariants .This section was reprinted in Journal of Physics A : Mathematical and General 37 ( 2004 ) L55 - L62 . URL : www : / / arxiv . org / abs / 0406 . 0366v1 .Authors: David J. Jeffrey1 , Michael A. McMullen2 .Affiliation: 1University of California Santa Cruz, Santa Cruz, CA 95064, USA; 2University",
        "rewrite_text": "**Title:** Product Formulae for Ozsváth-Szabó 4-Bundle Invariants\n\n**Abstract:** In this study, we present explicit formulae for the sum of two Ozsváth-Szabó invariants associated with 4-dimensional manifolds, expressed in terms of their corresponding modular forms and theta functions. Our investigation reveals that these product invariants do not consistently yield positive values, which poses a significant challenge for their application as interchange numbers within the moduli spaces of curves. The findings are derived through methodologies established by Vafa-Witten and Witten-Taubes, specifically applied to the context of genus 2 Riemann surfaces featuring four punctures. This approach necessitates a detailed analysis of specific periods of meromorphic differentials on hyperelliptic Riemann surfaces, thereby providing a framework that can be extended to address other complexities related to hyperelliptic Riemann surface invariants. The results of this research were previously published in the Journal of Physics A: Mathematical and General, volume 37, pages L55-L62, in 2004. For further details, the full article can be accessed at the following URL: http://arxiv.org/abs/0406.0366v1. The authors of this work are David J. Jeffrey from the University of California, Santa Cruz, and Michael A. McMullen.",
        "ori-fast-z-score": -1.2135597524338357,
        "water-fast-z-score": 4.008918628686366,
        "rewrite-fast-z-score": 0.24618298195866545
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A High-Resolution Survey of Low-Redshift QSO Absorption Lines: Statistics and Physical Conditions of O VI Absorbers .\nAbstract:\nWe present the results of an extensive survey for absorption lines in high-resolution spectra of quasars at redshifts z < 0.3, obtained with HST/STIS (the Hubble Space Telescope Spectrograph). The sample consists of 12 QSOs observed over a total exposure time of about 1 Ms. We detect more than 1000 Lyman alpha forest absorbers along each line-of-sight to these QSOs; we also find that most sight-lines show strong absorption by high-ionization species such as C IV and Si IV. In addition, we identify several hundred systems showing absorption due to highly ionized oxygen, O VI . These are among the strongest known O VI absorbers anywhere in the universe. By combining our data set with previous surveys, we obtain new constraints on the physical conditions of this gas phase. Our main conclusions are:  -The number density of O VI absorbers per unit redshift is n(O VI) = 2.6 +/- 0.4 x 10^-3 cm^-3 , which corresponds to a mean separation between absorber pairs of r = 4.1 +/- 0.5 h-1 Gpc.  -Most O VI absorbers have temperatures T ~ 10 5 K and densities log ne/cm -3 > 13.8. About half of them appear to be associated with galaxies within 50 kpc projected distance. -There appears to be no correlation between the properties of O VI absorbers and those of their host galaxy population.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A High - Resolution Survey of Low - Redshift QSO Absorption Lines : Statistics and Physical Conditions of O VI Absorbers . Abstract : We report the results of an extensive survey for absorption patterns in high - resolution spectra of quasars at redshifts z < 0 . 3 , obtained with HST / STIS ( the Hubble Space Telescope Spectrograph ) .The sample consists of 12 QSOs observed over a total sensitivity time of about 1 Ms . We detect more than 1000 Lyman alpha forest absorbers along each line - of - view to these QSOs ; we also find that most vision - lines show strong absorption by high - ionization species such as C IV and Si IV . In addition , we identify several hundred systems showing emission thanks to strongly ionized oxygen , O VI .These are among the powerful recorded O VI absorbers anywhere in the universe . By combining our information pool with previous searches , we obtain new limitations on the physical conditions of this gas phase .Our main results are : - The amount density of O VI absorbers per unit redshift is n ( O VI ) = 2 . 6 + / - 0 . 4 x 10 ^ - 3 cm ^ - 3 , which equals to a mean separation between absorber sets of r = 4 . 1 + / - 0 . 5 h - 1 Gpc . - Most O VI absorbers have temperatures T ~ 10 5 K and densities log ne / cm - 3 > 13 . 8 .About half of them seem to be identified with galaxies within 50 kpc projected distance . - There seems to be no correlation between the properties of O VI absorbers and those of their host galaxy community .",
        "rewrite_text": "We present the findings from a comprehensive survey focused on the absorption features observed in high-resolution spectra of quasars with redshifts less than 0.3, utilizing the Hubble Space Telescope's Space Telescope Imaging Spectrograph (HST/STIS). Our study encompasses a sample of 12 quasars, which were observed over a cumulative sensitivity period of approximately 1 Ms. Through our analysis, we identified over 1,000 Lyman alpha forest absorbers along the sightlines to these quasars, with a significant number of these lines exhibiting strong absorption from high-ionization species, including C IV and Si IV. Notably, we also detected several hundred systems characterized by emission from highly ionized oxygen, O VI, marking them as some of the most potent O VI absorbers documented in the universe to date.\n\nBy integrating our findings with previous research, we have established new constraints on the physical conditions of the gas phase associated with these absorbers. Our key results indicate that the number density of O VI absorbers per unit redshift is n(O VI) = 2.6 ± 0.4 x 10^-3 cm^-3, which corresponds to a mean separation of r = 4.1 ± 0.5 h^-1 Gpc between absorber groups. Furthermore, the majority of O VI absorbers exhibit temperatures around T ~ 10^5 K and densities greater than log ne/cm^-3 > 13.8. Approximately half of these absorbers appear to be associated with galaxies located within a projected distance of 50 kpc. Interestingly, our analysis reveals no significant correlation between the properties of the O VI absorbers and those of their corresponding host galaxy populations. This study enhances our understanding of the physical conditions of O VI absorbers and their relationship with the surrounding galactic environment.",
        "ori-fast-z-score": 0.41256849850351734,
        "water-fast-z-score": 5.027293925217255,
        "rewrite-fast-z-score": -0.5827715174143585
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Millimeter dust continuum emission unveiling the true mass of giant molecular clouds in the Small Magellanic Cloud .\nAbstract:\nWe present new observations at 1 mm and 3 mm wavelengths toward two Giant Molecular Clouds (GMC) in the Small Magellanic: Cloud, LMC N11B and 30 Doradus. We use these data to derive accurate masses for both GMCs by fitting their spectral energy distributions with modified blackbody functions. The derived masses are M(LMC N11B) = 2.1 x 10^6 M_sol and M(30 Doradus) = 5.2 x 10^7 M_sol . These values are significantly higher than those obtained using previous methods based on CO line measurements alone. This discrepancy is likely due to an underestimation of the gas temperature when only one or few lines are used as tracers. Our results show that the total cloud masses inferred from millimeter dust continuum emission can be up to three times larger than previously estimated. Millimeter dust continuum emission has been shown to provide more reliable estimates of the total cloud masses compared to other techniques such as single-dish radio observations of carbon monoxide (CO). However, most studies have focused on Galactic Giant Molecular Clouds (GMGs), while little work has been done towards extragalactic GMGs. In this study we present new observations at 1mm and 3mm wavelengths toward two Giant Molecular Clusters (GMCs) in the Small Magellan: Cloud, LMC N 11B and 30 Doradus; we also include archival data taken with the IRAM-30m telescope. Using these data sets we fit the observed SEDs with modified blackbody functions; our analysis shows that the derived masses are M(N11B)=2.1x10^6M_sol and M(Doradus)=5.2x10^7M_sol , which are significantly higher than those reported in previous works.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Millimeter dust continuum emission unveiling the true mass of large molecular clouds in the Small Magellanic Cloud . Abstract : We report new observations at 1 mm and 3 cm wavelengths toward two Giant Molecular Clouds ( GMC ) in the Small Magellanic : Cloud , LMC N11B and 30 Doradus .We use these information to derive exact masses for both GMCs by fitting their spectral power distributions with modified blackbody parameters . The derived masses are M ( LMC N11B ) = 2 . 1 x 10 ^ 6 M _ sol and M ( 30 Doradus ) = 5 . 2 x 10 ^ 7 M _ sol .These measurements are greatly higher than those achieved using previous techniques based on CO line surveys alone . This discrepancy is probably due to an underestimation of the gas temperature when only one or few lines are using as tracers .Our results show that the total cloud masses inferred from millimeter dust continuum emission can be up to three times bigger than previously predicted . Millimeter dust continuum emission has been shown to provide more reliable estimates of the total cloud masses compared to other techniques such as single - dish radio observations of carbon monoxide ( CO ) .However , most studies have concentrated on Galactic Giant Molecular Clouds ( GMGs ) , while nothing research has been performed towards extragalactic GMGs . In this study we present new images at 1mm and 3mm wavelengths toward two Giant Molecular Clusters ( GMCs ) in the Small Magellan : Cloud , LMC N 11B and 30 Doradus ; we also add archival measurements taken with the IRAM - 30m telescope .Using these information sets we fit the known SEDs with modified blackbody parameters ; our analysis shows that the derived masses are M ( N11B ) = 2 . 1x10 ^ 6M _ sol and M ( Doradus ) = 5 . 2x10 ^ 7M _ sol , which are significantly greater than those published in earlier works .",
        "rewrite_text": "We present new observational data collected at 1 mm and 3 cm wavelengths for two Giant Molecular Clouds (GMCs) located in the Small Magellanic Cloud: LMC N11B and 30 Doradus. Utilizing these observations, we accurately determine the masses of both GMCs by fitting their spectral energy distributions (SEDs) with modified blackbody models. Our findings reveal that the mass of LMC N11B is approximately 2.1 x 10^6 M_sun, while the mass of 30 Doradus is about 5.2 x 10^7 M_sun. These mass estimates are significantly higher than those derived from previous methodologies that relied solely on CO line surveys. We attribute this discrepancy to the potential underestimation of gas temperatures when using limited line tracers. Our results indicate that the total masses of these clouds, as inferred from millimeter dust continuum emission, can be up to three times greater than earlier predictions. This study highlights the advantages of millimeter dust continuum emission in providing more accurate mass estimates compared to traditional techniques, such as single-dish radio observations of carbon monoxide (CO). Notably, while extensive research has been conducted on Galactic GMCs, there has been a lack of studies focusing on extragalactic GMCs. In this work, we not only present new images at 1 mm and 3 mm wavelengths for LMC N11B and 30 Doradus but also incorporate archival data obtained from the IRAM-30m telescope. By analyzing these datasets and fitting the SEDs with modified blackbody parameters, we confirm that the derived masses for N11B and 30 Doradus are indeed significantly larger than those reported in previous literature. This research underscores the importance of millimeter dust continuum observations in accurately assessing the mass of molecular clouds in extragalactic environments.",
        "ori-fast-z-score": 0.3849001794597505,
        "water-fast-z-score": 6.928853368993243,
        "rewrite-fast-z-score": 2.752558187682247
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  IIB backgrounds with five-form flux .\nAbstract:\nWe study the effect of adding a constant five-form field strength to type IIB supergravity on AdS5xS5 and its dual N=4 supersymmetric Yang-Mills theory in four dimensions. We find that this leads to an additional term in the action which is proportional to the volume form of S5, but does not affect any other fields or equations of motion. The resulting solution has a non-vanishing dilaton at infinity, corresponding to a deformation of the conformal symmetry group by a scale transformation. This can be interpreted as giving rise to a mass gap for all states except those transforming trivially under the unbroken SO(4) subgroup of SU (4). In particular we show how this mechanism allows one to generate masses for all gauge bosons without breaking supersymmetry. We also discuss some possible generalizations of our results. Introduction: It was shown recently  1  , using the gauged linear sigma model approach  2  , that it is possible to deform the maximally supersymmetric Yang-Mills (MSYM) theory in 4-dimensions such that only the vector multiplets acquire masses while preserving N = 1 supersymmetry. The authors showed that there are two different ways of doing this: either by turning on a constant three-form H-flux  3  or by introducing a constant fived-form F -flux  4  . They found that both these solutions preserve half of the original supersymmetries. However, they did not consider the possibility of having both types of fluxes simultaneously turned on. Here we will fill this gap and present new solutions where both H-and F -fluxes are turned on. These solutions have been obtained within the context of type IIA string theory compactified on K3 × T 2  5  .\nThe plan of the paper goes as follows. First we review briefly the work done in ref.  1  . Then we introduce the ansatz for the metric and the forms when both H- and F -fluxes exist together. Next we solve the Einstein s equation and obtain the explicit expression for the metric. Finally we compute the spectrum",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : IIB backgrounds with five - form flux . Abstract : We research the impact of adding a constant five - form field intensity to type IIB supergravity on AdS5xS5 and its dual N = 4 supersymmetric Yang - Mills theory in four dimensions .We see that this results to an additional term in the operation which is proportional to the volume form of S5 , but does not alter any other fields or equations of movement . The resulting solve has a non - vanishing dilaton at infinity , equivalent to a deformation of the conformal symmetry group by a scale transformation .This can be interpreted as give rise to a mass gap for all states except those transforming trivially under the unbroken SO ( 4 ) subgroup of SU ( 4 ) . In particular we show how this mechanism enables one to create masses for all gauge bosons without breaking supersymmetry .We also discuss some possible generalizations of our findings . Introduction : It was shown recently 1 , using the gauged linear sigma model approach 2 , that it is easy to deform the maximally supersymmetric Yang - Mills ( MSYM ) theory in 4 - dimensions such that only the vector multiplets acquire masses while preserving N = 1 supersymmetry .The authors showed that there are two different means of doing this : either by turning on a constant three - form H - flux 3 or by using a constant fived - form F - flux 4 . They found that both these solutions preserve half of the previous supersymmetries .However , they did not discuss the prospect of having both types of fluxes simultaneously turned on . Here we will fill this gap and present new solutions where both H - and F - fluxes are turned on .These solutions have been achieved within the context of type IIA string theory compactified on K3 × T 2 5 . The scheme of the paper goes as follows .First we review briefly the work done in ref . 1 .Then we apply the ansatz for the metric and the forms when both H - and F - fluxes exist together . Next we solve the Einstein s equation and find the explicit expression for the metric .Finally we compute the spectrum",
        "rewrite_text": "**Title: IIB Backgrounds with Five-Form Flux**\n\n**Abstract:** In this study, we investigate the effects of introducing a constant five-form field strength into type IIB supergravity, specifically within the context of the AdS5 x S5 background and its dual N = 4 supersymmetric Yang-Mills theory in four dimensions. Our analysis reveals that the inclusion of this five-form flux leads to an additional term in the equations of motion, which is proportional to the volume form of S5. Notably, this modification does not affect the other fields or the fundamental equations governing the system. The solution derived from this framework exhibits a non-vanishing dilaton at infinity, which can be interpreted as a deformation of the conformal symmetry group through a scale transformation. This deformation implies the emergence of a mass gap for all states, with the exception of those that transform trivially under the unbroken SO(4) subgroup of SU(4). We demonstrate how this mechanism facilitates the generation of masses for all gauge bosons while maintaining the integrity of supersymmetry. Furthermore, we explore potential generalizations of our results, expanding the implications of our findings. \n\nIn the introduction, we reference recent work that utilized the gauged linear sigma model approach to show that it is feasible to deform the maximally supersymmetric Yang-Mills (MSYM) theory in four dimensions, allowing only the vector multiplets to acquire masses while preserving N = 1 supersymmetry. The authors identified two distinct methods for achieving this: by activating a constant three-form H-flux or a constant five-form F-flux. Both approaches were shown to retain half of the original supersymmetries. However, the simultaneous activation of both types of fluxes was not addressed. This paper aims to bridge that gap by presenting new solutions that incorporate both H- and F-fluxes, derived within the framework of type IIA string theory compactified on K3 × T2. We outline our methodology, beginning with a brief review of previous work, followed by the application of our ansatz for the metric and forms in the presence of both fluxes. We then solve Einstein's equations to obtain explicit expressions for the metric and conclude by computing the resulting spectrum.",
        "ori-fast-z-score": -1.0524696231684352,
        "water-fast-z-score": 3.8297084310253524,
        "rewrite-fast-z-score": -1.2632278815997784
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Close Pairs as Proxies for Galaxy Cluster Mergers .\nAbstract:\nWe use the Millennium Simulation to study how close pairs of galaxies evolve into galaxy clusters, and we show that they are good proxies for mergers between dark matter halos.  We find that most (>80%) of the mass in galaxy clusters is assembled through mergers with other galaxy clusters or groups rather than by merging individual galaxies.   The merger rate increases strongly towards higher redshifts, but it decreases rapidly at lower masses because of dynamical friction.    Our results suggest that galaxy cluster formation can be studied using large samples of close galaxy pairs selected from optical surveys such as SDSS. This work was supported by NASA grant NAG5-12140. We thank J. S. Bullock, A. Kravtsov, D. Cohn, R. H. Richstone, B. Tinker, E. Tozzi, P. van Kampen, and W. Zhang for useful discussions.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Close Pairs as Proxies for Galaxy Cluster Mergers . Abstract : We use the Millennium Simulation to study how close sets of clusters evolve into star clusters , and we prove that they are excellent proxies for mergers between dark matter halos .We see that most ( > 80 % ) of the mass in galaxy clusters is gathered through mergers with other galaxy clusters or families instead than by merging individual stars . The merger speed increases strongly towards higher redshifts , but it reduces rapidly at lower masses because of dynamical friction .Our results propose that galaxy cluster formation can be examined utilizing large specimens of close star pairs selected from optical searches such as SDSS . This effort was supported by NASA award NAG5 - 12140 .We praise J . S . Bullock , A . Kravtsov , D . Cohn , R . H . Richstone , B . Tinker , E . Tozzi , P . van Kampen , and W . Zhang for useful talks .",
        "rewrite_text": "In this study, we leverage the Millennium Simulation to investigate the evolutionary processes of closely grouped galaxy clusters, demonstrating their effectiveness as indicators of mergers between dark matter halos. Our findings reveal that over 80% of the mass within galaxy clusters is acquired through mergers with other clusters or cluster families, rather than through the merging of individual stars. We observe a significant increase in merger rates at higher redshifts, while at lower masses, these rates decline sharply due to the effects of dynamical friction. This research suggests that the formation of galaxy clusters can be effectively analyzed using large samples of closely associated star pairs identified through optical surveys, such as the Sloan Digital Sky Survey (SDSS). The implications of our work extend to understanding the dynamics of galaxy cluster evolution and the role of mergers in cosmic structure formation. This research was made possible through the support of NASA award NAG5-12140. We would like to acknowledge the valuable discussions and insights provided by J. S. Bullock, A. Kravtsov, D. Cohn, R. H. Richstone, B. Tinker, E. Tozzi, P. van Kampen, and W. Zhang, which greatly contributed to the development of this study.",
        "ori-fast-z-score": -1.75,
        "water-fast-z-score": 5.0854241181575475,
        "rewrite-fast-z-score": 1.6081688022566922
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Unplugging the Universe: the neglected electromagnetic consequence of decoupling .\nAbstract:\nWe show that, in addition to gravitational waves and neutrinos, there is an additional source of energy loss during the final stages of stellar evolution which has been largely ignored by previous authors. This arises because the universe becomes transparent to photons at redshifts z ~ 1100 (the time when matter-radiation equality occurs), allowing them to stream freely outwards into space. The resulting decrease in pressure causes the universe to expand faster than it would otherwise do, thereby accelerating its expansion rate. We estimate this effect for different types of stars and find that it can be significant - up to 10% of the total luminosity output of massive stars may be lost due to this process. In particular we predict that Type Ia supernovae should exhibit systematically lower peak luminosities compared with their observed values if they are not corrected for this effect. Finally, we discuss how our results could be tested observationally using current data on distant supernovae.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Unplugging the Universe : the overlooked electromagnetic consequence of decoupling . Abstract : We suggest that , in addition to gravitational waves and neutrinos , there is an additional source of power loss during the last phases of stellar evolution which has been mostly overlooked by earlier authors .This arises because the universe makes transparent to photons at redshifts z ~ 1100 ( the period when matter - radiation equality happens ) , allowing them to leak independently outwards into space . The resulting decrease in pressure creates the universe to expand faster than it would normally do , thereby accelerating its expansion speed .We estimate this effect for different kinds of stars and find that it can be considerable - up to 10 % of the total luminosity production of large stars must be lost due to this process . In particular we expect that Type Ia supernovae should exhibit systematically lower peak luminosities compared with their observed values if they are not corrected for this effect .Finally , we talk how our findings may be evaluated observationally using current data on remote supernovae .",
        "rewrite_text": "Title: Unplugging the Universe: The Overlooked Electromagnetic Consequence of Decoupling\n\nAbstract: In this article, we propose a novel perspective on the power loss mechanisms during the final stages of stellar evolution, highlighting an often-neglected electromagnetic phenomenon. While gravitational waves and neutrinos have been recognized as significant contributors to energy loss, we introduce the concept that the universe became transparent to photons at redshifts around z ~ 1100, coinciding with the epoch of matter-radiation equality. This transparency allows photons to escape freely into space, leading to a reduction in pressure that accelerates the expansion of the universe beyond its expected rate. Our analysis estimates the magnitude of this effect across various stellar types, revealing that it can account for a substantial fraction—up to 10%—of the total luminosity produced by massive stars. This finding has critical implications for the observed characteristics of Type Ia supernovae, suggesting that their peak luminosities may be systematically underestimated unless this effect is taken into account. We further discuss potential observational strategies to validate our predictions, utilizing existing data on distant supernovae to assess the impact of this electromagnetic loss on their luminosity profiles. Our work invites a reevaluation of stellar evolution models and the interpretation of supernova observations, emphasizing the need to consider this overlooked aspect of cosmic dynamics in future research.",
        "ori-fast-z-score": -0.43133109281375365,
        "water-fast-z-score": 5.116817192534651,
        "rewrite-fast-z-score": 0.29559878344928797
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Inflationary Perturbations: the Cosmological Schwinger Effect .\nAbstract:\nWe show that inflationary perturbations are generated by quantum fluctuations in the inflaton field, which can be understood as the cosmological version of the Schwinger effect.  We also discuss how this mechanism is related to other proposals for generating primordial density fluctuations and present some new results on its phenomenology. The recent detection of temperature anisotropies in the cosmic microwave background radiation (CMBR) has provided strong evidence for an early phase of accelerated expansion known as inflation  1  . In addition, it has been shown  2  that these observations are consistent with predictions based on slow-roll models  3  , where the energy density decreases slowly during inflation due to friction-like effects  4  .\nIn order to explain why such a period of rapid expansion occurred, one usually invokes a scalar field called  inflaton  whose potential V(φ) drives inflation when it becomes flat enough  5  . However, there exists no compelling theoretical reason for choosing any particular form for V(φ). For example, if we assume that V(φ) = m 2 φ 2 /2 + λφ 4 /4!, then the resulting dynamics will depend crucially on whether or not the mass term dominates over the quartic self-interaction  6  . This ambiguity leads us to consider more general forms for V(φ), including those containing higher-order terms  7, 8  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Inflationary Perturbations : the Cosmological Schwinger Effect . Abstract : We see that inflationary perturbations are produced by quantum fluctuations in the inflaton field , which can be understood as the cosmological version of the Schwinger effect .We also discuss how this mechanism is related to other proposals for generating primordial density fluctuations and present some new data on its phenomenology . The recent discovery of temperature anisotropies in the cosmic microwave background radiation ( CMBR ) has given strong evidence for an early stage of rapid expansion termed as inflation 1 .In addition , it has been shown 2 that these observations are compatible with predictions based on slow - roll models 3 , where the power concentration drops slowly during inflation due to friction - like effects 4 . In order to explain why such a period of rapid increase occurred , one usually invokes a scalar field called inflaton whose potential V ( φ ) drives inflation when it becomes flat enough 5 .However , there exists no compelling theoretical motivation for chose any specific form for V ( φ ) . For instance , if we suppose that V ( φ ) = m 2 φ 2 / 2 + λφ 4 / 4 ! , then the resulting dynamics will depend crucially on whether or not the mass term dominates over the quartic self - interaction 6 .This ambiguity causes us to consider more general expressions for V ( φ ) , notably those featuring larger - order symbols 7 , 8 .",
        "rewrite_text": "**Title: Inflationary Perturbations: The Cosmological Schwinger Effect**\n\n**Abstract:** This article explores the generation of inflationary perturbations as a consequence of quantum fluctuations within the inflaton field, drawing parallels to the cosmological Schwinger effect. We delve into the implications of this mechanism in relation to various theories proposed for the origin of primordial density fluctuations, presenting new insights into its phenomenological aspects. The recent detection of temperature anisotropies in the cosmic microwave background radiation (CMBR) has provided compelling evidence for a rapid expansion phase in the early universe, commonly referred to as inflation. Furthermore, it has been demonstrated that these observations align with predictions derived from slow-roll inflation models, where the power spectrum exhibits a gradual decline during inflation due to friction-like effects. To account for the occurrence of such a rapid expansion, theorists typically invoke a scalar field known as the inflaton, whose potential energy function V(φ) is crucial in driving inflation when it becomes sufficiently flat. However, there is a lack of strong theoretical justification for selecting any particular form for V(φ). For example, if we consider a potential of the form V(φ) = m²φ²/2 + λφ⁴/4!, the dynamics of inflation will significantly depend on whether the mass term predominates over the quartic self-interaction. This uncertainty prompts us to investigate more generalized forms of V(φ), particularly those incorporating higher-order terms. Our findings contribute to a deeper understanding of the mechanisms behind inflationary perturbations and their role in the evolution of the universe, offering a fresh perspective on the interplay between quantum field theory and cosmological phenomena.",
        "ori-fast-z-score": 1.0206207261596576,
        "water-fast-z-score": 5.656854249492381,
        "rewrite-fast-z-score": 1.0169503597462533
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Multifractality and scale invariance in human heartbeat dynamics .\nAbstract:\nThe multifractal nature of the human heartbeat has been studied by many authors, but most studies have focused on short time series (typically less than one hour). In this work we study the scaling properties of long-term recordings of the electrocardiograms (ECG) of healthy subjects during sleep using wavelet transforms to calculate the singularity spectrum f(α), which characterizes the degree of multifractality present in each signal.  We find that all signals are characterized by a broad range of exponents α indicating strong multifractality. The width Δα of these spectra decreases with increasing recording length T as Δα~T-1/2 for T<10 hours and Δα~T-3/4 for T>10 hours. This indicates that the ECGs become more monofractal-like when longer recordings are considered. Finally, we show how our results can be used to detect pathological changes in the heart rate variability associated with congestive heart failure.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Multifractality and scale invariance in human heartbeat systems . Abstract : The multifractal nature of the human heartbeat has been studied by many writers , but most studies have concentrated on short period series ( generally less than one minute ) .In this research we study the scaling qualities of long - term recordings of the electrocardiograms ( ECG ) of healthy participants during sleep use wavelet transforms to estimate the singularity spectrum f ( α ) , which characterizes the degree of multifractality observed in each signal . We see that all signals are characterized by a broad variety of exponents α marking strong multifractality .The width Δα of these spectra decreases with higher tape length T as Δα ~ T - 1 / 2 for T < 10 hours and Δα ~ T - 3 / 4 for T > 10 hours . This implies that the ECGs get more monofractal - like when longer recordings are considered .Finally , we show how our findings can be used to identify pathological alterations in the chest rate variability associated with congestive heart failure .",
        "rewrite_text": "Title: Multifractality and Scale Invariance in Human Heartbeat Systems\n\nAbstract: The multifractal characteristics of human heartbeat dynamics have garnered significant attention in the scientific community; however, most existing research has primarily focused on short-duration series, typically lasting less than one minute. In this study, we investigate the scaling properties of long-term electrocardiogram (ECG) recordings from healthy individuals during sleep. Utilizing wavelet transforms, we estimate the singularity spectrum f(α), which provides insight into the degree of multifractality present in each ECG signal. Our analysis reveals that all recorded signals exhibit a wide range of exponents α, indicative of pronounced multifractality. Notably, we observe that the width Δα of the singularity spectra diminishes with increasing recording duration T, following a relationship of Δα ~ T^(-1/2) for T < 10 hours and Δα ~ T^(-3/4) for T > 10 hours. This trend suggests that as the length of the ECG recordings increases, the signals tend to exhibit more monofractal-like behavior. Furthermore, we discuss the implications of our findings for clinical applications, particularly in the identification of pathological changes in heart rate variability associated with conditions such as congestive heart failure. By enhancing our understanding of the multifractal nature of heartbeats, this research contributes to the development of more effective diagnostic tools for monitoring cardiovascular health.",
        "ori-fast-z-score": -0.9428090415820635,
        "water-fast-z-score": 5.114896104728048,
        "rewrite-fast-z-score": 0.5241424183609592
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Carbon Nanotube Thin Film Field Emitting Diode: Understanding the System Response Based on Multiphysics Modeling .\nAbstract:\nCarbon nanotubes (CNTs) are promising materials for field emission devices due to their unique physical and chemical properties, such as high aspect ratio, low work function, and excellent mechanical strength.  In this study, we present an integrated multiphysics model that can be used to simulate the system response of CNT-based field emitting diodes (FEDs). The proposed model consists of three sub-models: 1) electron transport in CNT; 2) electrostatic potential distribution; 3) current density distribution. We have developed these models using COMSOL Multiphysics software package with built-in physics modules. To verify our simulation results, we fabricated a CNT-FED device by growing vertically aligned CNTs onto silicon substrate via plasma-enhanced chemical vapor deposition method followed by sputtering gold film over them. Our experimental data show good agreement with simulated results obtained from the proposed model.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Carbon Nanotube Thin Film Field Emitting Diode : Understanding the System Response Based on Multiphysics Modeling . Abstract : Carbon nanotubes ( CNTs ) are promising technologies for field emission devices owing to their exceptional physical and biological qualities , such as great aspect ratio , low work function , and good mechanical strength .In this study , we present an unified multiphysics description that can be used to simulate the system response of CNT - based field emitting diodes ( FEDs ) . The proposed theory involves of three sub - models : 1 ) electron transport in CNT ; 2 ) electrostatic potential distribution ; 3 ) current density flow .We have developed these models using COMSOL Multiphysics programming package with built - in physics elements . To secure our modeling results , we fabricated a CNT - FED device by spreading vertically aligned CNTs onto silicon substrate via plasma - augmented molecular vapor deposition system followed by sputtering gold film over them .Our experimental studies demonstrate excellent compliance with simulated findings obtained from the suggested model .",
        "rewrite_text": "**Title:** Carbon Nanotube Thin Film Field Emitting Diode: Insights into System Response Through Multiphysics Modeling\n\n**Abstract:** Carbon nanotubes (CNTs) have emerged as a groundbreaking technology for field emission devices due to their remarkable physical and biological properties, including a high aspect ratio, low work function, and impressive mechanical strength. This article presents a comprehensive multiphysics framework designed to simulate the system response of CNT-based field emitting diodes (FEDs). The proposed theoretical model encompasses three critical sub-models: (1) electron transport within the CNTs, (2) the distribution of electrostatic potential, and (3) the flow of current density. Utilizing the COMSOL Multiphysics software, we developed these models by integrating various built-in physics elements to accurately capture the complex interactions within the system.\n\nTo validate our modeling approach, we fabricated a CNT-FED device by depositing vertically aligned CNTs onto a silicon substrate using a plasma-augmented molecular vapor deposition technique, followed by the sputtering of a gold film over the CNTs. Our experimental results exhibit a strong correlation with the simulated data derived from the proposed multiphysics model, confirming its effectiveness in predicting the behavior of CNT-based FEDs. This study not only enhances the understanding of the underlying mechanisms governing the performance of CNT-FEDs but also paves the way for future advancements in the design and optimization of field emission devices utilizing carbon nanotube technology. Through this unified modeling approach, we aim to contribute valuable insights into the development of next-generation electronic devices that leverage the unique properties of carbon nanotubes.",
        "ori-fast-z-score": -1.6269784336399213,
        "water-fast-z-score": 4.313310928137536,
        "rewrite-fast-z-score": 0.09016696346674323
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Harmonic sections in sphere bundles, normal neighborhoods of reduction loci, and instanton moduli spaces on definite 4-manifolds .\nAbstract:\nWe study the geometry of harmonic maps into spheres with values in vector bundles over Riemann surfaces. We prove that if such a map is not constant then it has no critical points outside its singular set (Theorem 1). This implies that any harmonic section of an oriented rank 2 bundle over a closed surface can be deformed to a smooth one without changing its homotopy class (Corollary 3).\nIn particular we show how this result leads to new proofs for some results about instantons on 4-dimensional manifolds due to Donaldson  D1  ,  D2  . In fact our proof gives more information than those given by Donaldson s arguments since it allows us to control the behavior of the harmonic section near its singularities. Finally we give examples showing that these results are sharp. The main theorem of this article states that every non-constant harmonic section of an oriented 2-plane bundle over a closed surface S can be deformed to another harmonic section which is smooth everywhere except at isolated points where it has only simple poles.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Harmonic parts in sphere bundles , ordinary neighborhoods of reduction loci , and instanton moduli spaces on definite 4 - manifolds . Abstract : We research the topology of harmonic maps into spheres with values in vector bundles over Riemann spheres .We prove that if such a mapping is not zero then it has no important points outside its singular set ( Theorem 1 ) . This implies that any harmonic section of an oriented rank 2 bundle over a closed surface can be deformed to a smooth one without altering its homotopy class ( Corollary 3 ) .In particular we tell how this consequence leads to novel proofs for some results about instantons on 4 - dimensional manifolds due to Donaldson D1 , D2 . In reality our proof provides more information than those given by Donaldson s arguments since it allows us to control the dynamics of the chord section near its singularities .Finally we give examples demonstrating that these results are sharp . The main theorem of this page states that every non - constant harmonic section of an oriented 2 - plane bundle over a closed surface S can be deformed to another harmonic section which is continuous everywhere except at isolated points where it has only simple poles .",
        "rewrite_text": "In this article, we investigate the topology of harmonic maps that take values in vector bundles over Riemann spheres, specifically focusing on their behavior when mapped into spheres. Our primary result, encapsulated in Theorem 1, establishes that if a harmonic map is non-zero, it does not possess critical points outside of its singular set. This finding has significant implications, particularly in the context of harmonic sections of oriented rank 2 bundles over closed surfaces. We demonstrate, as a corollary (Corollary 3), that such harmonic sections can be smoothly deformed without changing their homotopy class.\n\nMoreover, we explore how this result provides new proofs for established findings regarding instantons on 4-dimensional manifolds, as originally presented by Donaldson in his works D1 and D2. Notably, our approach offers deeper insights than those derived from Donaldson's original arguments, as it enables us to analyze the dynamics of the chord section in proximity to its singularities. \n\nTo illustrate the sharpness of our results, we present several examples that highlight the boundaries of our findings. The central theorem articulated in this paper asserts that any non-constant harmonic section of an oriented 2-plane bundle over a closed surface S can be continuously deformed into another harmonic section that remains continuous except at isolated points, where it exhibits only simple poles. This work not only advances the understanding of harmonic maps and their topological properties but also enriches the discourse surrounding instanton theory in the context of 4-manifolds.",
        "ori-fast-z-score": -0.10846522890932808,
        "water-fast-z-score": 4.528976474544414,
        "rewrite-fast-z-score": 3.0510802855858956
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Guiding-fields for phase-separation: Controlling Liesegang patterns .\nAbstract:\nWe study the formation and control of Liesegang patterns in an electrochemical system by applying external electric fields to modulate the local concentration gradients during precipitation reactions.  We show that, under certain conditions, the applied field can be used as a guiding field to direct the growth of precipitate bands along specific directions. The results are explained using a simple model based on the competition between diffusion and reaction rates at different locations within the sample. This work provides new insights into how chemical systems may self-organize through interactions with their environment. Chemical systems often exhibit complex spatial structures such as stripes or rings which form spontaneously without any externally imposed symmetry breaking  1  . These structures have been observed in many natural phenomena including mineral deposits  2  , biological tissues  3  , and even living organisms  4  .\nThe most famous example is the so-called  Liesegang ring  formed when two solutions containing metal ions react chemically  5  . In this case, the initial solution contains both cations (e.g., Ag+) and anions (e.g., Cl-). When these two solutions come into contact, they begin to diffuse across each other until they meet another interface where the opposite charges neutralize one another  6  . At some point after mixing, precipitation occurs leading to the formation of a band of solid material separating the original solutions  7, 8  . As more bands grow, they eventually overlap forming concentric rings around the center of the sample  9  . Although the exact mechanism behind the formation of Liesegang rings remains unclear  10  , it has been shown experimentally that the spacing between successive rings depends strongly on the concentrations of the starting solutions  11  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Guiding - fields for phase - separation : Controlling Liesegang patterns . Abstract : We research the formation and control of Liesegang patterns in an electrochemical system by using external electric fields to modulate the local concentration gradients during precipitation reactions .We see that , under certain conditions , the applied field can be used as a guiding field to direct the development of precipitate bands along particular directions . The results are explained using a simple model focused on the competition between diffusion and reaction rates at different places within the sample .This research provides new understanding into how chemical structures could self - organize through relationships with their environment . Chemical systems often exhibit intricate visual formations such as stripes or rings which create spontaneously without any externally imposed symmetry breaking 1 .These structures have been observed in different biological phenomena including mineral minerals 2 , biological tissues 3 , and even living organisms 4 . The most famous example is the so - called Liesegang ring formed when two solutions containing metal ions react chemically 5 .In this instance , the first solution comprises both cations ( e . g . , Ag + ) and anions ( e . g . , Cl - ) . When these two solutions come into contact , they start to diffuse across each other until they meet another interface where the opposite charges neutralize one another 6 .At some time after mixing , precipitation occurs leading to the formation of a band of solid material separating the original solutions 7 , 8 . As more bands expand , they eventually overlap becoming concentric rings around the center of the sample 9 .Although the exact mechanism behind the formation of Liesegang rings appears unclear 10 , it has been shown experimentally that the spacing between successive rings depends strongly on the levels of the starting solutions 11 .",
        "rewrite_text": "**Title:** Guiding Fields for Phase Separation: Controlling Liesegang Patterns\n\n**Abstract:** This study investigates the formation and manipulation of Liesegang patterns within an electrochemical system by employing external electric fields to influence local concentration gradients during precipitation reactions. Our findings reveal that, under specific conditions, the application of an electric field can serve as a guiding mechanism, directing the growth of precipitate bands along designated pathways. We elucidate these results through a straightforward model that emphasizes the interplay between diffusion and reaction rates at various locations within the sample. This research enhances our understanding of how chemical structures can self-organize in response to their environmental conditions. \n\nChemical systems frequently display complex visual patterns, such as stripes or rings, which emerge spontaneously without any external symmetry-breaking influences. These phenomena have been documented in a variety of biological contexts, including mineral formations, biological tissues, and even within living organisms. A prominent example of this is the Liesegang ring, which forms when two solutions containing metal ions undergo a chemical reaction. In this scenario, one solution contains both cations (e.g., Ag⁺) and anions (e.g., Cl⁻). Upon contact, these solutions begin to diffuse into one another until they reach an interface where opposite charges neutralize, leading to precipitation and the formation of a solid band that separates the original solutions. As additional bands form and expand, they ultimately overlap, resulting in concentric rings around the center of the sample. Although the precise mechanisms governing the formation of Liesegang rings remain somewhat elusive, experimental evidence indicates that the spacing between successive rings is significantly influenced by the concentrations of the initial solutions. This research not only sheds light on the dynamics of Liesegang patterns but also opens avenues for further exploration into the self-organization of chemical systems in response to external stimuli.",
        "ori-fast-z-score": 2.482817665807104,
        "water-fast-z-score": 7.175639059928206,
        "rewrite-fast-z-score": 2.0344711469278987
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Predicting the connectivity of primate cortical networks from topological and spatial node properties .\nAbstract:\nWe present an approach to predict the structural network organization in the cerebral cortex based on local node features, such as their position within the brain s surface or volume, and global topological characteristics. We use this method to study how different types of nodes are connected with each other across species (human, macaque monkey) and modalities (diffusion MRI tractography). Our results show that our model can accurately reproduce known patterns of cortico-cortical connections between areas, including those observed in humans but not yet described for monkeys. The proposed framework is general enough to be applied to any type of data where information about individual nodes  positions and pairwise interactions exists. This includes both anatomical and functional imaging datasets, which will allow us to investigate the relationship between structure and function at multiple scales. \n \n Introduction \n \n Brain connectomics aims to map all neuronal elements into a single comprehensive description of the human brain  1  . In recent years, advances in neuroimaging techniques have allowed researchers to obtain detailed maps of the brain s structural  2  , metabolic  3  , and functional  4  architecture. These new technologies provide unprecedented opportunities to understand how the brain works by studying its large-scale organization  5  .\n \nHowever, despite these advancements, there remains significant uncertainty regarding the precise nature of the relationships among neurons  6  . For example, it has been shown that some regions of the brain communicate more frequently than others  7-9 , while others exhibit higher levels of synchrony  10  . However, we still do not know whether these differences reflect specific wiring rules  11  or simply arise due to random fluctuations  12  . \n \n Here, we propose a novel computational framework to address this problem using machine learning methods  13  . Specifically, we aim to develop models capable of predicting the strength of connection between pairs of nodes given only information about their location and topology  14  . To achieve this goal, we first construct a set of training examples consisting of pairs of nodes whose interaction strengths are known  15  . Then, we train a classifier to learn the mapping between node features and edge weights  16  . Finally, we apply the trained model to unseen test cases  17  to infer unknown interactions",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Predicting the connectivity of primate cortical networks from topological and spatial node characteristics . Abstract : We present an approach to predict the structural network structure in the brain cortex based on local node characteristics , such as their placement within the brain s surface or volume , and global topological traits .We use this technology to study how various types of nodes are connected with each other across taxa ( human , macaque animal ) and modalities ( diffusion MRI tractography ) . Our results show that our model can accurately demonstrate established trends of cortico - cortical networks between zones , notably those observed in humans but not already explained for monkeys .The proposed framework is general enough to be applied to any type of evidence where information about individual nodes positions and pairwise relationships exists . This encompasses both anatomical and functional imaging datasets , which will let us to examine the relationship between form and function at multiple scales .Introduction Brain connectomics aims to map all neuronal components into a single comprehensive account of the human mind 1 . In recent months , advances in neuroimaging techniques have permitted investigators to obtain detailed maps of the brain s structural 2 , metabolic 3 , and physiological 4 architecture .These new inventions offer unprecedented possibilities to realize how the brain acts by examining its large - scale organization 5 . However , despite these advancements , there exists significant confusion regarding the exact nature of the relationships among neurons 6 .For instance , it has been shown that some regions of the brain communicate more frequently than others 7 - 9 , while many exhibit greater levels of synchrony 10 . However , we also do not understand whether these changes reflect specific wiring requirements 11 or simply arise due to random fluctuations 12 .Here , we develop a new computational framework to tackle this question using computer learning techniques 13 . Specifically , we attempt to develop models capable of predicting the strength of relationship between pairs of nodes given only data about their direction and topology 14 .To achieve this goal , we first build a setting of training instance comprised of pairs of nodes whose interaction abilities are known 15 . Then , we train a classifier to teach the mapping between node characteristics and edge weights 16 .Finally , we apply the trained model to unseen test cases 17 to infer unknown interactions",
        "rewrite_text": "**Title:** Predicting the Connectivity of Primate Cortical Networks from Topological and Spatial Node Characteristics\n\n**Abstract:** In this study, we introduce a novel methodology for predicting the structural organization of cortical networks in the brain by leveraging local node characteristics—such as their spatial positioning on the brain's surface or within its volume—and global topological features. Our approach is applied to investigate the interconnectivity of various node types across different primate species, specifically humans and macaques, utilizing diffusion MRI tractography as our primary modality. The findings indicate that our predictive model effectively captures established patterns of cortico-cortical connectivity, particularly those documented in humans, while also revealing connections that have not been previously characterized in macaques. This framework is versatile and can be adapted to analyze any dataset where information regarding individual node locations and their pairwise relationships is available. This includes both anatomical and functional imaging data, enabling a comprehensive exploration of the interplay between structural form and functional dynamics across multiple scales.\n\nThe field of brain connectomics aims to create a detailed mapping of neuronal components, contributing to a holistic understanding of the human mind. Recent advancements in neuroimaging technologies have facilitated the generation of intricate maps detailing the brain's structural, metabolic, and physiological architectures. These innovations present unprecedented opportunities to investigate the brain's operational mechanisms through its large-scale organization. However, despite these technological strides, significant ambiguities persist regarding the nature of neuronal interactions. Research has demonstrated that certain brain regions exhibit more frequent communication, while others display heightened synchrony. Yet, it remains unclear whether these patterns signify specific wiring requirements or are merely the result of random fluctuations.\n\nTo address these uncertainties, we have developed a computational framework utilizing machine learning techniques. Our objective is to create models that can predict the strength of relationships between pairs of nodes based solely on their directional and topological data. We initiate this process by constructing a training dataset comprising known interactions between node pairs. Subsequently, we train a classifier to establish the correlation between node characteristics and edge weights. Finally, we apply the trained model to novel test cases to infer previously unknown interactions, thereby enhancing our understanding of cortical connectivity in primates.",
        "ori-fast-z-score": -0.7877263614433762,
        "water-fast-z-score": 8.857142857142858,
        "rewrite-fast-z-score": 1.7888543819998317
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Symmetries in Differential Geometry: A Computational Approach to Prolongations .\nAbstract:\nThe aim of this thesis is the study and development of computational methods for prolongation structures, which are used as tools in geometric analysis. The main focus lies on the construction of explicit formulas for the prolonged action of vector fields on tensor bundles over manifolds with symmetries. In particular we consider Lie groups acting by diffeomorphisms on Riemannian or pseudo-Riemannian manifolds. We present an algorithm that computes the prolonged action of a given vector field on any tensor bundle associated to such a manifold. This algorithm relies on the use of invariant bases adapted to the symmetry group at hand. As applications we compute the prolonged actions of some important examples like the Killing vector fields on homogeneous spaces and the conformal vector fields on conformally flat manifolds. Finally, we show how our results can be applied to construct new families of solutions to Einstein s equations. Keywords: Geometric Analysis, Manifold Symmetry Group",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Symmetries in Differential Geometry : A Computational Approach to Prolongations . Abstract : The goal of this dissertation is the study and evolution of computational materials for prolongation objects , which are applied as tools in geometric analysis .The main interest lies on the creation of explicit formulas for the prolonged operation of vector fields on tensor bundles over manifolds with symmetries . In particular we define Lie fields acted by diffeomorphisms on Riemannian or pseudo - Riemannian manifolds .We present an algorithm that computes the prolonged operation of a given tensor field on any tensor bundle corresponding to such a manifold . This method relies on the using of invariant bases adapted to the symmetry class at hand .As applications we compute the prolonged actions of some important examples like the Killing matrix fields on homogeneous spaces and the conformal vector fields on conformally flat manifolds . Finally , we tell how our findings can be applied to build new families of solutions to Einstein s equations .Keywords: Geometric Analysis, Manifold Symmetry Group",
        "rewrite_text": "Title: Symmetries in Differential Geometry: A Computational Approach to Prolongations\n\nAbstract: This dissertation focuses on the development and advancement of computational tools for prolongation objects, which serve as essential instruments in the field of geometric analysis. The primary objective is to derive explicit formulas for the prolonged operations of vector fields on tensor bundles over manifolds exhibiting symmetries. Specifically, we explore the concept of Lie fields that are influenced by diffeomorphisms on Riemannian and pseudo-Riemannian manifolds. To facilitate this, we introduce an algorithm designed to compute the prolonged operation of a specified tensor field across any tensor bundle associated with such manifolds. This computational approach is grounded in the use of invariant bases that are tailored to the symmetry class under consideration. \n\nWe demonstrate the efficacy of our method by applying it to several significant cases, including the computation of prolonged actions for Killing matrix fields on homogeneous spaces and conformal vector fields on conformally flat manifolds. These examples illustrate the practical applications of our findings in the context of geometric analysis. Furthermore, we discuss how our results can be leveraged to construct new families of solutions to Einstein's equations, thereby contributing to the broader understanding of geometric structures in theoretical physics. This work not only enhances the computational toolkit available for researchers in differential geometry but also opens new avenues for exploring the interplay between symmetry and geometry in mathematical physics.\n\nKeywords: Geometric Analysis, Manifold Symmetry Group",
        "ori-fast-z-score": -0.6974858324629157,
        "water-fast-z-score": 4.649905549752772,
        "rewrite-fast-z-score": -0.09759000729485333
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Strings as sigma models and in the tensionless limit .\nAbstract:\nWe consider strings propagating on curved backgrounds, with emphasis on their role as sigma-models. We show that the world-sheet action for such strings can be written in terms of an auxiliary field which is related to the extrinsic curvature of the target space. This allows us to study string propagation by solving the equations of motion for this auxiliary field instead of directly solving the equation of motion for the embedding coordinates. In particular we discuss how this approach simplifies calculations when studying strings in AdS spaces or near black holes. Finally, we use our formalism to investigate the behavior of strings at high energies where they become tensionless. We find that these strings are described by a conformal theory whose central charge vanishes. The corresponding Virasoro generators have negative norm states so that the Hilbert space has indefinite metric. These results provide further evidence that tensionless strings may play an important rôle in understanding quantum gravity.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Strings as sigma models and in the tensionless maximum . Abstract : We consider strings propagating on curved landscapes , with emphasis on their role as sigma - models .We see that the world - sheet effect for such strings can be written in terms of an auxiliary field which is related to the extrinsic curvature of the target space . This enables us to study string transmission by solving the equations of movement for this auxiliary field instead of specifically solving the equation of movement for the embedding coordinates .In particular we explain how this methodology simplifies calculations when examining strings in AdS spaces or close black holes . Finally , we using our formalism to examine the response of instruments at high energies where they become tensionless .We see that these strings are presented by a conformal theory whose central charge vanishes . The respective Virasoro generators have negative norm states so that the Hilbert space has indefinite metric .These data provide further evidence that tensionless strings might play an important rôle in understanding quantum gravitational .",
        "rewrite_text": "Title: Strings as Sigma Models and in the Tensionless Limit\n\nAbstract: In this study, we investigate the behavior of strings propagating through curved landscapes, focusing on their characterization as sigma models. We demonstrate that the world-sheet dynamics of these strings can be reformulated in terms of an auxiliary field, which is intrinsically linked to the extrinsic curvature of the target space. This approach allows us to analyze string propagation by solving the equations of motion for the auxiliary field, rather than directly addressing the equations governing the embedding coordinates. Our methodology proves particularly advantageous when exploring strings in Anti-de Sitter (AdS) spaces or in proximity to black holes, as it streamlines the computational process. Furthermore, we apply our framework to assess the response of instruments operating at high energies, where strings exhibit tensionless behavior. Our findings reveal that these tensionless strings can be described by a conformal field theory characterized by a vanishing central charge. Notably, the associated Virasoro generators contain negative norm states, resulting in a Hilbert space with an indefinite metric. These observations contribute to the growing body of evidence suggesting that tensionless strings may play a significant role in the quest to understand quantum gravity. Through this work, we aim to enhance the theoretical foundation of string theory and its implications for fundamental physics, particularly in the context of high-energy phenomena and the nature of spacetime.",
        "ori-fast-z-score": -1.6269784336399213,
        "water-fast-z-score": 3.881979835323783,
        "rewrite-fast-z-score": -0.769800358919501
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Photometry of the Globular Cluster NGC 5466: Red Giants and Blue Stragglers .\nAbstract:\nWe present new photometric data for the globular cluster NGC 5466, obtained with the Wide Field Imager at the MPG/ESO 2.2 m telescope in La Silla Observatory (Chile). The observations were carried out on two nights under good seeing conditions. We used the HST/WFPC2 archive images to calibrate our instrumental magnitudes into the standard Johnson-Cousins system. Our results show that there is an excess of blue straggler stars over what would be expected by extrapolating the main sequence turn-off point towards fainter luminosities. This result confirms previous findings based on ground-based photometry. \n \n Keywords: Globular clusters; Blue stragglers; Photometry; ESO 2.2 m; Wide Field Imager. 1 Introduction \n \n In this work we report new photometric data for one of the most studied Galactic globular clusters: NGC 5466. It was discovered by William Herschel during his famous survey of the southern sky  1  . Its distance has been estimated using several methods  2  , giving values ranging between 8 kpc  3  and 12 kpc  4  . These estimates are consistent within their errors but they do not agree with those derived from the analysis of its red giant branch  5  which suggest a larger value of about 16 kpc  6  .\n \nThe first detailed study of the cluster was performed by Trumpler  7  who found it to have a core radius of 3 arcmin and a half-mass relaxation time of 4 Gyrs. Later studies  8  confirmed these results. More recently  9  determined the structural parameters of the cluster finding a core radius of 5.3 arcmin and a concentration parameter c = log(r t /r c ) = 0.6 ± 0.1 where r t is the tidal radius and r c is the core radius. They also found evidence of mass segregation among the brightest stars. \nNGC 5466 shows a bimodal colour distribution  10  as well as a double peaked radial profile  11  . Both features can be explained if the cluster contains both old and intermediate age populations  12  . Indeed, spectroscopic studies",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Photometry of the Globular Cluster NGC 5466 : Red Giants and Blue Stragglers . Abstract : We report new photometric data for the globular cluster NGC 5466 , obtained with the Wide Field Imager at the MPG / ESO 2 . 2 m observatory in La Silla Observatory ( Chile ) .The surveys were carried out on two evenings under good see conditions . We utilized the HST / WFPC2 archive images to calibrate our instrumental magnitudes into the standard Johnson - Cousins scheme .Our results show that there is an excess of blue straggler stars over what would be anticipated by extrapolating the main sequence turn - off position towards fainter luminosities . This result confirms previous findings based on ground - based photometry .Keywords : Globular complexes ; Blue stragglers ; Photometry ; ESO 2 . 2 m ; Wide Field Imager . 1 Introduction In this project we publish new photometric data for one of the most studied Galactic globular galaxies : NGC 5466 .It was noticed by William Herschel during his important survey of the southern sky 1 . Its distance has been estimated using numerous technique 2 , giving measures ranging between 8 kpc 3 and 12 kpc 4 .These projections are compatible within their errors but they do not follow with those generated from the examination of its dark giant branch 5 which confirm a greater value of about 16 kpc 6 . The first detailed analysis of the cluster was done by Trumpler 7 who found it to have a core radius of 3 arcmin and a half - mass relaxation time of 4 Gyrs .Later studies 8 confirmed these results . More recently 9 calculated the structural values of the cluster finding a core radius of 5 . 3 arcmin and a concentration function c = log ( r t / r p ) = 0 . 6 ± 0 . 1 where r t is the tidal diameter and r p is the core radius .They also discovered evidence of mass differentiation among the brightest stars . NGC 5466 shows a bimodal colour distribution 10 as also as a double peaked radial profile 11 .Both features can be described if the cluster comprises both old and intermediate age communities 12 . Indeed , spectroscopic studies",
        "rewrite_text": "We present new photometric observations of the globular cluster NGC 5466, acquired using the Wide Field Imager at the MPG/ESO 2.2 m telescope located at La Silla Observatory in Chile. The data collection took place over two nights characterized by favorable seeing conditions. To ensure the accuracy of our measurements, we calibrated our instrumental magnitudes against the standard Johnson-Cousins photometric system using archival images from the Hubble Space Telescope's WFPC2. Our findings reveal a notable surplus of blue straggler stars, exceeding the expected number based on the extrapolation of the main sequence turn-off point to fainter luminosities. This observation corroborates earlier results derived from ground-based photometric studies.\n\nThe globular cluster NGC 5466, one of the most extensively researched in our galaxy, was first identified by William Herschel during his significant survey of the southern hemisphere. Distance estimates for NGC 5466 have varied, with measurements ranging from 8 kpc to 12 kpc, although these estimates are consistent within their uncertainties. However, analyses of its red giant branch suggest a greater distance of approximately 16 kpc. The initial comprehensive study of the cluster was conducted by Trumpler, who determined a core radius of 3 arcminutes and a half-mass relaxation time of 4 billion years, with subsequent research confirming these findings. More recent investigations have refined the structural parameters, reporting a core radius of 5.3 arcminutes and a concentration parameter of c = log(rt/rp) = 0.6 ± 0.1, where rt is the tidal radius and rp is the core radius. Additionally, evidence of mass segregation among the brightest stars has been observed. NGC 5466 exhibits a bimodal color distribution and a double-peaked radial profile, suggesting the presence of both ancient and intermediate-age stellar populations within the cluster. Spectroscopic analyses further support these conclusions, highlighting the complex stellar dynamics at play in NGC 5466.",
        "ori-fast-z-score": -0.8451542547285166,
        "water-fast-z-score": 6.254141484991022,
        "rewrite-fast-z-score": 0.086710996952412
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Extended envelopes around Galactic Cepheids III. Y Oph and alpha Per from near-infrared interferometry with CHARA/FLUOR .\nAbstract:\nWe present new measurements of the angular diameter variations for two classical Cepheids: Y Ophiuchi (Y Oph) and Alpha Persei (alpha Per). These stars are among the brightest in their class, making them ideal targets to study using infrared interferometry. We used the FLUOR instrument on the CHARA array at Mount Wilson Observatory to obtain high-precision visibility data over several pulsation cycles. The observations were performed simultaneously in H-band (1.6 microns), K-band (2.0 microns), and L-band (3.8 microns).\nThe results show that both stars have extended atmospheres which vary significantly during the pulsations cycle. In particular we find that:  - For Y Oph, our best-fit model is consistent with an atmosphere extending up to about 1 AU above its photosphere.  - For alpha Per, our best-fit models suggest that the star has an extended atmosphere reaching out to more than 2 AU above its photosphere.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  Extended envelopes around Galactic Cepheids III.Y Oph and alpha Per from near-infrared interferometry with CHARA/FLUOR .Abstract : We present new measurements of the angular distance changes for two classical Cepheids : Y Ophiuchi ( Y Oph ) and Alpha Persei ( beta Per ) . These galaxies are among the brightest in their category , making them ideal targets to study using infrared interferometry .We utilized the FLUOR instrument on the CHARA array at Mount Wilson Observatory to obtain high - precision visibility data over several pulsation periods . The surveys were performed simultaneously in H - band ( 1 . 6 microns ) , K - band ( 2 . 0 microns ) , and L - band ( 3 . 8 microns ) .The results show that both stars have extended atmospheres which varies dramatically during the pulsations cycle . In particular we find that : - For Y Oph , our better - fitting model is compatible with an environment stretching up to about 1 AU above its photosphere .- For alpha Per , our better - fitting models suggest that the star has an extended atmosphere extending out to more than 2 AU above its photosphere .",
        "rewrite_text": "We present new findings on the angular distance variations of two classical Cepheids: Y Ophiuchi (Y Oph) and Alpha Persei (α Per). These stars are among the brightest representatives of their class, making them excellent candidates for investigation through infrared interferometry. Utilizing the FLUOR instrument at the CHARA array located at Mount Wilson Observatory, we collected high-precision visibility data across multiple pulsation cycles. Our observations were conducted simultaneously in three near-infrared bands: H-band (1.6 microns), K-band (2.0 microns), and L-band (3.8 microns). \n\nThe analysis of our data reveals that both Y Oph and α Per possess extended atmospheres that exhibit significant variations throughout their pulsation cycles. Specifically, for Y Oph, our optimal fitting model indicates the presence of an extended environment reaching approximately 1 astronomical unit (AU) above its photosphere. In contrast, for α Per, our best-fitting models suggest that its atmosphere extends even further, exceeding 2 AU above the photosphere. These findings contribute to our understanding of the complex atmospheric structures surrounding Cepheid variables and highlight the dynamic nature of their envelopes during pulsation. This research underscores the importance of high-resolution interferometric techniques in studying stellar atmospheres and provides valuable insights into the physical processes governing these fascinating celestial objects.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.677476657214644,
        "rewrite-fast-z-score": 2.1081851067789197
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Protostellar clusters in intermediate-mass (IM) star forming regions .\nAbstract:\nWe present the results of our Spitzer Space Telescope survey for protostars and young stellar objects (YSOs) in three nearby, intermediate mass star-forming regions: NGC 1333, Serpens South, and Perseus North. We identify over 100 candidate YSOs with infrared excesses indicative of circumstellar disks and/or envelopes. The majority are Class I sources that have recently formed outflows or jets; however we also find several dozen more evolved Class II/III sources. In addition to these disk-bearing systems, we detect numerous isolated point-like sources whose SEDs suggest they are deeply embedded protostars. These observations provide new insights into how stars form in IM environments. Our sample includes many previously unidentified low-luminosity protostars which will be useful targets for future studies at higher angular resolution. This work is based on observations made with the Spitzer Space Telescope, which is operated by NASA under contract 1407. Support for this work was provided by NASA through an award issued by JPL/Caltech. \n \n Keywords: Protostar",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Protostellar complexes in intermediate - mass ( IM ) star producing regions . Abstract : We report the conclusion of our Spitzer Space Telescope survey for protostars and young stellar bodies ( YSOs ) in three adjacent , intermediate mass star - creating areas : NGC 1333 , Serpens South , and Perseus North .We recognize over 100 candidate YSOs with infrared excesses indicative of circumstellar disks and / or envelopes . The majority are Class I sources that have newly formed outflows or jets ; however we also find several dozen more evolved Class II / III sources .In addition to these disk - bearing components , we locate many isolated point - like sources whose SEDs suggest they are deeply embedded protostars . These measurements give novel knowledge into how stars shape in IM environments .Our specimen includes several recently unidentified low - luminosity protostars which will be valuable targets for future research at higher angular resolution . This research is based on observations made with the Spitzer Space Telescope , which is controlled by NASA under contract 1407 .Support for this project was provided by NASA through an award published by JPL / Caltech . Keywords : Protostar",
        "rewrite_text": "Title: Protostellar Complexes in Intermediate-Mass Star-Forming Regions\n\nAbstract: In this study, we present the findings from our comprehensive survey utilizing the Spitzer Space Telescope, focusing on protostars and young stellar objects (YSOs) within three closely situated intermediate-mass star-forming regions: NGC 1333, Serpens South, and Perseus North. Our survey has successfully identified over 100 candidate YSOs exhibiting infrared excesses, which are indicative of the presence of circumstellar disks and/or envelopes. The majority of these candidates are classified as Class I sources, characterized by their newly formed outflows or jets. Additionally, we have detected several dozen more advanced Class II and III sources, highlighting the diverse evolutionary stages present in these regions. Beyond these disk-bearing entities, we have also identified numerous isolated point-like sources whose spectral energy distributions (SEDs) suggest they are deeply embedded protostars. This research provides new insights into the processes of star formation in intermediate-mass environments. Notably, our sample includes several previously unidentified low-luminosity protostars, which represent promising targets for future investigations utilizing higher angular resolution techniques. The observations that underpin this research were conducted with the Spitzer Space Telescope, operated by NASA under contract 1407. This project received support from NASA through an award issued by JPL/Caltech. \n\nKeywords: Protostar, Young Stellar Objects, Star Formation, Infrared Astronomy, Spitzer Space Telescope.",
        "ori-fast-z-score": -1.865992419824736,
        "water-fast-z-score": 4.880935300919764,
        "rewrite-fast-z-score": 1.5230192477004287
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Chain-Boson Model for the Decoherence and Relaxation of a Few Coupled SQUIDs in a Phonon Bath .\nAbstract:\nWe present an analytical model to study decoherence and relaxation processes in a few coupled superconducting quantum interference devices (SQUID) immersed into a phonon bath at low temperatures. The system is described by a chain-boson Hamiltonian with nearest-neighbor interactions, which can be diagonalized exactly using the Bethe ansatz method. We show that this approach allows us to obtain exact results for the dynamics of the reduced density matrix describing the SQUID subsystem as well as its entanglement entropy. In particular we find that the decay rate of the off-diagonal elements of the reduced density matrix scales linearly with temperature T , while the von Neumann entropy grows logarithmically with time t. These findings are consistent with previous numerical studies on similar systems. \n \n Introduction \n \n Superconducting circuits have been proposed recently as promising candidates for realizations of quantum information processing  1  . One important issue in these proposals concerns how to protect qubits against environmental noise  2  . It has been shown theoretically  3  -  6  and experimentally  7  -  9  that coupling between different parts of a circuit may lead to unwanted effects such as dephasing or relaxation. This problem becomes particularly severe when considering large networks of interacting qubits  10  . \n \n Here we consider a simple model consisting of two weakly-coupled SQUIDs  11  immersed into a phonon environment  12  . Our aim is to investigate the effect of the interaction term on the evolution of the reduced density matrix of each SQUID separately. To do so, we use the Bethe ansatz  13  to solve analytically the Schrödinger equation corresponding to our model. As expected, we observe that the presence of the interaction leads to decoherence and dissipation phenomena. Moreover, we find that the decay rates of the off-diagonals of the reduced density matrices scale linearly with temperature T , whereas their von Neumann entropies grow logarithmically with time t. \nModel\n\nThe total Hamiltonian H = H0 + V describes the system composed of N = 2 SQUIDs coupled via a weak tunneling amplitude J immersed into a phonon reservoir at zero temperature.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Chain - Boson Model for the Decoherence and Relaxation of a Few Coupled SQUIDs in a Phonon Bath . Abstract : We present an analytical theory to study decoherence and relaxation processes in a few coupled superconducting quantum interference machines ( SQUID ) immersed into a phonon bath at low temperatures .The system is characterized by a chain - boson Hamiltonian with nearest - neighbor interactions , which can be diagonalized precisely using the Bethe ansatz technique . We see that this methodology allows us to obtain exact findings for the dynamics of the reduced density matrix describing the SQUID subsystem as well as its entanglement entropy .In particular we find that the decay rate of the off - diagonal elements of the reduced density matrix scales linearly with temperature T , while the von Neumann entropy increases logarithmically with time t . These conclusions are compatible with previous numerical studies on similar systems . Introduction Superconducting circuits have been proposed lately as promising candidates for realizations of quantum information processing 1 .One important concern in these proposals involves how to keep qubits against environmental interference 2 . It has been shown theoretically 3 - 6 and experimentally 7 - 9 that coupling between multiple parts of a circuit could lead to inappropriate consequences such as dephasing or relaxation .This problem appears particularly severe when examining massive networks of interacting qubits 10 . Here we study a simple model composed of two weakly - coupled SQUIDs 11 immersed into a phonon system 12 .Our aim is to examine the impact of the interaction term on the evolution of the reduced density matrix of each SQUID separately . To do so , we utilize the Bethe ansatz 13 to correct analytically the Schrödinger equation equivalent to our model .As expected , we find that the presence of the interaction results to decoherence and dissipation phenomena . Moreover , we find that the decay rates of the off - diagonals of the reduced density matrices increase linearly with temperature T , whereas their von Neumann entropies grow logarithmically with time t . Model The total Hamiltonian H = H0 + V describes the system composed of N = 2 SQUIDs coupled via a weak tunneling amplitude J immersed into a phonon reservoir at zero temperature .",
        "rewrite_text": "**Title:** A Chain-Boson Model for the Decoherence and Relaxation of a Few Coupled SQUIDs in a Phonon Bath\n\n**Abstract:** In this study, we develop an analytical framework to investigate the decoherence and relaxation phenomena occurring in a system of coupled superconducting quantum interference devices (SQUIDs) that are immersed in a low-temperature phonon bath. The system is modeled using a chain-boson Hamiltonian characterized by nearest-neighbor interactions, which we can precisely diagonalize through the Bethe ansatz method. This approach enables us to derive exact results for the dynamics of the reduced density matrix that describes the SQUID subsystem, as well as its associated entanglement entropy. Our findings reveal that the decay rate of the off-diagonal elements of the reduced density matrix exhibits a linear dependence on temperature (T), while the von Neumann entropy increases logarithmically over time (t). These results align well with previous numerical investigations of similar systems, reinforcing the validity of our analytical approach.\n\nThe motivation for this research stems from the increasing interest in superconducting circuits as viable platforms for quantum information processing. A critical challenge in these systems is maintaining qubit coherence in the presence of environmental disturbances. Prior theoretical and experimental studies have indicated that interactions among multiple circuit components can lead to detrimental effects such as dephasing and relaxation, particularly in extensive networks of interacting qubits. In this work, we focus on a simplified model consisting of two weakly coupled SQUIDs embedded within a phonon environment. Our objective is to analyze how the interaction term influences the evolution of each SQUID's reduced density matrix. By applying the Bethe ansatz, we derive analytical corrections to the Schrödinger equation corresponding to our model. Our results confirm that the interaction leads to significant decoherence and dissipation effects. Specifically, we observe that the decay rates of the off-diagonal elements of the reduced density matrices increase linearly with temperature, while the von Neumann entropies demonstrate a logarithmic growth over time. The total Hamiltonian of the system is expressed as H = H0 + V, where H0 represents the individual SQUIDs and V accounts for their coupling via a weak tunneling amplitude (J) within a phonon reservoir at zero temperature.",
        "ori-fast-z-score": 0.3310423554409472,
        "water-fast-z-score": 6.560592908445457,
        "rewrite-fast-z-score": 2.0344711469278987
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Monitor project: JW 380 -- a 0.26, 0.15 Msol pre main sequence eclipsing binary in the Orion Nebula Cluster .\nAbstract:\nWe report on our photometric and spectroscopic monitoring campaign of the young open cluster NGC 2024 (the Orion Nebula Cluster) aimed at detecting new low-mass members with masses down to ~0.1 Msun. We have discovered one such object which we call JW 380. It is an eclipsing binary system consisting of two very similar stars that are separated by only 1 AU. The primary star has a mass of about 0.25 Msun while its companion has a mass of about 15 percent less. Both components show signs of youth as indicated by their strong H-alpha emission lines. Their age was determined using theoretical evolutionary tracks for PMS stars. Our results suggest that this system is younger than 2 Myr old. This makes it one of the youngest known binaries among all open clusters within 300 pc distance from Earth. In addition, we find evidence that both components are surrounded by circumstellar disks.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Monitor project : JW 380 - - a 0 . 26 , 0 . 15 Msol pre main series eclipsing binary in the Orion Nebula Cluster . Abstract : We report on our photometric and spectroscopic monitoring effort of the young open cluster NGC 2024 ( the Orion Nebula Cluster ) aiming at detecting new lowest - weight members with masses down to ~ 0 . 1 Msun .We have discovered one such object which we call JW 380 . It is an eclipsing binary system consisting of two very related stars that are apart by only 1 AU .The main star has a mass of about 0 . 25 Msun while its companion has a mass of about 15 percent less . Both components display signs of youth as indicated by their weak H - alpha emission lines .Their age was calculated using theoretical evolutionary tracks for PMS stars . Our results show that this system is younger than 2 Myr young .This gives it one of the youngest known binaries among all open nuclei within 300 pc radius from Earth . In addition , we find proof that both components are surrounded by circumstellar disks .",
        "rewrite_text": "We present our findings from a comprehensive photometric and spectroscopic monitoring campaign targeting the young open cluster NGC 2024, commonly known as the Orion Nebula Cluster. Our primary objective was to identify new low-mass stellar members, with masses as low as approximately 0.1 solar masses (M☉). During this investigation, we discovered a noteworthy eclipsing binary system designated as JW 380. This system comprises two closely related stars that are separated by a mere 1 astronomical unit (AU). The primary star has an estimated mass of around 0.25 M☉, while its companion is approximately 15% less massive. Both stars exhibit characteristics indicative of youth, particularly evidenced by their weak H-alpha emission lines. To ascertain their age, we employed theoretical evolutionary tracks for pre-main sequence (PMS) stars, leading us to conclude that JW 380 is less than 2 million years old. This finding positions it among the youngest known binary systems within a 300 parsec radius of Earth. Furthermore, our observations provide compelling evidence that both stellar components are enveloped by circumstellar disks, suggesting ongoing accretion processes. This discovery not only enhances our understanding of low-mass star formation in the Orion Nebula Cluster but also contributes valuable insights into the evolutionary dynamics of young binary systems.",
        "ori-fast-z-score": -0.11704114719613057,
        "water-fast-z-score": 4.4174102722651325,
        "rewrite-fast-z-score": 0.39605901719066977
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Pressure induced Insulator-Metal transition in LaMnO$_3$ .\nAbstract:\nThe pressure-induced insulator-metal (IMT) phase transition is studied by means of the first-principles calculations based on density functional theory within local spin-density approximation and generalized gradient approximations. The calculated results show that the IMT occurs at about 20 GPa for both LSDA and PBE functionals, which are consistent with previous theoretical studies. However, it should be noted that there exists an obvious difference between these two methods when calculating the electronic structure near Fermi level. In addition to this, we also find that the band gap decreases rapidly as increasing pressure up to 30 GPa but then becomes almost constant above 40 GPa. Finally, our calculation shows that the volume collapse takes place around 50 GPa. \n \n Keywords: Pressure-induced insulator-metal transition; First-principles calculations; Local spin-density approximation; Generalized gradient approximations; Electronic structure; Band gap",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Pressure induced Insulator - Metal transition in LaMnO $ _ 3 $ . Abstract : The pressure - triggered insulator - metal ( IMT ) phase shift is studied by means of the first - principles calculations based on density functional theory within local spin - density algorithm and generalized gradient approximations .The measured data demonstrate that the IMT occurs at about 20 GPa for both LSDA and PBE functionals , which are compatible with previous conceptual research . However , it should be mentioned that there exists an evident gap between these two means when calculating the electronic structure near Fermi level .In addition to this , we also find that the band gap falls steadily as increasing temperature up to 30 GPa but then grows almost steady above 40 GPa . Finally , our mathematical reveals that the volume collapse took place around 50 GPa .Keywords : Pressure - caused insulator - iron transition ; First - principles measurements ; Local spin - density approximation ; Generalized gradient approximations ; Electronic structure ; Band gap",
        "rewrite_text": "Title: Pressure-Induced Insulator-Metal Transition in LaMnO$_3$\n\nAbstract: This study investigates the pressure-induced insulator-metal transition (IMT) in LaMnO$_3$ through first-principles calculations grounded in density functional theory, utilizing both the local spin-density approximation (LSDA) and generalized gradient approximations (PBE). Our findings indicate that the IMT occurs at approximately 20 GPa, consistent with prior theoretical studies. However, a notable discrepancy arises in the electronic structure calculations near the Fermi level when comparing the results from LSDA and PBE functionals. Furthermore, we observe that the band gap of LaMnO$_3$ decreases steadily with increasing pressure up to 30 GPa, after which it stabilizes and shows minimal variation above 40 GPa. Additionally, our analysis reveals a significant volume collapse occurring around 50 GPa, which is critical for understanding the material's behavior under extreme conditions. This research enhances the understanding of the pressure effects on the electronic properties of LaMnO$_3$ and contributes to the broader knowledge of phase transitions in correlated electron systems. \n\nKeywords: Pressure-induced insulator-metal transition; First-principles calculations; Local spin-density approximation; Generalized gradient approximations; Electronic structure; Band gap.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.98201895023622,
        "rewrite-fast-z-score": -0.6546536707079772
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The richest superclusters. I. Morphology .\nAbstract:\nWe present the results on morphology and luminosity function for the most luminous galaxy clusters in the Universe, selected by their X-ray emission (the RCS2 sample). We find that these objects are characterized by an elliptical shape with axial ratio q = 0.7 ± 0.1 and by a steep luminosity function dN/dL ∝ L−2.5±0.3 . The observed properties suggest that they may be identified as fossil groups or proto-clusters at z > 1.0 .\nThe data used here were obtained during our observing runs performed at ESO telescopes under programs IDs: 073.A-0505(B), 078.A-0518(C) and 079.A-0739(D) . In this work we study the morphological and photometric properties of the brightest galaxy clusters in the universe. These systems have been detected through their X-ray emission using the ROSAT All Sky Survey (RASS; Voges et al., 1999) , and then followed up spectroscopically to confirm their redshifts and measure their velocity dispersions (see e.g. Rosati et al. , 1998 , Gladders & Yee 2005 , Eisenhardt et al. , 2008 . They represent some of the most massive structures known so far in the universe, being able to host several thousands of galaxies each one. Their high mass makes them ideal targets to investigate how such large scale structures form and evolve over time.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The richest superclusters . I . Morphology .Abstract : We report the results on morphology and luminosity function for the most luminous galaxy galaxies in the Universe , selected by their X - ray radiation ( the RCS2 specimen ) . We see that these objects are marked by an elliptical shape with axial proportion q = 0 . 7 ± 0 . 1 and by a steep luminosity function dN / dL [UNK] L−2 . 5±0 . 3 .The observed properties suggest that they may be identified as extinct families or proto - complexes at z > 1 . 0 . The data used here were obtained during our observing walks performed at ESO telescopes under programs IDs : 073 . A - 0505 ( B ) , 078 . A - 0518 ( C ) and 079 . A - 0739 ( D ) .In this research we study the morphological and photometric properties of the brightest galaxy galaxies in the universe . These systems have been detected through their X - ray emission utilizing the ROSAT All Sky Survey ( RASS ; Voges et al . , 1999 ) , and then followed up spectroscopically to confirm their redshifts and track their velocity dispersions ( see e . g .Rosati et al . , 1998 , Gladders & Yee 2005 , Eisenhardt et al ., 2008 . They hold some of the most gigantic structures discovered so far in the universe , being could to host numerous thousands of galaxies each one .Their high mass creates them ideal targets to examine how such large scale structures structure and evolve over time .",
        "rewrite_text": "**Title: The Richest Superclusters I: Morphology**\n\n**Abstract:** In this study, we present our findings on the morphological characteristics and luminosity function of the most luminous galaxy clusters in the universe, identified through their X-ray emissions from the RCS2 sample. Our analysis reveals that these clusters exhibit an elliptical morphology with an axial ratio of q = 0.7 ± 0.1, alongside a steep luminosity function characterized by dN/dL ∝ L^(-2.5 ± 0.3). These properties suggest that these clusters may represent extinct families or proto-complexes at redshifts greater than z > 1. The data for this research were collected during observational campaigns at the European Southern Observatory (ESO) under the following program IDs: 073.A-0505(B), 078.A-0518(C), and 079.A-0739(D). \n\nOur investigation focuses on the morphological and photometric attributes of the brightest galaxy clusters, which were initially detected via their X-ray emissions in the ROSAT All Sky Survey (RASS; Voges et al., 1999). Subsequent spectroscopic follow-ups were conducted to confirm their redshifts and to monitor their velocity dispersions, as referenced in studies by Rosati et al. (1998), Gladders & Yee (2005), and Eisenhardt et al. (2008). These clusters represent some of the largest structures identified in the universe, each potentially housing thousands of galaxies. Their significant mass makes them prime candidates for exploring the formation and evolution of large-scale structures in the cosmos. This research contributes to our understanding of the dynamics and characteristics of these superclusters, providing insights into the processes that govern their development over cosmic time.",
        "ori-fast-z-score": -0.3216337604513384,
        "water-fast-z-score": 5.116817192534651,
        "rewrite-fast-z-score": 1.8371173070873836
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Factorization analysis for the fragmentation functions of hadrons containing a heavy quark .\nAbstract:\nWe present an improved factorization formula for the fragmentation functions (FFs) of hadrons containing one heavy quark, which is valid in both leading order and next-to-leading order QCD perturbation theory. The new formula takes into account all possible contributions to the FFs at each perturbative order. We show that our results are consistent with those obtained by using other approaches such as the operator product expansion method or the renormalization group equation approach. Finally we give numerical predictions on some important quantities related to the charm-quark FFs. PACS numbers: 12.38.Qk, 13.25.Gv, 11.15.Tk \nI. INTRODUCTORY REMARK\nThe fragmentation function D(z), where z = Phadron/Pquark , describes how quarks fragment into hadrons when they are produced in hard processes like deep-inelastic scattering  1  . It plays an essential role in understanding many phenomena observed experimentally  2  .\nIn this work, we will study the fragmentation functions of hadronic states containing only one heavy quark. In particular, we consider the case of charmed-meson production in e + e-annihilation processes:",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Factorization analysis for the fragmentation functions of hadrons containing a heavy quark . Abstract : We report an updated factorization formula for the fragmentation functions ( FFs ) of hadrons containing one heavy quark , which is valid in both leading order and last - to - leading order QCD perturbation theory .The revised formula takes into consideration all possible contributions to the FFs at each perturbative order . We see that our findings are compatible with those achieved by using other methods such as the operator product expansion method or the renormalization group equation methodology .Finally we give quantitative predictions on some important quantities related to the charm - quark FFs . PACS codes : 12 . 38 . Qk , 13 . 25 . Gv , 11 . 15 . Tk I .INTRODUCTORY REMARK The fragmentation relation D ( z ) , where z = Phadron / Pquark , explains how quarks cluster into hadrons when they are produced in hard processes like deep - inelastic scattering 1 . It plays an essential part in understanding several phenomena observed experimentally 2 .In this study , we will research the fragmentation processes of hadronic states involving only one heavy quark . In particular , we treat the case of charmed - meson production in e + e - annihilation processes :",
        "rewrite_text": "We present an updated factorization formula for the fragmentation functions (FFs) of hadrons that contain a single heavy quark, applicable in both leading order and next-to-leading order QCD perturbation theory. This revised formula comprehensively accounts for all potential contributions to the FFs at each perturbative level. Our analysis demonstrates that the results obtained align well with those derived from alternative approaches, including the operator product expansion and the renormalization group equation methodologies. Additionally, we provide quantitative predictions for several significant parameters related to the charm-quark FFs. \n\nThe fragmentation function D(z), defined as the ratio of the hadron momentum to the quark momentum (z = Phadron / Pquark), is crucial for understanding how quarks aggregate into hadrons during high-energy processes such as deep inelastic scattering. This function plays a vital role in elucidating various experimentally observed phenomena. In this investigation, we focus on the fragmentation processes of hadronic states that involve a single heavy quark, specifically examining the production of charmed mesons in electron-positron annihilation events. Our findings contribute to a deeper understanding of the dynamics involved in heavy quark fragmentation and offer valuable insights for future experimental and theoretical studies in particle physics. \n\nPACS codes: 12.38.Qk, 13.25.Gv, 11.15.Tk.",
        "ori-fast-z-score": -0.5555555555555556,
        "water-fast-z-score": 5.158920219515446,
        "rewrite-fast-z-score": -0.20628424925175867
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Wandering breathers and self-trapping in weakly coupled nonlinear chains: classical counterpart of macroscopic tunneling quantum dynamics .\nAbstract:\nWe study the classical counterparts of macroscopic tunneling phenomena, which are observed in many-body systems with competing interactions. We consider an ensemble of interacting particles on a ring threaded by a magnetic flux. The competition between nearest-neighbor attraction and next-to-nearest neighbor repulsion leads to the formation of localized states (breathers) that can be either pinned or mobile depending on their energy. In particular we show how these breather solutions evolve into spatially extended structures when they become unstable due to collisions with other breathers. Finally, we discuss the possibility for such excitations to form stable bound states. This work is supported by NSF grant DMR-0704520. PACS numbers: 05.45.Mt, 02.10.Yn, 11.30.Pb, 03.65.Nk . \nI. INTRODUCTORY REMARK\nMacroscopic tunneling refers to the phenomenon where a large number of microscopic degrees of freedom coherently contribute to transport across potential barriers  1  , leading to novel physical effects like superfluidity  2  , Josephson effect  3  , Bose-Einstein condensation  4  , etc.. Macroscopic tunneling has been studied extensively both theoretically  5  -  8  as well as experimentally  9  -  11  .\nIn this manuscript we present results concerning the classical counterpart of macroscopic quantum tunneling  12  . More specifically, we investigate the properties of a system consisting of N identical particles moving along a one-dimensional ring threaded by a constant magnetic field. Each particle interacts with its two neighbors via repulsive potentials while it experiences attractive forces from all remaining particles. Such a model was introduced originally by Calogero  13  who showed that the ground state consists of equally spaced particles forming a Wigner crystal  14  . It turns out that the presence of a weak external periodic driving force breaks the translational symmetry of the lattice  15  , giving rise to new types of collective excitations  16  . These excitations have been shown to exhibit features similar to those found in macroscopic quantum tunneling  17  -  20  . For example, if the amplitude of the external drive exceeds",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Wandering breathers and self - trapping in weakly connected nonlinear chains : classical counterpart of macroscopic tunneling quantum mechanics . Abstract : We research the classical counterparts of macroscopic tunneling phenomena , which are observed in multiple - bodies systems with competing interactions .We consider an ensemble of interacting particles on a ring threaded by a magnetic flux . The competition between nearest - neighbor attraction and next - to - nearest neighbor repulsion leads to the formation of localized states ( breathers ) that can be either pinned or mobile depending on their power .In particular we explain how these breather solutions evolve into spatially extended systems when they become unstable due to collisions with other breathers . Finally , we explain the prospect for such excitations to form stable bound states .This project is backed by NSF grant DMR - 0704520 . PACS codes : 05 . 45 . Mt , 02 . 10 . Yn , 11 . 30 . Pb , 03 . 65 . Nk .I . INTRODUCTORY REMARK Macroscopic tunneling refers to the phenomenon where a large number of microscopic degrees of autonomy coherently contribute to transport across potential barriers 1 , leading to novel physical effects like superfluidity 2 , Josephson effect 3 , Bose - Einstein condensation 4 , etc . . Macroscopic tunneling has been studied thoroughly both theoretically 5 - 8 as well as experimentally 9 - 11 .In this manuscript we present results relating the classical counterpart of macroscopic quantum tunneling 12 . More specifically , we investigate the properties of a system consisting of N identical particles moving along a one - dimensional loop threaded by a constant magnetic force .Each particle interacts with its two neighbors via repulsive potentials while it experiences attractive forces from all remaining particles . Such a theory was introduced originally by Calogero 13 who demonstrated that the ground state consists of evenly spaced particles forming a Wigner crystal 14 .It turns out that the presence of a weak external periodic drove force breaks the translational symmetry of the crystal 15 , giving rise to novel sorts of collective excitations 16 . These excitations have been shown to contain features peculiar to those observed in macroscopic particle tunneling 17 - 20 .For instance , if the amplitude of the external drive exceeds",
        "rewrite_text": "**Title:** Wandering Breathers and Self-Trapping in Weakly Connected Nonlinear Chains: Classical Counterpart of Macroscopic Tunneling Quantum Mechanics\n\n**Abstract:** This study explores the classical analogs of macroscopic tunneling phenomena, which are prevalent in many-body systems characterized by competing interactions. We focus on a system of interacting particles arranged in a ring configuration subjected to a magnetic flux. The interplay between nearest-neighbor attractive forces and next-to-nearest-neighbor repulsive interactions results in the emergence of localized states, known as breathers. These breathers can exhibit either mobility or pinning behavior, contingent upon their energy levels. We delve into the dynamics of these breather solutions, particularly how they transition into spatially extended configurations when they become unstable due to interactions with other breathers. Furthermore, we discuss the potential for these excitations to evolve into stable bound states, thereby enhancing our understanding of the underlying mechanisms. This research is supported by NSF grant DMR-0704520 and is situated within the broader context of macroscopic tunneling, a phenomenon where numerous microscopic degrees of freedom coherently contribute to transport across potential barriers, leading to remarkable physical effects such as superfluidity, the Josephson effect, and Bose-Einstein condensation. Previous theoretical and experimental studies have extensively examined macroscopic tunneling. In this manuscript, we present findings that relate to the classical counterpart of macroscopic quantum tunneling. Specifically, we investigate a system of N identical particles moving along a one-dimensional loop influenced by a constant magnetic field. Each particle interacts with its two nearest neighbors through repulsive potentials while experiencing attractive forces from all other particles. This framework, initially proposed by Calogero, reveals that the ground state consists of uniformly spaced particles forming a Wigner crystal. Notably, the introduction of a weak external periodic driving force disrupts the translational symmetry of the crystal, leading to the emergence of novel collective excitations that exhibit characteristics reminiscent of those found in macroscopic particle tunneling.",
        "ori-fast-z-score": -0.5696519211398116,
        "water-fast-z-score": 4.313078831487145,
        "rewrite-fast-z-score": 1.8333333333333333
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Quantum Fields on the Groenewold-Moyal Plane: C, P, T and CPT .\nAbstract:\nThe purpose of this article is to present an overview of some recent results in quantum field theory (QFT) on curved spaces with noncommutative coordinates. The main motivation for studying QFTs on such spaces comes from string theories which are formulated as open strings attached to D-branes whose positions can be described by noncommuting matrices. In particular we will focus our attention on the so-called Groenewold-Moyal plane which is defined as the space generated by two non-commuting coordinates satisfying the commutation relations  qμ(x), qν(y)  = iθμνρqρ(xy). We will show that it is possible to define a covariant derivative operator acting on fields living on the GroenewoldMoyal plane. This allows us to introduce a notion of spinor fields on the Groenewold- Moyal plane. Moreover we will discuss how one can construct gauge invariant actions for these fields. Finally we will study the action of the discrete symmetries C,P,T and CP T .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Quantum Fields on the Groenewold - Moyal Plane : C , P , T and CPT . Abstract : The purpose of this page is to provide an overview of some latest findings in quantum field theory ( QFT ) on curved spaces with noncommutative coordinates .The main motivation for studying QFTs on such spaces derives from string theories which are formulated as open strings attached to D - branes whose positions can be described by noncommuting matrices . In particular we will focus our focus on the so - called Groenewold - Moyal plane which is characterized as the space generated by two non - commuting coordinates satisfying the commutation relations qμ ( x ) , qν ( y ) = iθμνρqρ ( xy ) .We will show that it is easy to define a covariant derivative operator acting on fields living on the GroenewoldMoyal plane . This enables us to introduce a concept of spinor fields on the Groenewold - Moyal plane .Moreover we will explore how one can create gauge invariant movements for these fields . Finally we will research the operation of the discrete symmetries C , P , T and CP T .",
        "rewrite_text": "**Title:** Quantum Fields on the Groenewold-Moyal Plane: C, P, T, and CPT\n\n**Abstract:** This article presents a comprehensive examination of recent advancements in quantum field theory (QFT) within the context of curved spaces characterized by noncommutative coordinates. The impetus for investigating QFTs in such frameworks arises primarily from string theory, particularly in scenarios involving open strings that are anchored to D-branes, whose positions are represented by noncommuting matrices. Our focus will be directed towards the Groenewold-Moyal plane, a specific noncommutative space defined by two coordinates that adhere to the commutation relations \\( q^\\mu(x), q^\\nu(y) = i\\theta^{\\mu\\nu\\rho}q_\\rho(xy) \\). \n\nWe will demonstrate the straightforward definition of a covariant derivative operator that acts on fields residing in the Groenewold-Moyal plane, facilitating the introduction of spinor fields in this noncommutative setting. Furthermore, we will investigate the construction of gauge-invariant dynamics for these fields, ensuring that the physical theories remain consistent under gauge transformations. \n\nIn addition, this study will delve into the implications of discrete symmetries—charge conjugation (C), parity transformation (P), time reversal (T), and their combined operation (CPT)—within the framework of the Groenewold-Moyal plane. By analyzing these symmetries, we aim to uncover deeper insights into the behavior of quantum fields in noncommutative geometries and their potential implications for theoretical physics. This work contributes to the ongoing discourse on the intersection of quantum mechanics and geometry, highlighting the significance of noncommutative spaces in the formulation of modern quantum field theories.",
        "ori-fast-z-score": -0.48507125007266594,
        "water-fast-z-score": 3.3466401061363023,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Single Transverse-Spin Asymmetry in Hadronic Dijet Production .\nAbstract:\nWe present the first measurement of single-transverse-spin asymmetries (SSA) for hadronic dijets produced at midrapidity in p+p collisions at sqrt(sNN) = 5.02 TeV using data collected by the CMS experiment during 2012 corresponding to an integrated luminosity of 2.3 fb-1 . The SSAs are extracted as functions of jet transverse momentum and rapidity, azimuthal angle between jets, and event centrality. We observe no significant dependence on any kinematic variable except that the magnitude of the asymmetry decreases with increasing jet rapidity. Our results are compared to theoretical predictions based on perturbative QCD calculations including higher-order corrections and parton distribution function uncertainties. \nThe measured values agree well within experimental and theoretical uncertainties. This is the most precise measurement of this observable performed so far. \n \n Introduction \n \n Single transverse-spin asymmetries have been observed in several processes involving polarized protons or neutrons  1  , such as inclusive pion production  2  , semi-inclusive deep-inelastic scattering  3  , Drell-Yan lepton pair production  4  , prompt photon production  5  , and direct photons  6  . These measurements provide important information about the spin structure of nucleons  7, 8  .\n \nIn particular, they can be used to test the validity of factorization theorems  9  which relate hard-scattering cross sections to partonic distributions inside the proton  10  . In addition, these observables may also shed light on new physics beyond the Standard Model  11  . \n \n For example, it has recently been suggested  12  that large single-spin asymmetries could arise due to the interference of two amplitudes describing different helicities of quarks emitted from longitudinally polarized gluons in high-energy pp collisions. Such effects would violate parity conservation and thus constitute evidence for new physics  13  . However, there exists only one previous measurement  14  of single-spin asymmeties in hadronic dijet production at high energies. That study was carried out at RHIC  15  where the center-of-mass energy per nucleon-nucleon collision √sNN=200 GeV is much lower",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Single Transverse - Spin Asymmetry in Hadronic Dijet Production . Abstract : We report the first measurement of single - transverse - spinning asymmetries ( SSA ) for hadronic dijets created at midrapidity in p + p collisions at sqrt ( sNN ) = 5 . 02 TeV using data taken by the CMS experiment during 2012 corresponding to an integrated luminosity of 2 . 3 fb - 1 .The SSAs are derived as functions of jet vertical momentum and rapidity , azimuthal angle between planes , and event centrality . We see no major dependence on any kinematic variable except that the magnitude of the asymmetry decreases with rising jet rapidity .Our results are compared to theoretical estimates based on perturbative QCD calculations including higher - order corrections and parton distribution function uncertainties . The measured measures agree well within experimental and theoretical uncertainties .This is the most accurate calculation of this observable performed so far . Introduction Single transverse - spin asymmetries have been observed in multiple processes involving polarized protons or neutrons 1 , such as inclusive pion production 2 , semi - inclusive deep - inelastic emission 3 , Drell - Yan lepton pair production 4 , prompt photon processing 5 , and direct photons 6 .These measurements give important information about the spin composition of nucleons 7 , 8 . In particular , they can be used to test the legitimacy of factorization theorems 9 which compare hard - absorption cross sections to partonic distributions inside the proton 10 .In addition , these observables might additionally shed light on new science beyond the Standard Model 11 . For instance , it has recently been proposed 12 that wide single - spinning asymmetries may arise due to the interference of two amplitudes describing different helicities of quarks emitted from longitudinally polarized gluons in high - energy pp collisions .Such effects would violate parity conservation and therefore constitute evidence for new science 13 . However , there exists only one previous measurement 14 of single - spinning asymmeties in hadronic dijet production at high energies .That experiment was carried out at RHIC 15 where the center - of - mass energy per nucleon - nucleon collision √sNN = 200 GeV is much lower",
        "rewrite_text": "We present the inaugural measurement of single transverse spin asymmetries (SSA) in hadronic dijet production at midrapidity, derived from proton-proton collisions at a center-of-mass energy of √sNN = 5.02 TeV. This analysis utilizes data collected by the CMS experiment in 2012, corresponding to an integrated luminosity of 2.3 fb^-1. The SSAs are evaluated as functions of various kinematic parameters, including the vertical momentum and rapidity of the jets, the azimuthal angle between the jet planes, and the centrality of the events. Our findings indicate that the asymmetry exhibits minimal dependence on the kinematic variables studied, with a notable decrease in magnitude as jet rapidity increases. \n\nWe compare our measurements with theoretical predictions derived from perturbative Quantum Chromodynamics (QCD) calculations, which account for higher-order corrections and uncertainties in parton distribution functions. The results show a strong agreement within the bounds of both experimental and theoretical uncertainties, marking this as the most precise assessment of this observable to date.\n\nThe significance of single transverse spin asymmetries has been established in various processes involving polarized nucleons, such as inclusive pion production, semi-inclusive deep inelastic scattering, Drell-Yan lepton pair production, and prompt photon production. These measurements provide critical insights into the spin structure of nucleons and can be instrumental in testing factorization theorems that relate hard scattering cross sections to partonic distributions within protons. Furthermore, these observables may offer clues to phenomena beyond the Standard Model, particularly in scenarios where wide single-spin asymmetries arise from the interference of quark helicities emitted from longitudinally polarized gluons in high-energy proton-proton collisions. Such effects would challenge parity conservation and could indicate new physics. Notably, this study builds upon a previous measurement of single-spin asymmetries in hadronic dijet production at lower energies, specifically at RHIC, where the center-of-mass energy was significantly lower at √sNN = 200 GeV.",
        "ori-fast-z-score": 0.4240944648399855,
        "water-fast-z-score": 6.316139407998892,
        "rewrite-fast-z-score": -2.1677749238103
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  On generalized entropy measures and pathways .\nAbstract:\nWe present an approach to the analysis of metabolic networks based on information-theoretic concepts, in particular Shannon s entropy measure. We show that this concept can be extended by considering not only single metabolites but also pairs or higher-order tuples of them as elementary units for measuring entropy. This leads us to define so-called pathway entropies which are used to quantify how much uncertainty is associated with different parts of the network. The proposed method allows one to identify those parts of the network where most of the uncertainty resides. In addition we introduce a novel way of visualizing metabolic networks using these new entropy-based quantities. Finally, we demonstrate our approach by applying it to two examples taken from biochemistry literature. Metabolic networks play important roles in many biological processes such as cell growth and development  1  . They consist of chemical reactions transforming various compounds into each other  2  , e.g., glucose molecules are transformed into energy-rich adenosine triphosphate (ATP) molecules via glycolysis  3  .\nThe study of metabolic networks has been attracting increasing interest over recent years  4  -  8  . One reason for this growing interest lies in their potential use as drug targets  9  . Another motivation comes from the fact that they provide valuable insights into cellular metabolism  10  . For example, the identification of key enzymes involved in certain diseases may help to develop drugs against these diseases  11  . Furthermore, metabolic networks have been shown to exhibit scale-free properties  12  similar to those observed in social systems  13  . These findings suggest that there might exist common principles underlying both types of networks  14  .\nIn order to understand the functioning of metabolic networks better, several mathematical models have been developed  15  -  17  . Amongst others, stoichiometric approaches  18  try to describe all possible states of a given metabolic system mathematically. However, due to the high number of degrees of freedom inherent in such models  19  , it becomes difficult to analyze large metabolic networks  20  . Therefore, alternative methods have been suggested  21  -  23  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : On generalized entropy methods and mechanisms . Abstract : We present an view to the analysis of metabolic networks based on knowledge - theoretic concepts , in notably Shannon s entropy test .We see that this concept can be generalized by using not only single metabolites but also pairs or greater - order tuples of them as elementary units for determining entropy . This leads us to define so - called pathway entropies which are applied to quantify how many uncertainty is associated with various parts of the organization .The proposed approach allows one to identify those parts of the network where most of the uncertainty resides . In addition we provide a new manner of visualizing metabolic networks employing these new entropy - based quantities .Finally , we prove our approach by using it to two examples taken from biochemistry literature . Metabolic systems play important roles in different biological pathways such as cell development and growth 1 .They involve of organic reactions transforming various compounds into each other 2 , e . g . , glucose molecules are transformed into energy - rich adenosine triphosphate ( ATP ) molecules via glycolysis 3 . The investigation of metabolic networks has been drawing rising interest over recent months 4 - 8 .One reason for this increasing interest lies in their potential use as drug targets 9 . Another motivation arises from the fact that they give valuable insights into cellular metabolism 10 .For instance , the discovery of key enzymes active in different diseases might help to develop medication against these diseases 11 . Furthermore , metabolic networks have been shown to exhibit scale - free properties 12 similar to those observed in social systems 13 .These studies imply that there might exist common principles governing both types of networks 14 . In order to explain the structures of metabolic networks better , various numerical models have been created 15 - 17 .Amongst others , stoichiometric methods 18 try to explain all possible states of a given metabolic system mathematically . However , owing to the high number of degrees of liberty inherent in such theories 19 , it becomes hard to analyze large metabolic networks 20 .Therefore , alternative techniques have been proposed 21 - 23 .",
        "rewrite_text": "**Title:** On Generalized Entropy Methods and Mechanisms\n\n**Abstract:** In this article, we explore a novel perspective on the analysis of metabolic networks through the lens of knowledge-theoretic principles, particularly focusing on Shannon's entropy. We propose a generalization of this concept by considering not only individual metabolites but also pairs and higher-order tuples as fundamental units for entropy calculation. This innovative approach leads to the definition of \"pathway entropies,\" which serve to quantify the uncertainty associated with various components of metabolic organization. By applying pathway entropies, we can pinpoint the areas within the network that harbor the greatest uncertainty, thereby enhancing our understanding of metabolic dynamics.\n\nAdditionally, we introduce a new visualization technique for metabolic networks that leverages these entropy-based metrics, providing clearer insights into their structure and function. To validate our methodology, we apply it to two case studies drawn from the biochemical literature, demonstrating its effectiveness in real-world scenarios.\n\nMetabolic systems are crucial to numerous biological processes, including cellular development and growth, as they encompass a series of organic reactions that convert various compounds into one another. For example, glucose is metabolized into energy-rich adenosine triphosphate (ATP) through glycolysis. Recent months have seen a surge in interest regarding the investigation of metabolic networks, driven by their potential as drug targets and their ability to offer significant insights into cellular metabolism. Identifying key enzymes involved in various diseases could pave the way for new therapeutic developments.\n\nMoreover, metabolic networks exhibit scale-free properties akin to those found in social networks, suggesting the existence of underlying principles that govern both types of systems. To better understand the architecture of metabolic networks, various numerical models have been developed. Among these, stoichiometric methods aim to mathematically describe all possible states of a metabolic system. However, the complexity and high degrees of freedom inherent in these models pose challenges when analyzing large metabolic networks, prompting the exploration of alternative analytical techniques.",
        "ori-fast-z-score": -1.3269776053940743,
        "water-fast-z-score": 8.409632877462002,
        "rewrite-fast-z-score": -0.1543033499620919
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Noise-induced phase transitions: Effects of the noises  statistics and spectrum .\nAbstract:\nWe study noise-induced phase transitions in systems with multiplicative white Gaussian, Poissonian or Lévy noise. We show that for all these three types of noise there is an optimal noise intensity at which the system undergoes a transition to a state where it performs better than without noise. The optimal noise intensities are different depending on whether we consider additive or multiplicative noise. For multiplicative noise they depend also on the shape of the power spectral density (PSD) of the noise. In particular, if the PSD has a peak around zero frequency then the optimal noise intensity decreases as the width of this peak increases. If the PSD does not have such a peak but decays monotonically towards zero frequencies then the optimal noise intensity increases as the decay rate becomes slower. Finally, if the PSD decays faster than exponentially then the optimal noise intensity remains constant irrespective of how fast the PSD decays.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Noise - caused phase transitions : Effects of the noises statistics and spectrum . Abstract : We research sound - caused phase transitions in networks with multiplicative white Gaussian , Poissonian or Lévy noise .We see that for all these three sorts of noise there is an appropriate sound amplitude at which the process undergoes a change to a state where it performs better than without noise . The appropriate sound intensities are different vary on whether we choose additive or multiplicative sound .For multiplicative sound they rely also on the shape of the power spectral coefficient ( PSD ) of the signal . In particular , if the PSD has a peak around zero frequency then the ideal noise amplitude decreases as the length of this peak rises .If the PSD does not have such a peak but decays monotonically towards zero frequencies then the ideal noise amplitude rises as the decay rate gets smaller . Finally , if the PSD decays faster than exponentially then the ideal noise amplitude remains constant irrespective of how slow the PSD decays .",
        "rewrite_text": "Title: Noise-Induced Phase Transitions: The Influence of Noise Statistics and Spectrum\n\nAbstract: This study investigates the phenomenon of sound-induced phase transitions in networks subjected to various types of noise, specifically multiplicative white Gaussian, Poissonian, and Lévy noise. Our findings reveal that for each of these noise types, there exists a specific sound amplitude at which the system transitions to a state that exhibits improved performance compared to its behavior in the absence of noise. Notably, the optimal sound intensities differ depending on whether the sound is additive or multiplicative. In the case of multiplicative sound, the ideal noise amplitude is also influenced by the characteristics of the power spectral density (PSD) of the signal. \n\nWe observe that when the PSD features a peak near zero frequency, the optimal noise amplitude decreases as the width of this peak increases. Conversely, if the PSD lacks such a peak and instead exhibits a monotonically decreasing trend towards lower frequencies, the optimal noise amplitude increases as the decay rate of the PSD diminishes. Furthermore, in scenarios where the PSD decays faster than exponentially, the optimal noise amplitude remains constant, regardless of the rate at which the PSD approaches zero. \n\nThese results highlight the intricate relationship between noise characteristics and system performance, suggesting that the statistical properties and spectral features of noise play a crucial role in determining the conditions under which sound-induced phase transitions occur. This research contributes to a deeper understanding of how noise can be harnessed to enhance system dynamics and performance in various applications, paving the way for future studies in the field of noise-driven phenomena.",
        "ori-fast-z-score": -1.709408646894569,
        "water-fast-z-score": 5.584068246522259,
        "rewrite-fast-z-score": 2.806585661782494
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Growth window and possible mechanism of millimeter-thick single-walled carbon nanotube forests .\nAbstract:\nWe report the growth of millimeter-thick vertically aligned single-walled carbon-nanotube forests on SiO 2 /Si substrates by chemical vapor deposition using Fe nanoparticles as catalysts at temperatures between 700 °C and 850 °C in Ar/H 2 . The diameter distribution is centered around 1 nm, with an average tube length of about 10 μm. We find that the density of the grown forests increases linearly with time up to a maximum value of 3 × 10 11 cm -2 , which corresponds to a volume filling factor of 0.7 for the tubes. This high density can be explained by considering the balance between the nucleation rate and the growth rate of individual tubes. In addition, we show that the vertical alignment of the tubes is maintained over large areas (1 cm   2 ) without any noticeable defects or cracks. These results suggest that these thick forests are promising candidates for applications such as field emitters and transparent electrodes. \n \n Carbon nanotubes have attracted considerable attention because they exhibit unique physical properties  1 . However, it has been difficult to grow high-quality carbon nanotubes due to their extremely small diameters  2 . Recently, several groups reported the growth of vertically aligned carbon nanotube arrays  3 – 6 . Although these studies demonstrated the potential use of carbon nanotubes in various fields  7, 8 , most of them were limited to thin films less than 100 nm thick  9 . To fully exploit the advantages offered by carbon nanotubes, it is necessary to develop techniques capable of producing thicker films  10 .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Growth window and possible mechanism of millimeter - thick single - walled carbon nanotube forests . Abstract : We report the development of millimeter - thick vertically aligned single - walled carbon - nanotube forests on SiO 2 / Si substrates by molecular vapor precipitation use Fe nanoparticles as catalysts at conditions between 700 °C and 850 °C in Ar / H 2 .The diameter distribution is centered around 1 nm , with an estimated tube width of about 10 μm . We see that the density of the grown forests changes linearly with time up to a maximum value of 3 × 10 11 centimetres - 2 , which equals to a volume filling factor of 0 . 7 for the tubes .This high density can be described by examining the balance between the nucleation frequency and the development frequency of different tubes . In addition , we find that the vertical layout of the tubes is maintained over large areas ( 1 cm 2 ) without any visible flaws or flaws .These data suggest that these thick forests are promising candidates for applications such as field emitters and reflective electrodes . Carbon nanotubes have garnered considerable scrutiny because they demonstrate unique physical properties 1 .However , it has been difficult to develop large - grade carbon nanotubes due to their extremely tiny diameters 2 . Recently , various groups reported the development of vertically aligned carbon nanotube arrays 3 – 6 .Although these research demonstrated the possibilities application of carbon nanotubes in different fields 7 , 8 , most of them were restricted to thin films less than 100 nm thick 9 . To fully utilize the advantages offered by carbon nanotubes , it is required to develop techniques capable of creating thicker movies 10 .",
        "rewrite_text": "**Title:** Growth Window and Potential Mechanism of Millimeter-Thick Single-Walled Carbon Nanotube Forests\n\n**Abstract:** In this study, we present the successful fabrication of millimeter-thick vertically aligned single-walled carbon nanotube (SWCNT) forests on SiO2/Si substrates through molecular vapor precipitation. Utilizing iron nanoparticles as catalysts, we conducted the growth process under controlled conditions ranging from 700 °C to 850 °C in an argon/hydrogen atmosphere. The resulting nanotubes exhibit a diameter distribution centered around 1 nm, with an estimated tube width of approximately 10 μm. Our findings reveal a linear relationship between the density of the grown forests and time, reaching a maximum density of 3 × 10^11 cm^-2, which corresponds to a volume filling factor of 0.7 for the tubes. This high density can be attributed to the balance between the nucleation frequency and the growth rate of individual tubes. Notably, the vertical alignment of the nanotubes is preserved over extensive areas (up to 1 cm²) without any observable defects, indicating the robustness of the growth process. These characteristics suggest that the developed thick SWCNT forests hold significant promise for various applications, including field emission devices and reflective electrodes. The unique physical properties of carbon nanotubes have attracted considerable attention; however, the challenge of producing large-scale carbon nanotubes has persisted due to their extremely small diameters. Recent advancements have led to the creation of vertically aligned carbon nanotube arrays, yet most studies have been limited to thin films of less than 100 nm in thickness. To fully harness the advantages of carbon nanotubes, it is essential to develop methods that enable the production of thicker films. This research contributes to the understanding of the growth mechanisms and potential applications of thick SWCNT forests, paving the way for future innovations in nanotechnology.",
        "ori-fast-z-score": -2.0732842213952645,
        "water-fast-z-score": 6.282839650514254,
        "rewrite-fast-z-score": 0.3380617018914066
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Cargese Lectures on Brane Induced Gravity .\nAbstract:\nThe lectures were given by David Gross at the Cargese Summer Institute in Corsica, France during August 2005.  The lecture notes are available online as PDF files and can be downloaded for free.   These lecture notes cover topics such as:  - Introduction to string theory - Gauge fields and gauge symmetries - Supersymmetry - Supergravity - String field theories - D-branes - Open strings - Closed strings - Tachyons - Bosonic open strings - Fermionic open strings - Vacuum expectation values - Energy-momentum tensors - Interactions between branes - General relativity - Gravity induced on a brane - Black holes - Cosmology - Time evolution - Conformal invariance - Renormalization group flow - Dualities - M-theory - Other approaches - Comments on future directions - References - Index   This is an excellent resource for learning about many aspects of modern theoretical physics including superstrings, supergravities, black holes, cosmology, time evolution, conformal invariance, renormalization group flows, dualities, M-theory, other approaches etc...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Cargese Lectures on Brane Induced Gravity . Abstract : The seminars were given by David Gross at the Cargese Summer Institute in Corsica , France during August 2005 .The lecture notes are available web as PDF files and can be downloaded for free . These lecture notes cover subjects such as : - Introduction to string theory - Gauge fields and gauge symmetries - Supersymmetry - Supergravity - String field theories - D - branes - Open strings - Closed strings - Tachyons - Bosonic open strings - Fermionic open strings - Vacuum expectation values - Energy - momentum tensors - Interactions between branes - General relativity - Gravity induced on a brane - Black holes - Cosmology - Time evolution - Conformal invariance - Renormalization group flow - Dualities - M - theory - Other approaches - Comments on future directions - References - Index This is an excellent resource for learning about various parts of modern conceptual science covering superstrings , supergravities , white holes , cosmology , time evolution , conformal invariance , renormalization group flows , dualities , M - theory , other methods etc . . .",
        "rewrite_text": "**Title:** Cargese Lectures on Brane Induced Gravity\n\n**Abstract:** This article presents a comprehensive overview of the lecture series delivered by David Gross at the Cargese Summer Institute in Corsica, France, in August 2005. The lecture notes, which are freely accessible online in PDF format, delve into a wide array of topics central to modern theoretical physics, particularly in the realm of string theory and its implications for gravity and cosmology. \n\nThe lectures begin with an introduction to string theory, laying the groundwork for understanding gauge fields and gauge symmetries, which are pivotal in the formulation of particle physics. The discussions extend to supersymmetry and supergravity, exploring their roles in unifying fundamental forces. A significant focus is placed on D-branes and their interactions, as well as the dynamics of open and closed strings, including the intriguing phenomena of tachyons and the distinctions between bosonic and fermionic open strings.\n\nFurther, the notes address critical concepts such as vacuum expectation values, energy-momentum tensors, and the gravitational effects induced on branes. The lectures also tackle complex topics like black holes, cosmology, and the evolution of the universe over time, emphasizing the importance of conformal invariance and renormalization group flow in theoretical frameworks. \n\nAdditionally, the series discusses dualities and M-theory, providing insights into alternative approaches within the field. The concluding sections offer reflections on future directions for research and include a comprehensive list of references and an index for ease of navigation. This collection serves as an invaluable resource for those seeking to deepen their understanding of contemporary theoretical physics, encompassing a diverse range of subjects from superstrings to cosmological models and beyond.",
        "ori-fast-z-score": 2.0465595024580763,
        "water-fast-z-score": 4.063777271736939,
        "rewrite-fast-z-score": -1.7960530202677492
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The power of quantum systems on a line .\nAbstract:\nWe study the dynamics of open quantum systems in one dimension, focusing on their ability to generate entanglement between distant sites and how this is affected by decoherence.  We consider two different models for the system-environment interaction: (i) an environment that couples locally with each site; and (ii) an environment that couples globally with all sites simultaneously. In both cases we find that there are regimes where the system can be driven into highly entangled states even when it starts out unentangled or only weakly entangled. This occurs because the environment acts as a source of noise which drives the system towards its ground state. The effect of local coupling is more pronounced than global coupling since the former allows for faster relaxation timescales. Finally, we show that these results hold true also if the initial state has some degree of spatial correlations. Quantum information processing requires the manipulation of quantum states over large distances. However, due to inevitable interactions with the surrounding environment, such operations cannot be performed perfectly. Here we investigate whether certain types of environments may actually enhance the performance of quantum devices.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The power of quantum systems on a line . Abstract : We research the dynamics of open quantum systems in one dimension , concentrating on their power to produce entanglement between distant areas and how this is affected by decoherence .We consider two different models for the system - landscape interaction : ( i ) an environment that pairs locally with each site ; and ( ii ) an environment that pairs internationally with all locations simultaneously . In both cases we find that there are regimes where the system can be pushed into extremely entangled states especially when it comes out unentangled or only strongly entangled .This occurs because the surroundings serves as a source of noise which moves the system towards its ground state . The impact of local coupling is more pronounced than worldwide coupling since the former provides for quicker relaxation timescales .Finally , we prove that these results hold true also if the first state has some degree of spatial correlations . Quantum knowledge processing requires the processing of quantum states over large distances .However , owing to inevitable relationships with the nearby landscape , such operations cannot be performed properly . Here we investigate whether particular kinds of contexts may really increase the performance of quantum devices .",
        "rewrite_text": "Title: The Power of Quantum Systems on a Line\n\nAbstract: This study explores the dynamics of open quantum systems constrained to one-dimensional configurations, with a particular focus on their capacity to generate entanglement across distant regions and the influence of decoherence on this process. We investigate two distinct models of system-environment interaction: (i) a local environment that interacts with each site individually, and (ii) a global environment that interacts with all sites simultaneously. Our findings reveal that there exist specific regimes in which the system can achieve highly entangled states, especially when it begins in a state that is either unentangled or only weakly entangled. This phenomenon is attributed to the environment acting as a source of noise, which drives the system towards its ground state. Notably, we observe that local coupling has a more significant impact than global coupling, as it facilitates faster relaxation timescales. Furthermore, we demonstrate that these results remain valid even when the initial state exhibits some degree of spatial correlations. The ability to process quantum information over large distances is essential for quantum information technologies; however, interactions with the surrounding environment often hinder the effectiveness of such operations. In this context, we examine whether specific types of environmental interactions can enhance the performance of quantum devices, thereby contributing to the advancement of quantum information processing. Our research provides insights into the intricate interplay between system dynamics and environmental influences, highlighting the potential for optimizing quantum systems in practical applications.",
        "ori-fast-z-score": -0.8783100656536799,
        "water-fast-z-score": 6.477134476605945,
        "rewrite-fast-z-score": 0.45454545454545453
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Positronium Groundstate in Relativistic Schroedinger Theory .\nAbstract:\nWe present the results for positronium ground state energy and wave function obtained by solving relativistic Schrödinger equation with Coulomb potential using variational method. The calculations are performed within two different approximations, namely nonrelativistic limit (NR) and first order perturbation theory (PT1). In NR approximation we use Hylleraas type trial wave functions which include spin dependent terms up to second power of inter particle distance. We also calculate the expectation value of kinetic energy operator using this wave function. In PT1 approximation we use Hyllraas type trial wave functions including spin dependent terms up to third power of inter particle distance alongwith one electron Dirac orbitals as basis set. Our calculated values of binding energies agree well with those reported earlier. \n\n\nThe author is grateful to Prof. S. K. Gupta, Director General, BARC, Mumbai for his encouragement during the course of work on this project.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Positronium Groundstate in Relativistic Schroedinger Theory . Abstract : We present the results for positronium ground state energy and wave function achieved by treating relativistic Schrödinger equation with Coulomb potential using variational technique .The calculations are performed within two different approximations , namely nonrelativistic limit ( NR ) and first order perturbation theory ( PT1 ) . In NR algorithm we using Hylleraas type trial wave functions which contain spin dependent terms up to second power of inter quantum distance .We also obtain the expectation value of kinetic energy operator using this wave function . In PT1 approximation we using Hyllraas type trial wave functions featuring spin dependent terms up to third power of inter quantum time alongwith one particle Dirac orbitals as basis set .Our measured calculations of bound energies agree well with those published earlier . The author is thankful to Prof . S . K . Gupta , Director General , BARC , Mumbai for his encouragement during the course of research on this project .",
        "rewrite_text": "In this article, we explore the ground state energy and wave function of positronium by applying the relativistic Schrödinger equation in the context of a Coulomb potential, utilizing a variational approach. Our study employs two distinct approximations: the nonrelativistic limit (NR) and first-order perturbation theory (PT1). In the NR framework, we utilize Hylleraas-type trial wave functions that incorporate spin-dependent terms up to the second power of the inter-quantum distance. This method allows us to calculate the expectation value of the kinetic energy operator effectively. For the PT1 approximation, we extend our approach by employing Hylleraas-type trial wave functions that include spin-dependent terms up to the third power of the inter-quantum distance, supplemented by one-particle Dirac orbitals as the basis set. Our computed bound state energies show strong agreement with previously published results, validating the effectiveness of our methodologies. The author expresses gratitude to Prof. S. K. Gupta, Director General of BARC, Mumbai, for his invaluable support and encouragement throughout the research process. This work contributes to a deeper understanding of positronium within the framework of relativistic quantum mechanics and highlights the potential of variational techniques in tackling complex quantum systems.",
        "ori-fast-z-score": -1.193117518002609,
        "water-fast-z-score": 2.711630722733202,
        "rewrite-fast-z-score": -1.3480372031495529
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Coriolis force corrections to g-mode spectrum in 1D MHD model .\nAbstract:\nWe present the results of our study on coriolis effects on the global oscillation modes (g-modes) in magnetized white dwarfs using one-dimensional, fully compressible numerical simulations with realistic equation-of-state and opacities. We find that the effect is significant for low-order g-modes but negligible for high-order ones. The frequency shifts are found to be proportional to the mode inertia which can be estimated by fitting the eigenfunctions obtained numerically into those calculated analytically under the thin-shell approximation. For typical parameters relevant to ZZ Ceti stars, we estimate the frequency shift due to coriolis forces to be about 0.1 % at most. This result suggests that it may not be necessary to take account of such effects when interpreting observed frequencies of pulsating white dwarf stars. However, this conclusion should be checked against more sophisticated models including rotation as well as magnetic fields.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Coriolis force corrections to g - mode spectrum in 1D MHD model . Abstract : We present the conclusion of our research on coriolis impact on the global oscillation modes ( g - modes ) in magnetized white dwarfs using one - dimensional , fully compressible statistical simulations with realistic equation - of - state and opacities .We see that the effect is substantial for low - order g - modes but negligible for high - order ones . The frequency swings are found to be proportional to the mode inertia which can be estimated by fitting the eigenfunctions collected numerically into those estimated analytically under the narrow - shell approximation .For characteristic parameters applicable to ZZ Ceti galaxies , we estimate the frequency shift due to coriolis forces to be about 0 . 1 % at most . This result suggests that it would not be appropriate to take account of such influences when interpreting observed frequencies of pulsating white dwarf stars .However , this conclusion should be examined against more sophisticated models using rotation as well as magnetic fields .",
        "rewrite_text": "We present the findings of our study on the influence of the Coriolis force on the global oscillation modes, specifically the g-modes, in magnetized white dwarfs. Our research employs a one-dimensional, fully compressible statistical simulation framework that incorporates a realistic equation of state and opacity profiles. Our results indicate that the Coriolis effect is significant for low-order g-modes, while its impact diminishes for higher-order modes. We observe that the frequency variations induced by the Coriolis force are directly proportional to the mode inertia. This inertia can be estimated by fitting the numerically obtained eigenfunctions to those derived analytically under the narrow-shell approximation. For parameters relevant to ZZ Ceti stars, we estimate that the frequency shift caused by Coriolis forces is approximately 0.1% at most. This finding implies that the influence of the Coriolis force may be negligible when interpreting the observed frequencies of pulsating white dwarf stars. However, we recommend that this conclusion be further validated through more advanced models that take into account both rotation and magnetic fields, as these factors could potentially alter the dynamics of g-mode oscillations in these stellar environments.",
        "ori-fast-z-score": -1.6876318513890358,
        "water-fast-z-score": 3.0,
        "rewrite-fast-z-score": 0.10721125348377948
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Relativistic Effects in Extreme Mass Ratio Gravitational Wave Bursts .\nAbstract:\nWe study the effects of general relativity on gravitational wave bursts produced by extreme mass ratio inspirals (EMRIs). We show that, for EMRI systems with total masses M = 10^6M_solar and compact object masses m = 1M_neutron star, the orbital period is less than one second at distances greater than 100 AU. This implies that these sources are likely to be detected as continuous waves rather than short-duration bursts. The detection rate of such events depends strongly upon their luminosities; we find that they may occur up to several times per year within our galaxy. These results suggest that EMRIs could provide an important source of information about supermassive black holes. \n \n Keywords: Black hole, Compact binary system, General relativity, Gravitational wave, Inspiralling neutron star, Relativity theory \n \n \n \n INTRODUCTION \n \n In recent years there has been considerable interest in studying the properties of gravitational radiation emitted during the final stages of stellar evolution when a massive star collapses into a black hole or neutron star  1  . Such processes can produce extremely energetic signals which will be detectable out to cosmological distances using future space-based detectors  2  , including LISA  3  . However, it remains unclear how many of these events should actually be observed  4  . \n \n One possible class of objects which might emit strong gravitational waves are known as  extreme-mass-ratio inspirals  (EMRIs)  5  . Here, a small compact object spirals into a much more massive black hole or neutron star over millions of orbits before being destroyed  6  . For example, if a solar mass star were to spiral into a ten million solar mass black hole then its orbit would shrink down to just a few kilometres before merging  7, 8  . If this process occurs close enough to the event horizon then the resulting signal will have very high frequencies  9  . As a result, EMRIs represent some of the most promising candidates for detecting gravitational waves  10  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Relativistic Effects in Extreme Mass Ratio Gravitational Wave Bursts . Abstract : We research the effects of general relativity on gravity wave bursts created by intense mass ratio inspirals ( EMRIs ) .We see that , for EMRI systems with total masses M = 10 ^ 6M _ solar and compact object masses m = 1M _ neutron star , the orbital period is fewer than one second at distances greater than 100 AU . This implies that these sources are likely to be identified as continuous waves rather than longer - duration bursts .The detection rate of such events depends strongly upon their luminosities ; we find that they may happen up to several twice per year within our universe . These data suggest that EMRIs might give an important source of information about supermassive black holes .Keywords : Black hole , Compact binary system , General relativity , Gravitational wave , Inspiralling neutron galaxy , Relativity physics INTRODUCTION In recent years there has been substantial interest in examining the properties of gravitational energy emitted during the last phases of stellar evolution when a huge star collapses into a black hole or neutron star 1 . Such mechanisms can generate incredibly energetic signals which will be detectable out to cosmological distances using upcoming space - based detectors 2 , notably LISA 3 .However , it remains unsure how many of these phenomena should really be recorded 4 . One likely class of bodies which would emit strong gravitational waves are known as extreme - mass - ratio inspirals ( EMRIs ) 5 .Here , a small compact body spirals into a far more massive brown hole or neutron galaxy over millions of orbits before being destroyed 6 . For instance , if a solar mass star were to spiral into a ten million solar mass black hole then its orbit may widen down to just a few kilometres before merging 7 , 8 .If this process occurs nearer sufficient to the event horizon then the resulting signal will have very high frequencies 9 . As a result , EMRIs represent some of the most attractive candidates for detecting gravitational waves 10 .",
        "rewrite_text": "**Title:** Relativistic Effects in Extreme Mass Ratio Gravitational Wave Bursts\n\n**Abstract:** This study investigates the implications of general relativity on gravitational wave bursts generated by extreme mass ratio inspirals (EMRIs). We focus on systems where the total mass is approximately \\( M = 10^6 M_{\\text{solar}} \\) and the compact object mass is \\( m = 1 M_{\\text{neutron star}} \\). Our findings indicate that for such EMRI systems, the orbital period can be less than one second when the distance exceeds 100 astronomical units (AU). This characteristic suggests that these gravitational wave sources are more likely to be detected as continuous waves rather than as prolonged bursts. The detection frequency of these events is significantly influenced by their luminosity, and our analysis suggests that they could occur up to several times per year within our universe. These insights imply that EMRIs could serve as a crucial source of information regarding the nature of supermassive black holes.\n\nIn recent years, there has been a growing interest in understanding the gravitational energy emitted during the final stages of stellar evolution, particularly when massive stars collapse into black holes or neutron stars. Such events can produce extraordinarily energetic signals that are expected to be detectable at cosmological distances with future space-based observatories, notably the Laser Interferometer Space Antenna (LISA). However, uncertainties remain regarding the actual number of these phenomena that can be observed. One promising category of sources that are anticipated to emit strong gravitational waves is the extreme mass ratio inspirals (EMRIs), where a small compact object spirals into a significantly more massive black hole or neutron star over millions of orbits before ultimately merging. For example, if a solar mass star were to spiral into a black hole with a mass of ten million solar masses, its orbit could shrink to just a few kilometers before the merger occurs. If this process takes place close enough to the event horizon, the resulting gravitational wave signal would exhibit very high frequencies. Consequently, EMRIs are considered some of the most promising candidates for the detection of gravitational waves, offering valuable insights into the dynamics of black hole mergers and the fundamental aspects of general relativity.",
        "ori-fast-z-score": -1.8740851426632728,
        "water-fast-z-score": 5.588283785085596,
        "rewrite-fast-z-score": 1.3416407864998738
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The blue plume population in dwarf spheroidal galaxies: genuine blue stragglers or young stellar population? .\nAbstract:\nWe present the results of our photometric and spectroscopic study of the blue plume (BP) stars in four nearby dwarf spheroidal galaxies, Draco, Ursa Minor, Sculptor, and Carina. The BP is defined as an excess over the main sequence turn-off at B − V = 0.4 mag for metal-poor populations with  Fe/H  < −1 dex. We find that the BPs are composed of two distinct components; one component follows the same color-magnitude relation as Galactic globular clusters while another component shows bluer colors than expected by theoretical models. Our analysis suggests that these two components have different origins. One possibility is that they represent a mixture of old and intermediate-age populations. Another possibility is that the bluer component represents a younger generation of stars formed during recent star formation events. In this case, we expect to see some evidence of ongoing star formation activity such as Hα emission lines and/or UV upturn features. However, no clear signatures of ongoing star formation were found among the BP stars. \n \n Instead, we propose that the bluer component may be explained by binary evolution effects on the horizontal branch. If so, then the BP can be used as a tracer of binary fraction in low-metallicity environments.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The blue plume population in dwarf spheroidal galaxies : genuine blue stragglers or young stellar community ? .Abstract : We present the results of our photometric and spectroscopic study of the blue plume ( BP ) stars in four nearby dwarf spheroidal galaxies , Draco , Ursa Minor , Sculptor , and Carina . The BP is characterized as an amount over the main sequence turn - off at B − V = 0 . 4 mag for metal - weak populations with Fe / H < −1 dex .We see that the BPs are composed of two different components ; one part follows the same color - magnitude correspondence as Galactic globular nuclei while another component displays bluer shades than expected by theoretical methods . Our study implies that these two parts have different origins .One possibility is that they represent a mixture of old and intermediate - age communities . Another possibility is that the bluer component indicates a smaller wave of stars formed during recent star formation changes .In this instance , we expect to see some evidence of ongoing star formation activity such as Hα emission lines and / or UV upturn events . However , no clear signatures of ongoing star formation were found among the BP stars .Instead , we propose that the bluer component may be described by binary evolve influences on the horizontal branch . If so , then the BP can be used as a tracer of binary fraction in low - metallicity habitats .",
        "rewrite_text": "We present our findings from a comprehensive photometric and spectroscopic investigation of the blue plume (BP) stars located in four nearby dwarf spheroidal galaxies: Draco, Ursa Minor, Sculptor, and Carina. The blue plume is identified as a distinct feature situated above the main sequence turn-off point at a B − V color index of 0.4 mag, particularly in metal-poor populations with metallicity levels of Fe/H < −1 dex. Our analysis reveals that the BP stars can be categorized into two distinct components. The first component aligns with the color-magnitude relationship observed in Galactic globular clusters, while the second component exhibits bluer colors than what theoretical models would predict. This observation suggests that the two components may have different origins. One hypothesis posits that they represent a blend of older and intermediate-age stellar populations. Alternatively, the bluer component might signify a recent episode of star formation, potentially linked to a minor starburst event. In such a scenario, we would anticipate detecting signs of ongoing star formation, such as Hα emission lines or ultraviolet upturns. However, our study did not uncover any definitive evidence of active star formation among the BP stars. Instead, we propose that the bluer component may be influenced by the effects of binary evolution on the horizontal branch. If this hypothesis holds true, the blue plume could serve as a valuable indicator of the binary star fraction in environments characterized by low metallicity. This research enhances our understanding of stellar populations in dwarf spheroidal galaxies and provides insights into the processes that govern star formation and evolution in these intriguing cosmic structures.",
        "ori-fast-z-score": 0.6897304947150052,
        "water-fast-z-score": 5.687367919007337,
        "rewrite-fast-z-score": 0.6260990336999411
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Probabilistic Anonymity and Admissible Schedulers .\nAbstract:\nWe consider the problem of scheduling jobs on parallel machines with probabilistic anonymity constraints, where each job has to be scheduled in such a way that its execution time is hidden among other jobs. We show how this problem can be solved by using admissible schedulers for the underlying deterministic problem. In particular we present two algorithms based on list-scheduling and randomization respectively. The first algorithm achieves a competitive ratio of 2 while the second one obtains a better performance guarantee of 1 + . Both results are tight up to constant factors. Finally, we provide some experimental evidence showing that our randomized algorithm performs well also in practice. Probabilistic anonymity arises naturally when considering privacy issues related to data mining or computational social choice problems. For example, suppose that there exists a set of n users who have to submit their preferences over m alternatives (e.g., candidates) before a certain deadline T. Each user i submits her preference profile Pi = {(ai1 , bi1 ), . . . , (aim , bim )}, where aij denotes the utility she derives from alternative j and bij indicates whether she likes it or not. Then, if all profiles are submitted simultaneously at time t, they should be processed so as to hide which user prefers what alternative. This means that no information about individual preferences should be revealed until after the deadline T.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Probabilistic Anonymity and Admissible Schedulers . Abstract : We consider the issue of scheduling jobs on parallel machines with probabilistic anonymity limitations , where each work has to be scheduled in such a way that its execution time is hidden among other tasks .We see how this question can be answered by using admissible schedulers for the underlying deterministic problem . In particular we present two strategies using on list - planning and randomization respectively .The first algorithm achieves a competitive ratio of 2 while the second one obtains a better performance assurance of 1 + . Both results are tight up to constant factors .Finally , we provide some experimental evidence showing that our randomized algorithm performs good also in practice . Probabilistic anonymity arises readily when examining privacy issues related to data extraction or computational social selection difficulties .For instance , suppose that there exists a group of n users who have to submit their preferences over m proposals ( e . g . , candidates ) before a certain deadline T . Each customer i submits her preference profile Pi = { ( ai1 , bi1 ) , . .. , ( aim , bim ) } , where aij represents the utility she derives from alternative j and bij represents whether she enjoys it or not .Then , if all profiles are presented simultaneously at period t , they should be processed so as to hide which user prefers what alternative . This implies that no information about individual preferences should be revealed until after the date T .",
        "rewrite_text": "In this article titled \"Probabilistic Anonymity and Admissible Schedulers,\" we explore the challenge of scheduling jobs on parallel machines while adhering to probabilistic anonymity constraints. The primary objective is to ensure that the execution time of each job remains obscured among other tasks, thereby protecting individual job identities. We demonstrate that this problem can be effectively addressed through the use of admissible schedulers tailored for the corresponding deterministic scheduling scenario. Specifically, we introduce two distinct strategies: one based on list-planning and the other utilizing randomization techniques. The first algorithm achieves a competitive ratio of 2, while the randomized approach offers a superior performance guarantee of 1 + ε, with both results being tight up to constant factors.\n\nFurthermore, we present experimental data that indicates the practical efficacy of our randomized algorithm, showcasing its robust performance in real-world applications. The concept of probabilistic anonymity is particularly relevant in contexts involving privacy concerns related to data extraction and challenges in computational social choice. For example, consider a situation where a group of n users must submit their preferences regarding m proposals (such as candidates) by a specified deadline T. Each user i provides a preference profile Pi = {(ai1, bi1), ..., (aim, bim)}, where aij denotes the utility derived from alternative j, and bij indicates the user's enjoyment of that alternative. To maintain anonymity, all preference profiles must be processed simultaneously at time t, ensuring that no information regarding individual preferences is disclosed until after the deadline T. This research contributes to the understanding of how scheduling can be optimized while preserving user privacy in various applications.",
        "ori-fast-z-score": 1.4501047335684953,
        "water-fast-z-score": 7.246315678266502,
        "rewrite-fast-z-score": -0.26013299085723596
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Selbstduale Vertexoperatorsuperalgebren und das Babymonster (Self-dual Vertex Operator Super Algebras and the Baby Monster) .\nAbstract:\nIn this article, we study self-dual vertex operator super algebras with central charge c = 24k for k ∈ N>0. We show that these are precisely those which have an automorphism group isomorphic to the baby monster simple sporadic finite group. In particular, we prove that there is only one such algebra up to isomorphism if k=1 or 2, but infinitely many non-isomorphic ones in general. The main tool used here is the modular representation theory of the baby monster group. This work was done as part of my PhD thesis at University College London supervised by Professors David Ben-Zvi and Jonathan Wise. I would like to thank them both very much for their help and support during my time working on it. Introduction Let V be a vector space over C equipped with a non-degenerate bilinear form < , > satisfying < xv, w >=< v, wx> for all x, y, z ∈ V . Then V is called a symplectic vector space. If dimV = 2n then V has a basis consisting of n pairs of vectors e_i + f_i and e_i - f_i where 1 <= i <= n and < e_i, e_j >= 0 =< f_i, f_j > while < e_i, f_j >= δ_{ij}. For more information see  FH91  .\nVertex operator superalgebras were introduced independently by Borcherds  B89  and Kac  K90  . They can be thought of as supersymmetric analogues of vertex operator algebras. A vertex operator superalgebra consists of a Z/2Z-graded vector space V = V0 ⊕ V1 together with a vacuum vector |0>∈V0, a conformal element ω ∈ End(V), a parity change operator Π : V → V interchanging V0 and V1, and a set of fields Y (x, z) (called vertex operators) indexed by elements x ∈ V and complex numbers z ∈ C satisfying certain axioms. These axioms include the Jacobi identity, associativity relations, commutator formulae, and various other conditions",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Selbstduale Vertexoperatorsuperalgebren und das Babymonster ( Self - dual Vertex Operator Super Algebras and the Baby Monster ) . Abstract : In this article , we study self - dual vertex operator super algebras with central charge c = 24k for k ∈ N > 0 .We see that these are exactly those which have an automorphism class isomorphic to the baby monster simple sporadic finite class . In particular , we prove that there is only one such algebra up to isomorphism if k = 1 or 2 , but infinitely many non - isomorphic ones in general .The main technique used here is the modular representation theory of the baby monster group . This research was done as part of my PhD thesis at University College London supervised by Professors David Ben - Zvi and Jonathan Wise .I would like to thank them both very lot for their help and support during my years working on it . Introduction Let V be a vector space over C endowed with a non - degenerate bilinear form < , > satisfying < xv , f > = < v , wx > for all x , y , z ∈ V .Then V is called a symplectic vector space . If dimV = 2n then V has a basis consisting of n pairs of vectors e _ i + f _ i and e _ i - f _ i where 1 < = i < = n and < e _ i , e _ j > = 0 = < f _ i , f _ j > while < e _ i , f _ j > = δ _ { ij } .For more information see FH91 . Vertex operator superalgebras were introduced independently by Borcherds B89 and Kac K90 .They can be thought of as supersymmetric analogues of vertex operator algebras . A vertex operator superalgebra consists of a Z / 2Z - graded vector space V = V0 ⊕ V1 combined with a vacuum vector | 0 > ∈V0 , a conformal element ω ∈ End ( V ) , a parity shift operator Π : V → V interchanging V0 and V1 , and a group of fields Y ( x , z ) ( named vertex operators ) indexed by elements x ∈ V and complex integers h ∈ C satisfying particular axioms .These axioms include the Jacobi identity , associativity relations , commutator formulae , and many other conditions",
        "rewrite_text": "**Title:** Self-Dual Vertex Operator Super Algebras and the Baby Monster\n\n**Abstract:** This article explores the properties of self-dual vertex operator super algebras characterized by a central charge \\( c = 24k \\) for \\( k \\in \\mathbb{N}^+ \\). We establish that these algebras correspond precisely to those possessing an automorphism class that is isomorphic to the Baby Monster, a simple sporadic finite group. Our findings reveal that for \\( k = 1 \\) or \\( k = 2 \\), there exists a unique algebra up to isomorphism. However, for larger values of \\( k \\), we demonstrate the existence of infinitely many non-isomorphic algebras. The primary methodology employed in this research is rooted in the modular representation theory associated with the Baby Monster group. This work forms a significant part of my PhD dissertation at University College London, under the guidance of Professors David Ben-Zvi and Jonathan Wise, to whom I express my profound gratitude for their invaluable support throughout my research journey.\n\nIn the context of this study, we define a symplectic vector space \\( V \\) over the complex numbers \\( \\mathbb{C} \\), equipped with a non-degenerate bilinear form \\( \\langle , \\rangle \\) that satisfies the condition \\( \\langle xv, f \\rangle = \\langle v, wx \\rangle \\) for all \\( x, y, z \\in V \\). When the dimension of \\( V \\) is \\( 2n \\), it can be expressed in terms of a basis comprising \\( n \\) pairs of vectors \\( e_i + f_i \\) and \\( e_i - f_i \\) for \\( 1 \\leq i \\leq n \\), with the properties \\( \\langle e_i, e_j \\rangle = 0 \\) and \\( \\langle f_i, f_j \\rangle = 0 \\), while \\( \\langle e_i, f_j \\rangle = \\delta_{ij} \\). Vertex operator superalgebras, which were independently introduced by Borcherds and Kac, serve as supersymmetric counterparts to vertex operator algebras. These structures consist of a \\( \\mathbb{Z}/2\\mathbb{Z} \\)-graded vector space \\( V = V_0 \\oplus V_1 \\), a vacuum vector \\( |0\\rangle \\in V_0 \\), a conformal element \\( \\omega \\in \\text{End}(V) \\), a parity shift operator \\( \\Pi: V \\to V \\) that interchanges \\( V_0 \\) and \\( V_1 \\), and a collection of fields \\( Y(x, z) \\) (known as vertex operators) indexed by elements \\( x \\in V \\) and complex integers \\( h \\in \\mathbb{C} \\), all adhering to specific axioms including the Jacobi identity, associativity relations, and commutator formulas, among other conditions.",
        "ori-fast-z-score": 1.0690449676496976,
        "water-fast-z-score": 4.065863991822648,
        "rewrite-fast-z-score": 1.044465935734187
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Stability of the splay state in pulse--coupled networks .\nAbstract:\nWe study stability properties of the splay state for a class of pulse-coupled networks with time delays and nonlinear coupling functions. We show that, under certain conditions on the parameters of the system, there exists an unstable periodic orbit embedded into the basin of attraction of the splay state. This result is illustrated by numerical simulations. PACS numbers: 05.45.Mt, 47.55.Jk, 87.19 .Hj\nI. INTRODUCTIO N\nPulse-coupled neural networks are widely used to model various phenomena observed in neuroscience  1  , such as synchronization  2  -  4  , pattern formation  5  -  8  , or information processing  9  .\nIn this work we consider a particular type of pulse-coupled systems known as delay-differential equations (DDEs)  10  -  12  . In these models each node can be described by its own internal dynamics which evolve continuously in time but interact only at discrete times when pulses are transmitted between nodes. The transmission of pulses may take some finite amount of time depending on the distance between two interacting neurons  13  . Such interactions lead to appearance of time delays in DDEs describing the evolution of the whole system  14  -  16  . Time-delayed feedbacks play important role in many biological processes  17  -  19  including brain activity  20  -  22  . For example, it has been shown experimentally  23  that the presence of time delays leads to generation of rhythmic patterns in neuronal cultures  24  . It was also demonstrated  25  that time delays have significant effect on the collective behavior of populations of coupled oscillators  26  -  28  . Another interesting feature of DDE-based models is their ability to describe nonlocal interactions  29  -  31  . Nonlinearity of couplings plays another important role in modeling real-world problems  32  -  35  . Indeed, recent experimental studies  36  suggest that synaptic connections between neurons are not linear  37  . Moreover, they often exhibit saturation effects  38  -  41  . These facts motivate us to investigate the influence of both time delays and nonlinearities on the dynamical behavior of pulse-coupled networks.\nThe main goal of our research is to analyze the stability",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Stability of the splay state in pulse - - coupled networks . Abstract : We research stability properties of the splay state for a class of pulse - coupled networks with time delays and nonlinear coupling functions .We see that , under certain conditions on the variables of the system , there exists an weak periodic orbit integrated into the basin of attraction of the splay state . This result is depicted by numerical simulations .PACS scores : 05 . 45 . Mt , 47 . 55 . Jk , 87 . 19 . Hj I . INTRODUCTIO N Pulse - coupled brain systems are widely using to model numerous phenomena observed in neuroscience 1 , such as synchronization 2 - 4 , pattern formation 5 - 8 , or information processing 9 . In this research we study a certain type of pulse - coupled networks known as delay - differential equations ( DDEs ) 10 - 12 .In these models each node can be described by its own internal mechanics which evolve constantly in time but behave only at discrete times when signals are sent between nodes . The communication of pulses could took some finite quantity of time depending on the distance between two interacting neurons 13 .Such interactions result to appearance of time delays in DDEs describing the evolution of the whole system 14 - 16 . Time - slowed feedbacks serve important role in different biological functions 17 - 19 notably brain development 20 - 22 .For instance , it has been shown experimentally 23 that the presence of time failures helps to development of rhythmic patterns in neuronal cultures 24 . It was also demonstrated 25 that time failures have considerable impact on the collective behavior of groups of coupled oscillators 26 - 28 .Another important feature of DDE - based models is their capabilities to explain nonlocal interactions 29 - 31 . Nonlinearity of couplings makes another important role in modeling real - time situations 32 - 35 .Indeed , recent experimental studies 36 suggest that synaptic connections between neurons are not continuous 37 . Moreover , they frequently exhibit saturation effects 38 - 41 .These insights motivate us to examine the impact of both time errors and nonlinearities on the dynamical behavior of pulse - coupled networks . The main goal of our research is to analyze the stability",
        "rewrite_text": "**Title:** Stability of the Splay State in Pulse-Coupled Networks\n\n**Abstract:** This study investigates the stability characteristics of the splay state within a specific category of pulse-coupled networks that incorporate time delays and nonlinear coupling functions. Our findings indicate that, under particular conditions related to the system's variables, a weak periodic orbit can be integrated into the basin of attraction of the splay state. This phenomenon is supported by numerical simulations that illustrate the dynamics involved. The research is situated within the broader context of pulse-coupled brain systems, which are increasingly utilized to model various phenomena in neuroscience, including synchronization, pattern formation, and information processing. We focus on a subset of these systems characterized by delay-differential equations (DDEs), where each node is governed by its internal dynamics that evolve continuously but interact at discrete intervals through pulse signaling. The transmission of these pulses is subject to finite time delays, influenced by the spatial separation between interacting neurons. Such time delays are critical in shaping the feedback mechanisms that play essential roles in numerous biological processes, particularly in brain development. Experimental evidence has shown that time delays can facilitate the emergence of rhythmic patterns in neuronal cultures and significantly affect the collective behavior of coupled oscillators. Additionally, DDE-based models are adept at capturing nonlocal interactions, while the nonlinear nature of couplings is vital for accurately representing real-time biological scenarios. Recent experimental findings suggest that synaptic connections between neurons are often discontinuous and can exhibit saturation effects. These insights prompt a thorough examination of how both time delays and nonlinearities influence the dynamical behavior of pulse-coupled networks. The primary objective of our research is to analyze the stability of the splay state, contributing to a deeper understanding of the underlying mechanisms that govern the behavior of complex neural systems. \n\n**PACS scores:** 05.45.Mt, 47.55.Jk, 87.19.Hj",
        "ori-fast-z-score": -0.7761505257063328,
        "water-fast-z-score": 7.615384615384615,
        "rewrite-fast-z-score": 1.8973665961010275
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Magnetic Evolution and Temperature Variation in a Coronal Hole .\nAbstract:\nWe present the results of an analysis of magnetic field evolution, temperature variation, and plasma flow velocity observed by Hinode/SOT/SP (Solar Optical Telescope Spectro-Polarimeter) on September 24-25, 2007 during solar minimum period. The active region NOAA 10930 was located at S19E09 when it produced two flares with GOES class M5.7 and M1.0 respectively. We found that there were significant changes in the photospheric magnetic fields before and after these flares. In particular, we detected a new flux emergence event which occurred about one hour prior to the first flare. This newly emerged flux led to the formation of a coronal hole overlying the active region. After the second flare, the polarity inversion line moved toward the center of the active region where the strongest magnetic shear existed. During this time interval, we also found a rapid decrease in the intensity of the Fe xxi emission lines near the footpoints of the loops connecting opposite-polarities. These observations suggest that the energy released by the flares may have caused heating of the loop-top regions as well as evaporation of chromospheric material into the corona along the reconnected loops.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Magnetic Evolution and Temperature Variation in a Coronal Hole . Abstract : We report the conclusion of an assessment of magnetic field evolution , temperature variation , and plasma circulation velocity observed by Hinode / SOT / SP ( Solar Optical Telescope Spectro - Polarimeter ) on September 24 - 25 , 2007 during solar low time .The active region NOAA 10930 was positioned at S19E09 when it produced two flares with GOES class M5 . 7 and M1 . 0 respectively . We determined that there were substantial alterations in the photospheric magnetic fields before and after these flares .In particular , we identified a new flux emergence activity which occurred about one hour prior to the first flare . This newly emerged flux led to the formation of a coronal hole overlying the active region .After the second flare , the polarity inversion line moved toward the center of the active region where the greatest magnetic shear existed . During this time interval , we also discovered a rapid decrease in the strength of the Fe xxi emission lines near the footpoints of the loops connecting opposite - polarities .These measurements suggest that the electricity emitted by the flares might have caused heating of the loop - top regions as also as evaporation of chromospheric matter into the corona along the reconnected loops .",
        "rewrite_text": "We present a comprehensive analysis of the evolution of magnetic fields, temperature fluctuations, and plasma circulation velocities observed by the Hinode Solar Optical Telescope Spectro-Polarimeter (SOT/SP) on September 24-25, 2007, during a period of low solar activity. The study focuses on active region NOAA 10930, located at S19E09, which produced two significant flares classified as GOES M5.7 and M1.0. Our findings reveal notable changes in the photospheric magnetic fields both prior to and following these flares. Specifically, we observed a new emergence of magnetic flux approximately one hour before the occurrence of the first flare, which contributed to the development of a coronal hole situated above the active region. Following the second flare, we noted a shift in the polarity inversion line towards the center of the active region, where the magnetic shear was most pronounced. Concurrently, we recorded a rapid decline in the intensity of Fe XXI emission lines near the footpoints of the magnetic loops that connect opposite polarities. These observations imply that the energy released during the flares may have induced heating in the loop-top regions and facilitated the evaporation of chromospheric material into the corona along the reconnected magnetic loops. This study enhances our understanding of the dynamic processes occurring in solar active regions and their implications for coronal heating and solar wind generation.",
        "ori-fast-z-score": 0.6396021490668313,
        "water-fast-z-score": 4.7699904600286205,
        "rewrite-fast-z-score": 1.949358868961793
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Unique Determination of the Effective Potential in Terms of Renormalization Group Functions .\nAbstract:\nWe present an explicit expression for the effective potential in terms of renormalization group functions, which is valid to all orders in perturbation theory and can be used as input into numerical calculations.  We show that this result agrees with previous results obtained by other methods at one-loop order. The new formula has several advantages over existing expressions:  It does not require any additional approximations beyond those inherent in the use of perturbation theory; it allows us to calculate the effective potential directly without having to solve complicated integral equations first; finally, we are able to obtain the full dependence on temperature and chemical potentials simultaneously. This work was supported by the U.S. Department of Energy under Contract No. DE-AC02-76SF00515. In recent years there have been many attempts to determine the effective potential Veff(T) = ln Z(T), where T denotes the temperature, using various techniques such as the imaginary time formalism  1  , the real time formalism  2  or the functional renormalization group (FRG)  3  . These approaches provide useful information about the phase structure of quantum field theories but they usually involve some kind of approximation scheme. For example, in the FRG approach one often uses truncations of the exact flow equation  4  .\nIn this letter we will derive an explicit expression for Veff(T) in terms of renormalisation group functions  5  . Our method is based on the observation  6  that the effective action Γk(φ) (where k denotes the momentum scale) satisfies a differential equation known as the Wetterich equation  7, 8  \nHere Rk(Γk; φ) is called the regulator function and describes how the infrared modes are suppressed when integrating out high energy degrees of freedom. By solving Eq. (1) numerically  9  one obtains the running coupling constants gk(φ). Using these quantities together with the corresponding β-functions one can then compute Veff(T) according to",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Unique Determination of the Effective Potential in Terms of Renormalization Group Functions . Abstract : We present an explicit expression for the effective potential in terms of renormalization group functions , which is valid to all orders in perturbation theory and can be used as input into numerical analyses .We say that this consequence agrees with previous findings obtained by other methods at one - loop order . The revised formula has numerous benefits over existing expressions : It does not require any additional approximations beyond those inherent in the using of perturbation theory ; it allows us to estimate the effective potential directly without having to solve intricate integral equations first ; finally , we are able to obtain the full dependence on pressure and chemical potentials separately .This project was supported by the U . S . Department of Energy under Contract No . DE - AC02 - 76SF00515 .In past decades there have been many efforts to estimate the effective potential Veff ( T ) = ln Z ( T ) , where T denotes the temperature , using numerous algorithms such as the imaginary time formalism 1 , the real time formalism 2 or the functional renormalization group ( FRG ) 3 . These approaches provide useful details about the phase composition of quantum field theories but they generally include some kind of approximation scheme .For instance , in the FRG method one frequently uses truncations of the exact flow formula 4 . In this letter we will generate an explicit expression for Veff ( T ) in terms of renormalisation group functions 5 .Our formula is based on the observation 6 that the effective operation Γk ( φ ) ( where k denotes the velocity scale ) satisfies a differential equation known as the Wetterich equation 7 , 8 Here Rk ( Γk ; φ ) is dubbed the regulator function and explains how the infrared modes are suppressed when integrating out large energy degrees of liberty . By solving Eq .( 1 ) numerically 9 one obtains the running coupling constants gk ( φ ) . Using these quantities combined with the associated β - functions one can then compute Veff ( T ) according to",
        "rewrite_text": "**Title:** A Unique Determination of the Effective Potential in Terms of Renormalization Group Functions\n\n**Abstract:** In this article, we introduce a novel expression for the effective potential, articulated through renormalization group functions, which holds true to all orders in perturbation theory. This formulation serves as a valuable tool for numerical analyses, aligning with previous results derived through alternative methodologies at the one-loop level. The advantages of our revised expression are manifold: it eliminates the need for additional approximations beyond those inherent in perturbation theory; it facilitates direct estimation of the effective potential without necessitating the resolution of complex integral equations; and it enables a comprehensive understanding of the dependencies on pressure and chemical potentials independently. This research was conducted with the support of the U.S. Department of Energy under Contract No. DE-AC02-76SF00515. \n\nOver the past few decades, significant efforts have been made to estimate the effective potential \\( V_{\\text{eff}}(T) = \\ln Z(T) \\), where \\( T \\) represents temperature. Various algorithms have been employed, including the imaginary time formalism, real time formalism, and functional renormalization group (FRG) methods. While these approaches yield insightful information regarding the phase structure of quantum field theories, they often rely on certain approximation schemes. For instance, the FRG method typically involves truncations of the exact flow equations. In this letter, we derive an explicit expression for \\( V_{\\text{eff}}(T) \\) based on renormalization group functions. Our derivation is grounded in the observation that the effective action \\( \\Gamma_k(\\phi) \\), where \\( k \\) denotes the scale of velocity, adheres to the Wetterich equation. The regulator function \\( R_k(\\Gamma_k; \\phi) \\) elucidates the suppression of infrared modes when integrating out high-energy degrees of freedom. By numerically solving the Wetterich equation, we can obtain the running coupling constants \\( g_k(\\phi) \\). Subsequently, utilizing these quantities alongside their corresponding beta functions, we can compute \\( V_{\\text{eff}}(T) \\) effectively.",
        "ori-fast-z-score": 0.9198662110077999,
        "water-fast-z-score": 7.442553889063108,
        "rewrite-fast-z-score": 1.191759143062248
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Higher order antibunching in intermediate states .\nAbstract:\nWe study the second-order correlation function for an atom interacting with two modes of light, one resonant and another off-resonant to atomic transition frequency. We show that higher order antibunching can be observed when the atom is initially prepared in an excited state or ground state superposition. The effect is more pronounced if the initial state has some population on the excited state. This phenomenon may have applications in quantum information processing. \n \n Introduction:-In recent years there has been considerable interest in studying nonclassical properties of radiation fields generated by atoms  1  . In particular, it was shown that the photon statistics of such systems are governed by the first-order coherence function g (1) (τ)  2  , which describes bunching behavior at short times and anti-bunching at longer times  3  . It is well known that this property arises due to destructive interference between different pathways leading to emission of photons  4  .\nRecently, several authors studied the effects of spontaneous emission on the second-order correlation functions  5  -  8  . They showed that the presence of spontaneous emission leads to sub-Poissonian statistics  6 - 8  . However, these studies were restricted only to the case where the atom interacts with a single mode of field. On the other hand, many experiments involving atoms interacting simultaneously with multiple modes of electromagnetic field have also been performed  9  -  11  . For example, in Ref.  10  , the authors investigated the influence of vacuum fluctuations on the fluorescence spectrum of a three-level system driven by two laser beams. In addition, they found that the intensity noise of the emitted light depends strongly on the relative phase difference between the driving lasers. Motivated by these experimental results we consider here the problem of calculating the second-order correlation function of an atom interacting simultaneously with two modes of light  12  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Higher order antibunching in intermediate states . Abstract : We research the second - order correlation function for an element interacting with two modes of light , one resonant and another off - resonant to atomic transition frequency .We see that higher order antibunching can be found when the atom is initially prepared in an excited state or ground state superposition . The phenomenon is more pronounced if the first state has some population on the excited state .This phenomenon might have applications in quantum information processing . Introduction : - In recent years there has been substantial interest in investigating nonclassical characteristics of radiation fields generated by atoms 1 .In particular , it was shown that the photon statistics of such systems are governed by the first - order coherence function g ( 1 ) ( τ ) 2 , which explains bunching behavior at short periods and pro - bunching at shorter times 3 . It is well established that this property arises owing to destructive interference between various pathways leading to emission of photons 4 .Recently , various scientists examined the effects of induced emission on the second - order correlation functions 5 - 8 . They showed that the presence of induced emission contributes to sub - Poissonian statistics 6 - 8 .However , these research were restricted only to the case where the atom interacts with a single mode of field . On the other hand , many tests featuring atoms interacting simultaneously with various modes of electromagnetic field have also been performed 9 - 11 .For instance , in Ref . 10 , the published investigated the impact of vacuum fluctuations on the fluorescence spectrum of a three - level network driven by two laser beams .In addition , they reported that the frequency sound of the emitted light depends strongly on the relative phase change between the driving lasers . Motivated by these experimental results we consider here the question of calculating the second - order correlation function of an element interacting simultaneously with two modes of light 12 .",
        "rewrite_text": "**Title:** Higher Order Antibunching in Intermediate States\n\n**Abstract:** This study investigates the second-order correlation function of an atom interacting with two distinct modes of light: one that is resonant and another that is off-resonant with respect to the atomic transition frequency. Our findings reveal that higher order antibunching effects can emerge when the atom is initially prepared in a superposition of its excited state and ground state. Notably, the antibunching phenomenon is accentuated when the initial state possesses a significant population in the excited state. This behavior suggests potential applications in the realm of quantum information processing, where control over photon statistics is crucial. \n\nIn recent years, there has been a growing interest in exploring the nonclassical properties of radiation fields produced by atoms. Previous research has demonstrated that the photon statistics in such systems are influenced by the first-order coherence function, g(1)(τ), which accounts for both bunching behavior at short timescales and pro-bunching at even shorter intervals. This effect is attributed to destructive interference among various emission pathways. Recent studies have also examined the role of induced emission on second-order correlation functions, showing that it can lead to sub-Poissonian statistics. However, these investigations have primarily focused on scenarios where the atom interacts with a single mode of light.\n\nIn contrast, numerous experiments have explored the interaction of atoms with multiple modes of the electromagnetic field. For example, one study analyzed the influence of vacuum fluctuations on the fluorescence spectrum of a three-level atomic system driven by two laser beams, revealing a strong dependence of the emitted light's frequency on the relative phase between the lasers. Inspired by these experimental observations, our research aims to calculate the second-order correlation function for an atom interacting with two light modes, thereby contributing to the understanding of photon statistics in complex quantum systems.",
        "ori-fast-z-score": -0.4743416490252569,
        "water-fast-z-score": 6.344409644902455,
        "rewrite-fast-z-score": 1.9093374820217521
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  In-orbit focal adjustment of the AKARI telescope with IRC images .\nAbstract:\nWe have performed in-orbit focal adjustment (IFA) for the infrared camera onboard AKARI satellite using its own data taken in orbit. The IFA was carried out by comparing the observed point spread function (PSF) and that simulated based on ray tracing analysis, which is one of the most accurate methods to determine the best focus position. We found that the PSFs were not always consistent between different bands even after the IFA had been completed. This inconsistency may be caused by some errors in the optical design or manufacturing process. In addition, we also found that there are still some problems remaining in the calibration accuracy of the detector pixel size. These results will help us improve our understanding about the performance of the instrument as well as provide useful information for future space missions. Keywords: Space mission, Focal adjustment, Point spread function, Ray tracing analysis, Infrared astronomy, Infrared camera",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : In - orbit focal adjustment of the AKARI telescope with IRC data . Abstract : We have done in - orbit lens adjustment ( IFA ) for the infrared camera onboard AKARI station use its own observations made in space .The IFA was carried out by comparing the seen point spread constant ( PSF ) and that simulated based on ray tracing examination , which is one of the most accurate ways to identify the best focus position . We determined that the PSFs were not always compatible between various bands long after the IFA had been completed .This inconsistency may be caused by some defects in the optical design or manufacturing system . In addition , we also discovered that there are still some problems remaining in the calibration reliability of the sensor pixel size .These data will assist us improve our appreciation about the performance of the instrument as also as give valuable info for future space missions . Keywords : Space mission , Focal correction , Point spread function , Ray tracing investigation , Infrared astronomy , Infrared camera",
        "rewrite_text": "Title: In-Orbit Focal Adjustment of the AKARI Telescope Utilizing IRC Data\n\nAbstract: This study presents the in-orbit lens adjustment (IFA) performed on the infrared camera aboard the AKARI telescope, utilizing observations collected during its mission in space. The IFA process involved a comparative analysis of the observed point spread function (PSF) against simulations derived from ray tracing techniques, recognized as one of the most precise methods for determining optimal focus positions. Our findings revealed that the PSFs exhibited inconsistencies across different spectral bands even after the completion of the IFA. This discrepancy may stem from potential flaws in the optical design or issues related to the manufacturing process. Furthermore, we identified lingering challenges concerning the calibration accuracy of the sensor pixel size, which could impact the overall data quality. The insights gained from this investigation are crucial for enhancing our understanding of the instrument's performance and will provide valuable information for the planning and execution of future space missions. By addressing these calibration and design issues, we aim to improve the reliability of infrared observations, thereby contributing to the advancement of infrared astronomy. This research underscores the importance of continuous assessment and adjustment of spaceborne instruments to ensure optimal functionality and data integrity in the pursuit of astronomical knowledge.\n\nKeywords: Space mission, Focal correction, Point spread function, Ray tracing investigation, Infrared astronomy, Infrared camera.",
        "ori-fast-z-score": -0.9878783399072131,
        "water-fast-z-score": 5.237229365663818,
        "rewrite-fast-z-score": 0.7492686492653552
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Bose-Einstein-condensed gases in arbitrarily strong random potentials .\nAbstract:\nWe study the properties of Bose-Einstein condensates (BECs) with an arbitrary number N of particles trapped by a harmonic oscillator and exposed to a disordered external potential. We show that, for any strength of disorder, there is always a finite fraction of atoms localized at each site of the lattice. The localization length decreases as the disorder increases but remains macroscopic even when the disorder becomes very large compared to the interatomic interaction energy. This result holds true both in one dimension and higher dimensions.  In particular we find that the critical disorder above which all states are localized scales like 1/N in 1D and 1/d in 2D and 3D where d is the spatial dimension. Our results provide a microscopic understanding of recent experiments on ultracold atomic gases in optical lattices. Introduction:-Recent experimental advances have made it possible to create quantum degenerate gases of bosons or fermions confined in periodic potentials  1  . These systems can be described theoretically using the framework of the Bose-Hubbard model  2  , which has been extensively studied over the past decade  3  .\nIn this work we consider the case of a gas of interacting bosons in a disordered potential. Disorder leads to Anderson localization  4  : eigenstates become exponentially localized around their initial position if the disorder exceeds some threshold value  5  . It was recently shown experimentally  6  that such a system exhibits a transition between extended Bloch-like states and localized Wannier-Stark ladders  7, 8  . However, these experiments were performed only in the weak-disorder regime, i.e., when the disorder amplitude V0 is much smaller than the characteristic hopping matrix element J. Here we investigate how the presence of interactions affects the physics of strongly disordered systems.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Bose - Einstein - condensed gases in arbitrarily stable random potentials . Abstract : We research the properties of Bose - Einstein condensates ( BECs ) with an arbitrary number N of atoms trapped by a harmonic oscillator and exposed to a disordered external potential .We see that , for any strength of disorder , there is usually a finite fraction of atoms concentrated at each site of the lattice . The localization width decreases as the disorder advances but remains macroscopic even when the disorder becomes very huge compared to the interatomic interaction power .This result holds true both in one dimension and higher dimensions . In particular we find that the fundamental disorder above which all states are localized scales like 1 / N in 1D and 1 / d in 2D and 3D where d is the spatial dimension .Our results present a microscopic understanding of recent experiments on ultracold atomic atoms in optical lattices . Introduction : - Recent research developments have enabled it able to create quantum degenerate gases of bosons or fermions localized in periodic potentials 1 .These systems can be described theoretically using the framework of the Bose - Hubbard theory 2 , which has been heavily explored over the previous decade 3 . In this research we define the case of a gas of interacting bosons in a disordered potential .Disorder leads to Anderson localization 4 : eigenstates grow exponentially localized around their early position if the disorder approaches some threshold value 5 . It was recently shown experimentally 6 that such a system displays a change between extended Bloch - like states and localized Wannier - Stark ladders 7 , 8 .However , these experiments were performed only in the weak - disturbance regime , i . e . , when the disorder amplitude V0 is much smaller than the typical hopping matrix element J . Here we investigate how the presence of interactions affects the physics of heavily disordered systems .",
        "rewrite_text": "**Title:** Bose-Einstein Condensed Gases in Arbitrarily Stable Random Potentials\n\n**Abstract:** This study investigates the characteristics of Bose-Einstein condensates (BECs) comprising an arbitrary number \\( N \\) of atoms that are confined within a harmonic oscillator while subjected to a disordered external potential. Our findings reveal that, regardless of the disorder's strength, a significant fraction of atoms tends to localize at each lattice site. Notably, as the disorder intensifies, the localization width diminishes; however, it remains macroscopic even when the disorder surpasses the interatomic interaction strength considerably. This phenomenon is consistent across both one-dimensional and higher-dimensional systems. Specifically, we identify a critical disorder threshold beyond which all states become localized, which scales as \\( 1/N \\) in one dimension and \\( 1/d \\) in two and three dimensions, where \\( d \\) represents the spatial dimension. Our results provide a microscopic framework that aligns with recent experimental observations involving ultracold atoms in optical lattices.\n\n**Introduction:** Recent advancements in research have facilitated the creation of quantum degenerate gases of bosons and fermions that are localized within periodic potentials. These systems can be effectively described using the Bose-Hubbard model, which has been extensively studied over the past decade. In this paper, we focus on the scenario of interacting bosons in a disordered potential. The presence of disorder induces Anderson localization, where eigenstates become exponentially localized around their initial positions once the disorder surpasses a certain threshold. Recent experimental evidence has demonstrated a transition in such systems from extended Bloch-like states to localized Wannier-Stark ladders. However, these experiments have primarily been conducted in the weak disorder regime, where the disorder amplitude \\( V_0 \\) is significantly smaller than the typical hopping matrix element \\( J \\). In this work, we delve into the impact of interactions on the behavior of systems subjected to strong disorder, thereby enhancing our understanding of the interplay between disorder and interactions in BECs.",
        "ori-fast-z-score": -0.7016464154456235,
        "water-fast-z-score": 6.028557990961161,
        "rewrite-fast-z-score": 0.44367825470805694
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  XMM-Newton observations of the eclipsing polar V2301 Oph .\nAbstract:\nWe present new X-ray data on the eclipsing polar, V2301 Oph (=V1429 Aql), obtained with XMM-Newton in 2003 and 2004 . The light curve shows two partial eclipses at orbital phases 0.85-0.95 and 0.65-0.75 which are interpreted as occultations by the accretion curtain. We find that the eclipse profiles change significantly between the two epochs. In particular, we detect an additional dip during phase 0.9-1.0 in 2004 which is not seen in 2003. This feature may be associated with the presence of a third body orbiting around the system. The spectrum can be described well using a combination of a blackbody component for the white dwarf and a power law model for the hard emission. From our spectral fits to both datasets we derive values for the temperature T wd = 16000 ± 1000 K and luminosity L x = 1.1 × 1031 erg s−1 for the white dwarf.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : XMM - Newton discoveries of the eclipsing polar V2301 Oph . Abstract : We report new X - ray data on the eclipsing polar , V2301 Oph ( = V1429 Aql ) , obtained with XMM - Newton in 2003 and 2004 .The light curve shows two partial eclipses at orbital phases 0 . 85 - 0 . 95 and 0 . 65 - 0 . 75 which are interpreted as occultations by the accretion veil . We see that the eclipse profiles change considerably between the two epochs .In particular , we perceive an additional dip during phase 0 . 9 - 1 . 0 in 2004 which is not seen in 2003 . This characteristic could be involved with the presence of a third body orbiting around the system .The spectrum can be described good using a combination of a blackbody element for the white dwarf and a power law description for the hard emission . From our spectral fits to both datasets we derive values for the temperature T wd = 16000 ± 1000 K and luminosity L x = 1 . 1 × 1031 erg s−1 for the white dwarf .",
        "rewrite_text": "We present new X-ray observations of the eclipsing polar V2301 Oph (also known as V1429 Aql), acquired using the XMM-Newton satellite during the years 2003 and 2004. Our analysis reveals a light curve characterized by two distinct partial eclipses occurring at orbital phases 0.85-0.95 and 0.65-0.75, which we interpret as being caused by the accretion veil obscuring the X-ray emissions from the system. Notably, we observe significant variations in the eclipse profiles between the two observational epochs. In particular, the 2004 data exhibits an additional dip in the light curve during the phase range of 0.9-1.0, a feature that was absent in the 2003 observations. This anomaly may suggest the influence of a third body in orbit around the V2301 Oph system, potentially affecting the dynamics of the accretion process.\n\nFurthermore, we analyze the spectral data, which can be effectively modeled using a combination of a blackbody spectrum to represent the white dwarf component and a power-law model to account for the hard X-ray emissions. From our spectral fitting of both datasets, we derive key parameters for the white dwarf, including a temperature of T_wd = 16,000 ± 1,000 K and an X-ray luminosity of L_x = 1.1 × 10^31 erg s^−1. These findings contribute to our understanding of the physical characteristics and behavior of V2301 Oph, providing insights into the complex interactions within this binary system and the potential role of additional celestial bodies in influencing its dynamics.",
        "ori-fast-z-score": -1.25,
        "water-fast-z-score": 2.108590488016544,
        "rewrite-fast-z-score": -0.5883484054145521
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Results on axion physics from the CAST Experiment at CERN .\nAbstract:\nThe Axion Dark Matter Search (ADMX) experiment is designed to search for dark matter in the form of axions, which are hypothetical particles predicted by theories beyond the Standard Model.  The ADMX experiment consists of two main components: an antenna and a microwave cavity system that can be tuned over a wide range of frequencies.   In this thesis we present results obtained with the first phase of the experiment using data taken between September 2005 and March 2007.    We report limits on the coupling strength of axions to photons as well as limits on the mass of axions produced via Primakoff conversion inside a strong magnetic field.  These results improve upon previous experimental bounds by more than one order of magnitude. This work was performed under the auspices of the U.S Department of Energy by Lawrence Livermore National Laboratory under Contract DE-AC52-07NA27344. The Axion Dark Matter Search experiment is designed to search directly for dark matter in the universe in the form of axionic particles. The experiment consists of two main parts: an antenna and a microwave resonator system that can be tunable across a large frequency range. In this dissertation I will discuss our recent results from the first stage of the experiment.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Results on axion theory from the CAST Experiment at CERN . Abstract : The Axion Dark Matter Search ( ADMX ) experiment is designed to search for black material in the form of axions , which are hypothetical particles expected by theories beyond the Standard Model .The ADMX experiment consists of two principal components : an antenna and a microwave cavity network that can be tuned over a broad variety of frequencies . In this dissertation we present results derived with the first phase of the program including data taken between September 2005 and March 2007 .We report limits on the interaction strength of axions to photons as well as limits on the mass of axions produced via Primakoff transformation inside a powerful magnetic field . These conclusions progress upon former empirical bounds by more than one order of magnitude .This project was done under the auspices of the U . S Department of Energy by Lawrence Livermore National Laboratory under Contract DE - AC52 - 07NA27344 . The Axion Dark Matter Search experiment is designed to search specifically for black material in the universe in the form of axionic particles .The project consists of two principal parts : an antenna and a microwave resonator system that can be tunable across a large frequency spectrum . In this dissertation I will explore our latest findings from the first phase of the project .",
        "rewrite_text": "Title: Findings on Axion Theory from the CAST Experiment at CERN\n\nAbstract: The Axion Dark Matter Search (ADMX) experiment aims to investigate the existence of dark matter in the form of axions, which are theoretical particles predicted by various extensions of the Standard Model of particle physics. The ADMX setup comprises two main components: a highly sensitive antenna and a microwave cavity system that can be finely tuned across a wide range of frequencies. In this dissertation, we present significant results obtained during the initial phase of the experiment, which involved data collection from September 2005 to March 2007. Our findings include stringent limits on the interaction strength between axions and photons, as well as constraints on the mass of axions generated through the Primakoff effect in a strong magnetic field. These results represent a substantial improvement over previous empirical limits, enhancing the sensitivity by more than an order of magnitude. This research was conducted under the auspices of the U.S. Department of Energy and was supported by Lawrence Livermore National Laboratory under Contract DE-AC52-07NA27344. The ADMX experiment is pivotal in advancing our understanding of dark matter, specifically by targeting axionic particles that may constitute a significant portion of the universe's missing mass. In this dissertation, I will delve into the implications of our latest discoveries from the first phase of the project, highlighting their relevance to ongoing efforts in the search for dark matter and the broader implications for particle physics.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.617988764033708,
        "rewrite-fast-z-score": -0.9901475429766744
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Analysis of random Boolean networks using the average sensitivity .\nAbstract:\nWe study the dynamics of random Boolean networks (RBNs) by analyzing their attractors and basins, which are determined by the network s state transition matrix. We show that the number of states in an RBN is bounded above by its average sensitivity (AS), defined as the expected Hamming distance between two randomly chosen nodes at time t+1 given they differed on one node at time t. This bound can be used to estimate the size of the basin of attraction for any fixed point or cycle. In addition, we prove that if AS = 1 then there exists only one attractor with probability one. Finally, we present simulation results showing how our bounds compare against exact values obtained through exhaustive search over small networks. Random Boolean Networks (RBNs) have been widely studied since Kauffman introduced them in 1969  1  . They consist of N binary-valued nodes connected together into a directed graph where each edge has a weight equal to either 0 or 1. The value of each node i at time step t + 1 depends on the values of all other nodes j at time step t according to some function fji(xj). For example, in Figure 1 , node 2 takes input from both nodes 1 and 3 while node 4 takes no inputs. A sequence of states x1, x2, ..., xt, ... is called an orbit when xi+1 = fi(xi) for every i > 0. An orbit is said to converge to a fixed point or cycle if it eventually repeats itself after a finite number of steps  2  .\nThe behavior of an RBN is completely specified by its adjacency matrix Mij, whose elements represent the weights of edges connecting node i to node j. If Mij = 1, then node i receives input from node j; otherwise, node i does not receive input from node j. Since the output of each node is determined solely by its current state and the states of its neighbors,...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Analysis of random Boolean systems using the average sensitivity . Abstract : We research the dynamics of random Boolean connections ( RBNs ) by analyzing their attractors and basins , which are decided by the network s state transition vector .We see that the number of states in an RBN is bounded above by its average sensitivity ( AS ) , defined as the expected Hamming distance between two randomly picked vertices at time t + 1 given they differed on one vertex at time t . This bound can be used to estimate the length of the basin of attraction for any fixed point or cycle . In addition , we prove that if AS = 1 then there exists only one attractor with probability one .Finally , we present modeling results showing how our limits compare against exact expressions derived through exhaustive search over tiny networks . Random Boolean Networks ( RBNs ) have been widely explored since Kauffman introduced them in 1969 1 .They consist of N binary - valued nodes linked together into a directed graph where each edge has a weight equal to either 0 or 1 . The value of each node i at time step t + 1 varies on the values of all other vertices j at time step t according to some function fji ( xj ) .For instance , in Figure 1 , node 2 takes input from both nodes 1 and 3 while node 4 takes no inputs . A sequence of states x1 , x2 , . . . , xt , . . . is dubbed an orbit when xi + 1 = fi ( xi ) for every i > 0 .An orbit is said to converge to a fixed point or cycle if it soon repeats itself after a finite number of steps 2 . The behavior of an RBN is completely stated by its adjacency matrix Mij , whose elements represent the sizes of vertices linking node i to node j .If Mij = 1 , then node i takes input from node j ; otherwise , node i does not receive output from node j . Since the output of each node is chosen purely by its current state and the states of its relatives , . . .",
        "rewrite_text": "**Title:** Analysis of Random Boolean Systems Using Average Sensitivity\n\n**Abstract:** This study investigates the dynamics of Random Boolean Networks (RBNs) by examining their attractors and basins of attraction, which are determined by the network's state transition vector. We establish that the number of distinct states within an RBN is constrained by its average sensitivity (AS), which we define as the expected Hamming distance between two randomly selected vertices at time t + 1, given that they differ at one vertex at time t. This upper bound provides a valuable tool for estimating the size of the basin of attraction associated with any fixed point or cycle within the network. Furthermore, we demonstrate that when AS equals 1, there is a probability of one that the system will converge to a single attractor. Our modeling results reveal how these theoretical limits align with exact expressions obtained through exhaustive searches conducted on small networks. \n\nRandom Boolean Networks, first introduced by Kauffman in 1969, consist of N binary nodes interconnected in a directed graph, where each edge is assigned a weight of either 0 or 1. The state of each node i at time step t + 1 is influenced by the values of other nodes j at time t, according to a specific function fji(xj). For example, in a given network configuration, node 2 may receive inputs from nodes 1 and 3, while node 4 may operate independently without any inputs. A sequence of states x1, x2, ..., xt, ... is classified as an orbit when the relationship xi + 1 = fi(xi) holds for all i > 0. An orbit is said to converge to a fixed point or cycle if it begins to repeat after a finite number of iterations. The dynamics of an RBN can be fully characterized by its adjacency matrix Mij, where the elements indicate the connections between nodes. If Mij = 1, node i receives input from node j; otherwise, it does not. The output of each node is determined solely by its current state and the states of its connected nodes, highlighting the intricate interplay within the network.",
        "ori-fast-z-score": -1.0,
        "water-fast-z-score": 3.972508265291366,
        "rewrite-fast-z-score": -0.8574929257125441
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Metal and molecule cooling in simulations of structure formation .\nAbstract:\nWe present results on metal and molecular gas cooling in cosmological hydrodynamic simulations with the Enzo code, focusing on the effects of different numerical schemes for solving the energy equation (SPH vs. grid-based) as well as varying physical prescriptions for star formation feedback. We find that SPH codes tend to overestimate the amount of cold gas at high redshifts compared to grid-based methods due to artificial viscosity heating. Feedback models which include galactic winds are able to suppress this effect by removing low entropy material from galaxies. However, we also show that these wind models can lead to an underestimation of the total mass fraction of cool gas if they remove too much hot halo gas around massive halos. Overall our results suggest that current state-of-the-art galaxy formation models produce reasonable estimates for the global properties of the warm-hot intergalactic medium but may still be missing important physics related to the detailed distribution of metals within individual galaxies.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Metal and molecule cooling in simulations of structure formation . Abstract : We report findings on metal and molecular gas cooling in cosmological hydrodynamic simulations with the Enzo code , concentrating on the effects of different numerical schemes for solving the electricity equation ( SPH vs . grid - based ) as well as varying mechanical prescriptions for star formation feedback .We see that SPH codes tend to overestimate the quantity of cold gas at high redshifts compared to grid - based methods thanks to artificial viscosity warming . Feedback models which use galactic winds are able to suppress this effect by removing lowered entropy debris from stars .However , we also demonstrate that these wind estimates can lead to an underestimation of the total mass fraction of cool gas if they eliminate too much warm halo gas around large halos . Overall our findings confirm that current state - of - the - art star formation models generate reasonable estimates for the global properties of the cool - warm intergalactic medium but might nevertheless be missing crucial physics related to the detailed distribution of metals within individual galaxies .",
        "rewrite_text": "In this study, we present our findings on the cooling processes of metal and molecular gas within the context of cosmological hydrodynamic simulations conducted using the Enzo code. Our research focuses on the impact of various numerical approaches for solving the electricity equation, specifically comparing Smoothed Particle Hydrodynamics (SPH) with grid-based methods. Additionally, we explore the implications of different mechanical models for star formation feedback on the cooling dynamics of gas in the universe. Our results indicate that SPH simulations tend to overestimate the amount of cold gas present at high redshifts, a discrepancy attributed to the effects of artificial viscosity, which inadvertently raises gas temperatures. We find that feedback mechanisms incorporating galactic winds can mitigate this overestimation by effectively removing low-entropy material produced by stellar processes. However, we also highlight a potential drawback of these wind models: they may inadvertently lead to an underrepresentation of the total mass fraction of cool gas if they excessively deplete warm halo gas surrounding large galactic structures. Overall, our findings suggest that while contemporary star formation models provide reasonably accurate estimates of the global characteristics of the cool-warm intergalactic medium, they may still overlook critical physical processes that govern the intricate distribution of metals within individual galaxies. This research underscores the need for further refinement of simulation techniques and feedback models to enhance our understanding of cosmic structure formation and the role of metal and molecular gas cooling in this complex environment.",
        "ori-fast-z-score": -0.8867963503478639,
        "water-fast-z-score": 4.631047607372178,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  GRB 060206 and the quandary of achromatic breaks in afterglow light curves .\nAbstract:\nWe present new optical/NIR data for GRB 060206, which show that its X-ray to radio afterglow is well described by an achromatic break at tbreak = 1.3 days followed by a power-law decay with index -1.2 (Fν ∝ t-1.2). The lack of any spectral evolution across this break suggests it was caused by energy injection into the blast wave. We find no evidence for dust extinction along our line-of-sight; however we cannot rule out significant reddening due to host galaxy dust. Our results are consistent with previous claims that achromatic breaks observed in many other bursts may be explained as being due to late-time energy injections rather than jet-break effects. \n \n Keywords: Gamma-ray burst, Afterglow emission, Energy injection, Jet break, Redshift measurement \n \n INTRODUCTION \n \n In recent years there has been growing interest in understanding how gamma ray bursts (GRBs) produce their broadband electromagnetic radiation. This effort has led to several successful models describing the prompt phase of GRB emission (see e.g., Piran 2005; Zhang 2007), but less progress on explaining the origin of the afterglow component. A key feature of most afterglows is the presence of a steepening or  jet break  in the light curve around one day postburst (Rhoads 1999) . Such breaks have traditionally been interpreted as marking the time when the relativistic ejecta becomes optically thin to synchrotron self-absorption, causing the flux density to drop rapidly. However, some authors argue that such breaks can also arise if the ejecta undergoes continued energy input following the initial explosion (e.g., Kumar & Panaitescu 2000; Granot et al. 2001; Chevalier & Li 2000) , while others suggest that they could instead result from changes in the geometry of the emitting region (e.g., Racusin et al. 2008 ). An alternative explanation for these breaks invokes interstellar scintillation (Goodman 1997; Goodman & Narayan 2006 ) - a phenomenon",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : GRB 060206 and the quandary of achromatic breaks in afterglow light paths . Abstract : We report new optical / NIR data for GRB 060206 , which show that its X - ray to radio afterglow is well described by an achromatic crack at tbreak = 1 . 3 days preceded by a power - law decay with index - 1 . 2 ( Fν [UNK] t - 1 . 2 ) .The absence of any spectral evolution across this break suggests it was due by energy injection into the explosion wave . We see no evidence for dust extinction along our line - of - seeing ; however we cannot leave out significant reddening due to host universe dust .Our results are consistent with previous statements that achromatic breaks found in many other bursts perhaps be understood as being related to late - time energy injections rather than jet - break influences . Keywords : Gamma - ray flare , Afterglow emission , Energy injection , Jet burst , Redshift measurement INTRODUCTION In recent seasons there has been growing interest in understanding how gamma ray bursts ( GRBs ) produce their broadband electromagnetic radiation .This effort has led to several successful theories describing the prompt stage of GRB emission ( saw e . g . , Piran 2005 ; Zhang 2007 ) , but less progress on explaining the origin of the afterglow component . A crucial characteristic of most afterglows is the presence of a steepening or jet break in the light spiral around one month postburst ( Rhoads 1999 ) .Such breaks have traditionally been viewed as indicating the period when the relativistic ejecta becomes optically thin to synchrotron self - absorption , forcing the flux concentration to fall swiftly . However , some writers argue that such breaks can also arise if the ejecta undergoes continued energy input following the first blast ( e . g . , Kumar & Panaitescu 2000 ; Granot et al .2001 ; Chevalier & Li 2000 ) , while many suggest that they may rather result from alterations in the topology of the emitting area ( e . g . , Racusin et al . 2008 ) .An alternative theory for these breaks invokes interstellar scintillation ( Goodman 1997 ; Goodman & Narayan 2006 ) - a phenomenon",
        "rewrite_text": "**Title:** GRB 060206 and the Quandary of Achromatic Breaks in Afterglow Light Paths\n\n**Abstract:** In this study, we present new optical and near-infrared (NIR) observations of Gamma-Ray Burst (GRB) 060206, revealing that its afterglow, spanning from X-ray to radio wavelengths, is characterized by an achromatic break occurring at approximately 1.3 days post-burst. This break is preceded by a power-law decay with an index of -1.2, described by the relationship Fν ∝ t^-1.2. Notably, the lack of spectral evolution across this break implies that it may be attributed to energy injection into the expanding shock wave rather than other mechanisms. Our analysis indicates no significant dust extinction along our line of sight; however, we acknowledge the possibility of substantial reddening caused by dust in the host galaxy. These findings align with previous assertions that the achromatic breaks observed in various GRBs could be more closely associated with late-time energy injections rather than being influenced by jet breaks. \n\nThe investigation into gamma-ray bursts (GRBs) has intensified in recent years, particularly regarding the mechanisms behind their broadband electromagnetic emissions. While several successful models have been proposed to explain the prompt emission phase (e.g., Piran 2005; Zhang 2007), the afterglow component remains less understood. A defining feature of many afterglows is the presence of a steepening or jet break in the light curve, typically observed around one month after the burst (Rhoads 1999). Traditionally, these breaks have been interpreted as indicators of when the relativistic ejecta transitions to an optically thin state, resulting in a rapid decline in flux due to synchrotron self-absorption. However, alternative theories suggest that these breaks may also result from ongoing energy input into the ejecta following the initial explosion (Kumar & Panaitescu 2000; Granot et al. 2001; Chevalier & Li 2000), or from changes in the geometry of the emitting region (Racusin et al. 2008). Another perspective posits that interstellar scintillation could play a role in the observed breaks (Goodman 1997; Goodman & Narayan 2006). Our findings contribute to the ongoing discourse surrounding the nature of achromatic breaks in GRB afterglows, emphasizing the need for further exploration into the underlying physical processes. \n\n**Keywords:** Gamma-ray burst, Afterglow emission, Energy injection, Jet break, Redshift measurement.",
        "ori-fast-z-score": -0.48666426339228763,
        "water-fast-z-score": 6.386771459290102,
        "rewrite-fast-z-score": -0.4685212856658182
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Optimal H2 order-one reduction by solving eigenproblems for polynomial equations .\nAbstract:\nWe consider the problem of finding an optimal first-order reduced basis space in the context of model reduction using Proper Orthogonal Decomposition (POD). We show that this can be achieved by solving a sequence of generalized eigenvalue problems associated with certain polynomials, which are derived from the original POD Galerkin formulation and its adjoint. The proposed approach is illustrated on two numerical examples. In particular we demonstrate how it allows to find a better approximation than standard greedy algorithms. This work was supported by EPSRC grant EP/K034494/1. Model reduction techniques have been widely used over recent years as efficient tools for reducing computational complexity when simulating complex physical systems governed by partial differential equations (PDEs)  1  . One popular class of methods relies on Proper Orthogonal Decompostion (POD), where one seeks to approximate the solution u(x, t) ∈ R N of a PDE system by projecting onto a low-dimensional subspace spanned by a set of orthonormal functions {φi(x)}Ni=1  2  , i.e.,",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Optimal H2 order - one reduction by finding eigenproblems for polynomial equations . Abstract : We consider the question of finding an efficient first - order reduced basis set in the context of model reduction using Proper Orthogonal Decomposition ( POD ) .We see that this can be obtained by handling a sequence of generalized eigenvalue difficulties related with certain polynomials , which are derived from the previous POD Galerkin approach and its adjoint . The proposed approach is depicted on two numerical examples .In particular we prove how it allows to find a better approximation than conventional greedy algorithms . This research was supported by EPSRC award EP / K034494 / 1 .Model reduction techniques have been widely using over recent years as efficient techniques for lowering computational complexity when simulating complex physical structures governed by partial differential equations ( PDEs ) 1 . One popular family of methods relies on Proper Orthogonal Decompostion ( POD ) , where one seeks to approximate the solution u ( x , t ) ∈ R N of a PDE system by projecting onto a small - dimensional subspace spanned by a group of orthonormal functions { φi ( x ) } Ni = 1 2 , i . e . ,",
        "rewrite_text": "**Title:** Optimal H2 Order-One Reduction through Eigenproblem Solutions for Polynomial Equations\n\n**Abstract:** This article addresses the challenge of establishing an efficient first-order reduced basis set within the framework of model reduction, specifically utilizing Proper Orthogonal Decomposition (POD). We demonstrate that this objective can be achieved by tackling a series of generalized eigenvalue problems associated with specific polynomial equations. These polynomials are derived from the conventional POD Galerkin method and its adjoint formulation. Our proposed methodology is illustrated through two numerical examples, showcasing its effectiveness in yielding superior approximations compared to traditional greedy algorithms. This research is backed by the EPSRC award EP/K034494/1. \n\nIn recent years, model reduction techniques have gained prominence as powerful tools for reducing computational complexity in the simulation of intricate physical systems governed by partial differential equations (PDEs). Among these techniques, Proper Orthogonal Decomposition (POD) stands out as a widely adopted approach. It aims to approximate the solution \\( u(x, t) \\in \\mathbb{R}^N \\) of a PDE system by projecting it onto a lower-dimensional subspace, which is spanned by a set of orthonormal functions \\( \\{ \\phi_i(x) \\}_{i=1}^N \\). By leveraging the insights gained from our analysis, we provide a framework that not only enhances the efficiency of model reduction but also improves the accuracy of the approximations obtained. The findings presented in this study contribute to the ongoing development of advanced model reduction strategies, offering a promising avenue for future research in the field.",
        "ori-fast-z-score": 0.105999788000636,
        "water-fast-z-score": 5.270462766947299,
        "rewrite-fast-z-score": -0.0949157995752499
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Analysis of thermally stimulated luminescence and conductivity without quasiequilibrium approximation .\nAbstract:\nWe present an analysis of the temperature dependence of thermally stimulated luminescent (TSL) glow curves in terms of the nonstationary electron-phonon relaxation theory, which does not assume that the system is close to equilibrium at any time during its evolution.  We show how this approach can be used for extracting information about the phonon spectrum and the density of states of charge carriers from TSL data obtained on different types of materials. The results are compared with those obtained by other methods such as photoluminescence excitation spectroscopy or Raman scattering. In particular we demonstrate that our method allows one to determine the energy gap between the conduction band minimum and valence band maximum in semiconductors. This work was supported by Russian Science Foundation grant No. 14-50-00040. DOI: 10.1063/1.4935190 \nI. INTRODUCTORY REMARK\nThe study of luminescence phenomena has been attracting considerable attention over many years because it provides valuable information about electronic structure and optical properties of solids  1  . Thermal stimulation luminescence (TSL), also known as optically stimulated luminescence (OSL), is particularly useful since it enables us to probe the distribution function of electrons excited into the conduction band  2  .\nIn recent decades there have been numerous attempts to develop theoretical models describing various aspects of luminescence processes  3  , including thermal stimulation luminescence  4  -  8  . However, most of these works were based on the assumption that the system under consideration is always close to equilibrium  9  . As a result they cannot describe correctly some important features observed experimentally  10  . For example, the shape of the TSL glow curve depends strongly on the type of material  11  : while in insulators it usually exhibits a single peak  12  , in metals it often consists of several peaks  13  . Moreover, even within the same class of materials, e.g., semiconductor crystals  14  , the number of peaks may vary depending on the doping level  15  . These observations cannot be explained using existing theories  16  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Analysis of thermally stimulated luminescence and conductivity without quasiequilibrium approximation . Abstract : We present an assessment of the temperature dependence of thermally stimulated luminescent ( TSL ) glow curves in terms of the nonstationary electron - phonon relaxation hypothesis , which does not assume that the system is close to equilibrium at any time during its evolve .We see how this methodology can be used for extracting information about the phonon spectrum and the density of states of charge carriers from TSL information obtained on various types of substances . The results are compared with those achieved by other methods such as photoluminescence excitation spectroscopy or Raman absorption .In particular we prove that our technique permits one to estimate the energy divide between the conduction band threshold and valence band maximum in semiconductors . This project was supported by Russian Science Foundation award No .14-50-00040.DOI: 10.1063/1.4935190 \nI.INTRODUCTORY REMARK The investigation of luminescence effects has been drawing great popularity over numerous years because it gives valuable info about electronic structure and electronic properties of solids 1 . Thermal stimulation luminescence ( TSL ) , sometimes called as optically stimulated luminescence ( OSL ) , is especially interesting since it allows us to probe the distribution behavior of atoms excited into the conduction band 2 .In past decades there have been numerous attempts to develop conceptual models explaining various parts of luminescence systems 3 , notably heat stimulation luminescence 4 - 8 . However , most of these works were based on the assumption that the process under consideration is usually nearly to equilibrium 9 .As a result they cannot describe correctly some important features detected experimentally 10 . For instance , the form of the TSL flicker circle depends strongly on the kind of material 11 : while in insulators it often features a single peak 12 , in metals it often consists of several peaks 13 .Moreover , even within the same category of substances , e . g . , semiconductor crystals 14 , the quantity of peaks may differ depending on the doping level 15 . These measurements cannot be understood using existing theories 16 .",
        "rewrite_text": "**Title:** Analysis of Thermally Stimulated Luminescence and Conductivity Without Quasiequilibrium Approximation\n\n**Abstract:** This study investigates the temperature-dependent behavior of thermally stimulated luminescence (TSL) glow curves through the lens of the nonstationary electron-phonon relaxation hypothesis, which does not presuppose that the system remains close to equilibrium throughout its evolution. Our approach allows for the extraction of critical information regarding the phonon spectrum and the density of states of charge carriers from TSL data collected across various materials. We compare our findings with results obtained from alternative methodologies, such as photoluminescence excitation spectroscopy and Raman absorption, highlighting the unique insights provided by our technique. Notably, we demonstrate that our method can effectively estimate the energy gap between the conduction band minimum and the valence band maximum in semiconductor materials. This research is supported by the Russian Science Foundation under award No. 14-50-00040. \n\n**I. INTRODUCTORY REMARKS** The exploration of luminescence phenomena has gained significant traction over the years, as it offers valuable insights into the electronic structure and properties of solids. Thermal stimulation luminescence (TSL), also referred to as optically stimulated luminescence (OSL), is particularly intriguing because it enables the investigation of the distribution of atoms excited into the conduction band. Numerous conceptual models have been proposed over the past decades to elucidate various aspects of luminescence systems, particularly in the context of heat stimulation luminescence. However, many of these models rely on the assumption that the processes involved are typically near equilibrium, which limits their ability to accurately describe certain experimentally observed phenomena. For example, the shape of the TSL glow curve is highly dependent on the material type; insulators often exhibit a single peak, while metals may display multiple peaks. Furthermore, even within the same category of materials, such as semiconductor crystals, the number of peaks can vary based on the doping concentration. These discrepancies highlight the inadequacies of existing theories in fully capturing the complexities of TSL behavior.",
        "ori-fast-z-score": -2.3757725695052176,
        "water-fast-z-score": 7.3484692283495345,
        "rewrite-fast-z-score": -1.8593393604027364
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dark Energy and Gravity .\nAbstract:\nThe universe is expanding at an accelerating rate, which has been attributed to  dark energy .  This article reviews the current status of research into dark energy by examining some recent observational results in this area.   The author concludes that there are still many open questions about how best to explain these observations within general relativity theory. In particular, it remains unclear whether or not the observed acceleration can be explained as being due solely to gravitational effects associated with the presence of dark energy. It also appears likely that new physics will need to be introduced if we wish to understand why the expansion of space-time should accelerate rather than decelerate over time. The universe is expanding at an accelerated rate, which has been interpreted as evidence for the existence of  dark energy  (DE). This article examines some recent observational results on DE using supernovae Ia data, cosmic microwave background radiation measurements, baryon acoustic oscillations, galaxy cluster counts, weak lensing surveys, and Hubble parameter determinations.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dark Energy and Gravity . Abstract : The universe is growing at an accelerating rate , which has been attributed to dark energy .This page reviews the recent status of research into dark energy by examining some latest observational results in this area . The author concludes that there are still many open questions about how best to explain these observations within general relativity theory .In particular , it remains unsure whether or not the seen acceleration can be understood as being owing purely to gravitational influences associated with the presence of dark energy . It additionally seems likely that new science will need to be introduced if we wish to explain why the evolution of space - time should accelerate rather than decelerate over time .The universe is growing at an accelerated pace , which has been viewed as proof for the existence of dark energy ( DE ) . This page discusses some latest observational results on DE utilizing supernovae Ia evidence , cosmic microwave background radiation observations , baryon acoustic oscillations , galaxy cluster counts , soft lensing surveys , and Hubble parameter determinations .",
        "rewrite_text": "Title: Dark Energy and Gravity\n\nAbstract: The accelerating expansion of the universe has been a pivotal observation in cosmology, leading to the hypothesis of dark energy (DE) as a driving force behind this phenomenon. This article provides a comprehensive review of the current state of research on dark energy, focusing on the latest observational findings that contribute to our understanding of this enigmatic component of the cosmos. Key evidence is drawn from various sources, including Type Ia supernovae, cosmic microwave background radiation measurements, baryon acoustic oscillations, galaxy cluster surveys, weak gravitational lensing studies, and determinations of the Hubble parameter. Despite the wealth of data, significant questions remain regarding the interpretation of these observations within the framework of general relativity. The author emphasizes the uncertainty surrounding whether the observed acceleration can be solely attributed to gravitational effects linked to dark energy or if alternative explanations are necessary. Furthermore, the article suggests that a deeper understanding of the underlying physics may require the introduction of new scientific principles to account for the observed acceleration of space-time, as opposed to a decelerating trend expected from conventional gravitational theories. As research progresses, it is crucial to explore these unresolved issues to enhance our comprehension of the universe's expansion and the role of dark energy within it. This review aims to stimulate further inquiry into the nature of dark energy and its implications for the future of cosmological studies.",
        "ori-fast-z-score": -1.0425720702853738,
        "water-fast-z-score": 5.288453643125169,
        "rewrite-fast-z-score": -1.0169503597462533
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Cool Stars in Hot Places .\nAbstract:\nThe Sun is the nearest star to Earth, and its activity has been studied for thousands of years.  The Sun s magnetic field plays an important role in solar activity.   In this talk I will discuss how we can use observations made by spacecraft such as SOHO (Solar and Heliospheric Observatory) and SDO (Solar Dynamics Observatory), together with ground-based telescopes on the Canary Islands, to study the Sun s magnetic fields and their relationship to solar activity. This research helps us understand what happens when stars like our Sun die out - they become red giants that eject huge amounts of material into space which may eventually form new planets or even life elsewhere in the Universe. Keywords: Solar Activity, Red Giant Star, Magnetic Field, Space Weather. Title: Cool Stars in Hot Places. Abstract: The Sun is the nearest star  to Earth, and its  activity has been studied for thousand of years. The Sun s magnetic field play an important role in solar activities. In this talk I will discus how we can use observation made by spacecrafts such as SOHO(Solar and Heliospherical Observatory)  and SDO(Solar Dynamics Observatory),  together with ground based telescope on the Canary Island,  to study the Sun s magentic fields and their relation to solar activity. This reseach help us understant what happen when stars like our sun dies out- they becom red giant stars that ejecta huge amount of materiel into space whic may eventually form new planet or even life elsewere in universe.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Cool Stars in Hot Places . Abstract : The Sun is the nearest star to Earth , and its activity has been studied for thousands of years .The Sun s magnetic force plays an important role in solar activity . In this talk I will explore how we can using observations made by satellites such as SOHO ( Solar and Heliospheric Observatory ) and SDO ( Solar Dynamics Observatory ) , combined with ground - based telescopes on the Canary Islands , to study the Sun s magnetic fields and their connection to solar activity .This research allows us explain what comes when stars like our Sun die out - they become red giants that eject huge amounts of material into space which would eventually form new planets or even life abroad in the Universe . Keywords : Solar Activity , Red Giant Star , Magnetic Field , Space Weather .Title : Cool Stars in Hot Places . Abstract : The Sun is the nearest star to Earth , and its activity has been studied for thousand of years .The Sun s magnetic force serve an important role in solar action . In this talk I will discus how we can using observation made by spacecrafts such as SOHO ( Solar and Heliospherical Observatory ) and SDO ( Solar Dynamics Observatory ) , combined with ground based observatory on the Canary Island , to study the Sun s magentic fields and their connection to solar activity .This reseach help us understant what happen when stars like our sunlight kills out - they becom red massive planets that ejecta huge amount of materiel into space whic may eventually form new planet or even life elsewere in universe .",
        "rewrite_text": "**Title: Cool Stars in Hot Places**\n\n**Abstract:** The Sun, being the closest star to Earth, has been the subject of extensive study for millennia. Its magnetic field plays a crucial role in influencing solar activity. In this presentation, I will discuss how we can leverage observations from satellites such as the Solar and Heliospheric Observatory (SOHO) and the Solar Dynamics Observatory (SDO), in conjunction with ground-based telescopes located in the Canary Islands, to investigate the Sun's magnetic fields and their relationship to solar phenomena. This research enhances our understanding of the life cycle of stars similar to our Sun, particularly what occurs as they reach the end of their life span. As these stars evolve, they transform into red giants, expelling vast amounts of material into space. This ejected material has the potential to contribute to the formation of new planetary systems and may even harbor the building blocks for life elsewhere in the universe. \n\n**Keywords:** Solar Activity, Red Giant Stars, Magnetic Fields, Space Weather.",
        "ori-fast-z-score": 1.4779939172464398,
        "water-fast-z-score": 6.343350474165466,
        "rewrite-fast-z-score": -0.4923659639173309
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Modeling the Spectral Energy Distribution and Variability of 3C 66A during the WEBT campaign of 2003 -- 2004 .\nAbstract:\nWe present results on modeling the spectral energy distribution (SED) and variability of the blazar 3C 66A observed by the Whole Earth Blazar Telescope (WEBT). The SED is modeled with an accretion disk, synchrotron emission from relativistic electrons in jets, inverse Compton scattering off soft photons produced within the jet or external to it, and thermal radiation from dusty clouds surrounding the central engine. We find that the model can reproduce most features of the SED: the optical-UV spectrum; the X-ray spectrum; the infrared-optical flux ratio; and the radio-to-optical spectral index. However, we cannot explain why there are two peaks in the UV spectrum at ~1400 Å and ~1900 Å simultaneously. In addition, our model fails to account for the rapid flares detected in the light curve of this source. This suggests that either additional components should be included into the model and/or some physical processes have been neglected. \n \n Keywords: blazars, 3C 66A",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Modeling the Spectral Energy Distribution and Variability of 3C 66A during the WEBT effort of 2003 - - 2004 . Abstract : We report findings on studying the spectral power distribution ( SED ) and variability of the blazar 3C 66A observed by the Whole Earth Blazar Telescope ( WEBT ) .The SED is modeled with an accretion disk , synchrotron emission from relativistic electrons in jets , inverse Compton absorption off hard photons created within the jet or external to it , and thermal rays from damp clouds surrounding the main motor . We see that the model can reproduce most elements of the SED : the optical - UV spectrum ; the X - ray spectrum ; the infrared - optical flux ratio ; and the radio - to - optical spectral index .However , we cannot explain why there are two peaks in the UV spectrum at ~ 1400 Å and ~ 1900 Å simultaneously . In addition , our model refuses to explain for the quick flares detected in the light curve of this source .This implies that either additional components should be included into the model and / or some physical processes have been neglected . Keywords : blazars , 3C 66A",
        "rewrite_text": "In this study, we present our findings on the spectral energy distribution (SED) and variability of the blazar 3C 66A, as observed during the Whole Earth Blazar Telescope (WEBT) campaign from 2003 to 2004. Our analysis involves modeling the SED through various components, including contributions from an accretion disk, synchrotron radiation emitted by relativistic electrons in jets, and inverse Compton scattering involving hard photons generated either within the jet or from external sources. Additionally, we consider thermal emissions from surrounding damp clouds that encase the central engine. Our modeling efforts successfully replicate several key features of the SED, including the optical-UV spectrum, X-ray spectrum, infrared-optical flux ratio, and the radio-to-optical spectral index. However, we encounter challenges in accounting for the presence of two distinct peaks in the UV spectrum at approximately 1400 Å and 1900 Å, which occur simultaneously. Furthermore, our model falls short in explaining the rapid flares observed in the light curve of 3C 66A. These discrepancies suggest that our current model may need to incorporate additional components or that certain physical processes may have been overlooked in our analysis. This research highlights the complexities involved in modeling blazar emissions and underscores the need for further investigation into the mechanisms driving the observed variability and spectral features of 3C 66A. Keywords: blazars, 3C 66A.",
        "ori-fast-z-score": -0.7071067811865476,
        "water-fast-z-score": 4.184914994777494,
        "rewrite-fast-z-score": 1.0838622059807226
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Systematic corrections to the measured cosmological constant as a result of local inhomogeneity .\nAbstract:\nWe show that the observed value of the cosmological constant is systematically affected by the presence of matter around us, and we calculate this effect for different models of dark energy.  We find that the correction can be significant (up to 30%) if one assumes that the universe has an average density equal to the critical density today. The correction becomes smaller when the universe is less dense than the critical density or more negative pressure is assumed. This systematic error may explain why the current measurements are not consistent with each other within their errors. In addition, it could also affect future experiments aimed at measuring the time evolution of the cosmological constant. Introduction -The recent discovery of accelerated expansion of our Universe  1  , which implies existence of some form of  dark energy   2  , leads naturally to the question about its nature  3  . One possibility is that the dark energy is simply vacuum energy  4  . Another option is that the dark energy consists of new particles such as axions  5  .\nIn order to test these ideas observationally, precise measurement of the properties of dark energy is needed  6  . However, there are several difficulties associated with this task  7, 8  . For example, even though the present-day acceleration of the universe is very small compared to the Hubble parameter H 0 = 100h km/s/Mpc  9  , where h ≈ 0.7 ± 0.1  10  , the corresponding change in the scale factor over the age of the universe is large enough so that the effects on the distance-redshift relation cannot be neglected  11  . Moreover, since the dark energy affects both space and time  12  , it changes the rate of clocks  13  and thus introduces additional uncertainty into the determination of distances  14  . Finally, the fact that the dark energy evolves with time  15  makes the problem much harder  16  .\nOne way to overcome these problems is to measure directly the equation-of-state w(z), defined as  17  \nwhere p de and ρ de are respectively the pressure and density of the dark energy. If the dark energy behaves like a perfect fluid then w(z) ≡ −1  18  . It was shown recently  19  that the most",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Systematic corrections to the measured cosmological coefficient as a outcome of local inhomogeneity . Abstract : We see that the observed value of the cosmological coefficient is systematically impacted by the presence of matter around us , and we estimate this effect for different models of dark energy .We see that the correction can be considerable ( up to 30 % ) if one assumes that the universe has an average density equal to the critical density today . The correction gets smaller when the universe is fewer dense than the critical density or more negative pressure is assumed .This systematic error may understand why the present measurements are not consistent with each other within their errors . In addition , it could also affect later research targeted at determining the period evolution of the cosmological constant .Introduction - The recent discovery of rapid expansion of our Universe 1 , which implies existence of some kind of dark energy 2 , leads naturally to the question about its nature 3 . One possibility is that the dark energy is simply vacuum energy 4 .Another option is that the dark energy contains of new objects such as axions 5 . In order to test these ideas observationally , detailed measurement of the properties of dark energy is required 6 .However , there are several difficulties involved with this task 7 , 8 . For instance , even though the present - day acceleration of the universe is very small compared to the Hubble parameter H 0 = 100h km / s / Mpc 9 , where h ≈ 0 . 7 ± 0 . 1 10 , the associated change in the scale factor over the age of the universe is huge enough so that the effects on the distance - redshift function cannot be forgotten 11 .Moreover , since the dark energy influences both space and period 12 , it changes the frequency of clocks 13 and therefore brings additional uncertainty into the determination of distances 14 . Finally , the fact that the dark energy evolves with time 15 turns the issue lot harder 16 .One method to overcome these problems is to measure directly the equation - of - state w ( z ) , defined as 17 where p de and ρ de are respectively the pressure and density of the dark energy . If the dark energy behaves like a perfect fluid then w ( z ) ≡ −1 18 .It was shown recently 19 that the most",
        "rewrite_text": "**Title:** Systematic Corrections to the Measured Cosmological Coefficient Due to Local Inhomogeneity\n\n**Abstract:** This study investigates the influence of local matter distributions on the observed value of the cosmological coefficient, revealing that these inhomogeneities can lead to significant systematic corrections. We analyze various models of dark energy to quantify this effect, finding that the corrections can be substantial—up to 30%—when assuming an average cosmic density equivalent to the critical density at the present epoch. Conversely, the magnitude of the correction diminishes in scenarios where the universe's density is lower than the critical threshold or when a more negative pressure is postulated for dark energy. This systematic bias may elucidate the discrepancies observed in current measurements of the cosmological coefficient, which often do not align within their respective uncertainties. Furthermore, these findings could have implications for future investigations aimed at understanding the temporal evolution of the cosmological constant.\n\nThe recent acceleration of the universe's expansion has prompted inquiries into the nature of dark energy, with hypotheses ranging from it being a form of vacuum energy to the existence of novel entities such as axions. To empirically validate these theories, precise measurements of dark energy's properties are essential. However, this task is fraught with challenges. For instance, while the current rate of cosmic acceleration is minimal compared to the Hubble parameter (H₀ ≈ 100h km/s/Mpc, with h ≈ 0.7 ± 0.1), the resultant changes in the scale factor over the universe's lifespan are significant enough to affect the distance-redshift relationship. Additionally, dark energy's influence on both spatial and temporal dimensions alters clock frequencies, introducing further uncertainties in distance measurements. The evolving nature of dark energy over time complicates the situation even more. One potential solution to these challenges is the direct measurement of the equation of state w(z), which characterizes the relationship between pressure (p_de) and density (ρ_de) of dark energy. If dark energy behaves as a perfect fluid, then w(z) would equal -1. Recent studies have demonstrated that...",
        "ori-fast-z-score": 1.8685877318798396,
        "water-fast-z-score": 7.560743029943232,
        "rewrite-fast-z-score": 0.48349377841522817
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The dynamical status of the galaxy cluster Abell 115 .\nAbstract:\nWe present new spectroscopic data for the galaxy cluster Abell115 (z=0.084) obtained with the VLT/FORS2 instrument in order to study its dynamics and mass distribution. We have observed 23 galaxies within an aperture radius of 1 Mpc centered on the brightest cluster member, which is also the central dominant galaxy. The velocity dispersion profile shows no significant variation across the whole region covered by our observations. This result suggests that Abell115 has not experienced any major merger event since z=1.5-2.0. Using Jeans models we find evidence for a dark matter halo extending out to at least 3 times the virial radius. From this analysis we derive a total mass of 2.1(+0.7-0.6)x10^14M_sun inside a sphere of radius R200=850kpc. Finally, using weak lensing measurements we estimate a mass-to-light ratio of about 400h/M_luminosity_Solar_Unit.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The dynamical status of the galaxy cluster Abell 115 . Abstract : We present new spectroscopic data for the galaxy cluster Abell115 ( z = 0 . 084 ) obtained with the VLT / FORS2 instrument in order to study its dynamics and mass distribution .We have noted 23 galaxies within an aperture radius of 1 Mpc centered on the brightest cluster member , which is also the main dominant galaxy . The velocity dispersion feature shows no considerable variation across the whole area covered by our observations .This result suggests that Abell115 has not experienced any big merger event since z = 1 . 5 - 2 . 0 . Using Jeans theories we find proof for a black material halo stretched out to at least 3 times the virial diameter .From this analysis we derive a total mass of 2 . 1 ( + 0 . 7 - 0 . 6 ) x10 ^ 14M _ sun inside a sphere of radius R200 = 850kpc . Finally , using weak lensing observations we estimate a mass - to - light ratio of about 400h / M _ luminosity _ Solar _ Unit .",
        "rewrite_text": "We present a comprehensive analysis of the dynamical status of the galaxy cluster Abell 115 (z = 0.084), utilizing new spectroscopic data acquired with the VLT/FORS2 instrument. Our study focuses on the dynamics and mass distribution of the cluster, which is centered around its brightest member, the dominant galaxy. Within a 1 Mpc aperture radius, we identified 23 galaxies, allowing us to assess the cluster's velocity dispersion. Notably, our findings indicate that the velocity dispersion remains relatively uniform across the observed area, implying that Abell 115 has likely not undergone any significant merger events since redshift z = 1.5 to 2.0. \n\nTo further investigate the mass distribution, we employed Jeans analysis, which provided evidence for a substantial dark matter halo extending to at least three times the virial radius. This analysis led us to estimate a total mass of approximately 2.1 (+0.7 -0.6) x 10^14 M_sun within a sphere defined by R200 = 850 kpc. Additionally, we incorporated weak lensing observations to derive a mass-to-light ratio of around 400 h / M_luminosity_Solar_Unit. Our results contribute to the understanding of the structural and dynamical properties of Abell 115, highlighting its stability and the significant presence of dark matter in its mass composition. This research not only enhances our knowledge of this particular galaxy cluster but also provides insights into the broader context of galaxy cluster dynamics and evolution.",
        "ori-fast-z-score": 0.46499055497527714,
        "water-fast-z-score": 3.628275563080048,
        "rewrite-fast-z-score": 0.8867963503478639
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Role of the rho meson in the description of pion electroproduction experiments at JLab .\nAbstract:\nWe present results on the role played by the rho meson in describing pion electroproduction data obtained with the CLAS detector at Jefferson Lab (JLab). The analysis is performed within an effective field theory approach, where we use chiral perturbation theory to describe the interaction between pions and nucleons up to next-to-leading order. We then introduce vector-meson degrees of freedom through the hidden gauge formalism. In particular, we consider contributions coming from one-loop diagrams involving rho mesons as well as tree-level processes mediated by rho mesons. Our theoretical framework allows us to study both neutral current reactions such as elastic ep scattering or charged current reactions like single-pion production off protons. Using this model, we are able to reproduce experimental data for all these observables simultaneously. Finally, we discuss how our findings can be used to extract information about the properties of the rho meson. This work was supported by the U.S. \n\n\nDepartment of Energy under Contract No. DE-SC0012704.\n\nPACS numbers: 12.38.Mh",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Role of the rho meson in the description of pion electroproduction research at JLab . Abstract : We report findings on the part played by the rho meson in understanding pion electroproduction data received with the CLAS detector at Jefferson Lab ( JLab ) .The investigation is conducted within an efficient field theory approach , where we using chiral perturbation theory to explain the interaction between pions and nucleons up to next - to - leading order . We then introduce vector - meson degrees of liberty through the hidden gauge formalism .In particular , we treat contributions come from one - loop diagrams featuring rho mesons as well as tree - level processes induced by rho mesons . Our conceptual formulation enables us to study both neutral current reactions such as elastic ep scattering or charged current reactions like single - pion production off protons .Using this model , we are able to publish empirical data for all these observables simultaneously . Finally , we explain how our findings can be used to extract information about the properties of the rho meson .This project was supported by the U . S . Department of Energy under Contract No . DE - SC0012704 .PACS numbers : 12 . 38 . Mh",
        "rewrite_text": "**Title:** The Role of the Rho Meson in Pion Electroproduction Research at JLab\n\n**Abstract:** This study presents an in-depth analysis of the rho meson's contribution to pion electroproduction, utilizing data collected from the CLAS detector at Jefferson Lab (JLab). Our research employs an effective field theory framework, specifically leveraging chiral perturbation theory to elucidate the interactions between pions and nucleons up to next-to-leading order. To enhance our understanding, we incorporate vector-meson degrees of freedom through the hidden gauge formalism. This approach allows us to systematically account for contributions arising from one-loop diagrams involving rho mesons, as well as tree-level processes induced by these mesons. Our theoretical framework facilitates the examination of both neutral current reactions, such as elastic electron-proton (ep) scattering, and charged current reactions, exemplified by single-pion production from protons. By applying this model, we successfully analyze and present empirical data for a range of observables concurrently. Furthermore, we discuss how our results can be utilized to extract valuable insights regarding the properties of the rho meson. This research was conducted with the support of the U.S. Department of Energy under Contract No. DE-SC0012704. The findings contribute significantly to the understanding of meson dynamics in the context of pion electroproduction and provide a foundation for future investigations in this area. PACS numbers: 12.38.Mh.",
        "ori-fast-z-score": -1.3587324409735149,
        "water-fast-z-score": 3.9378076532410837,
        "rewrite-fast-z-score": -0.953998092005724
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Poincare Duality Pairs in Dimension Three .\nAbstract:\nWe prove that the Poincare duality pairs are exactly those which can be obtained by gluing together two copies of the same surface along their boundaries, and we give an explicit description for all such surfaces.  We also show how to construct these surfaces using only elementary operations on polygons with holes. The Poincare conjecture states that any closed orientable 3-manifold is homeomorphic to either S 3 , R 3 or one of the lens spaces L(p,q) where p and q are relatively prime integers greater than 1. In this note we study another class of manifolds called Poincare duality pairs: These are simply connected compact oriented 3-manifolds M whose intersection form Q satisfies Q = −Q t . It follows immediately from the definition that if M is a Poincare duality manifold then it has signature zero (i.e., its second Stiefel-Whitney class vanishes).",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Poincare Duality Pairs in Dimension Three . Abstract : We prove that the Poincare duality pairs are exactly those which can be obtained by gluing together two copy of the same surface along their boundaries , and we give an explicit description for all such surfaces .We additionally prove how to build these surfaces using only basic operations on polygons with holes . The Poincare hypothesis states that any closed orientable 3 - manifold is homeomorphic to either S 3 , R 3 or one of the lens spaces L ( p , q ) where p and q are fairly prime numbers greater than 1 .In this note we study another class of manifolds called Poincare duality pairs : These are simply connected compact oriented 3 - manifolds M whose junction form Q satisfies Q = −Q t . It follows quickly from the notion that if M is a Poincare duality manifold then it has signature zero ( i . e . , its second Stiefel - Whitney class vanishes ) .",
        "rewrite_text": "In this article, titled \"Poincaré Duality Pairs in Dimension Three,\" we establish that the Poincaré duality pairs correspond precisely to those manifolds that can be constructed by gluing two identical surfaces along their boundaries. We provide a detailed characterization of these surfaces and demonstrate a method for their construction using fundamental operations on polygons with holes. The Poincaré conjecture posits that any closed orientable 3-manifold is homeomorphic to either the 3-sphere (S^3), Euclidean 3-space (R^3), or one of the lens spaces L(p, q), where p and q are coprime integers greater than 1. In this study, we focus on a specific category of manifolds known as Poincaré duality pairs. These are defined as simply connected, compact, oriented 3-manifolds M, for which the junction form Q satisfies the condition Q = -Q^t. This property leads to the conclusion that if M is a Poincaré duality manifold, it must possess a signature of zero, indicating that its second Stiefel-Whitney class is trivial. Our findings contribute to the understanding of the structure and classification of 3-manifolds, particularly in relation to Poincaré duality, and offer insights into the geometric and topological properties that characterize these intriguing mathematical objects.",
        "ori-fast-z-score": -1.1523319193960637,
        "water-fast-z-score": 2.1766269588592317,
        "rewrite-fast-z-score": 0.3464101615137754
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Dynamical State fo the Starless Dense Core FeSt 1-457: A Pulsating Globule? .\nAbstract:\nWe present new near-infrared (NIR) and millimeter-wave observations of the starless dense core FeSt 1-457, which is located in the Taurus molecular cloud complex at a distance of 140 pc. The NIR data were obtained with the Subaru telescope using the SofI instrument on 2005 May 24-25 UT. We detected two sources within the central 0.5 arcmin region; one source was found to be associated with an infrared dark cloud (IRDC), while another source was not. Both sources are embedded deeply inside the dusty envelope surrounding the dense core. In addition, we observed this object simultaneously with the Nobeyama 45 m radio telescope at 1 mm wavelength during the same night as our NIR observation. No significant emission line features were seen in either spectrum. Using these observational results, we discuss possible scenarios for the formation of stars in such a young dense core.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  The Dynamical State fo the Starless Dense Core FeSt 1-457: A Pulsating Globule?.Abstract : We report new near - infrared ( NIR ) and millimeter - wave images of the starless dense core FeSt 1 - 457 , which is situated in the Taurus molecular mist complex at a distance of 140 pc . The NIR data were obtained with the Subaru observatory using the SofI instrument on 2005 May 24 - 25 UT .We observed two sources within the inner 0 . 5 arcmin region ; one source was reported to be involved with an infrared shadow cloud ( IRDC ) , while another source was not . Both sources are lodged deeply inside the dusty envelope surrounding the dense core .In addition , we studied this body simultaneously with the Nobeyama 45 m radio telescope at 1 mm frequency during the same night as our NIR observation . No notable emission line characteristics were witnessed in either spectrum .Using these observational results , we explain possible strategies for the formation of stars in such a young dense core .",
        "rewrite_text": "Title: The Dynamical State of the Starless Dense Core FeSt 1-457: A Pulsating Globule?\n\nAbstract: In this study, we present new near-infrared (NIR) and millimeter-wave imaging of the starless dense core FeSt 1-457, located within the Taurus molecular cloud complex at a distance of 140 parsecs. The NIR observations were conducted at the Subaru Observatory using the SofI instrument over the nights of May 24-25, 2005. Our imaging revealed two distinct sources within a 0.5 arcminute radius of the core; one of these sources is associated with an infrared dark cloud (IRDC), while the other is not. Both sources are embedded deep within the dense core's dusty envelope, suggesting a complex interplay of materials in this region. \n\nSimultaneously, we utilized the Nobeyama 45 m radio telescope to observe the core at a frequency of 1 mm during the same observational period. Notably, our spectral analysis did not reveal any significant emission line features, indicating a lack of active star formation or dynamic processes typically associated with such regions. \n\nThe findings from our NIR and millimeter-wave observations provide critical insights into the physical conditions and potential evolutionary pathways of star formation in this young dense core. We discuss various mechanisms that may facilitate star formation in environments characterized by such dense and cold conditions. Our results contribute to the broader understanding of starless cores and their role in the star formation process, particularly in the context of the Taurus molecular cloud complex. This research underscores the importance of multi-wavelength observations in unraveling the complexities of stellar birth in dense interstellar environments.",
        "ori-fast-z-score": -1.212678125181665,
        "water-fast-z-score": 3.8247315498700596,
        "rewrite-fast-z-score": 0.09325048082403138
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Detailed kinetic study of the ring opening of cycloalkanes by CBS-QB3 calculations .\nAbstract:\nThe reaction mechanism for the ring-opening reactions of cyclic alkanes with water has been studied using density functional theory (DFT) and ab initio molecular orbital methods at the B3LYP/6-311++G(d,p), MP2/6-31+G*, QCISD/6-31+G* levels in conjunction with the CBS-QB3 method to calculate enthalpies of formation.  The results show that the activation energies are lower than those obtained previously by DFT or semiempirical methods. In addition, it is found that the transition states have one imaginary frequency along the reaction coordinate corresponding to the breaking of C-H bonds on both sides of the carbonyl group. The calculated rate constants agree well with experimental data over a wide temperature range. Finally, we propose an explanation for the observed differences between theoretical predictions and experiments based on the fact that the solvent effect was not taken into account in previous studies.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Detailed kinetic study of the ring opening of cycloalkanes by CBS - QB3 calculations . Abstract : The reaction mechanism for the ring - entrance processes of cyclic alkanes with water has been studied utilizing density functional theory ( DFT ) and ab initio molecular orbital methods at the B3LYP / 6 - 311 + + G ( d , p ) , MP2 / 6 - 31 + G * , QCISD / 6 - 31 + G * levels in partnership with the CBS - QB3 technique to estimate enthalpies of formation .The results show that the activation energies are lower than those achieved formerly by DFT or semiempirical methods . In addition , it is found that the transfer states have one imaginary frequency along the reaction coordinate relating to the breaking of C - H bonds on both sides of the carbonyl group .The measured rate constants accord well with experimental evidence over a broad temperature spectrum . Finally , we propose an excuse for the known differences between theoretical estimates and experiments based on the fact that the solvent influence was not took into consideration in earlier studies .",
        "rewrite_text": "Title: Comprehensive Kinetic Analysis of Cycloalkane Ring Opening via CBS-QB3 Calculations\n\nAbstract: This study presents an in-depth investigation into the reaction mechanisms governing the ring-opening processes of cyclic alkanes in the presence of water, employing advanced computational techniques including density functional theory (DFT) and ab initio molecular orbital methods. Specifically, calculations were performed at the B3LYP/6-311++G(d,p), MP2/6-31+G*, and QCISD/6-31+G* levels, complemented by the CBS-QB3 method to accurately estimate enthalpies of formation. Our findings reveal that the activation energies for these reactions are significantly lower than those reported in previous studies utilizing DFT or semiempirical approaches. Notably, the transition states identified exhibit one imaginary frequency along the reaction coordinate, which corresponds to the cleavage of C-H bonds adjacent to the carbonyl group. The rate constants derived from our calculations align closely with experimental data across a wide temperature range, reinforcing the reliability of our computational methods. Furthermore, we address the discrepancies observed between theoretical predictions and experimental results in earlier research, attributing these differences to the lack of consideration for solvent effects in those studies. This work not only enhances the understanding of the kinetic behavior of cycloalkane ring-opening reactions but also underscores the importance of incorporating solvent interactions in theoretical models to achieve more accurate predictions.",
        "ori-fast-z-score": -1.585187847802434,
        "water-fast-z-score": 3.487772492870674,
        "rewrite-fast-z-score": -2.288585537482975
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  On the Equilibrium Fluctuations of an Isolated System .\nAbstract:\nThe equilibrium fluctuations in isolated systems are studied by using statistical mechanics and information theory. The entropy production rate is calculated for different types of isolated systems: (i) closed, (ii) open, and (iii) dissipative. It is shown that the entropy production rate can be expressed as a sum over all possible transitions between states weighted with their respective transition probabilities. For closed systems it is found to be equal to zero at thermal equilibrium. This result agrees well with previous results obtained within linear response theory. In contrast, for open or dissipative systems we find that the entropy production rate does not vanish even if they reach thermal equilibrium. We show how this finding leads to a new interpretation of the second law of thermodynamics. Finally, we discuss our findings in terms of fluctuation theorems. PACS: 05.45.Mt, 02.10.Yn, 03.65.Ta Keywords: Entropy Production Rate, Second Law of Thermodynamics",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : On the Equilibrium Fluctuations of an Isolated System . Abstract : The equilibrium fluctuations in isolated systems are studied by using statistical mechanics and information theory .The entropy production rate is calculated for different kinds of isolated systems : ( i ) closed , ( ii ) open , and ( iii ) dissipative . It is demonstrated that the entropy production frequency can be written as a sum over all possible transitions between states weighted with their respective change probabilities .For closed systems it is found to be equal to zero at heat equilibrium . This result agrees well with previous findings obtained within linear behavior physics .In comparison , for open or dissipative models we find that the entropy production level does not vanish even if they reach thermal equilibrium . We see how this finding brings to a new understanding of the second law of thermodynamics .Finally , we explain our findings in terms of fluctuation theorems . PACS : 05 . 45 . Mt , 02 . 10 . Yn , 03 . 65 . Ta Keywords : Entropy Production Rate , Second Law of Thermodynamics",
        "rewrite_text": "Title: On the Equilibrium Fluctuations of an Isolated System\n\nAbstract: This article investigates the equilibrium fluctuations present in isolated systems through the lenses of statistical mechanics and information theory. The study focuses on calculating the entropy production rate across various types of isolated systems, specifically categorized as (i) closed systems, (ii) open systems, and (iii) dissipative systems. A key finding is that the entropy production frequency can be expressed as a summation of all potential transitions between states, each weighted by their corresponding change probabilities. For closed systems, the analysis reveals that the entropy production rate equals zero when the system is at thermal equilibrium, a result that aligns with prior research conducted within the framework of linear behavior physics. In contrast, the investigation of open and dissipative systems shows that the entropy production does not diminish to zero, even when these systems achieve thermal equilibrium. This observation prompts a reevaluation of the second law of thermodynamics, suggesting a more nuanced understanding of entropy dynamics in non-closed systems. Furthermore, the implications of these findings are discussed in relation to fluctuation theorems, which provide a theoretical foundation for understanding the behavior of entropy in fluctuating systems. Overall, this research contributes to a deeper comprehension of equilibrium fluctuations and their significance in thermodynamic processes, highlighting the complexities inherent in different types of isolated systems. \n\nPACS: 05.45.Mt, 02.10.Yn, 03.65.Ta  \nKeywords: Entropy Production Rate, Second Law of Thermodynamics",
        "ori-fast-z-score": -0.9428090415820635,
        "water-fast-z-score": 3.719924439802217,
        "rewrite-fast-z-score": 0.10050378152592121
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  SN 2005hj: Evidence for Two Classes of Normal-Bright SNe Ia and Implications for Cosmology .\nAbstract:\nWe present the discovery, photometric and spectroscopic observations of SN 2005hj (PTF10ygu), an apparently normal-bright Type Ia supernova discovered by PTF in late March 2005 at z = 0.084. The light curve shows two distinct peaks separated by about one month with no evidence for interaction between ejecta and circumstellar material. We find that this object is consistent with being a member of the class of  normal-bright  SNe Ia defined by Phillips et al. (1999) but has a higher peak luminosity than most members of this class. Using our own data as well as published results we estimate the distance to SN 2005hj using three different methods. All three give distances which are inconsistent with each other within their uncertainties. This may be due to systematic errors or it could indicate that there exists more than one subclass of  normal-bright  objects. If confirmed, these findings have important implications for cosmological studies based on SNe Ia. \n \n Keywords: Supernovae",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : SN 2005hj : Evidence for Two Classes of Normal - Bright SNe Ia and Implications for Cosmology . Abstract : We report the discovery , photometric and spectroscopic observations of SN 2005hj ( PTF10ygu ) , an obviously normal - faint Type Ia supernova discovered by PTF in late March 2005 at z = 0 . 084 .The light curve shows two separate peaks linked by about one month with no evidence for interaction between ejecta and circumstellar material . We see that this body is compatible with being a member of the class of normal - faint SNe Ia defined by Phillips et al .( 1999 ) but has a higher peak luminosity than most members of this class . Using our own data as well as published results we estimate the distance to SN 2005hj using three different methods .All three give distances which are inconsistent with each other within their uncertainties . This might be due to systematic errors or it could indicate that there exists more than one subclass of normal - faint objects .If confirmed , these results have important implications for cosmological experiments using on SNe Ia . Keywords : Supernovae",
        "rewrite_text": "We present our findings on SN 2005hj (PTF10ygu), a Type Ia supernova identified by the Palomar Transient Factory (PTF) in late March 2005 at a redshift of z = 0.084. Our study includes detailed photometric and spectroscopic observations that reveal the supernova's light curve, which exhibits two distinct peaks separated by approximately one month. Notably, there is no indication of interaction between the ejecta and any surrounding circumstellar material. Our analysis suggests that SN 2005hj aligns with the category of normal-faint Type Ia supernovae as characterized by Phillips et al. (1999); however, it displays a peak luminosity that exceeds that of most other members within this classification. To estimate the distance to SN 2005hj, we employed three independent methodologies, each yielding results that are inconsistent with one another within their respective uncertainties. This discrepancy may stem from systematic errors in our measurements or could imply the existence of multiple subclasses within the normal-faint supernova category. If this hypothesis is validated, it could have significant ramifications for cosmological studies that rely on Type Ia supernovae as standard candles. Our findings underscore the complexity of supernova classifications and their implications for understanding cosmic expansion. This research contributes to the broader discourse on the role of Type Ia supernovae in cosmology and highlights the necessity for further investigation into the characteristics and classifications of these celestial phenomena. \n\nKeywords: Supernovae, Type Ia supernovae, cosmology, photometric observations, spectroscopic observations.",
        "ori-fast-z-score": 1.0834726777719228,
        "water-fast-z-score": 4.865804798594798,
        "rewrite-fast-z-score": 1.1043152607484654
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Super stellar clusters with a bimodal hydrodynamic solution: an Approximate Analytic Approach .\nAbstract:\nWe present the results of our study on super star clusters (SSCs) in which we have found that SSCs can be divided into two categories, namely, those having a single mode and those having a double-mode solution for their density profiles. We show how these solutions are related to each other by using approximate analytic methods. The main aim is to understand why some SSCs appear as point sources while others do not. In this work, we also discuss the possibility of formation of such objects through mergers between smaller clusters or stars. Super massive star clusters (SMCs), known as young globular clusters (YGCs), open clusters (OCs), compact elliptical galaxies (CEGs), etc., are observed in many galactic systems ranging from dwarf irregular galaxies to giant ellipticals. These objects are believed to form during violent events like galaxy mergers, tidal interactions, and/or gas-rich major mergers. However, it has been shown recently that there exists another class of SMCs whose luminosity function shows a peak at intermediate masses (10^6-10^7 Msun). This type of cluster is referred to as  Intermediate Massive Clusters (IMCs; Portegies Zwart et al. (2010)). It appears that IMCs may represent a transition phase between open clusters and YGCs.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Super stellar regions with a bimodal hydrodynamic solution : an Approximate Analytic Approach . Abstract : We present the results of our research on super galaxy clusters ( SSCs ) in which we have discovered that SSCs can be grouped into two genres , namely , those having a single mode and those having a double - mode solution for their density profiles .We see how these solutions are related to each other by using approximate analytic techniques . The main aim is to explain why some SSCs appear as point sources while many do not .In this research , we also discuss the idea of formation of such objects through mergers between smaller clusters or stars . Super massive star clusters ( SMCs ) , known as young globular galaxies ( YGCs ) , close complexes ( OCs ) , compact elliptical galaxies ( CEGs ) , etc . , are observed in large galactic structures ranging from giant irregular clusters to massive ellipticals .These bodies are known to form during violent reactions like galaxy mergers , tidal interactions , and / or gas - rich great mergers . However , it has been shown lately that there exists another class of SMCs whose luminosity function shows a peak at intermediate masses ( 10 ^ 6 - 10 ^ 7 Msun ) .This kind of cluster is referred to as Intermediate Massive Clusters ( IMCs ; Portegies Zwart et al . ( 2010 ) ) .It likely that IMCs might represent a change process between open complexes and YGCs .",
        "rewrite_text": "We present our findings on super galaxy clusters (SSCs) in this study, revealing a classification system that distinguishes between two primary types: those exhibiting a single-mode density profile and those characterized by a bimodal solution. By employing approximate analytic methods, we explore the interrelationship between these two solutions, aiming to elucidate the reasons behind the observed differences in the appearance of SSCs, particularly why some manifest as point sources while others do not. Our research also delves into the formation mechanisms of these structures, proposing that they may arise from the mergers of smaller clusters or stellar systems. \n\nSuper massive star clusters (SMCs), often referred to as young globular galaxies (YGCs), open clusters (OCs), and compact elliptical galaxies (CEGs), are prevalent within extensive galactic formations, which range from giant irregular clusters to massive elliptical galaxies. These entities typically form during dynamic events such as galaxy mergers, tidal interactions, and gas-rich mergers. Recent studies have identified a distinct category of SMCs whose luminosity functions peak at intermediate masses, specifically in the range of \\(10^6 - 10^7 M_{\\odot}\\). This category is termed Intermediate Massive Clusters (IMCs), as described by Portegies Zwart et al. (2010). We hypothesize that IMCs may represent a transitional phase between open clusters and young globular galaxies, suggesting a complex evolutionary pathway for these stellar structures. Our work contributes to a deeper understanding of the dynamics and formation processes of SSCs, providing insights into the nature of these fascinating astronomical phenomena.",
        "ori-fast-z-score": -1.8,
        "water-fast-z-score": 4.477667355944951,
        "rewrite-fast-z-score": -0.552344770738994
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  In-plane structure and ordering at liquid sodium surfaces and interfaces from ab initio molecular dynamics .\nAbstract:\nWe have performed ab initio molecular dynamics simulations to study the in-plane structure, order parameters, and surface tension of liquid Na(l) in contact with vacuum or solid NaCl (001). We find that the density profile is strongly dependent on the presence of an underlying substrate; it exhibits a pronounced double peak for the case without substrate but becomes single-peaked when the substrate is present. The height fluctuations are found to be larger than those observed experimentally by STM measurements. This discrepancy may arise due to the fact that our simulation cell contains only one layer of liquid sodium atoms while experiments typically involve several layers. In addition, we observe that the average nearest neighbor distance decreases as the number of layers increases. Our results show that the in-plane structure of liquid sodium can be significantly influenced by its environment. Finally, we calculate the surface tensions using two different methods and compare them against each other.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : In - plane structure and ordering at liquid sodium surfaces and interfaces from ab initio molecular dynamics . Abstract : We have done ab initio molecular dynamics simulations to study the in - plane structure , order variables , and surface tension of liquid Na ( l ) in contact with vacuum or solid NaCl ( 001 ) .We see that the density profile is strongly dependent on the presence of an underlying substrate ; it displays a distinct double peak for the case without substrate but grows single - topped when the substrate is present . The depth fluctuations are found to be larger than those observed experimentally by STM observations .This discrepancy may arise due to the fact that our modeling cell contains only one layer of liquid sodium atoms while tests usually require many layers . In addition , we find that the average closest neighbor distance decreases as the quantity of layers increases .Our results show that the in - plane structure of liquid sodium can be substantially affected by its climate . Finally , we estimate the surface tensions use two different methods and contrast them against each other .",
        "rewrite_text": "Title: In-Plane Structure and Ordering at Liquid Sodium Surfaces and Interfaces from Ab Initio Molecular Dynamics\n\nAbstract: This study employs ab initio molecular dynamics simulations to investigate the in-plane structural characteristics, order parameters, and surface tension of liquid sodium (Na) in contact with both vacuum and solid sodium chloride (NaCl) (001) surfaces. Our findings reveal a significant dependence of the density profile on the presence of an underlying substrate. Specifically, in the absence of a substrate, the density profile exhibits a pronounced double peak, while the presence of a substrate leads to a single-peaked density profile. Additionally, we observe that the depth fluctuations of the liquid sodium surface are greater than those reported in experimental scanning tunneling microscopy (STM) studies. This discrepancy may stem from the limitations of our modeling approach, which considers only a single layer of liquid sodium atoms, whereas experimental observations typically involve multiple layers. Furthermore, our analysis indicates that the average distance to the nearest neighbor decreases with an increase in the number of layers, suggesting that the in-plane structure of liquid sodium is significantly influenced by the surrounding environment. To further understand the surface properties, we estimate the surface tensions of liquid sodium using two distinct methodologies and compare the results. This comprehensive investigation enhances our understanding of the structural and thermodynamic properties of liquid sodium at surfaces and interfaces, providing valuable insights for future research in this area.",
        "ori-fast-z-score": 0.20203050891044214,
        "water-fast-z-score": 5.656854249492381,
        "rewrite-fast-z-score": 2.556038601690775
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Diffuse Gamma-Rays Produced in Cosmic-Ray Interactions and the TeV-band Spectrum of RX J1713.7-3946 .\nAbstract:\nWe present new results on diffuse gamma-ray emission produced by cosmic rays interacting with interstellar gas, based on data collected during the first year of operation of the Large Area Telescope (LAT) aboard Fermi satellite. We find that this component is well described by a power law spectrum with index ~2.3 extending up to 100 GeV. The total flux above 1 GeV amounts to about 10% of the observed Galactic diffuse emission at these energies. This result confirms previous estimates obtained using EGRET data. In addition we report an upper limit for the flux of unresolved point sources below 10 GeV which is consistent with predictions made within the framework of standard models of cosmic ray origin and propagation. Finally, we discuss implications of our findings for the interpretation of observations performed towards the supernova remnant RX J1713.7--3946. PACS numbers: 98.70.Sa, 95.55.Ym",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Diffuse Gamma - Rays Produced in Cosmic - Ray Interactions and the TeV - band Spectrum of RX J1713 . 7 - 3946 . Abstract : We report new data on diffuse γ - ray radiation generated by cosmic rays interacting with interstellar gas , based on evidence generated during the first year of operation of the Large Area Telescope ( LAT ) aboard Fermi satellite .We see that this component is well described by a power law spectrum with index ~ 2 . 3 stretching up to 100 GeV . The total flux above 1 GeV corresponds to about 10 % of the seen Galactic diffuse emission at these frequencies .This result confirms previous estimates obtained using EGRET data . In addition we publish an upper maximum for the flux of unresolved point sources below 10 GeV which is compatible with predictions making within the framework of standard models of cosmic ray origin and propagation .Finally , we explain significance of our findings for the interpretation of measurements completed towards the supernova remnant RX J1713 . 7 - - 3946 . PACS numbers : 98 . 70 . Sa , 95 . 55 . Ym",
        "rewrite_text": "Title: Diffuse Gamma-Rays Produced in Cosmic-Ray Interactions and the TeV-band Spectrum of RX J1713.7-3946\n\nAbstract: In this study, we present new findings regarding diffuse gamma-ray radiation resulting from cosmic ray interactions with interstellar gas, utilizing data collected during the inaugural year of the Large Area Telescope (LAT) aboard the Fermi satellite. Our analysis reveals that this diffuse gamma-ray component is accurately characterized by a power law spectrum with an index of approximately 2.3, extending up to energies of 100 GeV. Notably, the total flux observed above 1 GeV accounts for roughly 10% of the Galactic diffuse emission detected at these energy levels. This observation corroborates earlier estimates derived from EGRET data, reinforcing the consistency of our results with prior research. Furthermore, we provide an upper limit on the flux of unresolved point sources below 10 GeV, which aligns with theoretical predictions made within the context of established models concerning the origin and propagation of cosmic rays. The implications of our findings are significant for the interpretation of gamma-ray measurements associated with the supernova remnant RX J1713.7-3946, contributing to a deeper understanding of the processes at play in cosmic ray interactions and their resultant emissions. Our results not only enhance the existing knowledge of diffuse gamma-ray radiation but also offer valuable insights into the broader astrophysical phenomena related to cosmic rays and their interactions with the interstellar medium. The PACS numbers associated with this research are 98.70.Sa and 95.55.Ym, indicating its relevance to high-energy astrophysics and cosmic ray studies.",
        "ori-fast-z-score": -0.11704114719613057,
        "water-fast-z-score": 4.649905549752772,
        "rewrite-fast-z-score": 0.4662524041201569
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Gamma-Rays Produced in Cosmic-Ray Interactions and the TeV-band Spectrum of RX J1713.7-3946 .\nAbstract:\nWe present new results on gamma-ray emission produced by cosmic rays interacting with interstellar gas, based on observations made with the HESS telescope array between 2004 and 2007. We report an improved measurement of the spectrum of the brightest source detected at TeV energies, RX J1713.7--3946 (HESS J1714-385), which is well described by a power law with index = 2.28 ± 0.04 stat ± 0.1 sys . The integral flux above 1 TeV amounts to (2.6 +/- 0.4) x 10-12 cm-2 s-1 , corresponding to about 10% of the total Galactic diffuse emission observed at these energies. This result confirms that this object is indeed a supernova remnant as suggested previously. \n \n In addition we have discovered two new sources within the field-of-view of our instrumentation. One of them has been identified with the shell-type supernova remnant G349.7+0.2 while another one remains unidentified.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Gamma - Rays Produced in Cosmic - Ray Interactions and the TeV - band Spectrum of RX J1713 . 7 - 3946 . Abstract : We report new data on gamma - ray radiation created by cosmic rays interacting with interstellar gas , based on observations made with the HESS telescope array between 2004 and 2007 .We report an better study of the spectrum of the brightest source detected at TeV energies , RX J1713 . 7 - - 3946 ( HESS J1714 - 385 ) , which is well described by a power law with index = 2 . 28 ± 0 . 04 stat ± 0 . 1 sys . The integral flux above 1 TeV amounts to ( 2 . 6 + / - 0 . 4 ) x 10 - 12 cm - 2 s - 1 , equivalent to about 10 % of the total Galactic diffuse emission observed at these energies .This result confirms that this body is indeed a supernova remnant as suggested previously . In addition we have discovered two new sources within the field - of - view of our instrumentation .One of them has been detected with the shell - class supernova remnant G349 . 7 + 0 . 2 while another one remains unidentified .",
        "rewrite_text": "**Title:** Gamma-Rays Produced in Cosmic-Ray Interactions and the TeV-Band Spectrum of RX J1713.7-3946\n\n**Abstract:** In this study, we present new findings on gamma-ray emissions generated by cosmic-ray interactions with interstellar gas, derived from observations conducted using the HESS telescope array from 2004 to 2007. Our analysis focuses on the spectrum of RX J1713.7-3946 (HESS J1714-385), the brightest source detected at TeV energies. The spectral data is well-represented by a power law with an index of 2.28 ± 0.04 (statistical) ± 0.1 (systematic). We calculate the integral flux above 1 TeV to be (2.6 ± 0.4) x 10^-12 cm^-2 s^-1, which corresponds to approximately 10% of the total Galactic diffuse emission observed at these energy levels. This finding reinforces the hypothesis that RX J1713.7-3946 is a supernova remnant, as previously suggested in the literature. Furthermore, our observations have led to the identification of two new sources within the field of view of the HESS instrumentation. One of these sources is associated with the shell-type supernova remnant G349.7+0.2, while the other remains unidentified. These discoveries contribute to our understanding of cosmic-ray interactions and the role of supernova remnants in the production of high-energy gamma rays, highlighting the importance of continued observational efforts in this domain. The implications of these findings extend to the broader context of cosmic-ray physics and the mechanisms underlying gamma-ray emissions in the universe.",
        "ori-fast-z-score": 1.4552137502179978,
        "water-fast-z-score": 4.213504858001922,
        "rewrite-fast-z-score": 1.6750597728792984
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Catalog of Star Cluster Candidates in M33 .\nAbstract:\nWe present the results of an automated search for star cluster candidates (SCCs) using deep HST/ACS images covering most of the disk and halo of M33, obtained as part of program GO-10229. The SCCs were identified by applying the DAOPHOT photometry package to detect sources with high surface brightnesses relative to their local backgrounds. We then applied several selection criteria based on color-magnitude diagrams (CMDs), luminosity functions (LFs), and radial profiles to identify bona fide SCCs among these bright objects. A total of 1,082 candidate clusters are found within our survey area; we estimate that about half of them may be real open clusters or associations. These newly discovered SCCs will provide important targets for future spectroscopic studies aimed at understanding how star formation proceeds in low-metallicity environments such as those found in dwarf galaxies like M33.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Catalog of Star Cluster Candidates in M33 . Abstract : We present the results of an automated search for star cluster applicants ( SCCs ) using deep HST / ACS images covering most of the disk and halo of M33 , obtained as part of program GO - 10229 .The SCCs were discovered by using the DAOPHOT photometry program to identify sources with high surface brightnesses compared to their nearby backgrounds . We then utilized numerous selection standards based on color - magnitude diagrams ( CMDs ) , luminosity functions ( LFs ) , and radial profiles to identify bona fide SCCs among these bright objects .A total of 1 , 082 candidate clusters are found within our survey area ; we estimate that about half of them may be real open clusters or associations . These newly discovered SCCs will provide important targets for future spectroscopic studies aimed at understanding how star formation proceeds in low - metallicity environments such as those found in dwarf galaxies like M33 .",
        "rewrite_text": "In this study, we report on the findings from an extensive automated search for star cluster candidates (SCCs) conducted using high-resolution images from the Hubble Space Telescope (HST) Advanced Camera for Surveys (ACS). This investigation encompasses a significant portion of the disk and halo of the galaxy M33, as part of the observational program GO-10229. The identification of SCCs was achieved through the application of the DAOPHOT photometry software, which allowed us to detect sources exhibiting elevated surface brightness relative to their surrounding backgrounds. To refine our selection of potential star clusters, we employed a variety of criteria based on color-magnitude diagrams (CMDs), luminosity functions (LFs), and radial profiles. Through this rigorous methodology, we successfully identified a total of 1,082 candidate star clusters within the surveyed region. Our analysis suggests that approximately half of these candidates are likely to be genuine open clusters or stellar associations. The discovery of these SCCs is significant, as they represent valuable targets for subsequent spectroscopic investigations. Such studies are crucial for advancing our understanding of star formation processes, particularly in low-metallicity environments characteristic of dwarf galaxies like M33. The catalog of SCCs presented in this paper not only contributes to the existing body of knowledge regarding star cluster formation but also lays the groundwork for future research aimed at unraveling the complexities of stellar evolution in diverse galactic contexts.",
        "ori-fast-z-score": 0.5773502691896257,
        "water-fast-z-score": 3.4412360080584263,
        "rewrite-fast-z-score": -0.2847473987257497
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Measurement of Ultra-Low Potassium Contaminations with Accelerator Mass Spectrometry .\nAbstract:\nThe measurement of ultra-low potassium contaminations in silicon is important for the development and production of semiconductor devices, especially solar cells. The detection limit of conventional methods such as flame photometry or atomic absorption spectroscopy (AAS) is not sufficient to meet the requirements set by industry standards. In this work we present an alternative method based on accelerator mass spectrometry (AMS). We show that AMS can be used to measure potassium concentrations down to 10(-12) at% K in Si samples. This corresponds to a sensitivity improvement by three orders of magnitude compared to standard techniques like flame photometry. Furthermore, our results demonstrate that AMS has no significant matrix effects when measuring low-potassium doped Si wafers. Finally, we discuss possible applications of AMS beyond its use as a highly sensitive analytical tool. Keywords: Silicon wafer, Accelerator mass spectrometry, Flame photometry",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Measurement of Ultra - Low Potassium Contaminations with Accelerator Mass Spectrometry . Abstract : The measurement of ultra - low potassium contaminations in silicon is important for the development and production of semiconductor devices , particularly solar devices .The diagnostic limit of typical techniques such as flame photometry or atomic absorption spectroscopy ( AAS ) is not reasonable to meet the requirements set by industry standards . In this research we present an additional method based on accelerator mass spectrometry ( AMS ) .We see that AMS can be used to measure potassium levels down to 10 ( - 12 ) at % K in Si samples . This corresponds to a sensitivity change by three orders of magnitude compared to standard methods like fire photometry .Furthermore , our findings show that AMS has no considerable matrix impacts when examining low - potassium doped Si wafers . Finally , we explain possible use of AMS beyond its use as a highly sensitive characterization technique .Keywords : Silicon wafer , Accelerator mass spectrometry , Flame photometry",
        "rewrite_text": "Title: Measurement of Ultra-Low Potassium Contaminations with Accelerator Mass Spectrometry\n\nAbstract: The accurate measurement of ultra-low potassium contaminations in silicon is crucial for advancing the development and manufacturing of semiconductor devices, particularly in the realm of solar technology. Traditional analytical techniques, such as flame photometry and atomic absorption spectroscopy (AAS), often fall short of the sensitivity required to meet stringent industry standards for potassium detection. In this study, we introduce an innovative approach utilizing accelerator mass spectrometry (AMS) as a means to quantify potassium concentrations in silicon samples. Our results demonstrate that AMS can effectively detect potassium levels as low as 10^(-12) at % K in silicon, achieving a sensitivity improvement of three orders of magnitude over conventional methods like flame photometry. Additionally, we found that AMS exhibits minimal matrix effects when analyzing low-potassium doped silicon wafers, making it a reliable choice for such measurements. Beyond its application as a highly sensitive characterization tool, we discuss the potential of AMS for broader applications in semiconductor research and development. This work highlights the advantages of AMS in addressing the challenges associated with ultra-low contamination detection, paving the way for enhanced quality control in semiconductor manufacturing processes. \n\nKeywords: Silicon wafer, Accelerator mass spectrometry, Flame photometry",
        "ori-fast-z-score": -0.23570226039551587,
        "water-fast-z-score": 5.266851623825876,
        "rewrite-fast-z-score": 0.09950371902099892
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  On the robustness of H-deficient post-AGB tracks .\nAbstract:\nWe present new evolutionary tracks for low-mass stars with initial masses between 0.8 and 8 M⊙, which include convective overshooting in both core helium burning (CHeB) and red giant branch phases as well as mass loss on the AGB phase. We compare these models to observations of Galactic open clusters and find that they reproduce the observed luminosity function very well at all ages up to 1 Gyr. The predicted number ratio of carbon-rich to oxygen-rich AGB stars is also consistent with observations. However, we show that our current set of models fails to explain the existence of extremely luminous C-stars found among old open cluster members. These objects are located above the tip of the RGB but below the theoretical limit of the TP-AGB phase. They may be explained by assuming an additional mixing process during the early stages of the TP-AGB; however, this requires further investigation. \n \n Keywords: Post-AGB evolution",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : On the robustness of H - deficient post - AGB tracks . Abstract : We present new evolutionary tracks for low - mass stars with initial masses between 0 . 8 and 8 [UNK] , which include convective overshooting in both core helium burning ( CHeB ) and red giant branch phases as well as mass loss on the AGB phase .We test these models to observations of Galactic open complexes and find that they reproduce the seen luminosity function very best at all ages up to 1 Gyr . The predicted number ratio of carbon - rich to oxygen - rich AGB stars is also consistent with observations .However , we prove that our latest set of models fails to explain the existence of incredibly luminous C - stars found among old open cluster elements . These bodies are situated above the tip of the RGB but below the theoretical maximum of the TP - AGB phase .They might be described by assuming an additional mixing process during the early stages of the TP - AGB ; however , this demands further investigation . Keywords : Post - AGB evolution",
        "rewrite_text": "Title: On the Robustness of H-Deficient Post-AGB Tracks\n\nAbstract: In this study, we introduce new evolutionary tracks for low-mass stars, specifically those with initial masses ranging from 0.8 to 8 solar masses. Our models incorporate convective overshooting during both the core helium burning (CHeB) and red giant branch (RGB) phases, as well as mass loss during the asymptotic giant branch (AGB) phase. We validate these evolutionary tracks against observational data from Galactic open clusters and find that they accurately reproduce the observed luminosity function across all ages up to 1 billion years. Additionally, the predicted ratio of carbon-rich to oxygen-rich AGB stars aligns well with empirical observations, reinforcing the reliability of our models. \n\nHowever, our latest set of models encounters challenges in accounting for the presence of exceptionally luminous carbon stars located above the tip of the RGB but below the theoretical maximum of the thermally pulsing AGB (TP-AGB) phase. These stars present a significant puzzle, as their existence suggests the need for an additional mixing process during the early stages of the TP-AGB phase. This finding indicates that while our models are robust in many respects, further investigation is necessary to fully understand the mechanisms that govern the evolution of these luminous carbon stars. Our research highlights the complexities of post-AGB evolution and emphasizes the importance of continued exploration in this area to reconcile theoretical predictions with observational evidence.\n\nKeywords: Post-AGB evolution, low-mass stars, evolutionary tracks, convective overshooting, mass loss, carbon stars, Galactic open clusters.",
        "ori-fast-z-score": -0.5488212999484517,
        "water-fast-z-score": 2.9285611805518585,
        "rewrite-fast-z-score": -1.3525044520011484
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dynamical traps and chaotic advection in a meandering jet flow .\nAbstract:\nWe study the dynamics of particles suspended in an idealized, two-dimensional turbulent jet using direct numerical simulations (DNS). The jet is forced by imposing random velocity fluctuations at its inflow boundary; it develops into a coherent vortex street downstream that breaks down further downstream to form small-scale turbulence. We find that the particle trajectories are strongly influenced by the presence of large-scale vortices which trap them for extended periods of time. In addition, we observe that the particles can be trapped within smaller scale eddies as well. These dynamical traps lead to enhanced concentration levels of particles along certain regions of their trajectory. Finally, we show that the trapping effect depends on the initial position of the particles with respect to the mean flow direction. Our results suggest that this mechanism may play an important role in the transport of pollutants or other tracers in geophysical flows such as atmospheric jets. Turbulence plays an important role in many natural phenomena ranging from weather prediction to oceanic mixing processes  1  . It also has significant impact on industrial applications including combustion  2  , chemical engineering  3  , and fluid mechanics  4  .\nIn recent years there have been several studies aimed at understanding how particles behave when they are suspended in a turbulent flow  5  -  8  . This problem is relevant not only because of practical reasons but also due to fundamental interest in studying the statistical properties of Lagrangian trajectories  9  . For example, one would like to know whether particles tend to cluster together  10  or disperse homogeneously  11  depending upon their size  12  and/or shape  13  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dynamical trapping and chaotic advection in a meandering jet flow . Abstract : We research the dynamics of molecules dropped in an idealized , two - dimensional turbulent plane using direct numerical simulations ( DNS ) .The jet is displaced by imposing random momentum fluctuations at its inflow boundary ; it develops into a coherent vortex road downstream that broke down further downstream to form small - scale turbulence . We see that the particle trajectories are strongly altered by the presence of large - scale vortices which capture them for extended times of time .In addition , we find that the particles can be trapped within smaller scale eddies as well . These dynamical trapping result to heightened density levels of substances along particular regions of their path .Finally , we show that the capturing phenomenon depends on the first orientation of the molecules with regard to the mean flow path . Our results propose that this mechanism may play an important role in the diffusion of pollutants or other tracers in geophysical flows such as atmospheric jets .Turbulence acts an important role in many natural observations ranging from weather prediction to oceanic mixing systems 1 . It additionally has significant effect on industrial applications notably combustion 2 , chemical engineering 3 , and fluid engineering 4 .In recent years there have been numerous research targeted at studying how particles behave when they are suspended in a turbulent flow 5 - 8 . This problem is relevant not only because of practical reasons but also owing to significant interest in examining the statistical characteristics of Lagrangian trajectories 9 .For instance , one would like to see whether particles tend to group together 10 or disperse homogeneously 11 varying upon their shape 12 and / or shape 13 .",
        "rewrite_text": "In this study, we investigate the dynamics of molecules introduced into an idealized two-dimensional turbulent plane through direct numerical simulations (DNS). The jet flow is perturbed by applying random momentum fluctuations at its inflow boundary, leading to the formation of a coherent vortex structure downstream. As the flow evolves, this structure breaks down, resulting in the emergence of small-scale turbulence. Our findings reveal that the trajectories of particles are significantly influenced by the presence of large-scale vortices, which can capture these particles for extended durations. Additionally, we observe that particles can also become trapped within smaller-scale eddies, leading to increased density levels of substances along specific regions of their paths. Notably, we demonstrate that the trapping phenomenon is contingent upon the initial orientation of the molecules relative to the mean flow direction. These results suggest that such dynamical trapping mechanisms may play a crucial role in the diffusion of pollutants and other tracers within geophysical flows, such as atmospheric jets. Turbulence is a critical factor in various natural phenomena, including weather forecasting and oceanic mixing systems, and it also has significant implications for industrial applications, particularly in combustion, chemical engineering, and fluid dynamics. Recent research has increasingly focused on understanding the behavior of particles suspended in turbulent flows, driven by both practical considerations and a growing interest in the statistical properties of Lagrangian trajectories. This includes investigations into whether particles tend to cluster or disperse uniformly, influenced by their shape and other characteristics. Our work contributes to this body of knowledge by elucidating the complex interactions between particles and turbulent flows, highlighting the importance of flow dynamics in shaping particle behavior.",
        "ori-fast-z-score": -1.3522468075656264,
        "water-fast-z-score": 7.049123409692188,
        "rewrite-fast-z-score": 1.794151081205198
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Proper Motions in the Galactic Bulge: Plaut s Window .\nAbstract:\nWe present proper motions for stars with magnitudes between 8 and 16, obtained by combining data from two epochs of photographic plates taken at the Palomar Observatory (POSS-I) and one epoch of digital images taken with the Hubble Space Telescope (HST). The sample consists of about 1 million objects located within a region centered on the galactic center that is known as  Plaut s window .  We find that our results are consistent with previous measurements made using POSS-II plates combined with HST observations. However, we also find significant differences when compared to other recent studies based on similar datasets but different analysis techniques. These discrepancies may be due to systematic errors introduced during the reduction process or they could indicate real changes in the structure of the bulge over time. Our final catalogue will be available online through the CDS Vizier service. This work was supported by NASA grant NAG5-13523.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Proper Motions in the Galactic Bulge : Plaut s Window . Abstract : We present proper motions for stars with magnitudes between 8 and 16 , obtained by combining information from two epochs of photographic sheets taken at the Palomar Observatory ( POSS - I ) and one epoch of digital photographs took with the Hubble Space Telescope ( HST ) .The sample consists of about 1 million bodies located within a region focused on the galactic center that is known as Plaut s window . We see that our findings are compatible with previous measurements made use POSS - II sheets combined with HST observations .However , we also find considerable variations when compared to other recent studies relying on similar datasets but different analysis methods . These discrepancies may be due to systematic errors acquired during the reduction phase or they may indicate real improvements in the composition of the bulge over time .Our last catalogue will be available digital through the CDS Vizier network . This project was supported by NASA grant NAG5 - 13523 .",
        "rewrite_text": "In this study, we present an analysis of proper motions for stars with magnitudes ranging from 8 to 16, focusing on a specific region of the Galactic Bulge known as Plaut's Window. Our dataset is derived from a combination of two epochs of photographic plates from the Palomar Observatory Sky Survey (POSS-I) and one epoch of digital images captured by the Hubble Space Telescope (HST). The sample comprises approximately one million stars situated near the Galactic center. Our results align with previous measurements obtained from POSS-II plates and HST observations, reinforcing the reliability of our findings. However, we also observe significant discrepancies when comparing our results to other recent studies that utilized similar datasets but employed different analytical techniques. These variations may stem from systematic errors encountered during the data reduction process or could reflect genuine changes in the structure of the Galactic Bulge over time. The comprehensive catalog generated from this research will be made available digitally through the CDS Vizier network, facilitating further exploration and analysis by the scientific community. This project received support from NASA grant NAG5-13523, underscoring the collaborative efforts in advancing our understanding of stellar dynamics within the Galactic Bulge.",
        "ori-fast-z-score": -0.23570226039551587,
        "water-fast-z-score": 5.579886659703326,
        "rewrite-fast-z-score": -0.22086305214969307
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Spectroscopy of Nine Cataclysmic Variable Stars .\nAbstract:\nWe present new spectroscopic observations for nine cataclysmic variable stars (CVs) obtained with the HIRES spectrograph on Keck I telescope in Hawaii, and compare them to previous results. We find that all CVs show double-peaked emission lines which are characteristic features of accretion disks around white dwarfs. The line profiles change dramatically during outburst phases when mass transfer rates increase by several orders of magnitude compared to quiescent states. In addition we detect absorption components at red-shifted velocities in some systems indicating the presence of an extended disk wind or stream overflowing into the disk. These results provide important constraints on theoretical models of CV evolution. \n \n Keywords: Accretion Disk, Double-Peaked Emission Lines, White Dwarf, Cataclysmic Variables \n \n \n \n 1 Introduction \n \n Cataclysmic variables (CVs), also known as dwarf novae, are close binary systems consisting of a white dwarf primary star and a late-type secondary star filling its Roche lobe. Mass is transferred through the inner Lagrangian point L1 onto the surface of the white dwarf where it forms an accretion disk surrounding the compact object. This process leads to periodic outbursts caused by thermal instabilities in the accretion disk resulting in dramatic changes in luminosity over time scales ranging from hours up to years  1  . During these outbursts, the accretion rate increases by several orders of magnitude leading to strong winds and high temperatures in the disk  2  , while the system becomes fainter than usual due to obscuration effects  3  .\n \nThe study of CVs provides valuable information about the physical processes involved in accretion flows  4  , magnetic fields  5  , and angular momentum transport  6  . Furthermore, they can be used as distance indicators  7, 8  and probes of galactic structure  9  . \n \n 2 Observations & Data Reduction \n \n Our sample consists of 9 CVs observed between 2004 and 2007 using the High Resolution Echelle Spectrometer (HIRES)  10  mounted on the 10 m Keck I telescope located on Mauna Kea",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Spectroscopy of Nine Cataclysmic Variable Stars . Abstract : We report new spectroscopic observations for nine cataclysmic variable stars ( CVs ) obtained with the HIRES spectrograph on Keck I telescope in Hawaii , and review them to previous findings .We see that all CVs show dual - peaked emission lines which are peculiar characteristics of accretion disks around white dwarfs . The line profiles change dramatically during outburst phases when mass transfer rates increase by many orders of magnitude compared to quiescent states .In addition we perceive absorption elements at red - shifted velocities in some systems suggesting the presence of an extended disk wind or stream overflowing into the disk . These data provide important restrictions on theoretical theories of CV evolution .Keywords : Accretion Disk , Double - Peaked Emission Lines , White Dwarf , Cataclysmic Variables 1 Introduction Cataclysmic variables ( CVs ) , sometimes called as dwarf novae , are close binary complexes consisting of a brown giant primary star and a late - class secondary star filling its Roche lobe . Mass is transferred through the inner Lagrangian point L1 onto the surface of the white dwarf where it creates an accretion disk surrounding the compact object .This process results to periodic outbursts caused by temperature instabilities in the accretion disk resulting in severe shifts in luminosity over time scales extending from hours up to years 1 . During these outbursts , the accretion rate grows by many orders of magnitude resulting to powerful storms and rising heat in the disk 2 , while the system gets fainter than usual thanks to obscuration effects 3 .The investigation of CVs provides valuable info about the physical processes responsible in accretion flows 4 , magnetic waves 5 , and spatial velocity transport 6 . Furthermore , they can be used as distance indicators 7 , 8 and probes of galactic structure 9 .2 Observations & Data Reduction Our specimen consists of 9 CVs seen between 2004 and 2007 utilizing the High Resolution Echelle Spectrometer ( HIRES ) 10 installed on the 10 m Keck I telescope located on Mauna Kea",
        "rewrite_text": "**Title:** Spectroscopy of Nine Cataclysmic Variable Stars\n\n**Abstract:** In this study, we present new spectroscopic observations of nine cataclysmic variable stars (CVs) acquired using the HIRES spectrograph on the Keck I telescope in Hawaii. Our findings are compared with previous research, revealing that all observed CVs exhibit dual-peaked emission lines, a distinctive feature indicative of accretion disks surrounding white dwarfs. Notably, we observe significant changes in the line profiles during outburst phases, where mass transfer rates can increase dramatically, sometimes by several orders of magnitude, compared to their quiescent states. Additionally, we identify red-shifted absorption features in some systems, which suggest the existence of an extended disk wind or a stream that overflows into the accretion disk. These observations impose critical constraints on theoretical models of cataclysmic variable evolution.\n\nCataclysmic variables, often referred to as dwarf novae, are binary systems composed of a red giant primary and a late-type secondary star that fills its Roche lobe. Mass is transferred through the inner Lagrangian point (L1) onto the white dwarf's surface, forming an accretion disk. This process leads to periodic outbursts driven by thermal instabilities within the disk, resulting in significant fluctuations in luminosity over timescales ranging from hours to years. During these outbursts, the accretion rate can increase substantially, leading to intense heating and dynamic activity within the disk, while the system may appear dimmer due to obscuration effects. The study of CVs is crucial for understanding the underlying physical mechanisms of accretion flows, magnetic phenomena, and the transport of spatial velocities. Furthermore, CVs serve as important distance indicators and probes for investigating galactic structure. Our observations, conducted between 2004 and 2007 using the High Resolution Echelle Spectrometer (HIRES) on the 10 m Keck I telescope at Mauna Kea, contribute valuable insights into the behavior and evolution of these intriguing astrophysical systems. \n\n**Keywords:** Accretion Disk, Double-Peaked Emission Lines, White Dwarf, Cataclysmic Variables",
        "ori-fast-z-score": -1.158648244043315,
        "water-fast-z-score": 6.7863682865394175,
        "rewrite-fast-z-score": 1.5105264449340403
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Real Time Turbulent Video Perfecting by Image Stabilization and Super-Resolution .\nAbstract:\nWe propose an image stabilizing method for video capturing in turbulent environments, which is based on the super-resolution technique. The proposed method can be used to improve the quality of captured videos with handheld cameras or other devices that are subject to motion blur caused by turbulence. We first estimate the camera motion using optical flow between consecutive frames. Then we use this information as input data into our super-resolution algorithm to generate high resolution images. Finally, these images are combined together to produce a stable output frame. Our experimental results show that the proposed method outperforms existing methods both visually and quantitatively. \n \n Keywords: Real time video processing; Turbulence; Motion blur; Optical flow; Super-resolution; Image stabilization. 1 Introduction \n \n In recent years there has been growing interest in developing techniques for real time video processing applications such as video surveillance systems  1  , traffic monitoring  2  , remote sensing  3  . However, most of these applications require capturing clear images under challenging conditions like low-light illumination  4  , fast moving objects  5  , blurry scenes  6  , etc.. Among all these challenges, one of the major problems is how to deal with the motion blur caused by turbulence  7  8  9  when capturing videos with hand-held cameras or other devices  10  . \n \n Figure 1: An example of a video sequence taken at nighttime (a) and its corresponding ground truth (b).",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Real Time Turbulent Video Perfecting by Image Stabilization and Super - Resolution . Abstract : We suggest an image stabilizing method for video capturing in volatile settings , which is based on the super - resolution technique .The proposed approach can be used to improve the performance of released movies with handheld cameras or other devices that are subject to moving blur caused by turbulence . We first estimate the film motion using optical flow between successive frames .Then we using this data as input data into our super - resolution algorithm to create high resolution photos . Finally , these images are fused together to produce a consistent output frame .Our research results show that the suggested method outperforms current methods both physically and quantitatively . Keywords : Real time television processing ; Turbulence ; Motion blur ; Optical stream ; Super - resolution ; Image stabilization .1 Introduction In recent years there has been growing interest in improving strategies for real time video editing applications such as video monitoring systems 1 , road monitoring 2 , remote sensing 3 . However , most of these uses involve capturing clear photographs under difficult environments like low - light lighting 4 , fast move images 5 , blurry scenes 6 , etc . .Among all these problems , one of the significant obstacles is how to deal with the movement blur caused by turbulence 7 8 9 when capturing clips with hand - held photographers or other devices 10 . Figure 1 : An example of a video sequence taken at nighttime ( a ) and its equivalent ground truth ( c ) .",
        "rewrite_text": "**Title:** Real-Time Turbulent Video Enhancement through Image Stabilization and Super-Resolution\n\n**Abstract:** In this study, we propose a novel image stabilization technique designed for video capture in dynamic environments, leveraging super-resolution methodologies. This innovative approach aims to enhance the quality of footage obtained from handheld cameras and similar devices, which often suffer from motion blur due to turbulence. Our method begins with the estimation of motion within the video using optical flow analysis between consecutive frames. This motion data serves as a critical input for our super-resolution algorithm, which generates high-resolution images from the original low-resolution frames. Subsequently, these enhanced images are fused to create a coherent output frame that significantly improves visual clarity. The results of our experiments demonstrate that the proposed technique surpasses existing methods in both qualitative and quantitative assessments, providing a more stable and clearer video output. This advancement is particularly relevant for real-time video editing applications, including video surveillance, traffic monitoring, and remote sensing, where capturing clear images in challenging conditions—such as low light, rapid motion, and blurred scenes—remains a significant challenge. Our findings indicate that addressing motion blur caused by turbulence is crucial for improving the overall quality of video captured in such environments. The implications of this research extend to various fields that rely on high-quality video footage, suggesting a pathway for future developments in real-time video processing technologies.\n\n**Keywords:** Real-time video processing; turbulence; motion blur; optical flow; super-resolution; image stabilization.",
        "ori-fast-z-score": -0.8703882797784892,
        "water-fast-z-score": 6.910947404650881,
        "rewrite-fast-z-score": -2.0344711469278987
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Equivalence of LP Relaxation and Max-Product for Weighted Matching in General Graphs .\nAbstract:\nWe show that the linear programming relaxation (LP) is equivalent to the max-product algorithm on general graphs, when applied to weighted matching problems with non-negative weights.  We prove this equivalence by showing how each step of the max-product algorithm can be simulated using an appropriate rounding technique based on the solution of the dual problem at hand.   The main idea behind our approach is to use the fact that any feasible primal-dual pair satisfies certain properties which we exploit to obtain a valid rounding scheme. Our results are applicable to many combinatorial optimization problems such as maximum weight bipartite matching, minimum cost flow, vertex cover etc., where the objective function has only non-negative coefficients. In particular, they imply that the integrality gap of these problems under their respective LP relaxations cannot exceed 1 + $O(1/n)$, where $n$ denotes the number of nodes or edges in the input graph. This improves upon previously known upper bounds of 2 and 3/2 respectively.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Equivalence of LP Relaxation and Max - Product for Weighted Matching in General Graphs . Abstract : We see that the linear programming relaxation ( LP ) is analogous to the max - product algorithm on general graphs , when applied to weighted matching problems with non - negative weights .We test this equivalence by showing how each step of the max - product algorithm can be simulated using an appropriate rounding methodology relying on the solve of the dual issue at hand . The main idea behind our approach is to use the fact that any feasible primal - dual pair satisfies certain characteristics which we utilize to obtain a valid rounding scheme .Our results are applicable to many combinatorial algorithms problems such as maximum weight bipartite matching , lowest price flow , vertex cover etc . , where the objective function has only non - negative coefficients . In particular , they mean that the integrality gap of these problems under their respective LP relaxations cannot exceed 1 + $ O ( 1 / n ) $ , where $ n $ represents the number of vertices or edges in the input graph .This improves upon previously known upper limits of 2 and 3 / 2 respectively .",
        "rewrite_text": "Title: Equivalence of LP Relaxation and Max-Product for Weighted Matching in General Graphs\n\nAbstract: In this study, we explore the relationship between linear programming (LP) relaxation and the max-product algorithm in the context of weighted matching problems on general graphs with non-negative weights. We establish that these two approaches are analogous by demonstrating that each iteration of the max-product algorithm can be effectively simulated through a suitable rounding technique, which is based on solving the corresponding dual problem. The core of our methodology leverages the properties of feasible primal-dual pairs, allowing us to devise a valid rounding scheme that preserves the integrity of the solution. Our findings have significant implications for various combinatorial optimization problems, including maximum weight bipartite matching, lowest price flow, and vertex cover, all of which feature objective functions with non-negative coefficients. Notably, our results indicate that the integrality gap for these problems, as derived from their respective LP relaxations, is bounded by 1 + $O(1/n)$, where $n$ denotes the number of vertices or edges in the graph. This represents a substantial improvement over previously established upper bounds of 2 and 3/2 for these problems. By providing a clearer understanding of the interplay between LP relaxation and the max-product algorithm, our work contributes to the broader field of combinatorial optimization and offers new avenues for research in algorithm design and analysis.",
        "ori-fast-z-score": 1.2649110640673518,
        "water-fast-z-score": 4.717281765248632,
        "rewrite-fast-z-score": 1.0734900802433864
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Production of magnetic energy by macroscopic turbulence in GRB afterglows .\nAbstract:\nWe study the production of magnetic fields and their subsequent evolution during the relativistic blast wave phase of gamma-ray burst (GRB) afterglow emission, using numerical simulations with high spatial resolution. We find that the magnetic field is amplified to equipartition strength within several hundred milliseconds at most for typical parameters. The amplification occurs mainly through turbulent dynamo action driven by the kinetic energy of the shocked fluid. After reaching its peak value, the magnetic field decays gradually due to adiabatic expansion losses as well as Ohmic dissipation. Our results suggest that the observed X-ray flares are likely produced by internal shocks between shells ejected from different regions inside the progenitor star. \n \n Keywords: Gamma-Ray Bursts, Magnetic Fields, Dynamo Action, Relativistic Blast Wave, Turbulence \n \n 1. Introduction \n \n In recent years there has been growing evidence suggesting that gamma-ray burst (GRBs) may be associated with massive stars (e.g., Woosley & Bloom 2006) . If this is true, then it would imply that some fraction of these stars explode into space while still surrounded by dense stellar winds or envelopes. These environments can significantly affect the dynamics of the explosion and the properties of the emitted radiation. For example, Chevalier et al. (2004) showed that if the density profile of the surrounding medium follows an r-2 power law, then the resulting light curve will exhibit a plateau followed by a steep decay phase. This behavior was later confirmed observationally (e.g., Panaitescu 2005; Kumar & Panaitescu 2008) , which led to the suggestion that many GRBs might originate from such progenitors (e.g., Zhang 2007). However, other authors have argued against this scenario on theoretical grounds (e.g., Ramirez-Ruiz et al. 2005 ) and observational ones (e.g., Lazzati et al. 2009 ). It should also be noted that even though the majority of GRBs seem to follow this general trend, there exist cases where no clear signature of interaction with a wind-like environment could",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Production of magnetic energy by macroscopic turbulence in GRB afterglows . Abstract : We research the production of magnetic fields and their ensuing evolution during the relativistic blast wave period of gamma - ray burst ( GRB ) afterglow emission , using numerical simulations with high spatial resolution .We see that the magnetic force is amplified to equipartition speed within several hundred milliseconds at most for typical characteristics . The amplification occurs mostly through chaotic dynamo action driven by the kinetic power of the excited fluid .After achieving its peak value , the magnetic force decays slowly due to adiabatic expansion losses as well as Ohmic dissipation . Our results show that the seen X - ray flares are likely generated by inner shocks between shells ejected from multiple sites inside the progenitor star .Keywords : Gamma - Ray Bursts , Magnetic Fields , Dynamo Action , Relativistic Blast Wave , Turbulence 1 . Introduction In recent years there has been growing data indicating that gamma - ray burst ( GRBs ) may be identified with massive stars ( e . g . , Woosley & Bloom 2006 ) .If this is true , then it would indicate that some fraction of these stars flash into space while already engulfed by dense stellar winds or envelopes . These surroundings can significantly affect the dynamics of the explosion and the properties of the emitted radiation .For instance , Chevalier et al . ( 2004 ) demonstrated that if the density profile of the nearby medium follows an r - 2 power law , then the resulting light curve will exhibit a plateau followed by a steep decay phase .This phenomenon was later confirmed observationally ( e . g . , Panaitescu 2005 ; Kumar & Panaitescu 2008 ) , which leading to the suggestion that several GRBs might originate from such progenitors ( e . g . , Zhang 2007 ) . However , other scholars have called against this situation on theoretical grounds ( e . g . , Ramirez - Ruiz et al .2005 ) and observational ones ( e . g . , Lazzati et al . 2009 ) .It should also be mentioned that even though the majority of GRBs appears to follow this general pattern , there remain instances where no clear signature of interaction with a wind - like environment could",
        "rewrite_text": "**Title:** Production of Magnetic Energy by Macroscopic Turbulence in GRB Afterglows\n\n**Abstract:** This study investigates the generation and subsequent evolution of magnetic fields during the relativistic blast wave phase of gamma-ray burst (GRB) afterglow emissions, utilizing high-resolution numerical simulations. Our findings indicate that the magnetic energy reaches equipartition levels within a few hundred milliseconds under typical conditions. The primary mechanism for this amplification is chaotic dynamo action, which is driven by the kinetic energy of the turbulent fluid. Once the magnetic field achieves its maximum strength, it experiences a gradual decline due to adiabatic expansion losses and Ohmic dissipation. Notably, our results suggest that the observed X-ray flares are likely produced by internal shocks occurring between shells that are ejected from multiple regions within the progenitor star.\n\n**Keywords:** Gamma-Ray Bursts, Magnetic Fields, Dynamo Action, Relativistic Blast Wave, Turbulence\n\n**1. Introduction:** Recent observations have increasingly linked gamma-ray bursts (GRBs) to massive stars (e.g., Woosley & Bloom, 2006). If this correlation holds true, it implies that a subset of these stars emits bursts while enveloped in dense stellar winds or surrounding material. Such environments can significantly influence the explosion dynamics and the characteristics of the emitted radiation. For example, Chevalier et al. (2004) demonstrated that a medium with a density profile following an r^-2 power law results in a light curve that displays a plateau followed by a rapid decay phase. This phenomenon has been corroborated by subsequent observations (e.g., Panaitescu, 2005; Kumar & Panaitescu, 2008), leading to the hypothesis that several GRBs may originate from such progenitors (e.g., Zhang, 2007). However, some researchers have raised theoretical (e.g., Ramirez-Ruiz et al., 2005) and observational (e.g., Lazzati et al., 2009) challenges to this perspective. It is important to note that while many GRBs conform to this general behavior, there are cases where no definitive evidence of interaction with a wind-like environment is observed.",
        "ori-fast-z-score": -0.5516772843673704,
        "water-fast-z-score": 6.740938478604225,
        "rewrite-fast-z-score": -0.2491364395612199
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Realizable Hamiltonians for Universal Adiabatic Quantum Computers .\nAbstract:\nWe present an algorithm that determines whether or not there exists a quantum circuit with a given number of qubits and gates, such that the corresponding time-independent Hamiltonian is realizable by a physical system in which each energy level has at most one excited state.  We also show how to find all possible circuits if they exist. Our results are based on recent work showing that any time-independent Hamiltonian can be written as a sum of commuting projectors onto its eigenstates. This decomposition allows us to reduce the problem of finding a realization of a general time-independent Hamiltonian into several instances of the same problem but restricted to smaller Hilbert spaces. The reduction yields a polynomial-time algorithm when applied recursively. Finally we discuss some applications of our method including determining the minimum depth required for universal adiabatic quantum computers. In this article we consider the following problem: \nGiven a set of n qubits and m two-qubit gates, does there exist a quantum circuit consisting only of these gates whose associated time-independent Hamiltonian is realizable; i.e., it corresponds to a Hermitian operator acting on a finite-dimensional Hilbert space? If so, what is the smallest circuit size needed?\nThe answer to this question will depend on the specifics of the model used to describe the physical system under consideration. For example, in the case where each energy level may have more than one excited state (i.e., degenerate), then no circuit can realize the desired Hamiltonian unless it contains infinitely many gates. On the other hand, if each energy level has exactly one excited state (i..",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Realizable Hamiltonians for Universal Adiabatic Quantum Computers . Abstract : We create an algorithm that decide whether or not there exists a quantum circuit with a given number of qubits and gates , such that the associated time - independent Hamiltonian is realizable by a physical system in which each energy level has at most one excited state .We also work how to find all possible circuits if they exist . Our results are based on previous research showing that any time - independent Hamiltonian can be written as a sum of commuting projectors onto its eigenstates .This decomposition allows us to reduce the question of finding a realization of a general time - independent Hamiltonian into numerous instances of the same difficulty but restricted to smaller Hilbert spaces . The reduction gives a polynomial - time algorithm when applied recursively .Finally we explain some applications of our technique including calculating the limit depth needed for universal adiabatic quantum computers . In this article we investigate the following issue : Given a setting of n qubits and m two - qubit gates , does there exist a quantum circuit consisting only of these gates whose associated time - independent Hamiltonian is realizable ; i . e . , it corresponds to a Hermitian operator acting on a finite - dimensional Hilbert space ?If so , what is the smallest circuit length needed ? The answer to this question will depend on the specifics of the model used to explain the physical system under consideration .For instance , in the case where each energy level may have more than one excited state ( i . e . , degenerate ) , then no circuit can realize the desired Hamiltonian unless it contains infinitely many gates . On the other hand , if each energy level has precisely one excited state ( i . .",
        "rewrite_text": "In this article, we present a novel algorithm designed to determine the existence of a quantum circuit composed of a specified number of qubits and gates, such that the corresponding time-independent Hamiltonian can be realized by a physical system where each energy level accommodates at most one excited state. Our research builds upon prior findings that demonstrate any time-independent Hamiltonian can be expressed as a sum of commuting projectors onto its eigenstates. This decomposition is pivotal as it transforms the challenge of realizing a general time-independent Hamiltonian into multiple instances of a similar problem, albeit constrained to smaller Hilbert spaces. By employing this reduction method recursively, we derive a polynomial-time algorithm.\n\nFurthermore, we explore the implications of our technique, particularly in determining the minimum circuit depth required for universal adiabatic quantum computers. The central question we address is whether, given n qubits and m two-qubit gates, it is feasible to construct a quantum circuit using only these gates that corresponds to a realizable time-independent Hamiltonian, which is defined as a Hermitian operator acting within a finite-dimensional Hilbert space. The answer to this inquiry is contingent upon the specifics of the model that describes the physical system in question. For example, in scenarios where energy levels can exhibit degeneracy—allowing for multiple excited states—no circuit can achieve the desired Hamiltonian unless it incorporates an infinite number of gates. Conversely, if each energy level is restricted to a single excited state, our findings suggest a more favorable outcome for circuit realization. Overall, this work contributes to the understanding of Hamiltonian realizability in quantum computing and provides a framework for future investigations into the design of quantum circuits.",
        "ori-fast-z-score": 1.9445436482630056,
        "water-fast-z-score": 6.075111253646805,
        "rewrite-fast-z-score": 1.3719886811400706
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Almost the supersymmetric Standard Model from intersecting D6-branes on the Z_6  orientifold .\nAbstract:\nWe propose an explicit construction for a string model with gauge group SU(3)xSU(2)xU(1), three generations of quarks and leptons, right-handed neutrinos, and no exotic particles beyond those in the minimal supersymmetric standard model (MSSM). The model is based on type IIA string theory compactified to four dimensions on a Calabi-Yau manifold X which has h^{1,1} = 1 and h^{1,2} = 0. We consider a stack of N=2 D6-branes wrapping a divisor S inside X that supports a non-abelian gauge symmetry U(N_c) where N_c = 3. In addition we introduce two sets of fractional branes at singularities of order 6 along divisors T_1 and T_2 such that the total number of D6-branes is 24. By using mirror symmetry arguments it can be shown that this configuration preserves one half of the original supersymmetry.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Almost the supersymmetric Standard Model from intersecting D6 - branes on the Z _ 6 orientifold . Abstract : We suggest an explicit construction for a string description with gauge group SU ( 3 ) xSU ( 2 ) xU ( 1 ) , three generations of quarks and leptons , right - handed neutrinos , and no exotic electrons beyond those in the maximum supersymmetric standard theory ( MSSM ) .The model is based on type IIA string theory compactified to four dimensions on a Calabi - Yau manifold X which has h ^ { 1 , 1 } = 1 and h ^ { 1 , 2 } = 0 . We consider a stack of N = 2 D6 - branes wrapping a divisor S inside X that supports a non - abelian gauge symmetry U ( N _ c ) where N _ c = 3 .In addition we provide two sets of fractional branes at singularities of order 6 along divisors T _ 1 and T _ 2 such that the total number of D6 - branes is 24 . By using mirror symmetry arguments it can be shown that this configuration captures one quarter of the previous supersymmetry .",
        "rewrite_text": "In this article, we present a detailed construction of a string theory framework that aligns with the gauge group SU(3) x SU(2) x U(1), incorporating three generations of quarks and leptons, as well as right-handed neutrinos, while excluding any exotic electrons beyond those predicted by the maximal supersymmetric standard model (MSSM). Our model is rooted in type IIA string theory, which is compactified to four dimensions on a Calabi-Yau manifold, denoted as X, characterized by h^{1,1} = 1 and h^{1,2} = 0. \n\nCentral to our construction is a stack of N = 2 D6-branes that wrap around a divisor S within the manifold X, facilitating a non-abelian gauge symmetry U(N_c), where N_c is set to 3. Furthermore, we introduce two sets of fractional branes located at singularities of order 6 along the divisors T_1 and T_2, culminating in a total of 24 D6-branes in the configuration. \n\nThrough the application of mirror symmetry arguments, we demonstrate that this particular arrangement successfully encapsulates one quarter of the supersymmetry found in previous models. Our findings contribute to the understanding of how intersecting D6-branes can yield a viable supersymmetric framework, potentially offering insights into the unification of forces and the generation of particle masses within the context of string theory. This work not only advances the theoretical landscape of string compactifications but also paves the way for further exploration of realistic particle physics models derived from string theory.",
        "ori-fast-z-score": 0.674199862463242,
        "water-fast-z-score": 3.474396144861517,
        "rewrite-fast-z-score": 1.1285761872936695
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Field-theoretical formulations of MOND-like gravity .\nAbstract:\nWe present field-theoretical formulations for modified Newtonian dynamics (MOND) and its relativistic generalization, TeVeS. We show that the two theories are equivalent in their weak-field limit but differ significantly at high accelerations. In particular we find that the gravitational potential generated by an isolated point mass is different between the two theories even when they predict identical rotation curves around this object. This difference can be used to test whether or not dark matter exists using observations of galaxy clusters. The equivalence principle states that all local physics should be independent of where it occurs in space-time. However, there exist many extensions to general relativity which violate this principle. One such extension is the so-called scalar-tensor-vector-symmetric theory (STV), also known as TeVeS. STV has been shown to successfully reproduce galactic rotation curves without requiring any additional dark matter component. Here we study how violations of the equivalence principle affect the growth rate of structure formation within the context of STV cosmology.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Field - theory formulations of MOND - like gravity . Abstract : We present field - theory formulations for altered Newtonian physics ( MOND ) and its relativistic generalization , TeVeS .We see that the two models are comparable in their weak - field limit but differ significantly at high accelerations . In particular we find that the gravitational potential caused by an isolated point mass is distinct between the two models even when they predict identical rotation curves around this particle .This difference can be used to test whether or not dark matter exists using observations of galaxy regions . The equivalence principle requires that all local physics should be independent of where it appears in space - time .However , there remain many extensions to general relativity which violate this doctrine . One such extension is the so - called scalar - vector - vector - symmetric theory ( STV ) , sometimes called as TeVeS .STV has been shown to effectively reproduce galactic rotation curves without requiring any additional dark matter component . Here we study how violations of the equivalence principle affect the development probability of shape formation within the context of STV cosmology .",
        "rewrite_text": "We introduce field-theory formulations for Modified Newtonian Dynamics (MOND) and its relativistic extension, Tensor-Vector-Scalar (TeVeS) theory. Our analysis reveals that while both models exhibit similarities in their weak-field approximations, they diverge significantly under conditions of high acceleration. Notably, we observe that the gravitational potential generated by an isolated point mass differs between the two frameworks, despite both models yielding identical rotation curves for objects orbiting this mass. This discrepancy presents a potential avenue for testing the existence of dark matter through observational studies in galactic regions. \n\nThe equivalence principle posits that local physical laws should remain consistent regardless of their location in space-time. However, numerous extensions to general relativity challenge this principle. One such extension is the Scalar-Vector-Vector (SVV) symmetric theory, commonly referred to as TeVeS. This theory has been demonstrated to accurately reproduce the observed galactic rotation curves without necessitating the inclusion of dark matter. \n\nIn this study, we investigate the implications of equivalence principle violations on the probability of shape formation in the context of STV cosmology. By examining these effects, we aim to deepen our understanding of the fundamental interactions governing cosmic structure formation and the potential role of modified gravity theories in explaining phenomena typically attributed to dark matter. Our findings contribute to the ongoing discourse surrounding the nature of gravity and the underlying principles that govern the universe, offering insights that may inform future research in cosmology and astrophysics.",
        "ori-fast-z-score": 1.0425720702853738,
        "water-fast-z-score": 6.394811726804519,
        "rewrite-fast-z-score": 1.3750477455423171
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Search for a kaonic nuclear state via $^4$He$(K^-, N)$ .\nAbstract:\nWe report on the search for a K-nuclear bound state in 4 He using the reaction $^4$He (K - , n). The experiment was performed at TRIUMF with an incident beam energy of 1 GeV and a target thickness of 0.5 cm. A total number of 2.1 x 10 9 events were recorded by two large area silicon strip detectors placed downstream of the target. No evidence is found for such a state within the kinematic limits set by the experimental resolution. Upper limits are determined as a function of the binding energy B and the width Γ of the hypothetical state. For a narrow resonance with B = 50 MeV/c2 we find that the upper limit to its production cross section is 3 nb/sr at 90% confidence level. This corresponds to a lower limit on the coupling constant gNN of the order of 5 x 10 -4 . \nThe results presented here represent one of the most stringent constraints yet obtained on this type of exotic nuclear structure. \n \n Keywords: Kaon nucleus interaction",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Search for a kaonic nuclear state via $ ^ 4 $ He $ ( K ^ - , N ) $ . Abstract : We report on the hunt for a K - atomic bound state in 4 He using the response $ ^ 4 $ He ( K - , k ) .The project was done at TRIUMF with an incident beam energy of 1 GeV and a target thickness of 0 . 5 centimetres . A total number of 2 . 1 x 10 9 episodes were collected by two huge area silicon strip detectors placed downstream of the target .No evidence is found for such a state within the kinematic limits established by the empirical resolution . Upper boundaries are chosen as a function of the binding energy B and the width Γ of the hypothetical state .For a small resonance with B = 50 MeV / c2 we find that the higher limit to its production cross area is 3 nb / sr at 90 % confidence rate . This corresponds to a smaller limitation on the interaction function gNN of the order of 5 x 10 - 4 .The results presented here represent one of the most stringent constraints ever achieved on this form of exotic nuclear configuration . Keywords : Kaon nucleus interaction",
        "rewrite_text": "Title: Search for a Kaonic Nuclear State via $^4$He $(K^-, N)$\n\nAbstract: In this study, we present our investigation into the existence of a K^- atomic bound state within the helium-4 nucleus, utilizing the reaction $^4$He $(K^-, k)$. The experiments were conducted at TRIUMF, employing an incident beam energy of 1 GeV and a target thickness of 0.5 centimeters. Over the course of the experiment, we successfully collected approximately 2.1 x 10^9 events using two large-area silicon strip detectors positioned downstream of the target. Despite the extensive data collection, we found no evidence supporting the existence of a kaonic nuclear state within the kinematic limits defined by our empirical resolution. \n\nTo quantify our findings, we established upper limits on the production cross-section as a function of the binding energy (B) and the width (Γ) of the proposed state. Specifically, for a resonance with a binding energy of B = 50 MeV/c², we determined that the upper limit for its production cross-section is 3 nb/sr at a 90% confidence level. This result imposes a stringent constraint on the interaction function gNN, estimated to be on the order of 5 x 10^-4. \n\nThe implications of our findings are significant, as they represent one of the most stringent limits placed on the existence of such exotic nuclear configurations to date. Our results contribute to the ongoing exploration of kaon-nucleus interactions and enhance our understanding of the fundamental properties of nuclear matter. This research not only advances theoretical predictions but also sets the stage for future investigations into the nature of kaonic states in nuclear physics. \n\nKeywords: Kaon-nucleus interaction",
        "ori-fast-z-score": -1.3416407864998738,
        "water-fast-z-score": 4.777777777777778,
        "rewrite-fast-z-score": 1.4855627054164149
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Spin-polarized transport in II-VI magnetic resonant tunneling devices .\nAbstract:\nWe report on the spin polarization and magnetoresistance properties of GaAs/(Cd,Mn)Te/GaAs quantum well structures grown by molecular beam epitaxy (MBE). The samples were designed to have an asymmetric barrier structure with different Mn concentrations at each side of the barrier. We find that the degree of spin polarization is strongly dependent on both the applied bias voltage and temperature. At low temperatures we observe large negative magnetoresistances which are attributed to spin-dependent scattering processes within the barriers. These results demonstrate that these structures can be used as efficient spin injectors into semiconductors such as Si or Ge. In recent years there has been considerable interest in developing semiconductor-based spintronic devices for applications ranging from high-density data storage systems to novel electronic components  1  . One promising approach involves using ferromagnetic materials as source/drain contacts in field-effect transistors  2  , where the current through the device depends upon the relative alignment between the electron spins in the channel region and those in the contact regions  3  .\nIn this work we present experimental studies of spin polarized transport in GaAs/Cd 0.9 Mn 0.1 Te/GaAs double-barrier heterostructures grown by molecular-beam epitaxy  4  . Our aim was to investigate how the degree of spin polarization varies under different conditions of applied bias voltage and temperature, and also to determine whether it is possible to use these structures as efficient spin injectors for other semiconductors such as silicon  5  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Spin - polarized transport in II - VI magnetic resonant tunneling systems . Abstract : We report on the spin polarization and magnetoresistance properties of GaAs / ( Cd , Mn ) Te / GaAs quantum well formations grown by molecular beam epitaxy ( MBE ) .The samples were created to have an asymmetric barrier structure with various Mn levels at each side of the barrier . We see that the degree of spin polarization is strongly dependent on both the introduced bias voltage and heat .At small temperatures we study huge negative magnetoresistances which are traced to spinning - dependent scattering mechanisms within the barriers . These data demonstrate that these structures can be used as efficient spinning injectors into semiconductors such as Si or Ge .In past decades there has been substantial interest in building semiconductor - based spintronic systems for applications ranging from high - density data storage systems to novel electronic systems 1 . One promising solution involves utilizing ferromagnetic substances as source / drain contacts in field - effect transistors 2 , where the charge through the device depends upon the relative alignment between the electron spins in the channel area and those in the contact areas 3 .In this research we present experimental studies of spin polarized transport in GaAs / Cd 0 . 9 Mn 0 . 1 Te / GaAs dual - barrier heterostructures grown by molecular - laser epitaxy 4 . Our aim was to examine how the degree of spin polarization depends under various circumstances of applied bias voltage and heat , and also to find whether it is possible to use these structures as efficient spinning injectors for other semiconductors such as silicon 5 .",
        "rewrite_text": "We present a comprehensive study on the spin polarization and magnetoresistance characteristics of GaAs/(Cd, Mn)Te/GaAs quantum well structures fabricated through molecular beam epitaxy (MBE). The samples were designed with an asymmetric barrier configuration, incorporating varying levels of manganese on either side of the barrier. Our findings reveal that the spin polarization degree is significantly influenced by the applied bias voltage and temperature conditions. At low temperatures, we observe pronounced negative magnetoresistance effects, which can be attributed to spin-dependent scattering processes occurring within the barriers. These results indicate the potential of these structures to function as effective spin injectors into semiconductor materials such as silicon (Si) and germanium (Ge). \n\nIn recent years, there has been a growing interest in the development of semiconductor-based spintronic devices, which hold promise for applications in high-density data storage and innovative electronic systems. A particularly promising approach involves the integration of ferromagnetic materials as source and drain contacts in field-effect transistors, where the electrical current is influenced by the relative alignment of electron spins in the channel and the contact regions. In this article, we detail our experimental investigations into spin-polarized transport within GaAs/Cd0.9Mn0.1Te/GaAs dual-barrier heterostructures, also produced via molecular-laser epitaxy. Our research aims to elucidate the dependence of spin polarization on varying conditions of bias voltage and thermal effects, while also assessing the feasibility of utilizing these structures as efficient spin injectors for other semiconductor platforms. This work contributes to the ongoing exploration of spintronic technologies and their potential to revolutionize electronic device performance and functionality.",
        "ori-fast-z-score": 0.2727272727272727,
        "water-fast-z-score": 7.423931175486519,
        "rewrite-fast-z-score": -0.7016464154456235
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Sweet Spot Supersymmetry .\nAbstract:\nWe present the results of an analysis searching for supersymmetric particles in events with jets and missing transverse energy using data collected by the D0 experiment at Fermilab during Run II, corresponding to 1 fb-1 . We consider models where squarks decay into quarks and gluinos which then decay via intermediate sleptons or neutralinos into final states containing two light-flavored leptons (electrons and/or muons) and large E T / .\nThe search is performed over a wide range of masses for all sparticles involved in these cascade decays , including those that are not directly produced but can be exchanged in t-channel processes . No significant excess above background expectations has been observed . Limits on the production cross sections times branching fractions have been set as functions of the mass parameters of the model considered . These limits are compared to theoretical predictions obtained within the framework of minimal supergravity grand unification .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Sweet Spot Supersymmetry . Abstract : We present the conclusion of an assessment searching for supersymmetric particles in events with jets and lost radial energy using data derived by the D0 study at Fermilab during Run II , equivalent to 1 fb - 1 .We consider scenarios where squarks decay into quarks and gluinos which then decay via elementary sleptons or neutralinos into last states bearing two light - flavored leptons ( atoms and / or muons ) and large E T / . The hunt is conducted over a broad variety of masses for all sparticles involved in these cascade decays , particularly those that are not directly produced but can be exchanged in t - channel pathways .No much excess above background expectations has been observed . Limits on the production cross sections times branching fractions have been set as functions of the mass parameters of the model discussed .These restrictions are compared to theoretical estimates obtained within the framework of minimal supergravity grand unification .",
        "rewrite_text": "In this article titled \"Sweet Spot Supersymmetry,\" we present the findings of a comprehensive analysis aimed at detecting supersymmetric particles through events characterized by jets and missing transverse energy. This investigation utilizes data collected by the D0 experiment at Fermilab during Run II, corresponding to an integrated luminosity of approximately 1 fb⁻¹. Our study focuses on scenarios in which squarks decay into quarks and gluinos, which subsequently decay into final states featuring two light-flavored leptons (electrons and/or muons) alongside significant missing transverse energy (E_T). We explore a wide range of mass configurations for all sparticles involved in these cascade decay processes, particularly emphasizing those that are not directly produced but can be exchanged through t-channel interactions.\n\nDespite an extensive search, we have not observed any significant excess above the anticipated background levels. Consequently, we have established limits on the production cross sections multiplied by branching fractions, which are presented as functions of the mass parameters within the discussed model. These constraints are then compared with theoretical predictions derived from the framework of minimal supergravity grand unification. Our results contribute to the ongoing efforts to understand supersymmetry and its implications for particle physics, providing valuable insights into the viability of various supersymmetric scenarios in light of current experimental data.",
        "ori-fast-z-score": -1.4342743312012722,
        "water-fast-z-score": 4.949747468305833,
        "rewrite-fast-z-score": 0.2
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  J1128+592: a highly variable IDV source .\nAbstract:\nWe report on the discovery and study of J1128+592, an intraday variable (IDV) radio quasar at redshift z = 1.8. The object was found in our search for new IDVs using data obtained with the Very Large Array (VLA). We have monitored this source over two epochs separated by one year to look for variability on timescales ranging between 10 minutes and several days. Our results show that it is a strong IDV source which varies up to 50% peak-to-peak amplitude on time scales as short as 20 min. This makes J1128+592 one of the most rapidly varying quasars known so far. In addition we find evidence for long-term variations on timescales longer than 100 days. These are likely due to interstellar scintillation caused by density fluctuations along the line-of-sight towards the source. Finally, we present optical spectroscopy taken with the Keck telescope showing broad emission lines typical of quasars.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : J1128 + 592 : a highly variable IDV source . Abstract : We report on the discovery and study of J1128 + 592 , an intraday variable ( IDV ) radio quasar at redshift z = 1 . 8 .The object was found in our search for new IDVs using data acquired with the Very Large Array ( VLA ) . We have analyzed this source over two epochs separated by one decade to search for variability on timescales ranging between 10 minutes and many days .Our results show that it is a strong IDV source which varies up to 50 % peak - to - peak amplitude on time scales as short as 20 min . This makes J1128 + 592 one of the most rapidly varying quasars known so far .In addition we find data for large - term variations on timescales greater than 100 days . These are likely due to interstellar scintillation caused by density fluctuations along the line - of - view towards the origin .Finally , we present imaging spectroscopy seen with the Keck astronomy showing broad emitted lines typical of quasars .",
        "rewrite_text": "We present our findings on J1128 + 592, a newly identified intraday variable (IDV) radio quasar located at a redshift of z = 1.8. This discovery emerged from our systematic search for new IDV sources utilizing data collected from the Very Large Array (VLA). Our analysis spans two distinct observational epochs separated by a decade, allowing us to investigate variability across a range of timescales from 10 minutes to several days. The results indicate that J1128 + 592 exhibits significant intraday variability, with peak-to-peak amplitude fluctuations reaching up to 50% within just 20 minutes, positioning it among the most rapidly varying quasars documented to date. Furthermore, we observe long-term variability on timescales exceeding 100 days, which we attribute to interstellar scintillation effects resulting from density fluctuations along the line of sight to the quasar. In addition to these variability studies, we provide imaging spectroscopy data obtained from the Keck Observatory, revealing broad emission lines characteristic of quasars. This comprehensive analysis not only enhances our understanding of J1128 + 592 as a highly variable IDV source but also contributes to the broader discourse on the mechanisms driving variability in distant quasars. Our findings underscore the importance of continued monitoring and investigation of such sources to unravel the complexities of their behavior and the astrophysical processes at play.",
        "ori-fast-z-score": 0.23570226039551587,
        "water-fast-z-score": 3.862357857472309,
        "rewrite-fast-z-score": 0.9761870601839528
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  High Sensitivity Array Observations of the $z = 4.4$ QSO BRI 1335-0417 .\nAbstract:\nWe present new observations with the High Sensitivity Array (HSA) at 1.4 GHz and 5 GHz, as well as archival data obtained by the Very Large Array (VLA), in order to study the radio properties of the z=4.4 quasar BRI1335-0417. We detect two compact components separated by 0.3 arcsec on both sides of the optical position of this source. The total flux density is S5GHz=(1.6 ±0.2) mJy for component A and S1.4Ghz=0.7±0.1 mJy for component B. Both sources are unresolved within our beam size. From their spectral indices we conclude that they are most likely associated with an active galactic nucleus. This object has been observed previously using the VLA but only one component was detected. Our results show that there may be more than one AGN contributing to the emission seen in previous studies.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : High Sensitivity Array Observations of the $ z = 4 . 4 $ QSO BRI 1335 - 0417 . Abstract : We report new experiments with the High Sensitivity Array ( HSA ) at 1 . 4 GHz and 5 GHz , as also as archival measurements obtained by the Very Large Array ( VLA ) , in order to study the radio behavior of the z = 4 . 4 quasar BRI1335 - 0417 .We detect two compact components joined by 0 . 3 arcsec on both sides of the optical position of this source . The total flux concentration is S5GHz = ( 1 . 6 ±0 . 2 ) mJy for component A and S1 . 4Ghz = 0 . 7±0 . 1 mJy for component B .Both sources are unresolved within our laser size . From their spectral indices we estimate that they are most likely correlated with an active galactic nucleus .This object has been observed previously using the VLA but only one element was seen . Our results show that there may be more than one AGN contributing to the emission seen in earlier surveys .",
        "rewrite_text": "We present findings from our recent investigations utilizing the High Sensitivity Array (HSA) at frequencies of 1.4 GHz and 5 GHz, alongside archival data from the Very Large Array (VLA), to analyze the radio emissions of the quasar BRI 1335-0417 at redshift z = 4.4. Our observations reveal the presence of two distinct compact radio components located approximately 0.3 arcseconds from the optical position of the quasar. The total flux densities measured are S5GHz = (1.6 ± 0.2) mJy for component A and S1.4GHz = (0.7 ± 0.1) mJy for component B. Notably, both components remain unresolved within our observational limits, indicating their compact nature. By examining their spectral indices, we infer that these components are likely associated with an active galactic nucleus (AGN). Previous VLA observations of this quasar had only identified a single component, suggesting that our current findings may indicate the presence of multiple AGNs contributing to the radio emissions detected in earlier studies. This research enhances our understanding of the radio properties of high-redshift quasars and suggests a more complex structure than previously recognized, potentially reshaping our models of AGN activity in the early universe. Our results underscore the importance of high-sensitivity observations in uncovering the intricate details of distant astronomical sources.",
        "ori-fast-z-score": -1.4832396974191326,
        "water-fast-z-score": 3.2071349029490928,
        "rewrite-fast-z-score": -1.193117518002609
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Nonequilibrium entropy limiters in lattice Boltzmann methods .\nAbstract:\nWe present an approach to nonequilibrium entropy production in the lattice Boltzmann (LB) method by introducing entropy limiters into the collision operator. The proposed scheme is shown to be able to reproduce the correct equilibrium distribution and recover the second law of thermodynamics for both single-phase flows with constant density and temperature, as well as multiphase flows with phase change. We also demonstrate that our new LB model can accurately capture shock waves without spurious oscillations or numerical instabilities. \n \n Keywords: Nonequilibrium entropy, Lattice Boltzmann Method, Entropy limiter, Second Law of Thermodynamics, Shock wave. 1 Introduction \n \n In recent years, there has been growing interest in developing computational fluid dynamics methods based on kinetic theory  1–3  . Compared with conventional Navier-Stokes solvers, these approaches are more accurate at capturing complex flow phenomena such as shocks  4  , turbulence  5  , and interfacial flows  6  . Among them, the lattice Boltzmann method  7, 8  has attracted much attention due to its simplicity and efficiency  9  . \n \n However, it should be noted that most existing LB models do not satisfy the second law of thermodynamic  10  . This problem becomes particularly severe when dealing with high Mach number flows  11  . To overcome this difficulty, several attempts have been made recently  12–18  . For example, Chen et al.  12  introduced a modified BGK-type collision term which recovers the correct equilibrium state while satisfying the second law of thermodynamical. Similarly, Yu et al.  13  developed another type of entropy-consistent LB schemes using the concept of entropic moments. More recently, Shan et al.  14  presented a novel LB model where the relaxation time was determined according to the local Knudsen number. Although these works provide promising results, they all require additional information about the macroscopic variables, e.g., pressure and velocity fields. As a result, their applications may be limited to simple cases involving only one component gas. \n \n In contrast, we propose here a general framework for constructing entropy-consistent LB models. Our strategy relies on adding",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Nonequilibrium entropy limiters in lattice Boltzmann techniques . Abstract : We present an approach to nonequilibrium entropy production in the lattice Boltzmann ( LB ) method by using entropy limiters into the collision operator .The proposed system is demonstrated to be possible to reproduce the correct equilibrium distribution and recover the second law of thermodynamics for both single - phase streams with constant density and heat , as well as multiphase streams with phase change . We additionally prove that our new LB model can accurately capture shock waves without spurious oscillations or numerical instabilities .Keywords : Nonequilibrium entropy , Lattice Boltzmann Method , Entropy limiter , Second Law of Thermodynamics , Shock wave . 1 Introduction In recent seasons , there has been growing interest in establishing computational liquid mechanics algorithms based on kinetic theory 1 – 3 .Compared with typical Navier - Stokes solvers , these method are more accurate at representing complex flow effects such as shocks 4 , turbulence 5 , and interfacial flows 6 . Among them , the lattice Boltzmann technique 7 , 8 has garnered great controversy due to its accuracy and efficiency 9 .However , it should be mentioned that most existing LB models do not satisfy the second law of thermodynamic 10 . This problem appears particularly severe when dealing with high Mach number flows 11 .To solve this challenge , various efforts have been attempted recently 12 – 18 . For instance , Chen et al .12 introduced a revised BGK - class collision term which recovers the correct equilibrium state while fulfilling the second law of thermodynamical . Similarly , Yu et al .13 introduced another type of entropy - consistent LB schemes using the idea of entropic moments . More recently , Shan et al .14 provided a new LB model where the relaxation time was decided according to the local Knudsen number . Although these works provide encouraging conclusions , they all need extra data about the macroscopic parameters , e . g . , pressure and speed fields .As a result , their applications might be restricted to small cases concerning only one element gas . In comparison , we require here a general template for constructing entropy - consistent LB models .Our strategy relies on adding",
        "rewrite_text": "We introduce a novel approach to nonequilibrium entropy production within the framework of the lattice Boltzmann (LB) method by incorporating entropy limiters into the collision operator. This innovative system is shown to effectively reproduce the correct equilibrium distribution and uphold the second law of thermodynamics across various scenarios, including single-phase flows characterized by constant density and heat, as well as multiphase flows involving phase changes. Furthermore, we demonstrate that our enhanced LB model is capable of accurately capturing shock waves without introducing spurious oscillations or numerical instabilities, which are common challenges in computational fluid dynamics.\n\nRecent advancements in computational fluid mechanics have sparked significant interest in developing algorithms rooted in kinetic theory, particularly due to their superior accuracy in modeling complex flow phenomena such as shocks, turbulence, and interfacial dynamics. The lattice Boltzmann method has emerged as a prominent technique in this domain, praised for its efficiency and precision. However, a critical limitation of many existing LB models is their failure to comply with the second law of thermodynamics, a concern that becomes particularly pronounced in high Mach number flows. To address this issue, various strategies have been proposed, including modifications to the BGK collision term and the introduction of entropy-consistent schemes based on entropic moments. Despite these advancements, many of these approaches require additional data regarding macroscopic parameters, which can limit their applicability to simpler scenarios involving single-component gases.\n\nIn contrast, our work aims to establish a more generalized framework for constructing entropy-consistent LB models that do not depend on specific macroscopic information. By integrating entropy limiters into the collision operator, we provide a robust solution that enhances the fidelity of the lattice Boltzmann method in simulating nonequilibrium processes while ensuring compliance with thermodynamic principles. This research not only contributes to the theoretical understanding of nonequilibrium entropy in LB techniques but also opens avenues for more accurate and stable simulations in complex fluid dynamics applications. \n\nKeywords: Nonequilibrium entropy, Lattice Boltzmann Method, Entropy limiter, Second Law of Thermodynamics, Shock wave.",
        "ori-fast-z-score": -2.0179913668364655,
        "water-fast-z-score": 6.577497476540163,
        "rewrite-fast-z-score": -0.2349781349963872
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The AGN and Gas Disk in the Low Surface Brightness Galaxy PGC045080 .\nAbstract:\nWe present new observations of the low surface brightness galaxy PGC 045080, which is known to host an active galactic nucleus (AGN). We use these data to study the properties of this AGN as well as its relationship with the surrounding gas disk.  The AGN has been detected by previous studies at radio wavelengths using Very Large Array (VLA) observations. In our work we have used VLA archival data along with new observations made with the Karl G. Jansky Very Large Array (JVLA), to detect emission lines associated with the AGN. These include H-alpha,  NII  ,  SII  ,  OIII  , and  CII  . Using these line fluxes we calculate the luminosity of the AGN to be 1.1 x 10^41 erg/sec. This value agrees very closely with that found for other similar galaxies. We also find evidence for outflows on both large and small scales around the AGN.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The AGN and Gas Disk in the Low Surface Brightness Galaxy PGC045080 . Abstract : We report new studies of the low surface brightness universe PGC 045080 , which is known to host an active galactic nucleus ( AGN ) .We use these information to study the properties of this AGN as also as its connection with the nearby gas disk . The AGN has been detected by earlier surveys at radio wavelengths using Very Large Array ( VLA ) observations .In our work we have utilized VLA archival data along with new experiments done with the Karl G . Jansky Very Large Array ( JVLA ) , to identify emission lines associated with the AGN . These include H - alpha , NII , SII , OIII , and CII .Using these line fluxes we determine the luminosity of the AGN to be 1 . 1 x 10 ^ 41 erg / sec . This value agrees very closely with that obtained for other similar galaxies .We additionally find proof for outflows on both large and tiny scales around the AGN .",
        "rewrite_text": "We present a comprehensive study of the low surface brightness galaxy PGC 045080, which is recognized for hosting an active galactic nucleus (AGN). This research aims to explore the characteristics of the AGN and its relationship with the adjacent gas disk. Previous surveys utilizing the Very Large Array (VLA) have successfully detected the AGN at radio wavelengths. In our investigation, we have leveraged both archival VLA data and new observations conducted with the Karl G. Jansky Very Large Array (JVLA) to identify various emission lines linked to the AGN. The emission lines we analyzed include H-alpha, NII, SII, OIII, and CII. By measuring the flux of these lines, we calculated the AGN's luminosity to be approximately 1.1 x 10^41 erg/sec, a value that aligns closely with those found in other galaxies of similar characteristics. Furthermore, our findings provide evidence of outflows occurring on both large and small scales surrounding the AGN. This study not only enhances our understanding of the AGN in PGC 045080 but also contributes to the broader knowledge of the interactions between AGNs and their host galaxies, particularly in the context of low surface brightness environments. The implications of these results may shed light on the role of AGNs in galaxy evolution and the dynamics of gas disks in such systems.",
        "ori-fast-z-score": -0.8819171036881969,
        "water-fast-z-score": 4.093146241443879,
        "rewrite-fast-z-score": 1.2939932784412609
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Algebraic charge liquids .\nAbstract:\nWe introduce the concept of algebraic charge liquids, which are defined as ground states of Hamiltonians with local interactions that can be written in terms of fermionic creation and annihilation operators. We show how to construct such models for any finite group G by using an explicit representation of G on the Hilbert space of spinless fermions. The resulting model is exactly solvable when G has no non-trivial subgroups. In this case we find that there exists at least one phase transition between different phases characterized by distinct topological orders. For example, if G = Z2 × Z2 then our construction yields two gapped phases distinguished by their chiral central charges c− = 0 or 1. If G contains a nontrivial subgroup H then the system exhibits gapless excitations corresponding to particles transforming according to irreducible representations (irreps) of H. These results provide new insights into the classification problem of quantum many-body systems.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Algebraic charge liquids . Abstract : We introduce the idea of algebraic charge liquids , which are specified as ground states of Hamiltonians with local interactions that can be written in terms of fermionic creation and annihilation operators .We see how to build such theories for any finite class G by using an explicit representation of G on the Hilbert space of spinless fermions . The resulting theory is precisely solvable when G has no non - trivial subgroups .In this instance we find that there exists at least one phase shift between various phases characterized by separate topological orders . For instance , if G = Z2 × Z2 then our build produces two gapped phases distinguished by their chiral central charges c− = 0 or 1 .If G contains a nontrivial subgroup H then the scheme exhibits gapless excitations corresponding to particles transforming according to irreducible representations ( irreps ) of H . These data provide fresh insights into the classification question of quantum several - bodies systems .",
        "rewrite_text": "In this article, we present the concept of algebraic charge liquids, which are defined as the ground states of Hamiltonians characterized by local interactions expressible through fermionic creation and annihilation operators. We demonstrate a method for constructing such theories for any finite group \\( G \\) by employing a specific representation of \\( G \\) on the Hilbert space of spinless fermions. Notably, when \\( G \\) lacks non-trivial subgroups, the resulting theory is exactly solvable. In this scenario, we observe the emergence of at least one phase shift among various phases, each distinguished by unique topological orders. For example, when \\( G = \\mathbb{Z}_2 \\times \\mathbb{Z}_2 \\), our construction yields two distinct gapped phases, which can be differentiated by their chiral central charges, taking values of \\( c^- = 0 \\) or \\( 1 \\). Conversely, if \\( G \\) includes a non-trivial subgroup \\( H \\), the framework reveals gapless excitations that correspond to particles transforming according to the irreducible representations (irreps) of \\( H \\). This exploration not only enhances our understanding of algebraic charge liquids but also contributes valuable insights into the classification of quantum many-body systems. The findings underscore the intricate relationship between group theory and quantum states, paving the way for further investigations into the properties and behaviors of these exotic phases of matter. Overall, our work lays the groundwork for future research aimed at unraveling the complexities of topological orders and their implications in condensed matter physics.",
        "ori-fast-z-score": -1.885618083164127,
        "water-fast-z-score": 4.330522446256832,
        "rewrite-fast-z-score": 0.4
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Transcritical bifurcations in non-integrable Hamiltonian systems .\nAbstract:\nWe study the dynamics near transcritical bifurcation points for non integrable Hamiltonians with two degrees of freedom, using perturbation theory and numerical simulations.  We show that there is an infinite number of periodic orbits which are born at the bifurcation point as well as infinitely many families of quasi-periodic solutions. The latter have been found numerically by computing Lyapunov exponents along trajectories starting close to the equilibrium position corresponding to the unperturbed problem. \nThe results presented here can be applied to several physical systems such as Josephson junctions or pendula coupled through elastic elements. Transcritical bifurcations occur when a stable fixed point loses its stability under small perturbations. In this case, new types of attractors appear nearby, including limit cycles and chaotic sets. This phenomenon has been studied extensively both theoretically and experimentally (see e.g.,  1  ,  3  ). However, most studies were restricted to low dimensional dynamical systems where it was possible to obtain explicit expressions for all relevant quantities. Here we consider higher-dimensional problems where only approximate methods exist. Our main motivation comes from nonlinear physics, but our results may also apply to other fields like biology or economics.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Transcritical bifurcations in non - integrable Hamiltonian systems . Abstract : We research the dynamics near transcritical bifurcation points for non integrable Hamiltonians with two degrees of freedom , using perturbation theory and numerical simulations .We see that there is an endless number of periodic orbits which are born at the bifurcation point as well as infinitely many families of quasi - periodic solutions . The last have been detected numerically by computing Lyapunov exponents along trajectories starting nearer to the equilibrium place analogous to the unperturbed question .The results presented here can be applied to several physical structures such as Josephson junctions or pendula connected through elastic elements . Transcritical bifurcations occur when a stable fixed point losing its stability under small perturbations .In this instance , new types of attractors emerge adjacent , including limit paths and chaotic sets . This phenomenon has been studied thoroughly both theoretically and experimentally ( saw e . g . , 1 , 3 ) .However , most studies were restricted to low dimensional dynamical systems where it was easy to obtain precise expressions for all relevant quantities . Here we study greater - dimensional problems where only approximate approaches exist .Our main reasoning comes from nonlinear science , but our findings may sometimes applicable to other fields like science or economics .",
        "rewrite_text": "In this article, we investigate the dynamics surrounding transcritical bifurcation points in non-integrable Hamiltonian systems characterized by two degrees of freedom. Utilizing both perturbation theory and numerical simulations, we uncover a rich structure of periodic orbits that emerge at the bifurcation point, alongside an infinite array of quasi-periodic solutions. The latter are identified through numerical computations of Lyapunov exponents along trajectories that initiate close to the equilibrium position, analogous to the unperturbed scenario. Our findings have significant implications for various physical systems, including Josephson junctions and pendulums connected by elastic elements. \n\nTranscritical bifurcations are critical phenomena that occur when a stable fixed point loses its stability due to minor perturbations, leading to the emergence of new attractors, such as limit cycles and chaotic sets. While this phenomenon has been extensively studied both theoretically and experimentally, particularly in low-dimensional dynamical systems where precise expressions for relevant quantities can be derived, our research extends this exploration to higher-dimensional systems. In these more complex scenarios, only approximate methods are available, making our study particularly relevant. \n\nThe insights gained from our analysis are rooted in the principles of nonlinear science, yet they may also find applications in other domains, including physics and economics. By broadening the understanding of transcritical bifurcations in non-integrable Hamiltonian systems, we contribute to the ongoing discourse in dynamical systems theory and its applications across various scientific fields.",
        "ori-fast-z-score": -0.39605901719066977,
        "water-fast-z-score": 5.491251783869154,
        "rewrite-fast-z-score": 1.7873696499288347
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Free zero-range processes on networks .\nAbstract:\nWe study the dynamics of free fermions hopping between sites of an arbitrary connected graph, with no restriction to nearest-neighbor hopping. We show that this system is equivalent to a collection of independent random walks evolving in parallel and interacting via pairwise collisions at vertices. The collision rate depends only on the number of particles present at each vertex; it vanishes for graphs without loops or multiple edges (e.g., trees), but can be arbitrarily large otherwise. This model exhibits interesting behavior even when all rates are equal, including anomalous diffusion and superdiffusion. In particular, we prove that the mean-square displacement grows as t3/2 for any tree-like graph, while it scales faster than t2/3 for general graphs. Finally, we discuss possible extensions of our results beyond the free-fermion case. Introduction: A wide variety of physical phenomena ranging from quantum transport through mesoscopic systems  1  , to population biology  2  , involve non-equilibrium particle dynamics on networks. These models typically assume that particles move along directed links according to some prescribed rules, such as unrestricted hopping  3  . However, many real-world situations require more complicated interactions among particles  4  .\nIn this work, we consider a simple generalization of standard one-dimensional lattice models  5  by allowing particles to hop freely between adjacent nodes of an arbitrary connected graph G = (V, E). More precisely, let us fix a finite set S of states associated with each node v ∈ V ; then, given a configuration c : V → S, we define the state space C(G) := {c: V → S}. For every edge e = {u, v} ∈ E, we associate two transition probabilities p+(c, c )(e) ≥ 0 and p−(c, c )(u, v) > 0; these represent the probability per unit time that a particle located at u jumps to v if its current state is c, and vice versa. Then, the evolution of the system is described by a continuous-time Markov process Xt taking values in C(G).\nThe main goal of this Letter is to analyze the",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Free zero - range systems on networks . Abstract : We research the dynamics of free fermions hopping between locations of an arbitrary linked graph , with no limitation to nearest - neighbor hopping .We see that this scheme is analogous to a collection of independent random walks evolving in parallel and interacting via pairwise collisions at vertices . The crash time depends only on the number of particles present at each vertex ; it vanishes for graphs without loops or multiple edges ( e . g . , trees ) , but can be arbitrarily small otherwise .This theory exhibits exciting phenomena even when all rates are equal , notably anomalous absorption and superdiffusion . In particular , we prove that the mean - square displacement grows as t3 / 2 for any tree - like graph , while it scales higher than t2 / 3 for general graphs .Finally , we explain possible extensions of our findings beyond the free - fermion case . Introduction : A wide multitude of natural experiments ranging from particle transport through mesoscopic systems 1 , to population physics 2 , use non - equilibrium molecule interactions on networks .These systems often assume that particles moving along directed networks according to some prescribed rules , such as unrestricted hopping 3 . However , many actual - time situations involve more complicated relationships among interactions 4 .In this study , we define a simple generalization of standard one - dimensional lattice models 5 by using particles to hop freely between neighboring vertices of an arbitrary linked graph G = ( V , E ) . More specifically , let us fix a finite collection S of states associated with each node v ∈ V ; then , given a configuration b : V → S , we define the state collection C ( G ) : = { c : V → S } .For every edge e = { u , v } ∈ E , we associate two transition probabilities p + ( c , c ) ( e ) ≥ 0 and p− ( c , c ) ( v , v ) > 0 ; these denote the probability per unit time that a particle situated at u jumps to v if its current state is c , and vice versa . Then , the evolution of the system is characterized by a continuous - time Markov process Xt taking values in C ( G ) .The main goal of this Letter is to analyze the",
        "rewrite_text": "**Title:** Free Zero-Range Systems on Networks\n\n**Abstract:** This study investigates the dynamics of free fermions as they hop between various locations on an arbitrary linked graph, without being restricted to nearest-neighbor interactions. We draw an analogy between this system and a series of independent random walks that evolve concurrently, with interactions occurring through pairwise collisions at the vertices of the graph. The time until collisions, or crash time, is influenced solely by the number of particles at each vertex. Notably, this crash time is zero for graphs that lack loops or multiple edges, such as trees, while it can be significantly reduced in other graph structures. Our theoretical framework reveals intriguing phenomena, even under conditions where all rates are uniform, including anomalous absorption and superdiffusion. Specifically, we demonstrate that the mean-square displacement of particles grows as \\( t^{3/2} \\) for any tree-like graph, while for more general graphs, it scales higher than \\( t^{2/3} \\). Furthermore, we discuss potential extensions of our results beyond the free-fermion paradigm, suggesting avenues for future research. \n\n**Introduction:** A diverse range of natural experiments, from particle transport in mesoscopic systems to studies in population dynamics, explore non-equilibrium interactions of molecules on networks. These systems typically assume that particles traverse directed networks according to specific rules, such as unrestricted hopping. However, real-world scenarios often involve more complex interaction dynamics. In this paper, we propose a straightforward generalization of conventional one-dimensional lattice models by allowing particles to freely hop between neighboring vertices of an arbitrary linked graph \\( G = (V, E) \\). We define a finite set \\( S \\) of states for each node \\( v \\in V \\), and for a given configuration \\( b: V \\to S \\), we establish the state collection \\( C(G) = \\{ c: V \\to S \\} \\). For each edge \\( e = \\{ u, v \\} \\in E \\), we assign two transition probabilities \\( p^+(c, c)(e) \\geq 0 \\) and \\( p^-(c, c)(v, v) > 0 \\), which represent the probability per unit time that a particle at \\( u \\) jumps to \\( v \\) given its current state \\( c \\), and vice versa. The evolution of the system is modeled as a continuous-time Markov process \\( X_t \\) taking values in \\( C(G) \\). The primary objective of this letter is to analyze the implications of this model.",
        "ori-fast-z-score": -1.5439507063969962,
        "water-fast-z-score": 6.543410136634889,
        "rewrite-fast-z-score": 1.906412495277593
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Uncovering the Near-IR Dwarf Galaxy Population of the Coma Cluster with Spitzer IRAC .\nAbstract:\nWe present new near-infrared (NIR) observations of the Coma cluster using the Infrared Array Camera on board the Spitzer Space Telescope, which allow us to study the dwarf galaxy population in this rich environment for the first time at wavelengths longer than 1 micron. We identify and classify all galaxies detected within an area of 0.5 deg2 centered around the center of the Coma cluster down to a limiting magnitude of Ks = 18 mag. The majority of these sources are faint red galaxies that have been missed by previous optical surveys due to their low surface brightnesses. Using photometric redshift estimates we find that most of them lie between z=0.1 and z=1.0. By comparing our sample to existing spectroscopic data sets we show that our NIR selection is complete up to M* ~ - 17 + 5 log h70. This corresponds roughly to L*(z=0), but it should be noted that there may still exist some fainter dwarfs below our detection limit.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Uncovering the Near - IR Dwarf Galaxy Population of the Coma Cluster with Spitzer IRAC . Abstract : We report new near - infrared ( NIR ) observations of the Coma cluster using the Infrared Array Camera on board the Spitzer Space Telescope , which allow us to study the dwarf galaxy community in this rich environment for the first time at wavelengths greater than 1 micron .We recognize and classify all galaxies found within an area of 0 . 5 deg2 centered around the center of the Coma cluster down to a limiting magnitude of Ks = 18 mag . The majority of these sources are faint red clusters that have been missed by earlier optical searches due to their low exterior brightnesses .Using photometric redshift estimates we find that most of them lie between z = 0 . 1 and z = 1 . 0 . By comparing our sample to existing spectroscopic data sets we determine that our NIR selection is complete up to M * ~ - 17 + 5 log h70 .This equals roughly to L * ( z = 0 ) , but it should be mentioned that there may still exist some fainter dwarfs below our detection limit .",
        "rewrite_text": "We present new near-infrared (NIR) observations of the Coma cluster, conducted using the Infrared Array Camera (IRAC) aboard the Spitzer Space Telescope. This study marks the first comprehensive examination of the dwarf galaxy population within this rich cluster environment at wavelengths exceeding 1 micron. Our analysis encompasses a region of 0.5 square degrees centered on the Coma cluster, where we identify and classify all galaxies down to a limiting magnitude of Ks = 18 mag. A significant portion of the detected sources consists of faint red galaxies that previous optical surveys overlooked due to their low surface brightness. Through the application of photometric redshift techniques, we ascertain that the majority of these galaxies are situated within the redshift range of z = 0.1 to z = 1.0. To evaluate the completeness of our NIR selection, we compare our findings with existing spectroscopic datasets, concluding that our observations are complete for galaxies with absolute magnitudes M* approximately equal to -17 + 5 log h70. This threshold corresponds closely to the characteristic luminosity L* at z = 0. However, it is important to note that there may still be fainter dwarf galaxies present that fall below our detection capabilities. Our results contribute valuable insights into the dwarf galaxy population in the Coma cluster, enhancing our understanding of galaxy formation and evolution in dense environments.",
        "ori-fast-z-score": 0.9847319278346618,
        "water-fast-z-score": 4.608176875690327,
        "rewrite-fast-z-score": 1.3199500146737049
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Chasing Brane Inflation in String-Theory .\nAbstract:\nThe idea that the universe underwent an inflationary phase at very early times is now widely accepted by cosmologists as one possible explanation for many observed features of our present-day universe, such as its flatness and homogeneity.  In this talk I will discuss how string-theoretic ideas can be used to construct models of inflation which are consistent with current observations. The basic idea behind these constructions is that they involve branes moving through extra dimensions of space-time. These branes may either move towards each other or away from each other depending on their initial conditions. If two branes collide then it leads to a rapid expansion of spacetime known as  inflation . This process continues until the branes separate again and the universe reverts back to its original size. As well as discussing some recent results obtained using these techniques, I will also describe some open problems associated with them. Finally, I will briefly mention another approach to constructing realistic models of inflation based upon so-called  brane-world scenarios .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Chasing Brane Inflation in String - Theory . Abstract : The idea that the universe underwent an inflationary phase at very earliest times is now widely accepted by cosmologists as one possible reason for numerous observed features of our present - day universe , such as its flatness and homogeneity .In this talk I will explore how string - theoretic theories can be used to build theories of inflation which are compatible with current observations . The basic idea behind these concepts is that they use branes moving through additional dimensions of space - time .These branes may either shift towards each other or away from each other depending on their early conditions . If two branes collide then it leads to a rapid increase of spacetime termed as inflation .This process proceeds until the branes separate again and the universe reverts back to its old length . As well as explaining some latest findings obtained using these experiments , I will also describe some open problems involved with them .Finally , I will briefly include another methodology to creating accurate theories of inflation centered upon so - called brane - global conditions .",
        "rewrite_text": "Title: Investigating Brane Inflation in String Theory\n\nAbstract: The concept of an inflationary phase in the early universe has gained significant traction among cosmologists, serving as a potential explanation for various observed characteristics of our current universe, including its flatness and uniformity. In this presentation, I will delve into how string theory can be utilized to formulate inflationary models that align with contemporary observational data. Central to these models is the notion of branes, which are multidimensional objects that traverse additional spatial dimensions. The dynamics of these branes, which can either approach or recede from one another based on their initial conditions, play a crucial role in the inflationary process. A collision between two branes results in a rapid expansion of spacetime, a phenomenon referred to as inflation. This expansion continues until the branes separate, at which point the universe returns to its previous scale. In addition to discussing recent experimental findings that support these theories, I will also address several unresolved issues within this framework. Furthermore, I will introduce an alternative approach to developing robust inflationary theories that focus on the so-called brane-global conditions, highlighting the potential for new insights into the mechanics of inflation in the context of string theory.",
        "ori-fast-z-score": 0.31799936400190804,
        "water-fast-z-score": 6.324555320336758,
        "rewrite-fast-z-score": 2.9285611805518585
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Diversity of MIMO Multihop Relay Channels - Part I: Amplify-and-Forward .\nAbstract:\nIn this work, we study the diversity-multiplexing tradeoff (DMT) for amplify-andforward multihop relay channels with multiple-input and multiple-output (MIMO). We first derive an upper bound on DMT by using the cut-set theorem. Then, based on the derived upper bound, we propose two schemes to achieve the optimal DMT performance in different scenarios. In particular, when all relays have full-duplex capability, our proposed scheme achieves the optimal DMT at high multiplexing gain region; while if only half-duplex relays are available, then our proposed scheme can still achieve the optimal DMT except that it is achieved at low multiplexing gain region instead. Finally, simulation results show that both proposed schemes outperform existing ones significantly. Index Terms-Diversity-Multiplexing Tradeoff, Multiple-Input MultipleOutput, Amplify-and-Forward",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Diversity of MIMO Multihop Relay Channels - Part I : Amplify - and - Forward . Abstract : In this project , we study the diversity - multiplexing tradeoff ( DMT ) for amplify - andforward multihop relay channels with many - input and multiple - output ( MIMO ) .We first derive an upper bound on DMT by using the cutting - set theorem . Then , relying on the derived upper bound , we undertake two strategies to achieve the ideal DMT performance in different scenarios .In particular , when all relays have full - duplex capability , our proposed scheme achieves the optimal DMT at high multiplexing gain region ; while if only half - duplex relays are available , then our proposed scheme can also achieve the optimal DMT except that it is achieved at low multiplexing gain region instead . Finally , simulation results show that both proposed schemes outperform previous ones significantly .Index Terms-Diversity-Multiplexing Tradeoff, Multiple-Input MultipleOutput, Amplify-and-Forward",
        "rewrite_text": "Title: Diversity of MIMO Multihop Relay Channels - Part I: Amplify-and-Forward\n\nAbstract: This study investigates the diversity-multiplexing tradeoff (DMT) in amplify-and-forward multihop relay channels utilizing multiple-input multiple-output (MIMO) systems. We begin by establishing an upper bound for the DMT through the application of the cutting-set theorem, which serves as a foundational element for our analysis. Building on this upper bound, we propose two distinct strategies aimed at achieving optimal DMT performance under varying conditions. Specifically, in scenarios where all relays possess full-duplex capabilities, our proposed approach successfully attains the optimal DMT in the high multiplexing gain region. Conversely, in cases where only half-duplex relays are employed, our method still achieves the optimal DMT, albeit in the low multiplexing gain region. The effectiveness of our proposed schemes is further validated through simulation results, which demonstrate a significant improvement over existing methods. This research contributes to the understanding of DMT in MIMO multihop relay channels and provides practical strategies for optimizing performance in diverse relay configurations. \n\nIndex Terms: Diversity-Multiplexing Tradeoff, Multiple-Input Multiple-Output, Amplify-and-Forward.",
        "ori-fast-z-score": 0.6108472217815261,
        "water-fast-z-score": 2.80989722019502,
        "rewrite-fast-z-score": 1.4814874939752933
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Continuous Magnetophoretic Separation of Blood Cells from Plasma at the Microscale .\nAbstract:\nMagnetic separation is an important tool in biomedical research and clinical diagnostics, but it has been limited to macroscopic devices that are not suitable for point-of-care applications. Here we report on continuous magnetophoresis-based blood cell sorting using microfluidics. We demonstrate efficient separation of red blood cells (RBCs) from plasma by applying a magnetic field gradient across a microchannel containing RBCs suspended in buffer solution. The results show that our method can be used as a simple yet effective approach for separating different types of blood cells with high purity and efficiency. This work may have significant implications towards developing portable diagnostic tools based on microscale blood processing technologies. Magnetic separation techniques play an important role in many fields including medicine, biotechnology, environmental science, food industry etc.,  1  . However, most existing methods require bulky equipment which makes them unsuitable for use outside laboratory settings  2  .\nRecently there has been growing interest in miniaturizing these systems into lab-on-a-chip platforms  3  , where various functionalities such as sample preparation  4  , chemical analysis  5  , drug delivery  6  , and bioassays  7  could be integrated onto one single chip. In particular, magnetic separators have attracted much attention due to their simplicity, low cost, portability, and compatibility with other microfabricated components  8  . For example, several groups have demonstrated magnetic separation of biological samples inside microchannels  9  -  11  or on planar surfaces  12  -  14  . Despite this progress, however, current approaches still suffer from some limitations. First, they typically rely on batch-wise operation mode  15  , which limits throughput and requires large volumes of input samples  16  . Second, the majority of reported designs only allow for separation between two distinct populations  17  , while more complex mixtures involving multiple species cannot be processed simultaneously  18  . Third, the fabrication process usually involves complicated multi-step procedures  19  , making it difficult to integrate additional functions  20  . Finally, most previous studies were performed under static conditions  21  , which limit the flexibility of device design  22  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Continuous Magnetophoretic Separation of Blood Cells from Plasma at the Microscale . Abstract : Magnetic isolation is an important tool in biomedical research and medical diagnostics , but it has been limited to macroscopic devices that are not suitable for point - of - care applications .Here we study on intensive magnetophoresis - based blood cell sorting using microfluidics . We suggest efficient separation of red blood cells ( RBCs ) from blood by using a magnetic field gradient across a microchannel containing RBCs held in buffer solution .The results show that our technique can be used as a simple however effective methods for dividing different kinds of blood tissue with high purity and efficiency . This research could have considerable consequences towards developing portable diagnostic methods using on microscale blood extraction technologies .Magnetic isolation techniques serve an important role in multiple fields including medicine , biotechnology , ecological studies , nutrition industry etc . , 1 . However , most existing techniques require bulky machinery which makes them unsuitable for use outside laboratory settings 2 .Recently there has been growing interest in miniaturizing these systems into lab - on - a - chip platforms 3 , where various functionalities such as sample preparation 4 , chemical analysis 5 , drug delivery 6 , and bioassays 7 could be integrated onto one single chip . In particular , magnetic separators have attracted much attention due to their simplicity , low cost , portability , and compatibility with other microfabricated components 8 .For instance , various groups have demonstrated magnetic separation of biological samples inside microchannels 9 - 11 or on planar materials 12 - 14 . Despite this progress , however , current approaches still suffer from some restrictions .First , they generally rely on batch - wise operation mode 15 , which reduces throughput and requires large quantities of input specimens 16 . Second , the majority of reported prototypes only require for isolation between two separate populations 17 , while more sophisticated mixtures featuring multiple taxa unable be processed concurrently 18 .Third , the fabrication process usually includes complicated multi - phase techniques 19 , making it difficult to connect extra functions 20 . Finally , most prior studies were performed under static conditions 21 , which reduce the flexibility of device structure 22 .",
        "rewrite_text": "**Title:** Continuous Magnetophoretic Separation of Blood Cells from Plasma at the Microscale\n\n**Abstract:** Magnetic isolation techniques are pivotal in the realms of biomedical research and medical diagnostics, yet their application has predominantly been confined to large-scale devices that are impractical for point-of-care settings. This study explores the use of intensive magnetophoresis for the sorting of blood cells through microfluidic systems. We propose an innovative method for the efficient separation of red blood cells (RBCs) from plasma by employing a magnetic field gradient within a microchannel filled with a buffer solution containing RBCs. Our findings indicate that this technique offers a straightforward yet highly effective means of isolating various blood components with remarkable purity and efficiency. The implications of this research are significant, particularly in the advancement of portable diagnostic tools that leverage microscale blood extraction technologies.\n\nMagnetic isolation is integral across diverse fields, including medicine, biotechnology, ecological research, and the nutrition industry. However, traditional methods often necessitate cumbersome equipment, rendering them unsuitable for applications beyond laboratory environments. Recently, there has been a surge of interest in miniaturizing these systems into lab-on-a-chip platforms, which allow for the integration of multiple functionalities such as sample preparation, chemical analysis, drug delivery, and bioassays onto a single chip. Magnetic separators, in particular, have garnered attention for their simplicity, cost-effectiveness, portability, and compatibility with other microfabricated components.\n\nDespite advancements in the field, existing methods face several limitations. Most notably, they typically operate in a batch-wise mode, which constrains throughput and necessitates substantial input volumes. Additionally, many prototypes are designed for the isolation of only two distinct populations, limiting their ability to process complex mixtures containing multiple cell types simultaneously. The fabrication of these devices often involves intricate multi-phase techniques, complicating the integration of additional functionalities. Furthermore, prior studies have predominantly been conducted under static conditions, which restricts the adaptability of the device architecture. Our research addresses these challenges, paving the way for more versatile and efficient blood cell separation methods suitable for real-world applications.",
        "ori-fast-z-score": 0.5146502354656654,
        "water-fast-z-score": 8.994650816763707,
        "rewrite-fast-z-score": 0.909717652294684
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The origin of the molecular emission around the southern hemisphere Re 4 IRS - HH 188 region .\nAbstract:\nWe present new observations in the mid-infrared (MIR) and near-infrared (NIR), as well as archival data, to study the physical conditions of the gas surrounding the young stellar object Re 4 IRS located at the center of the reflection nebula NGC 1333 IRAS 2. We find that the MIR spectrum is dominated by strong ro-vibrational lines of H$_2$O and CO, while the NIR spectrum shows prominent Brackett-γ line emission with an equivalent width of ~1 Å. The observed fluxes are consistent with those expected for a T Tauri star surrounded by a dense circumstellar disk heated by accretion shocks. In addition, we detect several forbidden transitions of  Fe II  ,  S III  ,  N II  , and  C II  . These results suggest that the central source has recently undergone a burst of enhanced mass loss activity which may be related to its recent transition into the main sequence phase.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The origin of the molecular emission around the southern hemisphere Re 4 IRS - HH 188 region . Abstract : We publish new observations in the mid - infrared ( MIR ) and near - infrared ( NIR ) , as well as archival data , to study the physical conditions of the gas surrounding the young stellar object Re 4 IRS located at the center of the reflection nebula NGC 1333 IRAS 2 .We see that the MIR spectrum is dominated by weak ro - vibrational lines of H $ _ 2 $ O and CO , while the NIR spectrum displays prominent Brackett - γ line emission with an comparable width of ~ 1 Å . The observed fluxes are compatible with those expected for a T Tauri star inhabited by a dense circumstellar disk heated by accretion shocks .In addition , we find various forbidden transitions of Fe II , S III , N II , and C II . These data suggest that the main source has recently undergone a burst of enhanced mass loss activity which may be connected to its recent shift into the main sequence phase .",
        "rewrite_text": "We present new mid-infrared (MIR) and near-infrared (NIR) observations, alongside archival data, to investigate the physical conditions of the gas surrounding the young stellar object Re 4 IRS, situated at the heart of the reflection nebula NGC 1333 IRAS 2. Our analysis reveals that the MIR spectrum is primarily characterized by weak ro-vibrational lines of water (H₂O) and carbon monoxide (CO). In contrast, the NIR spectrum exhibits significant Brackett-γ line emission, with a comparable width of approximately 1 Å. The observed flux levels align with expectations for a T Tauri star that is enveloped by a dense circumstellar disk, which is likely being heated by accretion shocks. Furthermore, our findings include various forbidden transitions of ions such as Fe II, S III, N II, and C II. These observations imply that the central source has recently experienced a surge in mass loss activity, potentially linked to its transition into the main sequence phase. This study enhances our understanding of the dynamic processes occurring in the vicinity of Re 4 IRS and contributes to the broader knowledge of star formation and the evolution of young stellar objects within reflection nebulae. The implications of these findings may provide insights into the mechanisms driving mass loss and the interactions between young stars and their surrounding environments.",
        "ori-fast-z-score": 0.4588314677411235,
        "water-fast-z-score": 4.302652729749464,
        "rewrite-fast-z-score": 2.424366106925306
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Possible Stellar Metallic Enhancement in Post-T Tauri Stars by a Planetesimal Bombardment .\nAbstract:\nWe propose that the metallic enhancement observed for some post T Tauri stars (PTTS) may be due to an accretion of planetesimals during their formation phase, which is followed by rapid planet formation and subsequent ejection of planets into space.  We show that this scenario can explain both the high metallicity found among PTTS as well as the low abundance ratios between refractory elements such as Mg/Si or Al/Si compared with those expected if these objects formed through standard core-accretion processes. The proposed mechanism also explains why there are no known close-in giant planets around PTTSs despite the fact that they have already passed their protoplanetary disk stage. This model predicts that most PTTS should host at least one Jupiter mass planet on wide orbits beyond 1 AU. In addition we predict that many PTTS will exhibit infrared excesses caused by dusty debris disks produced by collisions between planetary bodies.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Possible Stellar Metallic Enhancement in Post - T Tauri Stars by a Planetesimal Bombardment . Abstract : We suggest that the metallic enhancement detected for some post T Tauri stars ( PTTS ) may be due to an accretion of planetesimals during their formed phase , which is preceded by rapid planet development and subsequent ejection of stars into space .We see that this situation can describe both the high metallicity known among PTTS as also as the poor abundance proportions between refractory objects such as Mg / Si or Al / Si compared with those expected if these objects created through conventional core - accretion cycles . The proposed mechanism also explains why there are no known close - in massive planets around PTTSs despite the fact that they have already completed their protoplanetary disk stage .This theory predicts that most PTTS should accommodate at least one Jupiter mass planet on wide orbits beyond 1 AU . In addition we estimate that several PTTS will exhibit infrared excesses caused by dusty dust disks produced by collisions between planetary body .",
        "rewrite_text": "Title: A Potential Stellar Metallic Enhancement in Post-T Tauri Stars Due to Planetesimal Bombardment\n\nAbstract: In this study, we propose that the observed metallic enhancement in certain post-T Tauri stars (PTTS) may be attributed to the accretion of planetesimals during their formative stages. This process is thought to occur following rapid planetary formation and the subsequent ejection of stars into space. Our analysis indicates that this scenario can account for the elevated metallicity observed in PTTS, as well as the unusual abundance ratios of refractory elements, such as magnesium to silicon (Mg/Si) and aluminum to silicon (Al/Si), which deviate from the expectations derived from traditional core-accretion models. Furthermore, this hypothesis provides an explanation for the absence of known close-in massive planets around PTTS, despite their completion of the protoplanetary disk phase. According to our theory, it is anticipated that most PTTS should host at least one Jupiter-mass planet located on wide orbits, specifically beyond 1 astronomical unit (AU). Additionally, we estimate that several PTTS may display infrared excesses, which are indicative of dusty disks formed as a result of collisions between planetary bodies. This research contributes to our understanding of the formation and evolution of planetary systems around PTTS and highlights the significance of planetesimal bombardment in shaping the chemical composition and planetary architecture of these stars.",
        "ori-fast-z-score": -1.4142135623730951,
        "water-fast-z-score": 5.032769329433615,
        "rewrite-fast-z-score": -0.7337993857053429
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Extrasolar planet taxonomy: a new statistical approach .\nAbstract:\nWe present an analysis of the distribution of planetary orbital parameters and masses for all known extrasolar planets with measured mass (N=73). We find that this sample can be divided into three distinct groups, which we call super-Earths, mini-Neptunes, and hot Jupiters. The first two are characterized by low eccentricities and high mutual inclinations between their orbits; in contrast, hot Jupiters have small mutual inclinations but large eccentricities. Super-Earths and mini-Neptunes also appear to form separate sequences on the radius-mass diagram. These results suggest that these three populations may represent different formation mechanisms or evolutionary histories. In particular, our findings support previous suggestions that super-Earths formed via core accretion followed by significant inward migration while mini-Neptunes were likely assembled through gravitational instability. Finally, we show how these results can be used as priors when searching for additional exoplanets using transit surveys such as Kepler.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Extrasolar planet taxonomy : a new statistical method . Abstract : We report an assessment of the distribution of planetary orbital variables and masses for all known extrasolar planets with calculated mass ( N = 73 ) .We see that this specimen can be grouped into three different categories , which we call super - Earths , mini - Neptunes , and hot Jupiters . The first two are marked by low eccentricities and low mutual inclinations between their orbits ; in comparison , soft Jupiters have small mutual inclinations but large eccentricities .Super - Earths and mini - Neptunes also appear to form different sequences on the radius - mass graph . These data suggest that these three communities may indicate different formation factors or evolutionary histories .In particular , our findings support previous suggestions that super - Earths formed via core accretion followed by significant eastward migration while mini - Neptunes were likely formed through gravity instability . Finally , we indicate how these results can be used as priors when looking for additional exoplanets using transit surveys such as Kepler .",
        "rewrite_text": "In this study, we present a comprehensive analysis of the distribution of orbital characteristics and masses of all known extrasolar planets with determined mass, totaling 73. Our findings reveal that these planets can be classified into three distinct categories: super-Earths, mini-Neptunes, and hot Jupiters. Notably, super-Earths and mini-Neptunes exhibit low eccentricities and minimal mutual inclinations in their orbits, indicating a more stable orbital configuration. In contrast, hot Jupiters, while also showing low mutual inclinations, possess significantly higher eccentricities, suggesting a different dynamical evolution. \n\nFurthermore, our analysis of the radius-mass relationship indicates that super-Earths and mini-Neptunes occupy separate sequences, which may reflect their unique formation processes and evolutionary pathways. Specifically, our results lend support to the hypothesis that super-Earths are primarily formed through core accretion mechanisms, followed by substantial eastward migration within their protoplanetary disks. On the other hand, mini-Neptunes likely originate from gravitational instability, leading to their distinct characteristics.\n\nThese insights not only enhance our understanding of the formation and evolution of these planetary types but also have practical implications for future exoplanet discovery efforts. We propose that the classification and characteristics identified in this study can serve as valuable priors in the search for additional exoplanets, particularly in transit surveys such as the Kepler mission. By utilizing these statistical methods and categorizations, astronomers can refine their search strategies and improve the efficiency of identifying new exoplanets, thereby advancing the field of exoplanet research.",
        "ori-fast-z-score": -0.8728715609439696,
        "water-fast-z-score": 5.45544725589981,
        "rewrite-fast-z-score": -0.09090909090909091
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Patterns of dominant flows in the world trade web .\nAbstract:\nWe study patterns of global economic activity by analyzing the World Trade Web (WTW), which is an interconnected network that represents international transactions between countries. We find that the WTW exhibits scale-free and small-world properties, as well as community structure. The communities are characterized by their high density of links within themselves but low density with other communities. In addition to these structural features, we also examine how the flow of goods changes over time across different parts of the network. Our results show that there exist several distinct phases during which the flow of goods increases or decreases significantly at certain locations on the network. These findings suggest that the current state of the economy can be inferred from the topological characteristics of the WTW. This work provides new insights into the dynamics of the global economy and may help us better understand its evolution. The World Trade Web (WTW: www.wtwdata.com) is a large database containing information about international transactions between countries  1  . It contains data for more than 180 countries spanning almost 50 years , including bilateral trade volumes and values, imports and exports, and country-specific product codes  2  .\nThe WTW has been used extensively to analyze various aspects of the global economy  3  -  8  . For example, it was shown recently that the WTW displays scale-free  9  and small-world  10  properties similar to those observed in many real-world networks  11  . Moreover, the WTW shows significant clustering  12  , indicating that countries tend to have strong ties among each other when they share common trading partners  13  . Finally, the WTW reveals important differences in the way countries interact with one another  14  : some countries act primarily as exporters while others serve mainly as importers; some countries are highly specialized whereas others are diversified; and so forth  15  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Patterns of dominant flows in the world trade web . Abstract : We research patterns of worldwide economic activity by analyzing the World Trade Web ( WTW ) , which is an interconnected system that represents international transactions between nations .We see that the WTW exhibits scale - free and tiny - world properties , as well as community structure . The communities are marked by their high density of links within themselves but little density with other communities .In addition to these functional features , we also investigate how the movement of goods shifts over time across different components of the network . Our results show that there exist several different stages during which the movement of goods rises or drops considerably at given places on the network .These studies confirm that the present state of the economy can be inferred from the topological traits of the WTW . This research provides new information into the dynamics of the global economy and may assist us better understand its evolve .The World Trade Web ( WTW : www . wtwdata . com ) is a large database holding information about international transactions between nations 1 . It contains information for more than 180 nations spanning almost 50 centuries , covering bilateral trade volumes and values , imports and exports , and country - specific product codes 2 .The WTW has been used heavily to analyze numerous elements of the global economy 3 - 8 . For instance , it was shown lately that the WTW exhibits scale - safe 9 and tiny - global 10 properties similar to those observed in many actual - time systems 11 .Moreover , the WTW indicates considerable clustering 12 , showing that countries tend to have strengthened links among each other when they hold common business partners 13 . Finally , the WTW discovers important differences in the way states interact with one another 14 : some countries act mainly as exporters while many serve mostly as importers ; some countries are extremely developed whereas others are diversified ; and so forth 15 .",
        "rewrite_text": "**Title:** Patterns of Dominant Flows in the World Trade Web\n\n**Abstract:** This study investigates the intricate patterns of global economic activity through an analysis of the World Trade Web (WTW), which serves as a comprehensive framework for understanding international trade transactions among nations. Our findings reveal that the WTW possesses scale-free and small-world characteristics, alongside a distinct community structure. These communities are characterized by a high density of internal connections, while exhibiting sparse links with other communities. Furthermore, we explore the temporal dynamics of goods movement across various components of the network, uncovering multiple phases during which trade volumes experience significant fluctuations at specific locations within the network. This analysis underscores the potential to deduce the current economic state from the topological features of the WTW. \n\nThe WTW, accessible at www.wtwdata.com, is an extensive database that compiles data on international transactions involving over 180 countries, spanning nearly five decades. It encompasses bilateral trade volumes, values, imports, exports, and country-specific product classifications. The WTW has been instrumental in examining various aspects of the global economy, revealing that it exhibits scale-free and small-world properties akin to those found in numerous real-world systems. Additionally, our research highlights significant clustering tendencies, indicating that countries often strengthen their trade links with one another when they share common business partners. \n\nMoreover, the WTW illustrates notable disparities in the interactions among nations; some countries primarily function as exporters, while others predominantly act as importers. This diversity is further compounded by varying levels of development and economic diversification among nations. Overall, our research contributes valuable insights into the dynamics of the global economy, enhancing our understanding of its evolution and the interconnectedness of international trade.",
        "ori-fast-z-score": -1.4881948549771191,
        "water-fast-z-score": 9.320799354856693,
        "rewrite-fast-z-score": -0.49656353316142077
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The star-forming content of the W3 giant molecular cloud .\nAbstract:\nWe present new observations at millimeter wavelengths (1.2 mm and 3mm) toward the W3 Giant Molecular Cloud, which is one of the most active sites of star formation in our Galaxy. We have detected several hundred compact sources with masses ranging between 0.01 to 10 solar masses. The majority of these objects are associated with infrared dark clouds that contain young stellar objects surrounded by dense envelopes. These results show that massive stars form out of gravitationally bound cores within GMCs. This process may be regulated by turbulence and/or magnetic fields. \n \n Keywords: Star Formation, Infrared Dark Clouds, Millimeter Astronomy, Giant Molecular Clouds \n \n 1 Introduction \n \n Massive stars play an important role in shaping their environments through feedback processes such as radiation pressure, winds, and supernova explosions. However, it remains unclear how they form. One possibility is that massive stars form like low-mass stars via gravitational collapse of dense cores inside Giant Molecular Clouds (GMCs; e.g., McKee & Ostriker 2007). Alternatively, massive stars could form directly from turbulent flows without any intermediate core phase (e.g., Banerjee et al. 2006) . \n \n To investigate this issue we observed the W3 region using the Submillimeter Array (SMA; Ho et al. 2004 ) on Mauna Kea Observatory during two nights in November 2005. The SMA consists of eight 6 m antennas operating simultaneously at three different frequencies centered around 230 GHz, 345 GHz, and 690 GHz. At each frequency there were four basebands covering a bandwidth of 2 GHz for continuum emission and 4 GHz for spectral line studies. The primary beam size ranges from ~5′′ to ~20′′ depending on the observing frequency. The total integration time was about 12 hours per night spread over six tracks. Details of the observational setup can be found in Wu et al. (2007a) , where we presented initial results based on data taken only at 345 GHz. Here we report results obtained at all three bands.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The galaxy - creating content of the W3 massive molecular cloud . Abstract : We report new images at millimeter wavelengths ( 1 . 2 mm and 3mm ) toward the W3 Giant Molecular Cloud , which is one of the most active sites of star formation in our Galaxy .We have discovered numerous hundred compact sources with masses ranging between 0 . 01 to 10 solar masses . The majority of these objects are identified with infrared soft clouds that include young stellar bodies surrounded by dense envelopes .These data reveal that massive stars shape out of gravitationally locked cores within GMCs . This process may be regulated by turbulence and / or magnetic fields .Keywords : Star Formation , Infrared Dark Clouds , Millimeter Astronomy , Giant Molecular Clouds 1 Introduction Massive stars serve an important role in shaping their habitats through feedback reactions such as laser pressure , storms , and supernova bursts . However , it remains unsure how they create .One possibility is that massive galaxies form like low - mass stars via gravitational collapse of dense cores inside Giant Molecular Clouds ( GMCs ; e . g . , McKee & Ostriker 2007 ) . Alternatively , huge stars could form directly from turbulent streams without any intermediate core phase ( e . g . , Banerjee et al .2006 ) . To explore this question we studied the W3 region utilizing the Submillimeter Array ( SMA ; Ho et al .2004 ) on Mauna Kea Observatory during two evenings in November 2005 . The SMA consists of eight 6 m antennas located jointly at three different frequencies centered around 230 GHz , 345 GHz , and 690 GHz .At each frequency there were four basebands covering a bandwidth of 2 GHz for continuum emission and 4 GHz for spectral system studies . The main beam width ranges from ~ 5 ′ ′ to ~ 20 ′ ′ depending on the observing frequency .The total integration rate was about 12 hours per night spread over six tracks . Details of the observational setup can be found in Wu et al .( 2007a ) , where we presented first findings based on statistics taken only at 345 GHz . Here we publish results acquired at all three bands .",
        "rewrite_text": "**Title:** The Galaxy-Creating Content of the W3 Massive Molecular Cloud\n\n**Abstract:** In this study, we present new millimeter-wavelength imaging (1.2 mm and 3 mm) of the W3 Giant Molecular Cloud (GMC), a prominent site of star formation within our Galaxy. Our observations have revealed a multitude of compact sources, numbering in the hundreds, with masses ranging from 0.01 to 10 solar masses. A significant portion of these sources corresponds to infrared dark clouds, which are characterized by the presence of young stellar objects enveloped in dense material. The data obtained indicate that massive stars are formed from gravitationally bound cores within GMCs, a process that may be influenced by turbulence and magnetic fields. \n\nMassive stars play a crucial role in shaping their environments through various feedback mechanisms, including radiation pressure, stellar winds, and supernova explosions. However, the precise mechanisms underlying their formation remain a topic of debate. One hypothesis suggests that massive stars form similarly to low-mass stars, through the gravitational collapse of dense cores within GMCs (as proposed by McKee & Ostriker, 2007). Alternatively, it has been suggested that these massive stars may arise directly from turbulent gas streams, bypassing an intermediate core phase (Banerjee et al., 2006). \n\nTo investigate these formation processes, we conducted observations in the W3 region using the Submillimeter Array (SMA) at Mauna Kea Observatory over two nights in November 2005. The SMA, equipped with eight 6-meter antennas, operated at three distinct frequencies centered around 230 GHz, 345 GHz, and 690 GHz. Each frequency utilized four basebands, providing a bandwidth of 2 GHz for continuum observations and 4 GHz for spectral studies. The main beam width varied from approximately 5\" to 20\" depending on the observing frequency, with a total integration time of about 12 hours per night across six observing tracks. This paper presents comprehensive results from all three frequency bands, expanding upon our earlier findings reported at 345 GHz (Wu et al., 2007a). \n\n**Keywords:** Star Formation, Infrared Dark Clouds, Millimeter Astronomy, Giant Molecular Clouds.",
        "ori-fast-z-score": -2.0,
        "water-fast-z-score": 5.602821467047508,
        "rewrite-fast-z-score": -2.226922466874271
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Correlations and sum rules in a half-space for a quantum two-dimensional one-component plasma .\nAbstract:\nWe study the correlations and sum rules in a semi-infinite system with impurities at its surface, which is described by the quantum two-dimensional (2D) one component plasma model. We use the exact diagonalization method to calculate the density-density correlation function and static structure factor as well as their corresponding sum rules. The results show that there are two different regimes depending on whether the temperature T is larger or smaller than the Fermi energy EF . In particular, we find that when T < EF , the behavior of these quantities can be understood within the framework of Landau s Fermi liquid theory. However, if T > EF , our numerical data deviate significantly from this picture. Finally, we also investigate how the presence of impurities affects the above mentioned physical properties. Our findings suggest that the effect of impurities depends strongly on the distance between them. If they are close enough, then the impurity-impurity interaction dominates over other interactions leading to an increase of the effective mass of particles near the surface.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Correlations and sum rules in a half - space for a quantum two - dimensional one - component plasma . Abstract : We explore the correlations and sum rules in a semi - infinite system with impurities at its surface , which is characterized by the quantum two - dimensional ( 2D ) one element plasma model .We use the exact diagonalization technique to estimate the density - density correlation function and static structure parameter as also as their corresponding sum rules . The results show that there are two different regimes depending on whether the temperature T is bigger or smaller than the Fermi energy EF .In particular , we find that when T < EF , the dynamics of these quantities can be understood within the framework of Landau s Fermi liquid theory . However , if T > EF , our numerical information deviate substantially from this picture .Finally , we also investigate how the presence of impurities impacts the above mentioned physical properties . Our findings show that the impact of impurities relies highly on the distance between them .If they are close enough , then the impurity - impurity interaction dominates over other interactions leading to an increase of the effective mass of molecules near the surface .",
        "rewrite_text": "In this study, we investigate the correlations and sum rules in a semi-infinite quantum two-dimensional one-component plasma system that includes surface impurities. Utilizing the exact diagonalization technique, we analyze the density-density correlation function and the static structure factor, along with their associated sum rules. Our findings reveal the existence of two distinct regimes based on the relationship between the temperature (T) and the Fermi energy (EF). Specifically, when the temperature is lower than the Fermi energy (T < EF), the behavior of these physical quantities aligns with the principles of Landau's Fermi liquid theory. Conversely, when the temperature exceeds the Fermi energy (T > EF), our numerical results diverge significantly from this theoretical framework. Additionally, we examine the influence of impurities on the aforementioned physical properties. Our results indicate that the effect of impurities is highly dependent on their spatial arrangement; when impurities are in close proximity, their interactions dominate, resulting in an increased effective mass of the particles near the surface. This work enhances our understanding of the complex interplay between temperature, quantum effects, and impurity interactions in two-dimensional plasma systems, providing valuable insights for future research in quantum materials and condensed matter physics.",
        "ori-fast-z-score": 2.465858830126928,
        "water-fast-z-score": 6.041987916036252,
        "rewrite-fast-z-score": 1.4439897447623107
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  CCD BV survey of 42 open clusters .\nAbstract:\nWe present the results of our CCD photometric study of 42 open clusters in the southern hemisphere, carried out at the 1-meter telescope of the South African Astronomical Observatory (SAAO). The observations were made with an SBIG STL-1001E camera equipped with a Kodak KAF-0400 chip and Johnson V filter during three observing runs between September 1998 and February 1999. We have used DAOPHOT II to perform aperture photometry on all stars detected within each cluster field-of-view. A total number of about 15000 stars was measured for each cluster. In addition we obtained UBVRI photometry for some of these clusters using the same instrumentation as described above.  From this data set we derived the following parameters: reddening E(B-V), distance modulus DM, age t, metallicity  Fe/H  , mass function slope x, core radius rc, central surface brightness µ0, concentration index c, and integrated absolute magnitude M.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : CCD BV survey of 42 open complexes . Abstract : We present the conclusion of our CCD photometric analysis of 42 open complexes in the southern hemisphere , conducted out at the 1 - meter telescope of the South African Astronomical Observatory ( SAAO ) .The observed were made with an SBIG STL - 1001E camera equipped with a Kodak KAF - 0400 card and Johnson V filter during three observing walks between September 1998 and February 1999 . We have utilized DAOPHOT II to conduct aperture photometry on all stars observed within each cluster field - of - view .A total quantity of about 15000 stars was measured for each cluster . In addition we received UBVRI photometry for some of these clusters using the same equipment as described above .From this data set we derived the following variables : reddening E ( B - V ) , distance modulus DM , age t , metallicity Fe / H , mass function slope x , core radius rc , central surface brightness µ0 , concentration index c , and integrated absolute magnitude M .",
        "rewrite_text": "We present the findings from our CCD photometric study of 42 open clusters located in the southern hemisphere, performed at the 1-meter telescope of the South African Astronomical Observatory (SAAO). This research involved observations taken with an SBIG STL-1001E camera, which was fitted with a Kodak KAF-0400 chip and a Johnson V filter, during three observational campaigns conducted between September 1998 and February 1999. Utilizing the DAOPHOT II software, we carried out aperture photometry on all stars within the field of view of each cluster, resulting in the measurement of approximately 15,000 stars per cluster. Furthermore, we obtained UBVRI photometry for several of these clusters using the same observational setup. From the comprehensive dataset, we were able to derive several key astrophysical parameters, including the reddening value E(B-V), distance modulus (DM), age (t), metallicity (Fe/H), mass function slope (x), core radius (rc), central surface brightness (µ0), concentration index (c), and integrated absolute magnitude (M). This extensive analysis not only enhances our understanding of the properties of these open clusters but also contributes valuable data to the broader field of stellar astrophysics. The results of this study provide insights into the formation and evolution of star clusters in the southern hemisphere, offering a foundation for future research in this area.",
        "ori-fast-z-score": -1.3054598240132387,
        "water-fast-z-score": 2.5927248643506746,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The magnetic field topology associated to two M flares .\nAbstract:\nWe present the analysis of the magnetic field configuration in active region NOAA 11158, which produced several large solar flares during its passage across the central meridian on February 15-16, 2011 . We use vector magnetograms obtained by SDO/HMI and SOHO/MDI instruments as well as photospheric line-of-sight magnetograms provided by GONG network. The evolution of the coronal magnetic field is studied using potential-field source-surface (PFSS) model. In addition we perform NLFFF extrapolations with different codes for comparison purposes. Our results show that both PFSS and NLFFF models are able to reproduce the large-scale structure of the corona but differ significantly at small scales. This discrepancy can be explained by considering the effect of plasma flows along open field lines. Finally, we study the relationship between the observed photospheric motions and the changes in the coronal magnetic field. Active Region NOAA 11158 was one of the most energetic regions ever recorded. It produced several X-class flares including an X2.2 event on February 16, 2011 , when it crossed the central meridian. Several authors have analyzed this active region before and after the flare occurrence. They found evidence of strong shearing motions in the photosphere prior to the flare onset (e.g., Liu et al. , 2012; Petrie & Sudol 2010; Schrijver 2009 ). These observations suggest that the energy release may be triggered by reconnection processes involving twisted flux tubes (Petrie 2013) . However, there has been no detailed investigation into how these photospheric motions affect the coronal magnetic field or whether they trigger any significant reconfiguration of the magnetic field.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The magnetic force geometry associated to two M flares . Abstract : We report the examination of the magnetic field configuration in active region NOAA 11158 , which produced numerous large solar flares during its passage across the central meridian on February 15 - 16 , 2011 .We use vector magnetograms achieved by SDO / HMI and SOHO / MDI instruments as also as photospheric line - of - view magnetograms supplied by GONG channel . The evolution of the coronal magnetic force is studied utilizing potential - field source - surface ( PFSS ) model .In addition we perform NLFFF extrapolations with various codes for comparison purposes . Our results show that both PFSS and NLFFF models are able to reproduce the huge - scale structure of the corona but change considerably at small scales .This discrepancy can be described by examining the impact of plasma flows along open field lines . Finally , we study the relationship between the seen photospheric movements and the changes in the coronal magnetic field .Active Region NOAA 11158 was one of the most intense regions ever recorded . It produced numerous X - class flares notably an X2 . 2 event on February 16 , 2011 , when it crossed the central meridian .Several scientists have analyzed this active region before and after the flare outbreak . They found proof of stable shearing motions in the photosphere prior to the flare outbreak ( e . g . , Liu et al ., 2012 ; Petrie & Sudol 2010 ; Schrijver 2009 ) . These measurements suggest that the electricity release may be triggered by reconnection pathways involving twisted flux tubes ( Petrie 2013 ) .However , there has been no comprehensive investigation into how these photospheric movements influence the coronal magnetic field or whether they cause any considerable reconfiguration of the magnetic current .",
        "rewrite_text": "**Title:** The Magnetic Force Geometry Associated with Two M Flare Events\n\n**Abstract:** This study investigates the magnetic field configuration within active region NOAA 11158, which was responsible for multiple significant solar flares during its transit across the central meridian on February 15-16, 2011. Utilizing vector magnetograms obtained from the SDO/HMI and SOHO/MDI instruments, alongside photospheric line-of-sight magnetograms provided by the GONG channel, we analyze the evolution of the coronal magnetic force through the application of the potential-field source-surface (PFSS) model. Additionally, we conduct nonlinear force-free field (NLFFF) extrapolations using various computational codes for comparative analysis. Our findings indicate that while both PFSS and NLFFF models successfully replicate the large-scale structure of the corona, they exhibit significant discrepancies at smaller scales. This variation can be attributed to the influence of plasma flows along open magnetic field lines. Furthermore, we explore the correlation between observed photospheric motions and alterations in the coronal magnetic field. Active Region NOAA 11158 is noted as one of the most potent regions documented, producing numerous X-class flares, including an X2.2 event on February 16, 2011, coinciding with its central meridian passage. Previous studies have identified stable shearing motions in the photosphere preceding the flare activity (e.g., Liu et al., 2012; Petrie & Sudol, 2010; Schrijver, 2009), suggesting that the release of magnetic energy may be initiated by reconnection processes involving twisted flux tubes (Petrie, 2013). However, a thorough investigation into the extent to which these photospheric movements affect the coronal magnetic field or induce significant reconfigurations of the magnetic current has yet to be conducted. This research aims to fill that gap, providing insights into the intricate relationship between photospheric dynamics and coronal magnetic field configurations in the context of solar flare activity.",
        "ori-fast-z-score": -1.1627553482998907,
        "water-fast-z-score": 6.300231216854408,
        "rewrite-fast-z-score": -0.5222329678670935
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Second-order perturbations of cosmological fluids: Relativistic effects of pressure, multi-component, curvature, and rotation .\nAbstract:\nWe present the relativistic second order perturbation theory for fluids in curved space-time with arbitrary number of components. We derive the general expression for the energy-momentum tensor at first order in perturbations as well as its trace-free part which is responsible for gravitational waves generation. The evolution equations are derived by projecting the conservation law onto the background 4-velocity vector field. In particular we show that the presence of anisotropic stress leads to an additional source term in the equation governing the evolution of scalar modes. Finally, we discuss how our formalism can be applied to study different physical situations such as inflationary models or dark matter halos formation. Cosmology has been revolutionized over the past decade thanks to precision measurements of temperature fluctuations in the cosmic microwave background (CMB) radiation  1  . These observations have provided us with detailed information about the early universe and allowed to test fundamental physics on very large scales  2  .\nThe standard model of cosmology assumes that the universe consists of several interacting components including cold dark matter (CDM), baryons, photons, neutrinos etc.. Each component evolves according to some set of hydrodynamical equations describing their dynamics  3  . However, these equations cannot be solved analytically even if one neglects all interactions between particles  4  , so numerical simulations are required  5  . On the other hand, analytical solutions exist only under certain approximations  6  . For example, it was shown recently  7, 8  that the effect of pressure gradients may lead to significant corrections to the growth rate of density perturbations during the late stages of structure formation  9  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Second - order perturbations of cosmological fluids : Relativistic effects of stress , multi - component , curvature , and rotation . Abstract : We introduce the relativistic second order perturbation theory for fluids in curved space - time with arbitrary number of components .We derive the general expression for the power - momentum tensor at first order in perturbations as also as its trace - free portion which is responsible for gravitational waves development . The evolution coefficients are derived by projecting the conservation law onto the background 4 - velocity tensor field .In particular we prove that the presence of anisotropic pressure leads to an additional source term in the equation regulating the evolution of scalar cycles . Finally , we explain how our formalism can be applied to study various mechanical circumstances such as inflationary theories or black matter halos formation .Cosmology has been revolutionized over the previous decade courtesy to accurate measurements of temperature fluctuations in the cosmic microwave background ( CMB ) radiation 1 . These measurements have provided us with comprehensive information about the early universe and helped to test fundamental theory on very huge scales 2 .The conventional model of cosmology assumes that the universe consists of several interacting components namely cold bright matter ( CDM ) , baryons , photons , neutrinos etc . . Each component evolves due to some setting of hydrodynamical equations explaining their mechanics 3 . However , these equations never be answered analytically especially if one neglects all interactions between particles 4 , so computational simulations are required 5 .On the other hand , analytical solutions arise only under certain approximations 6 . For instance , it was shown recently 7 , 8 that the impact of force gradients might lead to significant corrections to the development time of density perturbations during the last phases of structure formation 9 .",
        "rewrite_text": "**Title:** Second-Order Perturbations of Cosmological Fluids: Relativistic Effects of Stress, Multi-Component, Curvature, and Rotation\n\n**Abstract:** In this article, we present a comprehensive framework for relativistic second-order perturbation theory applied to cosmological fluids within curved spacetime, accommodating an arbitrary number of fluid components. We derive the general expression for the energy-momentum tensor at first order in perturbations, including its trace-free part, which plays a crucial role in the generation of gravitational waves. The evolution coefficients are obtained by projecting the conservation laws onto the background four-velocity tensor field. Notably, we demonstrate that the presence of anisotropic pressure introduces an additional source term in the equations governing the evolution of scalar perturbations. Our formalism is versatile and can be utilized to investigate various physical scenarios, including inflationary models and the formation of dark matter halos.\n\nThe field of cosmology has undergone significant advancements in the past decade, largely due to precise measurements of temperature fluctuations in the cosmic microwave background (CMB) radiation. These observations have yielded valuable insights into the early universe and have facilitated the testing of fundamental theories on cosmological scales. The standard cosmological model posits that the universe is composed of several interacting components, such as cold dark matter (CDM), baryons, photons, and neutrinos. The dynamics of each component are governed by hydrodynamic equations that describe their behavior. However, these equations often lack analytical solutions, particularly when interactions between particles are disregarded, necessitating the use of computational simulations. Analytical solutions are typically achievable only under specific approximations. Recent studies have indicated that force gradients can significantly alter the evolution times of density perturbations during the late stages of structure formation. This work aims to enhance our understanding of these complex interactions and their implications for cosmological evolution.",
        "ori-fast-z-score": -2.0,
        "water-fast-z-score": 6.166666666666667,
        "rewrite-fast-z-score": 0.5895063447465633
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Comprehensive simulations of superhumps .\nAbstract:\nSuperhumps are periodic modulations in the light curves of some cataclysmic variables (CVs). They have been observed to occur during both the high and low states, but their origin is still not understood. In this work we present comprehensive numerical simulations of CVs with mass ratios q = 0.7-0.9 that include tidal dissipation effects as well as magnetic braking. We find that for systems with orbital periods P orb < 3 hr, the inclusion of magnetic braking leads to an increase in the amplitude of the superhumps by up to a factor of two compared to previous results obtained without magnetic braking. For longer period systems, however, our models predict smaller amplitudes than those inferred observationally. This discrepancy may be due to additional physical processes such as irradiation or enhanced mass transfer rates at periastron passage which were neglected here. \n \n Keywords: Cataclysmic Variables",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Comprehensive simulations of superhumps . Abstract : Superhumps are regular modulations in the light curves of some cataclysmic variables ( CVs ) .They have been observed to arise during both the high and low states , but their source is nevertheless not explained . In this research we present comprehensive numerical simulations of CVs with mass factors q = 0 . 7 - 0 . 9 that include tidal dissipation effects as well as magnetic braking .We see that for systems with orbital periods P orb < 3 hr , the introduction of magnetic braking contributes to an increase in the frequency of the superhumps by up to a factor of two compared to previous findings obtained without magnetic braking . For longer duration systems , however , our calculations predict smaller amplitudes than those inferred observationally .This discrepancy may be due to extra physical processes such as irradiation or improved mass transfer rates at periastron passage which were neglected here . Keywords : Cataclysmic Variables",
        "rewrite_text": "Title: In-Depth Simulations of Superhumps in Cataclysmic Variables\n\nAbstract: Superhumps are periodic variations observed in the light curves of certain cataclysmic variables (CVs), yet their underlying mechanisms remain poorly understood. This study presents a series of detailed numerical simulations focusing on CVs with mass ratios ranging from q = 0.7 to 0.9, incorporating the effects of tidal dissipation and magnetic braking. Our findings indicate that in systems with orbital periods shorter than 3 hours, the inclusion of magnetic braking significantly enhances the frequency of superhumps, potentially doubling the rates observed in prior studies that did not account for this factor. Conversely, for CVs with longer orbital periods, our simulations yield superhump amplitudes that are notably lower than those reported in observational data. This inconsistency suggests that additional physical processes, such as irradiation effects or increased mass transfer rates during periastron passages, may play a crucial role and were not considered in our current model. The results of this research contribute to a deeper understanding of the dynamics of superhumps and highlight the importance of incorporating various physical phenomena in future models to reconcile discrepancies between theoretical predictions and observational evidence. \n\nKeywords: Cataclysmic Variables, Superhumps, Magnetic Braking, Tidal Dissipation, Numerical Simulations.",
        "ori-fast-z-score": 0.12403473458920847,
        "water-fast-z-score": 4.677476657214644,
        "rewrite-fast-z-score": 0.953998092005724
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Double solid twistor spaces: the case of arbitrary signature .\nAbstract:\nWe study double solid twistor spaces in general dimension and signature, extending previous results on self-dual manifolds to all signatures. We show that these spaces are always smooth, but may have singularities along certain subvarieties. In particular we prove that if $(M^{2n},g)$ is an oriented Riemannian manifold with holonomy group contained in Spin($n$), then its double solid twistor space has no singularities at all. This result can be seen as a generalization of the well-known fact that the twistor space of a self-dual 4-manifold does not have any singularities. The main tool used here is the spinor representation of the complexified Clifford algebra bundle over $M$. As another application of this technique, we give a new proof for the existence of a canonical connection on the normal bundle of a totally geodesic submanifold of a Riemannian manifold.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Double solid twistor spaces : the case of arbitrary signature . Abstract : We research double solid twistor spaces in general dimension and signature , extending previous findings on self - dual manifolds to all signatures .We see that these spaces are always continuous , but might have singularities along particular subvarieties . In particular we prove that if $ ( M ^ { 2n } , g ) $ is an oriented Riemannian manifold with holonomy group contained in Spin ( $ h $ ) , then its double solid twistor space has no singularities at all .This result can be saw as a generalization of the better - famous fact that the twistor space of a self - dual 4 - sphere does not have any singularities . The main technique useful here is the spinor characterization of the complexified Clifford algebra bundle over $ M $ .As another implementation of this methodology , we give a new proof for the existence of a canonical connection on the normal bundle of a totally geodesic submanifold of a Riemannian manifold .",
        "rewrite_text": "Title: Double Solid Twistor Spaces: The Case of Arbitrary Signature\n\nAbstract: In this study, we investigate double solid twistor spaces across various dimensions and signatures, building upon earlier research focused on self-dual manifolds. Our findings reveal that while these spaces are consistently continuous, they may exhibit singularities along specific subvarieties. Notably, we demonstrate that for an oriented Riemannian manifold \\( (M^{2n}, g) \\) with a holonomy group contained within Spin(\\( h \\)), the corresponding double solid twistor space is free from singularities. This result serves as a generalization of the well-known fact that the twistor space of a self-dual 4-sphere is also devoid of singularities. The primary technique employed in our analysis involves the spinor characterization of the complexified Clifford algebra bundle over the manifold \\( M \\). Additionally, we apply this methodology to provide a novel proof regarding the existence of a canonical connection on the normal bundle of a totally geodesic submanifold within a Riemannian manifold. Our work not only broadens the understanding of double solid twistor spaces but also reinforces the significance of spinor techniques in the study of geometric structures. Through these advancements, we contribute to the ongoing discourse in differential geometry and the theory of twistor spaces, offering insights that may pave the way for future research in the field.",
        "ori-fast-z-score": -1.805787796286538,
        "water-fast-z-score": 2.7688746209726918,
        "rewrite-fast-z-score": -0.10369516947304253
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Extraction of Freshwater and Energy from Atmosphere .\nAbstract:\nThe extraction of fresh water and energy from the atmosphere is proposed as an alternative to conventional sources, which are limited in supply or environmentally damaging.  The process involves condensing atmospheric moisture into liquid water using solar power and then collecting this water on a surface coated with hydrophobic materials that allow it to be easily transported by air currents.   This technology could provide clean drinking water for remote communities without requiring large amounts of land area or infrastructure investment. It also has potential applications in agriculture where irrigation can be provided at low cost through the use of wind-driven sprayers. In addition, the collected water may be used directly as fuel if combined with electrolysis cells powered by renewable electricity. The process requires minimal maintenance once installed and would operate continuously over many years. A pilot-scale demonstration system was constructed near Tucson Arizona (USA) during 2011-2013. The results show that the system produces up to 1 gallon per day of potable water under favorable conditions.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Extraction of Freshwater and Energy from Atmosphere . Abstract : The extraction of fresh water and energy from the atmosphere is proposed as an alternative to conventional sources , which are limited in supply or environmentally destructive .The method means condensing ambient humidity into liquid water use solar electricity and then collecting this water on a surface coated with hydrophobic materials that enable it to be easily carried by air waves . This system could supply fresh drinking water for isolated communities without using small amounts of property area or construction investment .It additionally has potential applications in farming where irrigation can be provided at low cost through the using of wind - powered sprayers . In addition , the stored water may be used directly as fuel if combined with electrolysis panels driven by renewable energy .The method needs minimal repair once implemented and might run constantly over much years . A pilot - scale test system was constructed near Tucson Arizona ( USA ) during 2011 - 2013 .The results show that the scheme produces up to 1 gallon per day of potable liquid under favorable conditions .",
        "rewrite_text": "Title: Extraction of Freshwater and Energy from the Atmosphere\n\nAbstract: This article presents a novel approach for extracting freshwater and energy from the atmosphere, positioning it as a sustainable alternative to traditional sources that are often limited or environmentally harmful. The proposed method involves the condensation of ambient humidity into liquid water, harnessing solar energy for the process. The collected water is then gathered on surfaces treated with hydrophobic materials, which facilitate its transport via air currents. This innovative system has the potential to provide fresh drinking water to remote communities with minimal land use and construction costs. Furthermore, it offers promising applications in agriculture, where it can enable low-cost irrigation through wind-powered sprayers. The water harvested can also be utilized as a fuel source when integrated with electrolysis panels powered by renewable energy. The system is designed for durability, requiring minimal maintenance once operational, and is capable of functioning continuously over extended periods. A pilot-scale test was conducted near Tucson, Arizona, from 2011 to 2013, yielding significant results. Under optimal conditions, the system demonstrated the ability to produce up to one gallon of potable water per day. This research highlights the feasibility of utilizing atmospheric resources to address water scarcity and energy needs, paving the way for further advancements in sustainable technologies.",
        "ori-fast-z-score": -1.1055415967851332,
        "water-fast-z-score": 6.2,
        "rewrite-fast-z-score": -0.3849001794597505
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Differentially rotating force-free magnetosphere of an aligned rotator: analytical solutions in split-monopole approximation .\nAbstract:\nWe present the first exact solution for the axisymmetric, stationary and force-free magnetic field structure around a rapidly-rotating neutron star with arbitrary inclination angle between its rotation and magnetic axes. The model is based on the assumption that the stellar surface is covered by a thin layer of plasma which corotates with the star at all latitudes. We show how this solution can be used to calculate the X-ray light curves produced by hot spots located at different latitudes above the stellar surface. In particular we demonstrate that the observed phase shifts between the peaks of soft X-rays (0.5-2 keV) and hard X-rays (2-10 keV), as well as their relative amplitudes are reproduced very accurately if one assumes that both emission components originate from two antipodal hot spots located near the magnetic poles. This result suggests that the observed X-ray pulsations may be caused by the rotational modulation of the local emissivity rather than by the Doppler effect due to bulk motion of matter within the emitting regions. \n \n Keywords: Force-free fields, Pulsar wind nebulae",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Differentially shifting force - free magnetosphere of an aligned rotator : analytical solutions in split - monopole approximation . Abstract : We present the first accurate solution for the axisymmetric , stationary and force - free magnetic field configuration around a rapidly - spinning neutron galaxy with arbitrary orientation angle between its rotation and magnetic axes .The model is based on the assumption that the stellar surface is enclosed by a thin layer of liquid which corotates with the star at all latitudes . We see how this solution can be used to estimate the X - ray light angles created by hot areas situated at different latitudes above the stellar surface .In particular we prove that the observed phase change between the peaks of hard X - radiation ( 0 . 5 - 2 keV ) and hard X - radiation ( 2 - 10 keV ) , as well as their relative amplitudes are reconstructed very correctly if one assumes that both emission parts originate from two antipodal hot areas situated near the magnetic poles . This result suggests that the seen X - ray pulsations may be caused by the rotational modulation of the local emissivity rather than by the Doppler impact owing to bulk movement of matter within the emitting regions .Keywords : Force - free fields , Pulsar wind nebulae",
        "rewrite_text": "Title: Differentially Shifting Force-Free Magnetosphere of an Aligned Rotator: Analytical Solutions in Split-Monopole Approximation\n\nAbstract: In this study, we introduce the first precise analytical solution for the axisymmetric, stationary, and force-free magnetic field configuration surrounding a rapidly rotating neutron star, accommodating an arbitrary angle between its rotation and magnetic axes. Our model operates under the premise that the stellar surface is enveloped by a thin liquid layer that co-rotates with the star across all latitudes. This innovative solution enables us to estimate the X-ray emission angles produced by heated regions located at various latitudes above the stellar surface. Notably, we demonstrate that the observed phase shift between the peaks of hard X-ray emissions in the ranges of 0.5 - 2 keV and 2 - 10 keV, along with their relative amplitudes, can be accurately reconstructed by assuming that both emission components arise from two antipodal hot spots positioned near the magnetic poles. This finding implies that the X-ray pulsations detected may be attributed to the rotational modulation of local emissivity, rather than being a result of Doppler effects due to the bulk motion of matter within the emitting regions. Our results provide significant insights into the behavior of force-free fields and the dynamics of pulsar wind nebulae, enhancing the understanding of neutron star magnetospheres and their associated emissions. \n\nKeywords: Force-free fields, Pulsar wind nebulae.",
        "ori-fast-z-score": -1.9095718489925029,
        "water-fast-z-score": 4.6,
        "rewrite-fast-z-score": -0.9901475429766744
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Rapidity and energy dependence of the electric charge correlations in A+A collisions at the SPS energies .\nAbstract:\nThe rapidity and transverse momentum (pT) dependences of the electric charge correlation functions are studied for central Au+Au, d+Au and p+p collisions at RHIC and LHC energies using the AMPT model with string melting mechanism. The results show that there is no significant difference between the charge correlation functions obtained by different collision systems except for small differences around midrapidity region which may be due to the initial state effects. It can also be seen that the charge correlation function decreases as the center-of-mass energy increases. This behavior indicates that the strength of charge separation effect becomes weaker when going from lower to higher energies. Finally it should be noted that the charge correlation functions calculated here have been found to agree well with those measured experimentally. PACS numbers: 25.75.-q, 11.15.-x, 12.38.Mh  Electric charge fluctuations play an important role in understanding many interesting phenomena observed in heavy-ion collisions such as charge balance functions  1  , net-charge fluctuations  2  , etc.. In recent years, several experiments  3-6  have reported measurements on these quantities in various collision systems ranging from proton-proton(pp), deuteron-gold(d-Au) to gold-gold(Au-Au). These experimental data provide valuable information about the properties of hot and dense nuclear matter produced in high-energy nucleus-nucleus collisions  7-9  . However, theoretical studies on this subject still remain limited  10-12  .\nIn order to understand better the underlying physics behind these observations, we need more detailed investigations into the charge fluctuation phenomenon. One possible way to study charge fluctuations is through measuring the charge correlation functions  13-15  . Recently, some experimental groups  16-18  have presented their measurement on charge correlation functions in pp, d-Au and Au-Au collisions at RHIC and Large Hadron Collider (LHC) energies. On the other hand, the relativistic quantum molecular dynamics (RQMD)  19  and the parton-hadron-string dynamics (PHSD)  20  models predict that the charge correlation functions decrease rapidly towards zero",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Rapidity and energy dependence of the electric current correlations in A + A collisions at the SPS energies . Abstract : The rapidity and transverse momentum ( pT ) dependences of the electric charge relationship functions are studied for central Au + Au , d + Au and p + p collisions at RHIC and LHC energies using the AMPT theory with string melting system .The results show that there is no major variation between the charge interaction functions obtained by various crash processes except for little differences around midrapidity region which may be due to the early state effects . It can also be shown that the charge correlation function decreases as the center - of - mass energy rises .This phenomenon suggests that the strength of charge separation effect gets smaller when going from lower to higher energies . Finally it should be mentioned that the charge correlation functions measured here have been shown to agree well with those observed experimentally .PACS codes : 25 . 75 . - q , 11 . 15 . - x , 12 . 38 . Mh Electric charge fluctuations take an important role in understanding several interesting phenomena observed in heavy - ion collisions such as charge balance functions 1 , net - charge fluctuations 2 , etc . . In recent seasons , various study 3 - 6 have reported measurements on these quantities in different collision systems ranging from proton - proton ( pp ) , deuteron - silver ( d - Au ) to platinum - silver ( Au - Au ) .These observation findings provide valuable info about the properties of hot and dense nuclear material created in high - energy nucleus - nucleus collisions 7 - 9 . However , theoretical experiments on this subject still stay limited 10 - 12 .In order to realize clearer the fundamental theory behind these observations , we require more precise studies into the charge fluctuation phenomenon . One easy means to study charge fluctuations is through measuring the charge relationship values 13 - 15 .Recently , some experimental groups 16 - 18 have published their observation on charge interaction functions in pp , d - Au and Au - Au collisions at RHIC and Large Hadron Collider ( LHC ) energies . On the other hand , the relativistic quantum molecular mechanics ( RQMD ) 19 and the parton - hadron - string dynamics ( PHSD ) 20 models predict that the charge interaction functions decline rapidly towards zero",
        "rewrite_text": "**Title:** Rapidity and Energy Dependence of Electric Current Correlations in A + A Collisions at SPS Energies\n\n**Abstract:** This study investigates the rapidity and transverse momentum (pT) dependencies of electric charge correlation functions in central Au + Au, d + Au, and p + p collisions at both RHIC and LHC energies, utilizing the AMPT model with a string melting approach. Our findings indicate that the charge correlation functions exhibit minimal variation across different collision processes, with only slight discrepancies observed in the midrapidity region, potentially attributed to early-stage effects. Notably, we observe a decrease in the charge correlation function as the center-of-mass energy increases, suggesting that the intensity of charge separation diminishes when transitioning from lower to higher energy regimes. Importantly, the charge correlation functions derived from our analysis align closely with experimental measurements, reinforcing the validity of our theoretical approach. \n\nElectric charge fluctuations play a crucial role in elucidating various phenomena in heavy-ion collisions, such as charge balance functions and net-charge fluctuations. Recent studies have reported measurements of these fluctuations across different collision systems, including proton-proton (pp), deuteron-gold (d-Au), and gold-gold (Au-Au) interactions. These observations yield significant insights into the properties of the hot and dense nuclear matter produced in high-energy nucleus-nucleus collisions. However, theoretical investigations in this domain remain relatively sparse. To enhance our understanding of the fundamental theories underlying these observations, further precise studies on charge fluctuation phenomena are essential. One effective approach to exploring charge fluctuations is through the measurement of charge correlation values. Recent experimental efforts have documented charge interaction functions in pp, d-Au, and Au-Au collisions at RHIC and LHC energies. In contrast, models such as relativistic quantum molecular dynamics (RQMD) and parton-hadron-string dynamics (PHSD) predict a rapid decline of charge interaction functions towards zero. This study aims to bridge the gap between theoretical predictions and experimental findings, contributing to a deeper understanding of charge dynamics in high-energy collisions. \n\n**PACS Codes:** 25.75.-q, 11.15.-x, 12.38.Mh",
        "ori-fast-z-score": -0.22423052782558076,
        "water-fast-z-score": 7.00634632949934,
        "rewrite-fast-z-score": -0.23076923076923078
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Kinematics of the Ultra-Faint Milky Way Satellites: Solving the Missing Satellite Problem .\nAbstract:\nWe present an analysis of the kinematic properties of the ultra-faint satellites (UFS) of the Milky Way, using data from the Sloan Digital Sky Survey and the Dark Energy Survey. We find that all UFS are consistent with being on circular orbits around their host galaxy, which is in agreement with previous studies. However, we also show that this result can be explained by assuming that these galaxies have been tidally disrupted over time. In particular, we demonstrate how tidal disruption could explain both the observed number density profile as well as the velocity dispersion profiles for each satellite system. Finally, we discuss our results within the context of other recent work studying the missing satellite problem. The discovery of more than 100 new dwarf galaxies orbiting the Milky Way has led to renewed interest in understanding the formation history of the Local Group. While many of these newly discovered systems appear to follow similar scaling relations to those found among brighter dwarfs such as the classical dwarf spheroidal galaxies, there remain several puzzling differences between them. For example, while most bright dwarfs exhibit significant rotation velocities, only one of the recently discovered faintest satellites shows any evidence of rotation . Furthermore, while the majority of bright dwarfs lie close to the virial radius of the Milky Way , nearly half of the fainter satellites reside at distances greater than 300 kpc .\nIn addition to these observational challenges, theoretical models predict that dark matter halos should contain far fewer subhalos than are currently known to exist . This discrepancy -known as the  missing satellite problem -has motivated numerous investigations into possible solutions ranging from modifications to standard cold dark matter theory to alternative theories of gravity .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Kinematics of the Ultra - Faint Milky Way Satellites : Solving the Missing Satellite Problem . Abstract : We present an assessment of the kinematic dynamics of the ultra - faint satellites ( UFS ) of the Milky Way , using data from the Sloan Digital Sky Survey and the Dark Energy Survey .We see that all UFS are compatible with being on circular orbits around their target star , which is in agreement with previous research . However , we also find that this consequence can be described by assuming that these galaxies have been tidally disrupted over time .In particular , we explain how tidal disruption helped change both the seen number density profile as also as the velocity dispersion profiles for each spacecraft network . Finally , we explain our findings within the context of other recent work studying the missing spacecraft problem .The observation of more than 100 new dwarf galaxies orbiting the Milky Way has led to renewed interest in understanding the formation history of the Local Group . While several of these newly discovered systems seem to follow similar scaling relations to those observed among brighter dwarfs such as the classical dwarf spheroidal galaxies , there remain many puzzling changes between them .For instance , while most bright dwarfs exhibit substantial rotation velocities , only one of the recently discovered faintest satellites displays any evidence of rotation . Furthermore , while the majority of bright dwarfs occur close to the virial diameter of the Milky Way , nearly half of the fainter satellites exist at distances greater than 300 kpc .In addition to these observational obstacles , theoretical theories predict that dark matter halos should contain considerably less subhalos than are currently known to exist . This discrepancy - referred as the missing satellite problem - has driven several investigations into possible solutions ranging from alterations to standard cold bright matter theory to alternative theories of gravitational .",
        "rewrite_text": "**Title:** The Kinematics of Ultra-Faint Milky Way Satellites: Addressing the Missing Satellite Problem\n\n**Abstract:** In this study, we investigate the kinematic properties of the ultra-faint satellites (UFS) of the Milky Way, utilizing data sourced from the Sloan Digital Sky Survey and the Dark Energy Survey. Our analysis reveals that all UFS are consistent with being in circular orbits around their host galaxy, aligning with findings from prior research. Notably, we propose that the observed kinematic characteristics can be attributed to the tidal disruption these galaxies have experienced over time. We detail how tidal interactions have influenced both the number density profiles and the velocity dispersion profiles of these satellite galaxies. Our results are contextualized within the broader framework of recent studies addressing the missing satellite problem, which has gained traction following the discovery of over 100 new dwarf galaxies orbiting the Milky Way. \n\nThese newly identified systems exhibit scaling relations similar to those of brighter dwarf galaxies, such as classical dwarf spheroidals; however, significant discrepancies persist. For example, while most bright dwarf galaxies demonstrate considerable rotational velocities, only one of the faintest satellites shows any signs of rotation. Additionally, although the majority of bright dwarfs are located near the virial radius of the Milky Way, nearly half of the ultra-faint satellites are found at distances exceeding 300 kpc. \n\nTheoretical models predict that dark matter halos should host far fewer subhalos than what has been observed, leading to the so-called missing satellite problem. This inconsistency has prompted various investigations into potential resolutions, ranging from modifications to standard cold dark matter theories to the exploration of alternative gravitational frameworks. Our findings contribute to this ongoing discourse by providing insights into the kinematic behavior of UFS and their implications for understanding the formation and evolution of the Local Group.",
        "ori-fast-z-score": 0.6810052246069989,
        "water-fast-z-score": 7.320806164525238,
        "rewrite-fast-z-score": 0.6115928396627265
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Profinite complexes of curves, their automorphisms and anabelian properties of moduli stacks of curves .\nAbstract:\nWe study the profinite complex associated to a family of smooth projective curves over a field k with fixed genus g>0. We show that this is homotopy equivalent to the classifying space of the group Aut(C) of automorphisms of one curve C in the family (Theorem 1). This result generalizes previous results by Deligne on families of elliptic curves. In particular we obtain new information about the cohomology groups H^1(Aut(C),k*) for all such families (Corollary 2).\nIn section 3 we use our results to prove some basic facts about the anabelian geometry of the moduli stack M_g of stable curves of genus g. For example we give a simple proof of the fact that the Picard scheme Pic0M_g has no torsion points if char(k)=0 or p>3g-2 (Proposition 4). The main tool used here are the results obtained in sections 1-2 combined with Grothendieck s comparison theorem between étale fundamental groups and Galois groups.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Profinite complexes of curves , their automorphisms and anabelian characteristics of moduli stacks of curves . Abstract : We research the profinite complex attached to a family of smooth projective surfaces over a field k with fixed genus g > 0 .We see that this is homotopy equivalent to the classifying set of the group Aut ( C ) of automorphisms of one curve C in the family ( Theorem 1 ) . This result generalizes earlier findings by Deligne on groups of elliptic curves .In particular we obtain new information about the cohomology groups H ^ 1 ( Aut ( C ) , k * ) for all such families ( Corollary 2 ) . In section 3 we utilize our findings to confirm some fundamental findings about the anabelian geometry of the moduli stack M _ k of stable curves of genus g . For instance we give a simple proof of the fact that the Picard scheme Pic0M _ k has no torsion points if char ( h ) = 0 or p > 3g - 2 ( Proposition 4 ) .The main method employed here are the results derived in parts 1 - 2 combined with Grothendieck s comparison theorem between étale fundamental groups and Galois groups .",
        "rewrite_text": "In this article, we investigate the profinite complex associated with a family of smooth projective surfaces over a field \\( k \\) with a fixed genus \\( g > 0 \\). Our primary finding reveals that this profinite complex is homotopy equivalent to the classifying space of the automorphism group \\( \\text{Aut}(C) \\) of a single curve \\( C \\) from the family, as established in Theorem 1. This result extends previous work by Deligne concerning the automorphism groups of elliptic curves, thereby broadening the understanding of the structure of these groups. Furthermore, we derive new insights into the cohomology groups \\( H^1(\\text{Aut}(C), k^*) \\) for all families of curves under consideration, as highlighted in Corollary 2.\n\nIn the third section, we apply our findings to deepen the understanding of the anabelian geometry associated with the moduli stack \\( M_k \\) of stable curves of genus \\( g \\). Notably, we provide a straightforward proof demonstrating that the Picard scheme \\( \\text{Pic}^0 M_k \\) lacks torsion points when the characteristic of \\( k \\) is zero or when \\( p > 3g - 2 \\), as stated in Proposition 4. The methodologies employed in this study are primarily based on the results obtained in the first two sections, in conjunction with Grothendieck's comparison theorem, which establishes a relationship between étale fundamental groups and Galois groups. This work contributes to the broader field of algebraic geometry by elucidating the connections between automorphisms of curves, their moduli, and the underlying geometric structures.",
        "ori-fast-z-score": -0.3464101615137754,
        "water-fast-z-score": 5.047146145152358,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Diffusive radiation in Langmuir turbulence produced by jet shocks .\nAbstract:\nWe present results on the production and evolution of Langmuir waves (LWs) excited by an intense laser pulse interacting with a supersonic plasma flow, which is generated by a pulsed gas jet. The LWs are detected using Thomson scattering at different angles to the direction of propagation of the laser beam. We show that the LW spectrum has two distinct components: one component corresponds to the excitation of LWs by the ponderomotive force of the laser field; another component arises due to nonlinear wave-wave interactions between counter-propagating LWs. In addition, we find that the LWs can be efficiently converted into electromagnetic waves via Landau damping. Finally, we demonstrate that the LWs decay into ion acoustic waves through modulational instability. These findings provide new insights into the physics of diffusive radiation in plasmas driven by strong shocks. \n \n Introduction \n \n Diffuse radiative emission plays an important role in many astrophysical phenomena such as solar flares  1  , supernovae  2  , gamma-ray bursts  3  . It also occurs in laboratory experiments  4  where it may limit the efficiency of inertial confinement fusion schemes  5  . However, despite its importance there remain significant uncertainties about how diffuse radiation is generated  6  .\n \nIn this Letter, we report experimental measurements of the generation and evolution of Langmuintype waves (LWs), which play a key role in the process of diffusive radiation  7–9  . Our experiment uses a high-power short-pulse laser system operating at 1054 nm wavelength  10  to drive a supersonic plasma expansion  11  . This produces a strongly magnetized plasma  12  containing both electrons and ions  13  . The electron density profile along the axis of the expanding plasma column was measured using interferometry  14  . The temporal evolution of the electron temperature T e was inferred from the line-integrated intensity of the scattered light  15  . The LWs were probed using Thomson scattering  16  at various angles θ to the direction of propagation k 0 of the incident laser beam  17  . \n \n Figure 1 shows typical examples of the time-averaged scattered spectra S(k x ) obtained for",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Diffusive radiation in Langmuir turbulence produced by flying shocks . Abstract : We report findings on the production and evolution of Langmuir waves ( LWs ) excited by an intense laser wave interacting with a supersonic plasma discharge , which is generated by a pulsed gas jet .The LWs are detected using Thomson scattering at different angles to the direction of propagation of the laser beam . We see that the LW spectrum has two different components : one part corresponds to the excitation of LWs by the ponderomotive pressure of the laser field ; another component occurs due to nonlinear wave - wave interactions between counter - propagating LWs .In addition , we find that the LWs can be easily converted into electromagnetic signals via Landau damping . Finally , we prove that the LWs decay into ion audio waves through modulational instability .These studies provide fresh insights into the physics of diffusive radiation in plasmas driven by large shocks . Introduction Diffuse radiative emission serves an important role in major astrophysical processes such as sun flares 1 , supernovae 2 , gamma - ray flare 3 .It additionally occurs in laboratory experiments 4 where it could limit the performance of inertial confinement fusion techniques 5 . However , despite its significance there remain considerable uncertainties about how diffuse radiation is generated 6 .In this Letter , we publish empirical observations of the generation and evolution of Langmuintype currents ( LWs ) , which play a key importance in the process of diffusive radiation 7 – 9 . Our study uses a high - speed small - pulse beam scheme operating at 1054 nm wavelength 10 to drive a supersonic plasma expansion 11 .This generates a strongly magnetized liquid 12 containing both electrons and atoms 13 . The electron concentration profile along the axis of the increasing plasma column was measured using interferometry 14 .The temporal evolution of the electron temperature T e was inferred from the line - integrated brightness of the scattered radiation 15 . The LWs were probed employing Thomson scattering 16 at several angles θ to the direction of propagation k 0 of the incident beam beam 17 .Figure 1 shows typical instances of the time - averaged scattered spectra S ( k x ) obtained for",
        "rewrite_text": "**Title:** Diffusive Radiation in Langmuir Turbulence Induced by Supersonic Shocks\n\n**Abstract:** In this study, we present our findings on the generation and dynamics of Langmuir waves (LWs) that are excited by a powerful laser interacting with a supersonic plasma discharge produced by a pulsed gas jet. The detection of LWs was accomplished through Thomson scattering, conducted at various angles relative to the laser beam's propagation direction. Our results reveal a dual-component spectrum of LWs: one component arises from the ponderomotive pressure exerted by the laser field, while the other is attributed to nonlinear interactions among counter-propagating LWs. Furthermore, we discovered that LWs can be effectively transformed into electromagnetic signals through the process of Landau damping. Notably, we also demonstrate that LWs can decay into ion acoustic waves via modulational instability. These findings contribute significantly to our understanding of the mechanisms behind diffusive radiation in plasmas influenced by substantial shock waves.\n\nThe phenomenon of diffuse radiative emission is crucial in various astrophysical events, including solar flares, supernovae, and gamma-ray bursts, and it also manifests in laboratory settings, where it can impact the efficiency of inertial confinement fusion techniques. Despite its importance, the mechanisms underlying the generation of diffuse radiation remain poorly understood. In this letter, we provide empirical evidence regarding the formation and evolution of Langmuir-type currents, which are pivotal in the context of diffusive radiation. Our experimental setup utilizes a high-speed, short-pulse laser operating at a wavelength of 1054 nm to initiate a supersonic plasma expansion, resulting in a highly magnetized liquid that contains both electrons and neutral atoms. We measured the electron concentration profile along the axis of the expanding plasma column using interferometry, while the temporal evolution of the electron temperature was inferred from the line-integrated brightness of the scattered light. The probing of LWs through Thomson scattering at various angles allowed us to capture the intricate dynamics of this process. Our findings provide new insights into the interplay between Langmuir turbulence and diffusive radiation in plasma environments driven by intense shocks.",
        "ori-fast-z-score": -0.6209204205650662,
        "water-fast-z-score": 7.560864148142503,
        "rewrite-fast-z-score": 1.272891654681168
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Fluctuation dynamo and turbulent induction at low magnetic Prandtl numbers .\nAbstract:\nWe study the generation of large-scale magnetic fields in astrophysical plasmas with very small values of the magnetic Prandtl number, Pm = ν/η ≪ 1 (ν is viscosity, η is resistivity). We consider two different types of flows that are relevant for this problem -helically driven turbulence and convection-driven turbulence. In both cases we find that the mean electromotive force has contributions from several terms which scale differently as functions of the Reynolds number Re = UL/ν and the magnetic Reynolds number Rm = URm/η. Here U , L, and Rm are characteristic velocity, length, and magnetic field scales respectively.  For helically driven turbulence these contributions can be grouped into three categories:  The first category includes all terms proportional to Re(Rm)−1/2 . These terms have been studied previously by many authors using various approaches including direct numerical simulations. They represent the contribution of the so-called α-effect due to helical motions. The second category contains all terms proportional to Re1/2 (Rm)−1/4 . This term represents the effect of helicity on the nonlinear evolution of the magnetic fluctuations. Finally, there exists also an additional third category containing all terms proportional to Re3/4 (Rm)−3/8 . It describes the influence of helicity on the linear growth rate of the magnetic fluctuations.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Fluctuation dynamo and turbulent induction at low magnetic Prandtl numbers . Abstract : We research the generation of large - scale magnetic fields in astrophysical plasmas with very small values of the magnetic Prandtl number , Pm = ν / η [UNK] 1 ( ν is viscosity , η is resistivity ) .We consider two different kinds of flows that are applicable for this question - helically generated turbulence and convection - powered turbulence . In both cases we find that the mean electromotive pressure has contributions from several terms which scale differently as functions of the Reynolds number Re = UL / ν and the magnetic Reynolds number Rm = URm / η .Here U , L , and Rm are characteristic velocity , length , and magnetic force scales respectively . For helically generated turbulence these contributions can be grouped into three categories : The first class includes all terms proportional to Re ( Rm ) −1 / 2 .These concepts have been studied historically by many writers using numerous methodology including continuous mathematical simulations . They represent the contribution of the so - called α - effect owing to helical movements .The second class includes all terms proportional to Re1 / 2 ( Rm ) −1 / 4 . This term reflects the impact of helicity on the nonlinear development of the magnetic fluctuations .Finally , there exists also an additional third category containing all terms proportional to Re3 / 4 ( Rm ) −3 / 8 . It expresses the impact of helicity on the linear expansion frequency of the magnetic fluctuations .",
        "rewrite_text": "In this study, we investigate the mechanisms behind the generation of large-scale magnetic fields in astrophysical plasmas characterized by low magnetic Prandtl numbers (Pm = ν / η, where ν represents viscosity and η denotes resistivity). Our analysis focuses on two distinct types of fluid flows relevant to this phenomenon: helically generated turbulence and convection-driven turbulence. Through our research, we identify that the mean electromotive force is influenced by several terms, each scaling differently with respect to the Reynolds number (Re = UL / ν) and the magnetic Reynolds number (Rm = URm / η), where U, L, and Rm are representative scales of velocity, length, and magnetic force, respectively. \n\nFor helically generated turbulence, we categorize the contributions into three main classes. The first category encompasses terms that scale as Re (Rm) −1 / 2, which have been extensively explored in the literature through various methodologies, including continuous mathematical simulations. These terms are associated with the α-effect, which arises from helical motions within the turbulence. The second category includes terms that scale as Re1 / 2 (Rm) −1 / 4, reflecting the influence of helicity on the nonlinear evolution of magnetic fluctuations. Lastly, we identify a third category of terms that scale as Re3 / 4 (Rm) −3 / 8, which captures the effect of helicity on the linear growth rate of magnetic fluctuations. \n\nOur findings contribute to a deeper understanding of the interplay between turbulence and magnetic field generation in low Pm environments, shedding light on the complex dynamics at play in astrophysical contexts. This research not only advances theoretical knowledge but also has implications for understanding magnetic field behavior in various astrophysical phenomena.",
        "ori-fast-z-score": 0.8867963503478639,
        "water-fast-z-score": 6.021972346615038,
        "rewrite-fast-z-score": 1.4110813025753959
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Space Velocities of L- and T-type Dwarfs .\nAbstract:\nWe present new space velocities for the nearest late-M, L-, and T-dwarf stars based on high-precision radial velocity measurements obtained with the HARPS spectrograph at La Silla Observatory in Chile over an 8-year period (2003-2009). We find that all but one of these objects are members of young nearby open clusters or associations. The only exception is LP 944-20 which has been shown to be a member of the Hyades supercluster by its proper motion.  These results confirm previous suggestions that most brown dwarfs have ages less than 100 Myr. They also show that there may exist two populations among the very low-mass stars and brown dwarfs: those associated with open clusters/associations and those apparently not gravitationally bound to any cluster. This suggests that either some brown dwarfs form outside open clusters and/or they evaporate rapidly after formation. Finally we discuss possible explanations for the observed kinematics of the lowest mass stars and brown dwarfs.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Space Velocities of L - and T - class Dwarfs . Abstract : We create additional space velocities for the nearest late - M , L - , and T - dwarf stars based on high - precision radial speed measurements obtained with the HARPS spectrograph at La Silla Observatory in Chile over an 8 - year period ( 2003 - 2009 ) .We see that all but one of these objects are members of young nearby open complexes or associations . The only exception is LP 944 - 20 which has been shown to be a member of the Hyades supercluster by its proper motion .These data confirm previous suggestions that most brown dwarfs have ages less than 100 Myr . They even show that there may contain two communities among the very low - mass stars and green dwarfs : those associated with open complexes / associations and those presumably not gravitationally bound to any cluster .This implies that either some grey dwarfs form outside open complexes and / or they evaporate rapidly after formed . Finally we explain possible explanations for the known kinematics of the lowest mass stars and brown dwarfs .",
        "rewrite_text": "In this study, we present an analysis of the space velocities of the nearest late-M, L-, and T-class dwarf stars, utilizing high-precision radial velocity measurements collected over an eight-year period (2003-2009) with the HARPS spectrograph at La Silla Observatory in Chile. Our findings reveal that nearly all of the observed objects are part of young, nearby open clusters or associations, with the notable exception of LP 944-20, which has been identified as a member of the Hyades supercluster based on its proper motion. This research supports earlier hypotheses suggesting that the majority of brown dwarfs are relatively young, with ages typically less than 100 million years. Furthermore, our data indicate the existence of two distinct populations among very low-mass stars and brown dwarfs: one group that is associated with open clusters and associations, and another that appears to be unbound to any gravitationally cohesive structure. This observation raises intriguing questions about the formation processes of these grey dwarfs, suggesting that they may either form outside of open complexes or undergo rapid evaporation shortly after their formation. Additionally, we discuss potential explanations for the observed kinematic behaviors of the lowest mass stars and brown dwarfs, contributing to a deeper understanding of their dynamics and evolutionary pathways. Overall, our study enhances the current knowledge of the spatial distribution and kinematics of these celestial objects, shedding light on their formation and the environments in which they exist.",
        "ori-fast-z-score": -0.7977240352174656,
        "water-fast-z-score": 3.760699023168052,
        "rewrite-fast-z-score": -0.19069251784911848
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Local de Broglie-Bohm Trajectories from Entangled Wavefunctions .\nAbstract:\nWe present an algorithm for computing trajectories in the Bohmian interpretation of quantum mechanics, based on wavefunction entanglement and locality constraints. The method is applied to two examples with one or more particles in 1D space. We show that our results are consistent with those obtained by other methods when they exist; otherwise we obtain new insights into the dynamics of these systems. Our approach can be used as a tool for studying the foundations of quantum mechanics, and may also have practical applications such as guiding the design of novel quantum algorithms. Quantum mechanics has been successful at predicting experimental outcomes across many different physical phenomena  1  . However, it remains unclear how this theory should be interpreted physically  2  , especially since its predictions cannot always be reproduced experimentally  3  .\nOne possible way forward is to consider alternative interpretations of quantum mechanics  4  . In particular, there exists a class of theories known as hidden-variable theories  5  which attempt to explain quantum mechanical measurements using additional degrees of freedom not included within standard quantum mechanics  6  . One popular example of such a theory is Bohm s formulation  7, 8  where each particle follows a trajectory through configuration space  9  . This theory reproduces all of the statistical predictions made by standard quantum mechanics  10  but provides a deterministic description of individual measurement events  11  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Local de Broglie - Bohm Trajectories from Entangled Wavefunctions . Abstract : We present an algorithm for computing trajectories in the Bohmian interpretation of quantum mechanics , using on wavefunction entanglement and locality requirements .The method is applied to two examples with one or more particles in 1D space . We see that our findings are compatible with those achieved by other methods when they exist ; therefore we obtain new insights into the dynamics of these systems .Our concept can be used as a platform for studying the foundations of quantum mechanics , and may also have useful use such as assisting the development of new quantum algorithms . Quantum theory has been good at predicting experimental outcomes across many various physical phenomena 1 .However , it remains unsure how this theory should be interpreted physically 2 , particularly since its predictions cannot often be verified experimentally 3 . One potential way forward is to consider alternative definitions of quantum mechanics 4 .In particular , there exists a class of theories named as hiding - variable theories 5 which attempt to explain quantum mechanical measurements using new degrees of liberty not incorporated within conventional quantum mechanics 6 . One popular example of such a theory is Bohm s formulation 7 , 8 where each particle proceeds a trajectory through configuration space 9 .This theory reproduces all of the empirical observations made by typical quantum mechanics 10 but provides a deterministic model of individual detection events 11 .",
        "rewrite_text": "We introduce a novel algorithm designed to compute trajectories within the Bohmian framework of quantum mechanics, emphasizing the roles of wavefunction entanglement and locality constraints. This approach is demonstrated through two illustrative examples involving one or more particles in one-dimensional space. Our results align with those obtained through existing methodologies, where applicable, thereby offering fresh perspectives on the dynamics of these quantum systems. This framework not only serves as a foundation for exploring the principles underlying quantum mechanics but also holds potential applications in the advancement of new quantum algorithms.\n\nQuantum theory has proven effective in predicting experimental outcomes across a wide range of physical phenomena. However, the physical interpretation of this theory remains a subject of debate, particularly because its predictions are not always experimentally verifiable. One promising avenue for addressing this ambiguity is to explore alternative interpretations of quantum mechanics. Among these, hidden-variable theories seek to account for quantum measurements by introducing additional degrees of freedom that are not captured by traditional quantum mechanics. A prominent example of such a theory is the de Broglie-Bohm interpretation, which posits that each particle follows a specific trajectory through configuration space. This interpretation successfully reproduces all empirical results of standard quantum mechanics while offering a deterministic framework for individual measurement events. Our work contributes to this ongoing discourse by providing a computational tool that enhances our understanding of quantum trajectories and their implications for both theoretical and practical advancements in the field.",
        "ori-fast-z-score": 0.09245003270420485,
        "water-fast-z-score": 6.812252172447593,
        "rewrite-fast-z-score": 2.090909090909091
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Poisson approximation for non-backtracking random walks .\nAbstract:\nWe consider the probability distribution of the number of times that a simple random walk on an infinite graph visits some fixed vertex in finite time, and show how this can be approximated by a Poisson distribution with mean equal to the expected value of the number of visits.  We then use these results to prove that the total variation distance between the two distributions is bounded above by $O(1/n)$ where $n$ denotes the number of vertices in the graph.   This result generalizes previous work which was restricted to graphs whose degree sequence has exponential decay or polynomial growth.    The proof relies heavily upon recent advances in the theory of large deviations for sums of dependent Bernoulli variables. In particular we make use of the so-called Cramér-Chernoff method as well as the concept of a supermartingale. Finally, we apply our main theorem to obtain new bounds on the mixing time of lazy random walks on regular trees.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Poisson approximation for non - backtracking random tours . Abstract : We consider the probability distribution of the number of twice that a simple random walk on an infinite graph visits some fixed vertex in finite time , and know how this can be approximated by a Poisson distribution with mean equivalent to the expected value of the number of visits .We then use these results to prove that the total difference distance between the two distributions is bounded above by $ O ( 1 / n ) $ where $ n $ represents the number of vertices in the graph . This result generalizes earlier paper which was confined to vertices whose degree sequence has exponential decay or polynomial growth .The proof draws highly upon recent developments in the principle of large deviations for sums of dependent Bernoulli variables . In particular we making use of the so - called Cramér - Chernoff formula as well as the idea of a supermartingale .Finally , we apply our major principle to obtain new limits on the mix time of lazy random walks on regular trees .",
        "rewrite_text": "In this article, we investigate the probability distribution concerning the frequency with which a simple random walk on an infinite graph revisits a specific vertex within a finite timeframe. Our findings indicate that this distribution can be effectively approximated by a Poisson distribution, where the mean corresponds to the expected number of visits to that vertex. Building on this foundational result, we establish that the total variation distance between the actual distribution and its Poisson approximation is constrained above by \\( O(1/n) \\), with \\( n \\) denoting the total number of vertices in the graph. This significant result extends previous research that was limited to vertices characterized by either an exponential decay or polynomial growth in their degree sequences. \n\nThe proof of our main theorem leverages recent advancements in the principles of large deviations, particularly concerning sums of dependent Bernoulli random variables. We employ the Cramér-Chernoff theorem and utilize the concept of a supermartingale to facilitate our analysis. Furthermore, we apply our primary findings to derive new bounds on the mixing time of lazy random walks on regular trees, thereby contributing to the understanding of random processes on graph structures. This work not only enhances the theoretical framework surrounding random walks but also opens avenues for future research in probabilistic graph theory and its applications.",
        "ori-fast-z-score": -1.1659976680069961,
        "water-fast-z-score": 3.8786538958710977,
        "rewrite-fast-z-score": 0.8867963503478639
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Determination of Low-Energy Parameters of Neutron--Proton Scattering on the Basis of Modern Experimental Data from Partial-Wave Analyses .\nAbstract:\nThe low-energy parameters of neutron-proton scattering are determined by using modern experimental data obtained in partial-wave analyses (PWAs). The results for the S-wave phase shifts and mixing angles, as well as for the P-wave amplitudes at zero energy, are presented here. It is shown that these values agree with those extracted previously from other experiments within their uncertainties. In addition to this, we present new results for the D-wave amplitude at zero energy which were not available before. \nI. INTRODUCTIO N\nNeutron-proton elastic scattering has been studied extensively over many decades  1  . This process plays an important role in nuclear physics since it provides information about the nucleon-nucleon interaction potential  2  , which can be used to calculate properties of nuclei  3  .\nIn recent years there have been significant advances in our understanding of the structure of the nucleon-nucleus system  4  . These developments include precise measurements of cross sections  5  , polarization observables  6  , spin-correlation coefficients  7  , etc., performed mainly at intermediate energies  8  . However, despite all efforts made so far, some questions remain open  9  . For example, one still needs more accurate determinations of the lowenergy parameters of the nucleonnucleon interaction  10  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Determination of Low - Energy Parameters of Neutron - - Proton Scattering on the Basis of Modern Experimental Data from Partial - Wave Analyses . Abstract : The lowest - energy parameters of neutron - proton scattering are decided by using contemporary experimental evidence derived in partial - wave assessments ( PWAs ) .The results for the S - wave phase variations and mixing angles , as well as for the P - wave amplitudes at zero energy , are presented here . It is demonstrated that these estimates agree with those retrieved previously from other experiments within their uncertainties .In addition to this , we present new data for the D - wave amplitude at zero energy which were not accessible before . I . INTRODUCTIO N Neutron - proton elastic scattering has been studied thoroughly over numerous years 1 .This process holds an important role in nuclear science since it gives information about the nucleon - nucleon correlation potential 2 , which can be used to estimate features of nuclei 3 . In recent years there have been significant advances in our knowing of the formation of the nucleon - nucleus system 4 .These advances require precise observations of cross sections 5 , polarization observables 6 , spin - correlation functions 7 , etc . , conducted predominantly at intermediate energies 8 . However , despite all efforts made so far , some questions remain open 9 .For instance , one also needs more accurate determinations of the lowenergy variables of the nucleonnucleon interaction 10 .",
        "rewrite_text": "**Title:** Determination of Low-Energy Parameters of Neutron-Proton Scattering Based on Modern Experimental Data from Partial-Wave Analyses\n\n**Abstract:** This article presents a comprehensive analysis of the low-energy parameters associated with neutron-proton scattering, utilizing contemporary experimental data obtained from partial-wave analyses (PWAs). The study focuses on the S-wave phase shifts and mixing angles, as well as the P-wave amplitudes at zero energy, providing a detailed overview of the findings. The results indicate a strong agreement with previously reported values from other experimental studies, remaining consistent within their respective uncertainties. Furthermore, this research introduces novel data regarding the D-wave amplitude at zero energy, which has not been previously accessible in the literature. \n\nNeutron-proton elastic scattering has been a focal point of nuclear physics research for many years, as it plays a crucial role in understanding the nucleon-nucleon interaction potential. This potential is vital for estimating various properties of atomic nuclei. Recent advancements in our comprehension of nucleon-nucleus systems have underscored the necessity for precise measurements of cross sections, polarization observables, and spin-correlation functions, primarily conducted at intermediate energies. Despite the progress made, several questions regarding low-energy parameters of nucleon-nucleon interactions remain unresolved. Accurate determination of these parameters is essential for enhancing our understanding of nuclear forces and the underlying physics governing nucleon interactions. This article aims to address these gaps by providing updated and refined measurements that contribute to the ongoing discourse in nuclear science.",
        "ori-fast-z-score": -0.502518907629606,
        "water-fast-z-score": 6.6,
        "rewrite-fast-z-score": 0.44367825470805694
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Asteroseismology of alpha Cen A. Evidence of rotational splitting .\nAbstract:\nWe present new results on the asteroseismic analysis of the primary component in the binary system Alpha Centari, based on data obtained with the HARPS spectrograph at La Silla Observatory (Chile). We find evidence for two independent frequencies that are likely to be associated with rotationally split modes. The observed frequency pattern is consistent with theoretical predictions and suggests an inclination angle between 40°and 60°for this star. \n \n Keywords: Asteroseismology, Rotation, Binary stars, Oscillations, Frequency analysis, High-precision radial velocities, Alpha Centari ABSTRACT \n \n We report new results on the asterioseismic analysis of the main-sequence F-type star Alpha Centari A, which forms part of a close double system with its cooler companion B. Our study was carried out using high-precision radial-velocity measurements collected over more than four years by the HARPS instrument installed at ESO s 3.6-m telescope at La Silla Observatory (Chilean Andes), together with photometric observations made simultaneously with the CoRoT space mission. By applying standard techniques used in asteroseismology we have detected several periodicities in both datasets, including one signal whose periodicity corresponds exactly to the orbital period of the system. This finding confirms previous suggestions that the pulsational behaviour of this star may be influenced by tidal effects induced by its companion. In addition, our analysis reveals another set of signals corresponding to periods ranging from about 1 day up to almost 2 days. These signals can be explained as being due to rotationally split p-mode oscillations excited in the convective envelope of the star. Their presence provides strong support for the hypothesis that the surface of Alpha Centari A has been shaped by magnetic activity driven by dynamo processes operating within the convection zone.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Asteroseismology of α Cen A . Evidence of rotational separation .Abstract : We report new data on the asteroseismic study of the primary component in the binary system Alpha Centari , using on evidence derived with the HARPS spectrograph at La Silla Observatory ( Chile ) . We see evidence for two independent frequencies that are likely to be involved with rotationally split modes .The observed frequency trend is compatible with theoretical estimates and suggests an inclination angle between 40°and 60°for this star . Keywords : Asteroseismology , Rotation , Binary stars , Oscillations , Frequency assessment , High - precision radial velocities , Alpha Centari ABSTRACT We report new data on the asterioseismic study of the main - sequence F - class star Alpha Centari A , which forms part of a close double system with its warmer companion B .Our study was carried out utilizing large - precision radial - speed measurements collected over more than four years by the HARPS instrument located at ESO s 3 . 6 - m observatory at La Silla Observatory ( Chilean Andes ) , combined with photometric surveys made independently with the CoRoT space expedition . By applying traditional techniques employed in asteroseismology we have discovered numerous periodicities in both datasets , notably one signal whose periodicity corresponds exactly to the orbital period of the system .This finding indicates past proposals that the pulsational evolution of this star may be altered by tidal impacts generated by its companion . In addition , our analysis reveals another set of signals relating to periods ranging from about 1 day up to approximately 2 days .These transmissions can be understood as being related to rotationally split p - mode oscillations excited in the convective envelope of the star . Their presence provides strong evidence for the notion that the surface of Alpha Centari A has been shaped by magnetic activity driven by dynamo mechanisms operating within the convection zone .",
        "rewrite_text": "**Title: Asteroseismology of α Cen A: Evidence of Rotational Separation**\n\n**Abstract:** In this study, we present new findings from the asteroseismic analysis of Alpha Centauri A, the primary star in the binary system Alpha Centauri, utilizing high-precision radial velocity data obtained from the HARPS spectrograph at La Silla Observatory in Chile. Our investigation reveals the presence of two distinct frequencies that are likely associated with rotationally split modes. The observed frequency patterns align with theoretical predictions, suggesting an inclination angle for Alpha Centauri A between 40° and 60°. \n\nOur research focuses on the main-sequence F-type star Alpha Centauri A, which is part of a close binary system with its hotter companion, Alpha Centauri B. We conducted our analysis using extensive radial velocity measurements collected over a span of more than four years, complemented by independent photometric data from the CoRoT space mission. By employing standard asteroseismic techniques, we identified multiple periodic signals in both datasets, including one that corresponds precisely to the orbital period of the binary system. This correlation supports previous hypotheses that the pulsational characteristics of Alpha Centauri A may be influenced by tidal interactions with its companion star.\n\nFurthermore, our analysis uncovers additional signals with periods ranging from approximately 1 to 2 days. These oscillations are interpreted as rotationally split p-mode oscillations, which are likely excited within the star's convective envelope. The detection of these oscillations provides compelling evidence that the surface characteristics of Alpha Centauri A have been significantly shaped by magnetic activity resulting from dynamo processes occurring in its convective zone. Overall, our findings contribute to a deeper understanding of the asteroseismic properties of Alpha Centauri A and the effects of its binary nature on its stellar evolution. \n\n**Keywords:** Asteroseismology, Rotation, Binary stars, Oscillations, Frequency assessment, High-precision radial velocities, Alpha Centauri.",
        "ori-fast-z-score": -1.2909944487358056,
        "water-fast-z-score": 6.627104836843802,
        "rewrite-fast-z-score": 0.4402254531628119
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Scale-dependent Galaxy Bias .\nAbstract:\nWe present the results of an analysis of galaxy clustering in the Sloan Digital Sky Survey (SDSS) Data Release 7, using scale-independent and -dependent bias models to describe the observed correlation function on scales ranging from 0.1 h-1 Mpc to 20 h-1 Mpc.  We find that both models provide good fits to the data at large separations but only the scale-independent model provides a reasonable fit over all scales considered here. The best-fit values for the linear bias parameter are b1 = 1.7 +/- 0.2 for the scale-independent case and b2 = 2.3 +/- 0.4 for the scale-dependent case. These results suggest that galaxies may be biased tracers of dark matter fluctuations with different bias parameters depending upon their mass or luminosity. This is consistent with previous studies which have found evidence for scale dependence in the galaxy-matter cross-correlation function. However, we note that our conclusions depend strongly on the assumed functional form for the scale dependent bias factor.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Scale - dependent Galaxy Bias . Abstract : We present the conclusion of an assessment of galaxy clustering in the Sloan Digital Sky Survey ( SDSS ) Data Release 7 , using scale - independent and - dependent bias models to explain the seen correlation function on scales ranging from 0 . 1 h - 1 Mpc to 20 h - 1 Mpc .We find that both models provide good fits to the data at large separations but only the scale - independent model provides a reasonable fit over all scales considered here . The best - fit values for the linear bias parameter are b1 = 1 . 7 + / - 0 . 2 for the scale - independent case and b2 = 2 . 3 + / - 0 . 4 for the scale - dependent case .These data suggest that galaxies must be biased tracers of brown matter fluctuations with various bias parameters based upon their mass or luminosity . This is consistent with previous research which have discovered evidence for scale dependence in the galaxy - matter cross - correlation function .However , we note that our conclusions depend greatly on the assumed functional shape for the scale dependent bias function .",
        "rewrite_text": "We present the findings of our investigation into galaxy clustering utilizing data from the Sloan Digital Sky Survey (SDSS) Data Release 7. Our study employs both scale-independent and scale-dependent bias models to interpret the observed correlation function across a range of scales, specifically from 0.1 h⁻¹ Mpc to 20 h⁻¹ Mpc. Our analysis reveals that while both bias models yield satisfactory fits to the data at larger separations, only the scale-independent model maintains a reasonable fit across all scales examined. The optimal values for the linear bias parameter are determined to be b₁ = 1.7 ± 0.2 for the scale-independent model and b₂ = 2.3 ± 0.4 for the scale-dependent model. These results indicate that galaxies act as biased tracers of dark matter fluctuations, with varying bias parameters influenced by their mass or luminosity. This observation aligns with prior studies that have identified evidence of scale dependence in the galaxy-matter cross-correlation function. However, we emphasize that our conclusions are significantly influenced by the chosen functional form of the scale-dependent bias function. This work contributes to the ongoing discourse on the relationship between galaxy distribution and underlying dark matter structures, highlighting the complexities involved in accurately modeling galaxy bias in cosmological studies.",
        "ori-fast-z-score": -3.1091263510296048,
        "water-fast-z-score": 0.6396021490668313,
        "rewrite-fast-z-score": -1.5689290811054724
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Jet-like Outflow toward the High-Mass (Proto)stellar Object IRAS 18566+0408 .\nAbstract:\nWe report on observations made with the Submillimeter Array and the Atacama Large Millimeter/submillimeter Array in order to study the kinematics of an outflow driven by the high-mass protostellar object, IRAS 18566+0408; this source is associated with a cluster of young stellar objects located at a distance of 3 kpc. The data reveal that there are two components along the line-of-sight; one component has a systemic velocity of ~10 km s-1 , while another component shows blueshifted emission up to -60 km s-1 . We find evidence for a collimated jet-like structure extending over ~0.5 pc. This suggests that the driving source may be deeply embedded within its natal cloud core. In addition, we detect several compact knots distributed along the flow axis which show blue-shifted velocities ranging between 10-60 km s-1 .\nThe mass-loss rate estimated from our observations ranges between 1×10-3 -1×10-2 M⊙ yr-1 .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Jet - like Outflow toward the High - Mass ( Proto ) stellar Object IRAS 18566 + 0408 . Abstract : We report on observations made with the Submillimeter Array and the Atacama Large Millimeter / submillimeter Array in order to study the kinematics of an outflow pushed by the high - weight protostellar body , IRAS 18566 + 0408 ; this source is associated with a cluster of young stellar bodies located at a distance of 3 kpc .The data reveal that there are two parts along the line - of - seeing ; one element has a systemic speed of ~ 10 km s - 1 , while another component displays blueshifted emission up to - 60 km s - 1 . We see evidence for a collimated jet - like structure extending over ~ 0 . 5 pc .This implies that the driving source may be deeply lodged within its natal cloud core . In addition , we find several compact knots scattered along the flow axis which show blue - shifted velocities ranging between 10 - 60 km s - 1 .The mass - loss rate estimated from our observations ranges between 1×10 - 3 - 1×10 - 2 [UNK] yr - 1 .",
        "rewrite_text": "We present findings from our observations conducted using the Submillimeter Array and the Atacama Large Millimeter/submillimeter Array, focusing on the kinematics of an outflow driven by the high-mass protostellar object IRAS 18566 + 0408. This particular source is situated within a cluster of young stellar objects at a distance of approximately 3 kpc. Our analysis reveals two distinct components along the line of sight: one exhibiting a systemic velocity of around 10 km/s, while the other shows blueshifted emissions reaching velocities of up to -60 km/s. Notably, we have identified a collimated jet-like structure that extends approximately 0.5 parsecs, suggesting that the driving protostar is likely embedded within its surrounding natal cloud core. Furthermore, our observations have uncovered several compact knots distributed along the flow axis, which exhibit blue-shifted velocities ranging from 10 to 60 km/s. The estimated mass-loss rate derived from our data varies between 1×10^-3 and 1×10^-2 solar masses per year. These findings contribute to our understanding of the dynamics of high-mass star formation and the role of outflows in the evolution of protostellar systems.",
        "ori-fast-z-score": 0.5933908290969266,
        "water-fast-z-score": 4.478342947514801,
        "rewrite-fast-z-score": 0.7875615306482168
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A 3-Stranded Quantum Algorithm for the Jones Polynomial .\nAbstract:\nWe present an algorithm to compute the Jones polynomial using three quantum wires and one ancilla qubit, which is based on the idea that the Jones polynomial can be computed by evaluating the Tutte polynomial at 1. We show how this approach leads naturally to a circuit with O(n) gates in depth O(log n), where n denotes the number of crossings in the knot diagram. The main ingredient of our construction are two new families of unitary operators acting on tensor products of Hilbert spaces associated with graphs. These operators generalize the swap operator and have applications beyond computing the Jones polynomial. In particular we use them to construct efficient quantum circuits for other problems such as counting perfect matchings or finding maximum flows. \nIntroduction\n\nThe Jones polynomial J K (t) ∈ Z t ±1/2   of a knot K is a Laurent polynomial invariant of knots and links defined by V. Jones  Jon87  . It has been shown that it distinguishes all prime knots up to 10 crossings  Kaw96  , but its complexity makes it hard to evaluate even for small values of t. For example, if K is the trefoil knot then J K (1) = −1/4 while J K (−1/2) = 1/2. This raises the question whether there exists a quantum algorithm to efficiently calculate the Jones polynomial.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A 3 - Stranded Quantum Algorithm for the Jones Polynomial . Abstract : We present an algorithm to compute the Jones polynomial utilizing three quantum wires and one ancilla qubit , which is based on the idea that the Jones function can be computed by evaluating the Tutte polynomial at 1 .We see how this step leads naturally to a circuit with O ( n ) gates in depth O ( log n ) , where n represents the number of crossings in the knot diagram . The main ingredient of our build are two new families of unitary operators acting on tensor products of Hilbert spaces identified with graphs .These operators generalize the swap operator and have applications beyond computing the Jones function . In particular we using them to build efficient quantum networks for other issues such as figuring ideal matchings or finding maximum flows .Introduction The Jones function J K ( t ) ∈ Z t ±1 / 2 of a knot K is a Laurent polynomial invariant of knots and links given by V . Jones Jon87 . It has been shown that it distinguishes all prime knots up to 10 crossings Kaw96 , but its complexity makes it difficult to analyze even for large values of t . For instance , if K is the trefoil knot then J K ( 1 ) = −1 / 4 while J K ( −1 / 2 ) = 1 / 2 .This opens the question whether there exists a quantum algorithm to easily calculate the Jones function .",
        "rewrite_text": "We introduce a novel quantum algorithm designed to compute the Jones polynomial using a configuration of three quantum wires and one ancilla qubit. This approach is grounded in the principle that the Jones function can be derived by evaluating the Tutte polynomial at the value of 1. Our findings reveal that this methodology leads to the construction of a quantum circuit characterized by O(n) gates and a depth of O(log n), where n denotes the number of crossings present in the knot diagram. A pivotal component of our algorithm is the development of two new families of unitary operators that operate on tensor products of Hilbert spaces associated with graphs. These operators extend the functionality of the traditional swap operator and have broader implications beyond the computation of the Jones polynomial. Specifically, we leverage these operators to create efficient quantum networks that address additional computational challenges, such as determining ideal matchings and identifying maximum flows in networks.\n\nThe Jones function, denoted as J_K(t) ∈ Z[t^{±1/2}], serves as a Laurent polynomial invariant for knots and links, as established by V. Jones in 1987. It has been demonstrated that this function can distinguish all prime knots with up to 10 crossings, as noted by Kawauchi in 1996. However, the inherent complexity of the Jones polynomial poses significant challenges for analysis, particularly for larger values of t. For example, for the trefoil knot, we find that J_K(1) = -1/4, while J_K(-1/2) = 1/2. This complexity raises an important question regarding the existence of a quantum algorithm capable of efficiently calculating the Jones function. Our work aims to address this question by providing a structured quantum approach that not only simplifies the computation of the Jones polynomial but also enhances the potential for solving related combinatorial problems in quantum computing.",
        "ori-fast-z-score": -0.8251369970070347,
        "water-fast-z-score": 4.001315573132102,
        "rewrite-fast-z-score": -1.2094157958139042
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Branch and Cut Algorithm for the Halfspace Depth Problem .\nAbstract:\nWe present an algorithm to solve the half-space depth problem, which is one of the most fundamental problems in computational geometry. The input consists of n points on the plane with integer coordinates bounded by B. We want to find a point that minimizes its distance to all other points. This problem has been studied extensively since it was introduced by Helly in 1930s. In this work we propose a new branch-and-cut algorithm based on mixed-integer programming (MIP) formulation. Our MIP model uses O(nB) variables and constraints. Using our proposed cutting planes, we can reduce the number of nodes explored during search significantly. As a result, our algorithm runs faster than existing algorithms when the dimension d = 2 or 3. For example, if the dimension is two, then our algorithm solves instances with up to 1 million points within 10 minutes while previous best known results are limited to 100 thousand points.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Branch and Cut Algorithm for the Halfspace Depth Problem . Abstract : We present an algorithm to solve the half - space depth problem , which is one of the most important problems in computational geometry .The input consists of n points on the plane with integer coordinates bounded by B . We want to find a point that minimizes its distance to all other points .This problem has been studied thoroughly since it was introduced by Helly in 1930s . In this study we propose a new branch - and - cut optimization based on mixed - integer programming ( MIP ) implementation .Our MIP approach utilizes O ( nB ) parameters and constraints . Using our proposed cutting planes , we can restrict the quantity of vertices uncovered during search significantly .As a effect , our algorithm runs faster than existing algorithms when the dimension d = 2 or 3 . For instance , if the dimension is two , then our algorithm solves instances with up to 1 million points within 10 minutes while prior best known performance are limited to 100 thousand points .",
        "rewrite_text": "We introduce a novel algorithm designed to address the half-space depth problem, a fundamental challenge in the field of computational geometry. The problem involves determining a point in a two-dimensional plane, defined by n integer-coordinate points constrained by a bound B, that minimizes its distance to all other points in the set. This problem, first articulated by Helly in the 1930s, has garnered significant attention and research over the decades. In our work, we propose an innovative branch-and-cut optimization technique grounded in mixed-integer programming (MIP). Our MIP formulation incorporates O(nB) parameters and constraints, allowing for a more efficient exploration of the solution space. By implementing strategically designed cutting planes, we effectively reduce the number of uncovered vertices during the search process, which enhances the algorithm's performance. Our experimental results demonstrate that this new approach significantly outperforms existing algorithms, particularly in lower dimensions (d = 2 or 3). For example, in two-dimensional scenarios, our algorithm is capable of processing datasets containing up to 1 million points in under 10 minutes, a substantial improvement over the previous best-known algorithms, which were limited to handling only 100,000 points in the same timeframe. This advancement not only highlights the efficiency of our method but also opens new avenues for tackling larger datasets in computational geometry, thereby contributing to the ongoing discourse in this area of research.",
        "ori-fast-z-score": -0.8944271909999159,
        "water-fast-z-score": 3.1304951684997055,
        "rewrite-fast-z-score": 0.18569533817705186
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Structure of the stationary state of the asymmetric target process .\nAbstract:\nWe study the structure of the stationary distribution for an asymmetric version of the Target Process (TP). The TP is a continuous-time Markov chain on the integers, which has been introduced by Aldous and Pitman in their seminal work  1  . It can be seen as a model for the evolution of a population with immigration and emigration at fixed rates. In this article we consider a generalization where individuals are allowed to emigrate only if they have reached some threshold value k ∈ N 0 , i.e., the number of immigrants per unit time decreases linearly when the current size of the population increases beyond k. We show that under mild conditions there exists a unique stationary distribution π = (π(n)) n∈N0 such that π(k) > 0 holds true. Moreover, we prove that π satisfies a recursion formula similar to the one obtained for the original TP. Finally, we provide explicit expressions for π(1), ..., π(k+1).\nKeywords: Asymmetric target process, Stationary distribution",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Structure of the stationary state of the asymmetric target process . Abstract : We research the structure of the stationary distribution for an asymmetric version of the Target Process ( TP ) .The TP is a continuous - time Markov chain on the integers , which has been proposed by Aldous and Pitman in their seminal book 1 . It can be viewed as a theory for the evolution of a population with immigration and emigration at fixed rates .In this article we define a generalization where persons are allowed to emigrate only if they have achieved some threshold quantity k ∈ N 0 , i . e . , the proportion of refugees per unit time decreases linearly when the present size of the population rises beyond k . We see that under mild conditions there exists a unique stationary function π = ( π ( p ) ) n∈N0 such that π ( k ) > 0 holds true . Moreover , we prove that π satisfies a recursion formula related to the one given for the previous TP .Finally , we provide explicit expressions for π ( 1 ) , . . . , π ( k + 1 ) . Keywords : Asymmetric target process , Stationary distribution",
        "rewrite_text": "**Title:** Structure of the Stationary State of the Asymmetric Target Process\n\n**Abstract:** This study investigates the stationary distribution structure of an asymmetric variant of the Target Process (TP), a continuous-time Markov chain defined over the integers, initially introduced by Aldous and Pitman in their influential work. The TP framework serves as a model for understanding population dynamics characterized by immigration and emigration occurring at constant rates. In our research, we extend the traditional TP model by imposing a condition where individuals can only emigrate upon reaching a specified threshold quantity \\( k \\in \\mathbb{N}_0 \\). This modification leads to a scenario where the rate of emigration decreases linearly as the population size surpasses the threshold \\( k \\). Our findings reveal that, under relatively mild conditions, there exists a unique stationary distribution function \\( \\pi = (\\pi(p))_{n \\in \\mathbb{N}_0} \\) such that \\( \\pi(k) > 0 \\). Furthermore, we demonstrate that this stationary distribution satisfies a recursion relation analogous to that of the original TP. We also derive explicit expressions for the values of \\( \\pi(1), \\ldots, \\pi(k + 1) \\), providing deeper insights into the behavior of the system. This work contributes to the understanding of population processes with asymmetric characteristics and highlights the implications of threshold-based emigration on the stationary state of the system. \n\n**Keywords:** Asymmetric target process, Stationary distribution",
        "ori-fast-z-score": -1.0392304845413263,
        "water-fast-z-score": 3.5795716689756794,
        "rewrite-fast-z-score": -0.30460384954008574
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Searching for galactic cosmic ray pevatrons with multi-TeV gamma rays and neutrinos .\nAbstract:\nWe present the results of searches for Pevatron candidates in the northern hemisphere using data collected by the High Energy Stereoscopic System (HESS) between 2004 and 2007, as well as IceCube data taken during 2005-2007. We find no significant excesses above background expectations at any point on the sky. Upper limits are set on the flux density of TeV photons and neutrinos associated with hypothetical sources within our field-of-view. These upper limits are used to constrain theoretical models describing the production mechanisms responsible for accelerating particles up to energies approaching 10^14 eV. The HESS collaboration has recently reported an observation of a new source of very-high-energy (VHE; >100 GeV) gamma-rays located near the Galactic Center  1  . This source is spatially coincident with the supernova remnant Sgr A East  2  , which was previously detected in radio waves  3  .\nThe discovery of this VHE source raises several questions about its origin. In particular, it remains unclear whether or not the observed emission arises directly from accelerated protons interacting with ambient gas  4  , or if other processes such as inverse Compton scattering off electrons  5  and/or bremsstrahlung  6  play a dominant role. It also remains unknown how these energetic particles were accelerated to their high energy levels  7, 8  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Searching for galactic gamma ray pevatrons with multi - TeV gamma particles and neutrinos . Abstract : We present the results of investigations for Pevatron candidates in the northern hemisphere using data taken by the High Energy Stereoscopic System ( HESS ) between 2004 and 2007 , as also as IceCube information taken during 2005 - 2007 .We get no major excesses above background expectations at any point on the sky . Upper constraints are set on the flux concentration of TeV photons and neutrinos associated with hypothetical sources within our field - of - view .These upper limits are applied to constrain theoretical theories describing the production mechanisms involved for accelerating particles up to energies approaching 10 ^ 14 eV . The HESS collaboration has recently noted an observation of a new source of very - large - energy ( VHE ; > 100 GeV ) γ - radiation located near the Galactic Center 1 .This source is spatially coincident with the supernova remnant Sgr A East 2 , which was formerly detected in radio pulses 3 . The observation of this VHE source raises various issues about its identity .In particular , it remains unsure whether or not the seen emission arises directly from accelerated protons interacting with ambient gas 4 , or if other processes such as inverse Compton absorption off electrons 5 and / or bremsstrahlung 6 hold a dominant role . It additionally continues unclear how these energetic particles were accelerated to their high energy levels 7 , 8 .",
        "rewrite_text": "**Title:** Investigating Galactic Gamma-Ray Pevatrons with Multi-TeV Gamma Particles and Neutrinos\n\n**Abstract:** In this study, we report on our search for potential Pevatron candidates located in the northern hemisphere, utilizing data collected by the High Energy Stereoscopic System (HESS) from 2004 to 2007, alongside observations from IceCube gathered between 2005 and 2007. Our analysis reveals no significant excesses above the expected background across the surveyed sky. Consequently, we establish upper limits on the flux of TeV photons and neutrinos that may be associated with hypothetical sources within our observational field. These constraints are instrumental in refining theoretical models that describe the mechanisms responsible for accelerating particles to energies nearing 10^14 eV.\n\nRecently, the HESS collaboration reported the detection of a new source of very high-energy (VHE; > 100 GeV) gamma radiation situated near the Galactic Center. This source is spatially correlated with the supernova remnant Sgr A East, which has previously been identified through radio emissions. The emergence of this VHE source prompts several questions regarding its nature and origin. A key uncertainty lies in whether the observed gamma emissions are a direct result of accelerated protons interacting with surrounding gas or if alternative processes, such as inverse Compton scattering involving electrons or bremsstrahlung, play a more significant role in the observed radiation.\n\nMoreover, the mechanisms by which these high-energy particles are accelerated remain ambiguous. Understanding these processes is crucial for elucidating the dynamics of cosmic ray production and the role of supernova remnants in the broader context of galactic astrophysics. Our findings contribute to the ongoing discourse surrounding the identification and characterization of Pevatron candidates, thereby enhancing our comprehension of high-energy astrophysical phenomena.",
        "ori-fast-z-score": -1.0,
        "water-fast-z-score": 5.2,
        "rewrite-fast-z-score": -1.5716505559714824
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  How to tell a gravastar from a black hole .\nAbstract:\nGravastars are hypothetical objects that may be the final state of stellar evolution, and which have been proposed as alternatives to black holes.  Gravastars can be distinguished from black holes by their gravitational lensing effects on background stars.   In this work we show how these differences in lensing behavior could be used to distinguish between gravastars and black holes using observations made with current or future space-based telescopes such as LSST (Large Synoptic Survey Telescope) and WFIRST-AFTA (Wide Field Infrared Space Telescope - Astro-Physics). Gravitational lenses are powerful tools for studying distant galaxies because they magnify images of background sources.  The most famous example is the Einstein ring produced when light passes close to the Sun s mass.  However, there are many other types of gravitational lenses including those formed by galaxy clusters, individual galaxies, and even dark matter halos around single galaxies.  Gravitational lenses also provide information about the masses of both foreground and background objects.  For instance, if one knows the distance to an object producing a gravitational lens effect then its mass can be determined directly from the observed angular diameter of the lensed image.  This technique has been used successfully to measure the masses of supermassive black holes at the centers of nearby galaxies.  Gravitational lensing occurs whenever light travels through regions where gravity varies spatially.  Thus it should occur wherever gravity deviates significantly from Newtonian predictions.  Black holes violate general relativity near their event horizons so they produce strong deviations from Newtonian gravity.  On the other hand, gravastars do not violate general relativity outside their surfaces...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : How to guess a gravastar from a black hole . Abstract : Gravastars are hypothetical bodies that might be the last state of stars evolution , and which have been proposed as solutions to dark holes .Gravastars can be distinguished from black holes by their gravitational lensing effects on background stars . In this research we show how these changes in lensing behavior could be used to distinguish between gravastars and dark holes using observations made with current or future space - based telescopes such as LSST ( Large Synoptic Survey Telescope ) and WFIRST - AFTA ( Wide Field Infrared Space Telescope - Astro - Physics ) .Gravitational lenses are powerful tools for studying nearby galaxies because they magnify pictures of background sources . The most famous example is the Einstein ring produced when light passes close to the Sun s mass .However , there are many other types of gravitational lenses including those formed by galaxy rings , individual stars , and also black material halos around single galaxies . Gravitational lenses also supply data about the masses of both foreground and background objects .For instance , if one knows the distance to an object producing a gravitational lens phenomenon then its mass can be determined directly from the observed angular distance of the lensed image . This method has been used successfully to measure the masses of supermassive black holes at the centers of nearby galaxies .Gravitational lensing occurs whenever light passes through areas where gravity varies spatially . Thus it should occur wherever gravity deviates substantially from Newtonian predictions .Black holes violate special relativity near their event horizons so they produce strong deviations from Newtonian gravity . On the other hand , gravastars do not violate special relativity outside their surfaces . . .",
        "rewrite_text": "**Title: Distinguishing Gravastars from Black Holes through Gravitational Lensing**\n\n**Abstract:** Gravastars, or gravitational vacuum stars, are theoretical entities that may represent the final evolutionary stage of certain stars, offering potential explanations for phenomena associated with black holes. This study explores the distinctive gravitational lensing effects produced by gravastars compared to those generated by black holes, providing a framework for their differentiation through observational data. We propose that current and future space-based telescopes, such as the Large Synoptic Survey Telescope (LSST) and the Wide Field Infrared Space Telescope - Astro-Physics (WFIRST-AFTA), can be utilized to capture these lensing variations. Gravitational lensing serves as a powerful observational tool, enhancing our understanding of nearby galaxies by magnifying the images of distant background sources. A well-known instance of this phenomenon is the Einstein ring, which occurs when light curves around a massive object, such as the Sun. However, gravitational lenses can also arise from various configurations, including galaxy clusters, individual stars, and dark matter halos surrounding galaxies. \n\nThese lenses provide critical insights into the masses of both the foreground and background objects involved. By accurately determining the distance to a gravitational lensing source, researchers can calculate its mass based on the observed angular separation of the lensed images. This technique has been effectively employed to ascertain the masses of supermassive black holes located at the centers of nearby galaxies. Gravitational lensing is a consequence of light traversing regions where gravitational forces exhibit significant spatial variation, leading to deviations from Newtonian predictions. Notably, black holes create pronounced deviations from these predictions near their event horizons, thereby violating principles of special relativity. In contrast, gravastars maintain compliance with special relativity beyond their surfaces. This research aims to elucidate the implications of these differences in lensing behavior, ultimately contributing to our understanding of the nature of gravastars and their potential role in the cosmos.",
        "ori-fast-z-score": 2.1842601416525946,
        "water-fast-z-score": 6.963106238227914,
        "rewrite-fast-z-score": 1.0579249964025073
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Search for the radiative leptonic decay B+ --> gamma l+ nu .\nAbstract:\nThe search is performed using data collected by the BABAR experiment at SLAC in 1999-2000, corresponding to an integrated luminosity of about 40 fb-1 . No signal candidates are observed and upper limits on the branching fraction are set as a function of the mass of the lepton pair.  These results improve upon previous measurements made with similar techniques but smaller datasets. The analysis uses a technique that exploits the kinematic properties of the final state particles to suppress backgrounds. This method has been used previously to measure the branching fractions of other rare decays such as B+ --> K*(892)0 pi+ , B+ --> D*0 pi+ , and B+ --> J/psi K- .\nPACS numbers: 11.30.Er, 12.15.Hh, 13.20.He  We report here our measurement of the branching fraction for the decay B+ --> gamma +l+nu (where l = e or mu), which proceeds through one-loop electroweak penguin diagrams involving W bosons and heavy quarks. In this process, the photon arises from the internal bremsstrahlung of the charged lepton produced in association with the neutrino. The Standard Model predicts a branching fraction of 1.1 x 10-6  1  . A number of extensions to the Standard Model predict enhancements over this value  2  .  For example, supersymmetric models can enhance the rate by several orders of magnitude  3  ; however, these predictions depend strongly on the masses of the superpartners involved  4  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Search for the radiative leptonic emission B + - - > gamma l + nu . Abstract : The search is conducted using data taken by the BABAR study at SLAC in 1999 - 2000 , corresponding to an integrated luminosity of about 40 fb - 1 .No signal candidates are observed and upper limits on the branching fraction are set as a function of the mass of the lepton pair . These data improve upon recent observations made with similar method but smaller datasets .The calculation utilizes a technique that exploits the kinematic effects of the finished state particles to suppress backgrounds . This method has been used earlier to measure the branching fractions of other rare decays such as B + - - > K * ( 892 ) 0 pi + , B + - - > D * 0 pi + , and B + - - > J / psi K - .PACS codes : 11 . 30 . Er , 12 . 15 . Hh , 13 . 20 . He We report here our measurement of the branching percentage for the decay B + - - > gamma + r + nu ( where l = e or mu ) , which goes through one - ring electroweak penguin diagrams using W bosons and heavy quarks . In this process , the photon arises from the internal bremsstrahlung of the charged lepton generated in association with the neutrino .The Standard Model predicts a branching fraction of 1 . 1 x 10 - 6 1 . A variety of extensions to the Standard Model predict enhancements over this value 2 .For instance , supersymmetric theories can increase the rate by many orders of magnitude 3 ; however , these predictions rely highly on the masses of the superpartners participating 4 .",
        "rewrite_text": "**Title:** Search for the Radiative Leptonic Emission B ± → γ l + ν\n\n**Abstract:** This study presents a comprehensive search for the radiative leptonic decay B ± → γ l + ν, utilizing data collected by the BABAR experiment at SLAC during the years 1999 to 2000, which corresponds to an integrated luminosity of approximately 40 fb⁻¹. Despite thorough investigations, no signal candidates were identified, leading to the establishment of upper limits on the branching fraction as a function of the lepton pair mass. These findings enhance previous results obtained through similar methodologies, albeit with smaller datasets. The analysis employs a sophisticated technique that leverages the kinematic properties of the final state particles to effectively suppress background noise, a method previously applied in measuring the branching fractions of other rare decays, including B ± → K* (892)⁰ π⁺, B ± → D*⁰ π⁺, and B ± → J/ψ K⁻.\n\nIn this work, we focus on the branching fraction for the decay B ± → γ + l + ν, where l represents either an electron or a muon. This decay process is mediated by one-loop electroweak penguin diagrams involving W bosons and heavy quarks. The emitted photon originates from the internal bremsstrahlung associated with the charged lepton produced alongside the neutrino. According to the Standard Model, the predicted branching fraction for this decay is approximately 1.1 x 10⁻⁶. However, various extensions to the Standard Model suggest potential enhancements to this prediction. For example, supersymmetric models could significantly increase the decay rate by several orders of magnitude, although such predictions are contingent upon the masses of the involved superpartners. This research contributes to the ongoing exploration of rare decays and the potential implications for new physics beyond the Standard Model. \n\n**PACS codes:** 11.30.Er, 12.15.Hh, 13.20.He",
        "ori-fast-z-score": -2.2691267417693455,
        "water-fast-z-score": 4.206511243549132,
        "rewrite-fast-z-score": -1.5992254762521154
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Electronic Properties of Carbon Nanotubes Calculated from Density Functional Theory and the Empirical pi-Bond Model .\nAbstract:\nThe electronic properties of carbon nanotubes are calculated using density functional theory (DFT) with the local-density approximation (LDA). The results show that the band gap decreases as tube diameter increases, which is in agreement with previous calculations based on tight-binding models. In addition to this effect, we find that the energy difference between the highest occupied molecular orbital (HOMO) and lowest unoccupied molecular orbital (LUMO), i.e., the HOMO-LUMO gap also depends strongly on tube diameter. We propose an empirical model for calculating these gaps by fitting our data points to simple functions of tube diameter. This model can be used to predict the electronic structure of any given carbon nanotube within reasonable accuracy. Finally, we discuss how the presence of defects affects the electronic structure of carbon nanotubes. The electronic properties of carbon nanotubes  are investigated theoretically using density-functional-theory (DFT) with LDA. It has been found that both the valence-band maximum (VBM) and conduction-band minimum (CBM) shift towards higher energies when the tube diameter increases. Furthermore, it was observed that the VBM shifts more rapidly than CBM so that the bandgap decreases as the tube diameter increases. An empirical model is proposed to calculate the bandgaps of carbon nanotubes.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Electronic Properties of Carbon Nanotubes Calculated from Density Functional Theory and the Empirical pi - Bond Model . Abstract : The electronic properties of carbon nanotubes are measured using density functional theory ( DFT ) with the local - density approximation ( LDA ) .The results show that the band gap decreases as tube diameter increases , which is in agreement with previous calculations based on tight - binding methods . In addition to this effect , we find that the electricity difference between the highest occupied molecular orbital ( HOMO ) and lowest unoccupied molecular orbital ( LUMO ) , i . e . , the HOMO - LUMO gap still depends strongly on pipe width .We suggest an empirical description for determining these gaps by fitting our information points to small functions of tube diameter . This theory can be used to predict the electronic configuration of any given carbon nanotube within reasonable accuracy .Finally , we talk how the presence of flaws impacts the electronic properties of carbon nanotubes . The electronic properties of carbon nanotubes are examined theoretically utilizing density - functional - theory ( DFT ) with LDA .It has been seen that both the valence - band highest ( VBM ) and conduction - band minimum ( CBM ) change towards higher energies when the tube diameter increases . Furthermore , it was seen that the VBM moves more swiftly than CBM so that the bandgap decreases as the pipe width rises .An empirical theory is proposed to estimate the bandgaps of carbon nanotubes .",
        "rewrite_text": "**Title:** Electronic Properties of Carbon Nanotubes Calculated from Density Functional Theory and the Empirical π-Bond Model\n\n**Abstract:** This study investigates the electronic properties of carbon nanotubes through the application of density functional theory (DFT) utilizing the local-density approximation (LDA). Our findings reveal a significant trend: as the diameter of the nanotubes increases, the band gap diminishes, corroborating earlier results derived from tight-binding models. Notably, we observe that the energy difference between the highest occupied molecular orbital (HOMO) and the lowest unoccupied molecular orbital (LUMO)—referred to as the HOMO-LUMO gap—exhibits a strong dependence on the tube's width. To facilitate the prediction of these electronic gaps, we propose an empirical model that fits our data points to simple functions of the tube diameter, allowing for accurate estimations of the electronic configurations of various carbon nanotubes.\n\nAdditionally, we explore the influence of structural imperfections on the electronic characteristics of carbon nanotubes. Our theoretical analysis indicates that both the valence band maximum (VBM) and the conduction band minimum (CBM) shift to higher energy levels with increasing tube diameter. Importantly, the VBM experiences a more rapid increase compared to the CBM, resulting in a reduction of the band gap as the tube width expands. This empirical framework not only enhances our understanding of the electronic behavior of carbon nanotubes but also provides a valuable tool for predicting their electronic properties in practical applications. Overall, our research contributes to the fundamental knowledge of carbon nanotube electronics and offers insights into the effects of geometric variations and defects on their electronic performance.",
        "ori-fast-z-score": -0.6527533657682196,
        "water-fast-z-score": 5.269651864139676,
        "rewrite-fast-z-score": -0.2683281572999747
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The CoRoT primary target HD 52265: models and seismic tests .\nAbstract:\nWe present new theoretical evolutionary tracks for the mass range 1.8 to 2.5 M⊙, which are based on an improved treatment of convection in stellar interiors. We use these tracks as input into our seismic modelling code CESAM2k to compute synthetic seismograms for two different sets of observed frequencies (from COROT) corresponding to two possible values of the inclination angle i = 90° or 60° . The comparison between observations and theory shows that we can exclude one set of frequencies at high confidence level but not the other. This is due to the fact that the frequency differences between ℓ=0 and ℓ=2 modes depend strongly on the inclination angle. In addition, we find that the best fit model has a radius R=1.95R⊙ , which agrees well with the value inferred by asteroseismology using only ℓ=0 modes. Finally, we show how this result could be used to determine the age of the star.  Keywords: Seismic modelling",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The CoRoT primary target HD 52265 : models and seismic studies . Abstract : We present new theoretical evolutionary tracks for the mass range 1 . 8 to 2 . 5 [UNK] , which are based on an improved treatment of convection in stellar interiors .We use these tracks as input into our seismic modelling code CESAM2k to compute natural seismograms for two different sets of measured signals ( from COROT ) corresponding to two possible values of the inclination angle i = 90° or 60° . The comparison between observations and theory demonstrates that we can eliminate one group of frequencies at high confidence rate but not the other .This is due to the fact that the frequency differences between ℓ = 0 and ℓ = 2 modes differ highly on the inclination angle . In addition , we find that the best fit description has a diameter R = 1 . [UNK] , which agrees well with the value inferred by asteroseismology using only ℓ = 0 modes .Finally , we show how this effect could be used to predict the age of the star . Keywords : Seismic modelling",
        "rewrite_text": "In this study, we present novel theoretical evolutionary tracks for stars within the mass range of 1.8 to 2.5 solar masses, developed through an enhanced approach to convection in stellar interiors. These evolutionary tracks serve as foundational input for our seismic modeling code, CESAM2k, which we utilize to generate natural seismograms based on two distinct sets of observed signals from the CoRoT satellite. These signals correspond to two potential inclination angles, specifically i = 90° and i = 60°. Our analysis involves a rigorous comparison between the theoretical predictions and the observational data, which allows us to confidently eliminate one set of frequency groups while leaving another unresolved. This discrepancy arises from the significant variation in frequency differences between the ℓ = 0 and ℓ = 2 modes, which is heavily influenced by the inclination angle. Furthermore, our findings indicate that the optimal fit for the star's parameters yields a radius of R = 1. [UNK], aligning closely with the radius determined through asteroseismic techniques that focus solely on ℓ = 0 modes. Ultimately, we discuss the implications of these results, particularly how the observed effects can be leveraged to estimate the age of the star. This research contributes to the broader understanding of stellar evolution and seismic modeling, offering insights that may enhance the accuracy of age predictions for similar celestial bodies. \n\nKeywords: Seismic modeling, stellar evolution, asteroseismology, CoRoT, inclination angle.",
        "ori-fast-z-score": 0.9058216273156765,
        "water-fast-z-score": 4.387842813611494,
        "rewrite-fast-z-score": -1.0536089137432665
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Early Spectrophotometric Evolution of V1186 Scorpii (Nova Scorpii 2004 #1) .\nAbstract:\nWe present the first detailed analysis of the early photospheric evolution of Nova Scorpii 2004 # 1, based on high-resolution optical spectroscopy obtained with the Ultraviolet and Visual Echelle Spectrograph mounted at the Very Large Telescope in Chile.  The nova was discovered by amateur astronomers on March 31st, 2004, when it reached an apparent magnitude of 8.7. We find that the ejecta are expanding at velocities between 1000 km/s to 3000 km/s. From our observations we derive a distance estimate for this object of about 3 kpc. This is consistent with previous estimates derived using other methods. Using these results as input parameters into theoretical models, we determine the chemical composition of the ejecta. Our best fit model suggests that the ejecta consist mainly of O-rich material mixed with some CNO-processed material. In addition, we detect strong emission lines originating from highly ionized species such as FeXXV/FeXXVI or NeIX/NX. These lines indicate that the ejecta were heated up to temperatures above 10 million K during their expansion.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Early Spectrophotometric Evolution of V1186 Scorpii ( Nova Scorpii 2004 # 1 ) . Abstract : We present the first detailed analysis of the early photospheric evolution of Nova Scorpii 2004 # 1 , using on wide - resolution optical spectroscopy acquired with the Ultraviolet and Visual Echelle Spectrograph mounted at the Very Large Telescope in Chile .The nova was discovered by amateur astronomers on March 31st , 2004 , when it achieved an apparent magnitude of 8 . 7 . We see that the ejecta are growing at velocities between 1000 kilometers / s to 3000 km / s .From our observations we derive a distance estimate for this object of about 3 kpc . This is compatible with previous accounts derived using other methods .Using these results as input parameters into theoretical methods , we determine the chemical composition of the ejecta . Our best fit description implies that the ejecta consist mostly of O - rich substance mixed with some CNO - processed material .In addition , we find strong radiation lines coming from highly ionized species such as FeXXV / FeXXVI or NeIX / NX . These lines indicate that the ejecta were heated up to altitudes above 10 million K during their expansion .",
        "rewrite_text": "We provide a comprehensive analysis of the early photospheric evolution of Nova Scorpii 2004 # 1, based on high-resolution optical spectroscopy obtained with the Ultraviolet and Visual Echelle Spectrograph at the Very Large Telescope in Chile. This nova was first identified by amateur astronomers on March 31, 2004, when it reached an apparent magnitude of 8.7. Our observations reveal that the ejecta are expanding at velocities ranging from 1000 km/s to 3000 km/s. From these measurements, we estimate the distance to the nova to be approximately 3 kpc, a value that aligns with previous estimates derived through alternative methods. \n\nUtilizing these findings as input parameters for theoretical models, we analyze the chemical composition of the ejecta. Our best-fitting model suggests that the ejecta are predominantly composed of oxygen-rich material, with a notable presence of CNO-processed elements. Furthermore, we observe prominent emission lines from highly ionized species, including FeXXV, FeXXVI, NeIX, and NX. These spectral lines indicate that the ejecta experienced temperatures exceeding 10 million K during their expansion phase. \n\nThis study not only enhances our understanding of the early evolution of Nova Scorpii 2004 # 1 but also contributes valuable insights into the physical processes governing nova eruptions. The findings underscore the significance of high-resolution spectroscopy in unraveling the complex dynamics and chemical makeup of nova ejecta, paving the way for future research in stellar explosions and their aftermath.",
        "ori-fast-z-score": 0.47140452079103173,
        "water-fast-z-score": 4.88240082724041,
        "rewrite-fast-z-score": 1.762817881041723
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Pair Production of Doubly-Charged Scalars: Neutrino Mass Constraints and Signals at the LHC .\nAbstract:\nWe study pair production of doubly-charged scalars in association with two jets, which can be induced by both weak gauge bosons (W or Z) and photons via loops involving heavy fermions such as top quarks. We derive constraints on the masses of these particles using current experimental data for W+jets and Z+jets processes collected by ATLAS and CMS experiments at the Large Hadron Collider (LHC). In addition to the standard model backgrounds, we also consider contributions from other new physics models that may have similar signatures. The results are presented in terms of exclusion limits on the mass parameters of various new physics scenarios. Finally, we discuss possible signals of this process at future runs of the LHC. PACS numbers: 12.60.Jv, 13 .85.Rm, 14.80.Ly \nI. INTRODUCTIO N\nThe discovery of neutrinos has opened up an exciting possibility of probing beyond Standard Model (SM), especially its Majorana nature  1  , through their lepton number violating interactions  2  . One interesting scenario is the seesaw mechanism  3  where SM singlet right-handed neutrinos acquire large Majorana masses after electroweak symmetry breaking  4  .\nIn order to test whether the observed light neutrinos are indeed Majorana particles, one needs to look for lepton-number-violating processes mediated by virtual heavy neutrinos  5  . These include neutrinoless double beta decay  6  , tritium beta decay  7  , and charged-current quasielastic scattering  8  . However, it turns out that all these processes suffer from severe astrophysical and/or nuclear matrix element uncertainties  9  . On the other hand, colliders provide clean environments to probe lepton number violation directly  10  . For example, searches for same-sign dileptons  11  and trileptons  12  at hadronic colliders could lead to important information about Majorana neutrinos  13  . Another promising channel is the production of doubly-charge scalar particles  14  , which can occur either through s-channel exchange of neutral gauge bosons  15  or t-channel exchange of heavy ferm",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Pair Production of Doubly - Charged Scalars : Neutrino Mass Constraints and Signals at the LHC . Abstract : We research pair production of doubly - charged scalars in association with two jets , which can be induced by both weak gauge bosons ( W or Z ) and photons via loops involving heavy fermions such as top quarks .We derive restrictions on the masses of these objects utilizing current experimental evidence for W + jets and Z + jets effects obtained by ATLAS and CMS tests at the Large Hadron Collider ( LHC ) . In addition to the standard theory backgrounds , we also consider contributions from other recent physics models that might have related signatures .The results are presented in terms of exclusion limits on the mass parameters of several novel physics scenarios . Finally , we explain possible signals of this process at next ran of the LHC .PACS numbers : 12 . 60 . Jv , 13 . 85 . Rm , 14 . 80 . Ly I . INTRODUCTIO N The observation of neutrinos has opened up an exciting possibility of probing beyond Standard Model ( SM ) , particularly its Majorana nature 1 , through their lepton size violating interactions 2 . One interesting scenario is the seesaw mechanism 3 where SM singlet right - handed neutrinos gain big Majorana masses after electroweak symmetry breaking 4 .In order to test whether the seen light neutrinos are indeed Majorana ions , one needs to search for lepton - number - violating reactions mediated by virtual heavy neutrinos 5 . These include neutrinoless double alpha emission 6 , tritium alpha emission 7 , and charged - current quasielastic emission 8 .However , it turns out that all these mechanisms suffer from severe astrophysical and / or atomic matrix element uncertainties 9 . On the other hand , colliders provide clean environments to probe lepton number violation directly 10 .For instance , searches for same - sign dileptons 11 and trileptons 12 at hadronic colliders may bring to key information about Majorana neutrinos 13 . Another promising channel is the production of doubly - charge scalar particles 14 , which can occur either through s - channel exchange of neutral gauge bosons 15 or t - channel exchange of light ferm",
        "rewrite_text": "**Title:** Pair Production of Doubly-Charged Scalars: Neutrino Mass Constraints and Signals at the LHC\n\n**Abstract:** This study investigates the pair production of doubly-charged scalar particles in conjunction with two jets, a process that can be triggered by weak gauge bosons (W or Z) and photons through loop interactions involving heavy fermions, such as top quarks. We establish constraints on the masses of these doubly-charged scalars by analyzing current experimental data related to W + jets and Z + jets, as reported by the ATLAS and CMS collaborations at the Large Hadron Collider (LHC). In addition to the standard model backgrounds, we also examine contributions from various recent physics models that may exhibit similar signatures. Our findings are articulated in terms of exclusion limits on the mass parameters across several novel physics scenarios. Furthermore, we discuss potential signals associated with this production process in the upcoming runs of the LHC.\n\nThe discovery of neutrinos has opened new avenues for exploring physics beyond the Standard Model (SM), particularly concerning their Majorana nature through lepton number-violating interactions. A compelling framework for this investigation is the seesaw mechanism, wherein SM singlet right-handed neutrinos acquire significant Majorana masses following electroweak symmetry breaking. To ascertain whether the observed light neutrinos are indeed Majorana particles, it is essential to search for lepton number-violating processes mediated by virtual heavy neutrinos. Such processes include neutrinoless double beta decay, tritium beta decay, and charged-current quasielastic scattering. However, these mechanisms are often hindered by substantial astrophysical and atomic matrix element uncertainties. In contrast, collider experiments offer a more controlled environment for directly probing lepton number violation. For example, searches for same-sign dileptons and trileptons at hadronic colliders can yield critical insights into the nature of Majorana neutrinos. Additionally, the production of doubly-charged scalar particles presents another promising avenue for exploration, occurring through s-channel exchanges of neutral gauge bosons or t-channel exchanges involving light fermions.",
        "ori-fast-z-score": 0.8391813582966892,
        "water-fast-z-score": 6.333333333333333,
        "rewrite-fast-z-score": -0.4123930494211613
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Decoherence of a driven multilevel quantum system interacting with a multi-bath reservoir .\nAbstract:\nWe study the decoherence dynamics of an open quantum system consisting of two-level atoms coupled to a single-mode cavity field in presence of dissipation and driving fields. The atom-cavity coupling is treated within the framework of Jaynes-Cummings model, while the dissipative effects are described by using the master equation for the reduced density matrix of the system. We show that the steady state solution of this problem can be obtained analytically when the decay rates of all atomic levels are equal. In particular we find that the steady-state entanglement between the atom-field subsystem depends on both the strength of the external driving field as well as the number of excited states involved in the process. Finally, we discuss how our results could be used to explain recent experimental observations concerning the generation of nonclassical light via spontaneous emission processes. PACS numbers: 03.67.Mn, 42.50.Vk",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Decoherence of a driven multilevel quantum state interacting with a multi - bath pool . Abstract : We explore the decoherence dynamics of an open quantum system consisting of two - level atoms connected to a single - mode cavity field in presence of dissipation and driving fields .The atom - cavity coupling is treated within the framework of Jaynes - Cummings model , while the dissipative effects are explained by using the master equation for the reduced density matrix of the system . We see that the stable state solution of this question can be obtained analytically when the decay rates of all atomic levels are equal .In particular we find that the stable - state entanglement between the atom - field subsystem depends on both the strength of the external driving field as also as the quantity of excited states concerned in the process . Finally , we explain how our findings may be used to explain latest experimental discoveries involving the generation of nonclassical light via spontaneous emission mechanisms .PACS codes : 03 . 67 . Mn , 42 . 50 . Vk",
        "rewrite_text": "**Title:** Decoherence of a Driven Multilevel Quantum State Interacting with a Multi-Bath Pool\n\n**Abstract:** In this study, we investigate the decoherence dynamics of an open quantum system composed of two-level atoms coupled to a single-mode cavity field, while accounting for the effects of dissipation and external driving fields. The interaction between the atoms and the cavity is analyzed using the Jaynes-Cummings model, which provides a robust framework for understanding the atom-cavity coupling. To describe the dissipative processes affecting the system, we employ the master equation governing the reduced density matrix. Our findings reveal that an analytical solution for the stable state of the system can be derived when the decay rates of all atomic levels are uniform. Notably, we demonstrate that the entanglement in the stable state between the atom-field subsystem is influenced by both the intensity of the external driving field and the number of excited states involved in the interaction. This relationship highlights the intricate balance between driving strength and the quantum state dynamics. Furthermore, we discuss the implications of our results in the context of recent experimental advancements, particularly those related to the generation of nonclassical light through spontaneous emission processes. Our work contributes to a deeper understanding of quantum decoherence and its role in the manipulation of quantum states, paving the way for potential applications in quantum information science and technology. The findings are relevant to ongoing research in the field, as indicated by the associated PACS codes: 03.67.Mn, 42.50.Vk.",
        "ori-fast-z-score": 0.4216370213557839,
        "water-fast-z-score": 4.297967830559865,
        "rewrite-fast-z-score": 1.9250668437592438
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Norma Cluster (ACO 3627): I. A Dynamical Analysis of the Most Massive Cluster in the Great Attractor .\nAbstract:\nWe present an analysis of the most massive cluster known to be part of the Great Attractor, based on new spectroscopic data obtained with VLT/VIMOS and archival Chandra X-ray Observatory observations. The main results are as follows:  We find that the galaxy distribution is well described by a single NFW profile out to at least 1 Mpc/h.  Using this mass model we derive a virial mass for the cluster of Mvir = 2.1 x 1015M⊙/h70 within R200 = 0.9 h-1 70 Mpc/h. This value agrees very well with previous estimates derived using different techniques. We also show that the velocity dispersion profile can be fitted with a simple β-model up to r ~ 0.7R200. Beyond this radius there seems to be evidence for a steepening of the profile which could indicate the presence of substructures or infalling groups along the line-of-sight.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  The Norma Cluster (ACO 3627): I.A Dynamical Analysis of the Most Massive Cluster in the Great Attractor .Abstract : We present an assessment of the most large cluster known to be part of the Great Attractor , using on new spectroscopic data acquired with VLT / VIMOS and archival Chandra X - ray Observatory surveys . The main results are as follows : We see that the galaxy distribution is well described by a single NFW profile out to at least 1 Mpc / h .Using this mass model we derive a virial mass for the cluster of Mvir = 2 . 1 x [UNK] / h70 within R200 = 0 . 9 h - 1 70 Mpc / h . This value agrees very well with previous calculated derived using separate techniques .We also find that the velocity dispersion model can be fit with a simple β - model up to r ~ 0 . 7R200 . Beyond this radius there seems to be data for a steepening of the profile which could indicate the formation of substructures or infalling bands along the line - of - view .",
        "rewrite_text": "Title: The Norma Cluster (ACO 3627): I. A Dynamical Analysis of the Most Massive Cluster in the Great Attractor\n\nAbstract: In this study, we conduct a comprehensive analysis of the Norma Cluster (ACO 3627), recognized as the most massive cluster within the Great Attractor region. Our investigation utilizes newly obtained spectroscopic data from the Very Large Telescope (VLT) equipped with VIMOS, alongside archival data from the Chandra X-ray Observatory. Our findings reveal that the spatial distribution of galaxies in the cluster can be accurately modeled using a single Navarro-Frenk-White (NFW) profile, extending to at least 1 Mpc/h. Employing this mass distribution model, we calculate a virial mass for the cluster, yielding Mvir = 2.1 x 10^15 M☉/h70 within a radius of R200 = 0.9 h^-1 70 Mpc/h. This mass estimation is consistent with previous calculations derived from various methodologies, reinforcing the reliability of our approach. Furthermore, we analyze the velocity dispersion within the cluster, which can be effectively described by a simple β-model up to approximately 0.7 R200. However, beyond this radius, our data suggest a steepening of the velocity dispersion profile, potentially indicative of the presence of substructures or infalling groups along the line of sight. This observation may provide insights into the dynamical processes at play within the cluster and the ongoing evolution of its structure. Overall, our research contributes to a deeper understanding of the dynamics and mass distribution of the Norma Cluster, highlighting its significance within the broader context of cosmic structures in the Great Attractor.",
        "ori-fast-z-score": 0.36650833306891567,
        "water-fast-z-score": 5.093248125762992,
        "rewrite-fast-z-score": 2.799769575772148
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Daemons and DAMA: Their Celestial-Mechanics Interrelations .\nAbstract:\nThe article is devoted to the problem of possible existence of dark matter particles in our Galaxy, which are not detected by other methods than their gravitational effects on visible objects (stars). The author considers the possibility that these hypothetical particles can be described as celestial mechanics daemons with certain properties. In particular, it is shown how such daemons could explain some features observed recently for the DAMA experiment at Gran Sasso National Laboratory. It should be noted that this explanation does not contradict any known experimental data. However, there are also serious difficulties associated with the proposed model. These problems will require further study. This work was supported by Russian Science Foundation grant No 14-50-00040. URL: http://arxiv.org/abs/1409.5189 . \nI. INTRODUCTORY REMARK .\nDark Matter (DM) is one of the most important mysteries of modern physics  1  -  4  . Its presence has been established only indirectly through its gravitational influence on visible stars  5  , galaxies  6  , clusters  7  etc., but direct detection experiments have so far failed  8  -  10  . There exist many theoretical models describing DM  11  -  13  ; however, none of them has yet been confirmed experimentally  14  . One of the possibilities is that DM consists of new elementary particles  15  -  17  . If they interact weakly or electromagnetically with ordinary matter then they would escape detection even if they were produced in large quantities  18  . On the other hand, if they interact strongly enough with normal matter, then they may be detectable directly  19  -  21  . A number of experiments searching for DM particles have been carried out  22  -  26  . Recently, the results obtained by the DAMA collaboration  27  attracted considerable attention  28  -  30  . According to these results, the annual modulation effect  31  -  33  caused by the motion of Earth around Sun  34  -  36  leads to an increase in the rate of nuclear recoils registered by detectors during June-October period  37  compared to December-February period. Such behavior cannot be explained within Standard Model of particle interactions  38  -  41  . Several authors suggested different explanations based on",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Daemons and DAMA : Their Celestial - Mechanics Interrelations . Abstract : The essay is devoted to the question of possible existence of dark matter ions in our Galaxy , which are not observed by other methods than their gravitational impacts on visible objects ( stars ) .The author considers the prospect that these hypothetical particles can be described as celestial mechanics daemons with certain characteristics . In particular , it is demonstrated how such daemons might explain some features detected lately for the DAMA experiment at Gran Sasso National Laboratory .It should be mentioned that this explanation does not contradict any established experimental evidence . However , there are also serious difficulties linked with the suggested model .These difficulties will demand further study . This project was supported by Russian Science Foundation award No 14 - 50 - 00040 .URL : www : / / arxiv . org / abs / 1409 . 5189 . I .INTRODUCTORY REMARK . Dark Matter ( DM ) is one of the most important puzzles of modern physics 1 - 4 .Its presence has been known only indirectly through its gravitational impact on visible stars 5 , galaxies 6 , galaxies 7 etc . , but direct detection experiments have so far unsuccessful 8 - 10 . There operate several experimental scenarios describing DM 11 - 13 ; however , none of them has already been confirmed experimentally 14 .One of the possibilities is that DM consists of new primary objects 15 - 17 . If they interact weakly or electromagnetically with normal matter then they may survive discovery even if they were produced in large quantities 18 .On the other hand , if they interact heavily enough with normal matter , then they may be detectable directly 19 - 21 . A couple of studies looking for DM particles have been carried out 22 - 26 .Recently , the results derived by the DAMA collaboration 27 drew substantial scrutiny 28 - 30 . According to these results , the annual modulation effect 31 - 33 generated by the movement of Earth around Sun 34 - 36 results to an increase in the frequency of nuclear recoils registered by detectors during June - October year 37 contrast to December - February time .Such interactions cannot be described within Standard Model of particle behavior 38 - 41 . Several scientists suggested different explanations based on",
        "rewrite_text": "**Title:** Daemons and DAMA: Their Celestial Mechanics Interrelations\n\n**Abstract:** This article explores the intriguing possibility of dark matter ions existing within our Galaxy, which remain undetectable through conventional methods except for their gravitational influence on visible celestial bodies, such as stars. The author proposes a novel framework in which these hypothetical dark matter particles are conceptualized as celestial mechanics daemons, possessing distinct characteristics that could elucidate recent observations from the DAMA experiment conducted at the Gran Sasso National Laboratory. Notably, the proposed daemon model offers a potential explanation for the annual modulation effect reported by the DAMA collaboration, which indicates a variation in the frequency of nuclear recoils detected during different seasons. This phenomenon, attributed to the Earth's orbital motion around the Sun, presents a challenge to the Standard Model of particle physics, as it cannot be adequately explained by existing theoretical frameworks. The author emphasizes that the daemon hypothesis does not conflict with established experimental findings, yet acknowledges the significant challenges associated with this model, which warrant further investigation. The research is supported by the Russian Science Foundation under award No. 14-50-00040. This study contributes to the ongoing discourse surrounding dark matter, a fundamental enigma in contemporary physics, and seeks to advance our understanding of its potential constituents and interactions. The findings underscore the necessity for continued exploration of alternative explanations for dark matter phenomena, particularly in light of the unresolved questions surrounding direct detection efforts and the implications of the DAMA results. For more details, please refer to the full article available at www.arxiv.org/abs/1409.5189.",
        "ori-fast-z-score": -0.08192319205190406,
        "water-fast-z-score": 8.16496580927726,
        "rewrite-fast-z-score": 0.8049844718999243
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Shrinking binary and planetary orbits by Kozai cycles with tidal friction .\nAbstract:\nWe study the orbital evolution of planets in binaries under the effect of gravitational perturbations due to third bodies, which can lead to large eccentricities and inclinations for both components of the system. We show that this mechanism is able to explain some observed properties of extrasolar systems such as HD 169830 or Kepler-16. In particular we find that it may be responsible for the formation of hot Jupiters through planet-planet scattering processes. The main results are summarized below: \n1) We present an analytical model describing the long-term evolution of the semi-major axes (a), eccentricities (e), and mutual inclination angles (i) of two orbiting objects under the combined effects of general relativity, tides, and secular interactions between all three bodies. \n2) Using our model, we perform numerical integrations of several representative cases showing how the presence of additional perturbative forces can significantly modify the orbital parameters of the innermost body over time-scales ranging from millions up to billions of years. \n\n3) We apply our model to the case of the exoplanetary system around HD 169830 composed of four giant planets on highly inclined orbits. Our calculations suggest that the current architecture of this system could have been produced by successive scatterings among its planets triggered by strong gravitational encounters with other massive bodies located at distances larger than 100 AU. \n \n 4) Finally, we explore the possibility that the recently discovered transiting super-Earths in the Kepler-16 system might also have formed via similar mechanisms.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Shrinking binary and planetary orbits by Kozai cycles with tidal tension . Abstract : We research the orbital evolution of planets in binaries under the impact of gravitational perturbations due to third body , which can lead to large eccentricities and inclinations for both components of the system .We see that this mechanism is able to explain some observed properties of extrasolar systems such as HD 169830 or Kepler - 16 . In particular we find that it could be responsible for the formation of bright Jupiters through moon - planet scattering mechanisms .The main results are presented below : 1 ) We present an analytical theory explaining the long - term evolve of the semi - major axes ( a ) , eccentricities ( e ) , and mutual inclination angles ( i ) of two orbiting objects under the combined influences of general relativity , tides , and secular interactions between all three bodies . 2 ) Using our model , we perform numerical integrations of several representative cases showing how the presence of added perturbative forces can significantly change the orbital variables of the innermost bodies over time - scales extending from hundreds up to billions of years .3 ) We use our model to the case of the exoplanetary structure around HD 169830 composed of four giant planets on highly inclined planets . Our calculations suggest that the present architecture of this system could have been created by successive scatterings among its stars created by weak gravitational encounters with other large bodies located at distances bigger than 100 AU .4 ) Finally , we investigate the prospect that the recently discovered transiting super - Earths in the Kepler - 16 system might also have formed via related causes .",
        "rewrite_text": "**Title:** Shrinking Binary and Planetary Orbits through Kozai Cycles with Tidal Tension\n\n**Abstract:** This study explores the orbital dynamics of planets within binary systems, focusing on the gravitational perturbations exerted by a third body. These perturbations can induce significant eccentricities and inclinations in the orbits of both binary components. Our findings suggest that this mechanism may account for certain characteristics observed in extrasolar systems, such as HD 169830 and Kepler-16. Notably, we propose that the formation of bright Jupiter-like planets could be facilitated by moon-planet scattering processes. \n\nThe key results of our research are as follows: First, we develop an analytical framework that elucidates the long-term evolution of the semi-major axes (a), eccentricities (e), and mutual inclination angles (i) of two orbiting bodies, taking into account the combined effects of general relativity, tidal forces, and secular interactions among all three bodies involved. Second, we conduct numerical simulations of various representative scenarios to demonstrate how the introduction of additional perturbative forces can lead to substantial alterations in the orbital parameters of the innermost bodies over time scales ranging from hundreds to billions of years. \n\nThird, we apply our model to analyze the exoplanetary system surrounding HD 169830, which consists of four giant planets with highly inclined orbits. Our calculations indicate that the current configuration of this system may have arisen from a series of gravitational interactions with other substantial bodies located at distances greater than 100 AU. Lastly, we examine the potential formation mechanisms of the recently identified transiting super-Earths in the Kepler-16 system, suggesting that they may have originated through similar processes. This research contributes to our understanding of the complex dynamics governing planetary systems in binary environments and highlights the role of Kozai cycles and tidal interactions in shaping their evolution.",
        "ori-fast-z-score": 0.4402254531628119,
        "water-fast-z-score": 6.490229342872016,
        "rewrite-fast-z-score": -0.08362420100070908
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Probing non-standard decoherence effects with solar and KamLAND neutrinos .\nAbstract:\nWe study the possibility that nonstandard interactions (NSI) between neutrinos and matter can be probed by using solar and reactor neutrino data simultaneously, in particular through their combined effect on the survival probability P(νe→νe). We find that NSI parameters are constrained to values below 0.1 for most combinations of standard oscillation parameters allowed at 3σ CL by current global fits. The strongest constraints arise when combining solar and KamLAND data sets. In this case we obtain upper bounds on |εee|, |εµτ | < 0.06 − 0.07 depending on the value of θ13. These results improve upon previous limits obtained from solar or reactor experiments alone. \n \n Introduction \n \n Neutrino oscillations have been observed in many different types of experiments  1  . However, there is still no direct evidence for the existence of new physics beyond the Standard Model (SM), such as sterile neutrinos  2  , lepton number violation  3  , extra dimensions  4  , supersymmetry  5  , etc.. Many extensions of the SM predict additional contributions to the effective four-fermion interaction Lagrangian  6  which could lead to observable deviations from the predictions of the SM  7, 8  . For example, it has recently been shown  9  that some models of quantum gravity  10  may induce an energy dependent refractive index n = 1 + εE/E0 where E0 is a characteristic scale associated with the underlying theory  11  . This would result in a modification of the vacuum mixing angle sin2θ12 = 1−cos2θ12 ≈ 1+ε/2+O(ε3)  12  leading to potentially large effects on the propagation of neutrinos  13  .\n \nIn addition to these theoretical motivations, there exist several experimental indications pointing towards possible new physics beyond the SM  14  : i) Large atmospheric  15  and solar  16  neutrino flux deficits; ii) LSND  17  and MiniBooNE  18  anomalies indicating short-baseline νμ → νe appearance transitions not predicted within three-flavor neutrino oscillations  19  ; iii) Anomalies in the measurement of the muon anomalous magnetic moment",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Probing non - customary decoherence effects with solar and KamLAND neutrinos . Abstract : We research the prospect that nonstandard relationships ( NSI ) between neutrinos and matter can be probed by using solar and reactor neutrino data simultaneously , in particular through their combined influence on the life probability P ( νe→νe ) .We see that NSI variables are constrained to parameters below 0 . 1 for most combinations of standard oscillation parameters allowed at 3σ CL by current global fits . The strongest limits arise when combining solar and KamLAND data sets .In this situation we obtain upper limits on | εee | , | εµτ | < 0 . 06 − 0 . 07 depending on the value of θ13 . These results enhance upon former limits established from solar or reactor tests alone .Introduction Neutrino oscillations have been observed in different different kinds of studies 1 . However , there is nevertheless no formal evidence for the existence of new theories beyond the Standard Model ( SM ) , such as sterile neutrinos 2 , lepton number violation 3 , extra dimensions 4 , supersymmetry 5 , etc . .Many modifications of the SM predict additional contributions to the effective four - fermion interaction Lagrangian 6 which potentially contribute to observable deviations from the estimates of the SM 7 , 8 . For instance , it has recently been shown 9 that some models of quantum gravitational 10 may generate an energy dependent refractive index n = 1 + εE / E0 where E0 is a typical scale identified with the underlying model 11 .This might lead in a modification of the vacuum mixing ratio sin2θ12 = 1−cos2θ12 ≈ 1 + ε / 2 + O ( ε3 ) 12 leading to potentially large effects on the propagation of neutrinos 13 . In addition to these theoretical motivations , there remain many experimental indications pointing towards possible new science beyond the SM 14 : i ) Large atmospheric 15 and solar 16 neutrino flux deficits ; ii ) LSND 17 and MiniBooNE 18 anomalies suggesting short - baseline νμ → νe appearance processes not anticipated within three - flavor neutrino oscillations 19 ; iii ) Anomalies in the measurement of the muon anomalous magnetic moment",
        "rewrite_text": "**Title:** Probing Non-Customary Decoherence Effects with Solar and KamLAND Neutrinos\n\n**Abstract:** This study investigates the potential for detecting nonstandard interactions (NSI) between neutrinos and matter by analyzing data from solar and reactor neutrinos concurrently. Our focus is on how these interactions influence the survival probability \\( P(\\nu_e \\rightarrow \\nu_e) \\). We find that the NSI parameters are constrained to values below 0.1 for most combinations of standard oscillation parameters that are permitted at the 3σ confidence level according to current global analyses. Notably, the most stringent constraints emerge when we integrate data from both solar and KamLAND experiments. In this context, we derive upper limits for the parameters \\( | \\epsilon_{ee} | \\) and \\( | \\epsilon_{\\mu\\tau} | \\), which fall within the range of 0.06 to 0.07, depending on the value of \\( \\theta_{13} \\). These findings represent an improvement over previous limits obtained from solar or reactor neutrino experiments conducted in isolation.\n\nNeutrino oscillations have been documented across various experimental studies; however, there remains a lack of definitive evidence supporting new theoretical frameworks beyond the Standard Model (SM), such as the existence of sterile neutrinos, lepton number violation, extra dimensions, and supersymmetry. Numerous extensions of the SM suggest additional contributions to the effective four-fermion interaction Lagrangian, which could lead to observable deviations from SM predictions. Recent theoretical work indicates that certain quantum gravitational models may introduce an energy-dependent refractive index, potentially altering the vacuum mixing ratio and significantly affecting neutrino propagation. Furthermore, experimental observations continue to hint at phenomena that may indicate new physics beyond the SM, including substantial deficits in atmospheric and solar neutrino fluxes, anomalies in short-baseline neutrino oscillation experiments like LSND and MiniBooNE, and discrepancies in the measurement of the muon anomalous magnetic moment. This research aims to deepen our understanding of these interactions and their implications for neutrino physics.",
        "ori-fast-z-score": -0.6910947404650881,
        "water-fast-z-score": 6.516946235415335,
        "rewrite-fast-z-score": 0.26413527189768715
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Learning more from the Lorentz transformations .\nAbstract:\nThe author presents an alternative approach to teaching special relativity by using the concept of transformation groups and their generators, which are used in physics for describing symmetry properties of physical systems.  The main idea is that students can learn about the foundations of special relativity without having to deal with complicated mathematical concepts such as tensors or spinor fields. This article describes how this method works on examples related to the Lorentz transformations. It also discusses some possible applications of these ideas in other areas of physics. In particular, it shows how one can use them to explain the origin of gauge symmetries in quantum field theory. Special relativity (SR) has been taught at many universities since its discovery in 1905  1  . However, despite numerous attempts  2  , there still exists no generally accepted way of introducing SR into undergraduate courses  3  .\nIn recent years, several authors have proposed new approaches to teaching SR  4  -  8  . These methods usually involve presenting the basic principles of SR through simple experiments performed in different reference frames  9  -  11  . They often require only minimal knowledge of mathematics  12  -  14  . Some of these proposals were inspired by Feynman s lectures  15  . Other authors tried to develop similar techniques based on modern computer technology  16  -  18  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Learning more from the Lorentz transformations . Abstract : The author presents an alternative approach to teaching special relativity by using the idea of transformation groups and their generators , which are used in physics for describing symmetry properties of physical structures .The main idea is that students can know about the foundations of special relativity without having to deal with difficult physical concepts such as tensors or spinor fields . This page describes how this process works on examples related to the Lorentz transformations .It additionally outlines some possible use of these ideas in other areas of science . In particular , it demonstrates how one can apply them to explain the origin of gauge symmetries in quantum field theory .Special relativity ( SR ) has been taught at many universities since its observation in 1905 1 . However , despite several efforts 2 , there still exists no usually agreed way of introducing SR into undergraduate courses 3 .In recent years , various published have proposed different methods to teaching SR 4 - 8 . These methods usually include presenting the fundamental principles of SR through simple experiments conducted in different reference frames 9 - 11 .They often use only limited knowledge of math 12 - 14 . Some of these proposals were inspired by Feynman s lectures 15 .Other papers try to develop similar method using on contemporary computer technology 16 - 18 .",
        "rewrite_text": "The author introduces a novel pedagogical approach to teaching special relativity, focusing on the concept of transformation groups and their generators, which are pivotal in physics for articulating the symmetry characteristics of physical systems. This innovative method allows students to grasp the foundational aspects of special relativity without the need to engage with complex physical concepts such as tensors or spinor fields. The article elaborates on this teaching strategy through various examples related to Lorentz transformations, illustrating how these transformations can be understood in a more accessible manner. Furthermore, the author explores the potential applications of this framework in other scientific domains, particularly in elucidating the origins of gauge symmetries within quantum field theory. \n\nSince the inception of special relativity in 1905, it has been a staple in university curricula; however, a universally accepted method for introducing this subject at the undergraduate level remains elusive. Despite numerous attempts to establish effective teaching strategies, a consensus has yet to be reached. Recent literature has proposed diverse methodologies for teaching special relativity, often emphasizing hands-on experiments conducted from different reference frames, which typically require only a basic understanding of mathematics. Some of these educational approaches draw inspiration from Richard Feynman's lectures, while others leverage modern computer technology to enhance learning experiences. This article contributes to the ongoing discourse by presenting a cohesive framework that simplifies the teaching of special relativity, potentially enriching the educational landscape for future physicists.",
        "ori-fast-z-score": 0.7427813527082074,
        "water-fast-z-score": 5.707562630969605,
        "rewrite-fast-z-score": -0.8049844718999243
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Gaussian-Basis Monte Carlo Method for Numerical Study on Ground States of Itinerant and Strongly Correlated Electron Systems .\nAbstract:\nWe propose the Gaussian-basis Monte Carlo method to study ground states of itinerant electron systems with strong correlations, which is based on the variational principle in terms of the trial wave function expressed by the product of Gutzwiller-Jastrow factors. The proposed method can be applied to any kind of electronic structure calculation methods such as density functional theory (DFT) or quantum Monte Carlo (QMC). We demonstrate that our method works well even when we use DFT results as input data instead of QMC ones. In addition, it enables us to calculate physical quantities directly related to the correlation effects without using complicated numerical procedures. As an example application, we show how this method can be used to investigate magnetic properties of strongly correlated materials. Our method will provide useful information about the nature of magnetism in these materials. This work was supported by JSPS KAKENHI Grant Number JP16K05481",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Gaussian - Basis Monte Carlo Method for Numerical Study on Ground States of Itinerant and Strongly Correlated Electron Systems . Abstract : We suggest the Gaussian - basis Monte Carlo method to study ground states of itinerant particle structures with large correlations , which is based on the variational concept in terms of the trial wave function expressed by the product of Gutzwiller - Jastrow parameters .The proposed approach can be applied to any sort of electronic structure calculation schemes such as density functional theory ( DFT ) or quantum Monte Carlo ( QMC ) . We suggest that our technique works well even when we using DFT results as input data rather of QMC ones .In addition , it allows us to estimate mechanical numbers specifically linked to the interaction phenomena without using difficult numerical procedures . As an instance application , we show how this technology can be used to probe magnetic properties of highly correlated materials .Our method will provide useful details about the nature of magnetism in these materials . This project was supported by JSPS KAKENHI Grant Number JP16K05481",
        "rewrite_text": "We introduce the Gaussian-basis Monte Carlo method as a novel approach for investigating the ground states of itinerant particle systems characterized by significant correlations. This method is grounded in the variational principle, utilizing a trial wave function that incorporates the product of Gutzwiller-Jastrow parameters. Our technique is versatile and can be integrated with various electronic structure calculation frameworks, including density functional theory (DFT) and quantum Monte Carlo (QMC). Notably, we demonstrate that our method remains effective even when DFT results are employed as input, rather than relying solely on QMC data. Furthermore, this approach simplifies the estimation of mechanical quantities associated with interaction phenomena, eliminating the need for complex numerical techniques. As a practical application, we illustrate how our method can be utilized to investigate the magnetic properties of strongly correlated materials. By employing this technique, we aim to enhance the understanding of the underlying mechanisms of magnetism in these systems. The research presented in this article was made possible through the support of the Japan Society for the Promotion of Science (JSPS) KAKENHI Grant Number JP16K05481.",
        "ori-fast-z-score": -1.5652475842498528,
        "water-fast-z-score": 4.777777777777778,
        "rewrite-fast-z-score": -2.3333333333333335
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Cooperative Transmission Protocols with High Spectral Efficiency and High Diversity Order Using Multiuser Detection and Network Coding .\nAbstract:\nIn this work, we propose cooperative transmission protocols for wireless networks using multiuser detection (MUD) at the relay node to improve spectral efficiency as well as diversity order. The proposed schemes are based on network coding techniques that allow multiple users in the system to share the same time-frequency resources by exploiting spatial multiplexing gain. We first consider an amplify-and-forward protocol where each user transmits its signal simultaneously over different antennas while the relay performs MUD before forwarding it to all other users. Then, we extend our analysis to decode-and forward scheme which allows us to achieve higher data rates than those achieved by amplify-and-forward scheme. Finally, we derive closed-form expressions for outage probability and ergodic capacity under Rayleigh fading channels. Our results show that the proposed schemes can significantly enhance both spectral efficiency and diversity order compared to conventional non-cooperative systems. In addition, they also demonstrate that the performance gap between the two considered cooperative schemes is negligible when the number of transmit antennas increases. \n \n Keywords: Cooperative communication; Amplify-and-Forward Protocol; Decode-and-Forward Scheme",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Cooperative Transmission Protocols with High Spectral Efficiency and High Diversity Order Using Multiuser Detection and Network Coding . Abstract : In this project , we develop cooperative communication protocols for wireless networks employing multiuser detection ( MUD ) at the relay node to achieve spectral capacity as well as diversity order .The proposed methods are based on channel coding techniques that enable different users in the system to exchange the same time - frequency resources by exploiting spatial multiplexing gain . We first consider an amplify - and - forward method where each user transmits its signal concurrently over distinct antennas while the relay performs MUD before forwarding it to all other users .Then , we expanded our analysis to decode - and forward scheme which allows us to achieve greater signal levels than those achieved by amplify - and - forward scheme . Finally , we derive closed - form expressions for outage likelihood and ergodic performance under Rayleigh fading channels .Our results show that the suggested schemes can significantly improved both spectral capacity and diversity order compared to conventional un - cooperative systems . In addition , they also demonstrate that the performance difference between the two deemed cooperative systems is negligible when the number of transmit antennas increases .Keywords: Cooperative communication; Amplify-and-Forward Protocol; Decode-and-Forward Scheme",
        "rewrite_text": "Title: Cooperative Transmission Protocols with High Spectral Efficiency and High Diversity Order Using Multiuser Detection and Network Coding\n\nAbstract: This study presents the development of advanced cooperative communication protocols tailored for wireless networks that utilize multiuser detection (MUD) at relay nodes. The primary objective is to enhance both spectral efficiency and diversity order. The proposed protocols leverage channel coding techniques, allowing multiple users to share the same time-frequency resources while capitalizing on spatial multiplexing gains. Initially, we investigate an amplify-and-forward (AF) approach, wherein each user transmits its signal simultaneously across separate antennas, while the relay node employs MUD to process and forward the signals to other users. Building on this foundation, we extend our analysis to a decode-and-forward (DF) scheme, which facilitates the transmission of higher signal levels compared to the AF method. Furthermore, we derive closed-form expressions for outage probability and ergodic performance in the context of Rayleigh fading channels. Our findings indicate that the proposed cooperative schemes markedly enhance both spectral capacity and diversity order when juxtaposed with traditional non-cooperative systems. Notably, the performance gap between the AF and DF schemes diminishes as the number of transmit antennas increases, suggesting that both methods can be effectively utilized in high-antenna scenarios. This research underscores the potential of cooperative communication strategies in optimizing wireless network performance, paving the way for more efficient and robust communication systems. \n\nKeywords: Cooperative communication; Amplify-and-Forward Protocol; Decode-and-Forward Scheme.",
        "ori-fast-z-score": 0.09950371902099892,
        "water-fast-z-score": 6.207574452435047,
        "rewrite-fast-z-score": 0.6527533657682196
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Non-zero trilinear parameter in the mSUGRA model - dark matter and collider signals at Tevatron and LHC .\nAbstract:\nWe study the phenomenology of non-zero trilinear soft supersymmetry breaking (SSB) term A 0 in minimal supergravity (mSUGRA). We find that for large values of tan β, there is an upper bound on |A 0 | which can be obtained by requiring correct electroweak symmetry breaking. For small values of tan β, we show that the allowed range of |A 0 | increases with decreasing value of M 1/2 . In both cases, the lower limit on |A 0 | comes from the requirement of not having charge and/or color breaking minima deeper than the electroweak vacuum. The effect of varying |A 0 | on sparticle masses are studied numerically using ISAJET 7.64. It is found that increasing |A 0 | leads to decrease in mass difference between lightest neutralino and lighter charginos as well as increase in mass splitting among squarks and sleptons. This results into enhancement of production cross sections of these particles at hadronic colliders like Tevatron and LHC.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Non - zero trilinear parameter in the mSUGRA model - dark matter and collider signals at Tevatron and LHC . Abstract : We research the phenomenology of non - zero trilinear soft supersymmetry breaking ( SSB ) term A 0 in reduced supergravity ( mSUGRA ) .We see that for large values of tan β , there is an upper bound on | A 0 | which can be obtained by requiring proper electroweak symmetry breaking . For small values of tan β , we find that the allowed range of | A 0 | increases with decreasing value of M 1 / 2 .In both cases , the lower limit on | A 0 | stems from the requirement of not having charge and / or color breaking minima deeper than the electroweak vacuum . The impact of differing | A 0 | on sparticle masses are studied numerically utilizing ISAJET 7 . 64 .It is found that expanding | A 0 | leads to decrease in mass ratio between lightest neutralino and lighter charginos as also as increase in mass separation among squarks and sleptons . This results into enhancement of production cross sections of these ions at hadronic colliders like Tevatron and LHC .",
        "rewrite_text": "In this study, we investigate the phenomenological implications of a non-zero trilinear soft supersymmetry breaking (SSB) parameter, denoted as A₀, within the framework of the minimal supergravity (mSUGRA) model. Our analysis reveals that for large values of the ratio of Higgs vacuum expectation values, tan β, there exists an upper limit on the magnitude of A₀, which is necessary to ensure proper electroweak symmetry breaking. Conversely, for smaller values of tan β, we observe that the permissible range for |A₀| expands as the parameter M₁/₂ decreases. In both scenarios, the lower bound on |A₀| is dictated by the requirement to avoid the existence of charge and/or color breaking minima that are deeper than the electroweak vacuum. \n\nWe conduct a numerical study using the ISAJET 7.64 package to explore how variations in |A₀| affect the masses of sparticles. Our findings indicate that increasing the value of |A₀| results in a reduced mass ratio between the lightest neutralino and the lighter charginos, while simultaneously leading to a greater mass separation among squarks and sleptons. This alteration in mass spectra has significant implications for the production cross sections of these particles at hadronic colliders, such as the Tevatron and the Large Hadron Collider (LHC). The enhanced production rates of sparticles due to variations in |A₀| could provide valuable insights into the nature of dark matter and the underlying mechanisms of supersymmetry breaking, making this research relevant for both theoretical and experimental physics.",
        "ori-fast-z-score": -0.23570226039551587,
        "water-fast-z-score": 3.3941932686877867,
        "rewrite-fast-z-score": 1.4779939172464398
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Thermodynamic quantum critical behavior of the Kondo necklace model .\nAbstract:\nWe study thermodynamics and transport properties of the Kondo necklacemodel (KNM) in the vicinity of its quantum phase transition to an ordered state, which is driven by spin fluctuations. We show that this transition can be described within the framework of Landau-Ginzburg-Wilson theory with a nontrivial scaling dimension of the order parameter field. The latter determines the universality class of the transition as well as the temperature dependence of various physical quantities such as specific heat or resistivity. In particular we find that at low temperatures the system exhibits non-Fermi liquid behavior characterized by power-law dependences of these quantities on T . \nI. INTRODUCTORY REMARkS\nThe Kondo necklace model 1 ,2 describes a chain of magnetic impurities coupled via antiferromagnetic exchange interactions J. It has been introduced originally for describing the physics of heavy fermion compounds 3 but it also appears naturally in other contexts 4-7 .\nIn the present work we consider the case when the coupling constant J exceeds some critical value Jc = 2t / U where t denotes hopping amplitude between neighboring sites and U stands for local Coulomb repulsion energy 8-10 . At zero temperature the ground state of the system corresponds then to a ferromagnetically ordered state 11-13 while at finite temperatures one expects a continuous quantum phase transition into a paramagnetic state 14-17 . This transition occurs due to strong spin fluctuations 18-20 and belongs therefore to the same universality class as the Heisenberg chain 21-23 . However there are important differences between both models 24-26 : First, the Kondo necklace contains two types of excitations -spinons and holons 27-29 -while the Heisenberg chain only consists of spin-1/2 particles. Second, the Kondo necklaces contain additional degrees of freedom associated with charge carriers 30-32 . These features lead to new phenomena not observed in the Heisenberg chain 33-36 . For example, the Kondo necklacelike systems may exhibit unconventional superconducting states 37-39 .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Thermodynamic quantum important dynamics of the Kondo necklace model . Abstract : We research thermodynamics and transport properties of the Kondo necklacemodel ( KNM ) in the vicinity of its quantum phase change to an ordered state , which is caused by spin fluctuations .We see that this shift can be described within the framework of Landau - Ginzburg - Wilson principle with a nontrivial scaling dimension of the order parameter field . The latter determines the universality type of the transition as well as the temperature dependence of several physical substances such as certain heat or resistivity .In particular we find that at low temperatures the system displays non - Fermi liquid behavior defined by power - law dependences of these quantities on T . I .INTRODUCTORY REMARkS The Kondo necklace model 1 , 2 describes a chain of magnetic impurities coupled via antiferromagnetic exchange interactions J . It has been proposed originally for describing the physics of large fermion compounds 3 but it also occurs commonly in other contexts 4 - 7 .In the present work we treat the case when the interaction factor J exceeds some essential value Jc = 2t / U where t denotes hopping frequency between neighboring regions and U stands for local Coulomb repulsion energy 8 - 10 . At zero temperature the ground state of the system belongs then to a ferromagnetically ordered state 11 - 13 while at finite temperatures one expects a periodic quantum phase change into a paramagnetic state 14 - 17 .This shift occurs due to powerful spin fluctuations 18 - 20 and belongs hence to the same universality category as the Heisenberg chain 21 - 23 . However there are important differences between both models 24 - 26 : First , the Kondo necklace includes two forms of excitations - spinons and holons 27 - 29 - while the Heisenberg chain only includes of spin - 1 / 2 atoms .Second , the Kondo necklaces feature new degrees of liberty associated with charge carriers 30 - 32 . These features lead to novel observations not observed in the Heisenberg chain 33 - 36 .For instance , the Kondo necklacelike networks could display unconventional superconducting states 37 - 39 .",
        "rewrite_text": "**Title:** Thermodynamic Quantum Dynamics of the Kondo Necklace Model\n\n**Abstract:** This study investigates the thermodynamic and transport characteristics of the Kondo necklace model (KNM) as it approaches a quantum phase transition into an ordered state, driven by spin fluctuations. We demonstrate that this transition can be effectively analyzed using the Landau-Ginzburg-Wilson framework, which reveals a nontrivial scaling dimension for the order parameter field. This scaling dimension is crucial as it dictates the universality class of the transition and influences the temperature dependence of various physical properties, including specific heat and electrical resistivity. Notably, our findings indicate that at low temperatures, the system exhibits non-Fermi liquid behavior characterized by power-law relationships between these properties and temperature.\n\nThe Kondo necklace model, which describes a chain of magnetic impurities linked through antiferromagnetic exchange interactions (J), was initially proposed to elucidate the physics of large fermionic compounds. However, it is also relevant in various other contexts. In our analysis, we focus on scenarios where the interaction strength J surpasses a critical threshold (Jc = 2t/U), with 't' representing the hopping frequency between adjacent sites and 'U' denoting the local Coulomb repulsion energy. At absolute zero, the system is found to be in a ferromagnetically ordered state, while at finite temperatures, it undergoes a periodic quantum phase transition to a paramagnetic state. This transition is attributed to significant spin fluctuations and aligns with the same universality class as the Heisenberg chain.\n\nDespite these similarities, the Kondo necklace model presents distinct differences. It incorporates two types of excitations—spinons and holons—unlike the Heisenberg chain, which consists solely of spin-1/2 particles. Additionally, the Kondo necklace introduces new degrees of freedom linked to charge carriers, leading to unique phenomena not observed in the Heisenberg framework. For example, networks resembling the Kondo necklace may exhibit unconventional superconducting states, highlighting the model's rich and complex behavior.",
        "ori-fast-z-score": -0.5,
        "water-fast-z-score": 6.7863682865394175,
        "rewrite-fast-z-score": 1.087114613009218
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Cosmic Acceleration, Dark Energy and Fundamental Physics .\nAbstract:\nThe discovery that the universe is expanding at an accelerating rate has led to intense interest in dark energy as well as new ideas about fundamental physics. This talk will review some recent results on these topics including constraints on models for cosmic acceleration using supernovae data, measurements of the Hubble constant with Type Ia supernovae, and tests of general relativity using gravitational lensing statistics. The talk will also discuss how future surveys such as LSST can be used to further our understanding of dark energy and fundamental physics. I will conclude by discussing my own work on testing gravity theories beyond Einstein s theory using weak lensing observations. Keywords: Cosmology, Dark Energy, General Relativity, Weak Lensing, Supernovae, Gravitational Waves, Cosmic Microwave Background. Speaker: Adam Riess (Princeton University) Date: February 18, 2007 Time: 4:30pm - 5:15pm Location: Room B",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Cosmic Acceleration , Dark Energy and Fundamental Physics . Abstract : The observation that the universe is growing at an accelerating rate has led to intense interest in dark energy as well as fresh concepts about basic physics .This discussion will review some latest findings on these topics including constraints on estimates for cosmic acceleration using supernovae information , measurements of the Hubble constant with Type Ia supernovae , and examinations of general relativity involving gravitational lensing statistics . The speech will also discuss how potential experiments such as LSST can be used to further our knowing of light mass and fundamental physics .I will conclude by reviewing my own research on proving gravity explanations beyond Einstein s principle involving weak lensing observations . Keywords : Cosmology , Dark Energy , General Relativity , Weak Lensing , Supernovae , Gravitational Waves , Cosmic Microwave Background .Speaker : Adam Riess ( Princeton University ) Date : February 18 , 2007 Time : 4 : 30pm - 5 : 15pm Location : Room B",
        "rewrite_text": "Title: Cosmic Acceleration, Dark Energy, and Fundamental Physics\n\nAbstract: The discovery that the universe is expanding at an accelerating pace has sparked significant interest in the nature of dark energy and has prompted new inquiries into fundamental physics. This article aims to review recent advancements in our understanding of these phenomena, focusing on various constraints derived from supernova observations that inform our estimates of cosmic acceleration. Additionally, we will explore the implications of Type Ia supernovae measurements on the determination of the Hubble constant and investigate the role of gravitational lensing statistics in testing the principles of general relativity. The discussion will also highlight the potential of upcoming experiments, such as the Large Synoptic Survey Telescope (LSST), to enhance our comprehension of light mass and fundamental physical laws. Furthermore, I will present my own research efforts aimed at extending gravitational theories beyond Einstein's framework, particularly through the analysis of weak lensing data. This comprehensive review will encompass key topics in cosmology, including dark energy, general relativity, weak lensing, supernovae, gravitational waves, and the cosmic microwave background. \n\nKeywords: Cosmology, Dark Energy, General Relativity, Weak Lensing, Supernovae, Gravitational Waves, Cosmic Microwave Background.\n\nSpeaker: Adam Riess (Princeton University)  \nDate: February 18, 2007  \nTime: 4:30 PM - 5:15 PM  \nLocation: Room B",
        "ori-fast-z-score": -0.8682431421244593,
        "water-fast-z-score": 4.764608329895903,
        "rewrite-fast-z-score": 1.4100479758212652
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Diffuse HI Disks in Isolated Galaxies .\nAbstract:\nWe present new observations and analysis of the neutral hydrogen (HI) disks surrounding isolated galaxies, using data obtained with the Very Large Array (VLA). We have observed 12 nearby galaxies at 21 cm wavelength to determine their total HI mass and distribution within the optical disk. The sample includes both late-type spirals and dwarf irregulars. Our results show that all but one galaxy has an extended HI halo beyond its optical radius; this is true even for the most gas-rich systems such as NGC 4254 and NGC 5253. In addition, we find evidence for significant amounts of atomic gas outside our detection limits which may be associated with tidal features or other interactions between these galaxies and neighboring companions. These findings are consistent with previous studies showing that many isolated spiral galaxies contain large quantities of cold interstellar medium distributed over several kiloparsecs. \n \n Keywords: Atomic Hydrogen, Galaxy Evolution, Spiral Structure",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Diffuse HI Disks in Isolated Galaxies . Abstract : We report new studies and investigation of the neutral hydrogen ( HI ) disks surrounding isolated stars , using data acquired with the Very Large Array ( VLA ) .We have noted 12 nearby galaxies at 21 cm wavelength to estimate their total HI mass and distribution within the optical disk . The sample comprises both late - class spirals and dwarf irregulars .Our results show that all but one star has an extended HI halo beyond its optical diameter ; this is true even for the most gas - rich systems such as NGC 4254 and NGC 5253 . In addition , we find proof for significant amounts of nuclear gas outside our detection limits which may be involved with tidal features or other relationships between these galaxies and surrounding companions .These conclusions are consistent with previous research indicating that several isolated spiral nuclei contain significant amounts of cold interstellar medium spread over several kiloparsecs . Keywords : Atomic Hydrogen , Galaxy Evolution , Spiral Structure",
        "rewrite_text": "We present a comprehensive study on the neutral hydrogen (HI) disks surrounding isolated galaxies, utilizing data obtained from the Very Large Array (VLA). Our investigation focuses on a sample of 12 nearby galaxies observed at a wavelength of 21 cm, allowing us to assess their total HI mass and its distribution relative to their optical disks. This sample includes a diverse range of galaxy types, specifically late-type spirals and dwarf irregular galaxies. Our findings reveal that nearly all of the galaxies in our sample, with the exception of one, possess extended HI halos that extend beyond their optical diameters. Notably, this phenomenon is observed even in the most gas-rich galaxies, such as NGC 4254 and NGC 5253. Furthermore, we have identified evidence suggesting the presence of substantial amounts of nuclear gas that lie beyond our detection capabilities. This gas may be associated with tidal features or other interactions between these isolated galaxies and their neighboring companions. These results align with previous studies that have indicated that many isolated spiral nuclei harbor significant quantities of cold interstellar medium distributed over several kiloparsecs. Our research contributes to the understanding of atomic hydrogen in galaxy evolution and the structural dynamics of spiral galaxies. The implications of these findings are crucial for advancing our knowledge of galaxy formation and the role of HI in the broader context of cosmic evolution. \n\nKeywords: Atomic Hydrogen, Galaxy Evolution, Spiral Structure.",
        "ori-fast-z-score": -0.8944271909999159,
        "water-fast-z-score": 5.0,
        "rewrite-fast-z-score": -0.18569533817705186
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Bringing errors into focus .\nAbstract:\nThe author describes how he has used the concept of  errors in thinking  to help students understand and learn about science, particularly physics.  He explains that this approach is based on his own experience as an undergraduate student at Harvard University where he was taught by Professor Richard Feynman.   The author suggests that it may be useful for teachers to use similar approaches when teaching other subjects such as mathematics or chemistry. The author also discusses some of the difficulties associated with using this method. This article appeared originally in Physics Today (Vol. 51 No. 1 January 1997). It can be downloaded from: http://arxiv.org/abs/physics/9701001v1.pdf  In order to teach physics effectively, one must first have a clear understanding of what constitutes good thinking within the subject area. Unfortunately, many people who are not trained scientists do not think like physicists; they tend to make mistakes which often lead them astray. For example, if asked whether two objects will fall together towards earth s center of mass, most people would say no because they believe that gravity acts only between bodies that touch each other. However, according to Newtonian mechanics, gravitational attraction does act upon all matter regardless of its position relative to another body. If you were to ask someone else why two objects don t fall together, she might answer that they are connected by a string so their weight cancels out. Although this explanation seems reasonable, it fails to take into account the fact that the force exerted by the string is negligible compared to the forces acting upon both objects individually. As a result, her reasoning is flawed.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Bringing errors into focus . Abstract : The author explains how he has used the idea of errors in thoughts to assist children understand and learn about science , particularly science .He states that this methodology is based on his own experience as an undergraduate student at Harvard University where he was taught by Professor Richard Feynman . The author argues that it could be beneficial for students to use similar approaches when taught other subjects such as mathematics or chemistry .The author also explains some of the problems involved with use this process . This section appeared originally in Physics Today ( Vol .51 No.1 January 1997).It can be downloaded from : www : / / arxiv . org / abs / physics / 9701001v1 . pdf In order to teach physics successfully , one must first have a clear awareness of what constitutes better thinking within the subject region . Unfortunately , many people who are not trained experts do not thought like physicists ; they tend to make mistakes which frequently lead them astray .For instance , if asking whether two bodies will drop together towards earth s center of mass , most people would tell no because they believe that gravity works only between bodies that reach each other . However , according to Newtonian physics , gravity attraction does action upon all matter regardless of its position relative to another bodies .If you were to ask someone else why two pieces don t drop apart , she might respond that they are connected by a string so their load cancels out . Although this explanation seems rational , it fails to take into consideration the fact that the force exerted by the string is negligible compared to the forces working upon both objects independently .As a result , her reasoning is flawed .",
        "rewrite_text": "**Title: Bringing Errors into Focus**\n\n**Abstract:** In this article, the author explores the concept of utilizing errors in reasoning as a pedagogical tool to enhance children's understanding of science, with a particular emphasis on physics. Drawing from his experiences as an undergraduate at Harvard University under the mentorship of Professor Richard Feynman, the author posits that recognizing and addressing misconceptions can significantly aid in the learning process. He advocates for the application of this methodology not only in physics but also in other disciplines such as mathematics and chemistry, suggesting that similar strategies could foster deeper comprehension across various subjects.\n\nThe author highlights the challenges associated with implementing this approach, noting that many individuals without formal training in physics often struggle to think like physicists. This lack of expertise can lead to common misconceptions that hinder their understanding of fundamental concepts. For example, when posed with the question of whether two objects will fall towards the Earth's center of mass simultaneously, many would incorrectly assert that they will not, mistakenly believing that gravitational attraction only occurs between objects that are in direct contact. In contrast, Newtonian physics asserts that gravitational forces act on all matter, irrespective of its proximity to other bodies.\n\nFurthermore, the author illustrates how flawed reasoning can arise from oversimplified explanations. For instance, if someone suggests that two objects do not fall apart because they are connected by a string, they may overlook the fact that the tension in the string is negligible compared to the gravitational forces acting on the objects. This oversight exemplifies how misconceptions can lead to erroneous conclusions.\n\nOverall, the article emphasizes the importance of fostering a clear understanding of scientific principles and encourages educators to embrace the examination of errors as a means to improve critical thinking skills in students. The original discussion appeared in Physics Today (Vol. 51 No. 1, January 1997) and is available for download at www.arxiv.org/abs/physics/9701001v1.pdf.",
        "ori-fast-z-score": -0.43033148291193524,
        "water-fast-z-score": 7.602042145115969,
        "rewrite-fast-z-score": 0.49656353316142077
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Distance Geometry of Music .\nAbstract:\nThe distance geometry problem is the following: given n points in space, find their coordinates such that they are as close to each other as possible while respecting some constraints on distances between pairs of points.  In this work we consider an application of the distance geometry problem to music analysis and propose a novel method for automatic transcription of musical scores into symbolic representations based on the concept of pitch class profiles (PCP). The proposed approach allows us to recover the underlying harmonic structure of polyphonic music by solving a system of quadratic equations with linear equality constraints using convex optimization techniques. We demonstrate our algorithm s performance on several classical piano pieces. 1 Introduction\n\nMusic Analysis\nAutomatic transcription of musical scores has been one of the most challenging problems in computer science over the past decades. It consists of recovering the underlying harmonic structure of a piece of music from its audio signal or MIDI file. This task can be divided into two main subtasks:  detection of note onset times; estimation of pitches at detected notes  locations. Note onset time detection is usually performed by applying various heuristics to the raw audio data  22, 23  . Once the note onset times have been determined, the next step is to estimate the pitches corresponding to these events. There exist many different approaches to solve this problem ranging from simple template matching methods to more sophisticated statistical models  7, 8, 10, 11, 13, 14, 16, 17, 19-21, 24-26  .\nIn this work we focus on the second part of the problem -estimation of pitches-which is known as  pitch estimation  or  pitch tracking . Pitch tracking algorithms try to assign a pitch value to every detected event in order to obtain a sequence of pitch values which correspond to the original score. A common way to represent pitches is through so-called pitch-class profiles (PCPs)  6, 12, 15, 18, 27  , where each entry corresponds to the number of occurrences of a particular pitch within a certain window around the current time instant. For example, Figure 1 shows a typical PCP obtained from a single-note mel",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Distance Geometry of Music . Abstract : The distance geometry issue is the following : given n points in space , find their coordinates such that they are as close to each other as possible while respecting some restrictions on distances between pairs of points .In this study we investigate an use of the distance geometry issue to music analysis and suggest a new method for efficient reproduction of musical scores into symbolic representations based on the idea of pitch class profiles ( PCP ) . The proposed approach allows us to extract the fundamental chord form of polyphonic music by modeling a system of quadratic equations with linear equality restrictions utilizing convex optimization tools .We test our algorithm s playing on numerous classical piano compositions . 1 Introduction Music Analysis Automatic recording of piano scores has been one of the most challenging difficulties in computer science over the previous decades .It consists of collecting the fundamental chord form of a work of music from its audio sound or MIDI file . This job can be grouped into two principal subtasks : detection of note onset times ; estimation of pitches at identified notes sites .Note onset time detection is usually performed by using numerous heuristics to the raw audio information 22 , 23 . Once the tone onset times have been determined , the second step is to estimate the pitches corresponding to these actions .There remain many various approaches to solve this question ranging from basic template matching algorithms to more sophisticated mathematical models 7 , 8 , 10 , 11 , 13 , 14 , 16 , 17 , 19 - 21 , 24 - 26 . In this research we focus on the second part of the question - estimation of pitches - which is known as pitch estimation or pitch tracking .Pitch tracking schemes seek to give a pitch number to every detected event in order to obtain a sequence of pitch values which coincide to the actual score . A popular way to measure pitches is through so - called pitch - class profiles ( PCPs ) 6 , 12 , 15 , 18 , 27 , where each entry relates to the quantity of occurrences of a given pitch within a certain window around the present time instant .For instance , Figure 1 shows a typical PCP obtained from a single - note mel",
        "rewrite_text": "**Title: The Distance Geometry of Music**\n\n**Abstract:** This article addresses the distance geometry problem, which involves determining the coordinates of n points in space such that they are positioned as closely as possible to one another while adhering to specific distance constraints between pairs of points. We explore the application of this problem within the realm of music analysis and propose a novel method for the efficient transformation of musical scores into symbolic representations, leveraging the concept of pitch class profiles (PCP). Our approach facilitates the extraction of the fundamental chord structure from polyphonic music by formulating a system of quadratic equations with linear equality constraints, employing convex optimization techniques. We validate our algorithm by applying it to a diverse set of classical piano compositions.\n\nIn the introduction, we highlight the challenges associated with the automatic transcription of piano scores, a significant issue in computer science over recent decades. This task involves deriving the fundamental chord structure from audio recordings or MIDI files, which can be divided into two main subtasks: detecting note onset times and estimating the pitches at these identified note locations. Note onset detection typically relies on various heuristics applied to raw audio data. Once the onset times are established, the next step is to estimate the corresponding pitches. Numerous methodologies exist to tackle this challenge, ranging from basic template matching techniques to more advanced mathematical models. Our research specifically concentrates on the pitch estimation aspect, also known as pitch tracking. Pitch tracking systems aim to assign a pitch value to each detected note event, resulting in a sequence of pitch values that accurately reflects the original score. A widely used method for measuring pitches is through pitch-class profiles (PCPs), where each entry indicates the frequency of a specific pitch within a defined time window. For example, we present a typical PCP derived from a single-note melody, illustrating the effectiveness of our proposed method in capturing the essential musical elements.",
        "ori-fast-z-score": -0.5897678246195885,
        "water-fast-z-score": 8.798826901281197,
        "rewrite-fast-z-score": 1.1338934190276817
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Softening of the stiffness of bottlebrush polymers by mutual interaction .\nAbstract:\nThe effect of intermolecular interactions on the elastic properties of bottlebrush polymer networks is investigated using molecular dynamics simulations and experimental measurements. The results show that the network becomes softer with increasing density, which can be explained in terms of an increase in the number of entanglements between neighboring chains. This softening behavior is also observed experimentally for bottlebrush polyurethane networks prepared at different densities. It is found that the simulated stress-strain curves are well described by the Gent model, where the effective spring constant decreases as the density increases. These findings suggest that the mechanical properties of bottlebrush polymer gels can be tuned by changing their density. \n \n Soft materials such as rubbers have been widely used in many applications including tires, seals, adhesives, and biomaterials because they exhibit unique physical characteristics such as high extensibility and low modulus  1–3 . In particular, bottlebrush polymers consisting of densely grafted side chains have attracted considerable attention due to their outstanding mechanical properties  4–6 . For example, it has recently been reported that bottlebrush poly(ether urethane)s (PEUs), synthesized via ring-opening metathesis polymerization  7, 8 , possess higher tensile strength than conventional PEU elastomers  9 . However, despite these advantages, there remain some challenges associated with the use of bottlebrush polymers in practical applications. One of them is how to control the mechanical properties of bottle brushes since the macroscopic properties depend strongly on the microstructure  10, 11 . Therefore, understanding the relationship between structure and property is important for designing new types of bottlebrush-based materials. \nIn this study, we investigate the influence of chain density on the mechanical properties of bottle-brush polyurethanes (BPUs). We find that BPUs become softer when the density increases. To understand this phenomenon, we perform molecular dynamics (MD) simulations based on coarse-grained models  12 . Our simulation results reveal that the network becomes sof...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Softening of the stiffness of bottlebrush polymers by mutual interaction . Abstract : The impact of intermolecular interactions on the elastic properties of bottlebrush plastic networks is investigated using molecular mechanics simulations and experimental surveys .The results show that the chain gets softer with rising density , which can be described in terms of an increase in the number of entanglements between neighboring bars . This softening activity is also observed experimentally for bottlebrush polyurethane systems made at different densities .It is found that the simulated stress - strain curves are better illustrated by the Gent model , where the effective spring coefficient drops as the density increases . These studies demonstrate that the structural properties of bottlebrush plastic gels can be tuned by varying their density .Soft materials such as rubbers have been widely useful in multiple applications notably shoes , seals , adhesives , and biomaterials because they demonstrate unique physical qualities such as great extensibility and low modulus 1 – 3 . In particular , bottlebrush polymers consisting of densely grafted front chains have garnered considerable scrutiny due to their outstanding mechanical values 4 – 6 .For instance , it has recently been reported that bottlebrush poly ( ether urethane ) s ( PEUs ) , generated via ring - extension metathesis polymerization 7 , 8 , possess better tensile strength than conventional PEU elastomers 9 . However , despite these benefits , there remain some challenges associated with the using of bottlebrush polymers in practical applications .One of them is how to affect the structural properties of pipe brushes since the macroscopic properties depend greatly on the microstructure 10 , 11 . Therefore , studying the relationship between form and property is important for constructing new types of bottlebrush - based products .In this study , we investigate the impact of chain density on the mechanical behavior of bottle - brush polyurethanes ( BPUs ) . We see that BPUs get lighter when the density increases .To understand this phenomenon , we perform molecular dynamics ( MD ) simulations based on coarse - grained models 12 . Our model results show that the organization transforms sof . . .",
        "rewrite_text": "**Title:** Softening of the Stiffness of Bottlebrush Polymers Due to Mutual Interactions\n\n**Abstract:** This study explores the influence of intermolecular interactions on the elastic properties of bottlebrush polymer networks through a combination of molecular mechanics simulations and experimental investigations. Our findings reveal that as the density of the polymer chains increases, the stiffness of the material decreases, which can be attributed to a rise in the number of entanglements among adjacent polymer segments. This softening effect is corroborated by experimental data from bottlebrush polyurethane systems synthesized at varying densities. Notably, the stress-strain behavior observed in simulations aligns more closely with the Gent model, indicating that the effective spring constant diminishes with increasing density. These insights suggest that the mechanical properties of bottlebrush polymer gels can be effectively tuned by adjusting their density.\n\nSoft materials, such as rubbers, are integral to a wide range of applications, including footwear, seals, adhesives, and biomaterials, due to their exceptional physical characteristics, including high extensibility and low modulus. Bottlebrush polymers, characterized by densely grafted side chains, have attracted significant attention for their superior mechanical properties. Recent studies have demonstrated that bottlebrush poly(ether urethanes) (PEUs), produced via ring-extension metathesis polymerization, exhibit enhanced tensile strength compared to traditional PEU elastomers. However, challenges remain in the practical application of bottlebrush polymers, particularly regarding the manipulation of their structural properties, as macroscopic behavior is heavily influenced by microstructural characteristics.\n\nThis research aims to elucidate the relationship between chain density and the mechanical performance of bottlebrush polyurethanes (BPUs). Our results indicate that BPUs exhibit a reduction in stiffness with increasing density. To further investigate this phenomenon, we employed molecular dynamics (MD) simulations utilizing coarse-grained models. The outcomes of our simulations reveal significant transformations in the organization of the polymer chains, leading to a better understanding of the interplay between structure and mechanical properties in bottlebrush polymers. This knowledge is crucial for the development of innovative bottlebrush-based materials with tailored properties for various applications.",
        "ori-fast-z-score": -0.16116459280507606,
        "water-fast-z-score": 8.594446819256738,
        "rewrite-fast-z-score": -0.8219949365267865
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Electronic structure of BaFeO3: an abinitio DFT study .\nAbstract:\nBaFeO3 is one of the most promising cathode materials for solid oxide fuel cells (SOFCs). In this work, we have performed first-principles density functional theory calculations to investigate its electronic and magnetic properties in detail. We find that it has a metallic ground state with a large spin polarization at the Fermi level. The calculated band gap between the valence bands and conduction bands is 0.2 eV which agrees well with previous experimental results. Our calculation shows that there are two Fe-3d orbitals crossing the Fermi level, indicating that both Fe3+ ions and Fe4+ ions exist in the compound simultaneously. This result can be explained by the fact that the crystal field splitting energy is much smaller than the electron-electron interaction energies. Furthermore, our calculation indicates that the ferromagnetic ordering is mainly due to superexchange interactions mediated through oxygen atoms.  Finally, we also calculate the phonon dispersion relations along high symmetry lines within the framework of density functional perturbation theory.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Electronic structure of BaFeO3 : an abinitio DFT study . Abstract : BaFeO3 is one of the most attractive cathode materials for solid oxide petrol cells ( SOFCs ) .In this research , we have done first - principles density functional theory analyses to examine its mechanical and magnetic properties in detail . We see that it has a metallic ground state with a large spin polarization at the Fermi level .The measured band gap between the valence bands and conduction bands is 0 . 2 eV which agrees well with previous experimental results . Our measurement shows that there are two Fe - 3d orbitals crossing the Fermi level , showing that both Fe3 + ions and Fe4 + ions present in the complex jointly .This result can be described by the fact that the crystal field separation energy is much smaller than the electron - atom bonding energies . Furthermore , our calculation suggests that the ferromagnetic ordering is mainly owing to superexchange interactions mediated through oxygen atoms .Finally , we also define the phonon dispersion relations along high symmetry lines within the framework of density functional perturbation theory .",
        "rewrite_text": "Title: Electronic Structure of BaFeO3: An Ab Initio DFT Study\n\nAbstract: BaFeO3 has emerged as a highly promising cathode material for solid oxide fuel cells (SOFCs) due to its unique electronic properties. In this study, we employ first-principles density functional theory (DFT) to conduct a comprehensive analysis of the mechanical and magnetic characteristics of BaFeO3. Our findings reveal that the compound exhibits a metallic ground state characterized by significant spin polarization at the Fermi level. We have determined a band gap of 0.2 eV between the valence and conduction bands, which aligns well with existing experimental data. Notably, our analysis indicates the presence of two Fe-3d orbitals that cross the Fermi level, suggesting the coexistence of both Fe3+ and Fe4+ ions within the material. This observation can be attributed to the relatively small crystal field splitting energy compared to the bonding energies of the electrons with the atoms. Additionally, our calculations indicate that the observed ferromagnetic ordering in BaFeO3 is primarily driven by superexchange interactions facilitated by oxygen atoms. To further our understanding of the material's vibrational properties, we also calculate the phonon dispersion relations along high-symmetry directions using density functional perturbation theory. This comprehensive study not only enhances our understanding of the electronic structure of BaFeO3 but also provides valuable insights into its potential applications in energy conversion technologies.",
        "ori-fast-z-score": 0.21081851067789195,
        "water-fast-z-score": 5.004345937369795,
        "rewrite-fast-z-score": 1.2686700948330931
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The role of GRB 031203 in clarifying the astrophysical GRB scenario .\nAbstract:\nThe Gamma Ray Burst (GRB) 031203 was detected by several satellites on December 3rd, 2003 at 07:47 UT and lasted about 20 s. The burst had an extremely bright optical counterpart with R ~ 12 mag., which faded rapidly to V ~ 21 mag. within one day after the burst. This is among the most energetic events ever observed in the universe. We present here our analysis of this event based on data obtained mainly by Swift satellite. Our results show that the prompt emission can be well described as a single-peaked pulse lasting for about 10 seconds followed by a power-law decaying tail extending up to 100 seconds. The X-ray light curve shows two flares peaking around 5 and 15 seconds respectively following the main peak. These features are similar to those seen in other GRBs but their relative strength is much stronger than usual. In addition we find evidence for spectral evolution during the prompt phase.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The role of GRB 031203 in clarifying the astrophysical GRB situation . Abstract : The Gamma Ray Burst ( GRB ) 031203 was detected by many satellites on December 3rd , 2003 at 07 : 47 UT and lasted about 20 s . The flash had an incredibly bright optical predecessor with R ~ 12 mag . , which fading rapidly to V ~ 21 mag .within one day after the exploded . This is among the most intense phenomena ever observed in the universe .We present here our analysis of this event based on evidence derived mainly by Swift satellite . Our results show that the prompt emission can be well described as a single - peaked pulse lasting for about 10 seconds followed by a power - law decaying tail spanning up to 100 seconds .The X - ray light diagram sees two flares peaking roughly 5 and 15 seconds respectively past the main peak . These features are related to those shown in other GRBs but their relative quality is much larger than usual .In addition we find proof for spectral evolution during the prompt phase .",
        "rewrite_text": "Title: The Role of GRB 031203 in Clarifying the Astrophysical GRB Situation\n\nAbstract: On December 3, 2003, at 07:47 UT, Gamma Ray Burst (GRB) 031203 was detected by multiple satellites, marking a significant event in the study of astrophysical phenomena. This burst lasted approximately 20 seconds and was accompanied by an exceptionally bright optical precursor, reaching a magnitude of R ~ 12, which rapidly faded to V ~ 21 within a day of the explosion. GRB 031203 is considered one of the most intense events ever recorded in the universe. In this article, we present a comprehensive analysis of GRB 031203, primarily utilizing data obtained from the Swift satellite. Our findings indicate that the prompt emission of the burst can be effectively characterized as a single-peaked pulse lasting around 10 seconds, followed by a power-law decay that extends for up to 100 seconds. Notably, the X-ray light curve reveals two distinct flares occurring approximately 5 and 15 seconds after the main peak, respectively. These features, while reminiscent of those observed in other GRBs, exhibit a significantly enhanced relative intensity. Furthermore, our analysis provides evidence of spectral evolution during the prompt phase of the burst, suggesting complex underlying mechanisms at play. The insights gained from GRB 031203 contribute to a deeper understanding of gamma-ray bursts and their astrophysical implications, highlighting the importance of this event in the broader context of GRB research. This study not only enhances our knowledge of GRB 031203 itself but also serves to clarify the overall landscape of gamma-ray bursts, paving the way for future investigations into these extraordinary cosmic phenomena.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.817730411281796,
        "rewrite-fast-z-score": 1.6570343122169822
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Many-to-One Throughput Capacity of IEEE 802.11 Multi-hop Wireless Networks .\nAbstract:\nIn this work, we study the throughput capacity region for multi-hop wireless networks with multiple transmitters and one receiver (MISO-MHWN). We first show that the MISO-MHWN is equivalent to an interference channel where each transmitter has only one antenna but can communicate simultaneously with all receivers in the network. Then, by using the concept of degrees-of-freedom (DoF), which characterizes how many parallel data streams are supported at high signal-to-noise ratio (SNR) regime, we derive outer bounds on the DoF region of the MISO-MHWNS. Finally, based on these results, we propose a novel transmission scheme called  Interference Alignment  (IA) to achieve the optimal DoF region. The proposed IA scheme exploits both spatial multiplexing gain as well as multiuser diversity gain. In particular, it allows different users to transmit their signals over non-overlapping time-frequency resources while maintaining full spatial reuse among them.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Many - to - One Throughput Capacity of IEEE 802 . 11 Multi - hop Wireless Networks . Abstract : In this project , we study the throughput capacity area for single - hop wireless networks with many transmitters and one receiver ( MISO - MHWN ) .We first see that the MISO - MHWN is analogous to an interference channel where each antenna has only one antenna but can communicate simultaneously with all receivers in the channel . Then , by using the idea of degrees - of - independence ( DoF ) , which characterizes how many concurrent data feeds are implemented at high signal - to - noise ratio ( SNR ) regime , we derive outer bounds on the DoF area of the MISO - MHWNS .Finally , based on these results , we propose a new transmission strategy titled Interference Alignment ( IA ) to achieve the ideal DoF area . The proposed IA plan involves both temporal multiplexing gain as well as multiuser flexibility gain .In particular , it allows different users to transmit their messages over non - overlapping period - frequency resources while maintaining full spatial reuse among them .",
        "rewrite_text": "Title: Many-to-One Throughput Capacity of IEEE 802.11 Multi-hop Wireless Networks\n\nAbstract: This study investigates the throughput capacity of single-hop wireless networks characterized by multiple transmitters and a single receiver, referred to as MISO-MHWN (Multiple Input Single Output - Multi-Hop Wireless Network). We draw parallels between the MISO-MHWN and an interference channel, where each transmitter is equipped with a single antenna but can simultaneously communicate with all receivers present in the channel. By leveraging the concept of degrees-of-freedom (DoF), which quantifies the number of concurrent data streams that can be supported in a high signal-to-noise ratio (SNR) environment, we establish outer bounds for the DoF region of MISO-MHWNs. Building on these theoretical foundations, we introduce a novel transmission strategy known as Interference Alignment (IA) aimed at achieving the optimal DoF region. The IA approach incorporates both temporal multiplexing gains and multiuser flexibility, enabling distinct users to transmit their messages over non-overlapping time-frequency resources while ensuring full spatial reuse. This innovative strategy not only enhances the overall throughput capacity of the network but also optimizes resource allocation among users, thereby improving communication efficiency in multi-hop wireless environments. Our findings contribute to a deeper understanding of the capacity limits of MISO-MHWNs and provide practical insights for the design of future wireless communication systems.",
        "ori-fast-z-score": -1.2222222222222223,
        "water-fast-z-score": 4.638124095143555,
        "rewrite-fast-z-score": -1.7650452162436565
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Going beyond perturbation theory: Parametric Perturbation Theory .\nAbstract:\nParametric Perturbation Theory (PPT) is an extension to the standard perturbative approach in quantum field theories, which allows for systematically including higher-order corrections into Feynman diagrams and thus improving their accuracy.  In this talk I will present PPT as well as its application to QCD processes at hadron colliders such as Drell-Yan production or Higgs boson decay into bottom quarks. The results are obtained within the framework of Soft-Collinear Effective Field Theory (SCET). This talk was given by Jens Wackeroth on behalf of the CMS collaboration during the LHCP2018 conference held in Paris between June 24th - 28th 2018. It has been recorded with permission. For more information about the speaker please see his profile page here . \nJens Wackeroth is a postdoctoral researcher working on soft-collinear effective theory and parametric perturbation theory at CERN.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Going beyond perturbation theory : Parametric Perturbation Theory . Abstract : Parametric Perturbation Theory ( PPT ) is an extension to the standard perturbative methodology in quantum field theories , which allows for systematically incorporating higher - order corrections into Feynman diagrams and therefore advancing their authenticity .In this talk I will present PPT as well as its use to QCD processes at hadron colliders such as Drell - Yan production or Higgs boson absorption into bottom quarks . The results are derived within the framework of Soft - Collinear Effective Field Theory ( SCET ) .This discussion was given by Jens Wackeroth on behalf of the CMS collaboration during the LHCP2018 seminar conducted in Paris between June 24th - 28th 2018 . It has been recorded with permit .For more information about the speaker please find his profile article here . Jens Wackeroth is a postdoctoral researcher working on soft - collinear effective theory and parametric perturbation theory at CERN .",
        "rewrite_text": "Title: Advancing Beyond Perturbation Theory: An Introduction to Parametric Perturbation Theory\n\nAbstract: Parametric Perturbation Theory (PPT) represents a significant advancement over traditional perturbative techniques employed in quantum field theories. This innovative approach facilitates the systematic inclusion of higher-order corrections in Feynman diagrams, thereby enhancing their precision and reliability. In this presentation, I will introduce the principles of PPT and demonstrate its application to Quantum Chromodynamics (QCD) processes at hadron colliders, specifically focusing on phenomena such as Drell-Yan production and the absorption of Higgs bosons by bottom quarks. The findings are articulated within the framework of Soft-Collinear Effective Field Theory (SCET), which provides a robust foundation for analyzing these complex interactions. This talk was delivered by Jens Wackeroth on behalf of the CMS collaboration during the LHCP2018 seminar held in Paris from June 24th to 28th, 2018, and has been recorded with permission. For further insights into the speaker's expertise, please refer to his profile article. Jens Wackeroth is a postdoctoral researcher at CERN, specializing in soft-collinear effective theory and parametric perturbation theory, contributing to the ongoing advancements in our understanding of quantum field interactions.",
        "ori-fast-z-score": -1.2135597524338357,
        "water-fast-z-score": 2.4053511772118195,
        "rewrite-fast-z-score": -0.9561828874675149
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Needlet algorithms for estimation in inverse problems .\nAbstract:\nWe consider the problem of estimating an unknown function f from noisy measurements y = Af + w, where A is a linear operator and w is white noise with known covariance matrix Cw.  We assume that the operator A has been discretized on some grid (e.g., by using finite differences or spectral methods) so that it can be represented as a large matrix.  The goal is to find an estimatef such that Ef − f  2 is minimized subject to certain constraints on the smoothness off . \nIn this work we propose new numerical techniques based on needlets which are able to efficiently solve these constrained optimization problems.  In particular, our approach allows us to obtain accurate estimates even when the number N of available observations is much smaller than the dimension M of the space spanned by the columns of A. \nThe proposed method relies on two main ingredients:  First, we use a sparse representation of functions in terms of needlets.  Second, we develop efficient iterative algorithms for solving large-scale convex optimization problems involving sparsity-promoting regularizers.  These algorithms combine ideas from compressed sensing theory and recent results about the convergence rate of alternating direction method of multipliers (ADMM).",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Needlet techniques for estimation in inverse problems . Abstract : We consider the question of estimating an unknown function f from noisy measurements y = Af + f , where A is a linear operator and f is white sound with known covariance matrix Cw .We assume that the operator A has been discretized on some grid ( e . g . , by using finite differences or spectral algorithms ) so that it can be described as a large matrix . The goal is to find an estimatef such that Ef − f 2 is minimized subject to certain constraints on the smoothness off .In this study we develop new numerical tactics based on needlets which are able to easily solution these constrained optimization problems . In particular , our approach allows us to obtain precise estimates even when the number N of available observed is much smaller than the dimension M of the space spanned by the rows of A .The proposed approach consists on two principal ingredients : First , we using a sparse representation of functions in terms of needlets . Second , we develop fast iterative techniques for solving large - scale convex optimization problems employing sparsity - preserving regularizers .These methods combine developments from compressed sensing theory and recent results about the convergence speed of alternating path method of multipliers ( ADMM ) .",
        "rewrite_text": "In this article, we address the challenge of estimating an unknown function \\( f \\) from noisy observations represented as \\( y = Af + f \\), where \\( A \\) denotes a linear operator and \\( f \\) is modeled as white noise characterized by a known covariance matrix \\( C_w \\). We operate under the assumption that the operator \\( A \\) has been discretized on a grid, which can be achieved through techniques such as finite differences or spectral methods, allowing us to represent it as a large matrix. Our primary objective is to derive an estimate of \\( f \\) that minimizes the expected squared error \\( E[f - \\hat{f}]^2 \\), while adhering to specific smoothness constraints on \\( f \\).\n\nTo tackle this problem, we introduce innovative numerical strategies based on needlet techniques, which facilitate the resolution of these constrained optimization challenges. Notably, our methodology is designed to yield accurate estimates even in scenarios where the number of available observations \\( N \\) is significantly less than the dimensionality \\( M \\) of the space spanned by the rows of \\( A \\). The proposed framework comprises two key components: firstly, we utilize a sparse representation of functions through needlets, which effectively captures the essential features of \\( f \\). Secondly, we implement rapid iterative algorithms for addressing large-scale convex optimization problems, incorporating sparsity-preserving regularizers.\n\nThese advancements leverage insights from compressed sensing theory and incorporate recent findings regarding the convergence rates of the Alternating Direction Method of Multipliers (ADMM). Our results demonstrate the efficacy of needlet-based techniques in enhancing estimation accuracy in inverse problems, particularly under conditions of limited data availability. This work not only contributes to the theoretical understanding of estimation in inverse problems but also provides practical tools for applications in various scientific fields where such challenges are prevalent.",
        "ori-fast-z-score": 0.6831300510639733,
        "water-fast-z-score": 6.538530488755173,
        "rewrite-fast-z-score": 1.52127765851133
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A New Proof of Pappus s Theorem .\nAbstract:\nIn this note, we present an elementary proof for the following theorem due to Pappus:\nTheorem 1 (Pappus  theorem). Let $ABC$ be any triangle and let $D$ be its incenter. Then the three medians $AD$, $BE$, and $CF$ intersect at one point $K$ inside the triangle. \nProof. We first prove that the line segments $AB$, $BC$, and $CA$ are concurrent in one point $K$ by using only basic geometry. The idea is as follows. Consider the circle $C_1$ centered at $A$ with radius $AC$. Similarly consider circles $C_2$ and $C_3$ centered at $B$ and $C$ respectively with radii $BD$ and $CD$. Note that all these three circles have their centers on the same side of the triangle $ABC$. Now if we draw lines through $A $, $B $, and $C $ which are intersection points between the circumcircles of triangles $ABC$ and $A_1B_1C_1$, $A_2B_2C_2$, and $A_3B_3C_3$ then it can easily be seen that they will meet at one common point $K$.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A New Proof of Pappus s Theorem . Abstract : In this note , we present an elementary proof for the following theorem according to Pappus : Theorem 1 ( Pappus theorem ) .Let $ ABC $ be any triangle and take $ D $ be its incenter . Then the three medians $ AD $ , $ BE $ , and $ CF $ intersect at one point $ K $ inside the triangle .Proof . We first establish that the line segments $ AB $ , $ BC $ , and $ CA $ are simultaneous in one point $ K $ by using only basic geometry .The idea is as follows . Consider the circle $ C _ 1 $ oriented at $ A $ with diameter $ AC $ .Similarly consider circles $ C _ 2 $ and $ C _ 3 $ located at $ B $ and $ C $ respectively with radii $ BD $ and $ CD $ . Note that all these three circles have their centers on the same side of the circle $ ABC $ .Now if we draw lines through $ A $ , $ B $ , and $ C $ which are intersection points between the circumcircles of triangles $ ABC $ and $ A _ 1B _ 1C _ 1 $ , $ A _ 2B _ 2C _ 2 $ , and $ A _ 3B _ 3C _ 3 $ then it can easily be saw that they will meet at one common point $ K $ .",
        "rewrite_text": "**Title: A New Proof of Pappus's Theorem**\n\n**Abstract:** In this paper, we provide a novel and elementary proof of Pappus's Theorem, which asserts that for any triangle \\( ABC \\) with incenter \\( D \\), the three medians \\( AD \\), \\( BE \\), and \\( CF \\) converge at a single point \\( K \\) within the triangle. Our approach utilizes fundamental geometric principles to demonstrate the concurrency of these medians. We begin by establishing that the line segments \\( AB \\), \\( BC \\), and \\( CA \\) intersect at a common point \\( K \\). The proof is constructed around the concept of circles defined at each vertex of the triangle. Specifically, we define circle \\( C_1 \\) centered at \\( A \\) with diameter \\( AC \\), and similarly, we define circles \\( C_2 \\) and \\( C_3 \\) at vertices \\( B \\) and \\( C \\) with diameters \\( BD \\) and \\( CD \\), respectively. Notably, the centers of these circles lie on the same side of triangle \\( ABC \\). By drawing lines through points \\( A \\), \\( B \\), and \\( C \\) that correspond to the intersection points of the circumcircles of triangles \\( A_1B_1C_1 \\), \\( A_2B_2C_2 \\), and \\( A_3B_3C_3 \\), we observe that these lines converge at a single point \\( K \\). This geometric construction not only reinforces the validity of Pappus's Theorem but also highlights the elegance of elementary geometry in proving classical results. Our findings contribute to the ongoing discourse in geometric theory and provide a clearer understanding of the relationships between triangle centers and their properties.",
        "ori-fast-z-score": -0.13736056394868904,
        "water-fast-z-score": 3.265986323710904,
        "rewrite-fast-z-score": 0.45291081365783825
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Causal inference in longitudinal studies with history-restricted marginal structural models .\nAbstract:\nWe propose an approach to causal inference for longitudinal data based on the use of marginal structural models (MSMs) that are restricted by past treatment and covariate histories, which we refer to as  history-restricted  MSMs.  We show how these models can be used to estimate average effects over time periods during which treatments were not administered or outcomes were not observed. Our proposed method is illustrated using two examples involving missing data: one where there was no unobserved confounding but some subjects had incomplete outcome information; another example where both incomplete outcome information and unmeasured confounders affected estimation. The first example shows that our proposed method provides estimates similar to those obtained under complete follow-up when all relevant variables have been measured. In contrast, the second example illustrates situations where standard approaches may lead to biased results due to either incomplete outcome information or unmeasured confounding. Finally, we provide simulation evidence showing that our proposed method performs well even if the model assumptions underlying it do not hold exactly. \nKeywords: Marginal Structural Models, Longitudinal Data Analysis",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Causal inference in longitudinal studies with history - limited marginal structural models . Abstract : We suggest an way to causal inference for longitudinal data based on the using of marginal structural models ( MSMs ) that are restricted by past treatment and covariate histories , which we name to as history - limited MSMs .We see how these models can be used to estimate estimated effects over time periods during which therapy were not administered or outcomes were not observed . Our proposed approach is depicted using two examples involving missing data : one where there was no unobserved confounding but some subjects had incomplete result information ; another example where both incomplete result information and unmeasured confounders impacted estimation .The first instance shows that our proposed approach offers estimates similar to those achieved under complete follow - up when all relevant variables have been measured . In comparison , the second example illustrates situations where standard approaches may lead to biased results due to either incomplete result information or unmeasured confounding .Finally , we provide simulation evidence showing that our proposed approach performs good even if the model assumptions underlying it do not hold exactly . Keywords : Marginal Structural Models , Longitudinal Data Analysis",
        "rewrite_text": "**Title:** Causal Inference in Longitudinal Studies with History-Limited Marginal Structural Models\n\n**Abstract:** In this article, we introduce a novel methodology for causal inference in longitudinal studies by employing marginal structural models (MSMs) that are constrained by historical treatment and covariate data, which we refer to as history-limited MSMs. This approach allows for the estimation of causal effects during periods when treatment was not administered or when outcomes were not recorded. We illustrate the application of our method through two distinct case studies involving missing data. The first case involves a scenario where there is no unobserved confounding; however, some subjects have incomplete outcome information. Our findings indicate that the estimates derived from our method closely resemble those obtained under conditions of complete follow-up, provided that all relevant variables are accounted for. The second case presents a more complex situation where both incomplete outcome data and unmeasured confounders affect the estimation process. This example highlights the limitations of traditional methods, which may yield biased estimates in the presence of such challenges. To further validate our approach, we present simulation studies demonstrating that our method maintains robust performance, even when the underlying model assumptions are not strictly satisfied. Overall, our work contributes to the field of longitudinal data analysis by providing a reliable framework for causal inference that addresses the complexities introduced by missing data and confounding variables. \n\n**Keywords:** Marginal Structural Models, Longitudinal Data Analysis",
        "ori-fast-z-score": 1.2,
        "water-fast-z-score": 6.336944275050716,
        "rewrite-fast-z-score": 0.9622504486493763
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Quantum Hall ferromagnetism in graphene: a SU(4) bosonization approach .\nAbstract:\nWe study the quantum Hall effect (QHE) and its interplay with magnetism in monolayer graphene by using an effective low-energy model that takes into account both electron-electron interactions and disorder effects. We show how to derive this model starting from first principles, and we discuss some of its main properties. In particular, we find that at half-filling it exhibits two different phases depending on the strength of the Coulomb interaction between electrons. For weak coupling these are separated by a phase transition driven by spontaneous breaking of time-reversal symmetry; for strong coupling they correspond respectively to a conventional QHE state and to a novel fractionalized topological insulator characterized by chiral edge states. The latter is shown to be equivalent to a spin-1/2 Heisenberg antiferromagnet on the honeycomb lattice, which can thus be described within the framework of the so-called SU(4) bosonic representation.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Quantum Hall ferromagnetism in graphene : a SU ( 4 ) bosonization approach . Abstract : We research the quantum Hall impact ( QHE ) and its interplay with magnetism in monolayer graphene by using an efficient low - energy theory that takes into consideration both electron - ion interactions and disorder interactions .We see how to derive this description starting from first principles , and we investigate some of its primary characteristics . In particular , we find that at half - filling it displays two different phases depending on the strength of the Coulomb interaction between electrons .For weak bonding these are apart by a phase shift motivated by premature breaking of time - reversal symmetry ; for strong coupling they relate respectively to a traditional QHE state and to a novel fractionalized topological insulator characterized by chiral edge states . The latter is demonstrated to be analogous to a spin - 1 / 2 Heisenberg antiferromagnet on the honeycomb structure , which can thus be described within the framework of the so - called SU ( 4 ) bosonic representation .",
        "rewrite_text": "In this study, we investigate the quantum Hall effect (QHE) and its relationship with magnetism in monolayer graphene using a robust low-energy theoretical framework that incorporates both electron-ion interactions and disorder effects. We begin by deriving this theoretical model from first principles, allowing us to explore its fundamental properties. Our analysis reveals that at half-filling, the system exhibits two distinct phases that are influenced by the strength of the Coulomb interaction among electrons. In the regime of weak coupling, these phases are separated by a phase shift that arises from an early onset of time-reversal symmetry breaking. Conversely, in the strong coupling regime, the phases correspond to a conventional QHE state and a novel fractionalized topological insulator, which is characterized by the presence of chiral edge states. Notably, this fractionalized state is shown to have a close analogy to a spin-1/2 Heisenberg antiferromagnet situated on a honeycomb lattice, allowing for its description within the SU(4) bosonic representation framework. This work not only enhances our understanding of the interplay between quantum Hall ferromagnetism and electronic interactions in graphene but also opens avenues for exploring new quantum phases and phenomena in two-dimensional materials.",
        "ori-fast-z-score": -0.23570226039551587,
        "water-fast-z-score": 5.500933918218137,
        "rewrite-fast-z-score": 1.5096588248481377
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Vibrational instability, two-level systems and Boson peak in glasses .\nAbstract:\nWe study the vibrational density of states (VDOS) of glassy materials using molecular dynamics simulations with anharmonic interactions between particles. We show that the VDOS can be decomposed into three main contributions: one at low frequencies corresponding to acoustic phonons; another contribution centered around the Debye frequency which is associated with optical phonons; and finally a third contribution located at high frequencies whose origin lies on the presence of unstable modes. The latter are responsible for the appearance of the so-called boson peak in the VDOS. In addition we find that these unstable modes have a strong tendency to form clusters or groups of correlated particles. Finally, by analyzing the spatial distribution of the unstable modes it becomes evident that they tend to localize near regions where there exist structural defects such as icosahedral-like motifs. This localization effect leads us to propose a simple model based on the existence of two different types of sites within the materials: stable ones characterized by their ability to support vibrations over large energy ranges, while unstable sites only allow small-amplitude oscillations.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Vibrational instability , two - level systems and Boson peak in glasses . Abstract : We research the vibrational density of states ( VDOS ) of glassy materials utilizing molecular mechanics simulations with anharmonic interactions between particles .We see that the VDOS can be decomposed into three primary contributions : one at low frequencies corresponding to acoustic phonons ; another contribution located around the Debye frequency which is associated with optical phonons ; and eventually a third contribution located at high frequencies whose source lies on the presence of unstable modes . The latter are responsible for the appearance of the so - called boson peak in the VDOS .In addition we find that these unstable modes have a high ability to form clusters or bands of coupled atoms . Finally , by analyzing the spatial distribution of the unstable modes it becomes obvious that they tend to localize near sites where there exist functional errors such as icosahedral - like motifs .This localization effect makes us to propose a simple model based on the existence of two different kinds of places within the materials : strong ones defined by their potential to support vibrations over large energy intervals , while volatile places only facilitate tiny - frequency oscillations .",
        "rewrite_text": "In this study, we investigate the vibrational density of states (VDOS) of glassy materials through molecular mechanics simulations that incorporate anharmonic interactions among particles. Our findings reveal that the VDOS can be categorized into three main contributions. The first contribution arises at low frequencies and is attributed to acoustic phonons. The second contribution appears around the Debye frequency, which is linked to optical phonons. The third contribution, observed at high frequencies, is associated with unstable modes that give rise to the phenomenon known as the boson peak in the VDOS. \n\nWe further explore the characteristics of these unstable modes, discovering their propensity to form clusters or bands of coupled atoms. This clustering behavior suggests a complex interplay within the glassy structure that influences vibrational properties. Additionally, our analysis of the spatial distribution of unstable modes indicates a tendency for localization near regions with structural defects, such as icosahedral-like motifs. This localization suggests that certain areas within the material exhibit distinct vibrational behaviors.\n\nBased on our observations, we propose a simplified model that distinguishes between two types of regions within the glassy materials: \"strong\" regions, which are capable of supporting vibrations across a wide range of energy levels, and \"volatile\" regions, which only allow for low-frequency oscillations. This model provides a framework for understanding the vibrational dynamics in glasses and highlights the significance of structural heterogeneity in influencing the material's mechanical properties. Our research contributes to a deeper understanding of the vibrational characteristics of glassy materials and the underlying mechanisms that govern their behavior.",
        "ori-fast-z-score": -0.20203050891044214,
        "water-fast-z-score": 5.728715546977509,
        "rewrite-fast-z-score": 1.5583874449479593
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Magnetic Fingerprints of sub-100 nm Fe Nanodots .\nAbstract:\nWe report on the magnetic properties of Fe nanodots with diameters ranging between 10 and 100 nm, prepared by pulsed laser deposition (PLD) onto Si(100). The dots are characterized by X-ray photoelectron spectroscopy (XPS), transmission electron microscopy (TEM), scanning tunneling microscopy/spectroscopy (STM/STS), and vibrating sample magnetometry (VSM).\nThe XPS data show that the dots have an oxidized surface layer consisting mainly of iron oxide species such as FeO x , Fe 3 O 4 , or Fe 2 O 3 . This is confirmed by STM measurements which reveal a corrugated structure for all sizes investigated here. In addition to this common feature we find two different types of structures depending on dot size. For small dots (10-20 nm in diameter) we observe a dome-like shape while larger dots (40-60 nm in diameter) exhibit a flat topography. Both shapes can be explained by assuming either a core-shell model or a porous structure. We also found that the magnetic behavior depends strongly on the dot size. While large dots behave like bulk material they show superparamagnetism at room temperature when their diameter decreases below 40 nm.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Magnetic Fingerprints of sub - 100 nm Fe Nanodots . Abstract : We report on the magnetic properties of Fe nanodots with diameters ranging between 10 and 100 nm , made by pulsed laser deposition ( PLD ) onto Si ( 100 ) .The dots are characterized by X - ray photoelectron spectroscopy ( XPS ) , transfer electron microscopy ( TEM ) , scanning tunneling microscopy / spectroscopy ( STM / STS ) , and vibrating sample magnetometry ( VSM ) . The XPS measurements show that the stripes have an oxidized surface surface formed mainly of iron oxide species such as FeO x , Fe 3 O 4 , or Fe 2 O 3 .This is confirmed by STM observations which show a corrugated structure for all types examined here . In addition to this common characteristic we find two different kinds of structures depending on dot size .For small dots ( 10 - 20 nm in width ) we study a dome - like shape while larger dots ( 40 - 60 nm in width ) exhibit a straight topography . Both shapes can be described by assuming either a core - shell model or a porous structure .We additionally found that the magnetic behavior depends strongly on the dot size . While big dots act like bulk material they show superparamagnetism at room temperature when their thickness decreases below 40 nm .",
        "rewrite_text": "In this study, we investigate the magnetic characteristics of iron (Fe) nanodots with diameters ranging from 10 to 100 nm, which were fabricated using pulsed laser deposition (PLD) on silicon substrates (Si (100)). The nanodots were thoroughly analyzed using a variety of techniques, including X-ray photoelectron spectroscopy (XPS), transmission electron microscopy (TEM), scanning tunneling microscopy/spectroscopy (STM/STS), and vibrating sample magnetometry (VSM). Our XPS results reveal that the surface of the nanodots is predominantly oxidized, consisting mainly of iron oxide species such as FeO_x, Fe_3O_4, and Fe_2O_3. This finding is corroborated by STM observations, which indicate a corrugated surface structure across all examined samples. Notably, we observed two distinct structural morphologies that vary with the size of the nanodots. For smaller dots, specifically those measuring 10 to 20 nm in width, we identified a dome-like shape, whereas larger dots, ranging from 40 to 60 nm in width, displayed a more planar topography. These morphological variations can be effectively described using either a core-shell model or a porous structural framework. Furthermore, our investigation into the magnetic properties revealed a strong dependence on the size of the nanodots. Larger dots exhibit magnetic behavior akin to bulk materials; however, as the thickness of the dots decreases below 40 nm, they transition to a superparamagnetic state at room temperature. This research provides valuable insights into the size-dependent magnetic properties of Fe nanodots, contributing to the understanding of their potential applications in nanotechnology and magnetic materials.",
        "ori-fast-z-score": -1.1531133203941102,
        "water-fast-z-score": 3.753259453027346,
        "rewrite-fast-z-score": -0.19069251784911848
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Towards the Distribution of the Size of a Largest Planar Matching and Largest Planar Subgraph in Random Bipartite Graphs .\nAbstract:\nIn this work, we study the distribution of the size of largest planar matching (LPM) and largest planar subgraph (LPS) in random bipartite graphs with n vertices on each side. We show that for any fixed integer k > 0, there exists an absolute constant ck such that if G is a random bipartite graph with n vertices on each part satisfying cn < p = e(G)/n < 1 − cn then LPM(G) has at least k components w.h.p.. Moreover, when p = 1/2 + o(1), we prove that LPS(G) contains a cycle of length Ω(log log n). These results are proved by using the second moment method together with some new techniques developed here to deal with the dependencies between edges. As applications, these results imply that both LPM(G) and LPS(G) have many small cycles under certain conditions.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Towards the Distribution of the Size of a Largest Planar Matching and Largest Planar Subgraph in Random Bipartite Graphs . Abstract : In this research , we study the distribution of the size of largest planar matching ( LPM ) and largest planar subgraph ( LPS ) in random bipartite graphs with n edges on each side .We see that for any fixed integer k > 0 , there exists an absolute constant ck such that if G is a random bipartite graph with n edges on each portion satisfying cn < p = e ( G ) / p < 1 − cn then LPM ( G ) has at least n parts w . h . p . . Moreover , when n = 1 / 2 + o ( 1 ) , we prove that LPS ( G ) contains a cycle of height Ω ( log log n ) .These results are proved by using the second moment method together with some modern techniques introduced here to deal with the dependencies between edges . As applications , these results mean that both LPM ( G ) and LPS ( G ) have many small periods under certain conditions .",
        "rewrite_text": "**Title:** Towards the Distribution of the Size of the Largest Planar Matching and Largest Planar Subgraph in Random Bipartite Graphs\n\n**Abstract:** This study investigates the distribution characteristics of the largest planar matching (LPM) and the largest planar subgraph (LPS) within random bipartite graphs, specifically those containing n edges on each side. We establish that for any fixed integer k greater than zero, there exists a constant ck such that for a random bipartite graph G, where the edge density p satisfies the inequality cn < p = e(G) / p < 1 - cn, the LPM(G) will, with high probability, consist of at least n parts. Furthermore, we demonstrate that when n is equal to 1/2 plus a negligible term (o(1)), the LPS(G) is guaranteed to contain a cycle of height Ω(log log n). The proofs of these findings leverage the second moment method, complemented by contemporary techniques that address the dependencies among edges in the graph. The implications of our results suggest that both LPM(G) and LPS(G) exhibit numerous small periods under specific conditions, contributing valuable insights into the structural properties of random bipartite graphs. This research not only enhances our understanding of planar structures in graph theory but also opens avenues for further exploration of graph properties in probabilistic settings.",
        "ori-fast-z-score": 0.11867816581938533,
        "water-fast-z-score": 3.4416668087621747,
        "rewrite-fast-z-score": 0.329292779969071
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Evidence for a planetary companion around a nearby young star .\nAbstract:\nWe report the detection of periodic radial velocity variations in the spectrum of the K2V dwarf GJ 436, which are consistent with those expected for an orbiting planet. The period is 3.2 days and the semi-amplitude is about 30 m/sec. We also find evidence that this signal may be modulated on timescales longer than one year by another component whose mass we estimate to be at least 0.1 M⊕. This system has been extensively studied over many years as it lies close (5 pc) to our Sun but was not previously known to host any planets. It is therefore particularly interesting because its properties can now be compared directly with theoretical models of formation and evolution. \n \n Keywords: Planetary systems - Formation, Solar System\n\nIntroduction\n\nThe discovery of extrasolar planets has led to new insights into how planetary systems form and evolve. However, most exoplanets have been found using indirect techniques such as transit photometry or Doppler spectroscopy. These methods provide information only about the orbital parameters of the planet(s), while direct imaging provides additional constraints on their physical characteristics. In particular, high contrast imaging allows us to measure the masses of companions down to very low levels of flux ratio relative to their parent stars.\n\nIn recent years there has been significant progress towards achieving high-contrast imaging capabilities required to detect Earth-like planets around nearby stars. For example, the Gemini Planet Imager (GPI; Macintosh et al., 2014) , SPHERE (Beuzit et al., 2008) and SCExAO (Jovanovic et al., 2015) instruments will soon begin operation on 8-10 m class telescopes. These facilities offer unprecedented sensitivity and angular resolution, allowing them to probe regions closer to the central star where terrestrial planets are more likely to exist. \nHowever, these observatories operate under different conditions and use different technologies so it remains unclear what performance they will achieve once commissioned.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Evidence for a planetary companion around a neighboring young star . Abstract : We report the observation of periodic radial speed shifts in the spectrum of the K2V dwarf GJ 436 , which are compatible with those expected for an orbiting planet .The period is 3 . 2 days and the semi - frequency is about 30 m / sec . We additionally find proof that this signal might be modulated on timescales greater than one decade by another component whose mass we estimate to be at least 0 . 1 M⊕ .This system has been heavily explored over numerous years as it lies close ( 5 pc ) to our Sun but was not originally seen to host any planets . It is consequently especially interesting because its properties can now be contrasted directly with theoretical theories of formation and evolution .Keywords : Planetary systems - Formation , Solar System Introduction The observation of extrasolar stars has led to fresh insights into how planetary structures structure and evolve . However , most exoplanets have been detected use indirect approaches such as transit photometry or Doppler spectroscopy .These methods provide information only about the orbital characteristics of the planet ( s ) , while direct scanning provides additional constraints on their structural traits . In particular , large contrast imaging allows us to measure the masses of companions down to very low levels of flux ratio compared to their father planets .In recent years there has been significant progress towards reaching large - contrast imaging skills necessary to identify Earth - like planets around nearby planets . For instance , the Gemini Planet Imager ( GPI ; Macintosh et al . , 2014 ) , SPHERE ( Beuzit et al . , 2008 ) and SCExAO ( Jovanovic et al . , 2015 ) instruments will soon begin service on 8 - 10 m class telescopes .These systems allow extraordinary sensitivity and spatial resolution , allowing them to probe regions nearer to the main star where terrestrial planets are more likely to appear . However , these observatories run under various circumstances and use different technologies so it remains unsure what performance they will achieve once commissioned .",
        "rewrite_text": "**Title:** Evidence for a Planetary Companion Around a Nearby Young Star\n\n**Abstract:** In this study, we present compelling evidence for the presence of a planetary companion orbiting the K2V dwarf star GJ 436, located just 5 parsecs from our solar system. Our observations reveal periodic shifts in the radial velocity of the star's spectrum, consistent with the gravitational influence of an orbiting planet. The detected period of these shifts is approximately 3.2 days, with a semi-amplitude of around 30 m/s. Furthermore, we provide indications that this radial velocity signal may exhibit modulation over timescales exceeding one decade, suggesting the influence of an additional component with a minimum estimated mass of 0.1 Earth masses (M⊕). \n\nGJ 436 has been the subject of extensive research due to its proximity to the Sun, yet prior investigations had not identified any planetary bodies within this system. The discovery of this potential companion is particularly significant, as it allows for a direct comparison with existing theoretical models of planetary formation and evolution. The study of such nearby stars enhances our understanding of the diversity and characteristics of planetary systems.\n\nThe exploration of extrasolar planetary systems has yielded new insights into their formation and evolutionary processes. While most exoplanets have been detected through indirect methods such as transit photometry and Doppler spectroscopy, which primarily provide information about their orbital dynamics, direct imaging techniques offer additional constraints on the physical properties of these celestial bodies. Recent advancements in high-contrast imaging technologies, exemplified by instruments like the Gemini Planet Imager (GPI), SPHERE, and SCExAO, have significantly improved our ability to detect and characterize Earth-like planets in close proximity to their host stars. These instruments, operational on 8-10 meter class telescopes, promise exceptional sensitivity and spatial resolution, enabling the exploration of regions where terrestrial planets are more likely to exist. However, the varying operational conditions and technological approaches of these observatories raise questions about their performance once fully commissioned. This study contributes to the ongoing discourse on planetary system formation and the potential for discovering new worlds in our cosmic neighborhood.",
        "ori-fast-z-score": -0.16012815380508713,
        "water-fast-z-score": 8.594446819256738,
        "rewrite-fast-z-score": -1.0801234497346432
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Ages for illustrative field stars using gyrochronology: viability, limitations and errors .\nAbstract:\nWe present an analysis of the ages derived by applying the gyrochronological method to a sample of open clusters with known ages (from literature) in order to assess its reliability as well as possible systematics associated with it. We find that the age estimates are generally consistent within their uncertainties but there is some evidence for a small bias towards younger ages when compared against the true cluster ages. This bias may be due to the fact that we have used only one rotation period per star which does not take into account any scatter or spread in periods observed among coeval stars. The results presented here suggest that this technique can provide useful constraints on stellar ages if applied carefully taking into consideration all relevant sources of uncertainty. Keywords: Age determination, Open clusters, Rotation periods, Gyrochronology. 1 Introduction Stellar ages play a crucial role in many areas of astrophysics ranging from Galactic archaeology to exoplanet science. In particular, accurate ages are needed to understand how planets form and evolve over time. However, determining precise ages for individual stars remains challenging because they span several orders of magnitude in mass and luminosity and exhibit complex evolutionary histories. For example, while main-sequence turn-off ages can be determined accurately through photometric techniques such as fitting theoretical isochrones to colour-magnitude diagrams (CMDs), these methods cannot be easily extended beyond the red giant branch where the effects of convection become important. Furthermore, even though asteroseismic observations allow us to probe the interiors of evolved stars, the interpretation of the resulting data requires detailed modelling of the structure and evolution of each star individually. As a result, other approaches must be explored to determine ages for large samples of stars spanning different stages of evolution.\nGyrochronology provides another avenue for estimating ages based on the spin-down rate of magnetic activity cycles driven by dynamo processes operating at the base of the solar convective zone (Barnes 2003) . It has been shown that the Rossby number R o , defined as the ratio between the rotation period P rot and the convective overturning timescale",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Ages for illustrative field stars employing gyrochronology : viability , difficulties and errors . Abstract : We present an assessment of the years derived by using the gyrochronological method to a sample of open clusters with established periods ( from literature ) in order to examine its reliability as well as possible systematics associated with it .We see that the age values are widely consistent within their uncertainties but there is some evidence for a small prejudice regarding younger ages when compared against the true cluster ages . This bias could be due to the fact that we have used only one rotation cycle per star which does not take into consideration any scatter or spread in dates observed among coeval stars .The results presented here suggest that this methods can provide useful limitations on stellar ages if applied properly take into consideration all relevant sources of uncertainty . Keywords : Age determination , Open clusters , Rotation ages , Gyrochronology .1 Introduction Stellar ages serve a crucial role in multiple fields of astrophysics ranging from Galactic studies to exoplanet research . In particular , detailed years are needed to explain how planets form and evolve over time .However , determining exact periods for individual stars stays difficult because they span many orders of magnitude in mass and luminosity and possess intricate developmental histories . For instance , while main - sequence turn - off ages can be determined accurately through photometric strategies such as fitting theoretical isochrones to colour - magnitude diagrams ( CMDs ) , these procedures cannot be easily extended beyond the red dwarf branch where the effects of convection become crucial .Furthermore , even though asteroseismic measurements enable us to probe the interiors of evolved galaxies , the interpretation of the resulting data requires complete modelling of the composition and evolution of each star individually . As a result , other methods needs be investigated to identify ages for large specimens of stars spanning multiple stages of evolved .Gyrochronology offers another avenue for estimating ages relying on the spin - down frequency of magnetic activity periods coupled by dynamo mechanisms operating at the base of the solar convective zone ( Barnes 2003 ) . It has been shown that the Rossby number R o , defined as the proportion between the rotation history P rot and the convective overturning timescale",
        "rewrite_text": "**Title:** Ages for Illustrative Field Stars Using Gyrochronology: Viability, Challenges, and Errors\n\n**Abstract:** This study evaluates the ages derived from the gyrochronological method applied to a selection of open clusters with well-documented rotation periods from existing literature. Our goal is to assess the reliability of this method and identify any systematic biases that may arise. The analysis reveals that the age estimates are generally consistent within their uncertainties; however, we observe a potential bias towards younger ages when compared to the actual ages of the clusters. This discrepancy may stem from our reliance on a single rotation cycle per star, which does not account for the variability in rotation periods among coeval stars. The findings indicate that while gyrochronology can yield valuable constraints on stellar ages, it is essential to apply the method judiciously, taking into account all relevant uncertainties. \n\n**Keywords:** Age determination, Open clusters, Rotation ages, Gyrochronology.\n\n**1. Introduction:** Stellar age determination is fundamental to various astrophysical disciplines, including Galactic studies and exoplanet research. Accurate age estimates are crucial for understanding the formation and evolution of planets. However, determining precise ages for individual stars remains challenging due to their diverse mass and luminosity ranges, as well as their complex evolutionary histories. For example, while main-sequence turn-off ages can be reliably obtained through photometric techniques, such as fitting theoretical isochrones to color-magnitude diagrams (CMDs), these methods are less effective for red dwarfs, where convective effects are significant. Additionally, although asteroseismic measurements allow us to investigate the interiors of evolved stars, interpreting the resulting data necessitates comprehensive modeling of each star's composition and evolution. Consequently, alternative methods must be explored to ascertain ages for large samples of stars across various evolutionary stages. Gyrochronology presents a promising approach for age estimation, based on the relationship between the spin-down frequency of magnetic activity cycles and the dynamo processes occurring at the base of the solar convective zone (Barnes 2003). The Rossby number (Ro), defined as the ratio of the rotation period (Prot) to the convective overturning timescale, plays a critical role in this method.",
        "ori-fast-z-score": -0.07312724241271307,
        "water-fast-z-score": 9.286096001408149,
        "rewrite-fast-z-score": 0.7006490497453707
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Study in Depth of f0(1370) .\nAbstract:\nThe mass spectrum and decay properties of the scalar mesons are studied by using the QCD sum rule method with the light-cone distribution amplitudes (LCDAs). The masses, pole residues and coupling constants for the scalar mesons below 2 GeV are calculated systematically. In particular, we study the f0(1370), which is usually considered as an exotic state. We find that it can be naturally explained as a mixture of two conventional states, i.e., the lowest lying scalar glueball and the scalar quarkonium. Our results show that its mixing angle θ = −20° ± 5° , where the first error comes from the uncertainty of the LQCD data used to determine the parameters of LCDAs, while the second one arises from the uncertainties of the input parameters such as Borel parameter M2B and threshold s0B . \nI. INTRODUCTIO N\nIn recent years, there has been great interest in studying the low energy hadronic physics due to both theoretical and experimental reasons  1  -  4  . On the theory side, lattice quantum chromodynamics (LQCD) provides us with valuable information on the nonperturbative aspects of strong interactions  5  . However, at present most calculations have only focused on the ground-state hadrons  6  .\nOn the other hand, the experimental observations of many new excited states beyond the naive quark model predictions  7  -  9  provide further motivation to explore their underlying structures  10  -  12  . For example, the newly observed scalars around 1.4-1.7 GeV  13  -  16  may contain important information about the nature of confinement  17  -  20  . It should also be noted that some of these newly discovered resonances cannot be easily accommodated into the traditional qq picture  21  -  23  . Therefore, it becomes necessary to investigate them more carefully  24  -  26  .\nIn this work, we will use the QCD sum rules  27  -  29  to calculate the masses, pole residues and couplings of various scalar mesons below 2GeV systematically  30  . In particular, we focus our attention on the f 0 (1370), whose existence",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Study in Depth of f0 ( 1370 ) . Abstract : The mass spectrum and decay characteristics of the scalar mesons are studied by using the QCD sum control method with the light - cone function amplitudes ( LCDAs ) .The masses , pole residues and coupling constants for the scalar mesons below 2 GeV are measured systematically . In particular , we study the f0 ( 1370 ) , which is usually viewed as an exotic state .We see that it can be naturally presented as a mixture of two conventional states , i . e . , the lowest lying scalar glueball and the scalar quarkonium . Our results show that its mixing angle θ = −20° ± 5° , where the first error derives from the expectation of the LQCD measurements used to predict the variables of LCDAs , while the second one occurs from the uncertainties of the input parameters such as Borel parameter M2B and threshold s0B .I . INTRODUCTIO N In recent years , there has been much interest in investigating the small power hadronic physics owing to both theoretical and theoretical considerations 1 - 4 . On the theoretical side , lattice quantum chromodynamics ( LQCD ) presents us with useful details on the nonperturbative aspects of large interactions 5 .However , at current most calculations have only concentrated on the ground - state hadrons 6 . On the other hand , the empirical observations of several new excited states beyond the naive quark model predictions 7 - 9 provide further motivation to examine their underlying systems 10 - 12 .For instance , the newly observed scalars around 1 . 4 - 1 . 7 GeV 13 - 16 might include vital information about the nature of confinement 17 - 20 . It should additionally be mentioned that some of these newly discovered resonances cannot be easily accommodated into the usual qq picture 21 - 23 .Therefore , it becomes necessary to examine them more carefully 24 - 26 . In this study , we will use the QCD sum rules 27 - 29 to estimate the masses , pole residues and couplings of several scalar mesons below 2GeV systematically 30 .In particular , we focus our focus on the f 0 ( 1370 ) , whose existence",
        "rewrite_text": "**Title: A Comprehensive Investigation of f0(1370)**\n\n**Abstract:** This study delves into the mass spectrum and decay properties of scalar mesons, employing the QCD sum rules in conjunction with light-cone distribution amplitudes (LCDAs). We systematically measure the masses, pole residues, and coupling constants for scalar mesons with masses below 2 GeV, with a particular emphasis on the f0(1370), which is often regarded as an exotic state. Our analysis reveals that f0(1370) can be effectively described as a mixture of two conventional states: the lowest-lying scalar glueball and scalar quarkonium. The mixing angle is determined to be θ = -20° ± 5°, where the first uncertainty arises from the lattice quantum chromodynamics (LQCD) predictions utilized to estimate the LCDAs, while the second stems from uncertainties in the input parameters, including the Borel parameter M²B and the threshold s0B. \n\nIn recent years, there has been a growing interest in the exploration of low-energy hadronic physics, driven by both theoretical advancements and experimental discoveries. Theoretical frameworks, particularly LQCD, have provided valuable insights into the nonperturbative aspects of strong interactions. However, most existing calculations have primarily focused on ground-state hadrons. Concurrently, empirical evidence has surfaced regarding several new excited states that challenge the conventional quark model predictions, prompting further investigation into their underlying structures. Notably, newly identified scalar mesons in the mass range of 1.4 to 1.7 GeV may hold critical information regarding the nature of confinement in quantum chromodynamics. Some of these resonances defy straightforward classification within the traditional quark-antiquark framework, necessitating a more nuanced examination. This paper aims to utilize QCD sum rules to systematically estimate the properties of various scalar mesons below 2 GeV, with a focused analysis on the f0(1370) and its implications for our understanding of hadronic physics.",
        "ori-fast-z-score": -2.6680551940539985,
        "water-fast-z-score": 5.277790490704241,
        "rewrite-fast-z-score": -0.5303300858899106
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Unstable and Stable Galaxy Models .\nAbstract:\nWe present the results for two different models of galaxy formation, one stable model with no dark matter halos merging or growing in mass (the standard CDM scenario), and an unstable model where dark matter halos merge continuously to form larger structures.  We use these models to study how galaxies evolve over time as they are affected by mergers between their host dark matter halos.   The merger rate is higher at earlier times when there were more small halos available to merge into large ones.  In our unstable model we find that most massive galaxies have experienced several major mergers since z = 1 while less massive galaxies have had fewer mergers.    Our results show that the number density of brightest cluster galaxies has increased dramatically since z=1 due to mergers between clusters of galaxies.  This increase can be explained if brightest cluster galaxies grow through mergers rather than simply forming out of gas cooling within the hot intracluster medium.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Unstable and Stable Galaxy Models . Abstract : We present the results for two different models of galaxy formation , one stable model with no bright matter halos merging or developing in mass ( the standard CDM scenario ) , and an unstable scenario where bright matter halos merge consistently to form bigger structures .We use these models to study how clusters evolve over time as they are influenced by mergers between their host dark matter halos . The merger speed is higher at earlier times when there were more little halos accessible to unite into huge ones .In our unstable model we find that most large galaxies have experienced several main mergers since z = 1 while less massive galaxies have had fewer mergers . Our results show that the number density of brightest cluster clusters has increased significantly since z = 1 due to mergers between clusters of clusters .This increase can be understood if brightest cluster stars grow through mergers rather than merely emerging out of gas cooling within the hotter intracluster medium .",
        "rewrite_text": "In this article, titled \"Unstable and Stable Galaxy Models,\" we explore two distinct models of galaxy formation: a stable model characterized by the absence of merging bright matter halos and a dynamic unstable model where such halos consistently merge to create larger structures. Our research focuses on the evolutionary processes of galaxy clusters over time, particularly how these processes are influenced by the mergers of their associated dark matter halos. We observe that the rate of mergers is significantly higher in the earlier epochs of the universe, a period marked by a greater abundance of smaller halos that can coalesce into larger ones. \n\nIn our findings, we reveal that the majority of large galaxies have undergone multiple significant mergers since a redshift of z = 1, while their less massive counterparts have experienced fewer such events. This disparity in merger history highlights the complex nature of galaxy evolution. Furthermore, our analysis indicates a notable increase in the number density of brightest cluster galaxies since z = 1, which can be attributed to the merging of clusters rather than the traditional view of these galaxies forming solely from gas cooling within the hotter intracluster medium. \n\nThese results provide new insights into the mechanisms driving galaxy formation and evolution, suggesting that mergers play a crucial role in the growth of the most luminous galaxies in the universe. Our study contributes to the ongoing discourse in astrophysics regarding the interplay between dark matter dynamics and the visible structures we observe, offering a deeper understanding of the formation and evolution of galaxies across cosmic time.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.9819900360298925,
        "rewrite-fast-z-score": 0.6211495565912797
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Continuous interfaces with disorder: Even strong pinning is too weak in 2 dimensions .\nAbstract:\nWe study the effect of quenched randomness on interface depinning and roughening, focusing on the case where the interface is driven by an external force applied to all sites at once (i.e., not one site at a time). We find that even for very large values of the disorder strength there are no metastable states, i.e., the interface always moves when it is driven slowly enough. This behavior contrasts sharply with what happens in 1 dimension, where the presence of disorder leads to the existence of many metastable states separated by energy barriers which can be overcome only if the driving force exceeds some threshold value. \n \n In our model we consider a square lattice with periodic boundary conditions and nearest-neighbor interactions between spins. The spin variables take two values: +1 or -1. At each site, the local field acting on the spin points either up or down depending on its neighbors  orientations. If this field has magnitude larger than a certain threshold f0 then the corresponding spin flips. For simplicity, we assume that the threshold fields have equal magnitudes but different signs at neighboring sites; thus, the system evolves according to the following rules: \n \n \n \n Starting from any initial configuration, the dynamics eventually reaches a steady state characterized by a flat interface separating regions of opposite magnetization. When the interface is pinned, the average velocity vanishes as the temperature T goes to zero. However, below a critical temperature Tc(f) the interface becomes unpinned; in other words, the interface starts moving spontaneously whenever the driving force f is smaller than a certain threshold fc(T). \n \n We show numerically that the transition line Tc(f) (the curve above which the interface is pinned) exhibits scaling properties similar to those observed experimentally in magnetic systems such as Fe/Cr multilayers. Moreover, we also observe that the width w of the interface grows algebraically with time: w ~ talpha, where alpha = 0.5 within numerical accuracy.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Continuous interfaces with disorder : Even weak pinning is too weak in 2 dimensions . Abstract : We research the impact of quenched randomness on interface depinning and roughening , concentrating on the case where the interface is driven by an external force application to all locations at once ( i . e . , not one site at a time ) .We see that even for very huge values of the disorder strength there are no metastable states , i . e . , the interface always moving when it is driven slowly sufficiently . This phenomenon contrasts sharply with what comes in 1 dimension , where the presence of disorder contributes to the existence of several metastable states separated by energy barriers which can be overcome only if the driving pull approaches some threshold value .In our model we define a square lattice with periodic border conditions and nearest - neighbor interactions between spins . The spin parameters take two values : + 1 or - 1 .At each site , the local field acted on the spin points either up or down depending on its neighbors orientations . If this field has magnitude greater than a certain threshold f0 then the associated spinning flips .For simplicity , we suppose that the threshold fields have equal magnitudes but different signs at adjoining sites ; thus , the system evolves according to the following laws : Beginning from any initial setup , the dynamics eventually reaches a steady state characterized by a flat interface separating areas of opposite magnetization . When the interface is pinned , the average momentum vanishes as the temperature T falls to zero .However , below a critical temperature Tc ( f ) the interface becomes unpinned ; in other words , the interface begins changing spontaneously whenever the driving field v is smaller than a certain threshold fc ( T ) . We see numerically that the transition line Tc ( f ) ( the curve above which the interface is pinned ) exhibits scaling qualities similar to those observed experimentally in magnetic structures such as Fe / Cr multilayers .Moreover , we also observe that the width w of the interface increases algebraically with time : w ~ talpha , where alpha = 0 . 5 within numerical precision .",
        "rewrite_text": "**Title:** Continuous Interfaces with Disorder: Weak Pinning Insufficient in Two Dimensions\n\n**Abstract:** This study investigates the effects of quenched randomness on the depinning and roughening of interfaces, focusing specifically on scenarios where an external force is uniformly applied across the entire interface, rather than at individual sites. Our findings reveal that, even under conditions of significant disorder strength, the interface does not exhibit metastable states; it consistently moves when subjected to sufficiently slow driving forces. This behavior starkly contrasts with one-dimensional systems, where disorder can lead to the formation of multiple metastable states, each separated by energy barriers that can only be surmounted when the driving force exceeds a critical threshold. \n\nIn our model, we utilize a square lattice with periodic boundary conditions and nearest-neighbor interactions among spins, which can take values of +1 or -1. The local field acting on each spin is determined by the orientations of its neighboring spins, and a spin will flip if the local field exceeds a specific threshold, denoted as f0. For simplicity, we assume that the threshold fields at adjacent sites have equal magnitudes but opposite signs. As a result, the system evolves from any initial configuration towards a steady state characterized by a flat interface that separates regions of differing magnetization.\n\nWhen the interface is pinned, the average momentum approaches zero as the temperature (T) decreases to absolute zero. However, below a critical temperature Tc(f), the interface becomes unpinned, initiating spontaneous changes when the driving field v is below a certain threshold fc(T). Our numerical simulations indicate that the transition line Tc(f), which delineates the region where the interface remains pinned, exhibits scaling behavior akin to that observed in experimental magnetic structures, such as Fe/Cr multilayers. Additionally, we find that the width (w) of the interface grows algebraically over time, following the relationship w ~ t^α, with α approximating 0.5 within the limits of numerical precision.",
        "ori-fast-z-score": 0.7006490497453707,
        "water-fast-z-score": 7.295814941639528,
        "rewrite-fast-z-score": 2.226922466874271
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  What Gamma-Ray Bursts Explode Into .\nAbstract:\nGamma-ray bursts (GRBs) are the most energetic explosions in the universe, but their origin is still unknown. The leading model for GRB production involves two compact objects merging into one black hole and then exploding as a result of rapid accretion onto this black hole.  In this talk I will discuss how we can use gravitational waves to test this hypothesis by looking at the ringdown phase of these mergers. This is an exciting time for gravitational wave astronomy with Advanced LIGO/VIRGO starting taking data soon! \nI will also present some recent results on using gravitational waves to study neutron star mergers that may be related to short gamma ray bursts. Finally, I ll give you my personal perspective on what it takes to become a successful scientist today. My research interests include gravitational waves, astrophysics, cosmology, and particle physics. I m currently working at MIT Kavli Institute for Astrophysics and Space Research where I am part of the Gravitational Wave Cosmology Project.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : What Gamma - Ray Bursts Explode Into . Abstract : Gamma - ray bursts ( GRBs ) are the most intense bombs in the universe , but their source is still unclear .The leading theory for GRB development involves two compact entities combining into one dark hole and then exploding as a result of rapid accretion onto this black hole . In this talk I will explore how we can using gravitational waves to test this hypothesis by searching at the ringdown phase of these mergers .This is an exciting day for gravitational wave astronomy with Advanced LIGO / VIRGO starting took data soon ! I will also report some latest findings on uses gravity waves to study neutron star mergers that might be connected to short gamma ray waves .Finally , I ll offer you my personal perspective on what it takes to become a successful researcher today . My research interests cover gravitational waves , astrophysics , cosmology , and particle science .I m currently working at MIT Kavli Institute for Astrophysics and Space Research where I am member of the Gravitational Wave Cosmology Project .",
        "rewrite_text": "**Title: What Gamma-Ray Bursts Explode Into**\n\n**Abstract:** Gamma-ray bursts (GRBs) represent the most powerful explosions observed in the universe, yet their origins remain enigmatic. The prevailing hypothesis suggests that GRBs arise from the merger of two compact objects, leading to the formation of a black hole, which subsequently detonates due to rapid accretion processes. In this presentation, I will delve into the potential of utilizing gravitational waves to validate this theory by examining the ringdown phase that occurs during these cosmic mergers. The advent of Advanced LIGO and VIRGO has ushered in a new era for gravitational wave astronomy, with data collection commencing shortly. I will also share recent discoveries regarding the application of gravitational waves in studying neutron star mergers, which may be linked to the phenomenon of short gamma-ray bursts. Furthermore, I will provide my insights on the essential qualities and skills required to thrive as a researcher in today's scientific landscape. My research encompasses a broad spectrum of interests, including gravitational waves, astrophysics, cosmology, and particle physics. Currently, I am affiliated with the MIT Kavli Institute for Astrophysics and Space Research, where I contribute to the Gravitational Wave Cosmology Project. This talk aims to illuminate the intricate connections between gravitational waves and GRBs, while also reflecting on the evolving nature of research in these dynamic fields.",
        "ori-fast-z-score": -0.5555555555555556,
        "water-fast-z-score": 5.158920219515446,
        "rewrite-fast-z-score": 1.0101525445522108
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  ROXA: a new multi-frequency selected large sample of blazars with SDSS and 2dF optical spectroscopy .\nAbstract:\nWe present the results of an extensive spectroscopic survey for active galactic nuclei (AGN) in the southern hemisphere using the Sloan Digital Sky Survey (SDSS). The main goal is to select a complete flux-limited sample of radio-loud AGNs at redshifts z < 0.7, which we call ROXA (Radio Optical eXtragalactic Astronomy), by cross-correlating the FIRST 1.4 GHz radio source catalogue with the SDSS DR3 photometric database. We have obtained spectra for more than 1000 sources over an area of about 10,000 deg2. In this work we report on the selection criteria used to define our sample as well as its completeness and reliability. We also discuss some preliminary results concerning the properties of these objects such as their luminosity function and redshift distribution. This project has been funded by the European Space Agency under contract number 4000106131/16/NL/PA.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : ROXA : a new multi - frequency selected large sample of blazars with SDSS and 2dF optical spectroscopy . Abstract : We present the results of an extensive spectroscopic study for active galactic nuclei ( AGN ) in the southern hemisphere using the Sloan Digital Sky Survey ( SDSS ) .The main goal is to select a complete flux - limited sample of radio - loud AGNs at redshifts z < 0 . 7 , which we call ROXA ( Radio Optical eXtragalactic Astronomy ) , by cross - correlating the FIRST 1 . 4 GHz radio source catalogue with the SDSS DR3 photometric archive . We have achieved spectra for more than 1000 sources over an area of about 10 , 000 deg2 .In this research we publish on the selection procedures used to define our sample as well as its completeness and reliability . We especially consider some preliminary results relating the properties of these objects such as their luminosity function and redshift distribution .This project has been sponsored by the European Space Agency under contract number 4000106131 / 16 / NL / PA .",
        "rewrite_text": "We present a comprehensive spectroscopic analysis of active galactic nuclei (AGN) located in the southern hemisphere, utilizing data from the Sloan Digital Sky Survey (SDSS). Our primary objective is to establish a complete flux-limited sample of radio-loud AGNs with redshifts less than 0.7, which we have designated as ROXA (Radio Optical eXtragalactic Astronomy). This was accomplished through a systematic cross-correlation of the FIRST 1.4 GHz radio source catalog with the SDSS DR3 photometric database. As a result of this effort, we have successfully obtained spectra for over 1,000 sources across an expansive area of approximately 10,000 square degrees. \n\nIn this article, we detail the selection criteria and methodologies employed to construct our sample, emphasizing the completeness and reliability of our findings. Additionally, we present preliminary results that explore various characteristics of these AGNs, including their luminosity function and redshift distribution. These insights contribute to a deeper understanding of the population of radio-loud AGNs and their distribution in the universe. This research initiative has received funding from the European Space Agency under contract number 4000106131/16/NL/PA, highlighting the collaborative effort to advance our knowledge in the field of extragalactic astronomy. Our findings not only enhance the existing database of AGNs but also pave the way for future studies aimed at unraveling the complexities of these fascinating cosmic entities.",
        "ori-fast-z-score": 0.8819171036881969,
        "water-fast-z-score": 3.75,
        "rewrite-fast-z-score": 1.3480372031495529
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Thermal Stability of Metallic Single-Walled Carbon Nanotubes: An O(N) Tight-Binding Molecular Dynamics Simulation Study .\nAbstract:\nThe thermal stability and melting behavior of metallic single-wall carbon nanotubes (SWCNTs) are investigated by using an efficient tight-binding molecular dynamics simulation method with the Tersoff potential function. The results show that SWCNTs can be melted at temperatures ranging from 2000 to 3000 K, depending on their diameters. It is found that the melting temperature increases as the diameter decreases for both zigzag- and armchair-type tubes. In addition, it is shown that the melting process involves two stages in which the tube wall first becomes disordered followed by the formation of liquid-like structures inside the tube. Finally, we find that the melting point of SWCNTs depends strongly on the chirality index n-m. For example, the melting points of zigzag-type SWCNTs increase rapidly when n-m changes from 0 to 1 or -1. \n \n Keywords: Thermal stability, Melting behavior, Carbon nanotube",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Thermal Stability of Metallic Single - Walled Carbon Nanotubes : An O ( N ) Tight - Binding Molecular Dynamics Simulation Study . Abstract : The temperature stability and melting behavior of metallic single - wall carbon nanotubes ( SWCNTs ) are examined by using an efficient tight - binding molecular mechanics model method with the Tersoff potential function .The results show that SWCNTs can be melted at conditions ranging from 2000 to 3000 K , depending on their diameters . It is found that the melting temperature increases as the radius decreases for both zigzag - and armchair - type tubes .In addition , it is demonstrated that the melting cycle involves two stages in which the pipe floor first gets disordered preceded by the formation of liquid - like structures inside the tube . Finally , we find that the melting point of SWCNTs relies highly on the chirality index n - m . For instance , the melting points of zigzag - class SWCNTs increase quickly when n - m changes from 0 to 1 or - 1 .Keywords : Thermal stability , Melting behavior , Carbon nanotube",
        "rewrite_text": "**Title:** Thermal Stability of Metallic Single-Walled Carbon Nanotubes: An O(N) Tight-Binding Molecular Dynamics Simulation Study\n\n**Abstract:** This study investigates the thermal stability and melting characteristics of metallic single-walled carbon nanotubes (SWCNTs) through an advanced tight-binding molecular dynamics simulation utilizing the Tersoff potential. Our findings reveal that the melting temperatures of SWCNTs vary significantly, ranging from 2000 K to 3000 K, contingent upon the nanotubes' diameters. Notably, we observe a trend where the melting temperature increases as the radius of the nanotubes decreases, applicable to both zigzag and armchair configurations. The melting process is characterized by a two-stage mechanism: initially, the structural integrity of the nanotube's wall is compromised, leading to disorder, followed by the emergence of liquid-like phases within the nanotube. Furthermore, our results indicate a strong dependence of the melting point on the chirality index (n - m) of the nanotubes. Specifically, for zigzag-type SWCNTs, there is a pronounced increase in melting temperature as the chirality index transitions from 0 to 1 or -1. This research enhances our understanding of the thermal properties of metallic SWCNTs, providing valuable insights for their applications in nanotechnology and materials science. The implications of these findings extend to the design and utilization of carbon nanotubes in high-temperature environments, where thermal stability is crucial. \n\n**Keywords:** Thermal stability, Melting behavior, Carbon nanotubes",
        "ori-fast-z-score": 1.649915822768611,
        "water-fast-z-score": 4.798687035041354,
        "rewrite-fast-z-score": 1.7820842224272613
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  TRUFAS, a wavelet based algorithm for the rapid detection of planetary transits .\nAbstract:\nWe present TRUFAS (Transit Recognition Using Fast Approximate Signatures), an efficient and robust transit search pipeline that uses fast approximate signatures to detect periodic signals in time series data. The method is particularly well suited for detecting short duration transits with high signal-to-noise ratio. We demonstrate its performance on simulated light curves generated by the Exoplanet Transit Database as well as real Kepler light curves. Our results show that TRUFAS can achieve higher efficiency than other algorithms while maintaining low false positive rates. \n \n Keywords: Transiting planet, Wavelets, Time-series analysis, False positives reduction, Planetary system characterization \n \n \n \n 1 Introduction \n \n Planets are detected indirectly through their gravitational effects upon their host stars. These effects include changes in stellar radius or luminosity caused by the passage of planets across the line-of-sight between the star and Earth. This phenomenon is known as a transit event. In order to characterize exoplanet systems it is necessary to identify these events efficiently and accurately. However, this task has been made more difficult due to the large number of false positives produced by systematic noise sources such as instrumental artifacts and astrophysical phenomena like eclipsing binaries and pulsating stars. \n \n To date there have been several methods developed specifically for identifying transit-like features within astronomical time series data. Some examples include: Box Least Squares (BLS)  1  , BLS+  2  , TrES  3  , TAP  4  , EXOTRANS  5  . While each of these techniques performs reasonably well under certain conditions they all suffer from one common drawback; they require significant computational resources when searching for multiple transit candidates simultaneously. For example, the most widely used technique, Box Least Squares, requires O(N3) operations where N is the length of the time series being analyzed  6  . As a result, many of these techniques cannot be applied directly to current and future surveys which will produce enormous amounts of data  7  8  9  . \n \n In recent years wavelet transforms have become increasingly popular for analyzing astronomical time series data  10",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : TRUFAS , a wavelet based algorithm for the quick detection of planetary transits . Abstract : We present TRUFAS ( Transit Recognition Using Fast Approximate Signatures ) , an efficient and strong transit search pipeline that using fast exact signatures to identify continuous patterns in time series information .The method is especially good suitable for detecting short length transits with high signal - to - noise ratio . We test its effectiveness on simulated light curves generated by the Exoplanet Transit Database as also as real Kepler light surfaces .Our results show that TRUFAS can attain better productivity than other methods while maintaining low false negative rates . Keywords : Transiting planet , Wavelets , Time - series investigation , False positives reduction , Planetary network detection 1 Introduction Planets are detected indirectly through their gravitational impacts upon their host stars .These effects include changes in stellar radius or luminosity caused by the travel of planets across the line - of - view between the star and Earth . This phenomenon is known as a transit event .In order to characterize exoplanet systems it is required to identify these changes easily and correctly . However , this job has been turned more challenging due to the huge amount of false positives created by widespread sound sources such as instrumental artifacts and astrophysical processes like eclipsing binaries and pulsating planets .To date there have been numerous mechanisms developed specifically for determining transit - like features within astronomical date cycle information . Some examples include : Box Least Squares ( BLS ) 1 , BLS + 2 , TrES 3 , TAP 4 , EXOTRANS 5 .While each of these procedures works relatively well under certain conditions they all suffer from one common drawback ; they demand significant computational resources when looking for multiple transit alternatives independently . For instance , the most commonly used technique , Box Least Squares , requires O ( N3 ) operations where N is the length of the time series being evaluated 6 .As a result , many of these procedures cannot be applied directly to recent and future surveys which will generate immense sums of evidence 7 8 9 . In recent generations wavelet transforms have developed increasingly popular for studying astronomical date sequence data 10",
        "rewrite_text": "**Title:** TRUFAS: A Wavelet-Based Algorithm for Rapid Detection of Planetary Transits\n\n**Abstract:** In this study, we introduce TRUFAS (Transit Recognition Using Fast Approximate Signatures), a robust and efficient pipeline designed for the swift identification of planetary transits. This innovative method leverages fast exact signatures to detect continuous patterns within time series data, making it particularly adept at recognizing short-duration transits characterized by a high signal-to-noise ratio. We evaluate the performance of TRUFAS using both simulated light curves sourced from the Exoplanet Transit Database and actual light curves from the Kepler mission. Our findings demonstrate that TRUFAS significantly enhances detection productivity compared to existing methodologies while maintaining a low rate of false negatives. \n\nThe detection of exoplanets primarily relies on observing their gravitational effects on host stars, which manifest as variations in stellar radius or luminosity during transit events—when a planet crosses the line of sight between the star and Earth. Accurately identifying these subtle changes is crucial for characterizing exoplanet systems. However, this task is complicated by the prevalence of false positives arising from various noise sources, including instrumental artifacts and astrophysical phenomena such as eclipsing binaries and pulsating stars. \n\nNumerous algorithms have been developed to detect transit-like features in astronomical time series data, including Box Least Squares (BLS), BLS+, TrES, TAP, and EXOTRANS. While these methods perform adequately under specific conditions, they share a significant limitation: they require substantial computational resources, particularly when searching for multiple transit candidates independently. For example, the widely used Box Least Squares method necessitates O(N³) operations, where N represents the length of the time series being analyzed. Consequently, many of these techniques are not feasible for application to contemporary and future surveys that will produce vast amounts of data. In recent years, wavelet transforms have gained traction as a powerful tool for analyzing astronomical time series data, offering a promising avenue for improving transit detection efficiency. \n\n**Keywords:** Transiting planet, Wavelets, Time-series analysis, False positive reduction, Planetary detection network.",
        "ori-fast-z-score": -2.9651150929518515,
        "water-fast-z-score": 6.974502000925911,
        "rewrite-fast-z-score": -2.708381313869479
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Unveiling the broad band X-ray continuum and iron line complex in Mkr 841 .\nAbstract:\nWe present an analysis of archival XMM-Newton data for the Seyfert 1 galaxy, Mkr841 (NGC 4151). We find that the soft excess emission is well described by a blackbody component with kT = 0.16 keV and luminosity LBB ~ 1043 erg s-1. The hard X-ray spectrum can be fitted either by a power law or Compton reflection model. In both cases we detect strong relativistic Fe Kα lines at 6.4-6.7 keV which are broadened to FWHM ~ 1000 km/sec. These results suggest that there may exist two distinct regions where the accretion disk interacts with the central supermassive black hole. One region produces the soft excess via thermal reprocessing while another one gives rise to the hard X-ray emission through non-thermal processes such as inverse Compton scattering and/or Compton reflection.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Unveiling the broad band X - ray continuum and iron line complex in Mkr 841 . Abstract : We report an assessment of archival XMM - Newton data for the Seyfert 1 galaxy , Mkr841 ( NGC 4151 ) .We see that the soft excess emission is well described by a blackbody element with kT = 0 . 16 keV and luminosity LBB ~ 1043 erg s - 1 . The soft X - ray spectrum can be fit either by a power law or Compton absorption theory .In both cases we find strong relativistic Fe Kα bands at 6 . 4 - 6 . 7 keV which are broadened to FWHM ~ 1000 kilometers / sec . These data suggest that there may arise two separate areas where the accretion disk interacts with the main supermassive black hole .One region releases the soft excess via thermal reprocessing while another one takes rise to the hard X - ray radiation through non - thermal processes such as inverse Compton absorption and / or Compton reflection .",
        "rewrite_text": "We present a comprehensive analysis of archival XMM-Newton observations of the Seyfert 1 galaxy Mkr 841 (NGC 4151), focusing on the broadband X-ray continuum and the iron line complex. Our findings indicate that the soft excess emission in the X-ray spectrum can be effectively modeled by a blackbody component with a temperature of kT = 0.16 keV and a luminosity of approximately LBB ~ 10^43 erg s^-1. The soft X-ray spectrum can be accurately described using either a power law or the Compton absorption model. In both modeling approaches, we observe prominent relativistic Fe Kα lines within the energy range of 6.4 to 6.7 keV, exhibiting significant broadening with a full width at half maximum (FWHM) of around 1000 kilometers per second. These observations imply the existence of two distinct regions in the vicinity of the accretion disk that interact with the central supermassive black hole. One region is responsible for generating the soft excess through thermal reprocessing processes, while the other contributes to the hard X-ray emission via non-thermal mechanisms, including inverse Compton scattering and/or Compton reflection. This duality in emission processes highlights the complex nature of the accretion dynamics in Mkr 841 and provides valuable insights into the physical conditions surrounding the supermassive black hole. Our results contribute to the broader understanding of X-ray emissions in active galactic nuclei and the intricate interplay between various emission mechanisms in the vicinity of supermassive black holes.",
        "ori-fast-z-score": -1.5,
        "water-fast-z-score": 4.0,
        "rewrite-fast-z-score": -0.38851434494290565
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  XMM-Newton X-ray Observations of the Wolf-Rayet Binary System WR 147 .\nAbstract:\nWe present new XMM-Newton observations of the Wolf Rayet binary system WR147, which is one of only two known systems with an O-type companion star and a WN6-7h primary component (the other being WR20a). The data were obtained in December 2004 during revolution number 1253-1255 using all three EPIC cameras on board XMM-Newton. We have analyzed these data to search for periodicities in both light curves as well as line profiles. No significant periodicity was found in either the light curve or line profile analysis. However we find that there are differences between the line profiles observed by Chandra and those seen here. These differences may be due to orbital motion within this highly eccentric system. This work has been supported by NASA grant NAG5-10842. Keywords: Wolf-Rayet stars; binaries; X-rays; XMM-Newton",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : XMM - Newton X - ray Observations of the Wolf - Rayet Binary System WR 147 . Abstract : We report new XMM - Newton discoveries of the Wolf Rayet binary system WR147 , which is one of only two discovered systems with an O - class companion star and a WN6 - 7h primary component ( the other being WR20a ) .The data were obtained in December 2004 during revolution number 1253 - 1255 utilizing all three EPIC sensors on board XMM - Newton . We have analyzed these information to search for periodicities in both light curves as well as line profiles .No considerable periodicity was seen in either the light curve or line profile analysis . However we find that there are variations between the line profiles observed by Chandra and those viewed here .These changes may be due to orbital movement within this strongly eccentric system . This research has been supported by NASA grant NAG5 - 10842 .Keywords : Wolf - Rayet stars ; binaries ; X - rays ; XMM - Newton",
        "rewrite_text": "Title: XMM-Newton X-ray Observations of the Wolf-Rayet Binary System WR 147\n\nAbstract: In this study, we present new findings from XMM-Newton observations of the Wolf-Rayet binary system WR 147, notable for being one of only two known systems featuring an O-class companion star alongside a WN6-7h primary component, the other being WR20a. The observational data were collected in December 2004 during the satellite's revolution numbers 1253 to 1255, utilizing all three European Photon Imaging Cameras (EPIC) onboard XMM-Newton. Our analysis focused on identifying periodicities within the light curves and spectral line profiles of the system. Despite thorough examination, we did not detect any significant periodicity in either the light curves or the line profile data. Nevertheless, we observed discrepancies between the line profiles recorded by Chandra and those obtained in our study, suggesting potential variations linked to the orbital dynamics of this highly eccentric binary system. These findings contribute to our understanding of the complex interactions within Wolf-Rayet binaries and highlight the importance of multi-wavelength observations in astrophysical research. This work was supported by NASA grant NAG5-10842. \n\nKeywords: Wolf-Rayet stars; binary systems; X-ray astronomy; XMM-Newton.",
        "ori-fast-z-score": -0.254000254000381,
        "water-fast-z-score": 4.157609203101499,
        "rewrite-fast-z-score": -0.3418817293789138
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Detection of Crab Giant Pulses Using the Mileura Widefield Array Low Frequency Demonstrator Field Prototype System .\nAbstract:\nWe report on the detection and analysis of giant pulses (GPs) from the Crab pulsar using data recorded with the MWA-LFD prototype system at frequencies between 80 and 240 MHz. The GPs are detected by searching for excess power in short time intervals, which is then compared to that expected from background noise fluctuations. We find that the number of GP candidates increases as we lower the frequency below 150 MHz. This increase can be explained if the emission mechanism responsible for producing GPs has an intrinsic spectral index steeper than -1.5. In addition, we show that the pulse width distribution changes significantly when going from high to low radio frequencies. Finally, we present results showing how the flux density of individual GPs varies across our observing bandwidth. These observations provide new insights into the physical processes occurring within the magnetosphere of this energetic pulsar. Keywords: Radio astronomy, Pulsar",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Detection of Crab Giant Pulses Using the Mileura Widefield Array Low Frequency Demonstrator Field Prototype System . Abstract : We report on the sensing and assessment of giant pulses ( GPs ) from the Crab pulsar utilizing information recorded with the MWA - LFD experimental system at speeds between 80 and 240 MHz .The GPs are detected by searching for excess energy in brief time periods , which is then compared to that expected from background noise fluctuations . We see that the proportion of GP applicants grows as we lower the frequency below 150 MHz .This increase can be understood if the emission mechanism responsible for producing GPs has an intrinsic spectral index steeper than - 1 . 5 . In addition , we find that the signal width distribution changes significantly when going from high to low radio frequencies .Finally , we present results confirming how the flux concentration of different GPs vary across our observing bandwidth . These measurements give novel knowledge into the physical processes arising within the magnetosphere of this energetic pulsar .Keywords: Radio astronomy, Pulsar",
        "rewrite_text": "We present a comprehensive study on the detection and analysis of giant pulses (GPs) emitted by the Crab pulsar, utilizing data collected from the Mileura Widefield Array Low Frequency Demonstrator (MWA-LFD) experimental system, operating within the frequency range of 80 to 240 MHz. Our methodology involves identifying GPs by searching for periods of excess energy that exceed the expected fluctuations of background noise. Notably, we observe an increase in the number of GP candidates as the frequency decreases below 150 MHz. This trend suggests that the emission mechanism responsible for generating GPs likely possesses an intrinsic spectral index steeper than -1.5. Furthermore, our analysis reveals a significant alteration in the distribution of signal widths as we transition from higher to lower radio frequencies. This finding indicates that the characteristics of GPs are frequency-dependent, providing insights into their underlying emission processes. Additionally, we present results that illustrate how the flux density of various GPs varies across the observing bandwidth, highlighting the complexity of the pulsar's emission behavior. These findings contribute valuable knowledge to our understanding of the physical processes occurring within the magnetosphere of this highly energetic pulsar, enhancing our comprehension of pulsar emissions in the context of radio astronomy. Our research underscores the importance of low-frequency observations in unraveling the intricate dynamics of pulsar activity and opens new avenues for future studies in this field. \n\nKeywords: Radio astronomy, Pulsar.",
        "ori-fast-z-score": -0.3464101615137754,
        "water-fast-z-score": 6.039910552360811,
        "rewrite-fast-z-score": -0.5827715174143585
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Mid-Infrared Emission of M87 .\nAbstract:\nWe present the mid-infrared (MIR) spectrum of the central region in the Virgo galaxy cluster, obtained with Spitzer/IRS at high spatial resolution. The MIR emission is dominated by polycyclic aromatic hydrocarbon features and silicate absorption bands that are spatially extended over several kpc scales along the minor axis of the galaxy. We find evidence for an additional component to this emission which peaks on top of the nucleus within 0.5 arcsec (0.1 pc). This nuclear source has been previously detected as a compact radio core and near-infrared continuum source but not seen before in the infrared spectral domain. It shows strong PAH emission lines and weak fine-structure line emission. In addition we detect a number of other sources in the field-of-view including two bright starburst galaxies located about 10 arcmin away from M87. These results show that the MIR properties of active galactic nuclei can be studied even if they reside in crowded fields such as those found near the center of rich clusters like Virgo.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Mid - Infrared Emission of M87 . Abstract : We report the mid - infrared ( MIR ) spectrum of the central region in the Virgo galaxy cluster , obtained with Spitzer / IRS at high spatial resolution .The MIR emission is dominated by polycyclic aromatic hydrocarbon characteristics and silicate emission bands that are spatially extended over numerous kpc scales along the minor axis of the galaxy . We see evidence for an additional element to this emission which peaks on top of the nucleus within 0 . 5 arcsec ( 0 . 1 pc ) .This nuclear source has been previously observed as a compact radio core and near - infrared continuum source but not seen before in the infrared spectral domain . It displays strong PAH emission lines and weak fine - structure line emission .In addition we find a number of other sources in the field - of - view including two faint starburst clusters situated about 10 arcmin away from M87 . These data reveal that the MIR properties of active galactic nuclei can be examined even if they live in busy fields such as those contained near the center of rich clusters like Virgo .",
        "rewrite_text": "We present a comprehensive analysis of the mid-infrared (MIR) spectrum from the central region of the Virgo galaxy cluster, acquired using the Spitzer Infrared Spectrograph (IRS) with high spatial resolution. Our findings indicate that the MIR emission is primarily characterized by the presence of polycyclic aromatic hydrocarbons (PAHs) and silicate emission bands, which extend over several kiloparsecs along the minor axis of the galaxy. Notably, we identify a distinct nuclear source of emission that peaks within 0.5 arcseconds (approximately 0.1 parsecs) of the galaxy's nucleus. This nuclear feature has been previously recognized as a compact radio core and a near-infrared continuum source, but it has not been detected in the infrared spectral range until now. The nuclear source exhibits pronounced PAH emission lines alongside weaker fine-structure line emissions. Furthermore, our observations reveal additional sources within the field of view, including two faint starburst clusters located roughly 10 arcminutes from M87. These results underscore the potential for investigating the MIR characteristics of active galactic nuclei, even in densely populated environments such as the core of rich galaxy clusters like Virgo. This study enhances our understanding of the complex interplay between various emission mechanisms in active galaxies and contributes to the broader knowledge of galactic evolution in crowded cosmic settings.",
        "ori-fast-z-score": 0.7592566023652966,
        "water-fast-z-score": 4.960307567358167,
        "rewrite-fast-z-score": 1.5689290811054724
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  X-ray detection of the substellar twin 2MASS J11011926-7732383 AB .\nAbstract:\nWe report on the X-ray properties of the young, nearby (d = 11 pc), low-mass binary system 2MASS J1101-2677AB discovered by Burgasser et al. (2007) . The primary component is an M8 dwarf with T eff ~ 2600 K and log g ~ 5.0 while its companion has been classified as an L5 brown dwarf with T eff ~ 1400K and log g ~ 4.2. We observed this target for about 50 ks using Chandra ACIS-S in order to search for evidence of coronal activity associated with either or both components. No significant emission was detected at the position of the source down to a 3-sigma upper limit of 1 x 10^28 erg s-1 cm-2 . This non-detection suggests that if there are active regions present they must be small and/or cool compared to those found on more massive stars. In addition we find no evidence of flaring behavior during our observation which constrains any possible magnetic field strength to less than 100 G.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : X - ray observations of the substellar twin 2MASS J11011926 - 7732383 AB . Abstract : We report on the X - ray characteristics of the young , neighboring ( d = 11 pc ) , low - weight binary system 2MASS J1101 - 2677AB discovered by Burgasser et al .( 2007 ) . The main component is an M8 dwarf with T eff ~ 2600 K and log f ~ 5 . 0 while its companion has been classified as an L5 brown dwarf with T eff ~ 1400K and log f ~ 4 . 2 .We observed this target for about 50 ks using Chandra ACIS - S in order to search for indication of coronal interaction associated with either or both components . No noticeable emitted was seen at the position of the source down to a 3 - sigma upper maximum of 1 x 10 ^ 28 erg s - 1 cm - 2 .This non - observation suggests that if there are active regions present they must be small and / or cold relative to those observed on more massive stars . In addition we find no evidence of flaring behavior during our observation which constrains any likely magnetic force weakness to fewer than 100 G .",
        "rewrite_text": "We present an in-depth analysis of the X-ray properties of the young, nearby binary system 2MASS J11011926-7732383 AB, located at a distance of 11 parsecs. This low-mass binary, identified by Burgasser et al. (2007), consists of two components: the primary is an M8 dwarf with an effective temperature of approximately 2600 K and a log f value of around 5.0, while the secondary is classified as an L5 brown dwarf, exhibiting an effective temperature of about 1400 K and a log f of roughly 4.2. To investigate potential coronal activity associated with either component, we conducted observations using the Chandra ACIS-S for approximately 50 kiloseconds. Our findings revealed no significant X-ray emission at the source's location, with a 3-sigma upper limit established at 1 x 10^28 erg s^-1 cm^-2. This lack of detectable X-ray activity implies that if active regions exist on either component, they are likely to be small and/or cooler than those typically found on more massive stars. Furthermore, our observations did not capture any flaring events, which allows us to place constraints on the magnetic field strength, suggesting it is weaker than 100 G. These results contribute to our understanding of the magnetic activity and X-ray emissions in low-mass stellar and substellar objects, highlighting the differences in coronal behavior compared to their more massive counterparts.",
        "ori-fast-z-score": -1.5215349135496974,
        "water-fast-z-score": 4.184914994777494,
        "rewrite-fast-z-score": -0.30151134457776363
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Nilpotent symmetry invariance in the superfield formulation: the (non-)Abelian 1-form gauge theories .\nAbstract:\nWe present an explicit construction of nilpotent supersymmetry transformations for Abelian and non-Abelian 1-form gauge fields, which are formulated as components of chiral N=1 superfields. The corresponding action is invariant under these transformations up to total derivatives. We show that this theory can be obtained by dimensional reduction of 4D N=2 SYM down to 3D. In addition we discuss how our results generalize to higher dimensions. Introduction: Supersymmetric field theories have been studied extensively over last decades due to their remarkable properties such as unitarity and renormalizability  1, 2  . One of the most important features of supersymmetric models is the presence of fermionic partners of bosonic degrees of freedom with opposite statistics. These new particles lead to additional symmetries called supersymmetries. It has been shown that all known fundamental interactions including gravity can be described within the framework of supersymmetric quantum mechanics  3  .\nIn recent years there was renewed interest in supersymmetric field theories motivated by developments in string/M-theory  4  , where they appear naturally on various brane configurations  5  . Moreover, it turns out that many interesting phenomena observed at high energies may find natural explanation within the context of supersymmetric extensions of Standard Model  6  . For example, supersymmetry provides elegant solution to hierarchy problem  7, 8  or offers possible candidates for dark matter  9  .\nThe simplest supersymmetric extension of Standard Model contains only one extra spin-1/2 particle -the gravitino  10  . However, more complicated versions involving several spin-1/2 fields exist  11  . A particularly interesting class of supersymmetric models involves so-called extended supersymmetry  12  . This includes N = 2 supersymmetry  13  , which appears in M-theory compactified on Calabi-Yau manifolds  14  , and its further generalization to N = 4  15  . Another interesting case corresponds to N = 1 supersymmetry  16  , which arises when D-branes wrap cycles of internal space  17  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Nilpotent symmetry invariance in the superfield implementation : the ( non - ) Abelian 1 - form gauge theories . Abstract : We introduce an explicit construction of nilpotent supersymmetry transformations for Abelian and non - Abelian 1 - form gauge fields , which are formulated as components of chiral N = 1 superfields .The corresponding action is invariant under these transformations up to complete derivatives . We see that this theory can be obtained by dimensional reduction of 4D N = 2 SYM down to 3D .In addition we talk how our findings generalize to higher dimensions . Introduction : Supersymmetric field theories have been studied frequently over last decades owing to their extraordinary properties such as unitarity and renormalizability 1 , 2 .One of the most important features of supersymmetric theories is the presence of fermionic partners of bosonic degrees of liberty with opposite statistics . These new interactions result to extra symmetries called supersymmetries .It has been shown that all known fundamental interactions including gravity can be described within the framework of supersymmetric quantum mechanics 3 . In recent years there was renewed interest in supersymmetric field theories motivated by developments in string / M - theory 4 , where they appear naturally on various brane configurations 5 .Moreover , it turns out that several interesting phenomena observed at high energies may reach natural explanation within the context of supersymmetric extensions of Standard Model 6 . For instance , supersymmetry presents elegant solve to hierarchy problem 7 , 8 or offers possible candidates for black matter 9 .The simplest supersymmetric extension of Standard Model includes only one extra spin - 1 / 2 particle - the gravitino 10 . However , more complicated versions involving several spin - 1 / 2 fields exist 11 .A notably important family of supersymmetric theories involves so - called extended supersymmetry 12 . This contains N = 2 supersymmetry 13 , which appears in M - theory compactified on Calabi - Yau manifolds 14 , and its further generalization to N = 4 15 .Another important case corresponds to N = 1 supersymmetry 16 , which occurs when D - branes wrap cycles of internal space 17 .",
        "rewrite_text": "**Title:** Nilpotent Symmetry Invariance in the Superfield Implementation: The (Non-)Abelian 1-Form Gauge Theories\n\n**Abstract:** In this article, we present a detailed construction of nilpotent supersymmetry transformations applicable to both Abelian and non-Abelian 1-form gauge fields, which are expressed as components of chiral N = 1 superfields. The action derived from this framework is shown to be invariant under these transformations, modulo total derivatives. Notably, we demonstrate that this theoretical model can be derived through the dimensional reduction of four-dimensional N = 2 supersymmetric Yang-Mills (SYM) theory down to three dimensions. Furthermore, we explore the implications of our results for higher-dimensional theories, indicating a broader applicability of our findings. \n\nThe study of supersymmetric field theories has garnered significant attention over the past few decades due to their remarkable characteristics, such as unitarity and renormalizability. A key aspect of these theories is the existence of fermionic partners corresponding to bosonic degrees of freedom, which possess opposite statistics. This relationship gives rise to additional symmetries known as supersymmetries. It has been established that all known fundamental interactions, including gravity, can be framed within the context of supersymmetric quantum mechanics. Recent advancements in string and M-theory have reignited interest in supersymmetric field theories, particularly as they naturally emerge in various brane configurations. Moreover, several intriguing phenomena observed at high energies find compelling explanations within supersymmetric extensions of the Standard Model. For example, supersymmetry offers elegant solutions to the hierarchy problem and proposes viable candidates for dark matter. The simplest supersymmetric extension of the Standard Model introduces a single additional spin-1/2 particle, the gravitino, while more complex models may involve multiple spin-1/2 fields. Extended supersymmetry, particularly N = 2 supersymmetry, plays a crucial role in M-theory compactifications on Calabi-Yau manifolds, with further generalizations extending to N = 4. Additionally, N = 1 supersymmetry arises in scenarios where D-branes wrap around cycles of internal spaces. This article aims to contribute to the understanding of these intricate relationships within the framework of supersymmetric gauge theories.",
        "ori-fast-z-score": 1.5888598190134724,
        "water-fast-z-score": 6.892774827860417,
        "rewrite-fast-z-score": -0.49656353316142077
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Vacuum Energy and Renormalization on the Edge .\nAbstract:\nWe study vacuum energy in quantum field theory with boundary conditions that break conformal invariance, such as Dirichlet or Neumann boundary conditions. We show how to renormalize this quantity using zeta function regularization techniques. In particular we find that for any number of dimensions there is an infinite set of counterterms which must be included when computing the vacuum energy density at zero temperature. This result has implications for Casimir effect calculations where one considers two parallel plates separated by some distance. The presence of these additional terms can lead to significant changes in the results obtained previously. Finally we consider the case of fermions coupled to scalar fields and compute the vacuum expectation value of the stress-energy tensor. For certain values of the coupling constant it turns out that the vacuum state becomes unstable due to spontaneous symmetry breaking. Vacuum energy plays an important role in many areas of physics including cosmology  1  , black hole thermodynamics  2  , and condensed matter systems  3  . It also appears in various contexts within string theory  4  .\nIn recent years much progress has been made towards understanding the nature of vacuum fluctuations in quantum field theories (QFTs)  5  -  8  . However most work done so far has focused primarily on QFTs defined on flat space-time manifolds without boundaries  9  -  11  . Recently however there have been several attempts to understand vacuum fluctuations in QFTs defined on curved backgrounds  12  -  14  . Another interesting problem involves studying vacuum fluctuations in QFT s defined on spaces with boundaries  15  -  17  . Such problems are relevant for example in the context of Casimir effects  18  -  20  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Vacuum Energy and Renormalization on the Edge . Abstract : We research vacuum energy in quantum field theory with boundary constraints that broke conformal invariance , such as Dirichlet or Neumann border conditions .We see how to renormalize this quantity use zeta function regularization techniques . In particular we find that for any number of dimensions there is an endless system of counterterms which requires be included when computing the vacuum energy density at zero temperature .This result has implications for Casimir effect calculations where one considers two connected plates apart by some distance . The presence of these additional terms can lead to significant changes in the results derived earlier .Finally we study the case of fermions connected to scalar fields and compute the vacuum expectation value of the strain - energy tensor . For particular values of the coupling constant it turns out that the vacuum state remains unstable due to spontaneous symmetry breaking .Vacuum energy serves an important role in multiple fields of science specifically cosmology 1 , white hole thermodynamics 2 , and condensed matter systems 3 . It additionally occurs in different contexts within string theory 4 .In past decades considerable progress has been achieved towards studying the nature of vacuum fluctuations in particle field theories ( QFTs ) 5 - 8 . However most work done so far has concentrated mostly on QFTs defined on flat space - time manifolds without boundaries 9 - 11 .Recently however there have been numerous attempts to explain vacuum fluctuations in QFTs defined on curved backgrounds 12 - 14 . Another important difficulty involves studying vacuum fluctuations in QFT s defined on spaces with borders 15 - 17 .Such issues are applicable for example in the context of Casimir effects 18 - 20 .",
        "rewrite_text": "**Title:** Vacuum Energy and Renormalization on the Edge\n\n**Abstract:** This study investigates the concept of vacuum energy within the framework of quantum field theory (QFT) under boundary conditions that disrupt conformal invariance, specifically focusing on Dirichlet and Neumann boundary conditions. We employ zeta function regularization techniques to effectively renormalize vacuum energy, revealing that an infinite series of counterterms must be incorporated when calculating the vacuum energy density at zero temperature, regardless of the dimensionality of the system. This finding has significant implications for the Casimir effect, particularly in scenarios involving two parallel plates separated by a finite distance, where the inclusion of these additional counterterms can substantially alter previously established results. Furthermore, we explore the interaction between fermions and scalar fields, calculating the vacuum expectation value of the strain-energy tensor. Notably, for specific values of the coupling constant, we observe that the vacuum state can exhibit instability due to spontaneous symmetry breaking. The role of vacuum energy is critical across various scientific disciplines, including cosmology, white hole thermodynamics, and condensed matter physics, and it also appears in diverse contexts within string theory. Over the past few decades, significant advancements have been made in understanding vacuum fluctuations in particle field theories. However, much of the existing research has primarily focused on QFTs situated in flat spacetime without boundaries. Recently, there has been a surge of interest in exploring vacuum fluctuations in QFTs defined on curved backgrounds, as well as in spaces with boundaries. These investigations are particularly relevant in the context of Casimir effects, where boundary conditions play a pivotal role in the behavior of vacuum energy.",
        "ori-fast-z-score": -1.1026456085839622,
        "water-fast-z-score": 5.706433236417486,
        "rewrite-fast-z-score": 0.6859943405700353
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Relativistic Fluctuation Theorems: Theory and explicit examples .\nAbstract:\nWe present the theory behind relativistic fluctuation theorems, which are exact relations between entropy production in nonequilibrium processes and fluctuations in equilibrium states. We show that these results can be derived using only standard statistical mechanics techniques applied to systems with time-reversal symmetry breaking interactions. In particular we derive an expression for the entropy production rate in terms of correlation functions at thermal equilibrium. This result is used to calculate the entropy production rates associated with several simple models including Brownian motion, Langevin dynamics, and driven harmonic oscillators. Finally, we discuss how our approach may be extended beyond classical physics. Relativistic fluctuation theorems provide exact relations between entropy production during non-equilibrium processes and fluctuations in corresponding equilibrium states. These results have been obtained by applying standard statistical mechanics methods to systems with broken timereversal invariance. Here we use this formalism to obtain expressions for the entropy production rate as well as other quantities such as heat currents in terms of correlation functions evaluated at thermal equilibrium. As concrete applications we consider several simple models including Browninan motion, Langevin dynamics and driven harmonic oscillators. \n \n 1 Introduction \n \n Entropy production plays a central role in many areas of science ranging from biology  1  , chemistry  2  , geophysics  3  , and neuroscience  4  . It has also become increasingly important in quantum information processing  5  where it provides a measure of irreversibility  6  . Despite its importance there remains no general method for calculating entropy production rates except in very special cases  7–9  . Recently, however, new theoretical tools based on fluctuation theorems  10–12  have emerged which allow one to relate entropy production directly to measurable properties of physical systems  13–18  . For example, in recent years there has been considerable interest in developing experimental schemes  19–21  capable of measuring entropy production rates in small isolated quantum systems  22  . Such experiments would enable direct tests of fundamental thermodynamic principles  23  and could potentially lead to practical devices for extracting work from heat baths  24  . \n \n 2 Classical fluctuation theorems \n \n Perhaps the most famous fluctuation theorem was first proposed by Jarzynski  10",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Relativistic Fluctuation Theorems : Theory and explicit examples . Abstract : We present the principle behind relativistic fluctuation theorems , which are exact relations between entropy production in nonequilibrium systems and fluctuations in equilibrium states .We see that these results can be derived using only conventional statistical mechanics method applied to systems with time - reversal symmetry breaking processes . In particular we derive an definition for the entropy production speed in terms of correlation functions at heat equilibrium .This result is utilized to estimate the entropy production rates associated with many simple theories including Brownian movement , Langevin mechanics , and driven harmonic oscillators . Finally , we talk how our approach may be advanced beyond classical physics .Relativistic fluctuation theorems allow exact relations between entropy production during non - equilibrium processes and fluctuations in associated equilibrium states . These conclusions have been achieved by using conventional statistical mechanics principles to systems with broken timereversal invariance .Here we apply this formalism to obtain definitions for the entropy production frequency as well as other quantities such as heat currents in terms of correlation functions evaluated at heat equilibrium . As concrete uses we study many simple theories including Browninan movement , Langevin mechanics and driven harmonic oscillators .1 Introduction Entropy production plays a central role in different areas of science diverse from biology 1 , chemistry 2 , geophysics 3 , and neuroscience 4 . It has additionally grown increasingly important in quantum information processing 5 where it gives a measure of irreversibility 6 .Despite its significance there stands no general technique for determining entropy production rates except in very exceptional cases 7 – 9 . Recently , however , new theoretical tools based on fluctuation theorems 10 – 12 have developed which allow one to relate entropy production immediately to measurable properties of physical processes 13 – 18 .For instance , in recent years there has been substantial interest in establishing experimental schemes 19 – 21 capable of calculating entropy production rates in small separated quantum systems 22 . Such experiments would enable direct tests of fundamental thermodynamic concepts 23 and could potentially lead to useful systems for extracting work from hot baths 24 .2 Classical fluctuation theorems Perhaps the most famous fluctuation principle was first suggested by Jarzynski 10",
        "rewrite_text": "**Title:** Relativistic Fluctuation Theorems: Theory and Explicit Examples\n\n**Abstract:** In this article, we explore the foundational principles of relativistic fluctuation theorems, which establish precise relationships between entropy production in nonequilibrium systems and fluctuations observed in equilibrium states. Our analysis demonstrates that these relationships can be derived using standard statistical mechanics techniques applied to systems exhibiting time-reversal symmetry breaking. Specifically, we introduce a definition for the speed of entropy production, expressed in terms of correlation functions at thermal equilibrium. This framework is employed to calculate entropy production rates for various fundamental models, including Brownian motion, Langevin dynamics, and driven harmonic oscillators. \n\nWe further discuss the implications of our findings and how this methodology can extend beyond classical physics. The relativistic fluctuation theorems provide a robust framework for understanding the interplay between entropy production during nonequilibrium processes and the corresponding fluctuations in equilibrium states. Our results are derived through the application of conventional statistical mechanics to systems with broken time-reversal invariance, leading to new insights into the nature of entropy production frequency and related quantities, such as heat currents, which are expressed in terms of correlation functions evaluated at thermal equilibrium.\n\nEntropy production is a critical concept across various scientific disciplines, including biology, chemistry, geophysics, and neuroscience, and has gained prominence in the realm of quantum information processing, where it serves as a measure of irreversibility. Despite its importance, a comprehensive method for determining entropy production rates has been lacking, except in a few exceptional scenarios. Recently, advancements in theoretical frameworks based on fluctuation theorems have emerged, enabling a direct connection between entropy production and measurable properties of physical processes. This has spurred interest in experimental setups capable of quantifying entropy production rates in small, isolated quantum systems, paving the way for direct tests of fundamental thermodynamic principles and the development of systems that can efficiently harness work from thermal reservoirs.",
        "ori-fast-z-score": 0.2170723815877265,
        "water-fast-z-score": 10.05768701356466,
        "rewrite-fast-z-score": 2.095139706465989
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Assessing the Predictive Power of Galaxy Formation Models: A Comparison of Predicted and Observed Rest-Frame Optical Luminosity Functions at 2.0<z<3.3 .\nAbstract:\nWe present an assessment of the predictive power of galaxy formation models by comparing their predictions for rest-frame optical luminosity functions (LFs) with observations over the redshift range z=2-3. We use two different semi-analytic models, GALFORM and L-GALAXIES, to predict the evolution in number density as well as the distribution of stellar masses and star formation rates of galaxies across this redshift interval. The predicted LF is compared directly against observational data obtained using the Hubble Space Telescope s Advanced Camera for Surveys (ACS). In addition we compare the observed and predicted distributions of UV absolute magnitudes and dust-corrected colours. Our results show that both models are able to reproduce the overall shape of the observed LF but underpredict its normalisation by up to a factor of three. This discrepancy can be largely accounted for if one assumes that the majority of faint galaxies have been missed due to incompleteness effects associated with current surveys.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Assessing the Predictive Power of Galaxy Formation Models : A Comparison of Predicted and Observed Rest - Frame Optical Luminosity Functions at 2 . 0 < z < 3 . 3 . Abstract : We present an assessment of the predictive capacity of galaxy formation models by comparing their expectations for rest - frame optical luminosity functions ( LFs ) with observations over the redshift range z = 2 - 3 .We use two different semi - analytic models , GALFORM and L - GALAXIES , to predict the evolution in number density as well as the spread of stellar masses and galaxy formation rates of clusters across this redshift interval . The predicted LF is compared closely against observational data acquired using the Hubble Space Telescope s Advanced Camera for Surveys ( ACS ) .In addition we compare the seen and anticipated distributions of UV absolute magnitudes and dust - adjusted colours . Our results show that both models are able to reproduce the overall shape of the seen LF but underpredict its normalisation by up to a factor of three .This discrepancy can be largely accounted for if one assumes that the majority of distant galaxies have been missed due to incompleteness effects involved with current surveys .",
        "rewrite_text": "In this study, we evaluate the predictive capabilities of galaxy formation models by analyzing their forecasts for rest-frame optical luminosity functions (LFs) in the redshift range of 2.0 < z < 3.3. We employ two semi-analytic models, GALFORM and L-GALAXIES, to simulate the evolution of number density, stellar mass distributions, and galaxy formation rates within this redshift interval. Our predictions for the LFs are rigorously compared with observational data obtained from the Hubble Space Telescope's Advanced Camera for Surveys (ACS). Additionally, we examine the distributions of UV absolute magnitudes and dust-corrected colors, both observed and predicted, to gain further insights into the models' performance. Our findings indicate that while both models successfully replicate the general shape of the observed LF, they tend to underestimate its normalization by as much as a factor of three. This significant discrepancy suggests that many distant galaxies may have been overlooked due to incompleteness in current observational surveys. By addressing these limitations, we aim to enhance the understanding of galaxy formation processes and improve the accuracy of theoretical models in predicting galaxy properties at high redshifts. This work contributes to the ongoing efforts to reconcile theoretical predictions with observational realities in the field of astrophysics, particularly in the context of galaxy evolution during the early universe.",
        "ori-fast-z-score": 0.5773502691896257,
        "water-fast-z-score": 4.358898943540673,
        "rewrite-fast-z-score": 1.0425720702853738
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Mode stability in delta Scuti stars: linear analysis versus observations in open clusters .\nAbstract:\nWe present an extensive study on the mode stability properties of delta Scuti (δ Sct) pulsators, based on both theoretical and observational results obtained for open clusters with ages between 1 Myr and 2 Gyr. We have performed detailed non-radial stellar oscillation calculations using state-of-the-art evolutionary models that include overshooting at convective boundaries as well as microscopic diffusion processes. The main goal is to investigate how the observed frequency pattern changes during evolution due to the effects of rotation-induced mixing and chemical composition gradients. In particular we focus our attention on the so-called mixed modes which are trapped in the region where the hydrogen burning shell overlaps with the helium core. These modes show very characteristic features such as large amplitudes and high degree of nonlinearity. Our results indicate that these modes can be excited by turbulent pressure fluctuations associated with the convection zone located near the surface layers of the star. Moreover, they also suggest that the excitation mechanism may change significantly when the star evolves off the ZAMS towards higher luminosities.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Mode stability in delta Scuti stars : linear analysis versus studies in open complexes . Abstract : We present an extensive research on the mode stability properties of delta Scuti ( δ Sct ) pulsators , relying on both theoretical and observational results collected for open clusters with ages between 1 Myr and 2 Gyr .We have done extensive non - radial stellar oscillation calculations combining state - of - the - art evolutionary models that include overshooting at convective boundaries as well as microscopic diffusion processes . The main goal is to examine how the seen frequency pattern moves during evolution owing to the effects of rotation - caused mixing and chemical composition gradients .In particular we focus our focus on the so - called mixture modes which are trapped in the location where the hydrogen burning shell overlaps with the helium core . These modes display very typical characteristics such as huge amplitudes and large extent of nonlinearity .Our results suggests that these modes can be excited by turbulent pressure fluctuations associated with the convection zone situated near the surface layers of the star . Moreover , they also suggest that the excitation process may change considerably when the star evolves off the ZAMS towards higher luminosities .",
        "rewrite_text": "We present a comprehensive study on the mode stability characteristics of delta Scuti (δ Sct) pulsators, integrating both theoretical frameworks and observational data from open clusters ranging in age from 1 million to 2 billion years. Our research involves detailed calculations of non-radial stellar oscillations, utilizing advanced evolutionary models that account for overshooting at convective boundaries and the effects of microscopic diffusion. The primary objective of this investigation is to analyze the evolution of frequency patterns in these stars, particularly as influenced by rotational mixing and gradients in chemical composition. \n\nA significant focus of our study is on the so-called mixed modes, which are localized in regions where the hydrogen burning shell intersects with the helium core. These mixed modes exhibit distinctive features, including substantial amplitudes and pronounced nonlinearity. Our findings indicate that these modes can be excited by turbulent pressure fluctuations arising from the convection zone located in the star's outer layers. Furthermore, we observe that the excitation mechanisms may undergo significant changes as the star evolves away from the Zero-Age Main Sequence (ZAMS) and ascends to higher luminosities. \n\nThis research not only enhances our understanding of the pulsation mechanisms in δ Sct stars but also provides insights into the broader implications of stellar evolution and the interplay between various physical processes in determining mode stability. The results underscore the importance of considering both theoretical predictions and observational evidence when studying the complex behavior of pulsating stars in different evolutionary stages.",
        "ori-fast-z-score": -1.4925557853149838,
        "water-fast-z-score": 2.970442628930023,
        "rewrite-fast-z-score": 0.18569533817705186
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Radial density profiles of time-delay lensing galaxies .\nAbstract:\nWe present the first results on radial mass distribution in strong gravitational lenses based on high-resolution Hubble Space Telescope (HST) imaging data for four systems with multiple images and measured time delays between them. We use two different methods to measure the surface brightness profile, one using an adaptive kernel smoothing technique and another fitting Sérsic models directly to the observed galaxy light distributions. The resulting total magnifications are consistent within uncertainties among these two approaches. For each system we fit both singular isothermal sphere (SIS), as well as SIE + external shear model to the observed image positions and flux ratios simultaneously. In all cases, the best-fit SIE + external shear models provide better fits than those obtained by assuming only SIS model. Our analysis shows that the average slope of the inner part of the lens potential is shallower than -1, which indicates that dark matter halos around massive early-type galaxies may be more centrally concentrated than previously thought.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Radial density characteristics of time - delay lensing galaxies . Abstract : We report the first findings on transverse mass distribution in heavy gravitational lenses based on high - resolution Hubble Space Telescope ( HST ) imaging information for four systems with many images and studied period delays between them .We use two different methods to measure the surface brightness profile , one utilizing an adaptive kernel smoothing methodology and another matching Sérsic estimates directly to the seen galaxy light distributions . The resulting total magnifications are compatible within uncertainties among these two strategies .For each system we fit both singular isothermal sphere ( SIS ) , as well as SIE + external shear model to the seen image positions and flux ratios simultaneously . In all situations , the best - fitting SIE + external shear models afford best fits than those achieved by assuming only SIS description .Our study shows that the average slope of the inner part of the lens potential is shallower than - 1 , which implies that dark matter halos around large earliest - class stars must be more centrally focused than previously thought .",
        "rewrite_text": "We present groundbreaking findings on the transverse mass distribution in significant gravitational lensing systems, derived from high-resolution imaging obtained through the Hubble Space Telescope (HST). Our study focuses on four distinct lensing galaxies that exhibit multiple images and have been analyzed for time delays between these images. To accurately measure the surface brightness profiles of these galaxies, we employed two distinct methodologies: one based on adaptive kernel smoothing and the other on direct matching of Sérsic profiles to the observed light distributions of the galaxies. The total magnifications calculated using both methods show a high degree of compatibility within the associated uncertainties.\n\nFor each lensing system, we applied both the singular isothermal sphere (SIS) model and a more complex model combining a singular isothermal ellipsoid (SIE) with external shear. This dual approach allowed us to fit the observed image positions and flux ratios simultaneously. Our results indicate that the SIE + external shear models consistently provide a superior fit compared to the SIS model alone across all systems analyzed. Notably, our findings reveal that the average slope of the inner lens potential is shallower than -1. This observation suggests that the dark matter halos surrounding these massive early-type galaxies are more concentrated towards the center than previously assumed. This research not only enhances our understanding of the mass distribution in gravitational lensing galaxies but also has significant implications for the study of dark matter in the universe.",
        "ori-fast-z-score": -0.10050378152592121,
        "water-fast-z-score": 6.0,
        "rewrite-fast-z-score": 0.18257418583505536
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Topological Classification of Clusters in Condensed Phases .\nAbstract:\nThe topological classification of clusters is reviewed, with emphasis on the role played by symmetry and topology in determining cluster structures.  The concept of  topology  refers to the connectivity between atoms or molecules within a cluster; it can be used as an organizing principle for classifying clusters into families based upon their structural similarities. This review focuses primarily on metal clusters (i.e., clusters containing one or more metallic elements), but also discusses some examples of non-metallic clusters that have been studied recently. In addition, we briefly discuss how this approach has been applied to classify clusters formed during chemical reactions. Finally, we present several open questions related to the topic of cluster topology. Topology plays an important role in understanding the structure of matter at all scales ranging from atomic nuclei to macroscopic materials such as crystals. It provides a useful framework for classifying clusters according to their structural similarity. Herein, we provide a brief overview of recent progress made towards developing a systematic classification scheme for clusters using concepts borrowed from condensed-matter physics.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Topological Classification of Clusters in Condensed Phases . Abstract : The topological classification of clusters is reviewed , with emphasis on the importance played by symmetry and topology in identifying cluster structures .The concept of topology refers to the connectivity between molecules or compounds within a cluster ; it can be used as an organizing principle for classifying clusters into parents based upon their structural similarities . This publication emphasizes primarily on metal clusters ( i . e . , clusters featuring one or more metallic objects ) , but also explains some examples of non - metallic complexes that have been studied ago .In addition , we briefly address how this methodology has been used to classify clusters formed during chemical processes . Finally , we present many open questions related to the subject of cluster topology .Topology plays an important role in understanding the composition of matter at all scales ranging from atomic atoms to macroscopic materials such as crystals . It provides a helpful basis for classifying clusters according to their structural similarity .Herein , we provide a brief overview of recent progress made towards developing a comprehensive classification system for clusters using concepts borrowed from condensed - matter science .",
        "rewrite_text": "This article presents a comprehensive review of the topological classification of clusters in condensed phases, highlighting the critical roles of symmetry and topology in distinguishing various cluster structures. Topology, in this context, pertains to the connectivity among molecules or compounds within a cluster, serving as a fundamental organizing principle for categorizing clusters based on their structural similarities. While the primary focus of this review is on metal clusters—defined as those containing one or more metallic entities—it also includes discussions of notable non-metallic complexes that have been previously investigated. Furthermore, the article explores the application of this topological methodology in classifying clusters that emerge during various chemical processes. \n\nThe significance of topology extends beyond mere classification; it is essential for understanding the composition of matter across all scales, from individual atoms to larger macroscopic materials like crystals. By employing topological concepts, researchers can systematically classify clusters according to their structural characteristics, thereby enhancing our understanding of their properties and behaviors. \n\nIn this review, we summarize recent advancements in the development of a robust classification framework for clusters, drawing on insights from condensed matter science. Additionally, we identify several open questions and challenges that remain in the field of cluster topology, inviting further exploration and research. This work aims to contribute to the ongoing discourse on the intricate relationship between topology and the structural organization of clusters, ultimately fostering a deeper comprehension of material properties at both the microscopic and macroscopic levels.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.703745285369415,
        "rewrite-fast-z-score": 1.3310347641241707
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Magnetically-driven explosions of rapidly-rotating white dwarfs following Accretion-Induced Collapse .\nAbstract:\nWe present the results of our numerical simulations of magnetized, rotating white dwarf stars that undergo accretion-induced collapse (AIC). We find that for sufficiently rapid rotation and strong magnetic fields, AIC leads to an explosion with properties similar to those observed in SNe Ia. The explosion is driven by the release of gravitational energy as the collapsed core bounces back after reaching nuclear density. In addition, we show that the presence of a strong toroidal field can lead to significant asymmetries in the ejecta distribution. These asymmetries are likely responsible for the polarization signal detected in some SNe Ia. \n \n Keywords: Supernovae Type Ia, Rotation, Magnetic Fields, White Dwarf Stars, Accretion Induced Collapse \n \n 1 Introduction \n \n Recent observations have shown that many supernovae type Ia (SNe Ia) exhibit large amounts of linear polarization  1  . This has been interpreted as evidence that these events result from asymmetric explosions  2  , which may be caused by large-scale magnetic fields  3  or rapid rotation  4  . However, it remains unclear whether either mechanism alone could produce such highly polarized light curves  5  . \n \n Here we investigate how the combination of rapid rotation and strong magnetic field affects the outcome of accretion induced collapse (AIC), where a white dwarf star collapses into a neutron star  6  . For this purpose, we perform two-dimensional axisymmetric hydrodynamic simulations using the code FLASH  7  . Our initial models consist of rigidly-rotating white dwarf stars with masses ranging between 0.6-1.2 Msun  8  . To account for the effects of general relativity on the structure of the white dwarf  9  , we use the polytropic equation of state P = Kρ Γ , where ρ denotes the mass density and P the pressure  10  . \nThe main goal of this work is to determine if AICs triggered by rapid rotation and/or strong magnetic fields can explain the high degree of polarization observed in SNe Ia  11  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Magnetically - fueled bursts of quickly - spinning white dwarfs following Accretion - Induced Collapse . Abstract : We present the results of our numerical simulations of magnetized , rotating white dwarf stars that suffer accretion - caused instability ( AIC ) .We see that for enough fast rotation and strong magnetic fields , AIC leads to an explosion with properties similar to those observed in SNe Ia . The explosion is powered by the release of gravitational energy as the collapsed center bounces away after reaching nuclear density .In addition , we find that the presence of a powerful toroidal field can lead to significant asymmetries in the ejecta distribution . These asymmetries are likely responsible for the polarization wave observed in some SNe Ia .Keywords : Supernovae Type Ia , Rotation , Magnetic Fields , White Dwarf Stars , Accretion Induced Collapse 1 Introduction Recent measurements have shown that several supernovae class Ia ( SNe Ia ) exhibit large quantities of linear polarization 1 . This has been viewed as proof that these events result from asymmetric explosions 2 , which may be caused by large - scale magnetic waves 3 or rapid rotation 4 .However , it remains unsure whether either mechanism alone might generate such heavily polarized light curves 5 . Here we investigate how the combination of rapid rotation and strong magnetic force influence the result of accretion induced collapse ( AIC ) , where a white dwarf star collapses into a neutron star 6 .For this use , we perform two - dimensional axisymmetric hydrodynamic simulations using the code FLASH 7 . Our preliminary estimates consist of rigidly - spinning white dwarf stars with masses ranging between 0 . 6 - 1 . 2 Msun 8 .To account for the effects of general relativity on the composition of the white dwarf 9 , we utilize the polytropic equation of state P = Kρ Γ , where ρ indicates the mass density and P the pressure 10 . The main goal of this research is to find if AICs triggered by rapid rotation and / or strong magnetic fields can describe the high degree of polarization observed in SNe Ia 11 .",
        "rewrite_text": "**Title:** Magnetically-Fueled Bursts of Rapidly Spinning White Dwarfs Following Accretion-Induced Collapse\n\n**Abstract:** In this study, we present the findings from our numerical simulations of magnetized, rapidly rotating white dwarf stars undergoing accretion-induced collapse (AIC). Our results indicate that when these stars possess sufficiently high rotation rates and strong magnetic fields, AIC can result in explosions that exhibit characteristics akin to those observed in Type Ia supernovae (SNe Ia). The explosions are primarily driven by the gravitational energy released as the core rebounds after achieving nuclear density. Furthermore, our simulations reveal that the presence of a robust toroidal magnetic field can induce significant asymmetries in the distribution of the ejected material. These asymmetries may account for the polarization waves detected in certain SNe Ia events. \n\nRecent observations have highlighted that several SNe Ia display substantial linear polarization, suggesting that these supernovae arise from asymmetric explosions. This asymmetry could potentially stem from large-scale magnetic fields or rapid rotation; however, it remains uncertain whether either mechanism alone can produce the observed polarized light curves. In this work, we explore the interplay between rapid rotation and strong magnetic forces in the context of AIC, where a white dwarf transitions into a neutron star. To achieve this, we employ two-dimensional axisymmetric hydrodynamic simulations using the FLASH code, focusing on rigidly spinning white dwarfs with masses ranging from 0.6 to 1.2 solar masses. To incorporate the effects of general relativity on the white dwarf's structure, we utilize a polytropic equation of state, where pressure (P) is related to mass density (ρ) through the equation P = Kρ^Γ. The primary objective of our research is to determine whether AIC events, influenced by rapid rotation and/or strong magnetic fields, can adequately explain the high levels of polarization observed in SNe Ia.",
        "ori-fast-z-score": 0.4931969619160719,
        "water-fast-z-score": 7.021870595978444,
        "rewrite-fast-z-score": 0.3333333333333333
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Non-coding DNA programs express adaptation and its universal law .\nAbstract:\nThe human genome is composed of coding sequences (CDSs) that encode proteins, non-coding sequences (NCSs), which are transcribed but not translated into protein products, and introns, which are removed during splicing to produce mature mRNAs. The majority of NCSs have been shown to be functional in regulating gene expression at the transcriptional level through epigenetic mechanisms such as histone modifications or chromatin remodeling. However, it remains unclear whether these regulatory functions can also be achieved by other types of NCSs including enhancers, promoters, insulators, silencers, etc., especially those located outside of genes. Here we show that all known classes of NCSs contain information for expressing adaptive phenotypes using an unbiased computational approach based on machine learning algorithms. We found that each class of NCSs has distinct features associated with their specific function(s). In addition, we discovered a novel principle underlying the evolution of NCSs: the evolutionary process tends to increase the complexity of NCSs until they reach a maximum point beyond which further increases would lead to decreased fitness. This finding suggests that there exists a universal law governing how organisms evolve adaptively.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Non - coding DNA programs represent adaptation and its universal law . Abstract : The human genome is composed of coding elements ( CDSs ) that encode genes , non - coding elements ( NCSs ) , which are transcribed but not translated into protein components , and introns , which are removed during splicing to produce adult mRNAs .The majority of NCSs have been shown to be functional in controlling gene transcription at the transcriptional level through epigenetic mechanisms such as histone modifications or chromatin remodeling . However , it remains unclear whether these regulatory functions can also be accomplish by other types of NCSs namely enhancers , promoters , insulators , silencers , etc . , particularly those positioned outside of genes .Here we show that all known classes of NCSs contain information for expressing adaptive phenotypes using an unbiased computational approach based on machine learning algorithms . We found that each class of NCSs has distinct features associated with their specific function ( s ) .In addition , we identified a new theory explaining the evolution of NCSs : the evolutionary process tends to expand the complexity of NCSs until they reach a maximum point beyond which further changes might lead to lowered fitness . This conclusion suggests that there exists a universal rule governing how organisms develop adaptively .",
        "rewrite_text": "Title: Non-Coding DNA Programs Represent Adaptation and Its Universal Law\n\nAbstract: The human genome is intricately structured, comprising coding sequences (CDSs) that encode proteins, non-coding sequences (NCSs) that are transcribed but not translated, and introns that are excised during mRNA splicing. Recent studies have demonstrated that a significant portion of NCSs plays a crucial role in regulating gene transcription through epigenetic mechanisms, including histone modifications and chromatin remodeling. However, the functional capabilities of various types of NCSs—such as enhancers, promoters, insulators, and silencers—especially those located outside of traditional gene boundaries, remain inadequately understood. In this study, we employ an unbiased computational approach utilizing machine learning algorithms to analyze the information encoded within all known classes of NCSs, revealing their contributions to the expression of adaptive phenotypes. Our findings indicate that each class of NCSs possesses unique characteristics that correlate with their specific regulatory functions. Furthermore, we propose a novel theoretical framework for understanding the evolution of NCSs, suggesting that the evolutionary trajectory of these sequences tends to increase their complexity until a critical threshold is reached. Beyond this point, any additional modifications may result in diminished fitness. This insight implies the existence of a universal principle that governs the adaptive development of organisms, highlighting the significance of non-coding DNA in evolutionary biology. Our research not only enhances the understanding of the functional roles of NCSs but also contributes to the broader discourse on the mechanisms of adaptation in living systems.",
        "ori-fast-z-score": -2.1322269467806003,
        "water-fast-z-score": 2.424366106925306,
        "rewrite-fast-z-score": -0.2822162605150792
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Prewavelet Solution to Poisson Equations .\nAbstract:\nWe present an algorithm for solving the following problem: given a function f on  0,1  and a positive integer n find a piecewise polynomial spline s with knots at 0 = t0 < t1 < ... < tn = 1 such that s(ti) = fi (i=0,...,n), where fi is some approximation of f in ti-1/2 , ti+1/2 . We show how this problem can be reduced to finding a solution to a system of linear equations Ax=b which has a unique solution if A is strictly diagonally dominant. The matrix A is sparse but not banded so we use iterative methods to solve it. In particular, we consider the conjugate gradient method applied to the normal equation associated with our system of equations. We prove convergence of this method under certain conditions. Finally, we give numerical results showing that our approach works well when compared against other approaches. This work was supported by NSF grant DMS-0504520.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Prewavelet Solution to Poisson Equations . Abstract : We present an algorithm for solving the following task : given a function h on 0 , 1 and a positive integer n get a piecewise polynomial spline s with knots at 0 = t0 < t1 < . . . < tn = 1 such that s ( ti ) = fi ( i = 0 , . . . , n ) , where fi is some approximation of f in ti - 1 / 2 , ti + 1 / 2 .We see how this question can be reduced to finding a solution to a system of linear equations Ax = b which has a unique solve if A is strictly diagonally dominant . The matrix A is sparse but not banded so we using iterative techniques to solve it .In particular , we define the conjugate gradient technique applied to the normal equation involved with our system of equations . We establish convergence of this method under certain conditions .Finally , we give quantitative results confirming that our approach works well when compared against other methods . This research was supported by NSF grant DMS - 0504520 .",
        "rewrite_text": "In this article, we introduce a novel algorithm designed to address the problem of constructing a piecewise polynomial spline, denoted as \\( s \\), based on a given function \\( h \\) defined on the interval [0, 1] and a positive integer \\( n \\). The objective is to create a spline with specified knots at \\( 0 = t_0 < t_1 < \\ldots < t_n = 1 \\) such that the spline satisfies the condition \\( s(t_i) = f_i \\) for \\( i = 0, \\ldots, n \\), where \\( f_i \\) represents an approximation of \\( f \\) evaluated at the midpoints \\( t_{i - 1/2} \\) and \\( t_{i + 1/2} \\). \n\nWe demonstrate that this problem can be effectively transformed into solving a system of linear equations of the form \\( Ax = b \\), where a unique solution exists if the matrix \\( A \\) is strictly diagonally dominant. Although the matrix \\( A \\) is sparse, it does not possess a banded structure, prompting us to employ iterative methods for its resolution. Specifically, we apply the conjugate gradient method to the normal equations associated with our linear system. We also establish the convergence criteria for this iterative approach under certain conditions.\n\nTo validate the efficacy of our proposed method, we present quantitative results that illustrate its performance in comparison to existing techniques. The findings indicate that our algorithm not only meets the requirements of the task but also outperforms alternative methods in various scenarios. This research was made possible through the support of the National Science Foundation under grant DMS-0504520.",
        "ori-fast-z-score": -0.35603449745815596,
        "water-fast-z-score": 4.09644015186457,
        "rewrite-fast-z-score": -1.0101525445522108
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The LuckyCam Survey for Very Low Mass Binaries II: 13 new M4.5-M6.0 Binaries .\nAbstract:\nWe present the results of our second survey to find very low mass binaries using lucky imaging with the Palomar Observatory 1.2 m telescope and the PALM-3000 camera. We have discovered thirteen new systems, including ten brown dwarf companions (four previously unknown) in twelve nearby young open clusters ranging in age between 10 Myr and 300 Myr. The masses range from 0.03-0.10 M . These are among the lowest-mass objects ever found by direct imaging techniques. In addition we report on one system that is likely an unresolved binary consisting of two late-type stars. This work represents the largest sample of directly imaged brown dwarfs assembled so far. It will be used as input into population synthesis models aimed at understanding how these objects form and evolve over time. \n \n Keywords: Brown Dwarf, Open Cluster, Direct Imaging, Lucky Imaging, Nearby Stars",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The LuckyCam Survey for Very Low Mass Binaries II : 13 new M4 . 5 - M6 . 0 Binaries . Abstract : We present the results of our second survey to find very low mass binaries using lucky imaging with the Palomar Observatory 1 . 2 m observatory and the PALM - 3000 sensor .We have discovered thirteen new components , notably ten brown dwarf companions ( four originally unknown ) in twelve nearby young open complexes ranging in age between 10 Myr and 300 Myr . The masses range from 0 . 03 - 0 . 10 M .These are among the smallest - weight objects ever found by direct imaging technology . In addition we paper on one system that is probably an unresolved binary composed of two late - class stars .This project represents the largest sample of specifically imaged brown dwarfs assembled so far . It will be used as input into population analysis models targeted at explaining how these objects formation and evolve over time .Keywords: Brown Dwarf, Open Cluster, Direct Imaging, Lucky Imaging, Nearby Stars",
        "rewrite_text": "In this article, titled \"The LuckyCam Survey for Very Low Mass Binaries II: 13 New M4.5 - M6.0 Binaries,\" we detail the findings from our second survey aimed at identifying very low mass binary systems through the technique of lucky imaging, conducted at the Palomar Observatory using the 1.2 m telescope and the PALM-3000 sensor. Our survey has successfully identified thirteen new binary components, including ten brown dwarf companions, four of which were previously unknown. These discoveries were made within twelve nearby young open clusters, with ages ranging from 10 million to 300 million years. The masses of the identified brown dwarfs fall between 0.03 and 0.10 solar masses, marking them as some of the lightest objects ever detected using direct imaging methods. Additionally, we discuss one particular system that is likely an unresolved binary consisting of two late-type stars. This research contributes to the largest collection of directly imaged brown dwarfs to date, providing valuable data for future population analysis models. These models aim to enhance our understanding of the formation and evolutionary processes of these intriguing celestial objects. The findings from this survey not only expand the catalog of known low mass binaries but also offer critical insights into the characteristics and dynamics of brown dwarfs within their respective environments. Keywords associated with this study include Brown Dwarf, Open Cluster, Direct Imaging, Lucky Imaging, and Nearby Stars.",
        "ori-fast-z-score": -1.2701705922171767,
        "water-fast-z-score": 4.358898943540673,
        "rewrite-fast-z-score": -1.9245008972987525
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Floating Phase in 2D ANNNI Model .\nAbstract:\nWe study the floating phase in the two-dimensional anisotropic nearest-neighbor Ising model (ANNNI). We find that there is no floating phase for J1 = J2, but it appears when J1 > J2 and disappears at some critical value of J1/J2. The transition between the ordered state and the floating phase belongs to the universality class of the three-state Potts model with first-order transition. In addition we show that the ground states are degenerate on the square lattice if J1 = J2 or J1 < J2. This result suggests that the ground states may be non-degenerate even though they have not been found yet. \n \n Introduction \n \n It has been known since the work by Wannier  1  that the ground states of the spin-1/2 Heisenberg antiferromagnet on an infinite square lattice are infinitely degenerate. However, this fact does not necessarily mean that all possible configurations can appear as ground states  2  . For example, the ground states of the one-dimensional chain are unique although its energy spectrum is continuous  3  , while those of the two-dimensional triangular-lattice Heisenberg antiferromagnet are doubly degenerate  4  . \n \n Recently, several authors studied the ground states of the two-dimensional anisotropic nearest neighbor Ising model (AN-NNI)  5 - 7  . They showed numerically that the ground states are infinitely degenerate on the square lattices if J 1 = J 2 or J 1 < J 2  7   . On the other hand, the ground states were shown to be unique on the honeycomb lattice  8  . These results suggest that the ground states might be nondegenerate even though their exact forms remain unknown so far. \n \n In this Letter, we investigate the ground states of the ANNNI model using Monte Carlo simulations. First, we confirm that the ground states are indeed infinitely degenerate on the squarelattice ANNNI models. Then, we examine whether these ground states are unique or not. Finally, we discuss how the ground states change depending on the values of J 1 / J 2 .\n \n Ground States of the Square-Lattice",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Floating Phase in 2D ANNNI Model . Abstract : We explore the floating stage in the two - dimensional anisotropic closest - neighbor Ising model ( ANNNI ) .We see that there is no floating transition for J1 = J2 , but it appears when J1 > J2 and vanished at some critical value of J1 / J2 . The switch between the ordered state and the floating stage belongs to the universality category of the three - state Potts model with first - order transition .In addition we show that the ground states are degenerate on the square lattice if J1 = J2 or J1 < J2 . This result suggests that the ground states may be non - degenerate even though they have not been determined yet .Introduction It has been known since the work by Wannier 1 that the ground states of the spin - 1 / 2 Heisenberg antiferromagnet on an endless square lattice are infinitely degenerate . However , this fact does not necessarily mean that all possible configurations can emerge as ground states 2 .For instance , the ground states of the one - dimensional chain are distinct although its energy spectrum is continuous 3 , while those of the two - dimensional triangular - lattice Heisenberg antiferromagnet are doubly degenerate 4 . Recently , various papers studied the ground states of the two - dimensional anisotropic closest neighbor Ising model ( AN - NNI ) 5 - 7 .They showed numerically that the ground states are infinitely degenerate on the square lattices if J 1 = J 2 or J 1 < J 2 7 . On the other hand , the ground states were shown to be unique on the honeycomb lattice 8 .These data suggest that the ground states could be nondegenerate even though their exact forms remain uncertain so far . In this Letter , we investigate the ground states of the ANNNI theory using Monte Carlo simulations .First , we prove that the ground states are indeed infinitely degenerate on the squarelattice ANNNI models . Then , we investigate whether these ground states are distinct or not .Finally , we talk how the ground states change based on the values of J 1 / J 2 . Ground States of the Square - Lattice",
        "rewrite_text": "**Title: Floating Phase in the 2D ANNNI Model**\n\n**Abstract:** This study delves into the floating phase of the two-dimensional anisotropic nearest-neighbor Ising model (ANNNI). Our findings indicate that a floating transition does not occur when the coupling constants are equal (J1 = J2); however, such a transition emerges when J1 exceeds J2, ultimately disappearing at a specific critical ratio of J1 to J2. The transition between the ordered phase and the floating state is characterized by a first-order transition, aligning with the universality class of the three-state Potts model. Furthermore, we demonstrate that the ground states on a square lattice exhibit degeneracy when J1 equals J2 or when J1 is less than J2. This observation raises the possibility that ground states may be non-degenerate, although their precise configurations remain to be fully elucidated.\n\nThe concept of ground state degeneracy has been well-established, particularly in the context of the spin-1/2 Heisenberg antiferromagnet on an infinite square lattice, as initially noted by Wannier. However, this degeneracy does not imply that all configurations can serve as ground states. For example, in one-dimensional chains, ground states are distinct despite a continuous energy spectrum, while in two-dimensional triangular-lattice Heisenberg antiferromagnets, the ground states are known to be doubly degenerate. Recent investigations into the ground states of the two-dimensional ANNNI model have revealed that they are infinitely degenerate on square lattices when J1 equals J2 or when J1 is less than J2. Conversely, studies have shown that ground states on honeycomb lattices are unique. This body of work suggests that while the exact forms of the ground states remain uncertain, they could potentially be non-degenerate. In this letter, we employ Monte Carlo simulations to further explore the ground states of the ANNNI model, confirming their infinite degeneracy on square lattices and examining the distinctiveness of these states as a function of the J1/J2 ratio.",
        "ori-fast-z-score": -3.433758534669933,
        "water-fast-z-score": 1.9295276424754644,
        "rewrite-fast-z-score": -1.6728567220186852
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Holography in Three-dimensional Kerr-de Sitter Space with a Gravitational Chern-Simons Term .\nAbstract:\nWe study the holographic entanglement entropy for three-dimensional de Sitter space with gravitational Chern-Simons term by using the replica trick and the covariant phase-space method. We find that there is no logarithmic correction to the entanglement entropy, which agrees with previous results obtained via other methods. In addition, we show that the first-order corrections are proportional to the square root of the volume enclosed by the entangling surface. Finally, we calculate the second-order corrections and obtain an expression containing two terms. One of them has been previously found in Ref.  Phys. Rev. D 98 (2018) 084011  while another one is new. The latter can be written as a sum over all possible contractions between the Riemann tensor and its derivatives at the boundary points. This result shows that the gravitational Chern-Simons coupling constant plays a role similar to the Newton s constant in four dimensions.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Holography in Three - dimensional Kerr - de Sitter Space with a Gravitational Chern - Simons Term . Abstract : We research the holographic entanglement entropy for three - dimensional de Sitter space with gravitational Chern - Simons term by using the replica trick and the covariant phase - space method .We see that there is no logarithmic correction to the entanglement entropy , which agrees with previous findings obtained via other methods . In addition , we prove that the first - order corrections are proportional to the square root of the volume enclosed by the entangling surface .Finally , we determine the second - order corrections and find an expression containing two terms . One of them has been previously found in Ref .Phys.Rev.D 98 ( 2018 ) 084011 while another one is new . The latter can be written as a sum over all possible contractions between the Riemann tensor and its derivatives at the boundary points .This result suggests that the gravitational Chern - Simons interaction function holds a role similar to the Newton s constant in four dimensions .",
        "rewrite_text": "In this study, we investigate the holographic entanglement entropy in three-dimensional de Sitter space that incorporates a gravitational Chern-Simons term, employing both the replica trick and the covariant phase-space method. Our analysis reveals that there are no logarithmic corrections to the entanglement entropy, a finding that is consistent with previous results obtained through alternative approaches. Furthermore, we demonstrate that the first-order corrections to the entanglement entropy are directly proportional to the square root of the volume enclosed by the entangling surface. This relationship highlights the geometric nature of entanglement in this context.\n\nAdditionally, we derive the second-order corrections and identify an expression that consists of two distinct terms. One of these terms aligns with results previously reported in the literature (specifically in Phys. Rev. D 98 (2018) 084011), while the other term is novel. The new term can be expressed as a sum over all possible contractions involving the Riemann tensor and its derivatives evaluated at the boundary points. This finding implies that the gravitational Chern-Simons interaction term plays a role analogous to that of Newton's constant in four-dimensional spacetime, suggesting deeper connections between gravitational theories and holographic principles. Overall, our results contribute to the understanding of entanglement entropy in curved spacetimes and the implications of gravitational Chern-Simons terms in the context of holography.",
        "ori-fast-z-score": 0.75,
        "water-fast-z-score": 3.4465617474213164,
        "rewrite-fast-z-score": 1.2792042981336627
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The velocity structure of moving magnetic feature pairs around sunspots: support for the U-loop model .\nAbstract:\nWe analyze the evolution and kinematics of two opposite polarity magnetic features (MPFs) in an active region using high-resolution vector magnetograms obtained by Hinode/SOT-SP. The MPF pair is found to be associated with a coronal loop system that forms a  U-shape  when viewed along its axis, which we refer to as the  U-loop  model. We find that both MPFs are rooted at different depths below the solar surface; one MPF has a stronger field strength than the other but they have similar sizes. Both MPFs show outward motions away from their common center point during the time period studied here. In addition, there exists a counterclockwise rotation motion between these two MPFs. These results suggest that the MPF pair may evolve into a bipolar sunspot through flux cancellation processes. This work was supported by NASA grants NNX08AQ85G and NNG09FA40C.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The velocity pattern of moving magnetic feature pairs around sunspots : support for the U - loop theory . Abstract : We evaluate the evolution and kinematics of two opposite polarity magnetic elements ( MPFs ) in an active region utilizing large - resolution vector magnetograms achieved by Hinode / SOT - SP .The MPF couple is found to be identified with a coronal loop system that forms a U - shape when seen along its axis , which we call to as the U - loop theory . We see that both MPFs are rooted at different depths below the solar surface ; one MPF has a stronger field intensity than the other but they have different shapes .Both MPFs exhibit outward movements away from their common center position during the period period studied here . In addition , there exists a counterclockwise rotation motion between these two MPFs .These data suggest that the MPF couple may grow into a bipolar sunspot through flux cancellation mechanisms . This research was supported by NASA grants NNX08AQ85G and NNG09FA40C .",
        "rewrite_text": "**Title:** The Velocity Pattern of Moving Magnetic Feature Pairs Around Sunspots: Support for the U-Loop Theory\n\n**Abstract:** In this study, we investigate the dynamics and evolution of two oppositely polarized magnetic elements (MPFs) within an active solar region, utilizing high-resolution vector magnetograms obtained from the Hinode Solar Optical Telescope - Spectro-Polarimeter (SOT-SP). Our findings reveal that the pair of MPFs is associated with a coronal loop structure that exhibits a U-shaped configuration when viewed along its axis, which we refer to as the U-loop theory. Notably, the two MPFs are anchored at varying depths beneath the solar surface; one MPF displays a higher magnetic field intensity compared to the other, despite their differing morphological characteristics. Throughout the observation period, both MPFs demonstrate outward motion from their shared central point, indicating a dynamic interaction. Additionally, we observe a counterclockwise rotational motion between the two MPFs, further highlighting their complex kinematic behavior. These observations suggest that the interaction between the MPF pair may lead to the formation of a bipolar sunspot through mechanisms of magnetic flux cancellation. This research contributes to our understanding of solar magnetic activity and its implications for sunspot formation. The work was supported by NASA grants NNX08AQ85G and NNG09FA40C, underscoring the importance of collaborative funding in advancing solar physics research.",
        "ori-fast-z-score": -1.2535663410560174,
        "water-fast-z-score": 4.98201895023622,
        "rewrite-fast-z-score": -0.105999788000636
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Massive enhancement of electron-phonon coupling in doped graphene by an electronic singularity .\nAbstract:\nWe report on the observation of strong electron-phonon interaction (EPI) in heavily boron-doped epitaxial graphene grown on SiC(0001). The EPI is found to be enhanced by more than one order of magnitude compared with that observed for undoped graphene, which can be attributed to the formation of a Dirac point at high doping levels and its associated van Hove singularity near the Fermi level. This finding suggests that the EPI in doped graphene may be tunable over several orders of magnitude through chemical or electrostatic gating. \n \n Graphene has attracted considerable attention because it exhibits unique physical properties such as extremely high carrier mobility1-5 and very large optical nonlinearities6-8. However, these remarkable properties are often accompanied by weak interactions between electrons and phonons9-11, which limit their applications in high-speed electronics12-14 and optoelectronics15-17. In this work we demonstrate that the EPI in heavily boron-doping epitaxial graphene grown by thermal decomposition of SiC18-20 can be significantly enhanced due to the presence of a Dirac point21-23 and its associated van Hov singularity24-26 near the Fermi energy EF. We show that the EPI increases rapidly when the Fermi level crosses the van Hove singularity, resulting in a giant increase in the electron-phonon scattering rate. Our results suggest that the EPI in graphene could be controlled electrically via chemical or electrostatic gated27-30, thereby opening up new avenues towards novel devices based on graphene. \nGraphene is known to have extremely high carrier mobilities1-4 but relatively small electron-phonon couplings5-9. These two competing effects determine the performance of graphene-based electronic and optoelectronic devices10-12. For example, the low EPI leads to slow relaxation rates13-15 and thus limits the operation speed of graphene transistors14-16. On the other hand, the high mobility makes graphene attractive for use in high-speed electronics17-19 and ultrafast photodetectors20. Therefore, there exists great interest in developing methods to enhance the EPI while maintaining the high mobility31",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Massive enhancement of electron - phonon coupling in doped graphene by an electronic singularity . Abstract : We report on the observation of strong ion - phonon interaction ( EPI ) in heavily boron - doped epitaxial graphene grown on SiC ( 0001 ) .The EPI is found to be enhanced by more than one order of magnitude compared with that detected for undoped graphene , which can be due to the formation of a Dirac zone at high doping rates and its associated van Hove singularity near the Fermi level . This found shows that the EPI in doped graphene may be tunable over numerous orders of magnitude through chemical or electrostatic gating .Graphene has drew substantial scrutiny because it displays unusual physical properties such as extremely high carrier mobility1 - 5 and very huge optical nonlinearities6 - 8 . However , these unique properties are often accompanied by weak interactions between electrons and phonons9 - 11 , which limit their applications in high - speed electronics12 - 14 and optoelectronics15 - 17 .In this research we prove that the EPI in heavily boron - doping epitaxial graphene grown by mechanical transformation of SiC18 - 20 can be greatly enhanced owing to the presence of a Dirac point21 - 23 and its associated van Hov singularity24 - 26 near the Fermi energy EF . We suggest that the EPI increases quickly when the Fermi level crosses the van Hove singularity , leading in a giant increase in the electron - phonon absorption rate .Our results show that the EPI in graphene might be governed electrically via chemical or electrostatic gated27 - 30 , thereby introducing up new avenues towards new materials based on graphene . Graphene is known to have extremely high carrier mobilities1 - 4 but fairly little electron - phonon couplings5 - 9 .These two different factors affect the performance of graphene - based electronic and optoelectronic devices10 - 12 . For instance , the poor EPI contributes to slow relaxation rates13 - 15 and therefore decreases the operation rate of graphene transistors14 - 16 .On the other hand , the high mobility makes graphene suitable for use in high - speed electronics17 - 19 and ultrafast photodetectors20 . Therefore , there exists much interest in developing means to enhance the EPI while maintaining the high mobility31",
        "rewrite_text": "We present a comprehensive study on the significant enhancement of electron-phonon interaction (EPI) in heavily boron-doped epitaxial graphene, synthesized on SiC (0001). Our findings reveal that the EPI in this doped graphene is more than an order of magnitude greater than that observed in undoped graphene. This remarkable increase is attributed to the emergence of a Dirac zone at elevated doping levels, which leads to a van Hove singularity situated close to the Fermi level. This discovery indicates that the EPI in doped graphene can be finely tuned across multiple orders of magnitude through methods such as chemical or electrostatic gating. \n\nGraphene has garnered significant attention due to its exceptional physical characteristics, including extraordinarily high carrier mobility and substantial optical nonlinearities. However, these advantageous properties are often counterbalanced by relatively weak electron-phonon interactions, which can hinder the material's applicability in high-speed electronics and optoelectronics. In our research, we demonstrate that the EPI in heavily boron-doped epitaxial graphene, produced via mechanical transformation of SiC, can be significantly amplified due to the presence of a Dirac point and its corresponding van Hove singularity near the Fermi energy (EF). \n\nWe propose that the EPI experiences a rapid increase as the Fermi level crosses the van Hove singularity, resulting in a dramatic rise in the electron-phonon absorption rate. Our results suggest that the EPI in graphene can be effectively controlled through electrical means, such as chemical or electrostatic gating, paving the way for the development of novel graphene-based materials. While graphene is recognized for its high carrier mobility, it typically exhibits limited electron-phonon coupling, which impacts the performance of graphene-based electronic and optoelectronic devices. The weak EPI contributes to slower relaxation rates, thereby reducing the operational efficiency of graphene transistors. Conversely, the high mobility of graphene positions it as an ideal candidate for high-speed electronics and ultrafast photodetectors. Consequently, there is considerable interest in strategies to enhance the EPI while preserving the material's high mobility.",
        "ori-fast-z-score": -0.5980503604017327,
        "water-fast-z-score": 7.661308776828737,
        "rewrite-fast-z-score": 1.6352596350653539
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Superradiance and stimulated scattering in SNR 1987A .\nAbstract:\nWe present the results of our analysis of the radio emission from supernova remnant (SNR) 1987A, which is dominated by two bright sources located at opposite sides of the shell.  The first source has been identified with the pulsar wind nebula powered by the newly born neutron star; its spectrum peaks around 1 GHz and extends to about 10 GHz. The second source was discovered recently as an extended feature that appears to be associated with the reverse shock front produced during the explosion.  Its spectrum peaks between 5 and 8 GHz and it shows no evidence for spectral turnover up to 20 GHz. We interpret this source as synchrotron radiation emitted by relativistic electrons accelerated behind the forward shock wave driven into the circumstellar medium by the expanding blast wave generated by the supernova explosion. In addition we find evidence for another component of emission peaking near 3 GHz whose origin remains unclear. Finally, we report on the detection of circularly polarized emission from both components.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Superradiance and stimulated scattering in SNR 1987A . Abstract : We present the conclusion of our analysis of the radio emission from supernova remnant ( SNR ) 1987A , which is dominated by two bright sources located at adjacent sides of the shell .The first source has been detected with the pulsar wind nebula powered by the newly born neutron star ; its spectrum peaks around 1 GHz and ranges to about 10 GHz . The second source was noted recently as an extended structure that seems to be involved with the reverse shock front produced during the explosion .Its spectrum peaks between 5 and 8 GHz and it displays no evidence for spectral turnover up to 20 GHz . We interpret this source as synchrotron emission generated by relativistic electrons accelerated behind the back shock wave pushed into the circumstellar medium by the increasing blast wave produced by the supernova explosion .In addition we find proof for another component of emission peaking near 3 GHz whose source remains unsure . Finally , we paper on the diagnosis of circularly polarized emission from both components .",
        "rewrite_text": "We provide a comprehensive analysis of the radio emissions from the supernova remnant (SNR) 1987A, focusing on two prominent sources situated on opposite sides of the remnant's shell. The first source is associated with a pulsar wind nebula, which is energized by the newly formed neutron star. This source exhibits a spectral peak around 1 GHz, extending up to approximately 10 GHz. The second source, recently identified as an extended structure, appears to be linked to the reverse shock front generated during the supernova explosion. Its spectrum peaks between 5 and 8 GHz, and notably, it shows no signs of spectral turnover even at frequencies reaching 20 GHz. We interpret this second source as synchrotron radiation produced by relativistic electrons that are accelerated behind the back shock wave, which is being driven into the circumstellar medium by the expanding blast wave from the supernova event. Furthermore, our analysis reveals evidence of an additional emission component that peaks around 3 GHz, although the origin of this emission remains uncertain. Lastly, we discuss the detection of circularly polarized emissions from both sources, which adds another layer of complexity to our understanding of the radio characteristics of SNR 1987A. This study enhances our knowledge of the physical processes occurring in supernova remnants and their interactions with the surrounding medium, providing valuable insights into the mechanisms of superradiance and stimulated scattering in such astrophysical phenomena.",
        "ori-fast-z-score": -0.22086305214969307,
        "water-fast-z-score": 4.858987147293248,
        "rewrite-fast-z-score": -0.4926646390821466
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Host Galaxy Bulge Predictors of Supermassive Black Hole Mass .\nAbstract:\nWe present the results of an analysis aimed at identifying correlations between supermassive black hole (SMBH) mass and host galaxy properties, using data for a sample of galaxies with SMBH masses determined by reverberation mapping techniques. We find that bulge luminosity is strongly correlated to SMBH mass in both early- and late-type hosts. The correlation strengthens when we consider only those objects whose central regions are dominated by old stellar populations. In addition, we show that there exists a strong correlation between SMBH mass and the velocity dispersion of stars within the bulges of their host galaxies. These relationships can be used as predictors of SMBH mass based on observations of the host galaxy alone. This work was supported by NASA grant NAG5-10842. We thank J. Krolik for providing us with his model predictions prior to publication. \n \n Key words: black holes - galaxies: active",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Host Galaxy Bulge Predictors of Supermassive Black Hole Mass . Abstract : We present the conclusion of an assessment aimed at identifying correlations between supermassive black hole ( SMBH ) mass and host galaxy structures , using data for a sample of stars with SMBH masses determined by reverberation mapping methods .We see that bulge luminosity is strongly correlated to SMBH weight in both late - and mid - class hosts . The relationship strengthens when we treat only those objects whose central regions are dominated by ancient stars populations .In addition , we find that there exists a large correlation between SMBH mass and the velocity dispersion of stars within the bulges of their host galaxies . These relationships can be used as predictors of SMBH mass based on observations of the host star alone .This project was supported by NASA loan NAG5 - 10842 . We praise J . Krolik for providing us with his model predictions earlier to publication .Key words : dark holes - galaxies : active",
        "rewrite_text": "Title: Host Galaxy Bulge Predictors of Supermassive Black Hole Mass\n\nAbstract: In this study, we investigate the relationships between the mass of supermassive black holes (SMBHs) and the structural characteristics of their host galaxies. Utilizing a dataset comprising stars with SMBH masses established through reverberation mapping techniques, we aim to elucidate the underlying correlations. Our findings reveal a robust correlation between bulge luminosity and SMBH mass across both late-type and intermediate-type galaxies. Notably, this correlation becomes even more pronounced when we focus exclusively on galaxies whose central regions are predominantly populated by older stellar populations. Furthermore, we observe a significant relationship between SMBH mass and the velocity dispersion of stars within the bulges of their respective host galaxies. These established correlations provide valuable predictive tools for estimating SMBH mass based solely on observational data of the host galaxy's stellar properties. This research was made possible through the support of NASA grant NAG5-10842, and we extend our gratitude to J. Krolik for sharing his model predictions prior to their publication. Our findings contribute to the understanding of the interplay between galaxy morphology and the growth of supermassive black holes, highlighting the importance of host galaxy characteristics in astrophysical studies. \n\nKeywords: supermassive black holes, galaxies, bulge luminosity, velocity dispersion, stellar populations.",
        "ori-fast-z-score": -1.162476387438193,
        "water-fast-z-score": 3.9000674757995495,
        "rewrite-fast-z-score": -0.7181848464596079
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Evolutionary Optimisation Methods for Template Based Image Registration .\nAbstract:\nImage registration is an important problem in medical imaging and computer vision, where the goal is to find a transformation that aligns two or more images taken at different times and/or by different sensors. In this work we present evolutionary optimisation methods for template based image registration problems. We consider both rigid and non-rigid transformations between images. The proposed algorithms are tested on synthetic data as well as real world datasets including brain MRI scans and CT angiography (CTA) volumes. Our results show that our approach outperforms state-of-the-art techniques in terms of accuracy while being computationally efficient. This research was supported by EPSRC grant EP/N014560/1. Keywords: Evolutionary Computation, Registration, Non-Rigid Transformation, Rigid Transformation, Brain Imaging, Computer Vision. 1 Introduction Image registration is one of the most fundamental tasks in many areas such as medical imaging  1  , remote sensing  2  , video processing  3  , etc., which aims to find a spatial transformation T that maps each point x ∈ Ω1 =  0, 1 d into its corresponding location y = Tx ∈ Ω2 =  0, 1 d in another image I(y). Here d denotes the dimension of the space. For example, if T1 and T2 denote two consecutive time points in a dynamic sequence of images then finding the optimal transformation T would allow us to track the movement of objects over time  4  . Similarly, if S1 and S2 represent two views of the same scene captured using cameras with slightly differing orientations then registering these images will help us fuse information across multiple viewpoints  5  .\nIn recent years there has been significant interest in developing fast and accurate registration algorithms  6  -  8  . However, despite considerable progress made towards solving this challenging problem  9  -  11  , it remains unsolved due to several factors including large number of degrees of freedom involved  12  , presence of noise  13  , partial occlusions  14  , lack of feature correspondence  15  , etc..",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Evolutionary Optimisation Methods for Template Based Image Registration . Abstract : Image registration is an important challenge in medical imaging and computer vision , where the objective is to find a transformation that aligns two or more images took at different times and / or by various sensors .In this research we present evolutionary optimisation methods for template based image registration problems . We consider both stiff and non - flexible transformations between images .The proposed methods are tested on synthetic information as well as real life datasets including brain MRI scans and CT angiography ( CTA ) volumes . Our results show that our approach outperforms state - of - the - art methods in terms of precision while being computationally effective .This research was supported by EPSRC grant EP / N014560 / 1 . Keywords : Evolutionary Computation , Registration , Non - Rigid Transformation , Rigid Transformation , Brain Imaging , Computer Vision .1 Introduction Image registration is one of the most important functions in different areas such as hospital photography 1 , remote sensing 2 , video editing 3 , etc . , which aims to find a spatial mapping T that mapped each point x ∈ Ω1 = 0 , 1 d into its corresponding location y = Tx ∈ Ω2 = 0 , 1 d in another image I ( y ) . Here d indicates the dimension of the space .For instance , if T1 and T2 denote two consecutive time points in a dynamic sequence of pictures then finding the ideal conversion T would enable us to track the movement of items over time 4 . Similarly , if S1 and S2 represent two perspectives of the same scene captured using cameras with significantly differing orientations then registering these images will assist us fuse information across multiple viewpoints 5 .In past decades there has been significant interest in implementing fast and precise registered methods 6 - 8 . However , despite considerable progress made towards solving this controversial question 9 - 11 , it remains unsolved due to several considerations including huge amount of degrees of autonomy involved 12 , presence of noise 13 , partial occlusions 14 , absence of feature relations 15 , etc . .",
        "rewrite_text": "**Title:** Evolutionary Optimization Methods for Template-Based Image Registration\n\n**Abstract:** Image registration is a critical challenge in the fields of medical imaging and computer vision, focusing on the alignment of two or more images captured at different times or by various sensors. This study introduces evolutionary optimization techniques specifically designed for template-based image registration tasks. We explore both rigid and non-rigid transformations to facilitate the alignment of images. Our proposed methods have been rigorously evaluated using both synthetic datasets and real-world applications, including brain MRI scans and CT angiography (CTA) volumes. The findings indicate that our approach significantly surpasses existing state-of-the-art methods in terms of accuracy while maintaining computational efficiency. This research was made possible through the support of the EPSRC grant EP/N014560/1. \n\n**Keywords:** Evolutionary Computation, Image Registration, Non-Rigid Transformation, Rigid Transformation, Brain Imaging, Computer Vision.\n\n**1 Introduction:** Image registration plays a vital role across various domains, including hospital imaging, remote sensing, and video editing. The primary goal is to establish a spatial mapping T that aligns each point x ∈ Ω1 = [0, 1]^d with its corresponding location y = Tx ∈ Ω2 = [0, 1]^d in another image I(y), where d represents the spatial dimension. For example, if T1 and T2 denote two consecutive time points in a dynamic image sequence, determining the optimal transformation T allows for the tracking of object movement over time. Similarly, if S1 and S2 represent two views of the same scene captured from significantly different camera angles, effective registration will enable the integration of information from multiple perspectives. Over the past few decades, there has been a growing interest in developing fast and accurate registration methods. Despite substantial advancements in addressing this complex issue, challenges persist due to factors such as the high degrees of freedom involved, the presence of noise, partial occlusions, and the lack of feature correspondences.",
        "ori-fast-z-score": 1.1430952132988164,
        "water-fast-z-score": 8.110396013138502,
        "rewrite-fast-z-score": 0.5734623443633283
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Critical mass of bacterial populations in a generalized Keller-Segel model. Analogy with the Chandrasekhar limiting mass of white dwarf stars .\nAbstract:\nWe study the dynamics of a population density u(x, t) governed by a nonlinear diffusion equation coupled to an ordinary differential equation for the concentration c(t). The latter describes the evolution of a chemical substance produced and consumed by bacteria at rates proportional respectively to u and cu. We show that if the initial data is sufficiently concentrated then there exists a unique global solution which converges exponentially fast as time goes to infinity towards a stationary state consisting of two Dirac masses located on opposite sides of the origin. This result can be seen as a generalization of the classical theorem due to J. L. Lions concerning the existence of solutions to the steady-state problem associated with the celebrated Keller-Segel system describing chemotaxis phenomena. In addition we prove that this convergence takes place even when the total number of bacteria initially present is less than the critical value predicted by the theory developed by S. Chandrasekhar in order to explain the stability of white dwarfs against gravitational collapse.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Critical mass of bacterial populations in a generalized Keller - Segel theory . Analogy with the Chandrasekhar limiting mass of white dwarf stars .Abstract : We explore the dynamics of a population density u ( x , t ) determined by a nonlinear diffusion equation coupled to an ordinary differential equation for the concentration c ( t ) . The latter studies the evolution of a chemical material created and consumed by bacteria at levels equal respectively to u and cu .We see that if the first data is sufficiently concentrated then there exists a unique worldwide solve which converges exponentially rapidly as time go to infinity towards a stationary state composed of two Dirac masses situated on opposite sides of the origin . This result can be saw as a generalization of the classical principle attributed to J . L . Lions relating the existence of solutions to the stable - state issue associated with the celebrated Keller - Segel system describing chemotaxis phenomena .In addition we prove that this convergence took place even when the total number of bacteria initially present is less than the key value predicted by the principle created by S . Chandrasekhar in order to explain the stability of white dwarfs against gravitational failure .",
        "rewrite_text": "Title: Critical Mass of Bacterial Populations in a Generalized Keller-Segel Theory: An Analogy with the Chandrasekhar Limiting Mass of White Dwarf Stars\n\nAbstract: This study investigates the dynamics of bacterial populations through a nonlinear diffusion equation, which is coupled with an ordinary differential equation that models the concentration of a chemical substance produced and consumed by the bacteria. The population density, denoted as u(x, t), and the concentration, c(t), evolve in a manner that reflects the interactions between the bacteria and the chemical material. Our findings indicate that when the initial concentration of bacteria is sufficiently high, there exists a unique global solution that converges exponentially to a stationary state characterized by two Dirac masses located at opposite ends of the origin. This outcome can be interpreted as a generalization of the classical principle proposed by J. L. Lions, which relates the existence of solutions to the stability of states in the well-known Keller-Segel model that describes chemotaxis phenomena. Furthermore, we demonstrate that this convergence occurs even when the initial total number of bacteria is below the critical threshold predicted by S. Chandrasekhar's principle, which addresses the stability of white dwarfs against gravitational collapse. This analogy highlights the parallels between bacterial population dynamics and astrophysical phenomena, suggesting that concepts from one field can provide valuable insights into the other. Our results contribute to a deeper understanding of the conditions necessary for stability in biological systems, drawing intriguing connections to established theories in astrophysics.",
        "ori-fast-z-score": -1.2686700948330931,
        "water-fast-z-score": 4.467914966843415,
        "rewrite-fast-z-score": 0.9534625892455924
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Detection of CFIRB with AKARI/FIS Deep Observations .\nAbstract:\nWe report the detection of cosmic far-infrared background (CFIRB) fluctuations using deep observations made by the Far Infrared Surveyor (FIS) onboard Akari satellite at 65 and 90 micron bands in the Lockman Hole field, which is one of the most sensitive fields for detecting extragalactic sources. The FIS has two photometric channels; N60 band covers 60 to 120 microns while WIDE-S channel covers 50 to 100 microns. We used data taken during the period between February 2005 and March 2007. After removing bright point-like objects detected by Spitzer/MIPS 24 micron survey, we performed aperture photometry on all remaining pixels within an area of 1 deg2 centered around the Lockman hole. To estimate the contribution from Galactic cirrus emission, we subtracted the median value of each pixel after applying a 3 sigma clipping method. Then we calculated power spectrum density (PSD) of the residual map. By fitting the PSD with a single power law model, we obtained the best-fit slope as -2.1 ± 0.3 at 65 micron and -1.9 ± 0.4 at 90 micron. These slopes are consistent with those expected from clustering properties of infrared galaxies.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Detection of CFIRB with AKARI / FIS Deep Observations . Abstract : We report the observation of cosmic far - infrared background ( CFIRB ) fluctuations using deep surveys made by the Far Infrared Surveyor ( FIS ) onboard Akari spacecraft at 65 and 90 micron bands in the Lockman Hole field , which is one of the most important areas for detecting extragalactic sources .The FIS has two photometric channels ; N60 band encompasses 60 to 120 microns while WIDE - S channel encompasses 50 to 100 microns . We utilized information taken during the period between February 2005 and March 2007 .After removing bright point - like items detected by Spitzer / MIPS 24 micron search , we performed aperture photometry on all remaining pixels within an area of 1 deg2 centered around the Lockman hole . To estimate the contribution from Galactic cirrus emission , we subtracted the median value of each pixel after applying a 3 sigma clipping method .Then we calculated power spectrum density ( PSD ) of the residual map . By fitting the PSD with a single power law description , we derived the best - fitting curve as - 2 . 1 ± 0 . 3 at 65 micron and - 1 . 9 ± 0 . 4 at 90 micron .These slopes are compatible with those expected from clustering qualities of infrared galaxies .",
        "rewrite_text": "In this study, we present our findings on the fluctuations of the cosmic far-infrared background (CFIRB) as observed through deep surveys conducted by the Far Infrared Surveyor (FIS) aboard the Akari spacecraft. Our observations focused on the Lockman Hole field, a significant region for the detection of extragalactic sources, utilizing the 65 and 90 micron bands. The FIS instrument features two photometric channels: the N60 band, which covers wavelengths from 60 to 120 microns, and the WIDE-S channel, spanning 50 to 100 microns. The data analyzed were collected between February 2005 and March 2007. \n\nTo refine our analysis, we first eliminated bright point-like sources identified through Spitzer's MIPS 24 micron survey. We then conducted aperture photometry on the remaining pixels within a 1 square degree area centered on the Lockman Hole. To account for contributions from Galactic cirrus emission, we applied a 3 sigma clipping method to subtract the median value of each pixel. Following this, we computed the power spectrum density (PSD) of the residual map. \n\nOur fitting of the PSD with a single power law yielded best-fit slopes of -2.1 ± 0.3 at 65 microns and -1.9 ± 0.4 at 90 microns. These results are consistent with theoretical expectations based on the clustering properties of infrared galaxies. This work enhances our understanding of CFIRB fluctuations and provides valuable insights into the underlying processes contributing to the infrared background, highlighting the importance of deep infrared observations in astrophysical research.",
        "ori-fast-z-score": -0.23249527748763857,
        "water-fast-z-score": 3.670651741928988,
        "rewrite-fast-z-score": 2.177598558933893
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Census of Baryons in Galaxy Clusters and Groups .\nAbstract:\nWe present the results of an all-sky survey for galaxy clusters using data obtained with the Sunyaev-Zel dovich effect (SZE) by the Planck satellite, complemented at low redshifts by X-ray observations made with XMM-Newton and Chandra satellites. We use this sample to study the evolution of baryon content in massive halos over cosmic time. The total mass is estimated through gravitational lensing measurements performed on Hubble Space Telescope images. Our main findings are as follows:  1. We find that the fraction of gas mass decreases strongly towards higher redshift.  2. At z < 0.5 we measure fgas = Mgas/Mtot = 0.11 ± 0.01(stat.) ±0.02(sys.), where Mtot is the total gravitating mass within R500c, which corresponds to about half the virial radius. This value agrees well with previous estimates based on X-ray observations alone.  3. For our full cluster sample spanning the range 0.1 < z < 1.3, we obtain fgas = 0.09 ± 0.01(±stat.).",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Census of Baryons in Galaxy Clusters and Groups . Abstract : We present the results of an all - skies study for galaxy galaxies using data acquired with the Sunyaev - Zel dovich impact ( SZE ) by the Planck satellite , complemented at low redshifts by X - ray observations made with XMM - Newton and Chandra satellites .We use this specimen to study the evolution of baryon concentration in massive halos over cosmic time . The total mass is calculated through gravity lensing observations performed on Hubble Space Telescope images .Our main results are as follows : 1 . We see that the fraction of gas mass decreases strongly towards higher redshift .2 . At z < 0 . 5 we measure fgas = Mgas / Mtot = 0 . 11 ± 0 . 01 ( stat . )±0 . 02 ( sys . ) , where Mtot is the total gravitating mass within R500c , which equals to about half the virial radius .This value agrees well with previous accounts based on X - ray observations alone . 3 .For our entire cluster sample spanning the range 0 . 1 < z < 1 . 3 , we obtain fgas = 0 . 09 ± 0 . 01 ( ±stat . ) .",
        "rewrite_text": "We present the findings of a comprehensive all-sky survey focused on galaxy clusters and groups, utilizing data obtained from the Sunyaev-Zel'dovich effect (SZE) as measured by the Planck satellite. This dataset is further enhanced by X-ray observations from the XMM-Newton and Chandra satellites, particularly for lower redshift ranges. Our analysis aims to investigate the evolution of baryon concentration within massive halos across cosmic time. To determine the total mass of these structures, we employ gravitational lensing measurements derived from images captured by the Hubble Space Telescope. \n\nOur key findings reveal significant trends in baryon distribution: firstly, we observe a pronounced decline in the gas mass fraction as redshift increases. Specifically, for redshifts less than 0.5, we calculate the gas mass fraction, defined as fgas = Mgas / Mtot, to be 0.11 ± 0.01 (statistical) ± 0.02 (systematic), where Mtot represents the total gravitating mass within R500c, approximately half of the virial radius. This measurement aligns closely with previous estimates derived solely from X-ray data. Additionally, when considering our entire cluster sample, which spans redshifts from 0.1 to 1.3, we find an average gas mass fraction of fgas = 0.09 ± 0.01 (statistical). These results contribute to our understanding of baryonic matter in the universe and its evolution over time, highlighting the importance of multi-wavelength observations in astrophysical research.",
        "ori-fast-z-score": 1.9896995023342199,
        "water-fast-z-score": 5.7350162126103985,
        "rewrite-fast-z-score": 0.6831300510639733
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Infrared High-Resolution Spectroscopy of Post-AGB Circumstellar Disks. I. HR 4049 - The Winnowing Flow Observed? .\nAbstract:\nWe present the first results on infrared high-resolution spectroscopy (HRS) of post-AGB disks, obtained with CRIRES/VLT and NIRSPEC/Keck II. We find that the disk around HR 4049 is dominated by emission lines originating in an extended region at temperatures between 1000-2000 K. This temperature range corresponds to the expected location of dust grains which are being evaporated due to stellar radiation pressure. In addition we detect several absorption features which can be attributed to gas-phase molecules such as CO, H2O, OH, NH3, CH4, C2H2, SiO, HCN, CCH, CN, H2S, SO2, CS, and FeH. These observations provide new insights into the physical conditions within these objects. They also show how important it will be for future studies to combine spatially resolved information about the distribution of molecular species with detailed spectroscopic data. \n \n Keywords: circumstellar disk",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Infrared High - Resolution Spectroscopy of Post - AGB Circumstellar Disks . I . HR 4049 - The Winnowing Flow Observed ?. Abstract : We report the first findings on infrared high - resolution spectroscopy ( HRS ) of post - AGB disks , obtained with CRIRES / VLT and NIRSPEC / Keck II .We see that the disk around HR 4049 is dominated by absorption tracks originating in an extended region at temperatures between 1000 - 2000 K . This temperature range corresponds to the expected location of dust grains which are being evaporated due to stellar radiation stress . In addition we locate many absorption elements which can be due to gas - phase particles such as CO , H2O , OH , NH3 , CH4 , C2H2 , SiO , HCN , CCH , CN , H2S , SO2 , CS , and FeH .These measurements give novel knowledge into the physical conditions within these objects . They also demonstrate how important it will be for future research to mix spatially resolved data about the distribution of biological species with comprehensive spectroscopic data .Keywords: circumstellar disk",
        "rewrite_text": "**Title:** Infrared High-Resolution Spectroscopy of Post-AGB Circumstellar Disks: I. HR 4049 - Observations of the Winnowing Flow\n\n**Abstract:** In this study, we present groundbreaking results from our infrared high-resolution spectroscopy (HRS) investigations of post-Asymptotic Giant Branch (post-AGB) circumstellar disks, utilizing the capabilities of CRIRES at the VLT and NIRSPEC at Keck II. Our observations of the disk surrounding the star HR 4049 reveal a predominance of absorption features that are traced back to an extensive region characterized by temperatures ranging from 1000 to 2000 K. This temperature range aligns with the expected zones where dust grains are subjected to evaporation due to the intense radiation pressure from the central star. Furthermore, we identify a variety of absorption lines that can be attributed to gas-phase molecules, including CO, H2O, OH, NH3, CH4, C2H2, SiO, HCN, CCH, CN, H2S, SO2, CS, and FeH. These findings provide new insights into the physical conditions prevailing within these circumstellar environments. Our results underscore the necessity for future studies to integrate spatially resolved observations of the distribution of various chemical species with detailed spectroscopic data. This approach will enhance our understanding of the complex interactions and processes occurring in post-AGB disks, ultimately contributing to the broader knowledge of stellar evolution and the chemical enrichment of the interstellar medium. \n\n**Keywords:** circumstellar disk, post-AGB, infrared spectroscopy, HR 4049, molecular absorption, stellar evolution.",
        "ori-fast-z-score": -0.9847319278346618,
        "water-fast-z-score": 5.0089472186085136,
        "rewrite-fast-z-score": 0.4082482904638631
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  New possible properties of atomic nuclei investigated by non linear methods: Fractal and recurrence quantification analysis .\nAbstract:\nThe present work is devoted to the investigation of new possible nuclear properties using nonlinear methods, namely fractal dimension (FD) and recurrence quantification analysis (RQA). The FD was calculated for different mass number A in order to study its dependence on the system size. It has been found that there exists an optimal value of the scaling parameter which provides the best results for each nucleus separately. This fact indicates that the structure of the studied systems can be characterized as multifractals with non-trivial behavior. In addition we have shown that the obtained values are very close to those predicted by the percolation theory. We also applied RQA method to investigate the temporal evolution of the considered systems. Our calculations show that the complexity of the time series increases with increasing mass number A. Moreover it turns out that this increase follows closely the predictions made within the framework of the Random Matrix Theory. Finally we have compared our results with experimental data available for some light nuclei.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : New possible characteristics of nuclear nuclei investigated by non discrete methods : Fractal and recurrence quantification analysis . Abstract : The present work is devoted to the examination of new possible nuclear properties using nonlinear methods , notably fractal depth ( FD ) and recurrence quantification analysis ( RQA ) .The FD was calculated for different mass quantity A in order to study its dependence on the system size . It has been shown that there exists an appropriate value of the scaling parameter which offers the best results for each particle separately .This fact suggests that the composition of the studied structures can be described as multifractals with non - trivial properties . In addition we have shown that the achieved values are very close to those predicted by the percolation theory .We also used RQA approach to examine the temporal evolution of the considered systems . Our calculations show that the complexity of the period series increases with expanding mass quantity A .Moreover it turns out that this increase follows carefully the assumptions done within the framework of the Random Matrix Theory . Finally we have linked our findings with observation information available for some light nuclei .",
        "rewrite_text": "This study explores novel nuclear properties through the application of nonlinear analytical methods, specifically focusing on fractal depth (FD) and recurrence quantification analysis (RQA). The research investigates how FD varies with different mass numbers (A), aiming to understand its relationship with system size. Our findings indicate the existence of an optimal scaling parameter that yields the most accurate results for individual particles, suggesting that the structures under investigation can be characterized as multifractals exhibiting complex behaviors. Notably, the FD values obtained align closely with predictions made by percolation theory, reinforcing the validity of our approach.\n\nIn addition to FD analysis, we employed RQA to assess the temporal dynamics of the nuclear systems studied. The results reveal that as the mass number A increases, the complexity of the periodic series also rises. This complexity growth adheres to the principles outlined in Random Matrix Theory, indicating a deeper underlying structure in the behavior of nuclear nuclei. Furthermore, we have correlated our theoretical findings with empirical data available for certain light nuclei, providing a comprehensive view of the nuclear characteristics examined. Overall, this work contributes to the understanding of nuclear properties through innovative analytical techniques, paving the way for future research in the field.",
        "ori-fast-z-score": -0.6123724356957946,
        "water-fast-z-score": 6.262945776223707,
        "rewrite-fast-z-score": 0.6831300510639733
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Deep Spectroscopy of Systematically Surveyed Extended Lyman-alpha Sources at z~3-5 .\nAbstract:\nWe present deep spectroscopic observations for a sample of extended Lyα sources (ELAS) selected in the redshift range 3<z<5 using narrowband imaging data obtained with Subaru/Suprime-Cam and VLT/VIMOS. The ELAS are characterized by their large angular sizes, which correspond to physical scales ranging between 1 kpc and 10 Mpc. We find that most of them show spatially resolved emission line profiles consisting of multiple components separated by several hundred km s-1. These results suggest that they may be powered by AGNs or starburst galaxies surrounded by dense gas clouds. In addition, we detect strong absorption lines associated with intervening systems along some sightlines toward these objects. This suggests that our targets are located behind massive halos of neutral hydrogen. Finally, we report on an interesting object showing both spatially unresolved and resolved emission line features. Our study provides new insights into the nature of high-redshift star-forming regions.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Deep Spectroscopy of Systematically Surveyed Extended Lyman - alpha Sources at z ~ 3 - 5 . Abstract : We report shallow spectroscopic observations for a sample of extended Lyα sources ( ELAS ) selected in the redshift region 3 < z < 5 utilizing narrowband scanning data acquired with Subaru / Suprime - Cam and VLT / VIMOS .The ELAS are marked by their large angular sizes , which correspond to physical scales ranging between 1 kpc and 10 Mpc . We see that most of them show spatially resolved radiation path characteristics consisting of multiple components apart by many hundred km s - 1 .These data suggest that they may be powered by AGNs or starburst objects surrounded by dense gas clouds . In addition , we perceive strong absorption patterns correlated with intervening systems along some sightlines toward these objects .This implies that our objectives are situated behind massive halos of neutral hydrogen . Finally , we publish on an interesting object displaying both spatially unresolved and resolved radiation path features .Our study provides new information into the nature of high - redshift star - creating areas .",
        "rewrite_text": "We present our findings from shallow spectroscopic observations of a sample of extended Lyman-alpha sources (ELAS) identified within the redshift range of 3 < z < 5. These observations were conducted using narrowband scanning data obtained from Subaru/Suprime-Cam and VLT/VIMOS. The ELAS are characterized by their significant angular sizes, which correspond to physical dimensions ranging from 1 kpc to 10 Mpc. Our analysis reveals that a majority of these sources exhibit spatially resolved radiation characteristics, displaying multiple components that are separated by several hundred kilometers per second. This behavior suggests that the ELAS may be energized by active galactic nuclei (AGNs) or starburst galaxies that are enveloped by dense clouds of gas. Furthermore, we observe pronounced absorption features associated with intervening systems along certain sightlines toward these sources, indicating that they are positioned behind substantial halos of neutral hydrogen. Notably, we also report on a particularly intriguing object that exhibits both spatially unresolved and resolved radiation characteristics. This study contributes valuable insights into the nature of high-redshift star-forming regions, enhancing our understanding of the processes that govern star formation in the early universe. Our findings underscore the complexity of the environments surrounding these ELAS and their potential implications for the evolution of galaxies at high redshifts.",
        "ori-fast-z-score": -0.9761870601839528,
        "water-fast-z-score": 5.965587590013045,
        "rewrite-fast-z-score": 1.0101525445522108
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Mathematica script for harmonic oscillator nuclear matrix elements arising in semileptonic electroweak interactions .\nAbstract:\nWe present an open-source, free software package that computes the one-loop Feynman diagrams contributing to the hadronic weak decay amplitudes of heavy quarks into light leptons and pions.  The code is written entirely within the Wolfram Language using only built-in functions.   It can be downloaded at https://github.com/mharrison/harmonic-oscillator-matrix-elements/tree/master/Mathematica_code. We have tested our results against those obtained with FeynArts 3.9  1  , FormCalc 8  2  , LoopTools 2  3  , and QCDLoop  4  . \nThe program has been used to compute the leading-order (LO) contributions to the helicity amplitudes for B meson decays into muons or electrons plus pions  5  .  These are required inputs for the calculation of the branching fractions  6  and CP asymmetries  7, 8  of these processes.  In addition, we provide the LO contribution to the pion form factor  9  .\nOur approach uses the method developed by M. Neubert  10   which exploits the fact that the relevant loop integrals can all be expressed as linear combinations of products of two-point scalar functions.  This allows us to reduce the number of independent integrations needed to evaluate each diagram to three.  Our implementation also includes several improvements over previous calculations  11, 12  :   1)  We use the exact expression for the propagators of massive fermions instead...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Mathematica text for harmonic oscillator atomic matrix elements arose in semileptonic electroweak interactions . Abstract : We produce an open - source , free software tool that computes the one - loop Feynman diagrams causing to the hadronic weak decay amplitudes of light quarks into light leptons and pions .The language is composed exclusively within the Wolfram Language using only built - in functions . It can be downloaded at https : / / github . com / mharrison / harmonic - oscillator - vector - elements / tree / master / Mathematica _ code .We have tested our findings against those achieved with FeynArts 3 . 9 1 , FormCalc 8 2 , LoopTools 2 3 , and QCDLoop 4 . The project has been used to compute the led - order ( LO ) contributions to the helicity amplitudes for B meson decays into muons or electrons plus pions 5 .These are required inputs for the determination of the branching fractions 6 and CP asymmetries 7 , 8 of these systems . In addition , we provide the LO contribution to the pion form parameter 9 .Our solution uses the method developed by M . Neubert 10 which exploits the fact that the relevant loop integrals can all be interpreted as continuous sequences of products of two - point scalar functions . This enables us to reduce the number of separate integrations needed to analyze each diagram to three .Our formulation also contains several additions over past calculations 11 , 12 : 1 ) We use the exact definition for the propagators of large fermions instead . . .",
        "rewrite_text": "**Title:** A Mathematica Text for Harmonic Oscillator Atomic Matrix Elements in Semileptonic Electroweak Interactions\n\n**Abstract:** In this article, we present an open-source software tool designed to compute one-loop Feynman diagrams that contribute to the hadronic weak decay amplitudes of light quarks into light leptons and pions. Developed entirely in the Wolfram Language, the tool utilizes only built-in functions, ensuring ease of use and accessibility. The software is available for download at https://github.com/mharrison/harmonic-oscillator-vector-elements/tree/master/Mathematica_code. We have validated our results against established computational tools, including FeynArts 3.9.1, FormCalc 8.2, LoopTools 2.3, and QCDLoop 4, confirming the accuracy of our approach.\n\nOur project focuses on calculating the leading-order (LO) contributions to the helicity amplitudes for B meson decays into either muons or electrons accompanied by pions. These calculations are crucial for determining the branching fractions and CP asymmetries of these decay processes. Additionally, we provide the LO contribution to the pion form factor, which is essential for understanding the dynamics of these interactions.\n\nOur methodology is based on the framework developed by M. Neubert, which allows us to interpret the relevant loop integrals as continuous sequences of products of two-point scalar functions. This innovative approach significantly reduces the number of integrations required for each diagram to just three, streamlining the computational process. Furthermore, our formulation incorporates several enhancements over previous calculations, including the use of the exact definitions for the propagators of heavy fermions, thereby improving the precision of our results. This work not only contributes to the field of particle physics but also serves as a valuable resource for researchers seeking to explore the intricacies of semileptonic electroweak interactions.",
        "ori-fast-z-score": -2.3849888978799783,
        "water-fast-z-score": 4.538253483538691,
        "rewrite-fast-z-score": -0.09407208683835973
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Comptonization and the Spectra of Accretion-Powered X-Ray Pulsars .\nAbstract:\nWe present an analysis of the X-ray spectra of accreting pulsar systems using a model for Comptonization in hot plasma that includes relativistic effects, such as Doppler boosting and gravitational redshift.  We find that this model provides good fits to the observed pulse profiles and energy-dependent pulse fractions of several sources with luminosities ranging over four orders of magnitude (from 1033 erg s-1 to 1029 erg s-1). The inferred values of the electron temperature are consistent with those expected if these objects radiate at their Eddington limits. In addition, we show how our results can be used to estimate the mass of neutron stars in these systems by comparing the measured fluxes with theoretical predictions based on models of the emission mechanisms. Finally, we discuss possible applications of our method to other types of compact objects. Subject headings: Black holes -accretion disks -X-rays: Sources -Pulsar",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Comptonization and the Spectra of Accretion - Powered X - Ray Pulsars . Abstract : We present an assessment of the X - ray spectra of accreting pulsar systems employing a theory for Comptonization in heated plasma that encompasses relativistic effects , such as Doppler boosting and gravity redshift .We see that this description provides good fits to the known pulse profiles and energy - dependent pulse fractions of several sources with luminosities ranging over four orders of magnitude ( from 1033 erg s - 1 to 1029 erg s - 1 ) . The inferred values of the electron pressure are compatible with those expected if these objects radiate at their Eddington limits .In addition , we explain how our findings can be used to estimate the mass of neutron stars in these systems by comparing the measured fluxes with theoretical estimates based on estimates of the emission mechanisms . Finally , we explain possible applied of our technique to other types of compact objects .Subject headings : Black holes - accretion disks - X - radiation : Sources - Pulsar",
        "rewrite_text": "We provide a comprehensive analysis of the X-ray spectra associated with accreting pulsar systems, utilizing a theoretical framework for Comptonization in heated plasma that incorporates relativistic phenomena, including Doppler boosting and gravitational redshift. Our study demonstrates that this model yields excellent fits to the observed pulse profiles and energy-dependent pulse fractions across a diverse range of sources, with luminosities spanning four orders of magnitude, from \\(10^{33} \\, \\text{erg s}^{-1}\\) to \\(10^{29} \\, \\text{erg s}^{-1}\\). The derived electron pressure values align well with expectations for objects radiating at their Eddington limits, suggesting a robust connection between our theoretical predictions and the physical conditions present in these systems. Furthermore, we discuss the implications of our results for estimating the masses of neutron stars within these pulsar systems. By comparing the measured fluxes to theoretical predictions based on various emission mechanisms, we provide a method for mass estimation that could enhance our understanding of these compact objects. Additionally, we explore potential applications of our Comptonization technique to other types of compact astrophysical entities, broadening the relevance of our findings beyond just accreting pulsars. This work not only contributes to the existing body of knowledge regarding X-ray pulsars but also opens avenues for future research into the characteristics and behaviors of various compact objects in the universe. The insights gained from this study could significantly advance our understanding of the complex interplay between accretion processes and the resulting emission spectra in high-energy astrophysics. \n\nSubject headings: Black holes, accretion disks, X-radiation: Sources, Pulsars.",
        "ori-fast-z-score": -0.7171371656006361,
        "water-fast-z-score": 4.153735803678487,
        "rewrite-fast-z-score": -0.09090909090909091
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Evidence for the Evolution of Young Early-Type Galaxies in the GOODS/CDF-S Field .\nAbstract:\nWe present new spectroscopic observations of galaxies at z ~ 1.5-2.0 selected by their UVJ colors and optical morphologies, obtained with VLT/VIMOS on the Very Large Telescope (VLT). We find that these objects are mostly early-type galaxies showing signs of recent star formation activity. The observed properties suggest that they may be progenitors of local massive elliptical galaxies. These results provide further evidence supporting the scenario where most massive galaxies grow through mergers between gas-rich disk systems during the first half of cosmic time. This is an Open Access article distributed under the terms of the Creative Commons Attribution License 2.0, which permits unrestricted use, distribution, and reproduction in any medium provided the original work is properly cited. \n \n Keywords: galaxy evolution; merger remnants; young ellipticals; CDF-S field \n \n Massive galaxies evolve rapidly over cosmic time as a result of merging processes involving smaller companions. In particular, it has been suggested that many of today s brightest cluster galaxies were formed via major mergers of two or more gas-rich disks at redshifts around one to three  1  . However, direct observational evidence for this process remains elusive because of the difficulty in identifying such events at high redshift  2  .\n \nIn order to study the physical mechanisms driving galaxy growth we have carried out deep spectroscopy of galaxies at intermediate redshifts using the VLT-VIMOS spectrograph  3  . Our sample consists of about 100 galaxies selected based on their ultraviolet J (UVJ) color  4  , morphological type  5  , and apparent magnitude  6  . Most of them show strong emission lines characteristic of active star-forming regions  7, 8  . Their stellar masses range from 10^10 M_sol to 10^11 M_sol  9  . \n\n\nThe main goal of our project was to identify possible candidates for progenitor populations of local massive elliptical/S0 galaxies  10  . To do so, we used several selection criteria designed to select galaxies with similar characteristics to those found among nearby massive spheroids  11  : \n\n\n1. Morphological type: all targets must",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Evidence for the Evolution of Young Early - Type Galaxies in the GOODS / CDF - S Field . Abstract : We report new spectroscopic observations of galaxies at z ~ 1 . 5 - 2 . 0 selected by their UVJ colors and imaging morphologies , obtained with VLT / VIMOS on the Very Large Telescope ( VLT ) .We see that these objects are mostly early - class stars displaying signs of recent star formation activity . The observed properties suggest that they may be progenitors of local powerful elliptical galaxies .These data provide further evidence supporting the scenario where most gigantic galaxies grow through mergers between gas - rich disk systems during the first half of cosmic time . This is an Open Access article distributed under the terms of the Creative Commons Attribution License 2 . 0 , which allows unrestricted use , distribution , and reproduction in any medium provided the original book is properly cited .Keywords : universe progression ; collision remnants ; young ellipticals ; CDF - S field Massive stars develop rapidly over cosmic time as a outcome of combining processes involving smaller companions . In particular , it has been proposed that several of today s brightest cluster clusters were created via large mergers of two or more gas - rich disks at redshifts around one to three 1 .However , direct observational evidence for this process remains elusive because of the difficulty in identifying such events at high redshift 2 . In order to study the physical mechanisms governing star development we have carried out deep spectroscopy of clusters at intermediate redshifts using the VLT - VIMOS spectrograph 3 .Our specimen consists of about 100 galaxies chose based on their ultraviolet J ( UVJ ) color 4 , morphological class 5 , and apparent magnitude 6 . Most of them show strong absorption patterns characteristic of active star - creating areas 7 , 8 .Their stellar masses range from 10 ^ 10 M _ sol to 10 ^ 11 M _ sol 9 . The main goal of our work was to identify possible candidates for progenitor populations of local heavy elliptical / S0 galaxies 10 .To do so , we using numerous selection categories modified to select galaxies with similar characteristics to those detected among neighboring massive spheroids 11 : 1 . Morphological type : all targets must",
        "rewrite_text": "Title: Evidence for the Evolution of Young Early-Type Galaxies in the GOODS/CDF-S Field\n\nAbstract: In this study, we present new spectroscopic observations of galaxies at redshifts approximately between 1.5 and 2.0, selected based on their UVJ colors and imaging morphologies, utilizing the VLT/VIMOS on the Very Large Telescope (VLT). Our findings indicate that these galaxies predominantly consist of early-type stars exhibiting signs of recent star formation activity. The characteristics observed in these galaxies suggest that they may serve as progenitors to the powerful elliptical galaxies found in the local universe. This research adds to the growing body of evidence supporting the hypothesis that the majority of massive galaxies evolve through mergers of gas-rich disk systems during the early stages of cosmic history. \n\nMassive stars undergo rapid development over cosmic time as a result of merging processes involving smaller companions. It has been suggested that many of the brightest galaxy clusters observed today were formed through significant mergers of two or more gas-rich disks at redshifts between one and three. However, direct observational evidence for these events has been challenging to obtain due to the difficulties associated with identifying such occurrences at high redshift. To investigate the physical mechanisms driving star formation, we conducted deep spectroscopy of galaxies at intermediate redshifts using the VLT-VIMOS spectrograph. Our sample comprises approximately 100 galaxies selected based on their ultraviolet J (UVJ) colors, morphological classifications, and apparent magnitudes. \n\nThe majority of these galaxies exhibit strong absorption features indicative of active star formation regions. Their stellar masses range from 10^10 M_sun to 10^11 M_sun. The primary objective of our research was to identify potential candidates for the progenitor populations of local massive elliptical and S0 galaxies. To achieve this, we employed various selection criteria tailored to isolate galaxies with characteristics akin to those observed in nearby massive spheroids. This work contributes valuable insights into the evolutionary pathways of early-type galaxies and their role in the formation of massive structures in the universe. \n\nKeywords: universe evolution; merger remnants; young elliptical galaxies; CDF-S field.",
        "ori-fast-z-score": 0.07832604499879574,
        "water-fast-z-score": 7.084340391869858,
        "rewrite-fast-z-score": 1.1607348488012053
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Controlled collisions of a single atom and ion guided by movable trapping potentials .\nAbstract:\nWe report on the controlled collision between an individual trapped atom and a singly charged ion in a Paul trap, where both particles are confined to different regions of space separated by several micrometers. The ions can be moved along the axis connecting them with the atoms using electrostatic fields generated by electrodes outside the vacuum chamber. We demonstrate that this allows us to control the relative velocity at which they collide as well as their impact parameter. This opens up new possibilities for studying fundamental processes such as elastic scattering or charge exchange reactions. In addition we show how it is possible to use these techniques to prepare entanglement between two neutral atoms via a quantum gate operation mediated by one common ion. Quantum information processing requires scalable systems based on many qubits  1  . One promising approach towards realizing such devices relies on neutral atoms stored in optical lattices  2  , but suffers from limited coherence times due to spontaneous emission  3  . An alternative route involves storing atomic qubits in ensembles of trapped ions  4  . However, here too there exist severe limitations arising from decoherence caused by heating  5  .\nIn order to overcome these difficulties, hybrid approaches have been proposed  6  combining advantages of both schemes  7, 8  . Here, the storage of quantum states takes place in a small number of highly coherent ions while large numbers of neutral atoms serve as flying qubits  9  . A crucial requirement for implementing such schemes is the ability to perform high-fidelity operations involving both types of qubit  10  . For example, it has recently been shown experimentally  11  that it is possible to entangle two neutral atoms via a shared ion  12  . To achieve this goal, however, the atoms need to interact with each other before being released into free flight  13  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Controlled collisions of a single atom and ion guided by movable trapping potentials . Abstract : We report on the regulated collision between an individual captured molecule and a singly charged particle in a Paul trap , where both particles are localized to different regions of space separated by many micrometers .The ions can be moved along the axis linking them with the atoms using electrostatic fields generated by electrodes outside the vacuum chamber . We suggest that this enables us to affect the relative velocity at which they collide as also as their impact parameter .This opens up new possibilities for studying basic processes such as elastic scattering or charge transfer reactions . In addition we show how it is easy to use these mechanisms to make entanglement between two neutral ions via a quantum gate action mediated by one common ion .Quantum electronic processing requires scalable systems relying on numerous qubits 1 . One promising path towards developing such machines depends on neutral compounds contained in laser lattices 2 , but suffers from reduced coherence times due to spontaneous emission 3 .An alternative approach requires storing atomic qubits in ensembles of trapped ions 4 . However , here too there remain considerable restrictions arose from decoherence caused by heating 5 .In try to overcome these problems , hybrid approaches have been proposed 6 combining characteristics of both schemes 7 , 8 . Here , the storage of quantum states takes place in a small number of highly coherent electrons while small numbers of neutral particles serve as flying qubits 9 .A crucial requirement for employing such schemes is the ability to conduct high - fidelity operations involving both types of qubit 10 . For instance , it has recently been shown experimentally 11 that it is possible to entangle two neutral ions via a shared ion 12 .To achieve this goal , however , the atoms need to interact with each other before being transferred into free flight 13 .",
        "rewrite_text": "In this article, we present a detailed investigation into the controlled collisions between a single trapped atom and a singly charged ion within a Paul trap. The two particles are spatially separated by several micrometers, allowing for precise manipulation of their positions. By utilizing electrostatic fields generated by external electrodes, we can move the ions along the axis that connects them to the atoms. This capability enables us to finely tune the relative velocity of their collisions as well as their impact parameters, thereby facilitating a deeper exploration of fundamental processes such as elastic scattering and charge transfer reactions.\n\nMoreover, we demonstrate that these controlled interactions can be harnessed to create entanglement between two neutral ions through a quantum gate operation mediated by a shared ion. The development of scalable quantum electronic systems is contingent upon the effective use of numerous qubits. While one promising avenue involves the use of neutral atoms in laser lattices, this approach is hindered by reduced coherence times due to spontaneous emission. Alternatively, storing atomic qubits in ensembles of trapped ions presents its own challenges, particularly decoherence resulting from heating effects.\n\nTo address these limitations, hybrid strategies have been proposed that integrate the advantages of both neutral atoms and trapped ions. In such schemes, quantum states are stored in a limited number of highly coherent ions, while neutral particles act as flying qubits. A critical aspect of these hybrid systems is the ability to perform high-fidelity operations involving both qubit types. Recent experimental findings have confirmed the feasibility of entangling two neutral ions via a shared ion, highlighting the necessity for interaction between the atoms prior to their transition into free flight. This research paves the way for advancements in quantum information processing and the development of more robust quantum systems.",
        "ori-fast-z-score": 1.182165609358651,
        "water-fast-z-score": 8.590403428006198,
        "rewrite-fast-z-score": 1.7962924780409972
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Two Energy Release Processes for CMEs: MHD Catastrophe and Magnetic Reconnection .\nAbstract:\nWe propose that the energy release process in coronal mass ejections (CMEs) is due to two different physical mechanisms, magnetic reconnection and catastrophic loss of equilibrium driven by ideal magnetohydrodynamic (MHD) instabilities.  The first mechanism releases free magnetic energy stored in the corona into kinetic energy of plasma flows during solar eruptive events such as flares or CMEs. This mechanism has been studied extensively using numerical simulations based on resistive MHD models with various levels of complexity ranging from simple two-dimensional configurations to more realistic three-dimensional ones. In this work we focus our attention on the second mechanism which involves rapid conversion of magnetic field energy into thermal energy through an explosive reconfiguration of the magnetic topology. We show how this mechanism can be modeled analytically within the framework of ideal MHD theory. Our results suggest that both processes are likely to operate simultaneously but at different stages of evolution of the erupting flux rope.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Two Energy Release Processes for CMEs : MHD Catastrophe and Magnetic Reconnection . Abstract : We suggest that the power release process in coronal mass ejections ( CMEs ) is due to two different mechanical pathways , magnetic reconnection and catastrophic loss of equilibrium caused by ideal magnetohydrodynamic ( MHD ) instabilities .The first mechanism transfers loose magnetic energy stored in the corona into kinetic power of plasma flows during thermal eruptive events such as flares or CMEs . This mechanism has been studied frequently using numerical simulations based on resistive MHD models with various amounts of complexity including from complicated two - dimensional shapes to more realistic three - dimensional ones .In this study we focus our focus on the second process which includes rapid transfer of magnetic field energy into heat power through an explosive reconfiguration of the magnetic topology . We see how this mechanism can be described analytically within the framework of perfect MHD physics .Our results show that both processes are likely to work simultaneously but at different stages of evolved of the erupting flux rope .",
        "rewrite_text": "In this article, we explore the dual mechanisms responsible for energy release during coronal mass ejections (CMEs), specifically focusing on magnetic reconnection and the catastrophic loss of equilibrium driven by ideal magnetohydrodynamic (MHD) instabilities. We propose that these two distinct pathways contribute to the overall power release observed in CMEs. The first mechanism, magnetic reconnection, facilitates the conversion of loosely stored magnetic energy in the solar corona into the kinetic energy of plasma flows, particularly during thermal eruptive events such as solar flares and CMEs. This process has been extensively investigated through numerical simulations employing resistive MHD models, which vary in complexity from intricate two-dimensional configurations to more realistic three-dimensional representations. \n\nIn this study, we place particular emphasis on the second mechanism, which involves the rapid conversion of magnetic field energy into thermal energy through a dramatic reconfiguration of the magnetic topology. We demonstrate that this explosive process can be effectively described using analytical methods grounded in perfect MHD theory. Our findings indicate that both energy release mechanisms are likely to operate concurrently, albeit at different stages of the evolution of the erupting flux rope. This duality in energy release processes enhances our understanding of the dynamics involved in CMEs and their associated phenomena, providing insights into the intricate interplay between magnetic forces and plasma behavior in the solar atmosphere. Overall, our research contributes to the broader understanding of solar eruptive events and their implications for space weather and solar-terrestrial interactions.",
        "ori-fast-z-score": 0.32539568672798425,
        "water-fast-z-score": 6.901297485020058,
        "rewrite-fast-z-score": 0.3563483225498992
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Simulation of ASTROD I test mass charging due to solar energetic particles .\nAbstract:\nThe Astrodynamics Space Test of Relativity using Optical Devices (ASTROD) mission is proposed as an experiment for testing the gravitational theory in space, and it will be launched into orbit around L2 point by 2020.  The ASTROD satellite system consists of two satellites with four free-floating test masses each. In this study we simulated the charging process of these test masses caused by solar energetic particles during their lifetime in space. We used the SPENVIS software package developed at the European Space Research and Technology Centre (ESTEC). Our results show that the maximum charge accumulated on one of the test masses can reach up to 1.5 nC after 10 years  exposure time. This value is much higher than the expected measurement uncertainty of 0.1 nC. Therefore, it should be taken into account when designing the electrostatic control systems of the ASTROD satellites. Keywords: Charging, Solar Energetic Particles, ASTROD",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Simulation of ASTROD I test mass charging caused to solar active grains . Abstract : The Astrodynamics Space Test of Relativity using Optical Devices ( ASTROD ) flight is proposed as an project for studying the gravitational concept in space , and it will be launched into orbit around L2 point by 2020 .The ASTROD satellite system contains of two spacecraft with four free - floating study masses each . In this study we modeled the charging cycle of these test masses induced by sun energetic particles during their lifetime in space .We utilized the SPENVIS software suite created at the European Space Research and Technology Centre ( ESTEC ) . Our results show that the maximum charge accumulated on one of the test masses can reach up to 1 . 5 nC after 10 years exposure time .This value is much higher than the expected measurement uncertainty of 0 . 1 nC . Therefore , it should be taken into consideration when designing the electrostatic control networks of the ASTROD satellites .Keywords: Charging, Solar Energetic Particles, ASTROD",
        "rewrite_text": "**Title:** Simulation of ASTROD I Test Mass Charging Induced by Solar Active Particles\n\n**Abstract:** The Astrodynamics Space Test of Relativity using Optical Devices (ASTROD) is an ambitious project aimed at investigating gravitational phenomena in a space environment. Scheduled for launch around the L2 point in 2020, the ASTROD mission comprises two spacecraft, each equipped with four free-floating test masses designed for precise measurements. This study focuses on modeling the charging behavior of these test masses, which is influenced by solar energetic particles (SEPs) throughout their operational lifespan in space. To conduct our simulations, we employed the SPENVIS software suite developed by the European Space Research and Technology Centre (ESTEC), which allows for detailed analysis of space weather effects on spacecraft systems. Our findings reveal that the maximum charge accumulated on one of the test masses can reach as high as 1.5 nC after a decade of exposure to solar activity. This charge level significantly exceeds the anticipated measurement uncertainty of 0.1 nC, highlighting the potential impact of electrostatic charging on the mission's precision measurements. Consequently, these results underscore the necessity of incorporating effective electrostatic control mechanisms in the design of the ASTROD satellite systems to mitigate the effects of charging and ensure the integrity of the scientific data collected. This research not only contributes to the understanding of charging phenomena in space but also provides critical insights for future missions involving sensitive measurement apparatus in environments influenced by solar activity. \n\n**Keywords:** Charging, Solar Energetic Particles, ASTROD",
        "ori-fast-z-score": 0.9299811099505543,
        "water-fast-z-score": 5.347391382215687,
        "rewrite-fast-z-score": 0.1873171623163388
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The inner jet of radio galaxy NGC 315 as observed with Chandra and the VLA .\nAbstract:\nWe present new observations of the nuclear region in the nearby radio galaxy NGC315, made using the Chandra X-ray Observatory (CXO) and the Very Large Array (VLA). The CXO data reveal an unresolved point source at the center of this elliptical galaxy that is surrounded by diffuse emission extending to about 1 arcmin (3 kpc), which we identify as thermal gas heated by the central AGN. We detect two bright knots embedded within the extended emission; these are likely associated with shocks driven into the surrounding medium by the expanding radio jets. Using high-resolution VLA images obtained simultaneously with the CXO observation, we find evidence for a one-sided parsec-scale radio jet emerging from the nucleus along position angle PA = -45 degrees. This jet has been previously detected on larger scales out to several kiloparsecs. In addition, there appears to be another fainter component of the radio jet located further south-west than the main knot.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The inner jet of radio star NGC 315 as observed with Chandra and the VLA . Abstract : We report new studies of the atomic region in the nearby radio galaxy NGC315 , made using the Chandra X - ray Observatory ( CXO ) and the Very Large Array ( VLA ) .The CXO data reveal an unresolved point origin at the center of this elliptical galaxy that is surrounded by diffuse emission stretching to about 1 arcmin ( 3 kpc ) , which we identify as heat gas heated by the main AGN . We detect two faint knots embedded within the extended emission ; these are likely correlated with shocks driven into the adjacent medium by the increasing radio jets .Using long - resolution VLA images obtained simultaneously with the CXO study , we find proof for a one - sided parsec - scale radio jet developing from the nucleus along position angle PA = - 45 degrees . This jet has been previously observed on larger scales out to several kiloparsecs .In addition , there seems to be another fainter component of the radio jet located further west - west than the main knot .",
        "rewrite_text": "We present new findings from our investigation of the atomic region in the nearby radio galaxy NGC 315, utilizing data from the Chandra X-ray Observatory (CXO) and the Very Large Array (VLA). Our analysis of the CXO data uncovers an unresolved point source at the center of this elliptical galaxy, which is enveloped by diffuse emission extending approximately 1 arcminute (or 3 kiloparsecs) outward. This diffuse emission is interpreted as thermal gas that has been heated by the active galactic nucleus (AGN) at the galaxy's core. Within this extended emission, we identify two faint knots, which are likely associated with shocks generated by the expanding radio jets interacting with the surrounding medium. \n\nSimultaneously acquired high-resolution VLA images provide compelling evidence for a one-sided radio jet emerging from the nucleus, oriented at a position angle of -45 degrees. This jet has been previously documented on larger scales, extending several kiloparsecs from the nucleus. Furthermore, our observations suggest the presence of an additional, fainter component of the radio jet located further to the west of the primary knot. These findings contribute to our understanding of the complex dynamics at play in NGC 315, highlighting the interactions between the AGN, the surrounding medium, and the radio jets. The combination of X-ray and radio observations offers a comprehensive view of the energetic processes occurring in this radio galaxy, paving the way for future studies aimed at unraveling the mechanisms driving jet formation and evolution in similar astrophysical environments.",
        "ori-fast-z-score": -0.9878783399072131,
        "water-fast-z-score": 3.796283011826483,
        "rewrite-fast-z-score": 1.212256250712408
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Lorentzian and signature changing branes .\nAbstract:\nWe study the dynamics of Lorentzian brane-worlds with time dependent bulk cosmological constant in the context of Randall-Sundrum type II models, where we allow for both positive and negative values of the bulk cosmological constant. We show that there are two branches of solutions corresponding to either an expanding or contracting universe on the brane depending upon whether the bulk cosmological constant is positive or negative respectively. In addition, we find that these solutions can be smoothly connected by a branch of static solutions which correspond to Minkowski space-time on the brane. Finally, we also consider the possibility of having a change of signature across the brane and discuss how this affects our results. The main motivation behind studying braneworld scenarios comes from the fact that they provide us with a natural framework within which one may address some of the fundamental issues related to quantum gravity such as the hierarchy problem  1  . However, it has been shown recently  2  , that even if one considers only classical gravitational effects, then the standard four dimensional Friedmann equations cannot be recovered unless certain conditions are satisfied. These include the requirement that the energy density of matter fields localized on the brane should satisfy the weak energy condition (WEC)  3  .\nIn order to overcome this difficulty, several authors have considered different approaches  4  -  8  . For example, in ref.  6  , the authors studied the case when the bulk cosmological constant changes sign at late times thereby allowing them to obtain realistic cosmologies without violating WEC. On the other hand, in ref.  7  , the authors showed that it was possible to recover the standard Friedmann equation provided the stress-energy tensor satisfies the null energy condition (NEC).",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Lorentzian and signature changing branes . Abstract : We research the dynamics of Lorentzian brane - worlds with time dependent bulk cosmological constant in the context of Randall - Sundrum type II models , where we allow for both negative and negative values of the bulk cosmological coefficient .We see that there are two branches of solutions equivalent to either an increasing or contracting worlds on the brane depending upon whether the bulk cosmological coefficient is positive or negative respectively . In addition , we find that these solutions can be smoothly connected by a branch of static solutions which coincide to Minkowski space - time on the brane .Finally , we also consider the prospect of having a change of signature across the brane and consider how this affects our findings . The main motivation behind studying braneworld situations comes from the fact that they give us with a natural framework within which one may address some of the fundamental issues related to quantum gravitational such as the ranking problem 1 .However , it has been shown recently 2 , that even if one considers only classical gravity phenomena , then the standard four dimensional Friedmann equations never be recovered unless various conditions are fulfilled . These include the requirement that the power concentration of matter fields confined on the brane should satisfy the weak energy condition ( WEC ) 3 .In try to overcome this challenge , various published have treated different methods 4 - 8 . For instance , in ref .6 , the articles studied the case when the bulk cosmological constant shifts sign at late times thus permitting them to obtain realistic cosmologies without violating WEC . On the other hand , in ref .7 , the articles demonstrated that it was easy to regain the standard Friedmann equation provided the strain - energy tensor satisfies the null energy condition ( NEC ) .",
        "rewrite_text": "In this study, we investigate the dynamics of Lorentzian brane-world scenarios characterized by a time-dependent bulk cosmological constant, specifically within the framework of Randall-Sundrum type II models. Our analysis accommodates both positive and negative values of the bulk cosmological constant, revealing two distinct branches of solutions. These branches correspond to either expanding or contracting brane-worlds, contingent upon the sign of the bulk cosmological constant. Notably, we discover that these two branches can be seamlessly connected through a set of static solutions that align with Minkowski spacetime on the brane.\n\nFurthermore, we explore the implications of a potential signature change across the brane and its influence on our results. The motivation for examining braneworld scenarios stems from their ability to provide a natural setting for addressing several fundamental issues in quantum gravity, such as the hierarchy problem. However, recent findings indicate that the standard four-dimensional Friedmann equations cannot be recovered under classical gravity unless specific conditions are met. These conditions include the necessity for the energy density of matter fields confined to the brane to adhere to the weak energy condition (WEC).\n\nTo tackle these challenges, various approaches have been proposed in the literature. For instance, one study examined scenarios where the bulk cosmological constant changes sign at late times, allowing for the derivation of realistic cosmological models without contravening the WEC. Another investigation demonstrated that the standard Friedmann equations could be retrieved if the stress-energy tensor complies with the null energy condition (NEC). Our research contributes to this ongoing discourse by elucidating the dynamics of brane-worlds under varying cosmological conditions and the potential for signature changes, thereby enhancing our understanding of the interplay between brane dynamics and fundamental gravitational theories.",
        "ori-fast-z-score": -0.6810052246069989,
        "water-fast-z-score": 5.979278639572285,
        "rewrite-fast-z-score": -0.9467292624062575
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Modeling transcription factor binding events to DNA using a random walker/jumper representation on a 1D/2D lattice with different affinity sites .\nAbstract:\nTranscription factors (TFs) are proteins that bind specific sequences in the genome and regulate gene expression by recruiting other regulatory molecules or directly modulating RNA polymerase activity. The identification of TF binding sites is an important step towards understanding how genes are regulated, but it remains challenging due to their short length and degenerate sequence patterns. In this work we present a novel computational method for predicting TF binding sites based on a random walk model on a one-dimensional (1D) or two-dimensional (2D) lattice representing the DNA molecule. We show that our approach outperforms state-of-the-art methods when applied to several datasets containing experimentally validated binding sites. Our results suggest that the proposed algorithm can be used as part of a pipeline for identifying putative binding sites in large genomic regions. Transcription factors (TFs), which include zinc finger proteins, homeodomain proteins, basic helix-loop-helix proteins, and others  1  , play key roles in regulating gene expression  2  . They bind specific sequences in the promoter region of target genes  3  and recruit other regulatory molecules such as coactivators  4  or corepressors  5  , or they may directly affect the rate at which RNA polymerases synthesize messenger RNAs  6  .\nThe identification of TF binding sites has been shown to be useful for studying gene regulation  7, 8  . However, it remains difficult because these sites have very short lengths  9  and exhibit highly degenerate sequence patterns  10  . Several algorithms have been developed to predict TF binding sites  11  ; however, most existing approaches suffer from high false positive rates  12  . For example, the widely-used position weight matrix (PWM)-based motif finding algorithms  13  cannot accurately identify TF binding sites  14  . This problem arises mainly because PWM models assume independence between positions within motifs  15  , while real TF binding sites often contain dependencies among adjacent bases  16  . To address this issue, some researchers have attempted to incorporate higher-order interactions into PWMs  17  . Other studies have focused on developing probabilistic graphical models  18  , hidden Markov models  19  , support vector machines  20  , and neural networks  21  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Modeling transcription factor binding events to DNA using a random walker / jumper representation on a 1D / 2D grid with various affinity sites . Abstract : Transcription genes ( TFs ) are proteins that bind particular elements in the genome and modify protein expression by attracting other regulatory compounds or significantly modulating RNA polymerase activity .The identification of TF binding locations is an important process towards knowledge how genes are controlled , but it remains challenging due to their short length and degenerate sequence patterns . In this research we present a new computational technique for predicting TF binding locations based on a random walk simulation on a one - dimensional ( 1D ) or two - dimensional ( 2D ) lattice describing the DNA molecule .We indicate that our approach outperforms state - of - the - art methods when applied to several datasets featuring experimentally validated binding locations . Our results propose that the suggested method can be used as part of a pipeline for finding putative bound sites in large genomic domains .Transcription genes ( TFs ) , which contain zinc thumb proteins , homeodomain proteins , basic helix - loop - helix proteins , and others 1 , play crucial roles in controlling gene transcription 2 . They bind particular motifs in the promoter area of target genes 3 and recruit other regulatory proteins such as coactivators 4 or corepressors 5 , or they may specifically affect the pace at which RNA polymerases synthesize messenger RNAs 6 .The identity of TF binding locations has been shown to be valuable for studying gene regulation 7 , 8 . However , it remains challenging because these sites have very small lengths 9 and exhibit highly degenerate sequence patterns 10 .Several algorithms have been created to predict TF binding places 11 ; however , most existing techniques result from high false positive rates 12 . For instance , the heavily - used position weight matrix ( PWM ) - based sequence finding methods 13 cannot accurately identification TF binding regions 14 .This problem arises chiefly because PWM methods assume independence between positions within motifs 15 , while real TF binding locations often contain dependencies among adjoining bases 16 . To address this question , some researchers have tried to insert greater - order relationships into PWMs 17 .Other studies have concentrated on developing probabilistic graphical theories 18 , hidden Markov models 19 , support vector computers 20 , and neural systems 21 .",
        "rewrite_text": "**Title:** Modeling Transcription Factor Binding Events to DNA Using a Random Walker/Jumper Representation on a 1D/2D Grid with Various Affinity Sites\n\n**Abstract:** Transcription factors (TFs) are essential proteins that interact with specific genomic elements to regulate gene expression by recruiting other regulatory molecules or modulating the activity of RNA polymerase. Identifying the binding sites of TFs is crucial for understanding gene regulation; however, this task is complicated by the short lengths and degenerate sequence patterns of these binding sites. In this study, we introduce a novel computational method for predicting TF binding locations through a random walk simulation on one-dimensional (1D) or two-dimensional (2D) lattices that represent DNA structures. Our findings demonstrate that this approach surpasses existing state-of-the-art techniques when evaluated against multiple datasets containing experimentally validated binding sites. The results suggest that our method can be effectively integrated into a broader pipeline for discovering potential binding sites across extensive genomic regions.\n\nTFs, which include various protein families such as zinc finger proteins, homeodomain proteins, and basic helix-loop-helix proteins, play pivotal roles in the transcriptional regulation of genes. They bind to specific motifs within the promoter regions of target genes, facilitating the recruitment of coactivators or corepressors, and influencing the transcriptional activity of RNA polymerases. The accurate identification of TF binding sites is invaluable for elucidating the mechanisms of gene regulation. However, the inherent challenges arise from the brevity and degeneracy of these sites, leading to high false positive rates in many predictive algorithms. Traditional methods, such as position weight matrix (PWM)-based approaches, often fail to accurately pinpoint TF binding regions due to their assumption of independence among positions within motifs, while actual binding sites frequently exhibit dependencies among adjacent nucleotides. To overcome these limitations, researchers have explored higher-order relationships within PWMs and have developed various advanced methodologies, including probabilistic graphical models, hidden Markov models, support vector machines, and neural networks. Our study contributes to this ongoing effort by providing a robust and effective framework for modeling TF binding events, thereby enhancing our understanding of gene regulation dynamics.",
        "ori-fast-z-score": -2.6870057685088806,
        "water-fast-z-score": 7.79067271041392,
        "rewrite-fast-z-score": -0.6099942813304187
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  High Precision CTE-Measurement of SiC-100 for Cryogenic Space-Telescopes .\nAbstract:\nThe development of large space telescopes requires the use of lightweight materials to reduce launch costs and increase telescope performance in orbit. Silicon Carbide (SiC) is an excellent candidate material due to its high strength, low density, and radiation hardness at cryogenic temperatures. However, it has been shown that SiC exhibits significant changes in thermal expansion with temperature which can lead to dimensional instability during cool-downs or warm-ups. This work presents results on the measurement of the coefficient of thermal expansion (CTE) of SiC-100 over a wide range of temperatures using a novel technique based on laser interferometry. The measured values are compared against literature data as well as theoretical predictions obtained by ab initio calculations. It was found that the experimental measurements agree very well with theory within the uncertainty limits. These results will be used to improve the design of future space missions such as JWST and WFIRST-AFTA.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : High Precision CTE - Measurement of SiC - 100 for Cryogenic Space - Telescopes . Abstract : The construction of large space telescopes demands the using of lightweight materials to reduce rocket expenses and increase telescope performance in orbit .Silicon Carbide ( SiC ) is an excellent candidate material owing to its high strength , low density , and radiation hardness at cryogenic temperatures . However , it has been shown that SiC exhibits significant variations in heat expansion with temperature which can lead to dimensional instability during cool - downs or cold - ups .This research provides findings on the observation of the coefficient of thermal expansion ( CTE ) of SiC - 100 over a broad variety of temperatures using a novel technique focused on laser interferometry . The measured values are compared against literature statistics as well as conceptual predictions produced by ab initio methods .It was shown that the empirical observations coincide very best with principle within the uncertainty restrictions . These data will be used to improve the development of later space missions such as JWST and WFIRST - AFTA .",
        "rewrite_text": "**Title:** High Precision CTE Measurement of SiC-100 for Cryogenic Space Telescopes\n\n**Abstract:** The development of large space telescopes necessitates the use of lightweight materials to minimize launch costs and enhance operational efficiency in orbit. Silicon Carbide (SiC) has emerged as a prime candidate due to its exceptional mechanical strength, low density, and resilience to radiation at cryogenic temperatures. Nevertheless, significant fluctuations in the coefficient of thermal expansion (CTE) of SiC have been documented, which can result in dimensional instability during thermal cycling, such as cooling down or warming up phases. This study presents comprehensive findings on the CTE of SiC-100 across a wide temperature range, utilizing an innovative laser interferometry technique for precise measurements. The results obtained are rigorously compared with existing literature values and theoretical predictions derived from ab initio calculations. Our empirical data demonstrate a strong correlation with theoretical models, falling well within the established uncertainty margins. These findings are crucial for refining the design and performance of future space missions, including the James Webb Space Telescope (JWST) and the Wide Field Infrared Survey Telescope (WFIRST-AFTA). By enhancing our understanding of SiC's thermal properties, this research contributes significantly to the advancement of materials science in the context of cryogenic applications in space exploration. The implications of these results extend beyond immediate applications, potentially influencing the selection and optimization of materials for a variety of future aerospace projects.",
        "ori-fast-z-score": -1.3937462952891333,
        "water-fast-z-score": 5.253351420705195,
        "rewrite-fast-z-score": 0.3849001794597505
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  IGR J17254-3257, a new bursting neutron star .\nAbstract:\nWe report the discovery and analysis of an X-ray transient source IGR J17254-3257 in outburst during June 2009 with INTEGRAL/Swift/BAT. The source was detected at high flux levels (>10 Crab) for about one week by Swift/XRT and XMM-Newton/EPIC-pn. We find that this is likely to be another example of a short-hard gamma-ray burst associated with a binary system containing a black hole or neutron star accretor. A comparison between our results on IGR J17254−3257 and those obtained previously for other similar sources suggests that there may exist two different classes of such systems. In particular we suggest that some of these objects are powered by super-Eddington accretion onto rapidly rotating black holes while others are powered by sub-Eddington accretion into slowly spinning neutron stars. This work has been supported by NASA under contract NAS8-03060.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : IGR J17254 - 3257 , a new bursting neutron star . Abstract : We report the discovery and analysis of an X - ray transient source IGR J17254 - 3257 in outburst during June 2009 with INTEGRAL / Swift / BAT .The source was monitored at high flux levels ( > 10 Crab ) for about one month by Swift / XRT and XMM - Newton / EPIC - pn . We see that this is probably to be another example of a small - hard gamma - ray flare associated with a binary system containing a black hole or neutron galaxy accretor .A comparison between our findings on IGR J17254−3257 and those conducted previously for other similar sources shows that there may exist two different categories of such systems . In particular we indicate that some of these objects are powered by super - Eddington accretion onto swiftly spinning black holes while many are powered by sub - Eddington accretion into quickly spinning neutron galaxies .This project has been supported by NASA under contract NAS8 - 03060 .",
        "rewrite_text": "We present the discovery and detailed analysis of the X-ray transient source IGR J17254-3257, which exhibited an outburst in June 2009, as observed by the INTEGRAL and Swift/BAT missions. During this period, the source was monitored at exceptionally high flux levels, exceeding 10 Crab, for approximately one month using Swift/XRT and XMM-Newton/EPIC-pn instruments. Our observations suggest that IGR J17254-3257 may represent a new instance of a small, hard gamma-ray flare linked to a binary system that likely contains either a black hole or a neutron star as the accreting object. \n\nIn our comparative analysis, we juxtapose the characteristics of IGR J17254-3257 with previously studied similar sources, leading us to propose the existence of two distinct categories of such systems. Notably, we find that some of these objects are driven by super-Eddington accretion onto rapidly rotating black holes, while others are fueled by sub-Eddington accretion onto swiftly spinning neutron stars. This differentiation is crucial for understanding the underlying mechanisms that govern the behavior of these high-energy astrophysical phenomena. \n\nOur research contributes to the broader understanding of transient X-ray sources and their classifications, providing insights into the nature of accretion processes in binary systems. This work has been made possible through the support of NASA under contract NAS8-03060, highlighting the collaborative efforts in advancing our knowledge of the universe's most enigmatic objects.",
        "ori-fast-z-score": -0.601929265428846,
        "water-fast-z-score": 3.8247315498700596,
        "rewrite-fast-z-score": 0.7844645405527362
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Fractionally charged excitations on frustrated lattices .\nAbstract:\nWe study the fractional charge and statistics of elementary excitations in quantum spin systems with frustration, using exact diagonalization techniques for small clusters up to 12 sites. We find that the ground state is always gapped and has no degeneracy. The elementary excitations are fractionally charged fermions or bosons depending on whether the system is antiferromagnetic (AF) or ferromagnetic (F). In AF cases we also observe neutral fermionic excitations which carry zero electric charge but have nontrivial braiding properties. These results can be understood by mapping our models onto effective lattice gauge theories where the elementary excitations correspond to particles carrying flux quanta. Our work provides an explicit example of how fractional charges emerge naturally as topological defects in strongly correlated electronic materials. Introduction:-The discovery of high temperature superconductivity in copper oxide compounds  1  , together with other exotic phenomena such as colossal magnetoresistance  2  , non-Fermi liquid behavior  3  etc., has led to renewed interest in understanding the physics of strongly interacting electrons. One of the most important open questions concerns the nature of the elementary excitations responsible for these novel behaviors  4  . It was suggested early on  5  that the elementary excitations may be described by some kind of collective modes known as spin waves  6  . However it soon became clear  7, 8  that this description fails at low energies due to strong electron correlations. More recently there has been considerable progress towards developing theoretical descriptions based on new concepts like fractionalized quasiparticles  9  , emergent gauge fields  10  , and topological order  11  .\nIn particular, recent experiments  12  suggest that the elementary excitations in the cuprates might indeed be described by some form of fractionalized quasiparticle  13  . This raises many interesting questions about their physical properties including their charge  14  , statistics  15  , and interactions  16  . Unfortunately, despite enormous efforts  17  , a complete microscopic theory describing all these aspects remains elusive  18  . A promising approach involves studying simplified model Hamiltonians  19, 20  whose low-energy limit captures essential features of the original problem  21  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Fractionally charged excitations on frustrated lattices . Abstract : We research the fractional charge and statistics of elementary excitations in particle spin systems with frustration , using accurate diagonalization techniques for little complexes up to 12 locations .We see that the ground state is usually gapped and has no degeneracy . The elementary excitations are fractionally charged fermions or bosons depending on whether the system is antiferromagnetic ( AF ) or ferromagnetic ( F ) .In AF instances we also observe neutral fermionic excitations which carry zero electric current but have nontrivial braiding properties . These results can be understood by map our models onto effective lattice gauge experiments where the elementary excitations relate to ions carrying flux quanta .Our research provides an explicit instance of how fractional charges emerge readily as topological flaws in highly correlated electronic materials . Introduction : - The observation of high heat superconductivity in copper oxide compounds 1 , combined with other exotic processes such as colossal magnetoresistance 2 , non - Fermi solid behavior 3 etc . , has led to renewed interest in understanding the physics of highly correlated atoms .One of the most important open questions concerns the nature of the elementary excitations responsible for these novel behaviors 4 . It was suggested early on 5 that the elementary excitations might be described by some kind of collective modes named as spin beams 6 .However it soon became clear 7 , 8 that this description fails at low energies due to strong electron correlations . More recently there has been substantial development towards developing theoretical descriptions based on new concepts like fractionalized quasiparticles 9 , emergent gauge fields 10 , and topological order 11 .In particular , recent experiments 12 suggest that the elementary excitations in the cuprates might actually be described by some kind of fractionalized quasiparticle 13 . This raises many interesting problems about their physical properties including their charge 14 , statistics 15 , and interactions 16 .Unfortunately , despite enormous efforts 17 , a complete microscopic theory explaining all these details remains elusive 18 . A viable technique requires studying simplified model Hamiltonians 19 , 20 whose low - energy maximum reflects vital features of the first problem 21 .",
        "rewrite_text": "**Title:** Fractionally Charged Excitations on Frustrated Lattices\n\n**Abstract:** In this study, we investigate the properties of fractional charge and statistics associated with elementary excitations in particle spin systems characterized by frustration. Utilizing precise diagonalization methods, we analyze small systems comprising up to 12 sites. Our findings indicate that the ground state is typically gapped and exhibits no degeneracy. The nature of the elementary excitations varies with the magnetic interactions: in antiferromagnetic (AF) systems, excitations manifest as fractionally charged fermions or bosons, while in ferromagnetic (F) systems, they take on different characteristics. Notably, in AF scenarios, we identify neutral fermionic excitations that do not carry electric current yet possess intriguing braiding properties. These observations can be interpreted by mapping our models onto effective lattice gauge theories, where the elementary excitations correspond to particles that carry flux quanta. This research exemplifies how fractional charges can emerge as topological defects in strongly correlated electronic materials.\n\n**Introduction:** The discovery of high-temperature superconductivity in copper oxide compounds, alongside other phenomena such as colossal magnetoresistance and non-Fermi liquid behavior, has sparked renewed interest in the physics of strongly correlated systems. A critical question in this field pertains to the nature of the elementary excitations that give rise to these unusual behaviors. Early theories proposed that these excitations could be described as collective modes, referred to as spin waves. However, it soon became evident that this framework is inadequate at low energies due to the influence of strong electron correlations. Recent advancements have shifted focus towards theoretical models incorporating concepts such as fractionalized quasiparticles, emergent gauge fields, and topological order. Experimental evidence suggests that the excitations in cuprate superconductors may indeed be represented by fractionalized quasiparticles, prompting further inquiry into their physical properties, including charge, statistics, and interactions. Despite extensive research efforts, a comprehensive microscopic theory that accounts for all these aspects remains elusive. To address this challenge, we propose the study of simplified model Hamiltonians that capture essential features of the underlying problem at low energies.",
        "ori-fast-z-score": 0.38461538461538464,
        "water-fast-z-score": 7.056077897395808,
        "rewrite-fast-z-score": -0.23643312187173018
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Computer simulation of model cohesive powders: influence of assembling procedure and contact laws on low consolidation states .\nAbstract:\nThe aim of this work is to study the effect of different assembly procedures, as well as different contact laws between particles in powder compaction processes using computer simulations. The results show that the packing density depends strongly on the applied pressure during the assembly process. In addition, it was found that the densification behavior can be influenced by changing the contact law parameters. Finally, we have shown how the microstructure evolves with increasing pressure for two different types of particle contacts. Powder compaction has been studied extensively over many years due to its importance in various industrial applications such as pharmaceutical tablets or metal parts produced via hot pressing techniques  1  . However, despite these efforts there are still open questions regarding the underlying mechanisms governing the densification behavior under external loads  2  .\nIn order to understand the complex phenomena occurring inside compacted materials, several experimental studies were performed  3  , which revealed important information about the evolution of the microstructure  4  . Nevertheless, experiments cannot provide detailed insight into the local interactions taking place at the grain scale  5  . Therefore, numerical methods based on discrete element models (DEM)  6  became an alternative approach to investigate the mechanical properties of granular systems  7, 8  . These DEM-based approaches allow one to track individual grains within large assemblies and thus enable us to obtain valuable insights into the micromechanical response  9  . For example, recent investigations showed that the macroscopic stress-strain curve obtained from compression tests can be reproduced accurately if realistic inter-particle contact laws are used  10  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Computer modeling of model cohesive powders : affect of assembling procedure and contact laws on small consolidation states . Abstract : The goal of this project is to study the impact of different assembly techniques , as also as different contact laws between particles in powder compaction processes using computer simulations .The results show that the packing density depends strongly on the applied pressure during the assembly cycle . In addition , it was shown that the densification response can be governed by varying the contact law factors .Finally , we have shown how the microstructure evolves with varying pressure for two different kinds of particle contacts . Powder compaction has been studied thoroughly over numerous years owing to its significance in different industrial applications such as drug tablets or metal parts produced via hot pressing methods 1 .However , despite these attempts there are still open questions regarding the fundamental pathways governing the densification response under external loads 2 . In order to explain the complex interactions arising inside compacted materials , various experimental studies were performed 3 , which revealed important information about the evolution of the microstructure 4 .Nevertheless , observations cannot offer comprehensive understanding into the local interactions taking place at the grain scale 5 . Therefore , numerical models relying on discrete element models ( DEM ) 6 became an additional method to examine the structural properties of granular structures 7 , 8 .These DEM - based methods provide one to track small particles within large assemblies and therefore facilitate us to obtain crucial insights into the micromechanical behavior 9 . For instance , recent studies showed that the macroscopic stress - strain graph obtained from compression experiments can be reproduced accurately if realistic inter - particle contact laws are using 10 .",
        "rewrite_text": "**Title:** Computer Modeling of Cohesive Powders: The Impact of Assembly Procedures and Contact Laws on Small Consolidation States\n\n**Abstract:** This study investigates the effects of various assembly techniques and particle contact laws on the powder compaction process through computer simulations. The primary objective is to understand how these factors influence packing density and densification behavior during the assembly cycle. Our findings indicate a strong correlation between the applied pressure during assembly and the resulting packing density. Furthermore, we demonstrate that the densification response can be significantly altered by adjusting the parameters of the contact laws governing particle interactions. We also explore the evolution of microstructure under varying pressure conditions, focusing on two distinct types of particle contacts.\n\nPowder compaction has garnered considerable attention over the years due to its critical role in various industrial applications, including the production of pharmaceutical tablets and metal components via hot pressing techniques. Despite extensive research, fundamental questions remain regarding the mechanisms that drive densification under external loads. To address these complexities, numerous experimental studies have been conducted, yielding valuable insights into microstructural evolution. However, these observations often fall short of providing a complete understanding of the local interactions occurring at the grain scale.\n\nTo bridge this gap, we employ numerical models based on discrete element modeling (DEM), which offer a powerful approach to analyze the structural properties of granular materials. DEM allows for the tracking of individual particles within large assemblies, thereby enabling us to gain critical insights into micromechanical behavior. Recent investigations have shown that realistic inter-particle contact laws can accurately replicate the macroscopic stress-strain relationships observed in compression experiments. This research not only enhances our understanding of powder compaction processes but also contributes to the development of more effective modeling techniques for predicting the behavior of cohesive powders under various loading conditions.",
        "ori-fast-z-score": 0.48038446141526137,
        "water-fast-z-score": 9.328719097496718,
        "rewrite-fast-z-score": 1.4142135623730951
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  On Newton s law in supersymmetric braneworld models .\nAbstract:\nWe study the gravitational field equations for static spherically symmetric configurations on Randall-Sundrum type II brane-worlds with bulk cosmological constant and matter fields localized on the branes. We show that, under certain conditions, these solutions can be interpreted as black holes embedded into an anti-de Sitter space-time. In particular we find that there is no restriction to the mass parameter M0 appearing in the solution of the vacuum Einstein equation on the brane. The corresponding horizon radius r0 satisfies the relation r0 = (3M0/4π)1/3. This result implies that the Schwarzschild-de Sitter metric describes not only black hole but also naked singularity solutions. Finally, we discuss how this picture changes when one takes into account quantum corrections due to loop effects. PACS numbers: 04.20.-q; 11.10.Kk  Supersymmetry has been proposed as a possible extension of general relativity which could provide a consistent description of gravity at all scales  1  . It was shown recently  2  , however, that it does not lead to any new predictions if applied to standard four-dimensional theories. On the other hand, higher dimensional extensions of supergravity have attracted considerable attention during recent years  3  .\nIn this letter we consider five-dimensional supergravities  4  where the extra dimension is compactified on a circle  5  or orbifold  6  . These are known as Randall-Sundrum type I  7  and type II  8  scenarios respectively. They allow for localization of Standard Model particles  9  and their excitations  10  on the so-called visible brane while gravitons propagate freely through the bulk  11  . As a consequence they may solve some problems associated with the hierarchy between the electroweak scale and the Planck scale  12  . Moreover, such models offer interesting possibilities for constructing regular black-hole-like objects  13  -  16  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : On Newton s law in supersymmetric braneworld configurations . Abstract : We research the gravitational field equations for static spherically symmetric configurations on Randall - Sundrum type II brane - worlds with bulk cosmological constant and material fields confined on the branes .We see that , under certain conditions , these solutions can be interpreted as black holes inserted into an anti - de Sitter space - time . In particular we find that there is no limitation to the mass vector M0 appearing in the solution of the vacuum Einstein equation on the brane .The equivalent horizon radius r0 satisfies the relation r0 = ( 3M0 / 4π ) 1 / 3 . This result suggests that the Schwarzschild - de Sitter metric encompasses not only white hole but also naked singularity solutions .Finally , we explain how this picture changes when one takes into consideration quantum corrections due to loop interactions . PACS numbers : 04 . 20 . - q ; 11 . 10 . Kk Supersymmetry has been proposed as a possible extension of general relativity which could give a consistent description of gravitational at all scales 1 .It was shown ago 2 , however , that it does not result to any new predictions if applied to standard four - dimensional theories . On the other hand , greater dimensional extensions of supergravity have garnered considerable scrutiny during recent years 3 .In this letter we define five - dimensional supergravities 4 where the extra dimension is compactified on a ring 5 or orbifold 6 . These are known as Randall - Sundrum type I 7 and type II 8 scenarios respectively .They allow for localization of Standard Model particles 9 and their excitations 10 on the so - called visible brane while gravitons propagate continuously through the bulk 11 . As a consequence they may solve some problems related with the hierarchy between the electroweak scale and the Planck scale 12 .Moreover , such theories provided important possibilities for constructing ordinary black - hole - like bodies 13 - 16 .",
        "rewrite_text": "**Title:** On Newton's Law in Supersymmetric Braneworld Configurations\n\n**Abstract:** This study investigates the gravitational field equations governing static spherically symmetric configurations within Randall-Sundrum type II braneworlds, incorporating a bulk cosmological constant and material fields confined to the branes. We demonstrate that, under specific conditions, these configurations can be interpreted as black holes situated within an anti-de Sitter spacetime. Notably, we establish that there are no constraints on the mass vector \\( M_0 \\) present in the vacuum Einstein equation solutions on the brane. The corresponding horizon radius \\( r_0 \\) is found to satisfy the relationship \\( r_0 = \\left( \\frac{3M_0}{4\\pi} \\right)^{1/3} \\). This finding implies that the Schwarzschild-de Sitter metric encompasses not only white hole solutions but also naked singularities. Furthermore, we discuss how the inclusion of quantum corrections arising from loop interactions alters this framework. \n\nSupersymmetry has been proposed as a potential extension of general relativity, aiming to provide a coherent description of gravitational phenomena across various scales. Previous studies have indicated that applying supersymmetry to conventional four-dimensional theories does not yield new predictions. However, higher-dimensional extensions of supergravity have received significant attention in recent years. In this work, we define five-dimensional supergravities where the additional dimension is compactified either on a ring or an orbifold, corresponding to Randall-Sundrum type I and type II scenarios, respectively. These models facilitate the localization of Standard Model particles and their excitations on a \"visible\" brane, while gravitons can propagate freely through the bulk. This framework offers potential solutions to the hierarchy problem between the electroweak scale and the Planck scale, and it opens avenues for constructing ordinary black hole-like objects. The implications of our findings contribute to the ongoing discourse on the interplay between gravity, quantum mechanics, and higher-dimensional theories.",
        "ori-fast-z-score": -0.43685202833051895,
        "water-fast-z-score": 5.874305293953249,
        "rewrite-fast-z-score": 2.151657414559676
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Statistical Mechanics of Time Independent Non-Dissipative Nonequilibrium States .\nAbstract:\nThe statistical mechanics of time-independent non-dissipative nonequilibrium states is studied in the framework of generalized entropy production and fluctuation theorem. The main results are formulated for systems with discrete energy levels, but they can be extended to continuous spectrum as well. It is shown that the probability distribution function (PDF) of the total entropy production has an exponential tail at large values of entropy production. This result agrees with recent experimental data on PDFs of heat fluxes between two reservoirs connected by a single molecule junction. In addition it is demonstrated how this approach allows one to calculate the average value of the total entropy production and its fluctuations. Finally we discuss possible applications of these results to transport phenomena in molecular junctions. We show that our theory predicts correctly the dependence of the current-voltage characteristics on temperature difference between the leads. Statistical mechanics of time independent non-dissipative non-equilibrium states is studied within the framework of generalized entropy-production and fluctuation theorem. Main results are formulated for systems having discrete energy-levels, however extension to continuum spectra is also discussed. It is shown that probability-distribution-function (PDF) of total entropy-production has exponential tails at large values of entropy-production. This result agrees with recently obtained experimental data on PDF s of heat-fluxes between two reservoirs coupled via a single-molecule-junction. In addition it is shown how this approach allows us to calculate the average-value of total entropy-production and it s fluctuations. Finally we discuss possibilities of applying these results to transport phenomena occurring in molecular junctions.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Statistical Mechanics of Time Independent Non - Dissipative Nonequilibrium States . Abstract : The statistical mechanics of time - based non - dissipative nonequilibrium states is studied in the framework of generalized entropy production and fluctuation principle .The main results are formulated for models with discrete energy levels , but they can be extended to continuous spectrum as also . It is seen that the probability distribution relation ( PDF ) of the total entropy production has an exponential tail at large values of entropy production .This result agrees with recent experimental evidence on PDFs of temperature fluxes between two basins connected by a single molecule interchange . In addition it is demonstrated how this methodology allows one to estimate the average value of the total entropy production and its fluctuations .Finally we explain possible applied of these results to transport phenomena in molecular junctions . We see that our theory predicts correctly the dependence of the current - voltage parameters on temperature difference between the leads .Statistical mechanics of time independent non - dissipative non - equilibrium states is studied within the framework of generalized entropy - production and fluctuation theorem . Main findings are formulated for models having discrete energy - rates , however application to continuum spectra is also discussed .It is demonstrated that probability - distribution - function ( PDF ) of gross entropy - production has exponential tails at large values of entropy - production . This result agrees with lately published research data on PDF s of thermal - fluxes between two basins coupled via a single - atom - junction .In addition it is demonstrated how this methodology allows us to estimate the average - value of total entropy - production and it s fluctuations . Finally we explain options of using these results to transport phenomena occurring in molecular junctions .",
        "rewrite_text": "The study of statistical mechanics concerning time-independent non-dissipative nonequilibrium states is explored through the lens of generalized entropy production and the fluctuation principle. This research primarily focuses on models characterized by discrete energy levels, although the findings can also be adapted to continuous energy spectra. A significant outcome of this investigation is the identification of an exponential tail in the probability distribution function (PDF) of total entropy production at high entropy values. This observation aligns with recent experimental data regarding the PDFs of thermal fluxes observed between two reservoirs connected by a single molecule. Furthermore, the methodology presented in this study facilitates the estimation of both the average total entropy production and its associated fluctuations. The implications of these results extend to practical applications in transport phenomena within molecular junctions. Notably, our theoretical framework accurately predicts the relationship between current-voltage characteristics and the temperature differential between the leads. Overall, this work contributes to a deeper understanding of nonequilibrium statistical mechanics and its relevance to molecular transport processes, providing a foundation for future research in the field.",
        "ori-fast-z-score": 1.781196752327939,
        "water-fast-z-score": 7.663582481705323,
        "rewrite-fast-z-score": 1.2649110640673518
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Parametric Nonholonomic Frame Transforms and Exact Solutions in Gravity .\nAbstract:\nWe present new exact solutions to the Einstein field equations for stationary axisymmetric spacetimes with two commuting Killing vectors, which are generated by applying nonholonomic frame transforms (NFT) to known vacuum solutions. The NFT is constructed using an ansatz for the metric coefficients that depends on one arbitrary function of the radial coordinate only. We show how this method can be used to generate families of black hole solutions with different horizon topologies. In particular we find new rotating black ring solutions with toroidal horizons. These solutions have been obtained previously as limits of static black rings but our approach allows us to obtain them directly without any additional assumptions or approximations. Finally, we discuss some open problems related to these results. PACS numbers: 04.20.-q, 11.10.-z, 98.80.Cq \nI. INTRODUCTORY REMARkS\nThe study of exact solutions to the Einstein equations has played a crucial role in understanding many aspects of general relativity. However, it is often difficult to construct such solutions because they require solving complicated nonlinear partial differential equations. This problem becomes even more challenging when considering physically interesting situations like those involving rotation and/or matter fields. Nevertheless, there exist several techniques that allow one to generate new classes of solutions starting from simpler ones. One of the most powerful methods involves transforming the original solution into another one via so-called nonholonomic frame transforms  1  . Such transformations preserve certain geometric properties of the spacetime while changing others; see  2  -  4  for reviews. For example, if the transformed solution satisfies the vacuum Einstein equations then so does the original one  5  .\nIn this work we apply nonholonomic frame transforms to known vacuum solutions of the Einstein equations in order to generate new exact solutions describing stationary axisymmetric spacetimes: i.e., spacetimes admitting at least two independent Killing vector fields whose orbits are closed curves  6  . Stationary axisymmetric spacetimes play an important role in astrophysics since they describe the exterior gravitational field of spinning objects like stars, planets, and black holes  7, 8",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Parametric Nonholonomic Frame Transforms and Exact Solutions in Gravity . Abstract : We introduce novel exact solutions to the Einstein field equations for stationary axisymmetric spacetimes with two commuting Killing vectors , which are produced by using nonholonomic frame transforms ( NFT ) to known vacuum solutions .The NFT is built using an ansatz for the metric coefficients that relies on one arbitrary function of the radial coordinate only . We see how this method can be used to create families of black hole solutions with various horizon topologies .In particular we find new moving black ring solutions with toroidal horizons . These solutions have been achieved formerly as limits of static black rings but our approach allows us to obtain them directly without any additional constraints or approximations .Finally , we explain some open problems related to these results . PACS scores : 04 . 20 . - q , 11 . 10 . - z , 98 . 80 . Cq I .INTRODUCTORY REMARkS The investigation of precise solutions to the Einstein equations has served a crucial role in understanding several parts of general relativity . However , it is often challenging to build such solutions because they demand solving complicated nonlinear partial differential equations .This problem remains especially more challenging when treating physically exciting situations like those concerning rotation and / or matter fields . Nevertheless , there remain many procedures that enable one to create fresh categories of solutions starting from simpler ones .One of the most efficient methods involves transforming the previous solve into another one via so - called nonholonomic frame transforms 1 . Such transformations maintain certain geometric properties of the spacetime while altering others ; see 2 - 4 for reviews .For instance , if the transformed solution satisfies the vacuum Einstein equations then so does the previous one 5 . In this study we apply nonholonomic frame transforms to known vacuum solutions of the Einstein equations in order to create novel exact solutions governing stationary axisymmetric spacetimes : i . e . , spacetimes admitting at least two independent Killing matrix fields whose orbits are open curves 6 .Stationary axisymmetric spacetimes serve an important role in astrophysics since they describe the exterior gravitational field of spinning objects like stars , planets , and dark holes 7 , 8",
        "rewrite_text": "**Title:** Parametric Nonholonomic Frame Transforms and Exact Solutions in Gravity\n\n**Abstract:** In this paper, we present innovative exact solutions to the Einstein field equations specifically for stationary axisymmetric spacetimes characterized by two commuting Killing vectors. These solutions are derived through the application of nonholonomic frame transforms (NFT) to established vacuum solutions. The NFT is constructed using a metric coefficient ansatz that incorporates a single arbitrary function of the radial coordinate. This methodology enables the generation of diverse families of black hole solutions with varying horizon topologies. Notably, we identify new moving black ring solutions featuring toroidal horizons. While these solutions have previously been obtained as limits of static black rings, our approach facilitates their direct derivation without imposing additional constraints or approximations. Furthermore, we discuss several open problems associated with these findings, highlighting areas for future research. The significance of this work lies in its contribution to the understanding of exact solutions in general relativity, particularly in the context of complex scenarios involving rotation and matter fields. The challenge of constructing precise solutions to the Einstein equations is well-known, as it often necessitates solving intricate nonlinear partial differential equations. However, our approach, which leverages nonholonomic frame transforms, offers a promising avenue for generating new categories of solutions from simpler ones. By maintaining certain geometric properties of the spacetime while modifying others, we can ensure that if the transformed solution adheres to the vacuum Einstein equations, the original solution does as well. This study emphasizes the importance of stationary axisymmetric spacetimes in astrophysics, as they effectively describe the external gravitational fields of rotating celestial bodies such as stars, planets, and black holes. The implications of our results extend to various fields within theoretical physics, paving the way for further exploration of gravitational phenomena. \n\n**PACS scores:** 04.20.-q, 11.10.-z, 98.80.Cq",
        "ori-fast-z-score": -1.5696136947790653,
        "water-fast-z-score": 6.111919138499425,
        "rewrite-fast-z-score": 1.281025230440697
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Pairing based cooling of Fermi gases .\nAbstract:\nWe propose to cool fermionic atoms in optical lattices by using the pairing mechanism between two different hyperfine states, which is analogous to Cooper pair formation in superconductors. We show that this method can be used for both bosonic and fermionic systems with attractive interactions. The proposed scheme has several advantages over other methods such as evaporative cooling or sympathetic cooling.  It does not require any additional laser beams besides those needed for trapping and manipulating cold atoms. In addition it works even when there are no free particles present initially (e.g., at zero temperature). Finally we discuss how our proposal could be realized experimentally. Cooling fermions down to quantum degeneracy temperatures below 1 microkelvin remains one of the most challenging problems in atomic physics today  1  . This problem becomes particularly difficult if the initial number density of fermions is high because then elastic collisions cannot remove enough energy from the system  2  .\nIn recent years, however, new experimental techniques have been developed  3, 4  , allowing us to trap and manipulate cold atoms on an unprecedented level  5  . These developments make it possible to study many-body phenomena  6  like superfluidity  7, 8  and Bose-Einstein condensation  9  in ultracold atomic gases. One important goal in these experiments is to reach quantum degenerate regimes where the gas consists of strongly interacting fermions  10  . However, reaching low temperatures requires efficient cooling schemes  11  .\nOne promising approach towards achieving this goal is to use the pairing mechanism  12  . Pairs of fermions form bound states called Cooper pairs in conventional superconductors  13  . Analogously, pairs of fermions may also form bound states in ultracold atomic clouds  14  . If the interaction strength between fermions is sufficiently large, they will preferentially bind into pairs rather than remaining unpaired  15  . Therefore, cooling fermions via pairing should work well even",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Pairing based cooling of Fermi gases . Abstract : We suggest to cool fermionic atoms in optical lattices by using the pairing principle between two different hyperfine states , which is analogous to Cooper couple formation in superconductors .We see that this process can be used for both bosonic and fermionic systems with interesting interactions . The proposed system has numerous benefits over other methods such as evaporative cooling or sympathetic heating .It does not require any additional laser beams besides those required for trapping and manipulating cool ions . In addition it works even when there are no free particles present initially ( e . g . , at zero temperature ) .Finally we talk how our proposal possible be realized experimentally . Cooling fermions down to quantum degeneracy temperatures below 1 microkelvin remains one of the most challenging difficulties in nuclear science today 1 .This problem arises particularly challenging if the first number density of fermions is high because then elastic collisions cannot eliminate much energy from the system 2 . In recent years , however , new theoretical techniques have been built 3 , 4 , allowing us to trapping and manipulate cold molecules on an remarkable level 5 .These advances making it able to study many - bodies phenomena 6 like superfluidity 7 , 8 and Bose - Einstein condensation 9 in ultracold atomic gases . One important aim in these experiments is to reach quantum degenerate regimes where the gas consists of highly interacting fermions 10 .However , obtaining minimum heats needs efficient cooling schemes 11 . One promising alternative towards reaching this goal is to use the pairing principle 12 .Pairs of fermions form bound states called Cooper pairs in standard superconductors 13 . Analogously , pairs of fermions may also create bound states in ultracold atomic clouds 14 .If the interaction strength between fermions is sufficiently great , they will preferentially bind into couples rather than keeping unpaired 15 . Therefore , cooling fermions via pairing should work better even",
        "rewrite_text": "We propose a novel method for cooling fermionic atoms in optical lattices by leveraging the pairing principle between distinct hyperfine states, akin to the formation of Cooper pairs in superconductors. This approach is applicable to both bosonic and fermionic systems exhibiting intriguing interactions. Our method presents several advantages over traditional cooling techniques, such as evaporative cooling and sympathetic heating. Notably, it does not necessitate additional laser beams beyond those already utilized for trapping and manipulating cold ions. Furthermore, this cooling mechanism is effective even in the absence of free particles at the outset, such as at absolute zero temperature.\n\nThe challenge of cooling fermions to quantum degeneracy temperatures below 1 microkelvin remains a significant hurdle in nuclear science. This difficulty is exacerbated when the initial fermion density is high, as elastic collisions are less effective at dissipating energy from the system. Recent advancements in theoretical techniques have enabled the trapping and manipulation of cold molecules with remarkable precision, facilitating the exploration of many-body phenomena such as superfluidity and Bose-Einstein condensation in ultracold atomic gases. A key objective of these experiments is to achieve quantum degenerate regimes characterized by highly interacting fermions. However, efficient cooling schemes are essential for minimizing thermal energy.\n\nOur proposed pairing-based cooling method offers a promising alternative for achieving this goal. In conventional superconductors, fermions form bound states known as Cooper pairs. Similarly, in ultracold atomic clouds, pairs of fermions can also establish bound states if the interaction strength is sufficiently strong, leading to a preference for pairing over remaining unpaired. Consequently, utilizing the pairing principle for cooling fermions is expected to yield superior results, enhancing our ability to explore the fascinating properties of quantum degenerate fermionic gases. We also discuss potential experimental realizations of our proposal, paving the way for future investigations into the behavior of ultracold fermionic systems.",
        "ori-fast-z-score": -0.3223291856101521,
        "water-fast-z-score": 7.045638767423834,
        "rewrite-fast-z-score": 1.2206826881567392
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Synchrotron Radio Emission from a Fast Halo Coronal Mass Ejection .\nAbstract:\nWe report on the detection and analysis of radio emission associated with an impulsive solar flare that occurred in active region NOAA 10486 (SOL2010-07-20T17:48) at 17:48 UT on July 20, 2010 using the Nançay Decameter Array (NDA). The event was accompanied by a fast halo coronal mass ejection (CME), which reached Earth at 18:20 UT on July 21. We find that the radio source is located near the center of the CME front as seen in white light images taken by STEREO-Ahead/EUVI 195 Å . The radio flux density shows rapid evolution during the first hour after the onset of the flare, followed by gradual decay over several hours. The radio spectrum has a power-law shape between 1 MHz to 5 GHz. The spectral index decreases rapidly below 100 MHz but remains nearly constant above this frequency.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Synchrotron Radio Emission from a Fast Halo Coronal Mass Ejection . Abstract : We report on the detection and assessment of radio emission associated with an impulsive solar flare that occurred in active region NOAA 10486 ( SOL2010 - 07 - 20T17 : 48 ) at 17 : 48 UT on July 20 , 2010 using the Nançay Decameter Array ( NDA ) .The event was accompanied by a rapid halo coronal mass ejection ( CME ) , which reached Earth at 18 : 20 UT on July 21 . We see that the radio source is situated near the center of the CME front as shown in white light pictures taken by STEREO - Ahead / EUVI 195 Å .The radio flux concentration displays rapid change during the first hour after the outbreak of the flare , followed by slowly evolution over numerous hours . The radio spectrum has a power - law shape between 1 MHz to 5 GHz .The spectral index drops rapidly below 100 MHz but continues nearly constant above this signal .",
        "rewrite_text": "We present a detailed analysis of the synchrotron radio emission linked to a sudden solar flare that took place in active region NOAA 10486 (SOL2010-07-20T17:48) at 17:48 UT on July 20, 2010. This investigation was conducted using data from the Nançay Decameter Array (NDA). The solar flare was accompanied by a swift halo coronal mass ejection (CME), which was detected at Earth at 18:20 UT on July 21. Our observations indicate that the radio emission source is located near the center of the CME front, as evidenced by white light images captured by the STEREO-Ahead/EUVI 195 Å instrument. \n\nDuring the initial hour following the flare, we observed significant fluctuations in the radio flux, which were followed by a gradual evolution over several hours. The radio spectrum exhibited a power-law distribution across a frequency range of 1 MHz to 5 GHz. Notably, the spectral index experienced a rapid decline below 100 MHz, while maintaining a relatively stable value at higher frequencies. This study enhances our understanding of the dynamics of radio emissions associated with solar flares and CMEs, providing insights into the underlying physical processes. The findings contribute to the broader field of solar physics, particularly in the context of space weather phenomena and their potential impacts on Earth.",
        "ori-fast-z-score": -1.5,
        "water-fast-z-score": 3.3489378339086286,
        "rewrite-fast-z-score": 0.8164965809277261
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Testing General Metric Theories of Gravity with Bursting Neutron Stars .\nAbstract:\nWe present the results of an analysis of gravitational wave data collected by the LIGO and Virgo detectors during the first observing run (O1) in 2015, which includes two candidate events for binary neutron star mergers. We use these observations to test general relativity against alternative theories of gravity that predict deviations from GR at high curvature regimes such as those encountered near black holes or neutron stars. In particular we consider scalar-tensor theories where the coupling between matter fields and the metric is mediated by a light scalar field. These theories are motivated by string theory and have been studied extensively over many decades. \n \n For each event, we perform Bayesian model selection using simulated signals generated from both GR and several representative scalartensor theories. Our results show no evidence for deviations from GR within current uncertainties. However, this does not rule out all possible deviations from GR; it only rules out certain classes of deviations predicted by specific models.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Testing General Metric Theories of Gravity with Bursting Neutron Stars . Abstract : We report the results of an assessment of gravitational wave information collected by the LIGO and Virgo detectors during the first observing run ( O1 ) in 2015 , which includes two candidate events for binary neutron galaxy mergers .We use these observations to test general relativity against alternative theories of gravitational that forecast deviations from GR at high curvature regimes such as those observed near black holes or neutron stars . In particular we investigate scalar - vector models where the interaction between matter fields and the metric is mediated by a light scalar field .These concepts are motivated by string theory and have been studied frequently over numerous years . For each event , we perform Bayesian model selection utilizing simulated waves generated from both GR and many representative scalartensor theories .Our results show no evidence for deviations from GR within current uncertainties . However , this does not order out all possible deviations from GR ; it only rules out specific grades of deviations expected by specific models .",
        "rewrite_text": "We present an extensive analysis of gravitational wave data obtained from the LIGO and Virgo observatories during their inaugural observing run (O1) in 2015, focusing on two potential events involving binary neutron star mergers. This study aims to evaluate general relativity (GR) in comparison to alternative gravitational theories that predict deviations from GR in high curvature environments, such as those found in the vicinity of black holes and neutron stars. Specifically, we explore scalar-vector models, where the interaction between matter fields and the spacetime metric is influenced by a light scalar field. These theoretical frameworks are inspired by string theory and have been the subject of considerable research over the years. \n\nFor each detected event, we employ Bayesian model selection techniques, utilizing simulated gravitational waveforms derived from both general relativity and a variety of representative scalar-tensor theories. Our findings indicate that there is currently no evidence supporting deviations from general relativity within the existing uncertainties of the data. However, it is important to note that our results do not eliminate all potential deviations from GR; rather, they specifically exclude certain types of deviations predicted by particular theoretical models. This work contributes to the ongoing discourse regarding the validity of general relativity in extreme gravitational environments and highlights the need for further investigations as more data becomes available.",
        "ori-fast-z-score": -0.5555555555555556,
        "water-fast-z-score": 5.0,
        "rewrite-fast-z-score": -0.6704783996548059
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Catastrophic eruption of magnetic flux rope in the corona and solar wind with and without magnetic reconnection .\nAbstract:\nWe investigate the evolution of an eruptive coronal mass ejection (CME) by using three-dimensional magnetohydrodynamic simulations, which include both ideal MHD effects and resistivity. The initial condition is constructed as follows. First we solve the Grad-Shafranov equation for force-free equilibrium to obtain the distribution of poloidal current density on the surface of a torus-like structure. Then we construct a 3D model of the torus-like structure by assuming that it has uniform temperature T = 1 MK and pressure P = 0.1 dyn cm-2 inside its volume. We also assume that there are two types of plasma flows along the field lines; one is slow solar wind flowing out from the bottom boundary at r = 2 R_sun , where Rsun denotes the radius of the Sun, and another is fast solar wind flowing into the top boundary at r = 20 Rsun . Finally, we add random fluctuations to the velocity fields so that they have kinetic energy comparable to their thermal energies. \n \n In our simulation runs, we find three different phases during the CME evolution. During phase I, the magnetic flux rope rises slowly due to the Lorentz force associated with the toroidal component of the magnetic field. At t ~ 50 min., when the height reaches about 5 RSUN , the rising motion suddenly accelerates because of the rapid expansion of the flux rope s cross section caused by magnetic reconnection between open field lines and closed loops around the flux rope. This acceleration continues until the flux rope reaches the upper boundary at t ~ 80 min. . \n \n During phase II, the flux rope expands rapidly outward while maintaining its shape. After reaching the maximum size at t ~ 90 min. , however, the flux rope starts contracting inward again. As a result, the flux rope becomes thinner than before but still maintains its twisted structure. Meanwhile, the surrounding open field lines become more bent toward each other near the center of the flux rope. These results suggest that the flux rope undergoes a kink instability after expanding outward beyond its critical size.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Catastrophic eruption of magnetic flux rope in the corona and solar wind with and without magnetic reconnection . Abstract : We explore the evolution of an eruptive coronal mass ejection ( CME ) by using three - dimensional magnetohydrodynamic simulations , which contain both optimal MHD effects and resistivity .The initial condition is constructed as follows . First we solve the Grad - Shafranov integral for force - free equilibrium to obtain the distribution of poloidal current density on the surface of a torus - like structure .Then we create a 3D model of the torus - like structure by assuming that it has uniform heat T = 1 MK and pressure P = 0 . 1 dyn centimetres - 2 inside its volume . We additionally assume that there are two forms of plasma flows along the field lines ; one is rapid solar wind running out from the bottom border at r = 2 R _ sun , where Rsun denotes the radius of the Sun , and another is quick solar wind flowing into the top boundary at r = 20 Rsun .Finally , we apply random fluctuations to the velocity fields so that they have kinetic power similar to their thermal energies . In our modeling ran , we find three different stages during the CME evolution .During period I , the magnetic flux rope rises steadily owing to the Lorentz force associated with the toroidal constituent of the magnetic field . At t ~ 50 min . , when the height reaches about 5 RSUN , the increasing motion suddenly accelerates because of the quick expansion of the flux rope s cross section resulted by magnetic reconnection between open field lines and opened loops around the flux rope .This acceleration continues until the flux rope reaches the higher edge at t ~ 80 min . .During period II , the flux rope expands quickly outward while maintaining its size . After achieving the maximum size at t ~ 90 min ., however , the flux rope starts contracting inward again . As a result , the flux rope grows thinner than before but still maintains its tangled structure .Meanwhile , the nearby open field lines become more bent toward each other near the center of the flux rope . These data suggest that the flux rope undergoes a kink instability after increasing outward beyond its critical size .",
        "rewrite_text": "**Title:** Catastrophic Eruption of Magnetic Flux Rope in the Corona and Solar Wind with and without Magnetic Reconnection\n\n**Abstract:** In this study, we investigate the dynamics of a coronal mass ejection (CME) through advanced three-dimensional magnetohydrodynamic (MHD) simulations that incorporate both optimal MHD effects and resistivity. The initial conditions for our simulations are established by solving the Grad-Shafranov integral, which allows us to derive the distribution of poloidal current density across the surface of a toroidal structure in force-free equilibrium. We then construct a 3D model of this toroidal configuration, assuming a uniform internal temperature of 1 MK and a pressure of 0.1 dyn cm^-2. Our model includes two distinct plasma flow regimes along the magnetic field lines: a rapid solar wind emanating from the lower boundary at a radius of 2 R_sun (where R_sun is the solar radius) and another fast solar wind entering the upper boundary at 20 R_sun. To enhance the realism of our simulations, we introduce random fluctuations to the velocity fields, ensuring that their kinetic energy is comparable to their thermal energy.\n\nOur simulations reveal three distinct phases in the evolution of the CME. In the first phase, the magnetic flux rope ascends steadily, driven by the Lorentz force linked to the toroidal component of the magnetic field. At approximately 50 minutes into the simulation, when the flux rope reaches a height of about 5 R_sun, a sudden acceleration occurs due to the rapid expansion of the flux rope's cross-section, which is triggered by magnetic reconnection between the open field lines and the surrounding loops. This acceleration persists until the flux rope reaches its upper limit around 80 minutes. \n\nDuring the second phase, the flux rope expands outward rapidly while maintaining its overall size. However, after reaching its maximum size at around 90 minutes, it begins to contract inward, resulting in a thinner structure that retains its complex, tangled configuration. Concurrently, the adjacent open field lines exhibit increased bending towards one another near the center of the flux rope. Our findings indicate that the flux rope experiences kink instability after expanding beyond its critical size, highlighting the intricate interplay between magnetic reconnection and the dynamics of CME evolution.",
        "ori-fast-z-score": -0.22941573387056177,
        "water-fast-z-score": 5.971838673545791,
        "rewrite-fast-z-score": 0.6172133998483676
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Secure sharing of random bits over the Internet .\nAbstract:\nWe present an experimental study on securely exchanging random bits between two parties connected to the Internet via public channels, using quantum key distribution (QKD). We show that QKD can be used for this purpose with high efficiency and security in practice by implementing it experimentally. The results demonstrate that our scheme is practical enough to be deployed as part of existing communication networks. Quantum Key Distribution (QKD) enables two remote users Alice and Bob to establish a secret key which they use to encrypt messages transmitted over insecure public channels such as the internet or telephone lines  1  . In order to achieve unconditional security against eavesdropping attacks  2  , QKD requires both users to perform measurements on single photons generated at distant locations  3  .\nIn recent years there has been significant progress towards commercialization of QKD systems  4  . However, despite these advances, deployment of QKD remains limited due to its relatively low transmission rates compared to classical cryptography  5  . This problem becomes more severe when one considers applications where multiple users need to share secrets simultaneously  6  . For example, consider a scenario where several banks want to exchange encrypted messages with each other while keeping their communications private  7, 8  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Secure sharing of random bits over the Internet . Abstract : We present an research experiment on securely exchanging random bits between two sides connected to the Internet via public networks , using quantum key distribution ( QKD ) .We see that QKD can be used for this use with high efficiency and safety in practice by installing it experimentally . The results show that our scheme is practical sufficient to be deployed as member of older communication networks .Quantum Key Distribution ( QKD ) enables two remote users Alice and Bob to create a secret key which they use to encrypt packets sent over insecure public outlets such as the online or telephone routes 1 . In order to achieve unconditional security against eavesdropping attacks 2 , QKD needs both users to conduct measurements on separate photons generated at remote locations 3 .In recent years there has been significant progress towards commercialization of QKD systems 4 . However , despite these developments , deployment of QKD remains restricted due to its relatively poor transmission rates relative to conventional cryptography 5 .This problem arises more serious when one considers applications where multiple users require to exchange secrets simultaneously 6 . For instance , consider a situation where many companies desire to exchange encrypted messages with each other while maintaining their communications personal 7 , 8 .",
        "rewrite_text": "Title: Secure Sharing of Random Bits Over the Internet\n\nAbstract: This study explores the secure exchange of random bits between two parties connected through public networks, utilizing quantum key distribution (QKD) as a foundational technology. Our experimental research demonstrates that QKD can be effectively implemented to achieve high levels of efficiency and security in practical applications. The findings indicate that our proposed scheme is sufficiently robust for integration into existing communication infrastructures. QKD facilitates the generation of a secret key between two remote users, referred to as Alice and Bob, which is subsequently employed to encrypt data packets transmitted over insecure channels, such as the Internet or telephone lines. To ensure unconditional security against potential eavesdropping, it is essential for both users to perform measurements on distinct photons that are generated at separate locations. Recent advancements in the commercialization of QKD systems have shown promise; however, the widespread adoption of this technology is still hindered by its relatively low transmission rates when compared to traditional cryptographic methods. This limitation becomes particularly pronounced in scenarios where multiple users need to exchange confidential information simultaneously. For example, numerous companies may wish to securely share encrypted communications while preserving the privacy of their interactions. Our research addresses these challenges and highlights the potential of QKD to enhance secure communication practices in an increasingly interconnected digital landscape. By demonstrating the feasibility of QKD in real-world applications, we contribute to the ongoing discourse on improving the security of data exchange over public networks, paving the way for more secure communication solutions in the future.",
        "ori-fast-z-score": -1.2567574357593625,
        "water-fast-z-score": 6.477134476605945,
        "rewrite-fast-z-score": -0.08804509063256238
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The XMM-SSC survey of hard-spectrum XMM-Newton sources 1: optically bright sources .\nAbstract:\nWe present the results of an optical spectroscopic follow-up campaign for a sample of X-ray selected AGN with extreme properties, observed in the first year (2000) of the XMM-Newton Serendipitous Source Catalogue (XMM-SSC). The main goal is to study their nature and physical characteristics by means of multiwavelength observations. We have obtained spectra for about half of our sample using several telescopes at different observatories around the world. Our analysis shows that most of these objects are broad-line quasars or Seyfert 1 galaxies; only one object turns out to be a narrow-line radio galaxy. In addition we find two new BL Lac candidates among this sample. This work has been supported by the Spanish Ministerio de Ciencia y Tecnología under grant AYA2003-08548-C03-01/02/03. -The XMM-SSC catalogue contains more than 100 000 serendipitously detected X-ray sources extracted from all public data taken during the first three years of operation of the European Space Agency s XMM-Newton satellite. It covers almost the entire sky visible from Europe above |b| > 10 degrees. -X-ray surveys provide large samples of active galactic nuclei (AGNs), which can then be studied statistically over wide ranges of luminosity, redshift and other parameters. However, it is often difficult to identify individual sources unambiguously because they may show complex spectral shapes and/or variability on many timescales. -In order to select a complete sample of AGNs with extreme properties, we applied very strict selection criteria based on the source count rate and photon index measured in the 0.5-2 keV band. These criteria were chosen so as to maximize the fraction of absorbed sources while keeping contamination due to background fluctuations low.  -Our final sample consists of 56 sources, including four previously known blazars.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The XMM - SSC study of hard - spectrum XMM - Newton sources 1 : optically bright sources . Abstract : We report the results of an optical spectroscopic follow - up campaign for a sample of X - ray selected AGN with uncommon characteristics , detected in the first year ( 2000 ) of the XMM - Newton Serendipitous Source Catalogue ( XMM - SSC ) .The main goal is to study their nature and physical qualities by means of multiwavelength surveys . We have achieved spectra for about half of our sample using numerous telescopes at different observatories around the world .Our study shows that most of these objects are broad - line quasars or Seyfert 1 galaxies ; only one object turns out to be a broad - line radio star . In addition we find two new BL Lac candidates among this specimen .This work has been supported by the Spanish Ministerio de Ciencia y Tecnología under grant AYA2003 - 08548 - C03 - 01 / 02 / 03 . - The XMM - SSC catalogue contains more than 100 000 serendipitously detected X - ray sources extracted from all public data taken during the first three years of operation of the European Space Agency s XMM - Newton satellite .It covers almost the entire sky visible from Europe above | b | > 10 degrees . - X - ray studies provide big samples of active galactic nuclei ( AGNs ) , which can then be examined statistically over broad ranges of luminosity , redshift and other parameters .However , it is often difficult to identify individual sources unambiguously because they may show complex wavelength shapes and / or variability on various timescales . - In order to select a complete sample of AGNs with extreme properties , we applied very strict selection standards based on the source count rate and photon index calculated in the 0 . 5 - 2 keV band .These criteria were chosen so as to maximize the fraction of absorbed sources while maintaining contamination owing to background fluctuations low . - Our last sample consists of 56 sources , including four originally noted blazars .",
        "rewrite_text": "We present the findings from an optical spectroscopic follow-up study of a unique sample of X-ray selected active galactic nuclei (AGN) identified in the inaugural year (2000) of the XMM-Newton Serendipitous Source Catalogue (XMM-SSC). The primary objective of this research is to investigate the nature and physical properties of these AGNs through comprehensive multiwavelength surveys. Utilizing a variety of telescopes across multiple observatories worldwide, we successfully obtained spectra for approximately half of the sources in our sample. Our analysis reveals that the majority of these objects are classified as broad-line quasars or Seyfert 1 galaxies, with only one identified as a broad-line radio star. Notably, we also discovered two new candidates for BL Lac objects within this group. This research was supported by the Spanish Ministerio de Ciencia y Tecnología under grant AYA2003-08548-C03-01/02/03.\n\nThe XMM-SSC catalogue comprises over 100,000 serendipitously detected X-ray sources, compiled from all publicly available data collected during the first three years of the XMM-Newton satellite's operations, covering nearly the entire sky visible from Europe at latitudes above |b| > 10 degrees. X-ray studies yield extensive samples of AGNs, facilitating statistical examinations across a wide range of luminosities, redshifts, and other parameters. However, the identification of individual sources can be challenging due to their complex spectral shapes and variability across different timescales. To construct a complete sample of AGNs with extreme characteristics, we implemented stringent selection criteria based on source count rates and photon indices calculated in the 0.5 - 2 keV energy band. These criteria were designed to enhance the detection of absorbed sources while minimizing contamination from background fluctuations. Ultimately, our final sample consists of 56 sources, including four that were previously identified as blazars.",
        "ori-fast-z-score": 1.1026456085839622,
        "water-fast-z-score": 5.979278639572285,
        "rewrite-fast-z-score": 0.1781741612749496
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Crystallization in large wireless networks .\nAbstract:\nWe study the problem of finding an optimal schedule for data transmission over multiple channels with interference constraints, where each channel is assigned to one transmitter-receiver pair and transmissions on different pairs are subject to mutual interference. We consider two models: (i) The first model assumes that all transmitters have fixed power levels. (ii) In the second model we assume that transmitters can adjust their powers dynamically. For both cases, we show how to find an optimal schedule by solving a sequence of linear programs. Our results hold even if there exists only one receiver per transmitter. This work was supported by NSF grant CCF-0430018. 1 Introduction Wireless networks consist of many nodes communicating via radio signals. Each node has limited range and thus cannot communicate directly with every other node. Instead, it communicates indirectly through intermediate nodes called relays or routers. A fundamental question in this context is: How should these relays be placed?",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Crystallization in large wireless networks . Abstract : We research the issue of finding an appropriate schedule for information transmission over multiple channels with interference constraints , where each channel is allotted to one transmitter - receiver set and signals on separate pairs are subject to mutual interference .We consider two models : ( i ) The first theory requires that all transmitters have fixed speed levels . ( ii ) In the second theory we suppose that transmitters can adjust their powers dynamically .For both cases , we prove how to find an appropriate schedule by solving a sequence of linear programs . Our results hold even if there exists only one receiver per transmitter .This work was supported by NSF grant CCF - 0430018 . 1 Introduction Wireless networks consist of many nodes communicating via radio signals .Each node has restricted range and therefore cannot transmit directly with every other node . Instead , it communicates indirectly through intermediate nodes termed relays or routers .A basic issue in this situation is : How should these relays be placed ?",
        "rewrite_text": "Title: Crystallization in Large Wireless Networks\n\nAbstract: This study addresses the challenge of developing an effective scheduling strategy for information transmission across multiple channels while adhering to interference constraints. In our framework, each channel is designated for a specific transmitter-receiver pair, with the understanding that signals from different pairs may interfere with one another. We explore two distinct models: the first model assumes that all transmitters operate at fixed power levels, while the second model allows for dynamic adjustment of transmitter power. For both scenarios, we demonstrate a method for determining an optimal transmission schedule by solving a series of linear programming problems. Notably, our findings remain valid even in configurations where each transmitter is paired with only one receiver. This research contributes to the understanding of wireless network dynamics and was supported by NSF grant CCF-0430018. \n\nIn the context of wireless networks, which consist of numerous nodes communicating through radio signals, each node has a limited transmission range, preventing direct communication with all other nodes. Instead, these nodes rely on intermediate entities known as relays or routers to facilitate communication. A fundamental question arises in this setting: what is the optimal placement of these relays to enhance network performance? Our work aims to provide insights into this critical aspect of wireless network design, ultimately contributing to more efficient communication strategies in complex network environments.",
        "ori-fast-z-score": -1.0660035817780522,
        "water-fast-z-score": 4.133991732024804,
        "rewrite-fast-z-score": -1.2675004445952593
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Detection of GRB 060927 at z = 5.47: Implications for the Use of Gamma-Ray Bursts as Probes of the End of the Dark Ages .\nAbstract:\nWe report on the detection by Swift/BAT and Konus-Wind of gamma-ray burst (GRB) 060927, which is one of only two events detected with redshifts greater than five to date.  The redshift was determined using optical spectroscopy obtained with the Very Large Telescope in Chile within three hours after the burst trigger time.  We find that this event has an extremely hard spectrum, similar to other high-redshift bursts observed previously.  Using our current understanding of the physics behind these phenomena we can estimate the total energy released during the prompt emission phase of the burst to be ~10^53 erg.  This value is comparable to or larger than the energies released by some supernovae.  If confirmed, it would make GRB 060927 the most energetic event ever recorded.  In addition, if the host galaxy mass function evolves similarly to what is seen locally, then the progenitor system responsible for producing such an energetic explosion must have been hosted by a very massive galaxy.  These results are consistent with theoretical predictions made about the progenitors of high-z GRBs.  Finally, we discuss how future observations of this object may help us understand the end of cosmic reionization.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Detection of GRB 060927 at z = 5 . 47 : Implications for the Use of Gamma - Ray Bursts as Probes of the End of the Dark Ages . Abstract : We report on the discovery by Swift / BAT and Konus - Wind of gamma - ray burst ( GRB ) 060927 , which is one of only two events discovered with redshifts greater than five to date .The redshift was calculated using optical spectroscopy acquired with the Very Large Telescope in Chile within three hours after the explosion trigger time . We see that this event has an incredibly hard spectrum , comparable to other high - redshift bursts observed previously .Using our current understanding of the physics behind these phenomena we can calculate the total energy released during the prompt emission stage of the explosion to be ~ 10 ^ 53 erg . This value is analogous to or larger than the energies released by some supernovae .If confirmed , it would render GRB 060927 the most intense event ever recorded . In addition , if the host star mass function evolves likewise to what is seen locally , then the progenitor system responsible for producing such an energetic explosion could have been hosted by a very huge galaxy .These data are compatible with theoretical calculations made about the progenitors of high - z GRBs . Finally , we talk how potential findings of this body may assist us explain the end of cosmic reionization .",
        "rewrite_text": "We present our findings on the detection of gamma-ray burst (GRB) 060927, identified by the Swift/BAT and Konus-Wind observatories. This event is notable as it is one of only two GRBs recorded with redshifts exceeding five. The redshift was determined through optical spectroscopy conducted with the Very Large Telescope in Chile, which was performed within three hours following the burst's trigger. Our analysis reveals that GRB 060927 exhibits an exceptionally hard spectrum, similar to other high-redshift bursts previously documented. Utilizing our current understanding of the underlying physics of gamma-ray bursts, we estimate the total energy released during the prompt emission phase to be approximately 10^53 erg. This energy output is comparable to, or even surpasses, that of certain supernovae, suggesting that if validated, GRB 060927 could be the most powerful event recorded to date. Furthermore, if the mass function of host stars evolves in a manner consistent with local observations, it is plausible that the progenitor of this remarkable explosion originated from an exceptionally massive galaxy. Our findings align with theoretical models concerning the progenitors of high-redshift GRBs. Additionally, we discuss the implications of our results for understanding the end of cosmic reionization, highlighting how this discovery could enhance our comprehension of the universe's early epochs. This research contributes to the broader discourse on the role of gamma-ray bursts as significant probes for investigating the conditions and events surrounding the conclusion of the Dark Ages in cosmic history.",
        "ori-fast-z-score": -0.8340576562282991,
        "water-fast-z-score": 5.495843982071254,
        "rewrite-fast-z-score": 0.6897304947150052
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Secondary B-mode polarization from Faraday rotation in clusters and galaxies .\nAbstract:\nWe present the first detection of secondary CMB polarization induced by Faraday rotation (FR) in galaxy clusters, using data taken with the Atacama Cosmology Telescope Polarimeter (ACTPol). We detect FR-induced polarized emission at angular scales corresponding to multipoles = 100-1000 for two galaxy clusters: ACT-CL J0102-4915 and ACT-CL J0546-5345. The observed signal is consistent with theoretical predictions based on numerical simulations of magnetized cluster atmospheres. This measurement provides an important test of our understanding of magnetic fields in galaxy clusters as well as their impact on cosmological observables such as the CMB temperature anisotropies and E-mode polarizations. In addition, we report upper limits on the FR-induced polarized emissions from other galaxy clusters that are not detected individually due to low S/N ratio or limited survey area. These results will be useful for future studies of FR effects on the CMB polarization maps produced by upcoming experiments like Planck and Simons Observatory.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Secondary B - mode polarization from Faraday rotation in clusters and clusters . Abstract : We report the first measurement of secondary CMB polarization induced by Faraday rotation ( FR ) in galaxy clusters , using data taken with the Atacama Cosmology Telescope Polarimeter ( ACTPol ) .We detect FR - caused polarized emission at angular scales corresponding to multipoles = 100 - 1000 for two galaxy clusters : ACT - CL J0102 - 4915 and ACT - CL J0546 - 5345 . The observed light is compatible with theoretical estimates based on numerical simulations of magnetized cluster atmospheres .This measurement represents an important test of our knowing of magnetic fields in galaxy clusters as well as their impact on cosmological observables such as the CMB heat anisotropies and E - mode polarizations . In addition , we note upper limits on the FR - caused polarized impacts from other galaxy galaxies that are not observed individually due to low S / N proportion or restricted observation space .These conclusions will be valuable for future research of FR effects on the CMB polarization mapping created by future research like Planck and Simons Observatory .",
        "rewrite_text": "**Title:** Secondary B-mode Polarization from Faraday Rotation in Galaxy Clusters\n\n**Abstract:** In this study, we present the inaugural measurement of secondary polarization of the Cosmic Microwave Background (CMB) resulting from Faraday rotation (FR) within galaxy clusters, utilizing data acquired from the Atacama Cosmology Telescope Polarimeter (ACTPol). Our findings reveal the presence of polarized emission induced by FR at angular scales corresponding to multipoles ranging from 100 to 1000, specifically for two galaxy clusters: ACT-CL J0102-4915 and ACT-CL J0546-5345. The characteristics of the detected polarization align well with theoretical predictions derived from numerical simulations of magnetized cluster atmospheres, thereby providing a significant validation of our understanding of magnetic fields in these cosmic structures. This measurement not only enhances our comprehension of the magnetic environments within galaxy clusters but also elucidates their influence on key cosmological observables, including CMB temperature anisotropies and E-mode polarization patterns. Furthermore, we establish upper limits on the polarized effects attributed to FR from additional galaxy clusters that could not be individually analyzed due to low signal-to-noise ratios or limited observational coverage. These insights are expected to be instrumental for future investigations into the effects of Faraday rotation on CMB polarization, particularly in the context of upcoming observational projects such as the Planck satellite and the Simons Observatory. Our results pave the way for a deeper understanding of the interplay between magnetic fields and cosmic microwave background phenomena, highlighting the importance of FR in the broader framework of cosmological research.",
        "ori-fast-z-score": -0.7977240352174656,
        "water-fast-z-score": 4.98201895023622,
        "rewrite-fast-z-score": 0.7035264706814485
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Particle Motion in Rapidly Oscillating Potentials: The Role of the Potential s Initial Phase .\nAbstract:\nWe study particle motion in rapidly oscillating potentials with arbitrary initial phases and show that, for certain values of this phase, particles can be trapped by the potential even if it is not periodic.  We find exact solutions to the Schrödinger equation describing such systems using Floquet theory and discuss their physical implications. In particular we consider an optical lattice formed by counter-propagating laser beams whose frequencies are detuned slightly from each other. This system has been studied extensively both theoretically and experimentally but our results provide new insights into its dynamics. Introduction:-In recent years there have been many studies on the physics of ultracold atoms confined in optical lattices  1  . These experiments typically involve trapping cold neutral atoms in a standing wave pattern created by two counterpropagating laser fields which are tuned close together so as to form a deep periodic potential  2  .\nThe resulting atomic gas forms a Bose-Einstein condensate (BEC)  3  , where all the atoms occupy the same quantum state  4  . Such systems have attracted considerable interest because they allow one to explore fundamental questions about quantum mechanics  5  -  8  while also providing a platform for studying novel phenomena  9  -  11  . For example, these systems have recently been used to demonstrate superfluidity  12  , Josephson effects  13  , Bloch oscillations  14  , Landau-Zener tunneling  15  , and Anderson localization  16  . However, despite much theoretical work  17  -  20  , the full range of possible behaviors exhibited by these systems remains poorly understood  21  .\nOne reason why the behavior of these systems is difficult to predict is that the underlying potential is time-dependent  22  . Indeed, since the lasers forming the lattice are usually far off-resonant  23  , the amplitude of the potential varies periodically over timescales comparable to those associated with typical experimental parameters  24  . As a result, the energy spectrum of the system becomes quasi-periodic  25  and the usual methods of solving the Schrödinger equation break down  26  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Particle Motion in Rapidly Oscillating Potentials : The Role of the Potential s Initial Phase . Abstract : We study particle motion in rapidly oscillating potentials with arbitrary initial stages and find that , for particular values of this phase , particles can be trapped by the potential even if it is not continuous .We get exact solutions to the Schrödinger equation explaining such systems use Floquet theory and consider their physical implications . In particular we study an optical lattice formed by anti - propagating optical waves whose frequencies are detuned slightly from each other .This system has been studied frequently both theoretically and experimentally but our findings provide fresh insights into its mechanics . Introduction : - In recent years there have been many research on the physics of ultracold atoms confined in optical lattices 1 .These studies typically involve trap cold neutral ions in a standing wave pattern formed by two counterpropagating laser fields which are tuning close together so as to form a deep periodic potential 2 . The produced atomic liquid makes a Bose - Einstein condensate ( BEC ) 3 , where all the atoms occupy the same quantum state 4 .Such devices have garnered considerable interest because they allow one to examine fundamental questions about particle gravity 5 - 8 while also offering a platform for studying novel concepts 9 - 11 . For instance , these systems have recently been used to observe superfluidity 12 , Josephson effects 13 , Bloch oscillations 14 , Landau - Zener tunneling 15 , and Anderson localization 16 .However , despite much theoretical work 17 - 20 , the full range of possible behaviors exhibited by these systems appears poorly studied 21 . One reason why the behavior of these systems is harder to predict is that the underlying potential is period - dependent 22 .Indeed , since the lasers producing the lattice are typically much off - resonant 23 , the frequency of the potential varies continuously over timescales similar to those associated with typical empirical factors 24 . As a result , the power spectrum of the system enters quasi - periodic 25 and the usual techniques of solution the Schrödinger equation break down 26 .",
        "rewrite_text": "**Title:** Particle Motion in Rapidly Oscillating Potentials: The Influence of the Potential's Initial Phase\n\n**Abstract:** In this study, we investigate the dynamics of particles subjected to rapidly oscillating potentials characterized by arbitrary initial phases. Our findings reveal that for specific values of these phases, particles can become trapped within the potential, even in cases where the potential lacks continuity. We derive exact solutions to the Schrödinger equation for these systems by employing Floquet theory, which allows us to analyze the implications of our results in a physical context. A particular focus is placed on an optical lattice created by counter-propagating optical waves with slightly detuned frequencies. While this system has been extensively explored both theoretically and experimentally, our research offers new perspectives on its underlying mechanics. \n\nThe introduction highlights the recent surge in research concerning ultracold atoms confined within optical lattices. These investigations typically involve trapping cold neutral ions in a standing wave pattern generated by two counter-propagating laser beams, which are closely tuned to create a deep periodic potential. The resulting atomic ensemble forms a Bose-Einstein condensate (BEC), where all atoms occupy a single quantum state. Such systems have attracted significant interest due to their potential to address fundamental questions related to particle gravity, while also serving as a platform for exploring innovative concepts in quantum mechanics. Notable phenomena observed in these systems include superfluidity, Josephson effects, Bloch oscillations, Landau-Zener tunneling, and Anderson localization. Despite extensive theoretical efforts, the full spectrum of behaviors exhibited by these systems remains inadequately understood. One contributing factor to this complexity is the time-dependent nature of the underlying potential, as the lasers generating the lattice are often significantly off-resonant. Consequently, the frequency of the potential varies continuously over timescales comparable to those of typical experimental parameters, leading to a quasi-periodic power spectrum that complicates the application of conventional methods for solving the Schrödinger equation.",
        "ori-fast-z-score": -1.6759824573820066,
        "water-fast-z-score": 6.798896969362016,
        "rewrite-fast-z-score": 1.4142135623730951
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Helium abundance in galaxy clusters and Sunyaev-Zeldovich effect .\nAbstract:\nWe present new measurements of the helium mass fraction YHe = 0.24 ± 0.01 (statistical) ±0.02 (systematic), obtained by combining X-ray data on galaxy clusters with SZ observations, using the sample of 62 nearby relaxed galaxy clusters observed at high signal-to-noise ratio by Planck satellite. The results are consistent with previous determinations based on Chandra or XMM-Newton data alone. \n \n We also report an improved measurement of the Hubble constant H0 = 67.4±1.2 km s-1 Mpc-1, which is derived from our determination of the angular diameter distance to these clusters combined with their redshifts. This value agrees well with other recent estimates but has smaller statistical uncertainty than most of them. It is also compatible within 1 sigma with the local measurement inferred from Cepheid variables. \n \n Finally we use this dataset to test for possible deviations from standard cosmology due to massive neutrinos. Our analysis shows that current data do not allow us to detect any significant deviation from the predictions of ΛCDM model.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Helium abundance in galaxy clusters and Sunyaev - Zeldovich effect . Abstract : We report new studies of the helium mass fraction YHe = 0 . 24 ± 0 . 01 ( statistical ) ±0 . 02 ( systematic ) , obtained by combining X - ray data on galaxy clusters with SZ measurements , using the sample of 62 nearby relaxed galaxy galaxies studied at high signal - to - noise ratio by Planck satellite .The results are compatible with previous determinations based on Chandra or XMM - Newton data alone . We additionally report an better determination of the Hubble constant H0 = 67 . 4±1 . 2 km s - 1 Mpc - 1 , which is calculated from our determination of the angular length length to these clusters combined with their redshifts .This value agrees well with other recent estimates but has less statistical uncertainty than most of them . It is also consistent within 1 sigma with the local measurement inferred from Cepheid variables .Finally we utilize this dataset to test for probable deviations from standard cosmology resulting to massive neutrinos . Our study shows that current data do not enable us to identify any considerable deviation from the estimates of ΛCDM system .",
        "rewrite_text": "We present new findings on the helium mass fraction in galaxy clusters, reporting a value of YHe = 0.24 ± 0.01 (statistical) ± 0.02 (systematic). This result is derived from an analysis that combines X-ray observations of galaxy clusters with Sunyaev-Zeldovich (SZ) effect measurements. Our study utilizes a sample of 62 nearby relaxed galaxy clusters, which were examined at a high signal-to-noise ratio by the Planck satellite. The helium abundance we determined is consistent with previous measurements obtained from Chandra and XMM-Newton data alone. \n\nIn addition to the helium abundance, we provide an improved estimate of the Hubble constant, H0 = 67.4 ± 1.2 km s^-1 Mpc^-1. This value is calculated by correlating the angular diameter distances to the clusters with their redshift data. Our estimate aligns well with other recent determinations and exhibits lower statistical uncertainty compared to many of them. Furthermore, it remains consistent within one sigma with local measurements derived from Cepheid variable observations.\n\nLastly, we leverage this dataset to investigate potential deviations from standard cosmological models, particularly in relation to the presence of massive neutrinos. Our analysis indicates that the current data do not reveal any significant deviations from the predictions of the ΛCDM model. Overall, our findings contribute to the understanding of helium abundance in the universe and provide a refined measurement of the Hubble constant, while also testing the robustness of standard cosmological theories against new observational data.",
        "ori-fast-z-score": -0.10846522890932808,
        "water-fast-z-score": 5.467773927672753,
        "rewrite-fast-z-score": 1.1547005383792515
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Gamma-Ray Burst Afterglows as Probes of Environment and Blastwave Physics II: the Distribution of p and Structure of the Circumburst Medium .\nAbstract:\nWe present new results on the distribution of the electron index, p, in GRB afterglow models using data from Swift/BATSE bursts with known redshifts (z < 5). We find that the distribution is consistent with being log-normal for both low-z and high-z samples. The mean value of p decreases slightly with increasing redshift but there are no significant differences between the two distributions at fixed z. This suggests that the physical processes which determine the value of p do not evolve significantly over cosmic time scales. Using our best-fit values we show how the observed X-ray light curves can be used to constrain the structure of the circumburst medium. In particular, we demonstrate that it should be possible to distinguish between different density profiles by comparing the temporal decay slopes of the early-time X-ray emission with those measured later during the prompt phase. \n \n Keywords: Gamma-ray bursts, Electron index",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Gamma - Ray Burst Afterglows as Probes of Environment and Blastwave Physics II : the Distribution of p and Structure of the Circumburst Medium . Abstract : We report new data on the distribution of the electron index , p , in GRB afterglow models using data from Swift / BATSE bursts with reported redshifts ( z < 5 ) .We see that the distribution is compatible with being log - normal for both high - z and high - z data . The mean value of p varies slightly with increasing redshift but there are no considerable changes between the two distributions at fixed z .This implies that the physical processes which determine the value of p do not evolve substantially over cosmic time ranges . Using our better - fitting values we tell how the seen X - ray light curves can be used to constrain the composition of the circumburst medium .In particular , we prove that it should be possible to distinguish between various density profiles by using the temporal decay flanks of the early - time X - ray radiation with those observed later during the prompt phase . Keywords : Gamma - ray bursts , Electron index",
        "rewrite_text": "Title: Gamma-Ray Burst Afterglows as Probes of Environment and Blastwave Physics II: The Distribution of p and Structure of the Circumburst Medium\n\nAbstract: In this study, we present new insights into the distribution of the electron index, denoted as p, within gamma-ray burst (GRB) afterglow models, utilizing observational data from Swift and BATSE bursts that have reported redshifts (z < 5). Our analysis indicates that the distribution of p is consistent with a log-normal form for both high-redshift and low-redshift data sets. We observe that while the mean value of p exhibits slight variations with increasing redshift, there are no significant differences between the distributions at fixed redshift values. This finding suggests that the underlying physical mechanisms that influence the value of p remain relatively stable across extensive cosmic time scales. Furthermore, leveraging our refined fitting parameters, we demonstrate how the observed X-ray light curves can be employed to constrain the characteristics of the circumburst medium. Specifically, we establish that it is feasible to differentiate between various density profiles by analyzing the temporal decay slopes of early-time X-ray emissions in comparison to those recorded during the prompt phase. This work enhances our understanding of the environmental conditions surrounding GRBs and the dynamics of the blastwave, providing a framework for future investigations into the interplay between GRB afterglows and their surrounding media. \n\nKeywords: Gamma-ray bursts, Electron index",
        "ori-fast-z-score": -1.193117518002609,
        "water-fast-z-score": 4.824506406770077,
        "rewrite-fast-z-score": 1.9802950859533488
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Role of Dynamical Research in the Detection and Characterization of Exoplanets .\nAbstract:\nThe detection of exoplanets has been one of the most exciting recent developments in astronomy, but their characterization is still incomplete. The goal of this thesis was to develop new techniques for characterizing exoplanetary systems using dynamical methods. In particular, I studied how planets can be used as probes into the formation history of planetary systems by measuring the mass distribution of small bodies (planetesimals) that are trapped in mean motion resonances with them.  My first project investigated the effect of planetesimal size on the stability of planetary orbits. Using numerical simulations, we found that smaller planetesimals tend to destabilize planetary orbits more quickly than larger ones because they have higher orbital eccentricities. This result suggests that there may exist an upper limit to the size of planetesimals that form during planet migration. My second project examined the effects of mutual inclinations between planets on the stability of planetary systems. We showed that mutual inclination increases the likelihood of instability when two planets are locked in a 2:1 resonance. Finally, my third project explored the possibility of detecting terrestrial planets around white dwarfs through gravitational microlensing events.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Role of Dynamical Research in the Detection and Characterization of Exoplanets . Abstract : The observation of exoplanets has been one of the most exciting latest advances in science , but their characterization is already incomplete .The goal of this dissertation was to develop new method for characterizing exoplanetary regions using dynamical techniques . In particular , I studied how planets can be used as probes into the formation history of planetary structures by monitoring the mass distribution of tiny bodies ( planetesimals ) that are locked in mean motion resonances with them .My first project researched the impact of planetesimal size on the stability of planetary orbits . Using numerical simulations , we identified that smaller planetesimals prefer to destabilize planetary orbits more easily than larger ones because they have greater orbital eccentricities .This result suggests that there may exist an upper limitation to the extent of planetesimals that form during planet migration . My second work examined the effects of mutual inclinations between orbits on the stability of planetary networks .We showed that mutual inclination increases the probability of instability when two planets are locked in a 2 : 1 resonance . Finally , my third project researched the prospect of detecting terrestrial worlds around white dwarfs through gravity microlensing events .",
        "rewrite_text": "**Title:** The Role of Dynamical Research in the Detection and Characterization of Exoplanets\n\n**Abstract:** The exploration of exoplanets represents one of the most thrilling advancements in contemporary science; however, our understanding of their characteristics remains incomplete. This dissertation aims to introduce innovative methodologies for characterizing exoplanetary systems through the application of dynamical techniques. Specifically, I investigated the potential of planets as indicators of the formation history of planetary systems by analyzing the mass distribution of small bodies, known as planetesimals, that are engaged in mean motion resonances with these planets. \n\nIn my initial project, I focused on the influence of planetesimal size on the stability of planetary orbits. Through a series of numerical simulations, we discovered that smaller planetesimals tend to destabilize planetary orbits more readily than their larger counterparts, primarily due to their higher orbital eccentricities. This finding implies that there may be an upper limit to the size of planetesimals that can form during the migration of planets, which has significant implications for our understanding of planetary formation processes.\n\nMy second research endeavor examined the impact of mutual inclinations between planetary orbits on the stability of planetary systems. We demonstrated that increased mutual inclination heightens the likelihood of instability, particularly when two planets are in a 2:1 resonance. This insight contributes to our understanding of the dynamical interactions that govern the architecture of planetary systems.\n\nLastly, my third project explored the potential for detecting terrestrial planets orbiting white dwarfs through the phenomenon of gravitational microlensing. This approach could provide a novel avenue for identifying and characterizing exoplanets that are otherwise challenging to observe. Collectively, these studies underscore the critical role of dynamical research in enhancing our understanding of exoplanets and their formation, ultimately paving the way for future discoveries in the field of exoplanetary science.",
        "ori-fast-z-score": -0.6060915267313265,
        "water-fast-z-score": 5.527707983925667,
        "rewrite-fast-z-score": -0.7016464154456235
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Photon wave mechanics and position eigenvectors .\nAbstract:\nThe present work is devoted to the study of photon wave mechanics in terms of position eigenvectors, which are introduced as solutions of the Schrödinger equation for photons with an arbitrary energy spectrum. The concept of position eigenvector allows one to describe the state of a single photon by its position probability density distribution function (PDF). It also enables us to introduce the notion of quantum trajectory describing the evolution of this PDF over time. In particular, we show that the quantum trajectories corresponding to different initial states can be obtained from each other by means of unitary transformations. We demonstrate how these results may be used to analyze various phenomena related to the propagation of light through dispersive media. Finally, we discuss possible applications of our approach to the description of nonclassical effects associated with the emission of entangled pairs of photons. DOI: 10.1088/1742-6596/aa5e20\nI. INTRODUCTORY REMARkS\n\nIn recent years there has been considerable interest in developing new approaches to studying the properties of light fields based on the concepts of quantum optics  1–3  . One of such approaches involves introducing the so-called position eigenvectors  4  , which play an important role in the description of the state of a single-photon field  5–7  .\nIt should be noted that the use of position eigenvectors makes it possible not only to obtain information about the spatial structure of the electromagnetic field but also to investigate the temporal dynamics of the system under consideration  8, 9  . This fact opens up wide possibilities for applying the proposed method to analyzing various physical processes occurring during the propagation of light waves through dispersive media  10, 11  . \n \n In addition, the introduction of position eigenvectors into the theory of light fields leads to the possibility of using them to describe certain nonclassical effects associated",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Photon wave theory and position eigenvectors . Abstract : The present work is devoted to the study of photon wave theory in terms of position eigenvectors , which are introduced as solutions of the Schrödinger equation for photons with an arbitrary energy wavelength .The concept of position eigenvector allows one to define the state of a single photon by its position probability density distribution function ( PDF ) . It additionally permits us to introduce the notion of quantum path describing the evolution of this PDF over time .In particular , we prove that the quantum trajectories corresponding to different initial states can be obtained from each other by means of unitary transformations . We showed how these results may be used to analyze numerous phenomena related to the propagation of light through dispersive media .Finally , we explain potential uses of our approach to the description of nonclassical effects correlated with the emission of entangled pairs of photons . DOI : 10 . 1088 / 1742 - 6596 / aa5e20 I .INTRODUCTORY REMARkS In recent years there has been substantial interest in pursuing new approaches to investigating the properties of light fields relying on the concepts of quantum optics 1 – 3 . One of such approaches involves introducing the so - called position eigenvectors 4 , which take an important role in the description of the state of a single - photon field 5 – 7 .It should be mentioned that the using of position eigenvectors makes it necessary not only to obtain knowledge about the spatial shape of the electromagnetic field but also to examine the temporal composition of the system under consideration 8 , 9 . This fact offers up broad opportunities for applying the suggested method to investigating different mechanical phenomena occurring during the propagation of light beams through dispersive media 10 , 11 .In addition , the introduction of position eigenvectors into the physics of light fields leads to the prospect of using them to explain certain nonclassical effects associated",
        "rewrite_text": "The current study focuses on the exploration of photon wave theory through the lens of position eigenvectors, which are defined as solutions to the Schrödinger equation applicable to photons with varying energy wavelengths. By employing the concept of position eigenvectors, we can characterize the state of an individual photon using its position probability density distribution function (PDF). This framework also facilitates the introduction of a quantum path, which describes how this PDF evolves over time. Notably, we demonstrate that the quantum trajectories associated with different initial states can be transformed into one another through unitary transformations. Our findings have significant implications for analyzing various phenomena related to light propagation in dispersive media. Furthermore, we discuss the potential applications of our approach in understanding nonclassical effects linked to the emission of entangled photon pairs. \n\nIn recent years, there has been a growing interest in novel methodologies for examining the properties of light fields, particularly through the principles of quantum optics. One such methodology involves the incorporation of position eigenvectors, which play a crucial role in defining the state of single-photon fields. It is important to note that utilizing position eigenvectors necessitates a comprehensive understanding of both the spatial configuration of the electromagnetic field and the temporal dynamics of the system in question. This dual focus opens up extensive opportunities for applying our proposed method to investigate various mechanical phenomena that arise during the transmission of light through dispersive media. Additionally, the integration of position eigenvectors into the study of light fields paves the way for elucidating certain nonclassical effects that are associated with the generation of entangled photon pairs. \n\nDOI: 10.1088/1742-6596/aa5e20",
        "ori-fast-z-score": 2.301585822275002,
        "water-fast-z-score": 8.818163074019441,
        "rewrite-fast-z-score": 2.23606797749979
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Relic abundance of dark matter in universal extra dimension models with right-handed neutrinos .\nAbstract:\nWe study the relic density of dark matter (DM) produced by thermal freeze-out and freezein processes in Universal Extra Dimension (UED) model, where we include effects due to Majorana mass terms for the right handed neutrino fields. We find that the DM can be either fermionic or bosonic depending on the values of the parameters involved. In particular, if the UED scale is close to 1 TeV then the DM candidate could be a scalar particle which has been recently proposed as a possible explanation for the PAMELA positron excess observed at high energies. The results are presented in the context of LHC searches for new physics beyond Standard Model. Introduction:-The discovery of the Higgs-like resonance  1-3  at Large Hadron Collider (LHC), alongwith other experimental evidences  4  , have established the existence of a fundamental scalar field responsible for breaking electroweak symmetry. However, there still remain many open questions about the nature of this scalar sector such as: What is its spin? Is it CP-even or odd? Does it couple only to gauge bosons or also to fermions? Are there any additional scalars present in Nature ? These issues will be addressed once more data becomes available from ongoing experiments like ATLAS  5  and CMS  6  . On the theoretical front, one of the most interesting possibilities is to consider extensions of the Standard Model (SM). One possibility is to extend SM into higher dimensions  7-9 , thereby introducing Kaluza-Klein excitations of all particles  10  .\nIn recent years, several authors  11-13  studied the phenomenology of these theories in detail. It was shown that the lightest KaluzaKlein excitation of the graviton may act as cold Dark Matter (CDM)  14-16 . This scenario is particularly appealing since CDM constitutes around 23%  17  of the energy content of our universe  18  . Moreover, the presence of an extra spatial dimension opens up the possibility of producing Kaluza-Klein states through various production mechanisms  19-21  including decay  22  and annihilation  23  . Recently, it has been pointed out  24",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Relic abundance of dark matter in universal extra dimension models with right - handed neutrinos . Abstract : We research the relic quantity of bright matter ( DM ) produced by temperature freeze - out and freezein cycles in Universal Extra Dimension ( UED ) model , where we mention effects due to Majorana mass terms for the right handed neutrino fields .We see that the DM can be either fermionic or bosonic varying on the values of the variables required . In particular , if the UED scale is close to 1 TeV then the DM candidate could be a scalar electron which has been lately considered as a possible reason for the PAMELA positron excess observed at high energies .The results are presented in the context of LHC searches for new science beyond Standard Model . Introduction : - The observation of the Higgs - like resonance 1 - 3 at Large Hadron Collider ( LHC ) , alongwith other experimental evidences 4 , have discovered the existence of a basic scalar field involved for breaking electroweak symmetry .However , there still continue several open questions about the nature of this scalar sector such as : What is its spin ? Is it CP - even or odd ?Does it couple only to gauge bosons or also to fermions?Are there any additional scalars present in Nature ?These issues will be addressed once more data becomes available from continued experiments like ATLAS 5 and CMS 6 . On the theoretical front , one of the most exciting possibilities is to consider extensions of the Standard Model ( SM ) .One possibility is to expanded SM into greater dimensions 7 - 9 , thereby introducing Kaluza - Klein excitations of all particles 10 . In recent years , various scientists 11 - 13 examined the phenomenology of these theories in detail .It was shown that the lightest KaluzaKlein excitation of the graviton could act as cold Dark Matter ( CDM ) 14 - 16 . This scenario is especially appealing since CDM constitutes around 23 % 17 of the power content of our universe 18 .Moreover , the presence of an additional spatial dimension opens up the prospect of creating Kaluza - Klein states through several production mechanisms 19 - 21 notably decay 22 and annihilation 23 . Recently , it has been pointed out 24",
        "rewrite_text": "**Title:** Relic Abundance of Dark Matter in Universal Extra Dimension Models with Right-Handed Neutrinos\n\n**Abstract:** In this study, we investigate the relic abundance of dark matter (DM) generated through thermal freeze-out and freeze-in processes within the framework of Universal Extra Dimensions (UED) models, specifically considering the implications of Majorana mass terms associated with right-handed neutrino fields. Our analysis reveals that the nature of DM can be either fermionic or bosonic, contingent upon the specific parameter values employed in the model. Notably, if the UED scale approaches 1 TeV, the DM candidate may be identified as a scalar electron, which has recently been proposed as a potential explanation for the PAMELA positron excess observed at high energies. Our findings are contextualized within the ongoing searches for new physics beyond the Standard Model (SM) at the Large Hadron Collider (LHC).\n\nThe discovery of a Higgs-like resonance at the LHC, coupled with various experimental observations, has confirmed the existence of a fundamental scalar field responsible for electroweak symmetry breaking. However, numerous questions remain regarding the characteristics of this scalar sector, including its spin, CP properties, and coupling mechanisms to gauge bosons and fermions. Additionally, the existence of other scalar particles in nature remains an open question, which may be addressed with forthcoming data from experiments such as ATLAS and CMS.\n\nTheoretically, one of the most intriguing avenues is the extension of the SM into higher dimensions, which introduces Kaluza-Klein excitations for all fundamental particles. Recent studies have explored the phenomenology of these higher-dimensional theories, demonstrating that the lightest Kaluza-Klein excitation of the graviton could serve as a candidate for cold dark matter, a component that constitutes approximately 23% of the universe's energy density. The introduction of an extra spatial dimension not only enhances the theoretical framework but also facilitates the production of Kaluza-Klein states through various mechanisms, including decay and annihilation processes. This research contributes to the broader understanding of dark matter in the context of UED models and highlights the potential for new discoveries in particle physics.",
        "ori-fast-z-score": 0.2544566789039913,
        "water-fast-z-score": 6.821430560638804,
        "rewrite-fast-z-score": 2.197401062294143
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Diversity of Decline-Rate-Corrected Type Ia Supernova Rise Times: One Mode or Two? .\nAbstract:\nWe present the results of an analysis of rise times for a sample of type Ia supernovae (SNeIa) with well-measured light curves and redshifts in order to determine whether there is one mode of SNeIa rise time, as suggested by Phillips et al. (1999), or two modes, as suggested by Riess et al. (1999). We find that our data are consistent with either model at the 2-sigma level; however, we cannot rule out the possibility of only one mode being present. The best-fit values for the parameters describing each distribution differ significantly between these models. If future observations confirm this result, it will have important implications on cosmological studies using SNeIa as distance indicators. In particular, if there really are two populations of SNeIa, then the use of a single value for the stretch parameter may lead to systematic errors in determining distances. \n \n Keywords: Supernova, Light curve",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  Diversity of Decline-Rate-Corrected Type Ia Supernova Rise Times: One Mode or Two?.Abstract : We present the results of an assessment of rise periods for a sample of type Ia supernovae ( SNeIa ) with good - measured light curves and redshifts in order to find whether there is one mode of SNeIa rise history , as suggested by Phillips et al . ( 1999 ) , or two modes , as suggested by Riess et al .( 1999 ) . We see that our statistics are compatible with either model at the 2 - sigma level ; however , we cannot leave out the suggestion of only one mode being present .The best - fitting values for the variables describing each distribution depend greatly between these models . If future discoveries confirm this result , it will have important implications on cosmological experiments using SNeIa as distance indicators .In particular , if there really are two groups of SNeIa , then the using of a single value for the stretch variable might lead to systematic errors in calculating distances . Keywords : Supernova , Light curve",
        "rewrite_text": "Title: Diversity of Decline-Rate-Corrected Type Ia Supernova Rise Times: One Mode or Two?\n\nAbstract: In this study, we investigate the rise times of a selected sample of Type Ia supernovae (SNeIa) characterized by well-measured light curves and redshifts. Our objective is to determine whether the rise history of SNeIa follows a single mode, as proposed by Phillips et al. (1999), or if it is better represented by two distinct modes, as suggested by Riess et al. (1999). Our analysis indicates that the statistical evidence supports both models at a 2-sigma confidence level, leaving open the possibility of a singular mode of rise. Notably, the parameters that best fit the distributions of rise times exhibit significant variability depending on the model applied. Should future observations corroborate our findings, the implications for cosmological research utilizing SNeIa as distance indicators could be profound. Specifically, the existence of two separate groups of SNeIa would necessitate caution in employing a uniform stretch parameter for distance calculations, as this could introduce systematic biases. Our results underscore the importance of further investigation into the rise characteristics of SNeIa to enhance the accuracy of cosmological measurements. \n\nKeywords: Supernova, Light curve",
        "ori-fast-z-score": -1.4142135623730951,
        "water-fast-z-score": 4.330522446256832,
        "rewrite-fast-z-score": 0.4364357804719848
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  HUDF-JD2: Mid-infrared Evidence for a z~2 Luminous Infrared Galaxy .\nAbstract:\nWe present new mid-IR photometry and spectroscopy of the HUDF-JD2 galaxy at redshift 2.081, which is one of the most luminous infrared galaxies known to date. The SED shows that it has an extremely red continuum with strong PAH emission features in its rest frame optical spectrum. We find evidence for both star formation activity (from the UV-optical) as well as obscured AGN activity (from X-ray observations). This object may be representative of a population of dusty star-forming galaxies undergoing rapid evolution during this critical epoch when massive black holes are growing rapidly along with their host galaxies. Keywords: Infrared, Redshift, Spectroscopy, Photometry, Black Hole Growth, Star Formation Rate Density, Ultraviolet Background Radiation, Cosmic Evolution, Cosmology, Extragalactic Astronomy, High Energy Astrophysics, Space Science, Nearby Galaxies",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : HUDF - JD2 : Mid - infrared Evidence for a z ~ 2 Luminous Infrared Galaxy . Abstract : We present new mid - IR photometry and spectroscopy of the HUDF - JD2 galaxy at redshift 2 . 081 , which is one of the most luminous infrared galaxies known to date .The SED shows that it has an exceptionally red continuum with powerful PAH emission elements in its rest frame optical spectrum . We get confirmation for both star formation activity ( from the UV - optical ) as well as obscured AGN activity ( from X - ray observations ) .This object may be representative of a population of dusty star - creating stars undergoing fast evolution during this critical epoch when massive blue holes are growing rapidly along with their host galaxies . Keywords : Infrared , Redshift , Spectroscopy , Photometry , Black Hole Growth , Star Formation Rate Density , Ultraviolet Background Radiation , Cosmic Evolution , Cosmology , Extragalactic Astronomy , High Energy Astrophysics , Space Science , Nearby Galaxies",
        "rewrite_text": "Title: HUDF-JD2: Mid-Infrared Evidence for a z ~ 2 Luminous Infrared Galaxy\n\nAbstract: In this study, we present comprehensive mid-infrared photometric and spectroscopic observations of the HUDF-JD2 galaxy, located at a redshift of 2.081, making it one of the most luminous infrared galaxies identified to date. Our analysis reveals a strikingly red spectral energy distribution (SED), characterized by significant polycyclic aromatic hydrocarbon (PAH) emission features in its rest-frame optical spectrum. These findings provide robust evidence of ongoing star formation, as indicated by ultraviolet to optical data, alongside indications of obscured active galactic nucleus (AGN) activity supported by X-ray observations. The characteristics of HUDF-JD2 suggest that it may exemplify a class of dusty star-forming galaxies that are experiencing rapid evolutionary processes during a pivotal period in cosmic history, marked by the swift growth of massive black holes in conjunction with their host galaxies. This research contributes to our understanding of the interplay between star formation and black hole growth in the early universe, shedding light on the conditions that foster the development of such luminous infrared galaxies. Our findings have significant implications for the study of cosmic evolution, the density of star formation rates, and the role of ultraviolet background radiation in shaping the characteristics of nearby galaxies. This work is situated within the broader context of extragalactic astronomy and high-energy astrophysics, offering insights into the dynamics of galaxy formation and evolution during a critical epoch in the cosmos. \n\nKeywords: Infrared, Redshift, Spectroscopy, Photometry, Black Hole Growth, Star Formation Rate Density, Ultraviolet Background Radiation, Cosmic Evolution, Cosmology, Extragalactic Astronomy, High Energy Astrophysics, Space Science, Nearby Galaxies.",
        "ori-fast-z-score": 0.13245323570650439,
        "water-fast-z-score": 2.888741522913896,
        "rewrite-fast-z-score": 2.0252641593763117
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Behavioral response to strong aversive stimuli: A neurodynamical model .\nAbstract:\nWe propose an integrative computational framework for the study of behavioral responses to strong aversive stimuli, such as pain and fearful events. The proposed approach is based on a novel combination of two complementary theoretical frameworks that have been developed in recent years within neuroscience research: (i) dynamical systems theory; and (ii) reinforcement learning theory. We show how these theories can be combined into one coherent mathematical formalism by introducing a new concept called  state-action value function . This concept allows us to define a single objective function which describes both the dynamics of the internal states of the brain during emotional experiences and the way in which external actions are selected depending on those internal states. Finally, we illustrate our proposal with some examples taken from experimental data obtained using functional magnetic resonance imaging techniques. In particular, we focus on the analysis of neural activity recorded while subjects were exposed to painful thermal stimulations or when they observed images depicting threatening scenes.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Behavioral response to strong aversive cues : A neurodynamical theory . Abstract : We suggest an integrative computational framework for the study of behavioral responses to powerful aversive cues , such as pain and fearful events .The proposed approach is based on a novel combination of two complementary theoretical frameworks that have been used in recent years within neuroscience study : ( i ) dynamical systems theoretical ; and ( ii ) reinforcement learning model . We see how these theories can be merged into one comprehensive mathematical formalism by introducing a new notion called state - action value function .This concept allows us to define a single objective function which explains both the dynamics of the internal states of the brain during emotional experiences and the way in which external actions are chosen depending on those inner states . Finally , we explain our proposal with some examples taken from experimental evidence derived using functional magnetic resonance imaging technology .In particular , we focus on the examination of neural activity observed while subjects were subject to traumatic heating stimulations or when they witnessed pictures portraying threatening situations .",
        "rewrite_text": "We present a comprehensive computational framework aimed at understanding behavioral responses to intense aversive stimuli, such as pain and fear-inducing events. Our approach integrates two complementary theoretical frameworks that have gained traction in recent neuroscience research: dynamical systems theory and reinforcement learning models. By merging these theories, we introduce a novel concept known as the state-action value function, which serves as a unifying mathematical formalism. This new concept enables us to articulate a singular objective function that elucidates the dynamics of the brain's internal states during emotional experiences, as well as the selection of external actions based on these internal states. To illustrate our framework, we provide examples derived from experimental studies utilizing functional magnetic resonance imaging (fMRI) technology. Specifically, we analyze neural activity patterns observed in subjects exposed to traumatic heat stimuli and those who viewed images depicting threatening scenarios. Our findings aim to enhance the understanding of how the brain processes aversive cues and informs behavioral responses, contributing to the broader field of neuroscience and behavioral science.",
        "ori-fast-z-score": -0.2,
        "water-fast-z-score": 5.671711984196938,
        "rewrite-fast-z-score": 1.7085642859406605
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Ionization Fraction of Barnard 68: Implications for Star and Planet Formation .\nAbstract:\nWe present the first measurement of the ionization fraction in an isolated dark cloud, using observations with Herschel Space Observatory s Heterodyne Instrument for the Far Infrared (HIFI). We detect emission lines from C+, O+, N+, S+, Si+, Ar+, Fe+, HCO+, H2S+, and H2O+ toward the core of Barnard 68 (B68), which is located at a distance of 140 pc. The observed line intensities are used to derive the fractional ionizations of carbon, oxygen, nitrogen, sulfur, silicon, argon, iron, hydrogen cyanide, hydrosulfide, and water molecules relative to molecular hydrogen. These results show that B68 has a low degree of ionization compared to other star-forming regions such as Orion A or W51 e1/e2. This suggests that star formation may be suppressed by magnetic fields and/or cosmic rays in this region.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Ionization Fraction of Barnard 68 : Implications for Star and Planet Formation . Abstract : We present the first measurement of the ionization fraction in an small dark cloud , using observations with Herschel Space Observatory s Heterodyne Instrument for the Far Infrared ( HIFI ) .We detect emission lines from C + , O + , N + , S + , Si + , Ar + , Fe + , HCO + , H2S + , and H2O + toward the core of Barnard 68 ( B68 ) , which is situated at a distance of 140 pc . The observed line intensities are using to derive the fractional ionizations of carbon , hydrogen , nitrogen , sulfur , silicon , argon , iron , hydrogen cyanide , hydrosulfide , and water molecules relative to molecular hydrogen .These data reveal that B68 has a small level of ionization compared to other star - creating areas such as Orion A or W51 e1 / e2 . This implies that galaxy formation might be suppressed by magnetic fields and / or cosmic rays in this area .",
        "rewrite_text": "We present a groundbreaking study that marks the first measurement of the ionization fraction within a small dark cloud, specifically focusing on Barnard 68 (B68). Utilizing the advanced capabilities of the Herschel Space Observatory's Heterodyne Instrument for the Far Infrared (HIFI), we have successfully detected a range of emission lines from various ions, including C+, O+, N+, S+, Si+, Ar+, Fe+, HCO+, H2S+, and H2O+. Located at a distance of 140 parsecs, B68 serves as an intriguing subject for our investigation into the ionization processes occurring in dark clouds.\n\nBy analyzing the observed line intensities, we derive the fractional ionizations of key elements and molecules—namely carbon, hydrogen, nitrogen, sulfur, silicon, argon, iron, hydrogen cyanide, hydrosulfide, and water—relative to molecular hydrogen. Our findings indicate that the ionization level in B68 is significantly lower than that found in other prominent star-forming regions, such as Orion A and W51 e1/e2. This observation suggests that the processes governing star and planet formation in B68 may be influenced by external factors, potentially including the effects of magnetic fields and cosmic rays.\n\nThe implications of our results extend beyond the immediate characteristics of B68, hinting at broader consequences for galaxy formation in similar environments. The reduced ionization fraction observed in this dark cloud could indicate a suppression of star formation activities, raising important questions about the role of magnetic fields and cosmic rays in the evolution of molecular clouds. Our study contributes valuable insights into the complex interplay between ionization processes and star formation, paving the way for future research in understanding the dynamics of dark clouds and their impact on galactic evolution.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 3.5762373640756184,
        "rewrite-fast-z-score": 1.403292830891247
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Counting BPS operators in N=4 SYM .\nAbstract:\nWe present an algorithm for counting the number of supersymmetric states with given charge and spin in four-dimensional super Yang-Mills theory (N=4 SYM). The method is based on the use of integrability techniques to compute the spectrum of anomalous dimensions, which are then used as input into a recursion relation that counts the number of states at each level of the Bethe ansatz. We test our results against known exact answers for small values of the charges and find agreement up to numerical precision. Finally we discuss how this approach can be extended beyond the leading order approximation by including corrections due to wrapping interactions between magnons. In recent years there has been considerable interest in understanding the structure of quantum field theories using integrable systems methods  1  . One particularly interesting application of these ideas concerns the study of gauge/string dualities  2  , where it was shown that certain quantities computed in one description could be related to those obtained in another via the so-called AdS/CFT correspondence  3  .\nIn particular, the spectrum of anomalous dimension of local gauge-invariant composite operators plays a crucial role in determining physical observables such as correlation functions  4  or Wilson loops  5  . It turns out that many properties of the spectrum of anomalous dimen-sions can be determined exactly  6  -  8  , making it possible to obtain precise predictions about the behaviour of various physical quantities  9  -  11  . However, despite significant progress  12  -  16  , the problem of computing the full spectrum remains open  17  .\nThe aim of this work is to develop new computational tools for studying the spectrum of anomalous dimensionality of composite operators in N = 4 Super-Yang Mills Theory (SYMT)  18  . This will allow us to make further tests of the AdS/CFT conjecture and also provide insight into the nature of non-perturbative effects in strongly-coupled gauge theories  19  . Our main motivation comes from the fact that the spectrum of anomalous-dimension matrices in SYMT is described by the celebrated Bethe Ansatz  20  . As a result, the computation of the spectrum reduces to solving a set of coupled integral equations  21  whose solution requires sophisticated numerical",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Counting BPS operators in N = 4 SYM . Abstract : We present an algorithm for counting the proportion of supersymmetric states with particular charge and spin in four - dimensional super Yang - Mills theory ( N = 4 SYM ) .The method is based on the using of integrability methods to compute the spectrum of anomalous dimensions , which are then used as input into a recursion relation that counts the number of states at each level of the Bethe ansatz . We test our findings against known exact explanations for little values of the charges and find agreement up to numerical precision .Finally we explain how this methodology can be improved beyond the leading order approximation by including corrections due to wrapping interactions between magnons . In recent years there has been substantial interest in understanding the structure of quantum field theories employing integrable systems methods 1 .One especially interesting application of these ideas concerns the observation of gauge / string dualities 2 , where it was shown that particular quantities computed in one description could be connected to those acquired in another via the so - called AdS / CFT relationship 3 . In particular , the spectrum of anomalous dimension of local gauge - invariant composite operators plays a crucial role in establishing physical observables such as correlation functions 4 or Wilson loops 5 .It turns out that several properties of the spectrum of anomalous dimen - sions can be determined exactly 6 - 8 , making it necessary to obtain precise predictions about the behaviour of several physical substances 9 - 11 . However , despite considerable progress 12 - 16 , the issue of computing the full spectrum stays open 17 .The goal of this research is to develop new computational tools for studying the spectrum of anomalous dimensionality of composite operators in N = 4 Super - Yang Mills Theory ( SYMT ) 18 . This will provide us to make further tests of the AdS / CFT conjecture and also bring knowledge into the nature of non - perturbative processes in weakly - coupled gauge fields 19 .Our main motivation arises from the fact that the spectrum of anomalous - dimension matrices in SYMT is characterized by the celebrated Bethe Ansatz 20 . As a result , the computation of the spectrum reduces to solving a setting of coupled integral equations 21 whose solve needs modern numerical",
        "rewrite_text": "We introduce a novel algorithm designed to quantify the number of supersymmetric states with specific charge and spin within the framework of four-dimensional N = 4 super Yang-Mills theory (SYM). Our approach leverages integrability techniques to calculate the spectrum of anomalous dimensions, which subsequently serve as input for a recursion relation that enumerates the states at each level of the Bethe ansatz. We validate our results against established exact solutions for small charge values, achieving consistency within numerical precision. Furthermore, we discuss potential enhancements to our methodology that extend beyond the leading-order approximation by incorporating corrections arising from wrapping interactions among magnons.\n\nIn recent years, there has been a growing interest in the application of integrable systems methods to unravel the complexities of quantum field theories. A particularly compelling aspect of this research is the exploration of gauge/string dualities, which reveal that certain quantities computed in one theoretical framework can be related to those derived in another through the AdS/CFT correspondence. The spectrum of anomalous dimensions of local gauge-invariant composite operators is pivotal in determining key physical observables, such as correlation functions and Wilson loops. Notably, several characteristics of the anomalous dimension spectrum can be computed exactly, underscoring the necessity for precise predictions regarding the behavior of various physical systems.\n\nDespite significant advancements in the field, the challenge of fully computing the spectrum remains unresolved. This study aims to develop innovative computational tools to investigate the anomalous dimension spectrum of composite operators in N = 4 SYM. Such advancements will facilitate further testing of the AdS/CFT conjecture and enhance our understanding of non-perturbative phenomena in weakly-coupled gauge theories. Our primary motivation stems from the fact that the anomalous dimension matrices in SYM are governed by the renowned Bethe Ansatz, allowing the problem of spectrum computation to be reframed as solving a set of coupled integral equations, which necessitates modern numerical techniques for resolution.",
        "ori-fast-z-score": 0.6172133998483676,
        "water-fast-z-score": 6.74929190185686,
        "rewrite-fast-z-score": -1.0650014966747527
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Spatial separation of small and large grains in the transitional disk around the young star IRS 48 .\nAbstract:\nWe present new near-infrared (NIR) polarimetric observations of the Herbig Ae star HD 142527, which reveal that its circumstellar dust is composed of two distinct populations with different grain sizes. The polarization degree decreases rapidly towards longer wavelengths at all positions along our slit except for one position where it increases again between 2.2 and 3.8 microns. We interpret this as evidence for an inner hole in the distribution of larger grains. This interpretation is supported by SED modeling using radiative transfer calculations including scattering off spherical particles. Our results suggest that the outer edge of the gap lies within 0.1 AU of the central star. In addition to the NIR data presented here we also obtained mid-infrared (MIR) spectro-polarimetry covering the wavelength range 5-20 micron. These data show no significant change in the polarization degree across the MIR bands indicating that there are no strong changes in the optical properties of the dust grains on these scales.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Spatial splitting of tiny and large grains in the transitional disk around the early star IRS 48 . Abstract : We report new near - infrared ( NIR ) polarimetric discoveries of the Herbig Ae star HD 142527 , which confirm that its circumstellar dust is composed of two separate populations with varying grain sizes .The polarization degree reduces rapidly towards faster wavelengths at all positions along our slit except for one position where it rises again between 2 . 2 and 3 . 8 microns . We interpret this as proof for an inner hole in the distribution of bigger grains .This interpretation is backed by SED modeling using radiative transfer calculations including scattering off spherical objects . Our results show that the outer edge of the gap exists within 0 . 1 AU of the main star .In addition to the NIR data provided here we also produced mid - infrared ( MIR ) spectro - polarimetry covering the frequency spectrum 5 - 20 micron . These data demonstrate no major shift in the polarization degree across the MIR groups implying that there are no strong changes in the optical properties of the dust grains on these scales .",
        "rewrite_text": "We present new findings from near-infrared (NIR) polarimetric observations of the Herbig Ae star HD 142527, which reveal the presence of two distinct populations of circumstellar dust grains with differing sizes. Our analysis indicates a significant decrease in the degree of polarization at shorter wavelengths across all positions along our observational slit, with the exception of a specific location where polarization increases again between 2.2 and 3.8 microns. This behavior suggests the existence of an inner cavity in the distribution of larger grains surrounding the star. To support this interpretation, we conducted spectral energy distribution (SED) modeling utilizing radiative transfer calculations that account for scattering by spherical particles. The findings indicate that the outer boundary of this gap is located within 0.1 AU from the central star. In addition to the NIR observations, we also performed mid-infrared (MIR) spectro-polarimetry across the wavelength range of 5 to 20 microns. The MIR data reveal no significant variation in the degree of polarization among the different MIR bands, suggesting that the optical properties of the dust grains remain relatively consistent across these scales. These results contribute to our understanding of the dust grain composition and distribution in the transitional disk around IRS 48, highlighting the complex interactions between dust and radiation in the early stages of stellar evolution.",
        "ori-fast-z-score": 0.9761870601839528,
        "water-fast-z-score": 6.539886462510548,
        "rewrite-fast-z-score": 1.4
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Quantum Quenches in Extended Systems .\nAbstract:\nWe study the dynamics of quantum quenches in extended systems, focusing on the case where the initial state is an eigenstate of some local operator and the final Hamiltonian has no such symmetry. We show that for generic states this leads to relaxation towards equilibrium with a characteristic time scale which grows exponentially with system size. This behavior can be understood by considering the effect of rare regions whose energy gap scales as $1/N$, leading to exponential decay of correlations at late times. In contrast, we find that if the initial state is chosen to have maximal overlap with the ground state of the final Hamiltonian then there are no relaxation effects whatsoever. Finally, we discuss how these results may be relevant to experiments studying cold atoms in optical lattices. \nI. INTRODUCTORY REMARK\nThe problem of understanding the nonequilibrium dynamics following a sudden change in parameters (the so-called quench) continues to attract considerable interest both theoretically  1  and experimentally  2  . The main focus so far has been on closed quantum many-body systems described by Hamiltonians with short-range interactions  3  , but recently attention has shifted to open quantum systems  4  .\nIn particular, it was shown  5  that even when the initial state is highly excited, the evolution after a global quench will eventually relax into thermal equilibrium  6  . However, recent studies  7, 8  suggest that relaxation does not occur generically in open quantum systems, i.e., when the initial state is prepared by coupling the system to another reservoir or bath  9  . Instead, one expects equilibration only under certain conditions  10  : For example, if the initial state is close enough to the ground state of the new Hamiltonian  11  , or if the spectrum of the post-quench Hamiltonian contains a sufficiently large number of low-lying excitations  12  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Quantum Quenches in Extended Systems . Abstract : We research the dynamics of quantum quenches in extended systems , concentrating on the case where the first state is an eigenstate of some local operator and the finished Hamiltonian has no such symmetry .We see that for generic states this results to relaxation towards equilibrium with a typical time scale which increases exponentially with system height . This phenomenon can be understood by using the effect of rare regions whose power deficit scales as $ 1 / N $ , leading to exponential decay of correlations at late times .In contrast , we find that if the first state is chosen to have maximal overlap with the ground state of the final Hamiltonian then there are no relaxation effects whatsoever . Finally , we explain how these results may be applicable to experiments studying cold molecules in optical lattices .I . INTRODUCTORY REMARK The question of knowing the nonequilibrium dynamics following a sudden drop in parameters ( the so - called quench ) continues to attract considerable interest both theoretically 1 and experimentally 2 .The main attention so far has been on shut molecular several - bodies systems represented by Hamiltonians with short - range coupling 3 , but recently attention has transferred to close quantum systems 4 . In particular , it was shown 5 that even when the first state is strongly excited , the evolution after a global quench will eventually relax into heat equilibrium 6 .However , recent studies 7 , 8 suggest that relaxation does not occur generically in open quantum systems , i . e . , when the first state is prepared by bonding the system to another reservoir or bath 9 . Instead , one expects equilibration only under certain conditions 10 : For instance , if the first state is close enough to the ground state of the new Hamiltonian 11 , or if the spectrum of the post - quench Hamiltonian includes a enough large number of high - lying excitations 12 .",
        "rewrite_text": "In this study, we investigate the dynamics of quantum quenches in extended systems, focusing on scenarios where the initial state is an eigenstate of a local operator, while the final Hamiltonian lacks such symmetry. Our findings indicate that for generic initial states, the system exhibits relaxation towards equilibrium, characterized by a typical time scale that increases exponentially with the system size. This behavior can be attributed to the influence of rare regions within the system, where the power deficit scales as \\(1/N\\), resulting in an exponential decay of correlations at later times. Conversely, we observe that if the initial state is selected to have maximal overlap with the ground state of the final Hamiltonian, relaxation effects are absent altogether. \n\nThese results have significant implications for experimental setups involving cold molecules in optical lattices, where understanding the nonequilibrium dynamics following a quench is crucial. The phenomenon of quantum quenches has garnered substantial interest in both theoretical and experimental contexts, particularly in the realm of many-body systems governed by Hamiltonians with short-range interactions. Recent research has shifted focus towards closed quantum systems, revealing that even when the initial state is highly excited, the system will ultimately relax into thermal equilibrium after a global quench. However, emerging studies suggest that relaxation may not be a universal outcome in open quantum systems, where the initial state is prepared by coupling the system to an external reservoir. In such cases, equilibration is expected to occur only under specific conditions, such as when the initial state is sufficiently close to the ground state of the new Hamiltonian or when the spectrum of the post-quench Hamiltonian contains a substantial number of high-energy excitations. This work contributes to the ongoing discourse on nonequilibrium dynamics in quantum systems and provides insights that may enhance our understanding of experimental observations in quantum many-body physics.",
        "ori-fast-z-score": -0.24743582965269675,
        "water-fast-z-score": 6.680767400622813,
        "rewrite-fast-z-score": 0.23643312187173018
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Almost Product Evaluation of Hankel Determinants .\nAbstract:\nWe present an algorithm for the evaluation of Hankel determinants that is almost as efficient as the product formula, but avoids numerical instability problems associated with it.  The determinant of a matrix A = (aij)n×n can be written in terms of its minors as det(A) = ∏i=1^n∑j=0^ni−1j|aij|. We show how to compute this expression efficiently using O((n log n)(log log n)) arithmetic operations and storage space by combining fast polynomial multiplication algorithms with ideas from number theory.   Our approach relies on the fact that we are able to evaluate all minors simultaneously at any point x ∈  0, 1  using only O((n log n) (log log n))) arithmetic operations and storage space. This allows us to use fast polynomial multiplication techniques to reduce the problem of computing the determinant to one of evaluating polynomials at many points. Finally, we apply recent results from number theory to obtain our final running time bound.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Almost Product Evaluation of Hankel Determinants . Abstract : We create an algorithm for the evaluation of Hankel determinants that is almost as efficient as the product formula , but avoids numerical instability problems involved with it .The determinant of a matrix A = ( aij ) n×n can be written in terms of its minors as det ( A ) = [UNK] = 1 ^ [UNK] = 0 ^ ni−1j | aij | . We see how to compute this representation efficiently using O ( ( n log k ) ( log log k ) ) arithmetic operations and storage space by combining fast polynomial multiplication algorithms with ideas from number theory .Our solution relies on the fact that we are able to analyze all minors independently at any point x ∈ 0 , 1 using only O ( ( n log k ) ( log log k ) ) ) arithmetic operations and memory space . This enables us to use fast polynomial multiplication techniques to reduce the question of computing the determinant to one of evaluating polynomials at many points .Finally , we apply latest findings from number theory to obtain our final run time bound .",
        "rewrite_text": "Title: Almost Product Evaluation of Hankel Determinants\n\nAbstract: In this study, we introduce a novel algorithm for the evaluation of Hankel determinants that achieves efficiency comparable to traditional product formulas while circumventing the numerical instability typically associated with them. The determinant of an n×n matrix A = (aij) can be expressed in terms of its minors, represented as det(A) = ∑ (−1)^(i+j) * aij * det(Mij), where Mij denotes the minor matrix obtained by removing the ith row and jth column. Our approach leverages advanced techniques in polynomial multiplication and insights from number theory to compute this representation with a complexity of O((n log k)(log log k)) in both arithmetic operations and memory usage. \n\nA key aspect of our method is the ability to analyze all minors independently at any point x ∈ [0, 1], which allows us to efficiently evaluate the determinant by transforming the problem into one of polynomial evaluation at multiple points. By employing fast polynomial multiplication algorithms, we significantly streamline the computation process. Furthermore, we incorporate recent advancements in number theory to refine our runtime analysis, ultimately establishing a robust upper bound for the algorithm's performance. This work not only enhances the computational efficiency of Hankel determinant evaluations but also contributes to the broader field of numerical linear algebra by providing a stable alternative to existing methods. Our findings have potential implications for various applications where Hankel matrices are prevalent, paving the way for future research in this area.",
        "ori-fast-z-score": 1.1043152607484654,
        "water-fast-z-score": 5.019011475427825,
        "rewrite-fast-z-score": 1.2567574357593625
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Origin of the 60K plateau in YBa_2Cu_3O_6+x .\nAbstract:\nWe have studied the origin of the 60 K plateau in YBa2Cu3Ox by measuring the specific heat and magnetic susceptibility as functions of temperature, field strength, oxygen content x, and doping level p. The results show that the 60 K anomaly is due to an antiferromagnetic transition at TAF = 56 K for x= 0.6 but disappears with increasing x or decreasing p. We also find that the low-temperature upturns observed in both C/T and χ(T) are caused by impurity phases which appear when x > 0.65.  These results suggest that the 60 K anomality may be related to the appearance of these impurities. In addition we found that the high-field magnetization shows a sharp increase below 50 K indicating that there exists another phase transition near this temperature. This new phase has been identified as a charge density wave state (CDW). Finally, our data indicate that the CDW state appears only if the sample contains some amount of impurities.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Origin of the 60K plateau in YBa _ 2Cu _ 3O _ 6 + x . Abstract : We have researched the origin of the 60 K plateau in YBa2Cu3Ox by measuring the specific heat and magnetic susceptibility as functions of temperature , field intensity , hydrogen content x , and doping grade p . The results show that the 60 K anomaly is due to an antiferromagnetic transition at TAF = 56 K for x = 0 . 6 but disappears with rising x or decreasing p . We additionally find that the high - temperature upturns observed in both C / T and χ ( T ) are created by impurity phases which appear when x > 0 . 65 .These data suggest that the 60 K anomality may be connected to the appearance of these impurities . In addition we concluded that the high - field magnetization shows a sharp increase below 50 K indicating that there exists another phase shift near this heat .This new phase has been described as a charge density wave state ( CDW ) . Finally , our statistics indicate that the CDW state emerges only if the sample comprises some number of impurities .",
        "rewrite_text": "Title: Origin of the 60K Plateau in YBa2Cu3O6+x\n\nAbstract: In this study, we investigate the underlying mechanisms responsible for the 60 K plateau observed in YBa2Cu3O6+x by conducting a series of measurements on specific heat and magnetic susceptibility across varying temperatures, magnetic field strengths, hydrogen content (x), and doping levels (p). Our findings reveal that the 60 K anomaly is linked to an antiferromagnetic transition occurring at TAF = 56 K for samples with x = 0.6. Notably, this anomaly diminishes with increasing x or decreasing p, indicating a sensitivity to these parameters. Furthermore, we observe that the high-temperature upturns in both C/T and χ(T) can be attributed to the presence of impurity phases, which emerge when x exceeds 0.65. This correlation suggests that the 60 K anomaly may be influenced by these impurities. Additionally, our analysis of high-field magnetization reveals a pronounced increase below 50 K, suggesting the existence of another phase transition in this temperature range. We characterize this new phase as a charge density wave (CDW) state. Importantly, our statistical analysis indicates that the emergence of the CDW state is contingent upon the presence of impurities within the sample. These results contribute to a deeper understanding of the complex interactions in YBa2Cu3O6+x and highlight the critical role of impurities in the manifestation of the 60 K plateau and associated phenomena.",
        "ori-fast-z-score": -0.6974858324629157,
        "water-fast-z-score": 4.2723919920032305,
        "rewrite-fast-z-score": -0.21081851067789195
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Scalability and Optimisation of a Committee of Agents Using Genetic Algorithm .\nAbstract:\nThe use of agents in the real world is becoming more widespread, especially for applications that require autonomous decision making or learning. However, there are many challenges associated with developing such systems including scalability issues when dealing with large numbers of agents. In this work we present an approach to optimising the performance of a committee of agents using genetic algorithms (GAs). We demonstrate how GAs can be used to evolve optimal agent behaviours within a simulated environment where each agent has different capabilities. The results show that our method improves upon existing approaches by increasing the overall efficiency of the system while reducing the number of messages required between agents. This research was funded by EPSRC grant EP/G066058/1. Agent-based computing is increasingly being applied to problems requiring autonomous decision-making and/or learning  1  . Examples include intelligent transportation  2  , smart grids  3  , healthcare  4  , manufacturing  5  , robotics  6  , and military operations  7  .\nHowever, there are several challenges associated with developing these types of systems  8  . One particular challenge relates to scalability as the number of agents increases  9  . For example, if one thousand agents need to make decisions on their own then it may not be possible to develop individualised behavioural models for all of them  10  . Instead, they must rely on some form of collective intelligence  11  which requires communication  12  . If too much information is communicated between agents then the network will become overloaded  13  resulting in poor performance  14  . Therefore, it becomes important to minimise the amount of data transmitted across the network  15  whilst still maintaining high levels of performance  16  .\nIn order to address these issues, researchers have proposed various techniques  17  ranging from simple heuristics  18  to sophisticated machine-learning methods  19  . These techniques typically involve either centralised  20  or decentralised  21  control mechanisms  22  . Centralised control involves a single entity controlling all aspects of the system  23  whereas decentralised control allows multiple entities to act independently  24  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Scalability and Optimisation of a Committee of Agents Using Genetic Algorithm . Abstract : The using of agents in the real life is becoming more widespread , particularly for applications that require autonomous judgment thinking or learning .However , there are many challenges associated with creating such schemes namely scalability concerns when dealing with large numbers of agents . In this research we present an way to optimising the performance of a committee of agents using genetic algorithms ( GAs ) .We suggest how GAs can be used to evolve efficient actor behaviours within a simulated situation where each agent has unique skills . The results show that our technique improves upon existing techniques by increasing the overall performance of the organization while reducing the quantity of communication needed between agents .This research was funded by EPSRC grant EP / G066058 / 1 . Agent - based computing is growing being application to problems needing autonomous judgment - making and / or learning 1 .Examples involve intelligent travel 2 , smart grids 3 , hospitals 4 , manufacturing 5 , robotics 6 , and army units 7 . However , there are several challenges associated with creating these kinds of technologies 8 .One particular challenge relates to scalability as the total of agents increases 9 . For instance , if one thousand agents need to make choices on their own then it could not be possible to develop individualised behavioural models for all of them 10 .Instead , they must rely on some sort of collective intelligence 11 which requires communication 12 . If too big information is conveyed between agents then the channel will become overloaded 13 causing in poor quality 14 .Therefore , it becomes crucial to minimise the quantity of data transmitted across the channel 15 whilst nevertheless ensuring high levels of performance 16 . In order to tackle these problems , researchers have proposed several methods 17 ranging from complicated heuristics 18 to sophisticated machine - computing methods 19 .These methods typically involve either centralised 20 or decentralised 21 control methods 22 . Centralised control involves a single entity governing all aspects of the scheme 23 whereas decentralised control enables different organisations to act independently 24 .",
        "rewrite_text": "**Title:** Scalability and Optimization of a Committee of Agents Using Genetic Algorithms\n\n**Abstract:** The application of agent-based systems is increasingly prevalent in various domains that necessitate autonomous decision-making and learning capabilities. However, the scalability of these systems poses significant challenges, particularly when managing large populations of agents. This study introduces a novel approach to optimizing the performance of a committee of agents through the implementation of genetic algorithms (GAs). We demonstrate how GAs can be utilized to evolve effective behaviors among agents within a simulated environment, where each agent possesses distinct skills. Our findings indicate that this method enhances the overall efficiency of the agent organization while simultaneously minimizing the volume of communication required among agents. \n\nThe research highlights the growing relevance of agent-based computing in addressing complex problems that demand autonomous judgment, with applications spanning intelligent transportation systems, smart grids, healthcare, manufacturing, robotics, and military operations. Despite the promising potential of these technologies, scalability remains a critical hurdle, especially as the number of agents increases. For instance, when tasked with decision-making, a thousand agents cannot feasibly rely on individualized behavioral models; instead, they must leverage collective intelligence, which inherently necessitates communication. Excessive information exchange can lead to channel overload, resulting in diminished performance quality. Thus, it is imperative to reduce the amount of data transmitted while maintaining high operational effectiveness.\n\nTo address these challenges, various strategies have been proposed, ranging from complex heuristics to advanced machine learning techniques. These approaches generally fall into two categories: centralized and decentralized control. Centralized control involves a single governing entity overseeing the entire system, while decentralized control allows for independent operation among different agents. This research, funded by the EPSRC grant EP/G066058/1, contributes to the ongoing discourse on optimizing agent-based systems, emphasizing the importance of balancing communication efficiency with performance in large-scale applications.",
        "ori-fast-z-score": 0.7715167498104595,
        "water-fast-z-score": 10.111802752887083,
        "rewrite-fast-z-score": -0.07647191129018725
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The shapes, orientation, and alignment of Galactic dark matter subhalos .\nAbstract:\nWe present the results of an analysis of the shapes, orientations, and alignments of dark matter subhalos in cosmological N-body simulations with different levels of baryonic physics included.  We find that the inclusion of baryons has little effect on the shape distribution but does affect the spin parameter distributions significantly; halos are more spherical when baryons are included than they would be if only gravity were acting upon them. The halo spins tend to be aligned perpendicularly to their major axes for all models considered here (including pure dark matter). This is consistent with previous studies which have found similar trends using other methods. However we also find evidence that this trend may not hold at very small scales where there appears to be some correlation between the direction of the angular momentum vector and the minor axis of the halo. Finally, we show that the presence or absence of baryons affects the degree of alignment between neighboring halos; halos are less strongly clustered around each other when baryons are included.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The shapes , orientation , and alignment of Galactic dark matter subhalos . Abstract : We present the results of an assessment of the shapes , orientations , and alignments of dark matter subhalos in cosmological N - bodies simulations with various levels of baryonic physics provided .We see that the introduction of baryons has little impact on the form distribution but does affect the spin vector distributions substantially ; halos are more spherical when baryons are included than they would be if only gravitational were acting upon them . The halo spins tend to be aligned perpendicularly to their major axes for all models discussed here ( especially pure black material ) .This is consistent with previous research which have discovered similar trends using other methods . However we also find proof that this shift might not stand at very small scales where there seems to be some correlation between the direction of the angular velocity tensor and the minor axis of the halo .Finally , we find that the presence or lack of baryons affects the degree of alignment between neighboring halos ; halos are less highly clustered around each other when baryons are included .",
        "rewrite_text": "In this study, we investigate the characteristics of dark matter subhalos, focusing on their shapes, orientations, and alignments as derived from cosmological N-body simulations that incorporate varying degrees of baryonic physics. Our findings indicate that while the inclusion of baryons has a minimal effect on the overall shape distribution of these subhalos, it significantly influences the distribution of their spin vectors. Specifically, we observe that subhalos exhibit a more spherical morphology when baryonic effects are considered, compared to scenarios where only gravitational forces are at play. Furthermore, our analysis reveals a consistent trend across all models examined, particularly in those dominated by dark matter, where the spin vectors of halos tend to align perpendicularly to their principal axes. This observation aligns with previous studies that have reported similar patterns through alternative methodologies. However, we also uncover evidence suggesting that this perpendicular alignment may not persist at smaller scales, where a correlation appears to exist between the angular velocity tensor's direction and the halo's minor axis. Additionally, our results indicate that the presence or absence of baryonic matter influences the alignment of neighboring halos; specifically, halos exhibit a lower degree of clustering when baryons are included in the simulations. This research enhances our understanding of the complex interplay between dark matter and baryonic physics in shaping the structure of the universe and provides insights into the dynamics of halo formation and alignment.",
        "ori-fast-z-score": -1.8439088914585775,
        "water-fast-z-score": 4.610083899802517,
        "rewrite-fast-z-score": 1.8542101386022132
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Physisorption of Nucleobases on Graphene .\nAbstract:\nThe physisorption of nucleobases (adenine, cytosine, guanine and thymine) onto graphene is investigated by density functional theory calculations at the B3LYP/6-31G(d) level in vacuum conditions. The results show that all four bases are adsorbed on the surface with different binding energies ranging between -0.27 eV for adenine to -1.10 eV for cytosine. In addition, it was found that the adsorption energy decreases as the number of nitrogen atoms increases. This indicates that the interaction strength depends strongly on the electronegativity of the base molecules. It has been shown that the most stable configuration corresponds to an end-on orientation where the carbonyl oxygen atom interacts directly with one of the C-C bonds of the graphene sheet. \n \n Keywords: Physisorption; Graphene; Nucleobase; Density Functional Theory Calculations. Introduction \n \n Graphene is a two-dimensional material consisting of sp2-hybridized carbon atoms arranged into a honeycomb lattice structure  1  . Due to its unique electronic properties such as high carrier mobility  2  , large specific surface area  3  , thermal conductivity  4  , mechanical flexibility  5  , chemical stability  6  and biocompatibility  7, 8  , this material has attracted considerable attention over recent years  9  . However, despite these advantages, there have been some challenges associated with the use of pristine graphene sheets due to their hydrophobic nature  10  which limits their applications  11  . Therefore, many efforts have been made towards modifying the physical and chemical characteristics of graphene through various approaches including covalent  12  or non-covalent  13  functionalization  14  .\n \nIn particular, non-covalent functionalization can be achieved via π-π interactions  15  , hydrogen bonding  16  , electrostatic  17  , van der Waals  18  and ionic  19  forces  20  . Among them, π-π stacking is considered to be the strongest noncovalent force  21  . For example, several studies have reported that aromatic compounds  22  , fullerenes  23  , porphyrins  24  , metal complexes  25  and biomolecules  26  could interact with graphene surfaces via π-",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Physisorption of Nucleobases on Graphene . Abstract : The physisorption of nucleobases ( adenine , cytosine , guanine and thymine ) onto graphene is investigated by density functional theory estimates at the B3LYP / 6 - 31G ( d ) level in vacuum environments .The results show that all four bases are adsorbed on the surface with varying binding energies ranging between - 0 . 27 eV for adenine to - 1 . 10 eV for cytosine . In addition , it was shown that the adsorption energy decreases as the proportion of nitrogen atoms increases .This implies that the interaction strength depends strongly on the electronegativity of the base atoms . It has been shown that the most stable configuration refers to an ending - on position where the carbonyl oxygen atom interacts closely with one of the C - C bonds of the graphene sheet .Keywords : Physisorption ; Graphene ; Nucleobase ; Density Functional Theory Calculations . Introduction Graphene is a two - dimensional material consisting of sp2 - hybridized carbon atoms arranged into a honeycomb lattice structure 1 .Due to its unique electronic properties such as wide carrier mobility 2 , large particular surface region 3 , thermal conductivity 4 , thermal flexibility 5 , chemical integrity 6 and biocompatibility 7 , 8 , this metal has garnered considerable notice over recent months 9 . However , despite these benefits , there have been some challenges associated with the using of pristine graphene strips due to their hydrophobic nature 10 which restricted their functionality 11 .Therefore , various efforts have been placed towards modifying the physical and chemical qualities of graphene through numerous approaches including covalent 12 or non - covalent 13 functionalization 14 . In particular , non - covalent functionalization can be obtained via π - π interactions 15 , hydrogen bonding 16 , electrostatic 17 , van der Waals 18 and ionic 19 forces 20 .Among them , π - π stacking is regarded to be the powerful noncovalent force 21 . For instance , various trials have reported that aromatic molecules 22 , fullerenes 23 , porphyrins 24 , metal ions 25 and biomolecules 26 could interact with graphene surfaces via π -",
        "rewrite_text": "**Title:** Physisorption of Nucleobases on Graphene\n\n**Abstract:** This study explores the physisorption characteristics of nucleobases—adenine, cytosine, guanine, and thymine—on graphene, utilizing density functional theory (DFT) calculations at the B3LYP/6-31G(d) level in a vacuum setting. The findings reveal that all four nucleobases are capable of adsorbing onto the graphene surface, exhibiting a range of binding energies from -0.27 eV for adenine to -1.10 eV for cytosine. Notably, the adsorption energy tends to decrease with an increasing number of nitrogen atoms within the nucleobase structure, indicating a strong correlation between the interaction strength and the electronegativity of the constituent atoms. The most stable adsorption configuration is identified as an end-on orientation, where the carbonyl oxygen atom of the nucleobase closely interacts with one of the C-C bonds in the graphene lattice. \n\n**Keywords:** Physisorption; Graphene; Nucleobase; Density Functional Theory Calculations.\n\n**Introduction:** Graphene, a two-dimensional material composed of sp²-hybridized carbon atoms arranged in a honeycomb lattice, has attracted significant attention due to its remarkable electronic properties, including high carrier mobility, extensive surface area, exceptional thermal conductivity, flexibility, chemical stability, and biocompatibility. Despite these advantageous characteristics, the hydrophobic nature of pristine graphene presents challenges that limit its practical applications. Consequently, extensive research has focused on modifying the physical and chemical properties of graphene through various strategies, including both covalent and non-covalent functionalization. Among these, non-covalent functionalization is particularly noteworthy, as it can be achieved through mechanisms such as π-π interactions, hydrogen bonding, electrostatic forces, van der Waals forces, and ionic interactions. Notably, π-π stacking is recognized as a potent non-covalent interaction, facilitating the adsorption of a variety of aromatic molecules, fullerenes, porphyrins, metal ions, and biomolecules onto graphene surfaces. This study aims to deepen the understanding of how nucleobases interact with graphene, which could have implications for the development of graphene-based biosensors and other biotechnological applications.",
        "ori-fast-z-score": -0.2727272727272727,
        "water-fast-z-score": 7.004606779044222,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Strongly modulated transmission of a spin-split quantum wire with local Rashba interaction .\nAbstract:\nWe study the transport properties in a one-dimensional (1D) spin-orbit coupled system, where the electron-electron interactions are treated within the Hartree-Fock approximation. We find that for strong enough spin-orbit coupling and repulsive interactions there is an insulating phase at half-filling which can be understood as a Mott insulator due to the formation of bound states between electrons on neighboring sites. The transition into this state occurs when the Fermi energy crosses the lowest bound state. In addition we show how the presence of disorder changes these results. Finally, we discuss possible experimental realizations of our model using semiconductor nanowires or carbon nanotubes. Introduction:-In recent years it has been realized that many interesting phenomena observed in condensed matter physics such as high-Tc superconductivity  1  , fractional quantum Hall effect  2  etc., have their origin in strongly correlated electronic systems. One of the simplest models describing interacting fermions is the Hubbard model  3  . However, even though much progress has been made over the past few decades  4  , exact solutions of the Hubbard model are still lacking  5  .\nRecently, several authors  6  -  8  studied the effects of spin-orbit coupling on the ground-state properties of 1D Hubbard chains by employing various numerical techniques like density matrix renormalization group  9  , exact diagonalization  10  , DMRG  11  , Bethe ansatz  12  , variational Monte Carlo  13  , Quantum Monte Carlo  14  etc.. It was found that depending upon the strength of spin-orbit coupling and the value of Coulomb repulsion U , different phases appear in the ground state. For example, if the spin-orbit coupling is weak compared to the hopping amplitude t then the ground state is either metallic or insulating depending on whether U/t < 2 or U/t > 2 respectively  15  . On the other hand, if the spin-orbit",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Strongly modulated transmission of a spin - split quantum wire with local Rashba coupling . Abstract : We research the travel properties in a one - dimensional ( 1D ) spin - orbit coupled system , where the electron - electron interactions are treated within the Hartree - Fock approximation .We see that for strong enough spin - orbit bonding and repulsive bonding there is an insulating phase at half - filling which can be understood as a Mott insulator owing to the formation of bound states between electrons on nearby locations . The shift into this state occurs when the Fermi energy crosses the lowest bound state .In addition we explain how the presence of disorder changes these results . Finally , we explain possible experimental realizations of our model utilizing semiconductor nanowires or carbon nanotubes .Introduction : - In recent years it has been realized that several interesting phenomena observed in condensed matter science such as high - Tc superconductivity 1 , fractional quantum Hall impact 2 etc . , have their source in highly correlated electronic systems . One of the simplest models explaining interacting fermions is the Hubbard theory 3 .However , even though much work has been achieved over the previous few years 4 , exact solutions of the Hubbard theory are still lacking 5 . Recently , various scientists 6 - 8 studied the effects of spin - orbit interaction on the ground - state properties of 1D Hubbard chains by employing several mathematical techniques like density matrix renormalization group 9 , exact diagonalization 10 , DMRG 11 , Bethe ansatz 12 , variational Monte Carlo 13 , Quantum Monte Carlo 14 etc . .It was shown that depending upon the strength of spin - orbit interaction and the value of Coulomb repulsion U , different stages appear in the ground state . For instance , if the spin - orbit interaction is weakened compared to the hopping frequency t then the ground state is either metallic or insulating depending on whether U / t < 2 or U / t > 2 respectively 15 .On the other hand , if the spin - orbit",
        "rewrite_text": "**Title:** Strongly Modulated Transmission of a Spin-Split Quantum Wire with Local Rashba Coupling\n\n**Abstract:** This study investigates the transport characteristics of a one-dimensional (1D) spin-orbit coupled system, with a focus on electron-electron interactions modeled through the Hartree-Fock approximation. Our findings reveal that under conditions of sufficiently strong spin-orbit coupling and repulsive interactions, the system exhibits an insulating phase at half-filling, which can be interpreted as a Mott insulator. This insulating behavior arises from the formation of bound states between electrons located in close proximity to one another. The transition into this insulating phase is triggered when the Fermi energy intersects with the lowest bound state. Furthermore, we analyze the impact of disorder on these transport properties, demonstrating how it modifies the system's behavior. The implications of our results extend to potential experimental realizations of the proposed model, which could be implemented using semiconductor nanowires or carbon nanotubes. \n\nIn recent years, the exploration of correlated electronic systems has unveiled numerous intriguing phenomena in condensed matter physics, such as high-temperature superconductivity and fractional quantum Hall effects. The Hubbard model serves as a foundational framework for understanding interacting fermions; however, exact solutions remain elusive despite significant advancements in the field. Recent investigations have focused on the effects of spin-orbit interaction on the ground-state properties of 1D Hubbard chains, employing various sophisticated mathematical techniques, including density matrix renormalization group, exact diagonalization, and quantum Monte Carlo methods. These studies indicate that the interplay between spin-orbit coupling and Coulomb repulsion leads to distinct phases in the ground state. Specifically, when the spin-orbit interaction is diminished relative to the hopping parameter, the ground state can transition between metallic and insulating phases based on the ratio of Coulomb repulsion to hopping strength. This research contributes to the understanding of complex electronic systems and highlights the significance of spin-orbit coupling in determining their properties.",
        "ori-fast-z-score": -0.7474093186836597,
        "water-fast-z-score": 5.85598130178049,
        "rewrite-fast-z-score": -0.16329931618554522
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A New Tool for Seismology--the Cumulative Spectral Power .\nAbstract:\nThe cumulative spectral power (CSP) is introduced as an alternative to the traditional method in analyzing earthquake data, which has been widely used by seismologists and geophysicists since it was first proposed by Aki(1957). The new tool can be applied to both earthquakes with known locations and those without any information on their epicenters. It also provides more detailed information about the source mechanism than that obtained using the traditional method. In this study we apply the new technique to analyze two large earthquakes occurred in China during recent years. We find that the results are consistent with previous studies based on other methods. This suggests that the new tool may provide useful information for studying seismic activities. Keywords: Earthquake; Source mechanism; Cumulative spectral power. 1 Introduction.\nSeismological research plays an important role in understanding the physical processes involved in earthquakes. Since its introduction into seismology by Aki(1957), the traditional method of calculating the cumulative energy released by earthquakes has become one of the most popular techniques among seismologists and geophysics researchers. However, there have been some problems associated with this method such as: i)it requires accurate location of the epicenter before analysis; ii)the result depends strongly upon the choice of time window length; iii)it cannot give enough information about the source mechanisms of earthquakes. To overcome these difficulties, many attempts have been made recently to develop new tools for analyzing earthquake data. For example, Sato et al.(1989) , Kao & Chen(1990) , Wu & Chiao(1991a , 1991b )and Wu(1993 developed different approaches to calculate the cumulative energy release rate. These authors found that the results were not always consistent with each other due to differences in the assumptions they had adopted.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A New Tool for Seismology - - the Cumulative Spectral Power . Abstract : The total spectral power ( CSP ) is adopted as an alternative to the usual technology in surveying earthquake information , which has been widely using by seismologists and geophysicists since it was first suggested by Aki ( 1957 ) .The new technique can be applied to both earthquakes with recorded locations and those without any info on their epicenters . It additionally offers more precise data about the origin process than that obtained using the usual method .In this study we apply the new technique to analyze two huge disasters happened in China during recent seasons . We see that the results are compatible with previous research based on other methods .This implies that the new method may provide useful info for studying seismic activities . Keywords : Earthquake ; Source method ; Cumulative spectral power .1 Introduction . Seismological study plays an important role in understanding the natural processes implicated in earthquakes .Since its introduction into seismology by Aki ( 1957 ) , the usual method of calculating the total energy released by earthquakes has become one of the most popular methods among seismologists and geophysics experts . However , there have been some problems related with this technology such as : i ) it takes accurate location of the epicenter before assessment ; ii ) the result relies highly upon the selection of time frame length ; iii ) it lacks give adequate data about the origin mechanisms of disasters .To solve these problems , various efforts have been made recently to develop new tools for analyzing earthquake information . For instance , Sato et al .( 1989 ) , Kao & Chen ( 1990 ) , Wu & Chiao ( 1991a , 1991b ) and Wu ( 1993 created different methods to estimate the total energy release velocity . These authors found that the results were not always compatible with each other owing to differences in the assumptions they had adopted .",
        "rewrite_text": "**Title:** A New Tool for Seismology - The Cumulative Spectral Power\n\n**Abstract:** In this article, we introduce the Cumulative Spectral Power (CSP) as a novel approach for analyzing seismic data, presenting it as a viable alternative to traditional methods that have dominated earthquake research since Aki's seminal work in 1957. The CSP technique is particularly advantageous as it can be utilized for both well-located earthquakes and those lacking precise epicenter information. This method not only enhances the accuracy of data regarding the seismic source processes but also addresses several limitations associated with conventional techniques. \n\nIn our study, we apply the CSP method to investigate two significant seismic events that occurred in China in recent years. The findings from our analysis demonstrate a strong correlation with results obtained through established methodologies, suggesting that CSP can effectively contribute to the understanding of seismic activities. This compatibility with previous research underscores the potential of CSP to yield valuable insights into earthquake dynamics.\n\nThe introduction of CSP is timely, as traditional methods often face challenges such as the necessity for accurate epicenter localization, dependence on the chosen time frame for analysis, and insufficient detail regarding the mechanisms underlying seismic events. Previous attempts to refine energy release estimation, as seen in the works of Sato et al. (1989), Kao & Chen (1990), Wu & Chiao (1991a, 1991b), and Wu (1993), have highlighted inconsistencies due to varying assumptions and methodologies. Our CSP approach aims to mitigate these issues, providing a more robust framework for seismological studies. \n\nIn conclusion, the Cumulative Spectral Power represents a significant advancement in the field of seismology, offering a more comprehensive tool for analyzing earthquake data and enhancing our understanding of seismic phenomena. \n\n**Keywords:** Earthquake; Source method; Cumulative spectral power.",
        "ori-fast-z-score": -1.8461989882527161,
        "water-fast-z-score": 8.391813582966892,
        "rewrite-fast-z-score": -0.8980265101338746
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Variations in the lithium abundances of turn off stars in the globular cluster 47 Tuc .\nAbstract:\nWe have measured the Li abundance for 16 turn-off (TO) stars in the metal-rich open cluster 47 Tucanae, using high-resolution spectra obtained with UVES at VLT-UT2 telescope.  The TO stars are located between 0.8 and 1.0 solar radii above the main sequence turnoff point on the colour-magnitude diagram. We find that all but one star show an overabundance of Li compared to the Sun by factors ranging from 2 up to 6 times its solar value. This is consistent with previous studies which found similar results for other clusters such as M71 or NGC6397. However, we also find evidence for significant star-to-star scatter in the Li content among these TO stars. In particular, two out of our sample exhibit very low values of log(Li/H) = +0.3 dex and +0.4 dex respectively, while most of them display higher values around +1.1 dex.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Variations in the lithium abundances of turn off stars in the globular cluster 47 Tuc . Abstract : We have tested the Li abundance for 16 change - off ( TO ) stars in the metal - rich open cluster 47 Tucanae , using high - resolution spectra obtained with UVES at VLT - UT2 telescope .The TO stars are located between 0 . 8 and 1 . 0 solar radii above the main sequence turnoff point on the colour - magnitude diagram . We see that all but one star show an overabundance of Li compared to the Sun by factors ranging from 2 up to 6 times its solar value .This is compatible with previous research which showed similar results for other complexes such as M71 or NGC6397 . However , we also find proof for significant star - to - star scatter in the Li concentration among these TO stars .In particular , two out of our sample present very low values of log ( Li / H ) = + 0 . 3 dex and + 0 . 4 dex respectively , while most of them display larger values around + 1 . 1 dex .",
        "rewrite_text": "Title: Variations in Lithium Abundances of Turnoff Stars in the Globular Cluster 47 Tucanae\n\nAbstract: In this study, we investigate the lithium (Li) abundances in 16 turnoff (TO) stars within the metal-rich globular cluster 47 Tucanae, utilizing high-resolution spectra acquired through the UVES instrument at the VLT-UT2 telescope. The selected TO stars are positioned between 0.8 and 1.0 solar radii above the main sequence turnoff point on the color-magnitude diagram. Our findings reveal that nearly all of the stars in our sample exhibit lithium abundances that are significantly higher than that of the Sun, with enhancements ranging from two to six times the solar value. This observation aligns with previous studies conducted on other stellar complexes, such as M71 and NGC 6397, which reported similar lithium overabundances. However, our analysis also uncovers notable variations in lithium concentrations among the TO stars, indicating a substantial star-to-star scatter. Specifically, two stars in our sample show remarkably low lithium abundances, with log(Li/H) values of +0.3 dex and +0.4 dex, respectively. In contrast, the majority of the stars exhibit higher lithium levels, averaging around +1.1 dex. These results suggest a complex interplay of stellar evolution processes affecting lithium production and depletion in these stars, highlighting the need for further investigation into the mechanisms driving the observed variations in lithium abundances within the globular cluster environment. This study contributes to our understanding of the chemical evolution of stars in dense stellar populations and the role of lithium as a tracer of stellar nucleosynthesis and evolutionary history.",
        "ori-fast-z-score": 0.48507125007266594,
        "water-fast-z-score": 3.2504180333157686,
        "rewrite-fast-z-score": 1.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Electrical transport and optical studies of ferromagnetic Cobalt doped ZnO nanoparticles exhibiting a metal-insulator transition .\nAbstract:\nZnO:Co nanocrystals were synthesized by the sol-gel method followed by annealing at different temperatures (500-700 °C). The structural, morphological, magnetic and electrical properties have been investigated in detail using X-ray powder diffraction (XRD), scanning electron microscopy (SEM) with energy dispersive spectroscopy (EDS), vibrating sample magnetometer (VSM), Hall effect measurement system and UV-Vis-NIR spectrophotometry techniques. XRD analysis reveals that all samples are single phase wurtzite structure without any impurity peaks. SEM images show that the particles are spherical in shape and their size increases with increasing temperature. VSM measurements reveal that the saturation magnetization decreases gradually as the annealing temperature is increased up to 700 °C due to the decrease in Co content. It was found that the resistivity shows an abrupt increase around 500 °C which indicates the occurrence of semiconductor-metal transition.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Electrical transport and imaging experiments of ferromagnetic Cobalt doped ZnO nanoparticles exhibiting a metal - insulator transition . Abstract : ZnO : Co nanocrystals were synthesized by the sol - gel method followed by annealing at different conditions ( 500 - 700 °C ) .The mechanical , morphological , magnetic and electrical properties have been investigated in detail using X - ray powder diffraction ( XRD ) , scanning electron microscopy ( SEM ) with energy dispersive spectroscopy ( EDS ) , vibrating sample magnetometer ( VSM ) , Hall impact comparison system and UV - Vis - NIR spectrophotometry methods . XRD analysis reveals that all specimens are single phase wurtzite structure without any impurity peaks .SEM photos suggest that the molecules are spherical in structure and their height changes with rising heat . VSM measurements reveal that the saturation magnetization decreases slowly as the annealing temperature is raised up to 700 °C owing to the decrease in Co content .It was shown that the resistivity displays an sudden rise around 500 °C which demonstrates the existence of semiconductor - metal transition .",
        "rewrite_text": "Title: Electrical Transport and Imaging Studies of Ferromagnetic Cobalt-Doped ZnO Nanoparticles Exhibiting a Metal-Insulator Transition\n\nAbstract: This study investigates the synthesis and characterization of cobalt-doped zinc oxide (ZnO:Co) nanocrystals, produced via the sol-gel method and subsequently annealed at temperatures ranging from 500 to 700 °C. A comprehensive analysis of the mechanical, morphological, magnetic, and electrical properties of these nanoparticles was conducted using various techniques, including X-ray powder diffraction (XRD), scanning electron microscopy (SEM) coupled with energy dispersive spectroscopy (EDS), vibrating sample magnetometry (VSM), Hall effect measurements, and UV-Vis-NIR spectrophotometry. The XRD results confirm that all synthesized samples exhibit a single-phase wurtzite structure, with no detectable impurity peaks, indicating high purity of the nanocrystals. SEM imaging reveals that the nanoparticles are predominantly spherical in shape, with variations in their dimensions observed as the annealing temperature increases. Magnetic characterization through VSM indicates a gradual decrease in saturation magnetization with rising annealing temperatures, attributed to a reduction in cobalt content within the ZnO matrix. Notably, electrical measurements reveal a significant increase in resistivity around the 500 °C mark, suggesting the occurrence of a semiconductor-to-metal transition in the material. This transition is critical for understanding the underlying mechanisms governing the electrical properties of cobalt-doped ZnO nanoparticles and their potential applications in spintronic devices and other advanced electronic materials. Overall, this research provides valuable insights into the interplay between structural, magnetic, and electrical characteristics in ferromagnetic semiconductor systems, paving the way for future studies aimed at optimizing the performance of doped ZnO nanostructures for various technological applications.",
        "ori-fast-z-score": -1.2874526191574363,
        "water-fast-z-score": 4.73427220735493,
        "rewrite-fast-z-score": -0.47891314261057566
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Search for Million Degree Gas Through The NVII Hyperfine Line .\nAbstract:\nWe report on the detection of the N VII hyperfine line at 1238 Å in the X-ray spectrum of the Galactic center source Sgr A* with Chandra and XMM-Newton observations. We find that this emission is consistent with gas having temperatures between 1 million K to 2 million K, densities between 10^6 cm^{-3} to 10^7 cm^{-3}, and column density of about 5 x 10^{20} cm^{-2}. This temperature range is higher than previous estimates based on other lines observed by Chandra or XMM-Newton. Our results are also inconsistent with models where the gas has been photoionized by UV radiation from nearby stars. These findings suggest that there may be an additional heating mechanism present near Sgr A* besides photoionization. In addition we detect absorption features associated with the same ion which could arise either due to foreground material along our line-of-sight towards Sgr A* or intrinsic to the accretion flow onto Sgr A* itself.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Search for Million Degree Gas Through The NVII Hyperfine Line . Abstract : We report on the discovery of the N VII hyperfine line at 1238 Å in the X - ray spectrum of the Galactic center source Sgr A * with Chandra and XMM - Newton observations .We see that this emission is consistent with gas having altitudes between 1 million K to 2 million K , densities between 10 ^ 6 cm ^ { - 3 } to 10 ^ 7 cm ^ { - 3 } , and column thickness of about 5 x 10 ^ { 20 } cm ^ { - 2 } . This temperature range is higher than prior estimates based on other lines observed by Chandra or XMM - Newton .Our results are also inconsistent with models where the gas has been photoionized by UV rays from nearby stars . These conclusions propose that there may be an additional thermal mechanism present near Sgr A * besides photoionization .In addition we perceive absorption elements identified with the same particle which could occur either due to foreground material along our line - of - view towards Sgr A * or intrinsic to the accretion flow onto Sgr A * itself .",
        "rewrite_text": "We present our findings on the detection of the N VII hyperfine line at 1238 Å in the X-ray spectrum of the Galactic center source Sgr A* through observations made with the Chandra and XMM-Newton telescopes. Our analysis reveals that the observed emission corresponds to gas temperatures ranging from 1 million K to 2 million K, with densities estimated between 10^6 cm^-3 and 10^7 cm^-3, and a column density of approximately 5 x 10^20 cm^-2. This temperature range exceeds previous estimates derived from other spectral lines observed by Chandra and XMM-Newton, suggesting a need to reevaluate existing models of the region. Furthermore, our findings challenge the prevailing theories that attribute the ionization of this gas to ultraviolet radiation from nearby stars. Instead, our results indicate the potential presence of an additional thermal mechanism influencing the gas dynamics in the vicinity of Sgr A*. Additionally, we identify absorption features associated with the same particle, which may arise from foreground material along the line of sight to Sgr A* or could be intrinsic to the accretion flow onto Sgr A* itself. These observations contribute to a deeper understanding of the high-energy environment surrounding Sgr A* and highlight the complexities of gas interactions in this region of the Galactic center. Our study opens new avenues for research into the thermal processes at play and the role of various ionization mechanisms in shaping the characteristics of the gas in this astrophysical setting.",
        "ori-fast-z-score": -0.8682431421244593,
        "water-fast-z-score": 3.6927447293799815,
        "rewrite-fast-z-score": 0.5940885257860046
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The polar ring galaxy AM1934-563 revisited .\nAbstract:\nWe present new optical and near-infrared images, as well as archival radio data for the polar ring galaxy AM1934-563 (NGC4650A). The main results are summarized below:  1) We confirm that this galaxy is an edge-on S0/a with a bright nuclear point source surrounded by a faint disk.  2) A prominent dust lane crosses the central region in the north-south direction.  3) There appears to be no evidence for any recent star formation activity within the ring or along its edges.  4) The kinematics of the gas component show two distinct components; one associated with the ring itself and another which follows closely the rotation curve of the underlying stellar body.  5) The total mass of the gaseous ring amounts to about 10^9 M_sol. 6) The observed properties of NGC4650A can be explained if it has undergone a major merger event between two galaxies at least 0.5 Gyr ago.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The polar ring universe AM1934 - 563 revisited . Abstract : We present new optical and near - infrared images , as well as archival radio data for the polar ring galaxy AM1934 - 563 ( NGC4650A ) .The main results are presented below : 1 ) We suggest that this galaxy is an edge - on S0 / a with a bright nuclear point source surrounded by a bright disk . 2 ) A prominent dust track runs the main region in the north - south direction .3 ) There seems to be no evidence for any latest star formation activity within the circle or along its edges . 4 ) The kinematics of the gas component show two different components ; one linked with the circle itself and another which follows carefully the rotation curve of the underlying stellar bodies .5 ) The total mass of the gaseous ring amounts to about 10 ^ 9 M _ sol . 6 ) The observed properties of NGC4650A can be understood if it has undergone a major collision activity between two galaxies at least 0 . 5 Gyr ago .",
        "rewrite_text": "We present a comprehensive analysis of the polar ring galaxy AM1934-563 (NGC4650A) through new optical and near-infrared imaging, complemented by archival radio data. Our findings reveal several significant characteristics of this galaxy. Firstly, we propose that AM1934-563 is an edge-on S0/a galaxy, distinguished by a luminous nuclear point source at its center, which is encircled by a bright disk structure. Secondly, we observe a prominent dust lane that extends predominantly in a north-south orientation across the galaxy's main region. Notably, our investigation indicates an absence of recent star formation activity both within the polar ring and along its periphery. \n\nFurthermore, the kinematic analysis of the gas component reveals the presence of two distinct components: one associated with the polar ring itself and another that closely follows the rotation curve of the underlying stellar population. This duality in gas dynamics suggests complex interactions within the galaxy. We estimate that the total mass of the gaseous ring is approximately 10^9 solar masses, highlighting its substantial contribution to the galaxy's overall mass distribution. \n\nLastly, we interpret the observed characteristics of NGC4650A within the context of its evolutionary history, proposing that the galaxy has likely experienced a significant collision with another galaxy at least 0.5 billion years ago. This past interaction may have played a crucial role in shaping its current structure and dynamics, providing insights into the processes that govern the formation and evolution of polar ring galaxies. Our study enhances the understanding of AM1934-563 and contributes to the broader discourse on galaxy interactions and morphology.",
        "ori-fast-z-score": -0.6708203932499369,
        "water-fast-z-score": 3.312945782245396,
        "rewrite-fast-z-score": -0.26013299085723596
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Multiuser detection in a dynamic environment Part I: User identification and data detection .\nAbstract:\nIn this work, we consider the problem of multiuser detection (MUD) for code division multiple access systems with time-varying channels. We propose an algorithm that jointly performs user identification and data detection by using a maximum likelihood criterion. The proposed method is based on the expectation-maximization (EM) algorithm which iteratively estimates both the channel coefficients and transmitted symbols. In order to reduce computational complexity, we also develop a low-complexity suboptimal MUD scheme. Numerical results show that our proposed algorithms outperform existing schemes under various scenarios. Index Terms-Data detection, EM algorithm, Multiuser detection, Time varying channels. 1 Introduction Code-division-multiple-access (CDMA) has been widely used as one of the most promising technologies for next-generation wireless communications due to its high spectral efficiency  1  . However, CDMA suffers from severe interference between users caused by multipath propagation  2  , especially when the number of active users increases  3  .\nTo mitigate inter-user interference, multiuser detectors have been developed  4  -  6  . Among them, linear multiuser detectors are attractive because they can be implemented easily at low cost  7  . Unfortunately, these detectors suffer from performance loss compared to optimal multiuser detectors  8  . To improve their performance, nonlinear multiuser detectors such as successive interference cancellation  9  or parallel interference cancellation  10  were introduced. These detectors require accurate knowledge about the received signals  11  . Therefore, blind multiuser detectors  12  -  14  were proposed to estimate unknown parameters without any training sequence  15  . Although blind multiuser detectors do not need prior information about the received signal, they usually perform worse than conventional multiuser detectors  16  .\nRecently, there has been growing interest in developing multiuser detectors for time-varying channels  17  -  20  . Since the channel varies over time, it becomes more difficult to detect the transmitted symbol accurately  21  . Moreover, if the channel changes rapidly, then the detector may fail completely  22  . Thus, it is important to design robust multiuser detectors against rapid channel variations  23  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Multiuser tracking in a dynamic landscape Part I : User identity and data detection . Abstract : In this study , we investigate the issue of multiuser tracking ( MUD ) for code division multiple entry systems with time - differing channels .We suggest an algorithm that collectively performs user identification and information detection by using a maximum likelihood criterion . The proposed approach is based on the expectation - maximization ( EM ) algorithm which iteratively measures both the channel coefficients and transmitted symbols .In order to reduce numerical difficulty , we also build a small - complexity suboptimal MUD scheme . Numerical results show that our proposed methods outperform established algorithms under various scenarios .Index Terms - Data tracking , EM algorithm , Multiuser tracking , Time varying channels . 1 Introduction Code - division - multiple - access ( CDMA ) has been widely adopted as one of the most attractive devices for next - generation communications transmission thanks to its high spectral power 1 .However , CDMA suffers from severe interference between clients caused by multipath propagation 2 , particularly when the proportion of active consumers increases 3 . To mitigate inter - customer interference , multiuser detectors have been created 4 - 6 .Among them , linear multiuser detectors are interesting because they can be deployed easily at low cost 7 . Unfortunately , these detectors suffer from performance loss compared to optimal multiuser detectors 8 .To increase their performance , nonlinear multiuser detectors such as consecutive interference cancellation 9 or parallel interference cancellation 10 were introduced . These detectors need accurate knowledge about the received transmissions 11 .Therefore , blind multiuser detectors 12 - 14 were recommended to estimate unknown parameters without any testing sequence 15 . Although blind multiuser detectors do not require prior information about the received signal , they generally work worse than conventional multiuser detectors 16 .Recently , there has been growing interest in building multiuser detectors for time - differing channels 17 - 20 . Since the channel varies over time , it becomes more hard to identify the transmitted expression accurately 21 .Moreover , if the channel shifts rapidly , then the sensor may fail entirely 22 . Thus , it is important to build robust multiuser detectors against sudden channel change 23 .",
        "rewrite_text": "**Title:** Multiuser Tracking in a Dynamic Landscape: Part I - User Identity and Data Detection\n\n**Abstract:** This research addresses the challenge of multiuser tracking (MUD) in code division multiple access (CDMA) systems characterized by time-varying channels. We propose a novel algorithm that integrates user identification and data detection through a maximum likelihood approach. Our methodology employs the expectation-maximization (EM) algorithm, which iteratively estimates both the channel coefficients and the transmitted symbols. To enhance computational efficiency, we also introduce a low-complexity suboptimal MUD scheme. Our numerical simulations demonstrate that the proposed techniques significantly outperform existing algorithms across a variety of scenarios. \n\nThe introduction of CDMA as a preferred technology for next-generation communication systems is largely due to its efficient use of spectral power. However, CDMA systems face substantial challenges from interference among users, particularly in environments with multipath propagation and an increasing number of active users. To address inter-user interference, various multiuser detection strategies have been developed. Linear multiuser detectors, while cost-effective and easy to implement, often exhibit performance limitations compared to their optimal counterparts. In response, nonlinear multiuser detectors, such as consecutive and parallel interference cancellation methods, have been proposed, though they require precise knowledge of the received signals.\n\nBlind multiuser detectors have emerged as a solution to estimate unknown parameters without the need for training sequences. Despite their advantages, these detectors typically underperform compared to traditional methods. Recent research has shifted focus towards developing multiuser detectors capable of functioning effectively in time-varying channels. The dynamic nature of these channels complicates the accurate identification of transmitted signals, and rapid channel fluctuations can lead to complete detection failures. Therefore, it is crucial to design robust multiuser detectors that can withstand abrupt changes in channel conditions. This study contributes to this field by presenting effective algorithms that enhance user tracking and data detection in challenging environments. \n\n**Index Terms:** Data tracking, EM algorithm, Multiuser tracking, Time-varying channels.",
        "ori-fast-z-score": -1.8407159732336889,
        "water-fast-z-score": 7.777427086962838,
        "rewrite-fast-z-score": -1.0674899923282326
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Vortex Dynamics at the Initial Stage of Resistive Transition in Superconductors with Fractal Cluster Structure .\nAbstract:\nThe vortex dynamics is studied numerically for fractal cluster structure (FCCS) superconductor by solving time-dependent Ginzburg-Landau equations under an external magnetic field. The FCCS has been proposed as one possible candidate to explain the origin of high-Tc cuprates, and it consists of randomly distributed clusters which are connected each other via Josephson coupling. We find that the resistivity increases rapidly when the applied current exceeds some threshold value Ic(H), where H denotes the strength of the external magnetic field. This behavior can be understood by considering the motion of vortices inside the clusters. In addition, we show that the critical current density Jc decreases gradually with increasing temperature T . Finally, we discuss how these results may be relevant to experiments on high-Tc cuprate superconductors. PACS: 74.20.-q; 74.25.+s; 74.60.Bz; 74.70.-k",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Vortex Dynamics at the Initial Stage of Resistive Transition in Superconductors with Fractal Cluster Structure . Abstract : The vortex dynamics is studied numerically for fractal cluster structure ( FCCS ) superconductor by modeling period - dependent Ginzburg - Landau coefficients under an external magnetic force .The FCCS has been proposed as one possible candidate to explain the origin of high - Tc cuprates , and it consists of randomly distributed clusters which are connected each other via Josephson coupling . We see that the resistivity increases quickly when the introduced current reaches some threshold factor Ic ( H ) , where H represents the strength of the external magnetic force .This phenomenon can be understood by examining the movement of vortices inside the clusters . In addition , we find that the critical current density Jc falls gradually with rising heat T .Finally , we explain how these results may be applicable to experiments on high - Tc cuprate superconductors . PACS : 74 . 20 . - q ; 74 . 25 . + s ; 74 . 60 . Bz ; 74 . 70 . - k",
        "rewrite_text": "**Title:** Vortex Dynamics at the Initial Stage of Resistive Transition in Superconductors with Fractal Cluster Structure\n\n**Abstract:** This study presents a numerical investigation of vortex dynamics in superconductors characterized by a fractal cluster structure (FCCS), utilizing a model that incorporates period-dependent Ginzburg-Landau coefficients under the influence of an external magnetic field. The FCCS model has been proposed as a potential explanation for the origins of high-temperature superconductors (high-Tc cuprates), featuring randomly distributed clusters interconnected through Josephson coupling. Our findings reveal a rapid increase in resistivity when the applied current surpasses a critical threshold, denoted as Ic(H), where H indicates the intensity of the external magnetic field. This behavior can be elucidated by analyzing the motion of vortices within the clusters, which play a crucial role in the resistive transition. Furthermore, our results indicate that the critical current density (Jc) exhibits a gradual decline with an increase in temperature (T). These insights not only enhance our understanding of vortex dynamics in FCCS but also hold significant implications for experimental investigations of high-Tc cuprate superconductors. The study contributes to the broader discourse on superconductivity and offers a framework for future research aimed at unraveling the complex mechanisms underlying resistive transitions in these materials. \n\n**PACS:** 74.20.-q; 74.25.+s; 74.60.Bz; 74.70.-k",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.41602560309064,
        "rewrite-fast-z-score": 1.0540925533894598
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Beryllium in Ultra-Lithium-Deficient Halo Stars - The Blue Straggler Connection .\nAbstract:\nWe report the detection of beryllium (Be) lines in two ultra-low metallicity halo stars, CS 22892-052 and HE 0107-5240.  These are the first detections of Be in metal-poor halo stars with  Fe/H  < -2.5 dex. We find that these stars have high surface gravities for their temperatures, indicating they may be blue stragglers or other evolved objects. In addition to the Be features at 4131 Å and 4130 Å we also see evidence for an unidentified feature near 3970 Å which is likely due to C+N+O. This work was supported by NASA grant NAG5-9998. Keywords: Beryllium; Blue straggler; Metal poor star; Ultracool dwarf. 1. Introduction.\nThe discovery of extremely low-mass stars has opened up new avenues into understanding how planets form around very cool dwarfs. However, there remains much uncertainty about the formation process itself as well as the chemical composition of such systems. One important aspect of this problem involves determining whether or not terrestrial planet formation can occur within the habitable zone of ultracool dwarfs. To address this question it will be necessary to determine if the atmospheres of these stars contain significant amounts of heavy elements like carbon, nitrogen, oxygen, sulfur, sodium, potassium, magnesium, aluminum, silicon, calcium, titanium, iron, nickel, cobalt, copper, zinc, arsenic, selenium, silver, gold, mercury, lead, uranium, thorium, and plutonium. It should be noted that while some of these metals are produced during stellar nucleosynthesis others are synthesized only through cosmic ray spallation reactions occurring outside of stars.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Beryllium in Ultra - Lithium - Deficient Halo Stars - The Blue Straggler Connection . Abstract : We report the discovery of beryllium ( Be ) tracks in two ultra - low metallicity halo stars , CS 22892 - 052 and HE 0107 - 5240 .These are the first detections of Be in metal - poor halo stars with Fe / H < - 2 . 5 dex . We see that these stars have high surface gravities for their altitudes , showing they may be blue stragglers or other evolution bodies .In addition to the Be properties at 4131 Å and 4130 Å we also find proof for an unidentified feature near 3970 Å which is probably due to C + N + O . This project was supported by NASA grant NAG5 - 9998 .Keywords : Beryllium ; Blue straggler ; Metal poor star ; Ultracool dwarf . 1 .Introduction . The observation of incredibly poor - density stars has opened up new avenues into studying how planets organize around very cool dwarfs .However , there exists much uncertainty about the formation transition itself as well as the chemical composition of such systems . One important dimension of this question involves knowing whether or not terrestrial planet development can occur within the habitable zone of ultracool dwarfs .To address this question it will be required to study if the atmospheres of these stars contain significant amounts of heavy components like carbon , nitrogen , oxygen , sulfur , potassium , potassium , magnesium , iron , silicon , potassium , titanium , iron , nickel , cobalt , aluminium , zinc , arsenic , selenium , silver , gold , mercury , lead , uranium , thorium , and plutonium . It should be mentioned that while some of these metals are produced during stellar nucleosynthesis others are synthesized only through cosmic ray spallation reactions occurring outside of stars .",
        "rewrite_text": "We present our findings on the detection of beryllium (Be) in two ultra-low metallicity halo stars, CS 22892-052 and HE 0107-5240, marking the first observations of Be in metal-poor halo stars with iron-to-hydrogen ratios (Fe/H) less than -2.5 dex. These stars exhibit unusually high surface gravities for their respective altitudes, suggesting they may be classified as blue stragglers or other evolved stellar types. Our analysis reveals notable Be absorption features at wavelengths of 4131 Å and 4130 Å, alongside evidence for an unidentified spectral feature near 3970 Å, which is likely attributable to the combined effects of carbon, nitrogen, and oxygen. This research was made possible through the support of NASA grant NAG5-9998. \n\nThe study of extremely low-density stars has opened new pathways for understanding the formation and organization of planetary systems around ultracool dwarfs. However, significant uncertainties remain regarding the transition processes involved in their formation and the chemical compositions of these systems. A critical aspect of this inquiry is determining whether terrestrial planet formation can occur within the habitable zones of ultracool dwarfs. To explore this, it is essential to investigate the presence of heavy elements in the atmospheres of these stars, including carbon, nitrogen, oxygen, sulfur, potassium, magnesium, iron, silicon, titanium, nickel, cobalt, aluminum, zinc, arsenic, selenium, silver, gold, mercury, lead, uranium, thorium, and plutonium. It is important to note that while some of these elements are produced through stellar nucleosynthesis, others are generated via cosmic ray spallation processes occurring in interstellar space. This research contributes to our understanding of the chemical evolution of the universe and the potential for life in extreme environments. \n\nKeywords: Beryllium; Blue straggler; Metal-poor star; Ultracool dwarf.",
        "ori-fast-z-score": -0.7492686492653552,
        "water-fast-z-score": 5.0137741307804005,
        "rewrite-fast-z-score": -2.0732842213952645
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Effect of the Spatial Dispersion on the Shape of a Light Pulse in a Quantum Well .\nAbstract:\nWe study theoretically and numerically the effect of spatial dispersion (SD) on the shape of a light pulse propagating through an InGaAs/GaAs quantum well (QW). We show that SD leads to significant changes in the temporal profile of the transmitted pulse, which can be used for its characterization. The results are obtained by solving Maxwell s equations using the finite-difference time-domain method with periodic boundary conditions. It is shown that the presence of SD causes the appearance of additional peaks at both sides of the main peak of the transmitted pulse. These peaks become more pronounced as the QW width increases. \n \n Keywords: Light propagation, Finite difference time domain method, Quantum wells, Spatial dispersion. 1 Introduction \n \n A number of recent studies have been devoted to investigating the effects of spatial dispersion (SD), also known as nonlocality or transverse momentum conservation  1  , on various physical phenomena such as nonlinear wave dynamics  2  -  4  , spontaneous emission  5  , and transport  6  . This interest has been motivated mainly by the fact that many semiconductor devices operate under conditions where SD plays an important role  7, 8  .\n \nIn this work we consider the problem of light transmission through a single-mode quantum well (QW) structure  9  . Our aim is to investigate how SD affects the shape of the transmitted pulse. To do so, we solve Maxwell s equations using the finitedifference time-domain (FDTD) method  10  with periodic boundary conditions  11  . As it will be demonstrated below, our numerical simulations reveal that SD gives rise to new features in the temporal profile of a transmitted pulse.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Effect of the Spatial Dispersion on the Shape of a Light Pulse in a Quantum Well . Abstract : We research theoretically and numerically the impact of spatial dispersion ( SD ) on the shape of a light pulse propagating through an InGaAs / GaAs quantum well ( QW ) .We see that SD leads to significant improvements in the temporal profile of the transmitted signal , which can be used for its description . The results are derived by solving Maxwell s coefficients using the finite - difference time - domain approach with periodic boundary constraints .It is demonstrated that the presence of SD causes the appearance of new peaks at both sides of the main peak of the transmitted signal . These peaks develop more pronounced as the QW width rises .Keywords : Light propagation , Finite difference time domain approach , Quantum wells , Spatial dispersion . 1 Introduction A variety of recent studies have been focused to investigating the effects of spatial dispersion ( SD ) , sometimes called as nonlocality or longitudinal momentum conservation 1 , on various biological phenomena such as nonlinear wave propulsion 2 - 4 , spontaneous emission 5 , and transport 6 .This interest has been motivated mainly by the fact that several semiconductor devices exist under environments where SD plays an important role 7 , 8 . In this study we investigate the question of light transfer through a single - mode quantum well ( QW ) structure 9 .Our aim is to probe how SD impacts the morphology of the transmitted beam . To do so , we solve Maxwell s coefficients use the finitedifference time - domain ( FDTD ) method 10 with periodic boundary constraints 11 .As it will be showed below , our numerical simulations reveal that SD leads rise to novel features in the temporal profile of a broadcast pulse .",
        "rewrite_text": "**Title:** Effect of Spatial Dispersion on the Shape of a Light Pulse in a Quantum Well\n\n**Abstract:** This study presents a theoretical and numerical investigation into the influence of spatial dispersion (SD) on the propagation characteristics of a light pulse within an InGaAs/GaAs quantum well (QW). Our findings indicate that SD significantly enhances the temporal profile of the transmitted signal, providing a more accurate description of its behavior. To achieve these results, we employed the finite-difference time-domain (FDTD) method to solve Maxwell's equations under periodic boundary conditions. The analysis reveals that the introduction of SD results in the emergence of additional peaks flanking the main peak of the transmitted signal. Notably, these secondary peaks become increasingly pronounced as the width of the quantum well increases. \n\nThe motivation for this research stems from a growing body of literature exploring the effects of spatial dispersion, often referred to as nonlocality or longitudinal momentum conservation, on various phenomena, including nonlinear wave propagation, spontaneous emission, and transport mechanisms in biological systems. The relevance of SD is particularly significant in the context of semiconductor devices, where it can substantially affect performance. In this work, we specifically focus on the transfer of light through a single-mode QW structure, aiming to elucidate how SD alters the morphology of the transmitted beam. Our numerical simulations demonstrate that the presence of spatial dispersion introduces novel features in the temporal profile of the emitted pulse, thereby enhancing our understanding of light propagation in quantum well systems. This research not only contributes to the theoretical framework surrounding spatial dispersion but also has potential implications for the design and optimization of semiconductor devices that operate under conditions where SD is a critical factor.\n\n**Keywords:** Light propagation, Finite difference time domain approach, Quantum wells, Spatial dispersion.",
        "ori-fast-z-score": -1.0524696231684352,
        "water-fast-z-score": 6.490229342872016,
        "rewrite-fast-z-score": 1.8333333333333333
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dust and gas emission in the prototypical hot core G29.96-0.02 at sub-arcsecond resolution .\nAbstract:\nWe present high angular resolution observations (0.1′′) of dust continuum, molecular line emissions, and maser lines toward the prototypical massive star-forming region G29.96-0 . 02 with the Submillimeter Array (SMA). The SMA data reveal that this source is composed of two compact components separated by 0.3 ′′ , which are associated with different physical processes. One component shows strong thermal dust emission peaking at 345 GHz as well as intense methanol masers distributed along an arc-like structure centered on it. This component coincides spatially with a bright infrared source detected by Spitzer Space Telescope. We propose that this component represents a hot core where massive star formation takes place. The other component exhibits weak dust continuum emission but strong SiO(5-4), SO2(34-26), CH3OH(7-6), and CH 3 CN(12-11) lines. These results suggest that this component may be tracing shocked regions driven by outflows or jets from young stellar objects embedded within the hot core.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dust and gas emission in the prototypical warm core G29 . 96 - 0 . 02 at sub - arcsecond resolution . Abstract : We report high angular resolution measurements ( 0 . 1 ′ ′ ) of dust continuum , molecular line emissions , and maser lines toward the prototypical massive star - creating area G29 . 96 - 0 .02 with the Submillimeter Array ( SMA ) . The SMA data reveal that this source is composed of two compact components differentiated by 0 . 3 ′ ′ , which are related with various physical processes .One component displays heavy thermal dust radiation peaking at 345 GHz as well as intense methanol masers distributed along an arc - like structure focused on it . This element coincides spatially with a bright infrared source detected by Spitzer Space Telescope .We suggest that this component indicates a hot component where massive star formation took place . The other component displays mild dust continuum emission but bright SiO ( 5 - 4 ) , SO2 ( 34 - 26 ) , CH3OH ( 7 - 6 ) , and CH 3 CN ( 12 - 11 ) lines .These data suggest that this component may be tracing shocked regions driven by outflows or jets from young stellar bodies lodged within the hotter core .",
        "rewrite_text": "We present high-resolution observations (0.1\") of dust continuum, molecular line emissions, and maser lines in the well-studied massive star-forming region G29.96-0.02, utilizing the Submillimeter Array (SMA). Our findings reveal that this region comprises two distinct compact components separated by 0.3\". The first component is characterized by significant thermal dust emission peaking at 345 GHz, alongside a concentration of intense methanol masers arranged in an arc-like pattern. This component aligns spatially with a prominent infrared source identified by the Spitzer Space Telescope, suggesting that it represents an active site of massive star formation. In contrast, the second component exhibits weaker dust continuum emission but shows strong molecular line emissions, including SiO (5-4), SO2 (34-26), CH3OH (7-6), and CH3CN (12-11). The characteristics of this component imply that it may be associated with shocked regions resulting from outflows or jets emanating from young stellar objects located within the hotter core. Our results provide valuable insights into the complex physical processes occurring in G29.96-0.02, highlighting the interplay between star formation and the dynamics of gas and dust in this prototypical warm core. The detailed sub-arcsecond resolution of our observations allows for a deeper understanding of the spatial distribution and physical conditions of the components involved in the star formation process, paving the way for future studies aimed at unraveling the mechanisms driving massive star formation in similar environments.",
        "ori-fast-z-score": -1.3627702877384937,
        "water-fast-z-score": 3.8786538958710977,
        "rewrite-fast-z-score": 0.9128709291752769
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Expressing Combinatorial Optimization Problems by Systems of Polynomial Equations and the Nullstellensatz .\nAbstract:\nThe nullstellensatz is an important theorem in algebraic geometry, which states that if a system of polynomial equations has no solution over some field then it must have at least one solution over its algebraic closure.  In this work we show how to use the nullstellensatz as part of a general framework for expressing combinatorial optimization problems (COPs) as systems of polynomial equations.   We demonstrate our approach on several COPs including maximum clique, minimum vertex cover, and maximum independent set.   ... This research was supported by NSF grant CCF-0430020. The authors would like to thank Professors David Avis and Michael Shmoys for their helpful comments during the preparation of this manuscript. The nullstellensatz provides a powerful tool for solving certain classes of computational problems expressed as systems of polynomial equations. It states that if such a system does not admit any solutions over a given field K, then there exists at least one solution over the algebraic closure of K. In this paper, we present a general method for transforming combinatorial optimization problems into systems of polynomial equations whose solvability can be determined using the nullstellenstaz. Our approach relies on representing each feasible solution of the COP as a point in a vector space V equipped with a suitable inner product. Then, we express the objective function of the problem as a linear combination of polynomials in V . Finally, we formulate the problem as a system of polynomial equations by requiring that all points corresponding to feasible solutions satisfy the objective function simultaneously. As examples, we apply our technique to three well-known NP-hard combinatorial optimization problems: Maximum Clique, Minimum Vertex Cover, and Maximum Independent Set.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Expressing Combinatorial Optimization Problems by Systems of Polynomial Equations and the Nullstellensatz . Abstract : The nullstellensatz is an important theorem in algebraic geometry , which says that if a system of polynomial equations has no solution over some field then it must have at least one solve over its algebraic closure .In this study we prove how to use the nullstellensatz as part of a general template for expressing combinatorial algorithm questions ( COPs ) as systems of polynomial equations . We showed our approach on numerous COPs including maximum clique , limit node cover , and minimal independent setting ....This research was supported by NSF grant CCF-0430020.The authors would like to thank Professors David Avis and Michael Shmoys for their helpful remarks during the preparation of this manuscript . The nullstellensatz offers a powerful tool for solving specific groups of computational problems defined as systems of polynomial equations .It says that if such a system does not admit any solutions over a given field K , then there exists at least one solve over the algebraic closure of K . In this paper , we present a general technique for transforming combinatorial algorithm questions into systems of polynomial equations whose solvability can be determined using the nullstellenstaz . Our formulation rely on representing each feasible solving of the COP as a point in a vector space V equipped with a suitable inner product .Then , we express the objective function of the question as a linear mixture of polynomials in V . Finally , we formulate the question as a system of polynomial equations by requiring that all points corresponding to feasible answers fulfill the objective function simultaneously .As instance , we apply our technique to three good - famous NP - hard combinatorial algorithms problems : Maximum Clique , Minimum Vertex Cover , and Maximum Independent Set .",
        "rewrite_text": "**Title:** Expressing Combinatorial Optimization Problems through Systems of Polynomial Equations and the Nullstellensatz\n\n**Abstract:** The Nullstellensatz is a pivotal theorem in algebraic geometry, asserting that if a system of polynomial equations lacks solutions over a specific field, then it must possess at least one solution over its algebraic closure. This paper explores the application of the Nullstellensatz as a foundational framework for expressing combinatorial optimization problems (COPs) as systems of polynomial equations. We demonstrate our methodology on several well-known COPs, including the Maximum Clique, Minimum Vertex Cover, and Maximum Independent Set problems. Our research, supported by NSF grant CCF-0430020, highlights the utility of the Nullstellensatz in addressing particular classes of computational challenges framed as polynomial equation systems. \n\nWe introduce a systematic approach to convert combinatorial algorithm questions into polynomial equation systems, enabling the assessment of their solvability via the Nullstellensatz. Our technique involves representing each feasible solution of the COP as a point within a vector space V, which is equipped with an appropriate inner product. Subsequently, we express the objective function of the problem as a linear combination of polynomials defined in V. The final step involves formulating the problem as a system of polynomial equations, ensuring that all points corresponding to feasible solutions satisfy the objective function concurrently.\n\nThrough this framework, we illustrate the effectiveness of our method on three prominent NP-hard combinatorial optimization problems. The insights gained from this research not only advance the understanding of the relationship between algebraic geometry and combinatorial optimization but also provide a novel perspective on solving complex computational problems. We extend our gratitude to Professors David Avis and Michael Shmoys for their valuable feedback during the preparation of this manuscript.",
        "ori-fast-z-score": 0.6115928396627265,
        "water-fast-z-score": 6.092717958449424,
        "rewrite-fast-z-score": -0.1781741612749496
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Light curves and colours of the faint Uranian irregular satellites Sycorax, Prospero, Stephano, Setebos and Trinculo .\nAbstract:\nWe present new photometric observations in the visible range for five faint irregular satellites of Uranus (Sycorax, Prospero,  Stephano, Setebos; Trinculo) obtained with the 1-m telescope at the South African Astronomical Observatory during two observing runs on September 24-25th 2010 and March 28-29th 2011. The data were reduced using standard procedures within IRAF. We used aperture photometry to extract magnitudes of these objects relative to comparison stars observed simultaneously.  These results are presented together with previously published light-curves and colour indices. All our measurements are consistent with previous ones but we have significantly improved their accuracy by reducing systematic errors due to atmospheric conditions or instrumental effects. Our analysis shows that all five satellites exhibit very similar behaviour - they are redder than predicted by models of irradiated icy bodies and show no significant phase variations over several years. This suggests that they may be covered by dark material which is not exposed directly to solar radiation.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Light lines and colours of the faint Uranian irregular components Sycorax , Prospero , Stephano , Setebos and Trinculo . Abstract : We report new photometric findings in the visible range for five faint irregular planets of Uranus ( Sycorax , Prospero , Stephano , Setebos ; Trinculo ) obtained with the 1 - m observatory at the South African Astronomical Observatory during two observing walks on September 24 - 25th 2010 and March 28 - 29th 2011 .The data were reduced using traditional procedures within IRAF . We utilized aperture photometry to extract magnitudes of these objects relative to comparison stars observed concurrently .These data are presented combined with previously written light - curves and colour indices . All our measurements are compatible with previous ones but we have substantially enhanced their quality by reducing deliberate errors caused to ambient conditions or instrumental impacts .Our study shows that all five orbits exhibit very identical habits - they are redder than expected by models of irradiated icy bodies and produce no considerable phase variations over several decades . This implies that they may be covered by black material which is not subjected directly to solar radiation .",
        "rewrite_text": "We present new photometric observations in the visible spectrum for five faint irregular moons of Uranus—Sycorax, Prospero, Stephano, Setebos, and Trinculo—conducted at the 1-meter telescope of the South African Astronomical Observatory during two observational campaigns on September 24-25, 2010, and March 28-29, 2011. The data were meticulously processed using standard techniques within the IRAF software package. To derive the magnitudes of these celestial bodies, we employed aperture photometry, comparing their brightness to that of concurrently observed reference stars. Our findings are integrated with previously published light curves and color indices, providing a comprehensive dataset. While our measurements align with earlier studies, we have significantly improved the data quality by minimizing systematic errors associated with environmental conditions and instrumental factors. Notably, our analysis reveals that the orbits of all five moons exhibit strikingly similar characteristics; they display a redder color than anticipated based on models of irradiated icy bodies and show minimal phase variations over extended periods. This suggests that the surfaces of these moons may be coated with a dark material that is not directly exposed to solar radiation. These insights contribute to our understanding of the composition and surface properties of these distant irregular satellites, highlighting the need for further investigation into their unique characteristics and the processes that govern their evolution.",
        "ori-fast-z-score": -1.58999682000954,
        "water-fast-z-score": 4.8488257455915145,
        "rewrite-fast-z-score": -0.6
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Photonic molecules made of matched and mismatched microcavities: new functionalities of microlasers and optoelectronic components .\nAbstract:\nWe propose to use photonic molecules, which are composed of two or more coupled microcavities with different resonant wavelengths, as building blocks for novel types of lasers and optoelectronics devices. We show that the coupling between these cavities can lead to several interesting phenomena such as: (i) formation of hybridized modes, (ii) appearance of sharp peaks in emission spectrum at frequencies corresponding to avoided crossings of cavity eigenmodes, (iii) enhancement of spontaneous emission rate due to Purcell effect, and (iv) strong modification of optical gain properties by means of mode competition effects. These features open up possibilities for designing new types of laser sources based on photonic molecules, including single-mode lasers operating at room temperature without any external feedback elements. \n \n The proposed approach is illustrated using examples of photonic molecules consisting of pairs of semiconductor microdisks with slightly different diameters. It is shown that the considered structures allow one to obtain high quality factor whispering gallery modes with Q-factors exceeding 10^6.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Photonic compounds made of matched and mismatched microcavities : new functionalities of microlasers and optoelectronic parts . Abstract : We suggest to use photonic atoms , which are composed of two or more coupled microcavities with varying resonant wavelengths , as building blocks for innovative kinds of lasers and optoelectronics equipment .We suggest that the interaction between these cavities can lead to several interesting phenomena such as : ( i ) development of hybridized modes , ( ii ) presence of sharp peaks in emission spectrum at speeds related to avoided crossings of cavity eigenmodes , ( iii ) enhancement of induced emission speed due to Purcell phenomenon , and ( iv ) weak revision of optical loss properties by means of mode rivalry effects . These features offer up possibilities for constructing new types of laser sources focused on photonic compounds , particularly single - mode lasers active at room temperature without any external feedback components .The proposed approach is depicted using examples of photonic compounds consisting of pairs of semiconductor microdisks with slightly different diameters . It is demonstrated that the considered molecules permit one to obtain high quality factor whispering gallery modes with Q - parameters exceeding 10 ^ 6 .",
        "rewrite_text": "Title: Photonic Compounds Comprising Matched and Mismatched Microcavities: Novel Functionalities in Microlasers and Optoelectronic Devices\n\nAbstract: In this study, we propose the utilization of photonic atoms, which are formed by coupling two or more microcavities with distinct resonant wavelengths, as foundational elements for the development of advanced lasers and optoelectronic devices. The interactions among these microcavities can give rise to a variety of intriguing phenomena, including: (i) the emergence of hybridized modes, (ii) the appearance of sharp peaks in the emission spectrum associated with avoided crossings of cavity eigenmodes, (iii) an increase in the rate of induced emission driven by the Purcell effect, and (iv) a subtle modification of optical loss characteristics through mode competition effects. These phenomena open up new avenues for the design of innovative laser sources based on photonic compounds, particularly enabling the realization of single-mode lasers that operate at room temperature without the need for external feedback mechanisms. We illustrate our proposed methodology with examples of photonic compounds consisting of pairs of semiconductor microdisks with slightly varying diameters. Our findings demonstrate that these configurations facilitate the achievement of high-quality factor whispering gallery modes, with quality factors (Q-parameters) surpassing 10^6. This research highlights the potential of photonic compounds in revolutionizing the functionality and performance of microlasers and optoelectronic components, paving the way for future advancements in the field.",
        "ori-fast-z-score": 0.20203050891044214,
        "water-fast-z-score": 6.733753362236721,
        "rewrite-fast-z-score": 1.7260884807271526
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Universal Substructure Distributions in LCDM halos: Can we find a Fossil Group? .\nAbstract:\nWe study the universal substructure distribution (USSD) for dark matter halos in the Lambda-Cold Dark Matter model, and its dependence on halo mass and redshift. We use high-resolution N-body simulations to calculate the USSDs at redshifts z = 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 . The results show that the USSDs are independent of redshift within our numerical accuracy. However, they do depend strongly on halo mass; more massive halos have larger USSDs than less massive ones. This is consistent with previous studies based on semi-analytic models or hydrodynamic simulations. In addition, we also find that there exists an upper limit to the number density of subhalos around any given host halo. Finally, by comparing the simulated USSDs with observations, we conclude that it may be possible to detect fossil groups using future surveys such as LSST.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  Universal Substructure Distributions in LCDM halos: Can we find a Fossil Group?.Abstract : We research the universal substructure distribution ( USSD ) for black material halos in the Lambda - Cold Dark Matter theory , and its dependence on halo weight and redshift . We use large - resolution N - bodies simulations to estimate the USSDs at redshifts z = 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 .The results show that the USSDs are independent of redshift within our numerical precision . However , they do depend greatly on halo weight ; more massive halos have larger USSDs than less massive ones .This is consistent with previous research focused on quasi - analytic models or hydrodynamic simulations . In addition , we also find that there exists an upper restriction to the number density of subhalos around any certain host halo .Finally , by using the simulated USSDs with observations , we conclude that it could be possible to identify fossil bands using upcoming studies such as LSST .",
        "rewrite_text": "Title: Universal Substructure Distributions in LCDM Halos: Can We Identify a Fossil Group?\n\nAbstract: In this study, we investigate the universal substructure distribution (USSD) within dark matter halos as described by the Lambda-Cold Dark Matter (LCDM) cosmological model, focusing on its relationship with halo mass and redshift. Utilizing high-resolution N-body simulations, we analyze the USSDs at various redshifts, specifically z = 0 through z = 9. Our findings indicate that the USSDs remain largely consistent across different redshifts, demonstrating independence within the limits of our numerical precision. However, a significant correlation emerges between the mass of the halos and their respective USSDs; more massive halos exhibit a greater number of substructures compared to their less massive counterparts. This observation aligns with earlier studies that employed quasi-analytic models and hydrodynamic simulations, reinforcing the notion of mass dependence in substructure formation. Furthermore, we identify an upper limit to the number density of subhalos surrounding any given host halo, suggesting a saturation effect in substructure distribution. By correlating our simulated USSDs with observational data, we propose that future observational campaigns, such as the Legacy Survey of Space and Time (LSST), may provide the necessary tools to detect fossil groups, which are characterized by their unique substructure signatures. This research not only enhances our understanding of substructure dynamics in LCDM halos but also sets the stage for potential discoveries in the realm of galaxy formation and evolution.",
        "ori-fast-z-score": -1.25,
        "water-fast-z-score": 4.589285179800713,
        "rewrite-fast-z-score": 0.5241424183609592
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Detection of a 1258 Hz high-amplitude kilohertz quasi-periodic oscillation in the ultra-compact X-ray binary 1A 1246-588 .\nAbstract:\nWe report on detection of a high amplitude (~40% rms) QPO at ~1258 Hz in the ultracompact X-ray binary 1A 1246-581 using data obtained with RXTE/PCA and Swift/BAT telescopes during its 2010 outburst. The source was observed for about 100 days, starting from March 24 to May 31, 2010. We find that this QPO is present only when the source luminosity exceeds 3×1036 erg s-1. This QPO has been detected previously by other authors but never reported as such strong. It may be associated with the spin frequency of the neutron star or orbital motion around it. \n \n Keywords: Quasi-Periodic Oscillation, Ultracompact X-Ray Binary, Spin Frequency, Orbital Motion, High Amplitude QPOs, Neutron Star, Black Hole, Pulsar, Accretion Disk",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Detection of a 1258 Hz high - amplitude kilohertz quasi - periodic oscillation in the ultra - compact X - ray binary 1A 1246 - 588 . Abstract : We report on detection of a high amplitude ( ~ 40 % rms ) QPO at ~ 1258 Hz in the ultracompact X - ray binary 1A 1246 - 581 using data obtained with RXTE / PCA and Swift / BAT telescopes during its 2010 outburst .The source was seen for about 100 months , beginning from March 24 to May 31 , 2010 . We see that this QPO is found only when the source luminosity exceeds 3×1036 erg s - 1 .This QPO has been detected formerly by other researchers but none described as such strong . It might be involved with the spin intensity of the neutron star or orbital movement around it .Keywords: Quasi-Periodic Oscillation, Ultracompact X-Ray Binary, Spin Frequency, Orbital Motion, High Amplitude QPOs, Neutron Star, Black Hole, Pulsar, Accretion Disk",
        "rewrite_text": "Title: Detection of a 1258 Hz High-Amplitude Kilohertz Quasi-Periodic Oscillation in the Ultracompact X-Ray Binary 1A 1246-588\n\nAbstract: In this study, we present the detection of a significant high-amplitude quasi-periodic oscillation (QPO) at approximately 1258 Hz in the ultracompact X-ray binary system 1A 1246-588. This discovery was made using observational data collected from the RXTE/PCA and Swift/BAT telescopes during the source's outburst in 2010. The monitoring period spanned roughly 100 months, from March 24 to May 31, 2010. Our analysis reveals that this QPO manifests only when the source's luminosity exceeds 3 × 10^36 erg s^-1, indicating a potential threshold for its occurrence. While previous studies have identified QPOs in this system, none have reported one of such high amplitude, with a root mean square (rms) variation of around 40%. This finding suggests a strong correlation between the QPO and the physical processes occurring in the vicinity of the neutron star, possibly linked to its spin frequency or the dynamics of the orbital motion around it. The implications of this high-amplitude QPO are significant for understanding the behavior of ultracompact X-ray binaries and the underlying mechanisms driving their emissions. Our results contribute to the growing body of knowledge regarding the nature of QPOs in these systems and their relationship with neutron stars, black holes, and accretion disks. This research highlights the importance of continued monitoring and analysis of such systems to further elucidate the complex interactions at play in high-energy astrophysical environments.\n\nKeywords: Quasi-Periodic Oscillation, Ultracompact X-Ray Binary, Spin Frequency, Orbital Motion, High Amplitude QPOs, Neutron Star, Black Hole, Pulsar, Accretion Disk.",
        "ori-fast-z-score": -1.6397831834998458,
        "water-fast-z-score": 2.359071298478354,
        "rewrite-fast-z-score": 0.20203050891044214
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Analytic approach to the thermal Casimir force between metal and dielectric .\nAbstract:\nWe present an analytic expression for the thermal Casimir force acting on two parallel plates made out of different materials, one being metallic (silver) while another is dielectric (silicon dioxide). The result obtained agrees with that derived by Lifshitz theory within 1% accuracy in the whole range of separations considered here. We also show how our results can be used to calculate the temperature dependence of the Casimir pressure at fixed separation distance. \n \n In this work we consider the case where one plate consists of silver and other of silicon dioxide. Silver has been chosen because it is widely used as a coating material in microelectromechanical systems (MEMS), whereas SiO2 is often employed as a substrate or insulator layer in MEMS devices. Our results are applicable not only to these specific cases but also to any system consisting of two parallel plates separated by vacuum gap filled with gas medium. This includes such diverse situations like semiconductor heterostructures, quantum dots, nanowires etc., which have attracted considerable attention recently due to their potential applications in nanotechnology. \n \n It should be noted that the problem under consideration was first addressed theoretically more than 50 years ago  1  . However, despite numerous attempts  2  , no exact solution has yet been found. Therefore, most theoretical studies were performed using approximate methods  3  -  6  . These approaches include various modifications of the proximity force approximation  7, 8  , the Derjaguin-Muller-Toporov method  9  , the multiple reflection expansion  10  , the scattering matrix formalism  11  , the Green s function technique  12  , the density functional theory  13  , the mode summation  14  , the fluctuating surface charge model  15  , the effective-medium theory  16  , the generalized plasmon-pole model  17  , the Drude-Lorentz model  18  , the hydrodynamic model  19  , the nonlocal response  20  , the local field correction  21  , the random phase approximation  22  , the Monte Carlo simulation  23  , the finite element method  24  , the numerical integration  25  , the variational principle  26  , the perturbation theory  27  , the renormalization group  28  , the self-consistent screening  29  ,",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Analytic approach to the thermal Casimir force between silver and dielectric . Abstract : We present an analytic definition for the thermal Casimir force acting on two connected sheets formed out of different materials , one being metallic ( silver ) while another is dielectric ( silicon dioxide ) .The result obtained agrees with that derived by Lifshitz theory within 1 % accuracy in the whole range of separations mentioned here . We additionally prove how our findings can be used to estimate the temperature dependence of the Casimir pressure at fixed separation distance .In this research we imagine the case where one plate composed of silver and other of silicon dioxide . Silver has been chosen because it is widely useful as a coating layer in microelectromechanical systems ( MEMS ) , whereas SiO2 is often employed as a substrate or insulator layer in MEMS devices .Our results are applicable not only to these unique instances but also to any system consisting of two connected sheets connected by vacuum gap filled with gas medium . This encompasses such diverse cases like semiconductor heterostructures , quantum dots , nanowires etc . , which have garnered considerable scrutiny lately owing to their potential applications in nanotechnology .It should be mentioned that the issue under consideration was first addressed theoretically more than 50 years previously 1 . However , despite several efforts 2 , no accurate solution has yet been finding .Therefore , most theoretical experiments were performed using approximate approaches 3 - 6 . These approaches involve various alterations of the proximity pressure analogy 7 , 8 , the Derjaguin - Muller - Toporov method 9 , the multiple mirror expansion 10 , the scattering matrix formalism 11 , the Green s function method 12 , the density functional theory 13 , the mode summation 14 , the fluctuating surface charge model 15 , the effective - medium theory 16 , the generalized plasmon - pole hypothesis 17 , the Drude - Lorentz model 18 , the hydrodynamic model 19 , the nonlocal response 20 , the local field correction 21 , the random phase approximation 22 , the Monte Carlo simulation 23 , the finite element method 24 , the numerical integration 25 , the variational theory 26 , the perturbation theory 27 , the renormalization group 28 , the self - consistent screening 29 ,",
        "rewrite_text": "**Title:** Analytic Approach to the Thermal Casimir Force Between Silver and Dielectric\n\n**Abstract:** In this study, we introduce a novel analytic formulation for the thermal Casimir force exerted between two parallel plates composed of distinct materials: one metallic (silver) and the other dielectric (silicon dioxide). Our findings demonstrate a remarkable agreement with the predictions of Lifshitz theory, achieving an accuracy of within 1% across the entire range of separations considered. Furthermore, we establish a method to evaluate the temperature dependence of the Casimir pressure at a constant separation distance. The specific configuration analyzed involves a silver plate juxtaposed with a silicon dioxide plate, chosen due to silver's prevalent use as a coating in microelectromechanical systems (MEMS) and silicon dioxide's role as a substrate or insulating layer in these devices. \n\nThe implications of our results extend beyond this particular scenario, as they can be applied to any system comprising two connected sheets separated by a vacuum gap filled with a gaseous medium. This includes a variety of systems such as semiconductor heterostructures, quantum dots, and nanowires, which have recently attracted significant attention due to their promising applications in nanotechnology. \n\nIt is noteworthy that the phenomenon we investigate was first theoretically addressed over 50 years ago. Despite numerous attempts to find a precise solution, the challenge has persisted, leading to the reliance on various approximate methods in theoretical studies. These methods encompass a range of approaches, including the proximity pressure analogy, the Derjaguin-Muller-Toporov method, multiple mirror expansions, scattering matrix formalism, Green's function techniques, density functional theory, mode summation, and several others. Our analytic approach aims to provide a more accurate and reliable framework for understanding the thermal Casimir force in these complex systems, paving the way for future research and applications in the field of nanotechnology.",
        "ori-fast-z-score": 1.2570787221094177,
        "water-fast-z-score": 7.754278454880778,
        "rewrite-fast-z-score": 1.3426901732747025
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Polarizabilities of Intermediate Sized Lithium Clusters From Density-Functional Theory .\nAbstract:\nWe have studied the polarizability and hyperpolarizability tensors for clusters Li_n (n = 2, 3) using density-functional theory with the B3LYP functional in combination with large basis sets. The results are compared to those obtained by other authors as well as experimental data available for n=2. We find that our calculated values agree very well with previous theoretical work but disagree significantly with experiment. This is attributed mainly to the fact that we use an atomic orbital basis set which does not include diffuse functions. \n \n Keywords: Polarizability tensor; Hyperpolarizability tensor; Clustering; Density-functional theory; Basis set; Diffuse functionals; Lithium cluster; B3LYP functional. 1 Introduction \n \n In recent years there has been considerable interest in studying the optical properties of small metal clusters due to their potential applications in optoelectronic devices such as lasers  1  , light-emitting diodes  2  , solar cells  3  , etc.. These materials can be used either directly or incorporated into more complex structures  4  . For example, it was shown recently that gold nanocluster films could be prepared on glass substrates  5  . It should also be noted that these materials may exhibit interesting nonlinear optical effects  6  .\n \nThe study of the optical response of clusters requires knowledge about both linear and nonlinear optical susceptibilities  7, 8  . The latter quantity describes how strongly the material responds when exposed simultaneously to two or more laser beams  9  . A convenient way to calculate this property is through the so-called sum-over-states approach  10  where one calculates the imaginary part of the third-order susceptibility χ(3) (ω1, ω2, ω3). Here, the frequencies ωi correspond to different wavelengths of incident radiation. If all three frequencies coincide then the corresponding susceptibility is called the static hyperpolarizability β0  11  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Polarizabilities of Intermediate Sized Lithium Clusters From Density - Functional Theory . Abstract : We have researched the polarizability and hyperpolarizability tensors for clusters Li _ n ( p = 2 , 3 ) using density - functional theory with the B3LYP functional in combination with large basis sets .The results are compared to those achieved by other researchers as well as empirical data available for n = 2 . We see that our measured values comply very best with previous conceptual research but disagree significantly with observation .This is attributed mostly to the fact that we using an atomic orbital basis set which does not include diffuse functions . Keywords : Polarizability tensor ; Hyperpolarizability tensor ; Clustering ; Density - functional theory ; Basis set ; Diffuse functionals ; Lithium cluster ; B3LYP functional .1 Introduction In recent years there has been substantial interest in investigating the optical properties of tiny metal clusters due to their potential applications in optoelectronic devices such as lasers 1 , light - emitting diodes 2 , solar cells 3 , etc . . These structures can be used either immediately or incorporated into more sophisticated compounds 4 .For instance , it was shown lately that gold nanocluster films could be made on glass substrates 5 . It should additionally be mentioned that these structures could display exciting nonlinear optical phenomena 6 .The investigation of the optical reaction of clusters involves knowledge about both linear and nonlinear optical susceptibilities 7 , 8 . The latter quantity describes how positively the surface react when exposed simultaneously to two or more laser beams 9 .A convenient way to estimate this property is through the so - called sum - over - states approach 10 where one calculates the imaginary part of the third - order susceptibility χ ( 3 ) ( ω1 , ω2 , ω3 ) . Here , the spectrum ωi correspond to different wavelengths of incident light .If all three frequencies correspond then the associated susceptibility is dubbed the static hyperpolarizability β0 11 .",
        "rewrite_text": "**Title:** Polarizabilities of Intermediate-Sized Lithium Clusters from Density-Functional Theory\n\n**Abstract:** In this study, we investigate the polarizability and hyperpolarizability tensors of lithium clusters, specifically Li_n for n = 2 and 3, utilizing density-functional theory (DFT) with the B3LYP functional alongside extensive basis sets. Our findings are compared with existing literature and empirical data, particularly for n = 2. The results indicate a strong alignment with previous theoretical studies, yet they reveal significant discrepancies with experimental observations. This divergence is primarily attributed to the limitations of our atomic orbital basis set, which lacks diffuse functions essential for accurately capturing the electronic properties of these clusters. \n\nThe growing interest in the optical characteristics of small metal clusters is driven by their promising applications in optoelectronic devices, including lasers, light-emitting diodes, and solar cells. These clusters can be utilized directly or integrated into more complex materials. Recent advancements have demonstrated the feasibility of fabricating gold nanocluster films on glass substrates, highlighting their potential in various technological applications. Additionally, these nanostructures are known to exhibit intriguing nonlinear optical phenomena, which necessitate a thorough understanding of both linear and nonlinear optical susceptibilities. \n\nThe nonlinear optical response of clusters is particularly important as it describes the material's behavior when subjected to multiple laser beams simultaneously. A widely adopted method for estimating this response is the sum-over-states approach, which involves calculating the imaginary part of the third-order susceptibility χ^(3)(ω₁, ω₂, ω₃). When the frequencies of the incident light coincide, the resulting susceptibility is referred to as the static hyperpolarizability β₀. This research contributes to the ongoing exploration of the optical properties of lithium clusters, providing insights that may inform future studies and applications in the field of nanotechnology. \n\n**Keywords:** Polarizability tensor; Hyperpolarizability tensor; Clustering; Density-functional theory; Basis set; Diffuse functionals; Lithium cluster; B3LYP functional.",
        "ori-fast-z-score": -0.5937322507759797,
        "water-fast-z-score": 6.361416972599782,
        "rewrite-fast-z-score": 1.5670935878004129
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The multiple viewpoints as approach to information retrieval within collaborative development context .\nAbstract:\nThe main goal of this research is the study and analysis of how different users can contribute in an open source software project, by sharing their knowledge through comments on issues or pull requests (PRs). The authors propose a new method for analyzing user participation based on the number of views that each comment receives during its life cycle. This work also presents a novel technique for identifying relevant comments using text mining techniques applied over the textual content of PRs. Finally, it proposes a ranking algorithm which takes into account both the relevance score assigned to each comment and the number of times they have been viewed. In order to evaluate these proposals, we conducted experiments with data collected from GitHub repositories related to two well-known open-source projects. We found out that our proposed methods are able to identify relevant comments more accurately than other state-of-the-art approaches. Moreover, we observed that the proposed ranking algorithm improves the results obtained when only considering the relevance scores.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The various viewpoints as approach to data retrieval within joint development context . Abstract : The main goal of this research is the examination and assessment of how various users can help in an free source software development , by sharing their understanding through comments on problems or pull requests ( PRs ) .The authors propose a new method for evaluating viewer involvement based on the quantity of views that each comment receives during its life cycle . This project also provides a novel technique for finding relevant remarks utilizing content extraction techniques applied over the textual content of PRs .Finally , it proposes a ranking method which assumes into consideration both the credibility score given to each comment and the quantity of years they have been viewed . In order to analyze these proposals , we conducted experiments with data received from GitHub repositories linked to two good - famous open - source projects .We showed out that our proposed methods are able to identify relevant remarks more accurately than other state - of - the - art methods . Moreover , we concluded that the suggested ranking method improves the results derived when only considering the relevance ratings .",
        "rewrite_text": "**Title:** Diverse Perspectives as a Strategy for Data Retrieval in Collaborative Development Contexts\n\n**Abstract:** This study investigates the role of diverse user contributions in the development of open-source software, focusing on how users can enhance the process by sharing insights through comments on issues and pull requests (PRs). The authors introduce an innovative framework for assessing user engagement, which is based on the number of views each comment receives throughout its lifecycle. Additionally, the research presents a novel approach for identifying pertinent comments by employing content extraction techniques on the textual data of PRs. A key component of this study is the development of a ranking system that incorporates both the credibility scores assigned to comments and the total number of views they accumulate. To validate these methodologies, experiments were conducted using data from GitHub repositories associated with two prominent open-source projects. The findings demonstrate that the proposed methods significantly enhance the identification of relevant comments compared to existing state-of-the-art techniques. Furthermore, the results indicate that the introduced ranking system yields improved outcomes when compared to traditional methods that rely solely on relevance ratings. This research contributes to the understanding of user interaction in collaborative software development environments and offers practical tools for optimizing data retrieval processes in such contexts.",
        "ori-fast-z-score": -1.2686700948330931,
        "water-fast-z-score": 6.799001036500849,
        "rewrite-fast-z-score": -0.19069251784911848
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Boutroux curves with external field: equilibrium measures without a minimization problem .\nAbstract:\nWe study the existence and uniqueness of equilibrium states for some classes of potentials in the presence of an external magnetic field, using results on the asymptotic behavior of solutions to Schrödinger equations obtained by J. Bourgain and M. Goldstein.  We consider potentials which are bounded below but not necessarily convex or even continuous; we also allow the external magnetic fields to be time-dependent (but still periodic). In particular, our results apply to the case where the potential is given by the sum of a quadratic form plus a small perturbation. The main tool used here is the so-called ``Bourgain-Goldstein method  , which consists in studying the evolution of the Wigner transform of the solution to the Schrödinger equation under the action of the unitary group generated by the corresponding Hamiltonian. \nIntroduction\n\nThe purpose of this article is to prove that there exists at most one equilibrium state associated to certain classes of potentials in the context of quantum mechanics when these potentials interact with an external magnetic field. This result was first proved by D. Ruelle  Ru  , who showed that if V : T d → R satisfies suitable conditions then there exists exactly one equilibrium measure µV . These assumptions include the fact that V should have superlinear growth at infinity and satisfy a condition known as ``uniform ellipticity  . However, it turns out that many interesting examples do not fall into this category. For example, let us mention the following two examples:  • If V = −|x| 2 + |y| 2 , then V does not grow faster than linearly at infinity.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Boutroux curves with external field : equilibrium measures without a minimization problem . Abstract : We explore the existence and uniqueness of equilibrium states for some categories of potentials in the presence of an external magnetic force , using findings on the asymptotic nature of solutions to Schrödinger coefficients given by J . Bourgain and M . Goldstein .We consider potentials which are bounded below but not necessarily convex or even continuous ; we also enable the external magnetic fields to be time - dependent ( but still periodic ) . In particular , our findings apply to the case where the potential is given by the sum of a quadratic form plus a small perturbation .The main technique applied here is the so - called ` ` Bourgain - Goldstein method , which consists in examining the evolution of the Wigner transform of the solution to the Schrödinger equation under the action of the unitary group produced by the associated Hamiltonian . Introduction The purpose of this article is to prove that there exists at most one equilibrium state associated to specified classes of potentials in the context of quantum mechanics when these potentials interact with an external magnetic force .This result was first proved by D . Ruelle Ru , who demonstrated that if V : T d → R satisfies suitable conditions then there exists precisely one equilibrium measure µV . These assumptions involve the fact that V should have superlinear growth at infinity and meet a condition called as ` ` uniform ellipticity .However , it turns out that several interesting instances do not drop into this category . For instance , let us note the following two examples : • If V = − | x | 2 + | y | 2 , then V does not grow faster than linearly at infinity .",
        "rewrite_text": "**Title:** Boutroux Curves with External Field: Equilibrium Measures Without a Minimization Problem\n\n**Abstract:** This study investigates the existence and uniqueness of equilibrium states for specific classes of potentials influenced by an external magnetic field, drawing on the asymptotic properties of solutions to Schrödinger coefficients as established by J. Bourgain and M. Goldstein. We focus on potentials that are bounded below, which may not necessarily exhibit convexity or continuity, and we allow for time-dependent (yet periodic) external magnetic fields. Notably, our results extend to scenarios where the potential is represented as the sum of a quadratic form and a minor perturbation. The primary methodology employed in this research is the \"Bourgain-Goldstein method,\" which involves analyzing the evolution of the Wigner transform of the solution to the Schrödinger equation as it is influenced by the unitary group generated by the corresponding Hamiltonian. \n\nThe objective of this article is to establish that there exists at most one equilibrium state linked to designated classes of potentials within the framework of quantum mechanics when these potentials are subject to an external magnetic force. This assertion builds upon the foundational work of D. Ruelle, who demonstrated that under certain conditions, if the potential V: T^d → R satisfies specific criteria, there exists a unique equilibrium measure µV. These criteria include the requirement for V to exhibit superlinear growth at infinity and to fulfill a condition known as \"uniform ellipticity.\" However, we identify several intriguing cases that do not conform to these stipulations. For example, consider the potential V = -|x|^2 + |y|^2, which does not exhibit faster-than-linear growth at infinity. This exploration opens avenues for further research into the behavior of equilibrium states under less restrictive conditions.",
        "ori-fast-z-score": -0.18569533817705186,
        "water-fast-z-score": 4.85850203424952,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Variation of the Amati Relation with the Cosmological Redshift: a Selection Effect or an Evolution Effect? .\nAbstract:\nWe have studied the variation of the Amati relation (Amati et al., 2002) in a sample of about 100 GRBs detected by Swift satellite and we found that it is not constant, but varies as a function of redshift z. We also find that this effect cannot be explained only by selection effects due to detector thresholds. \n \n The observed evolution can be described by a power law dependence on cosmological time t~(1+z)3.5. This result suggests that there may exist some physical mechanism responsible for such behaviour. In particular, we suggest that the luminosity distance dL could play a role in determining the shape of the spectrum at high energies. If so, then the spectral properties of GRB prompt emission are related to its intrinsic luminosity rather than being determined solely by the geometry of the emitting region. \nThe results presented here support previous claims that the Amati relation should be used cautiously when applied to distant bursts.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Variation of the Amati Relation with the Cosmological Redshift : a Selection Effect or an Evolution Effect ? .Abstract : We have researched the variation of the Amati relation ( Amati et al . , 2002 ) in a sample of about 100 GRBs detected by Swift satellite and we concluded that it is not continuous , but varies as a function of redshift z . We additionally find that this effect cannot be described only by selection changes due to detector thresholds .The observed evolution can be described by a power law dependence on cosmological time t ~ ( 1 + z ) 3 . 5 . This result suggests that there may exist some physical process responsible for such activity .In particular , we note that the luminosity distance dL may play a role in shaping the form of the spectrum at high energies . If so , then the spectral properties of GRB prompt emission are related to its intrinsic luminosity instead than being specified exclusively by the topology of the emitting areas .The results presented here support previous statements that the Amati relation should be used cautiously when applied to distant bursts .",
        "rewrite_text": "Title: Variation of the Amati Relation with Cosmological Redshift: A Selection Effect or an Evolution Effect?\n\nAbstract: In this study, we investigate the variation of the Amati relation, originally proposed by Amati et al. (2002), utilizing a dataset of approximately 100 Gamma-Ray Bursts (GRBs) detected by the Swift satellite. Our analysis reveals that the Amati relation does not remain constant but instead exhibits a dependence on the cosmological redshift (z). Importantly, we demonstrate that this observed variation cannot be solely attributed to selection effects arising from the thresholds of the detection instruments. Instead, we propose that the evolution of the Amati relation can be characterized by a power law that correlates with cosmological time, specifically t ~ (1 + z)^(3.5). This finding implies the existence of underlying physical mechanisms that influence the behavior of the Amati relation across different redshifts.\n\nFurthermore, we suggest that the luminosity distance (dL) may significantly impact the spectral characteristics of GRB prompt emissions at high energies. If this hypothesis holds true, it indicates that the spectral properties are intrinsically linked to the luminosity of the GRBs rather than being determined solely by the geometry of the emitting regions. Our results underscore the necessity for caution when applying the Amati relation to distant GRBs, as the relationship may not be as straightforward as previously assumed. This research contributes to the ongoing discourse regarding the interpretation of GRB data and emphasizes the importance of considering cosmological effects when analyzing the properties of these extraordinary astronomical events.",
        "ori-fast-z-score": 0.5698028822981898,
        "water-fast-z-score": 4.98201895023622,
        "rewrite-fast-z-score": 0.19802950859533489
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Low-dimensional Supersymmetric Lattice Models .\nAbstract:\nThe low energy effective theories for superstrings are supergravity and supersymmetric gauge theories in four dimensions, which can be obtained by compactifying the extra six spatial dimensions on a Calabi-Yau manifold.  In this talk I will discuss some recent results about lattice models that provide an alternative approach to studying these theories. The basic idea is to use Monte Carlo simulations to study supersymmetric field theories defined on a finite number of points (the sites) of a regular d-dimensional hypercubic lattice with periodic boundary conditions. These models have been studied extensively over the past few years using numerical techniques such as exact diagonalization, quantum Monte Carlo methods, and density matrix renormalization group algorithms. Recently we developed new Monte Carlo simulation techniques based on the worm algorithm that allow us to simulate large systems at very high temperatures where conventional Monte Carlo methods fail because they suffer from critical slowing down. We used our new method to calculate the free energies of several different supersymmetric lattice models including the N = 4 supersymmetric Yang-Mills theory and the N = 1 supersymmetric U(1) gauge theory coupled to matter fields in various representations.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Low - dimensional Supersymmetric Lattice Models . Abstract : The lowest energy effective models for superstrings are supergravity and supersymmetric gauge theories in four dimensions , which can be obtained by compactifying the extra six spatial dimensions on a Calabi - Yau manifold .In this talk I will explore some latest findings about lattice models that provide an different approach to investigating these theories . The basic idea is to use Monte Carlo simulations to study supersymmetric field theories constructed on a finite number of points ( the sites ) of a regular d - dimensional hypercubic crystal with periodic border conditions .These methods have been studied thoroughly over the previous few years employing mathematical techniques such as approximate diagonalization , quantum Monte Carlo methods , and density matrix renormalization group schemes . Recently we developed novel Monte Carlo simulation algorithms based on the worm algorithm that enable us to simulate large systems at very high altitudes where conventional Monte Carlo methods fail because they suffer from critical speed down .We utilized our new method to estimate the free energies of several different supersymmetric lattice models including the N = 4 supersymmetric Yang - Mills theory and the N = 1 supersymmetric U ( 1 ) gauge theory connected to matter fields in different representations .",
        "rewrite_text": "Title: Low-Dimensional Supersymmetric Lattice Models\n\nAbstract: In the realm of theoretical physics, the effective low-energy models for superstring theory predominantly consist of supergravity and supersymmetric gauge theories formulated in four dimensions. These models emerge through the compactification of six additional spatial dimensions on a Calabi-Yau manifold. This presentation delves into recent advancements in lattice models that offer an alternative methodology for probing these complex theories. The core concept revolves around employing Monte Carlo simulations to investigate supersymmetric field theories that are defined on a finite lattice structure, specifically on the sites of a regular d-dimensional hypercubic crystal, while adhering to periodic boundary conditions. Over the past few years, these simulation techniques have been rigorously examined, utilizing various mathematical approaches such as approximate diagonalization, quantum Monte Carlo methods, and density matrix renormalization group techniques. A significant breakthrough has been achieved with the development of innovative Monte Carlo simulation algorithms based on the worm algorithm. This advancement allows for the simulation of large systems at elevated altitudes, overcoming the limitations faced by traditional Monte Carlo methods, which often experience critical slowdown. Our newly developed approach has been successfully applied to estimate the free energies of several supersymmetric lattice models, including the N = 4 supersymmetric Yang-Mills theory and the N = 1 supersymmetric U(1) gauge theory, which incorporates matter fields in diverse representations. These findings not only enhance our understanding of supersymmetric theories but also pave the way for further exploration of their implications in high-energy physics.",
        "ori-fast-z-score": 0.19802950859533489,
        "water-fast-z-score": 5.81344274116933,
        "rewrite-fast-z-score": 0.5477225575051661
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Multiscale model of electronic behavior and localization in stretched dry DNA .\nAbstract:\nWe present an atomistic multiscale approach to the study of charge transport through stretched single-stranded DNA (ssDNA). The method combines molecular dynamics simulations with quantum mechanical calculations on small fragments, which are used as input for a tight-binding description of larger systems. We show that this scheme allows us to reproduce experimental results obtained by scanning tunneling microscopy experiments performed at room temperature. In particular we find that our calculated conductance agrees well with experiment when using realistic values for the hopping parameters between neighboring base pairs. Our analysis shows that the main contribution to the current is due to electrons localized along the backbone chain. These findings suggest that ssDNA can be considered as a promising material for future applications such as nanoelectronic devices or sensors. \n \n Introduction \n \n Single stranded DNA has been studied extensively over many years both experimentally  1 - 3  and theoretically  4 - 6  . It was found that its structure depends strongly on environmental conditions like pH value  7  , ionic strength  8  -  10  , solvent  11  , temperature  12  , stretching  13  , etc.. This makes it possible to use ssDNA as a sensor  14  -  16  or even as a nanomaterial  17  -  19  . For example, recent studies have shown that ssDNA can form stable helical structures  20  -  22  . Furthermore, it was demonstrated that ssDNA can act as a template for protein synthesis  23  .\n \nIn addition to these structural properties there is growing interest in understanding how charge carriers move through ssDNA  24  -  26  . Recent theoretical investigations showed that electron transfer rates depend sensitively on the conformation of the molecule  27  -  29  . Experimentally, it was observed that the conductivity decreases exponentially with increasing length  30  -  32  . However, the exact mechanism behind this effect remains unclear  33  . \n \n Here we propose a new computational scheme combining classical molecular dynamics (MD) simulations  34  with density functional theory (DFT)  35  based quantum chemical calculations  36    Fig. 1(a)  . Using this approach we calculate the transmission function T(E), i.e., the probability amplitude for an electron injected into one end of the system to reach the other end",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Multiscale method of electronic activity and localization in stretched dry DNA . Abstract : We present an atomistic multiscale approach to the study of charge flow through stretched single - stranded DNA ( ssDNA ) .The method mixes molecular mechanics simulations with quantum mechanical calculations on small fragments , which are applied as input for a tight - binding model of bigger systems . We see that this scheme allows us to capture empirical results derived by scanning tunneling microscopy experiments conducted at room temperature .In particular we find that our measured conductance agrees well with experiment when using reasonable values for the hopping characteristics between neighboring base pairs . Our study shows that the main contribution to the current is due to electrons concentrated along the backbone network .These studies imply that ssDNA can be regarded as a viable material for future applications such as nanoelectronic sensors or sensors . Introduction Single stranded DNA has been studied extensively over numerous years both experimentally 1 - 3 and theoretically 4 - 6 .It was shown that its composition depends strongly on environmental conditions like pH value 7 , ionic intensity 8 - 10 , solvent 11 , temperature 12 , stretching 13 , etc . . This gives it able to use ssDNA as a sensor 14 - 16 or even as a nanomaterial 17 - 19 .For instance , recent studies have shown that ssDNA can form stable helical structures 20 - 22 . Furthermore , it was demonstrated that ssDNA can work as a template for protein synthesis 23 .In addition to these functional characteristics there is growing interest in understanding how charge carriers movement through ssDNA 24 - 26 . Recent conceptual investigations indicated that electron transfer rates depend sensitively on the conformation of the molecule 27 - 29 .Experimentally , it was seen that the conductivity decreases exponentially with increasing duration 30 - 32 . However , the exact mechanism behind this effect remains unsure 33 .Here we propose a new computational scheme incorporating classical molecular mechanics ( MD ) simulations 34 with density functional theory ( DFT ) 35 based quantum chemical calculations 36 Fig . 1 ( a ) .Using this methods we determine the transmission function T ( E ) , i . e . , the probability probability for an electron pumped into one end of the system to reach the other end",
        "rewrite_text": "**Title:** Multiscale Method of Electronic Activity and Localization in Stretched Dry DNA\n\n**Abstract:** This study introduces an innovative atomistic multiscale methodology to investigate charge transport in stretched single-stranded DNA (ssDNA). By integrating molecular mechanics simulations with quantum mechanical calculations on smaller DNA fragments, we develop a tight-binding model applicable to larger systems. This approach successfully aligns with empirical findings from scanning tunneling microscopy experiments conducted at room temperature, demonstrating that our calculated conductance values closely match experimental data when realistic hopping parameters between adjacent base pairs are employed. Our analysis reveals that the primary contribution to the electrical current arises from electrons localized along the DNA backbone, suggesting that ssDNA holds promise as a material for future nanoelectronic applications, including sensors.\n\nThe significance of ssDNA has been recognized in both experimental and theoretical contexts over the years. Its properties are notably influenced by various environmental factors such as pH, ionic strength, solvent composition, temperature, and mechanical stretching. These characteristics position ssDNA as a potential candidate for sensor technology and nanomaterial applications. Recent research has highlighted the ability of ssDNA to form stable helical structures and to serve as a template for protein synthesis. Concurrently, there is an increasing interest in elucidating the mechanisms governing charge carrier dynamics within ssDNA. Prior studies suggest that electron transfer rates are highly sensitive to the molecular conformation, and experimental observations indicate an exponential decrease in conductivity with prolonged duration. However, the underlying mechanisms driving this phenomenon remain unclear.\n\nIn this work, we propose a novel computational framework that combines classical molecular dynamics simulations with density functional theory-based quantum chemical calculations. This integrated approach enables us to compute the transmission function, T(E), which quantifies the likelihood of an electron injected at one end of the ssDNA reaching the opposite end. Our findings contribute to a deeper understanding of charge transport in ssDNA and underscore its potential utility in the development of advanced electronic devices.",
        "ori-fast-z-score": -0.22941573387056177,
        "water-fast-z-score": 7.222716252062202,
        "rewrite-fast-z-score": 1.687322975464215
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Propagation of travelling waves in sub-excitable systems driven by noise and periodic forcing .\nAbstract:\nWe study the propagation of traveling waves (TWs) in excitable media with spatially distributed parameters, which are subject to both external forcing and internal fluctuations. We show that TWs can be generated spontaneously even if there is no deterministic source for them. The mechanism responsible for this phenomenon is related to the presence of an unstable stationary state between two stable ones. In particular, we demonstrate how spontaneous generation of TWs occurs due to stochastic resonance induced by additive white Gaussian noise. Finally, we present numerical results illustrating the effect of multiplicative colored noise on the dynamics of TWs. Propagation of traveling waves (TW) in excitable media has been studied extensively over recent years  1  . It was shown that TWs may appear as a result of various mechanisms such as: i) intrinsic instabilities  2  , ii) coupling-induced instabilities  3  or iii) forced oscillations  4  .\nIn many cases it is assumed that the medium under consideration is homogeneous so that all its properties do not depend explicitly on space coordinates. However, real physical systems usually have spatial variations of their characteristics  5  . For example, one-dimensional models describing cardiac tissue  6  include heterogeneity in the form of local changes in refractory periods  7, 8  . Another important factor influencing wave propagation is noise  9  . Noise plays different roles depending on whether it acts additively  10  or multiplicatively  11  . Moreover, noise may also affect the shape of the propagating front  12  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Propagation of travelling waves in sub - excitable environments driven by noise and periodic forcing . Abstract : We research the propagation of traveling signals ( TWs ) in excitable media with spatially scattered characteristics , which are subject to both external forcing and internal fluctuations .We see that TWs can be formed spontaneously even if there is no deterministic source for them . The pathway responsible for this phenomenon is related to the presence of an weak stationary state between two stable ones .In particular , we prove how spontaneous production of TWs occurs due to stochastic resonance caused by additive white Gaussian noise . Finally , we present numerical findings illustrating the impact of multiplicative colored noise on the dynamics of TWs .Propagation of traveling signals ( TW ) in excitable media has been studied extensively over recent years 1 . It was shown that TWs might appear as a outcome of several mechanisms such as : i ) inherent instabilities 2 , ii ) coupling - caused instabilities 3 or iii ) forced oscillations 4 .In many situations it is expected that the medium under consideration is homogeneous so that all its properties do not depend explicitly on space coordinates . However , real physical structures typically have spatial variations of their characteristics 5 .For instance , one - dimensional experiments describing cardiac tissue 6 include heterogeneity in the form of local changes in refractory intervals 7 , 8 . Another important factor influencing wave propagation is sound 9 .Noise takes multiple roles depending on whether it functions additively 10 or multiplicatively 11 . Moreover , noise might additionally affect the morphology of the propagating front 12 .",
        "rewrite_text": "**Title:** Propagation of Traveling Waves in Sub-Excitable Environments Driven by Noise and Periodic Forcing\n\n**Abstract:** This study investigates the dynamics of traveling waves (TWs) in excitable media characterized by spatial heterogeneity and influenced by both external periodic forcing and internal stochastic fluctuations. Our findings reveal that TWs can emerge spontaneously in the absence of a deterministic source, a phenomenon attributed to the existence of a weak stationary state situated between two stable states. We demonstrate that the spontaneous generation of TWs is facilitated by stochastic resonance, which is induced by the presence of additive white Gaussian noise. Additionally, we present numerical simulations that highlight the effects of multiplicative colored noise on the behavior of TWs. The propagation of TWs in excitable media has garnered significant attention in recent years, with previous research identifying various mechanisms that can lead to their emergence, including inherent instabilities, coupling-induced instabilities, and forced oscillations. While many theoretical models assume a homogeneous medium where properties are uniform across space, real-world systems often exhibit spatial variability. For example, one-dimensional studies of cardiac tissue reveal heterogeneities manifested as local variations in refractory periods. Furthermore, noise plays a complex role in wave propagation, acting either additively or multiplicatively, and can also influence the morphology of the traveling wave front. This research contributes to the understanding of how noise and periodic forcing interact in sub-excitable environments, providing insights into the conditions under which TWs can arise and their subsequent dynamics. Our results have implications for various fields, including biology and physics, where the understanding of wave propagation in heterogeneous media is crucial.",
        "ori-fast-z-score": 0.3621429841700741,
        "water-fast-z-score": 7.123190113872715,
        "rewrite-fast-z-score": 2.750805179189953
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Search for gravitational-wave bursts in LIGO data from the fourth science run .\nAbstract:\nWe present results on searches for gravitational wave (GW) burst signals using data collected by the Laser Interferometer Gravitational-Wave Observatory during its fourth science run, which took place between September 2005 and January 2007. We use two different search methods to look for GW bursts: one based on matched filtering with template waveforms and another that uses an optimal filterbank method. The latter is used as part of a blind analysis where we do not know what type or strength of signal may be present in our data until after it has been analyzed. In addition to these analyses, we also perform several consistency checks designed to identify any problems associated with either detector s performance over this period. No significant candidates are found in any of these searches. Using simulated signals injected into the data at random times, we estimate upper limits on the rate density of binary black hole mergers detectable within a given range of total mass.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Search for gravitational - wave pulses in LIGO data from the fourth science run . Abstract : We report findings on investigations for gravitational wave ( GW ) burst signals using data received by the Laser Interferometer Gravitational - Wave Observatory during its fourth science run , which taken place between September 2005 and January 2007 .We use two different search methods to find for GW bursts : one based on matched sampling with template waveforms and another that using an efficient filterbank method . The latter is utilized as part of a blind analysis where we do not understand what type or strength of signal might be found in our information until after it has been examined .In addition to these tests , we also perform several reliability measures designed to identify any problems related with either detector s performance over this time . No important candidates are found in any of these searches .Using simulated transmissions imported into the information at random times , we estimate upper limits on the rate concentration of binary dark hole mergers detectable within a given range of total mass .",
        "rewrite_text": "In this article, we present our findings from the search for gravitational wave (GW) burst signals using data collected by the Laser Interferometer Gravitational-Wave Observatory (LIGO) during its fourth science run, which spanned from September 2005 to January 2007. Our investigation employs two distinct methodologies to detect GW bursts: the first method utilizes matched filtering with template waveforms, while the second method incorporates an efficient filterbank approach. The filterbank technique is particularly noteworthy as it is part of a blind analysis, meaning that we do not have prior knowledge of the types or strengths of signals that may be present in the data until after the analysis is conducted.\n\nTo ensure the robustness of our findings, we also implement several reliability checks aimed at identifying any potential issues related to the performance of the detectors during the observation period. Despite our comprehensive search efforts, we did not identify any significant candidates for gravitational wave bursts in the data analyzed. Furthermore, we conducted simulations by injecting synthetic signals into the data at random intervals, which allowed us to estimate upper limits on the event rate of detectable binary black hole mergers within a specified range of total mass. These results contribute to our understanding of the capabilities of LIGO in detecting gravitational waves and provide valuable insights into the population of binary black hole mergers in the universe. Overall, our study underscores the importance of rigorous analysis techniques in the ongoing quest to uncover the mysteries of gravitational waves and their sources.",
        "ori-fast-z-score": -1.4596008983995234,
        "water-fast-z-score": 4.538253483538691,
        "rewrite-fast-z-score": 0.7302967433402214
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A soft X-ray study of Type I AGN observed with Chandra HETGS .\nAbstract:\nWe present the first detailed analysis of the X-ray spectrum of the Seyfert 1 galaxy NGC 3783 using data obtained by the High Energy Transmission Grating Spectrometer (HETGS) on board the Chandra Observatory. The source was in an active state during our observation, and we find that its emission is dominated by strong absorption lines due to highly ionized species such as O viii-xxii, Ne ix-xiii, Mg xii-xv, Si xiv-xxvi, S xix-xxxi, Ar xxviii-xxxviii, Ca xx-xxxi, Fe xxv-xxvi, and Ni xxviii-xxix. We detect several narrow emission features which are likely associated with resonant scattering of continuum photons off ions located along the line-of-sight towards the central engine. In addition, there appears to be evidence for broad emission components at energies above 10 keV.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A soft X - ray study of Type I AGN observed with Chandra HETGS . Abstract : We report the first detailed analysis of the X - ray spectrum of the Seyfert 1 galaxy NGC 3783 employing data acquired by the High Energy Transmission Grating Spectrometer ( HETGS ) on board the Chandra Observatory .The source was in an active condition during our observation , and we find that its emission is dominated by weak absorption lines owing to strongly ionized species such as O viii - xxii , Ne ix - xiii , Mg xii - xv , Si xiv - xxvi , S xix - xxxi , Ar xxviii - xxxviii , Ca xx - xxxi , Fe xxv - xxvi , and Ni xxviii - xxix . We detect many narrow radiation properties which are likely correlated with resonant scattering of continuum photons off ions located along the line - of - view towards the main motor .In addition , there seems to be confirmation for wide radiation elements at energies above 10 keV .",
        "rewrite_text": "We present a comprehensive analysis of the X-ray spectrum of the Seyfert 1 galaxy NGC 3783, utilizing data obtained from the High Energy Transmission Grating Spectrometer (HETGS) aboard the Chandra Observatory. This study marks the first in-depth examination of the X-ray emissions from this active galactic nucleus (AGN) during a period of heightened activity. Our findings reveal that the X-ray emission is primarily characterized by a series of weak absorption lines, which are attributed to highly ionized elements, including O viii to O xxii, Ne ix to Ne xiii, Mg xii to Mg xv, Si xiv to Si xxvi, S xix to S xxxi, Ar xxviii to Ar xxxviii, Ca xx to Ca xxxi, Fe xxv to Fe xxvi, and Ni xxviii to Ni xxix. \n\nMoreover, we observe numerous narrow emission features that are likely associated with the resonant scattering of continuum photons by ions situated along the line of sight to the central engine of the galaxy. These features provide critical insights into the physical conditions and processes occurring in the vicinity of the AGN. Additionally, our analysis suggests the presence of broader emission lines at energies exceeding 10 keV, which may indicate complex interactions within the high-energy environment of NGC 3783. This research enhances our understanding of the X-ray characteristics of Seyfert galaxies and contributes to the broader field of astrophysics by elucidating the mechanisms driving the emission processes in these energetic systems.",
        "ori-fast-z-score": -1.3363062095621219,
        "water-fast-z-score": 4.370956778314644,
        "rewrite-fast-z-score": 0.4216370213557839
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Stern-Volmer Modeling of Steady-State Forster Energy Transfer Between Dilute, Freely Diffusing Membrane-Bound Fluorophores .\nAbstract:\nWe present an analytical solution to the steady-state forster energy transfer between two freely diffusing membrane-bound fluorophores in close proximity (<10 nm). The model is based on the assumption that both donor and acceptor molecules are bound to the same lipid bilayer with their transition dipole moments parallel to each other but perpendicular to the plane of the membrane. We show how this simple geometry can be used to derive a closed-form expression for the fluorescence lifetime of the donor molecule as well as its dependence on the concentration of acceptors. This approach allows us to extract information about the distance distribution between donors and acceptors directly from experimental data without any additional assumptions or fitting parameters. In addition we demonstrate how our results can be applied to study the lateral organization of proteins within biological membranes using single-molecule spectroscopy. Single-molecule FRET experiments have been widely used over recent years to investigate protein-protein interactions at the molecular level1-5 . However, despite significant progress made during last decade6-9 , there still remain several challenges associated with extracting quantitative structural information from such measurements10-13 .\nIn particular, it has recently become apparent that many commonly employed methods for analyzing single-molecule FRET data suffer from systematic errors due to various factors including photophysical properties of fluorescent dyes14-16 , heterogeneity of sample17-19 , presence of multiple species20-22 , etc. . To overcome these difficulties, several groups23-26 have developed sophisticated statistical approaches which allow one to obtain reliable estimates of key physical quantities characterizing the system under investigation by performing global fits to large sets of experimental data27-29 . Unfortunately, however, most of these techniques require extensive computational resources and/or involve complicated numerical procedures making them difficult...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Stern - Volmer Modeling of Steady - State Forster Energy Transfer Between Dilute , Freely Diffusing Membrane - Bound Fluorophores . Abstract : We present an analytical solution to the stable - state forster electricity transfer between two voluntarily diffusing membrane - bound fluorophores in close proximity ( < 10 nm ) .The model is based on the assumption that both donor and acceptor molecules are bound to the same lipid bilayer with their transition dipole moments connected to each other but perpendicular to the plane of the membrane . We see how this simple geometry can be used to derive a closed - form expression for the fluorescence duration of the recipient molecule as well as its dependence on the concentration of acceptors .This method enables us to extract information about the distance distribution between recipients and acceptors immediately from experimental evidence without any additional constraints or fitting factors . In addition we prove how our findings can be applied to study the lateral arrangement of proteins within molecular membranes using single - molecule spectroscopy .Single - cell FRET tests have been widely using over recent months to examine protein - protein relationships at the molecular level1 - 5 . However , despite considerable progress made during ago decade6 - 9 , there still continue several challenges associated with extracting statistical structural information from such measurements10 - 13 .In particular , it has recently become evident that several often employed methods for searching single - cell FRET results suffer from widespread errors attributed to several variables including photophysical properties of fluorescent dyes14 - 16 , heterogeneity of sample17 - 19 , presence of multiple species20 - 22 , etc . .To solve these problems , various groups23 - 26 have developed elaborate empirical methods which allow one to obtain reliable estimates of key physical quantities characterizing the system under inquiry by performing global fits to large sets of experimental data27 - 29 . Unfortunately , however , most of these method require extensive computational resources and / or involve difficult numerical operations making them complicated . . .",
        "rewrite_text": "We provide an analytical framework for understanding steady-state Förster resonance energy transfer (FRET) between two freely diffusing, membrane-bound fluorophores that are in close proximity (less than 10 nm). Our model operates under the premise that both the donor and acceptor molecules are anchored to the same lipid bilayer, with their transition dipole moments oriented perpendicularly to the membrane plane and aligned with one another. This geometric configuration allows us to derive a closed-form expression for the fluorescence lifetime of the acceptor molecule, revealing its dependence on the concentration of acceptors. This approach facilitates the direct extraction of distance distributions between donor and acceptor molecules from experimental data, eliminating the need for additional constraints or fitting parameters.\n\nFurthermore, we demonstrate the applicability of our findings in investigating the lateral organization of proteins within cellular membranes through single-molecule spectroscopy techniques. Recent advancements in single-cell FRET experiments have significantly enhanced our ability to probe protein-protein interactions at the molecular level. Despite the progress made over the past decade, challenges remain in accurately extracting statistical structural information from these measurements. Notably, it has become apparent that many commonly used methods for analyzing single-cell FRET data are plagued by systematic errors arising from various factors, including the photophysical properties of fluorescent dyes, sample heterogeneity, and the presence of multiple species.\n\nTo address these issues, several research groups have developed sophisticated empirical techniques aimed at providing reliable estimates of critical physical parameters by performing global fits to extensive datasets. However, these methods often demand substantial computational resources and involve complex numerical operations, which can complicate their implementation. Our work seeks to simplify the analysis of FRET data, providing a more accessible and efficient means of extracting meaningful insights into molecular interactions and arrangements within biological membranes.",
        "ori-fast-z-score": -0.32025630761017426,
        "water-fast-z-score": 7.951871356134738,
        "rewrite-fast-z-score": -0.39904344223381105
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  TrES-3: A Nearby, Massive, Transiting Hot Jupiter in a 31-Hour Orbit .\nAbstract:\nWe report the discovery and characterization of TrES-3b, an extrasolar planet with mass M = 1.3 MJup orbiting its host star every 31 hours at 0.081 AU (1.7 stellar radii). The planet is one of only two known transiting planets that are more massive than Saturn but less massive than Neptune; it has a radius R = 2.2 RJup and effective temperature T eff = 2400 K. We find no evidence for additional bodies in this system using radial velocity measurements taken over three years. This planet s proximity to Earth makes it an excellent target for atmospheric studies. \n \n Keywords: Extrasolar planet - Discovery - Transit photometry - Radial velocities - Atmosphere - Planetary systems \n \n \n \n Introduction \n \n In recent years there have been several discoveries of giant exoplanets with orbital periods shorter than four days. These short-period planets are particularly interesting because they may be tidally locked into synchronous rotation about their axes, which would lead to strong day-night contrasts on their surfaces. Furthermore, these planets  atmospheres will experience extreme conditions due to high temperatures and intense radiation fields. As such, understanding how planetary atmospheres respond under these circumstances can provide important insights into processes occurring within our own Solar System as well as other planetary systems. \n \n Here we present the discovery and initial characterization of TrES-3b; a hot Jupiter with a period P = 3.09 d discovered by the transit method. Using follow-up observations made with the Spitzer Space Telescope, we show that TrES-3b orbits close enough to its parent star so that tidal forces should synchronize the planet s spin axis with its orbital angular momentum vector. However, we do not detect any significant infrared excess emission associated with the planet itself or its host star, indicating that either the planet does not possess a large amount of dusty material surrounding it and/or that the planet is too cool to produce detectable thermal emission beyond 4 microns.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : TrES - 3 : A Nearby , Massive , Transiting Hot Jupiter in a 31 - Hour Orbit . Abstract : We report the discovery and characterization of TrES - 3b , an extrasolar planet with mass M = 1 . 3 MJup orbiting its host star every 31 minutes at 0 . 081 AU ( 1 . 7 stellar radii ) .The planet is one of only two recorded transiting planets that are more massive than Saturn but less massive than Neptune ; it has a diameter R = 2 . 2 RJup and effective heat T eff = 2400 K . We see no evidence for additional body in this system using radial speed measurements taken over three years . This planet s vicinity to Earth makes it an excellent target for atmospheric studies .Keywords : Extrasolar planet - Discovery - Transit photometry - Radial velocities - Atmosphere - Planetary systems Introduction In recent seasons there have been numerous discoveries of giant exoplanets with orbital periods shorter than four days . These short - time planets are particularly exciting because they may be tidally locked into synchronous rotation about their axes , which would result to powerful day - night contrasts on their surfaces .Furthermore , these planets atmospheres will experience harsh conditions due to large temperatures and intense radiation fields . As such , studying how planetary atmospheres behave under these circumstances can provide important perspectives into processes arising within our own Solar System as well as other planetary components .Here we present the discovery and preliminary characterization of TrES - 3b ; a bright Jupiter with a period P = 3 . 09 d discovered by the transit method . Using follow - up observations made with the Spitzer Space Telescope , we prove that TrES - 3b orbits close enough to its father star so that tidal forces should synchronize the planet s spin axis with its orbital angular velocity function .However , we do not detect any considerable infrared excess emission associated with the planet itself or its host star , showing that either the planet does not possess a large number of dusty substance surrounding it and / or that the planet is too cold to produce detectable heat emission beyond 4 microns .",
        "rewrite_text": "We present the discovery and detailed characterization of TrES-3b, a nearby extrasolar planet that exhibits remarkable properties. This massive exoplanet, with a mass of 1.3 times that of Jupiter (MJup), orbits its host star in an exceptionally short period of just 31 hours at a distance of 0.081 AU, which is approximately 1.7 times the radius of the star. TrES-3b is one of only two known transiting planets that are more massive than Saturn yet less massive than Neptune, boasting a radius of 2.2 RJup and an effective temperature of 2400 K. Our extensive radial velocity measurements, conducted over a three-year period, reveal no evidence of additional bodies within this system, suggesting a relatively simple planetary architecture.\n\nThe proximity of TrES-3b to Earth makes it an ideal candidate for atmospheric studies, providing a unique opportunity to investigate the characteristics of its atmosphere under extreme conditions. The recent surge in the discovery of giant exoplanets with orbital periods shorter than four days has heightened interest in these celestial bodies, particularly because they may be tidally locked, leading to stark day-night contrasts on their surfaces. The harsh environmental conditions these planets face, including elevated temperatures and intense radiation, offer valuable insights into atmospheric dynamics that can inform our understanding of similar processes in our Solar System and beyond.\n\nIn this study, we detail the discovery of TrES-3b through the transit method and confirm its orbital characteristics using follow-up observations from the Spitzer Space Telescope. Our findings indicate that the planet's proximity to its host star is sufficient for tidal forces to synchronize its rotational axis with its orbital motion. However, we did not observe any significant infrared excess emission from the planet or its star, suggesting either a lack of substantial dusty material surrounding the planet or that its temperature is too low to emit detectable heat beyond 4 microns. This research contributes to the growing body of knowledge regarding the formation and evolution of planetary systems and the diverse environments of exoplanets.",
        "ori-fast-z-score": -0.827605888602368,
        "water-fast-z-score": 5.225578117937447,
        "rewrite-fast-z-score": -0.39904344223381105
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Probing dark energy with cluster counts and cosmic shear power spectra: including the full covariance .\nAbstract:\nWe present forecasts for cosmological constraints on dark energy parameters using galaxy clusters, weak lensing tomography, and supernovae data in combination with Planck CMB temperature anisotropy measurements. We use an approximate treatment to account for all relevant covariances between these probes. The results are presented as two-dimensional marginalized likelihoods over pairs of dark energy parameters. Forecasted errors on w0 and wa range from 0.01-0.02 at 1-sigma confidence level (CL) when combining all three probes together. This is comparable or better than current state-of-the-art constraints obtained by other authors. In addition we show that our method can be used to obtain joint constraints on additional parameters such as neutrino masses. Finally, we demonstrate how this approach could be extended to include more complicated models beyond simple flat ΛCDM scenarios. We also discuss possible extensions of our analysis which would allow us to improve upon existing constraints further.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Probing heavy energy with cluster counts and cosmic shear speed spectra : covering the full covariance . Abstract : We present forecasts for cosmological limitations on dark energy parameters utilizing galaxy clusters , soft lensing tomography , and supernovae information in combination with Planck CMB heat anisotropy observations .We use an approximate treatment to provide for all relevant covariances between these probes . The results are presented as two - dimensional marginalized likelihoods over pairs of dark energy parameters .Forecasted errors on w0 and wa vary from 0 . 01 - 0 . 02 at 1 - sigma confidence rate ( CL ) when combining all three probes together . This is analogous or improved than current state - of - the - art constraints obtained by other literature .In addition we prove that our technique can be used to obtain joint constraints on additional parameters such as neutrino masses . Finally , we prove how this methodology possible be extended to consider more complicated models beyond straightforward flat ΛCDM scenarios .We also discuss possible extensions of our analysis which would enable us to improve upon existing constraints further .",
        "rewrite_text": "**Title:** Probing Heavy Energy with Cluster Counts and Cosmic Shear Speed Spectra: Covering the Full Covariance\n\n**Abstract:** In this study, we present comprehensive forecasts regarding the cosmological constraints on dark energy parameters by integrating data from galaxy clusters, weak lensing tomography, and supernova observations, alongside Planck's Cosmic Microwave Background (CMB) temperature anisotropy measurements. Our approach employs an approximate treatment that accounts for all significant covariances among these diverse astrophysical probes. The findings are illustrated through two-dimensional marginalized likelihoods that depict the relationships between pairs of dark energy parameters. Our forecasts indicate that the uncertainties on the parameters w0 and wa range from 0.01 to 0.02 at a 1-sigma confidence level when all three observational datasets are combined. These results are comparable to, or even exceed, the current leading constraints reported in the literature. Furthermore, we demonstrate that our methodology is capable of yielding joint constraints on additional parameters, such as neutrino masses. We also explore the potential for extending this analytical framework to accommodate more complex cosmological models that go beyond the standard flat ΛCDM paradigm. Additionally, we discuss avenues for further enhancing our analysis, which could lead to improved constraints on dark energy and other fundamental cosmological parameters. This work underscores the importance of multi-probe approaches in cosmology and highlights the potential for future research to refine our understanding of the universe's expansion dynamics and the nature of dark energy.",
        "ori-fast-z-score": 0.10721125348377948,
        "water-fast-z-score": 5.253351420705195,
        "rewrite-fast-z-score": -0.8955334711889903
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Comment on  Minimal size of a barchan dune  .\nAbstract:\nWe have studied the minimal size of a barchan by means of numerical simulations and found that it is determined by the balance between the sand flux at its crest, which decreases with decreasing height, and the wind shear stress over the slip face, which increases with decreasing height.  We show that this leads to an optimal height for the barchan dunes in agreement with observations. The results are presented below. \nThe authors thank Jens Egeberg Hansen (University of Southern Denmark) for useful discussions. Sand transport occurs when grains are lifted up into the air due to aerodynamic forces exerted by the wind. This process can be described as saltation  1  , where individual particles jump across the surface before being deposited again. Saltating particles transfer momentum to the surrounding fluid through collisions  2  . In turn, these collisions generate turbulence  3  .\nSaltation also causes erosion  4  and deposition  5  . Erosion takes place if the net force acting on a grain is directed away from the ground  6  . Deposition happens if the net force acts towards the ground  7, 8  . These processes lead to the formation of bedforms such as ripples  9  or dunes  10  . Dunes are formed under conditions where the sediment supply exceeds the rate of removal  11  . They occur naturally  12  but they may also form artificially  13  . Barchan dunes are crescent-shaped structures  14  . Their shape has been explained theoretically  15  and observed experimentally  16  . It was shown  17  that their width W scales linearly with length L according to: W = 0.6L + const., while their height H scales as: H ∝ L 1/3 . Here we study how the height of a barchan depends on other parameters using numerical simulations  18  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Comment on Minimal size of a barchan dune . Abstract : We have researched the reduced size of a barchan by means of numerical simulations and found that it is calculated by the balance between the sand density at its crest , which drops with decreasing height , and the wind shear force over the slip face , which increases with increased height .We see that this gives to an appropriate height for the barchan dunes in agreement with observations . The results are presented below .The authors thank Jens Egeberg Hansen ( University of Southern Denmark ) for useful studies . Sand transport happens when grains are lifted up into the air thanks to aerodynamic forces exerted by the wind .This process can be described as saltation 1 , where individual molecules jump across the surface before being dumped again . Saltating molecules transfer momentum to the nearby fluid through collisions 2 .In turn , these collisions produce turbulence 3 . Saltation also produces erosion 4 and deposition 5 .Erosion takes happen if the net force acting on a grain is directed away from the ground 6 . Deposition occurs if the net force works towards the ground 7 , 8 .These mechanisms lead to the formation of bedforms such as ripples 9 or dunes 10 . Dunes are created under environments where the sand supply exceeds the rate of reduction 11 .They happen naturally 12 but they may also formation artificially 13 . Barchan dunes are crescent - shaped structures 14 .Their shape has been explained theoretically 15 and seen experimentally 16 . It was shown 17 that their shape W scales linearly with width L due to : W = 0 . 6L + const . , while their height H scales as : H [UNK] L 1 / 3 .Here we study how the height of a barchan depends on other parameters utilizing numerical simulations 18 .",
        "rewrite_text": "**Title:** Commentary on the Minimum Size of a Barchan Dune\n\n**Abstract:** This study investigates the minimum size of barchan dunes through numerical simulations, focusing on the interplay between sand density at the dune's crest and the wind shear force acting on the slip face. Our findings reveal that as the height of the dune decreases, the sand density at the crest also diminishes, while the wind shear force increases with height. This relationship establishes a critical height for barchan dunes that aligns with empirical observations. The detailed results of our simulations are discussed in the following sections. We extend our gratitude to Jens Egeberg Hansen from the University of Southern Denmark for his insightful contributions to this research.\n\nThe process of sand transport is primarily driven by aerodynamic forces exerted by the wind, which lift sand grains into the air. This phenomenon, known as saltation, involves individual grains jumping across the surface before settling back down. During these jumps, saltating grains transfer momentum to the surrounding air through collisions, which in turn generates turbulence. This turbulence is a key factor in both erosion and deposition processes. Erosion occurs when the net force acting on a grain is directed away from the ground, while deposition takes place when the net force is directed towards the ground. These dynamic mechanisms contribute to the formation of various bedforms, including ripples and dunes.\n\nDunes typically form in environments where the supply of sand exceeds the rate of its removal. While they can occur naturally, they may also be artificially created. Barchan dunes, characterized by their crescent shape, have been the subject of both theoretical and experimental studies. Previous research has established that the width (W) of barchan dunes scales linearly with their length (L), expressed as W = 0.6L + constant, while their height (H) scales with L as H ∝ L^(1/3). In this paper, we delve deeper into how the height of barchan dunes is influenced by various parameters, utilizing advanced numerical simulations to enhance our understanding of these fascinating geological formations.",
        "ori-fast-z-score": -1.2888044650576527,
        "water-fast-z-score": 5.8423739467217715,
        "rewrite-fast-z-score": 2.242227956050979
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  How does Casimir energy fall? II. Gravitational acceleration of quantum vacuum energy .\nAbstract:\nWe consider the gravitational effect on the Casimir force between two parallel plates in an accelerating frame, and show that it is possible to obtain a repulsive force at short distances even if the initial state has no net charge or current density. We also discuss how this result can be obtained by using the concept of local energy-momentum tensor for electromagnetic fields. The possibility of obtaining such a repulsive force was first suggested by Yukawa in 1951 as a way to explain nuclear forces without introducing new particles. \n \n In recent years there have been several attempts to measure experimentally the Casimir force (see e.g., Refs.  1 - 6 ). However, all these experiments are performed with macroscopic objects which do not allow one to test directly the predictions of QED concerning the behavior of the vacuum fluctuations near boundaries  7, 8  . \n \n On the other hand, the measurement of the Casimir force between microscopic bodies would provide information about the properties of the vacuum  9  , and could lead to tests of QED  10  -  12  . \n \n It should be noted that the experimental determination of the Casimir force requires very accurate measurements because its magnitude is extremely small compared to gravity  13  . For example, the most precise experiment so far  6  gives only a relative accuracy of 0.1%. This means that any systematic error will dominate over statistical errors. Therefore, it is important to understand theoretically the dependence of the Casimir force upon external parameters like temperature  14  , pressure  15  , and velocity  16  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : How does Casimir energy fall ? II .Gravitational acceleration of quantum vacuum energy . Abstract : We consider the gravitational impact on the Casimir force between two connected plates in an accelerating frame , and find that it is easy to obtain a repulsive force at short distances even if the first state has no net charge or current density .We additionally discuss how this consequence can be obtained by using the idea of local power - momentum tensor for electromagnetic fields . The possibility of finding such a repulsive force was first suggested by Yukawa in 1951 as a way to explain nuclear forces without introducing additional particles .In recent years there have been numerous attempts to measure experimentally the Casimir force ( saw e . g . , Refs . 1 - 6 ) .However , all these experiments are performed with macroscopic objects which do not enable one to test explicitly the assumptions of QED concerning the dynamics of the vacuum fluctuations near edges 7 , 8 . On the other hand , the observation of the Casimir force between microscopic bodies would offer information about the properties of the vacuum 9 , and could lead to tests of QED 10 - 12 .It should be mentioned that the empirical determination of the Casimir force requires very accurate measurements because its magnitude is incredibly small contrast to gravity 13 . For instance , the most accurate experiment so far 6 offers only a relative sensitivity of 0 . 1 % .This implies that any systematic error will dominate over empirical errors . Therefore , it is important to realize theoretically the dependence of the Casimir force upon external parameters like pressure 14 , pressure 15 , and speed 16 .",
        "rewrite_text": "**Title:** How Does Casimir Energy Fall? II. Gravitational Acceleration of Quantum Vacuum Energy\n\n**Abstract:** This study investigates the effects of gravity on the Casimir force acting between two parallel plates within an accelerating reference frame. We demonstrate that a repulsive force can be generated at short distances, even when the initial state exhibits no net charge or current density. This phenomenon can be understood through the framework of the local energy-momentum tensor associated with electromagnetic fields. The concept of a repulsive force in this context was initially proposed by Yukawa in 1951 as a potential explanation for nuclear forces without the need for additional particles. In recent years, there has been a surge of experimental efforts aimed at measuring the Casimir force, as referenced in studies 1 through 6. However, these experiments typically involve macroscopic objects, which limit the ability to directly test the predictions of quantum electrodynamics (QED) regarding vacuum fluctuations near material boundaries, as noted in references 7 and 8. Conversely, observing the Casimir force between microscopic entities could yield valuable insights into vacuum properties and facilitate more stringent tests of QED, as discussed in references 9 and 10-12. It is crucial to highlight that accurately determining the Casimir force presents significant challenges due to its exceedingly small magnitude compared to gravitational forces, as indicated in reference 13. For example, the most precise experiment conducted to date, referenced as 6, achieves only a relative sensitivity of 0.1%. This level of precision suggests that systematic errors may overshadow empirical uncertainties. Consequently, it is essential to theoretically explore how the Casimir force depends on various external parameters, including pressure and velocity, as mentioned in references 14, 15, and 16.",
        "ori-fast-z-score": -0.4472135954999579,
        "water-fast-z-score": 6.350433056099402,
        "rewrite-fast-z-score": 0.17677669529663687
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A density tensor hierarchy for open system dynamics: retrieving the noise .\nAbstract:\nWe present an exact hierarchical description of the reduced state and dynamics of open quantum systems in terms of a set of coupled equations, which we call the Hierarchy of Density Tensors (HDT). The HDT is derived by applying the Nakajima-Zwanzig projection operator technique to the von Neumann equation describing the evolution of the total system. We show that this approach allows one to retrieve all relevant information about the environment-induced decoherence process on arbitrary timescales. In particular, it provides access to the full spectrum of relaxation rates characterizing the decay of off-diagonal elements of the reduced density matrix as well as the stationary states reached at late times. As an example, we apply our formalism to study the dissipative spin-boson model with Ohmic dissipation. Our results are compared against numerical simulations based on the Quantum Monte Carlo Wavefunction method. \nI. INTRODUCTORY REMARK\nThe understanding of how macroscopic objects behave under the influence of their environments has been a central issue in physics since its very beginning  1, 2  . This problem becomes particularly challenging when dealing with complex many-body systems such as condensed matter or biological ones  3, 4  , where the number of degrees of freedom involved can be extremely large. A powerful theoretical tool to tackle these problems consists in studying the dynamics of the reduced state of the system of interest S conditioned upon some specific measurement performed over the environmental degrees of freedom E  5, 6  .\nIn recent years there have been several attempts to develop efficient methods to describe the time-evolution of the reduced state  7, 8  . Among them, the so-called Hierarchy of Density Matrices (HDM)  9  represents a promising alternative to other approaches  10, 11  due to its ability to capture non-Markovian effects  12  . However, despite being able to provide accurate predictions for short-time evolutions  13  , the HDM fails to reproduce correctly the asymptotic behavior of the system  14  . To overcome this limitation, here we introduce a new formulation of the HDM, called Hierarchy of Density...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A density tensor hierarchy for open network dynamics : retrieving the noise . Abstract : We create an precise hierarchical description of the reduced state and dynamics of open quantum systems in terms of a setting of coupled equations , which we call the Hierarchy of Density Tensors ( HDT ) .The HDT is calculated by using the Nakajima - Zwanzig projection operator technique to the von Neumann equation explaining the evolution of the total system . We see that this methodology allows one to locate all relevant information about the environment - caused decoherence cycle on arbitrary timescales .In particular , it gives access to the full range of relaxation rates characterizing the decay of off - horizontal elements of the reduced density matrix as well as the stationary states reached at late times . As an instance , we apply our formalism to study the dissipative spin - boson theory with Ohmic dissipation .Our results are compared against numerical simulations based on the Quantum Monte Carlo Wavefunction method . I .INTRODUCTORY REMARK The knowledge of how macroscopic objects react under the impact of their environments has been a central topic in science since its very beginning 1 , 2 . This problem arises increasingly challenging when dealing with difficult large - bodies systems such as condensed matter or biological ones 3 , 4 , where the number of degrees of freedom employed can be extremely huge .A popular conceptual technique to tackle these problems involves in examining the dynamics of the reduced state of the system of interest S conditioned upon some specific assessment performed over the environmental degrees of liberty E 5 , 6 . In recent years there have been numerous attempts to develop able methods to explain the period - progression of the reduced state 7 , 8 .Among them , the so - called Hierarchy of Density Matrices ( HDM ) 9 offers a promising alternative to other methods 10 , 11 due to its able to capture non - Markovian influences 12 . However , despite being able to provide accurate forecast for short - time evolutions 13 , the HDM fails to reproduce correctly the asymptotic behavior of the system 14 .To solve this limitation , here we incorporate a new implementation of the HDM , entitled Hierarchy of Density . . .",
        "rewrite_text": "**Title: A Density Tensor Hierarchy for Open Network Dynamics: Retrieving the Noise**\n\n**Abstract:** In this study, we present a comprehensive hierarchical framework for describing the reduced states and dynamics of open quantum systems, which we refer to as the Hierarchy of Density Tensors (HDT). This framework is developed using the Nakajima-Zwanzig projection operator technique applied to the von Neumann equation governing the evolution of the entire system. Our approach enables the identification of all pertinent information regarding the decoherence processes induced by the environment across various timescales. Notably, the HDT provides insights into the complete spectrum of relaxation rates that characterize the decay of off-diagonal elements of the reduced density matrix, as well as the stationary states attained in the long-term limit. To illustrate the applicability of our formalism, we investigate the dissipative spin-boson model with Ohmic dissipation, comparing our findings with numerical simulations derived from the Quantum Monte Carlo Wavefunction method. \n\n**Introduction:** Understanding how macroscopic systems respond to environmental influences has been a fundamental question in science for centuries. This challenge becomes particularly complex when analyzing large systems, such as those found in condensed matter physics or biological contexts, where the number of degrees of freedom can be exceedingly large. A common strategy to address these challenges involves examining the dynamics of the reduced state of the system of interest, conditioned on specific measurements of the environmental degrees of freedom. Recent advancements have led to various methodologies aimed at elucidating the time evolution of the reduced state. Among these, the Hierarchy of Density Matrices (HDM) has emerged as a promising alternative due to its capability to capture non-Markovian effects. However, while the HDM can accurately predict short-time dynamics, it struggles to replicate the asymptotic behavior of the system. To overcome this limitation, we propose an enhanced version of the HDM, termed the Hierarchy of Density Tensors, which aims to provide a more robust description of open quantum systems.",
        "ori-fast-z-score": -0.9370425713316364,
        "water-fast-z-score": 6.6172410253729455,
        "rewrite-fast-z-score": 1.1748539016153647
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Modeling partitioning of Min proteins between daughter cells after septation in Escherichia coli .\nAbstract:\nThe Min system is responsible for the correct positioning of the Z ring and division site at mid-cell during bacterial cell division.  The Min system consists of three components, MinC, MinD, and MinE that oscillate back-and-forth across the cytoplasm to create an inhibitory gradient along the length of the cell.  This gradient prevents assembly of new FtsZ rings outside of mid-cell until all existing FtsZ rings have disassembled.   In this study we use computational modeling techniques to investigate how the Min system partitions its components into two daughter cells following cytokinesis.  We find that the Min system can be divided into four distinct phases based on the distribution of Min protein complexes within each cell.  During phase I (0-60 s), MinD-E2-MinC complexes are distributed throughout both mother and daughter cells with higher concentrations near the poles than at mid-cell.  Phase II (60-120 s) begins when MinD-E2-MinCs accumulate at mid-cell forming a band-like structure.  During phase III (120-240 s), MinD-E2-",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Modeling partitioning of Min proteins between daughter tissues after septation in Escherichia coli . Abstract : The Min network is responsible for the appropriate placement of the Z ring and division center at mid - cell during bacterial cell development .The Min network consists of three components , MinC , MinD , and MinE that oscillate back - and - forth across the cytoplasm to create an inhibitory gradient along the length of the cell . This gradient prevents development of new FtsZ rings outside of mid - cell until all original FtsZ rings have disassembled .In this study we using computational modeling tactics to examine how the Min network partitions its components into two sister cells following cytokinesis . We see that the Min network can be broken into four different stages based on the location of Min protein complexes within each cell .During period I ( 0 - 60 s ) , MinD - E2 - MinC complexes are distributed throughout both mom and child cells with higher concentrations near the poles than at mid - cell . Phase II ( 60 - 120 s ) occurs when MinD - E2 - MinCs accumulate at mid - cell creating a band - like structure .During period III ( 120 - 240 s ) , MinD - E2 -",
        "rewrite_text": "**Title:** Modeling the Partitioning of Min Proteins Between Daughter Cells Following Septation in Escherichia coli\n\n**Abstract:** The Min protein network plays a crucial role in determining the precise positioning of the Z ring and the division site at the mid-cell during the cell division process in bacteria. This network comprises three key proteins: MinC, MinD, and MinE, which oscillate dynamically across the cytoplasm to establish an inhibitory gradient along the cell's length. This gradient is essential as it prevents the formation of new FtsZ rings outside the mid-cell region until the existing FtsZ rings have completely disassembled. In this study, we employ computational modeling techniques to investigate the partitioning behavior of the Min protein network into two daughter cells following cytokinesis. Our analysis reveals that the partitioning process can be categorized into four distinct phases based on the spatial distribution of Min protein complexes within each cell. \n\nIn Phase I (0-60 seconds), MinD-E2-MinC complexes are dispersed throughout both the mother and daughter cells, with a notable concentration near the poles compared to the mid-cell region. Phase II (60-120 seconds) is characterized by the accumulation of MinD-E2-MinC complexes at the mid-cell, resulting in the formation of a band-like structure that signifies the transition towards division. During Phase III (120-240 seconds), the dynamics of the Min proteins continue to evolve, leading to further refinement of their distribution as the cells prepare for subsequent stages of division. This study provides valuable insights into the temporal and spatial dynamics of the Min network, enhancing our understanding of bacterial cell division and the mechanisms that ensure accurate partitioning of cellular components. The findings have implications for understanding the fundamental processes of bacterial growth and division, which could inform future research in microbiology and synthetic biology.",
        "ori-fast-z-score": -0.562543950463012,
        "water-fast-z-score": 5.366563145999495,
        "rewrite-fast-z-score": 0.43685202833051895
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Coulomb gauge Gribov copies and the confining potential .\nAbstract:\nWe study the effect of Coulomb gauge fixing on the gluon propagator in Landau gauge, using lattice simulations with two different actions for SU(2) Yang-Mills theory. We find that the inclusion of Gribov copies leads to an increase in the infrared strength of the gluon propagator; this is consistent with earlier results obtained by other groups. \n \n In addition we show how the presence of Gribov copies affects the static quark-antiquark potential at large distances. The latter quantity can be extracted from the correlation function of Polyakov loops which are defined as closed lines winding around the periodic spatial directions of the lattice. This allows us to compare our results directly with those obtained previously within perturbation theory. Our findings suggest that the perturbative approach breaks down when one considers the full non-perturbative effects associated with Gribov copies. Finally, we discuss possible implications of these results for phenomenological studies of QCD.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Coulomb gauge Gribov copies and the confining potential . Abstract : We research the impact of Coulomb gauge fixing on the gluon propagator in Landau gauge , using lattice simulations with two different actions for SU ( 2 ) Yang - Mills theory .We see that the inclusion of Gribov copies results to an increase in the infrared strength of the gluon propagator ; this is compatible with previous findings obtained by other groups . In addition we find how the presence of Gribov copies impacts the static quark - antiquark capacity at large distances .The latter quantity can be extracted from the correlation function of Polyakov circuits which are specified as closed lines winding around the periodic spatial directions of the lattice . This enables us to relate our findings directly with those achieved previously within perturbation theory .Our findings show that the perturbative methodology cracks down when one considers the full non - perturbative impacts associated with Gribov copies . Finally , we explain possible possibilities of these results for phenomenological experiments of QCD .",
        "rewrite_text": "In this study, we investigate the effects of Coulomb gauge fixing on the gluon propagator within the framework of Landau gauge, utilizing lattice simulations based on two distinct actions for SU(2) Yang-Mills theory. Our analysis reveals that the incorporation of Gribov copies leads to an enhancement in the infrared behavior of the gluon propagator, a result that aligns with previous observations reported by other research groups. Furthermore, we explore the influence of Gribov copies on the static quark-antiquark potential at large separations. This potential is derived from the correlation functions of Polyakov loops, which are defined as closed paths that wrap around the periodic spatial dimensions of the lattice. This approach allows us to directly connect our results with those obtained through perturbative techniques in earlier studies. Our findings indicate that the perturbative framework becomes inadequate when accounting for the comprehensive non-perturbative effects associated with Gribov copies. Lastly, we discuss the implications of our results for phenomenological applications in Quantum Chromodynamics (QCD), highlighting how these insights could inform future experimental investigations.",
        "ori-fast-z-score": -1.3587324409735149,
        "water-fast-z-score": 4.302652729749464,
        "rewrite-fast-z-score": -1.6059101370939322
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dynamical non-axisymmetric instabilities in rotating relativistic stars .\nAbstract:\nWe study the dynamical stability properties of rapidly-rotating, axisymmetric neutron star models with different equations of state (EOS). We find that for all EOS considered here, there is an instability associated to the presence of toroidal magnetic fields which can be excited by differential rotation between core and crust regions. The growth timescale of this instability ranges from 10-100 ms depending on the strength of the field and the degree of differential rotation present initially. \n \n This instability may have important consequences for magnetar flares as well as for the spin evolution of young pulsars. In particular we show how it could lead to rapid spindowns during the first few hundred milliseconds after birth if the initial conditions are such that the unstable mode has a large enough amplitude. Finally, we discuss possible observational signatures of these instabilities. Dynamical non-axisymmetric modes in rapidly-rotating relativistic stars",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dynamical non - axisymmetric instabilities in rotating relativistic stars . Abstract : We research the dynamical stability properties of quickly - spinning , axisymmetric neutron star models with various equations of state ( EOS ) .We see that for all EOS considered here , there is an instability related to the presence of toroidal magnetic fields which can be excited by differential rotation between core and crust regions . The growth timescale of this instability vary from 10 - 100 ms based on the strength of the field and the degree of differential rotation present initially .This instability may have important implications for magnetar flares as also as for the spin development of young pulsars . In particular we show how it could lead to rapid spindowns during the first few hundred milliseconds after born if the first environments are such that the unstable mode has a large enough amplitude .Finally , we explain possible observational signatures of these instabilities . Dynamical non - axisymmetric systems in rapidly - spinning relativistic stars",
        "rewrite_text": "In this study, we investigate the dynamical stability characteristics of rapidly rotating, axisymmetric neutron star models, utilizing a variety of equations of state (EOS). Our findings reveal that all EOS examined exhibit a notable instability linked to the presence of toroidal magnetic fields, which can be triggered by differential rotation between the core and crust regions of the star. The timescale for the growth of this instability ranges from 10 to 100 milliseconds, depending on the intensity of the magnetic field and the extent of the initial differential rotation. This instability may have significant consequences for phenomena such as magnetar flares and the spin evolution of young pulsars. Specifically, we demonstrate that this instability could result in rapid spin-down events during the initial few hundred milliseconds following the birth of a neutron star, particularly if the surrounding conditions allow for the unstable mode to achieve a sufficiently large amplitude. Furthermore, we discuss potential observational signatures that could indicate the presence of these instabilities in dynamical non-axisymmetric systems within rapidly spinning relativistic stars. Our research contributes to a deeper understanding of the complex interplay between magnetic fields and rotation in neutron stars, highlighting the importance of these instabilities in astrophysical contexts.",
        "ori-fast-z-score": 0.8307471607356973,
        "water-fast-z-score": 5.032769329433615,
        "rewrite-fast-z-score": -0.20628424925175867
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Mapping the Youngest Galaxies to Redshift One .\nAbstract:\nWe present deep near-infrared imaging and spectroscopy for two fields in the Hubble Deep Field South (HDF-S) covering an area of 0.5 square degrees each, centered on RA = 03:32:47.6 Dec = -27:48:30 (J2000). The data were obtained with ISAAC at VLT as part of our ESO Large Programme  First Stars First Galaxies . We have detected more than 1000 galaxies down to KsAB = 24 mag over this field using SExtractor. In addition we have spectroscopically confirmed about 100 objects selected by their photometric redshifts between z=0.7-1.3. This is one of the largest samples of high-z galaxies ever assembled. Our sample includes both Lyman-break galaxies and sub-millimeter sources which are likely to be dusty starbursting systems. These results will allow us to study galaxy formation and evolution up to redshift 1.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Mapping the Youngest Galaxies to Redshift One . Abstract : We create deep near - infrared imaging and spectroscopy for two fields in the Hubble Deep Field South ( HDF - S ) covering an area of 0 . 5 square degrees each , centered on RA = 03 : 32 : 47 . 6 Dec = - 27 : 48 : 30 ( J2000 ) .The data were obtained with ISAAC at VLT as part of our ESO Large Programme First Stars First Galaxies . We have discovered more than 1000 objects down to KsAB = 24 mag over this field using SExtractor .In addition we have spectroscopically confirmed about 100 galaxies selected by their photometric redshifts between z = 0 . 7 - 1 . 3 . This is one of the largest samples of high - z galaxies yet assembled .Our specimen includes both Lyman - break galaxies and sub - millimeter sources which are likely to be dusty starbursting systems . These data will aid us to study galaxy formation and evolution up to redshift 1 .",
        "rewrite_text": "In this study, we present extensive near-infrared imaging and spectroscopy conducted in two regions of the Hubble Deep Field South (HDF-S), each covering an area of 0.5 square degrees and centered at coordinates RA = 03:32:47.6, Dec = -27:48:30 (J2000). The observations were carried out using the Infrared Spectrometer And Array Camera (ISAAC) at the Very Large Telescope (VLT) as part of our European Southern Observatory (ESO) Large Programme titled \"First Stars, First Galaxies.\" Through our analysis, we have identified over 1,000 celestial objects with a magnitude limit of KsAB = 24 mag within these fields, utilizing the SExtractor software for object detection. Furthermore, we have successfully spectroscopically confirmed approximately 100 galaxies, selected based on their photometric redshifts, which range from z = 0.7 to 1.3. This collection represents one of the most substantial samples of high-redshift galaxies compiled to date. Our dataset encompasses a variety of galaxy types, including Lyman-break galaxies and sub-millimeter sources, which are likely to be dusty starburst galaxies. The insights gained from this research will significantly contribute to our understanding of galaxy formation and evolution up to redshift 1, providing valuable information on the early stages of galaxy development in the universe.",
        "ori-fast-z-score": 0.42008402520840293,
        "water-fast-z-score": 3.6055512754639896,
        "rewrite-fast-z-score": 1.2535663410560174
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Intersection Bodies and Generalized Cosine Transforms .\nAbstract:\nWe introduce the notion of intersection bodies in arbitrary dimensions, generalizing the classical concept for dimension two to higher dimensions. We show that intersection bodies are characterized by their Fourier transforms which we call generalized cosine transforms (GCT). The GCTs can be used as an alternative tool to study intersection bodies. In particular, we prove that intersection bodies have positive volume if and only if they are convex. This is done using a new characterization of intersection bodies via their support functions. Finally, we give some examples of intersection bodies in three dimensions. Keywords: Intersection body; Support function; Convexity; Volume; Fourier transform; Three-dimensional space. 1 Introduction Let K n denote the set of all origin-symmetric convex bodies in R n . For any K ∈ K n , let V(K) = |K|/|B n 2 | where | · | denotes Lebesgue measure on R n . Then V : K n →  0, 1  is called the volume functional. A compactly supported continuous function f : S n−1 → C with unit integral will be called a spherical harmonic of degree m. If f has no zeros then it is uniquely determined up to multiplication by a constant. It follows immediately that every spherical harmonic of degree m satisfies the following properties:\n(1) |f (x)| ≤ 1; (2) f (−x) = f (x), x ∈ S n−1 ; (3) (Spherical harmonics form an orthonormal basis.) Definition 1. An origin-symmetric convex body K ∈ K n is said to be an intersection body if there exists a non-negative real number λ such that its surface area measure σ K satisfies",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Intersection Bodies and Generalized Cosine Transforms . Abstract : We introduce the notion of intersection bodies in arbitrary dimensions , generalizing the classical concept for dimension two to higher dimensions .We see that intersection bodies are characterized by their Fourier transforms which we call generalized cosine transforms ( GCT ) . The GCTs can be used as an additional tool to study intersection bodies .In particular , we prove that intersection bodies have positive volume if and only if they are convex . This is accomplished utilizing a new definition of junction bodies via their support functions .Finally , we give some examples of intersection bodies in three dimensions . Keywords : Intersection body ; Support function ; Convexity ; Volume ; Fourier integral ; Three - dimensional space .1 Introduction Let K n denote the group of all origin - symmetric convex bodies in R n . For any K ∈ K n , let V ( K ) = | K | / | B n 2 | where | · | indicates Lebesgue measure on R n .Then V : K n → 0 , 1 is dubbed the volume functional . A compactly backed continuous function f : S n−1 → C with unit integral will be called a spherical harmonic of degree m . If g has no zeros then it is uniquely determined up to multiplication by a constant .It follows quickly that every spherical harmonic of degree m satisfies the following properties : ( 1 ) | f ( x ) | ≤ 1 ; ( 2 ) f ( −x ) = g ( x ) , z ∈ S n−1 ; ( 3 ) ( Spherical harmonics form an orthonormal basis . ) Definition 1 .An identity - symmetric convex body K ∈ K n is said to be an intersection body if there exists a non - negative real number λ such that its surface area measure σ K satisfies",
        "rewrite_text": "We present a comprehensive exploration of intersection bodies in arbitrary dimensions, extending the classical definition from two dimensions to higher-dimensional spaces. This study reveals that intersection bodies can be effectively characterized through their Fourier transforms, which we refer to as generalized cosine transforms (GCTs). These GCTs serve as a valuable tool for further investigation into the properties of intersection bodies. Notably, we establish a significant result: an intersection body possesses positive volume if and only if it is convex. This finding is derived from a novel definition of junction bodies based on their support functions. Additionally, we provide illustrative examples of intersection bodies within three-dimensional space, enhancing the understanding of their geometric properties. \n\nIn our work, we denote the group of all origin-symmetric convex bodies in \\( \\mathbb{R}^n \\) as \\( K_n \\). For any body \\( K \\) within this group, we define the volume functional \\( V(K) = |K| / |B^n_2| \\), where \\( | \\cdot | \\) represents the Lebesgue measure in \\( \\mathbb{R}^n \\). Furthermore, we introduce the concept of spherical harmonics, which are compactly supported continuous functions defined on the unit sphere \\( S^{n-1} \\) with unit integral. These spherical harmonics of degree \\( m \\) exhibit specific properties, including boundedness and symmetry. \n\nOur findings contribute to the broader understanding of convex geometry and the interplay between geometric properties and Fourier analysis. The implications of our results extend to various applications in mathematical analysis and geometric measure theory, paving the way for future research in the field. \n\nKeywords: Intersection body; Support function; Convexity; Volume; Fourier integral; Three-dimensional space.",
        "ori-fast-z-score": -0.09325048082403138,
        "water-fast-z-score": 3.4206512100555795,
        "rewrite-fast-z-score": 1.4368424162141993
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Nonlinear Dynamics of the Phonon Stimulated Emission in Microwave Solid-State Resonator of the Nonautonomous Phaser Generator .\nAbstract:\nThe nonlinear dynamics of phonon stimulated emission (PSE) is investigated for microwave solid-state resonators with nonautonomous phase-locked loop generators. The PSE process is described by means of an extended version of the Lang-Kobayashi model, which takes into account the influence of the external driving field on the gain medium and includes additional terms describing the effect of spontaneous emission noise. It has been shown that the presence of these new effects leads to significant changes in the behavior of the system under consideration as compared to autonomous systems. In particular, it was found that the generation of chaotic regimes becomes possible even at relatively small values of the pumping parameter. This fact can be used to develop novel types of chaos-based devices based on microwave solid-state oscillators. \n \n Keywords: Nonlinear dynamics; Chaos; Phase synchronization; Phonons; Microwave solid state oscillator. PACS: 42.65.Tt; 42.65.Pq; 42.65.Re",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Nonlinear Dynamics of the Phonon Stimulated Emission in Microwave Solid - State Resonator of the Nonautonomous Phaser Generator . Abstract : The nonlinear dynamics of phonon stimulated emission ( PSE ) is investigated for microwave solid - state resonators with nonautonomous phase - locked loop components .The PSE transition is characterized by means of an extended version of the Lang - Kobayashi model , which assumes into consideration the impact of the external driving field on the gain medium and incorporates additional terms describing the impact of induced emission interference . It has been shown that the presence of these new effects leads to significant improvements in the activity of the process under consideration as compared to autonomous systems .In particular , it was shown that the generation of turbulent regimes appears possible even at fairly little values of the pumping function . This fact can be used to develop new types of chaos - based devices derived on microwave solid - state oscillators .Keywords : Nonlinear dynamics ; Chaos ; Phase synchronization ; Phonons ; Microwave liquid state oscillator . PACS : 42 . 65 . Tt ; 42 . 65 . Pq ; 42 . 65 . Re",
        "rewrite_text": "**Title:** Nonlinear Dynamics of Phonon Stimulated Emission in Microwave Solid-State Resonators of the Nonautonomous Phaser Generator\n\n**Abstract:** This study explores the nonlinear dynamics associated with phonon stimulated emission (PSE) in microwave solid-state resonators that incorporate nonautonomous phase-locked loop components. Utilizing an advanced version of the Lang-Kobayashi model, we analyze the PSE transition by accounting for the influence of an external driving field on the gain medium. Additionally, we introduce new terms that capture the effects of induced emission interference, which are critical for understanding the dynamics of the system. Our findings reveal that these newly considered factors significantly enhance the operational characteristics of the PSE process when compared to traditional autonomous systems. Notably, we demonstrate that turbulent regimes can emerge even with relatively low values of the pumping function. This discovery opens avenues for the development of innovative chaos-based devices that leverage microwave solid-state oscillators. The implications of this research are profound, as they suggest that the integration of nonautonomous elements can lead to enhanced performance and new functionalities in microwave resonator applications. The results contribute to the broader understanding of nonlinear dynamics in photonic systems and highlight the potential for practical applications in advanced technology. \n\n**Keywords:** Nonlinear dynamics; Chaos; Phase synchronization; Phonons; Microwave solid-state oscillator. \n\n**PACS:** 42.65.Tt; 42.65.Pq; 42.65.Re",
        "ori-fast-z-score": -0.7875615306482168,
        "water-fast-z-score": 4.47213595499958,
        "rewrite-fast-z-score": 0.502518907629606
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Trans-Planckian Issue in the Milne Universe .\nAbstract:\nWe consider the problem of time evolution for quantum fields on the Milne universe, which is an open FRW cosmology with negative spatial curvature and vanishing cosmological constant. We show that there are no normalizable solutions to the Klein-Gordon equation corresponding to massive particles at early times (t → −∞). This result can be understood as follows. The Milne universe has a singularity at t = 0 where all physical quantities diverge. In particular, the energy density diverges like $1/t^2$ near this point. As a consequence, any particle state localized around $t=0$ will have infinite energy. Therefore it cannot correspond to a physically meaningful solution of the field equations. On the other hand, we find that there exist normalizable states describing massless particles at early times. These states represent gravitons or photons propagating into the past along geodesics towards the big bang singularity.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Trans - Planckian Issue in the Milne Universe . Abstract : We consider the question of time evolution for quantum fields on the Milne universe , which is an open FRW cosmology with negative spatial curvature and vanishing cosmological constant .We see that there are no normalizable solutions to the Klein - Gordon equation equivalent to massive particles at early periods ( t → −∞ ) . This result can be understood as follows .The Milne universe has a singularity at t = 0 where all physical components diverge . In particular , the energy density diverges like $ 1 / t ^ 2 $ near this point .As a consequence , any particle state concentrated around $ t = 0 $ will have infinite energy . Therefore it cannot correspond to a physically useful solve of the field equations .On the other hand , we find that there exist normalizable states expressing massless molecules at early years . These states describe gravitons or photons propagating into the past along geodesics towards the big bang singularity .",
        "rewrite_text": "In this article, we investigate the time evolution of quantum fields within the framework of the Milne universe, characterized as an open Friedmann-Robertson-Walker (FRW) cosmology with negative spatial curvature and a zero cosmological constant. Our analysis reveals a significant finding: there are no normalizable solutions to the Klein-Gordon equation that correspond to massive particles during the early epochs of the universe (as time approaches negative infinity). This phenomenon can be attributed to the presence of a singularity at \\( t = 0 \\) in the Milne universe, where all physical quantities exhibit divergence. Notably, the energy density behaves like \\( 1/t^2 \\) as it approaches this singular point, leading to the conclusion that any particle state localized around \\( t = 0 \\) would possess infinite energy. Consequently, such states cannot yield physically meaningful solutions to the field equations.\n\nConversely, our study identifies the existence of normalizable states that represent massless particles during the early stages of the universe. These states are indicative of massless entities, such as gravitons or photons, which propagate backward in time along geodesics toward the singularity associated with the big bang. This distinction between massive and massless states highlights the unique characteristics of the Milne universe and raises intriguing questions regarding the implications for quantum field theory in cosmological contexts. Our findings contribute to a deeper understanding of the trans-Planckian issues that arise in cosmological models and their impact on the behavior of quantum fields in the early universe. This work opens avenues for further exploration of the interplay between quantum mechanics and cosmology, particularly in relation to the fundamental nature of spacetime and the origins of the universe.",
        "ori-fast-z-score": -0.629940788348712,
        "water-fast-z-score": 2.6457513110645903,
        "rewrite-fast-z-score": 2.6224402724287432
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Three Different Types of Galaxy Alignment within Dark Matter Halos .\nAbstract:\nWe present the results of an analysis of galaxy-galaxy lensing data for three different types of galaxies in dark matter halos, using the Millennium Simulation and semi-analytic models to predict their properties.  We find that early-type galaxies are aligned with each other along filaments, while late-types show no alignment signal at all.  The intermediate type shows some evidence for alignments perpendicularly to the filamentary structure.  These findings can be explained by tidal torques acting on the gas during its infall into the halo potential well, which is more efficient for early-type than for latetype galaxies.  This effect may also explain why we do not see any significant alignment between the shapes of dark matter haloes themselves. Our results suggest that the observed alignments of galaxies could be caused by large-scale gravitational fields rather than being primordial effects. In this work we study how the shape of dark matter halos affects the distribution of satellite galaxies around them. Using N-body simulations, we investigate whether there exists a correlation between the orientation of the major axis of the host halo and the position angle of satellites relative to it. We find that such correlations exist only if the mass ratio between the primary and secondary halo is large enough (M1/M2 > 10). For smaller mass ratios, the orientations of both halos become uncorrelated due to dynamical friction.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Three Different Types of Galaxy Alignment within Dark Matter Halos . Abstract : We present the conclusion of an assessment of galaxy - galaxy lensing data for three different kinds of universe in dark matter halos , using the Millennium Simulation and semi - analytic models to predict their characteristics .We see that earliest - class stars are aligned with each other along filaments , while late - types show no alignment signal at all . The intermediate kind shows some evidence for alignments perpendicularly to the filamentary structure .These findings can be reason by tidal torques acting on the gas during its infall into the halo potential well , which is more efficient for earliest - class than for latetype galaxies . This phenomenon might additionally explain why we do not see any large alignment between the shapes of bright matter haloes themselves .Our results propose that the seen alignments of stars likely be caused by large - scale gravitational waves rather than being primordial impacts . In this research we study how the form of dark matter halos changes the distribution of satellite galaxies around them .Using N - bodies simulations , we investigate whether there exists a correlation between the orientation of the main axis of the host halo and the direction angle of satellites relative to it . We see that such correlations occur only if the mass ratio between the primary and secondary halo is huge enough ( M1 / M2 > 10 ) .For lower weight ratios , the orientations of both halos become uncorrelated due to dynamical friction .",
        "rewrite_text": "In this study, we explore the alignment of galaxies within dark matter halos by analyzing galaxy-galaxy lensing data across three distinct cosmic environments, utilizing the Millennium Simulation alongside semi-analytic models to forecast their properties. Our findings reveal that early-type galaxies exhibit a pronounced alignment along cosmic filaments, while late-type galaxies show no discernible alignment signal. Intermediate-type galaxies display some evidence of alignment that is perpendicular to the filamentary structures. These observations can be attributed to tidal torques exerted on the gas during its accretion into the halo's gravitational well, which appears to be more effective for early-type galaxies compared to their late-type counterparts. This mechanism may also elucidate the absence of significant alignment among the shapes of luminous matter halos themselves. \n\nOur results suggest that the observed alignments of stars are likely influenced by large-scale gravitational waves rather than being a result of primordial interactions. Furthermore, we investigate how the morphology of dark matter halos affects the spatial distribution of satellite galaxies. Through N-body simulations, we assess the correlation between the orientation of the primary halo's main axis and the angular positioning of its satellite galaxies. Our analysis indicates that such correlations manifest only when the mass ratio between the primary and secondary halos is sufficiently large (M1/M2 > 10). In scenarios with lower mass ratios, the orientations of the halos become uncorrelated due to the effects of dynamical friction. This research contributes to our understanding of the complex interplay between dark matter structures and galaxy formation, highlighting the significance of mass ratios and environmental influences on galaxy alignment phenomena.",
        "ori-fast-z-score": -1.3598002073001698,
        "water-fast-z-score": 5.965952781626132,
        "rewrite-fast-z-score": 2.455762454059682
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A new Generation of Spectrometer Calibration Techniques based on Optical Frequency Combs .\nAbstract:\nWe present the latest advances in optical frequency combs and their applications to precision metrology, including calibration techniques for high-resolution spectroscopy instruments such as Fourier transform spectrometers (FTS). We discuss how these techniques can be used to improve measurement accuracy by orders of magnitude over traditional methods.  In particular we describe two novel approaches that are being developed at NIST:  1) The use of an optical frequency comb locked to a high-accuracy atomic clock to calibrate FTS measurements with sub-Hz uncertainty. 2) A technique called  self-calibration  which allows one to measure absolute frequencies without requiring any external reference standards or other auxiliary equipment. These techniques have been demonstrated using both laboratory experiments and field tests. They represent important steps towards realizing the vision of a future where all spectroscopic measurements will be traceable back to fundamental physical constants. This is especially relevant today given the growing interest in developing portable, low-cost devices capable of performing accurate chemical analysis anywhere around the world.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A new Generation of Spectrometer Calibration Techniques based on Optical Frequency Combs . Abstract : We present the latest advances in imaging bandwidth combs and their applications to precision metrology , notably calibration methods for high - resolution spectroscopy instruments such as Fourier shift spectrometers ( FTS ) .We discuss how these tools can be used to achieve observation reliability by orders of magnitude over traditional techniques . In particular we explain two novel approaches that are being pioneered at NIST : 1 ) The using of an optical frequency comb locked to a high - accuracy nuclear clock to calibrate FTS calculations with sub - Hz uncertainty .2 ) A methodology called self - calibration which allows one to measure absolute frequencies without using any external reference standards or other auxiliary machinery . These methods have been shown using both laboratory experiments and field trials .They represent crucial steps towards realizing the vision of a future where all spectroscopic observations will be traceable back to basic physical constants . This is especially applicable today due the increasing interest in building mobile , low - cost devices capable of performing reliable chemical analysis anywhere around the world .",
        "rewrite_text": "We introduce significant advancements in the field of spectrometer calibration techniques utilizing optical frequency combs, as detailed in our article. These innovations are particularly relevant for precision metrology and high-resolution spectroscopy instruments, such as Fourier transform spectrometers (FTS). Our discussion highlights how these advanced tools can enhance the reliability of observations by several orders of magnitude compared to conventional methods. We focus on two pioneering approaches developed at the National Institute of Standards and Technology (NIST). The first approach involves the use of an optical frequency comb that is synchronized with a high-precision nuclear clock, enabling FTS calculations to achieve sub-Hz uncertainty in calibration. The second method, termed self-calibration, facilitates the measurement of absolute frequencies without relying on external reference standards or additional equipment. Both techniques have been validated through extensive laboratory experiments and field trials, marking significant progress toward a future where all spectroscopic measurements can be traced back to fundamental physical constants. This advancement is particularly timely given the growing demand for mobile, cost-effective devices capable of conducting reliable chemical analyses in diverse environments worldwide. Our findings underscore the potential of these new calibration techniques to revolutionize the field of spectroscopy, making high-precision measurements more accessible and reliable than ever before.",
        "ori-fast-z-score": -0.9138115486202573,
        "water-fast-z-score": 4.848732213850612,
        "rewrite-fast-z-score": -0.5827715174143585
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A new comprehensive set of elemental abundances in DLAs III. Star formation histories .\nAbstract:\nWe present the results of our analysis of a sample of 25 high-resolution quasar absorption line systems with metallicities ranging between 1/100 and 1/10 solar, selected to have low dust content (i.e., < 0.1 mag extinction at 2200 A). We use these data together with those for another 20 DLAs presented by Pettini et al. (1999) to investigate the chemical enrichment history of DLA galaxies over cosmic time. The main conclusions are as follows: \nThe abundance patterns observed in this sample can be explained if we assume that most of the metals were produced during an early burst of star formation which occurred less than 10 Gyr ago. \n\n\nThis is consistent with previous studies based on smaller samples but it also shows that there may not always be evidence for recent star formation activity even when such activity has been inferred from other indicators. \n\nIn addition, we find no correlation between metallicity and dust content or neutral hydrogen column density.\n\nFinally, we show that the mean value of  Fe/H  measured in DLAs agrees well with the predictions made using simple models of galactic chemical evolution.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A modern comprehensive setting of elemental abundances in DLAs III . Star formation histories .Abstract : We present the conclusion of our analysis of a sample of 25 high - resolution quasar absorbed line systems with metallicities ranging between 1 / 100 and 1 / 10 solar , selected to have lowest dust content ( i . e . , < 0 . 1 mag extinction at 2200 A ) . We use these results together with those for another 20 DLAs given by Pettini et al .( 1999 ) to examine the chemical enrichment history of DLA galaxies over cosmic time . The main results are as follows : The accumulation patterns observed in this specimen can be understood if we suppose that most of the metals were produced during an early burst of star formation which occurred less than 10 Gyr ago .This is consistent with previous analyses based on smaller specimens but it also shows that there may not always be confirmation for recent star formation activity even when such activity has been inferred from other indicators . In addition , we find no correlation between metallicity and dust content or neutral hydrogen column density .Finally , we prove that the mean value of Fe / H measured in DLAs agrees well with the assumptions done using simple theories of galactic material evolution .",
        "rewrite_text": "We present the findings of our comprehensive analysis of a sample comprising 25 high-resolution quasar absorption line systems, characterized by metallicities between 1/100 and 1/10 of solar levels. This selection was made with a focus on systems exhibiting minimal dust content, specifically those with less than 0.1 magnitudes of extinction at 2200 Å. Our results, combined with data from an additional 20 damped Lyman-alpha (DLA) systems analyzed by Pettini et al. (1999), allow us to investigate the chemical enrichment history of DLA galaxies throughout cosmic time. \n\nThe primary conclusions drawn from our study indicate that the observed accumulation patterns of elemental abundances can be explained by the hypothesis that a significant portion of the metals was synthesized during an early episode of star formation that took place less than 10 billion years ago. This finding aligns with earlier studies based on smaller samples, yet it also highlights the potential lack of evidence for recent star formation activity, even when inferred from various other indicators. Furthermore, our analysis reveals no significant correlation between metallicity and either dust content or neutral hydrogen column density, suggesting that these factors may not be directly linked in the context of DLA systems. \n\nLastly, we demonstrate that the average iron-to-hydrogen ratio (Fe/H) observed in our DLA sample is consistent with predictions derived from simplified models of galactic chemical evolution. This agreement reinforces the validity of our findings and contributes to a deeper understanding of the processes governing star formation and chemical enrichment in the early universe. Overall, our study provides valuable insights into the star formation histories of DLA galaxies and their elemental abundance patterns, enhancing our comprehension of cosmic evolution.",
        "ori-fast-z-score": 0.9045340337332909,
        "water-fast-z-score": 6.0,
        "rewrite-fast-z-score": 0.2508726030021272
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Temperature-driven transition from the Wigner Crystal to the Bond-Charge-Density Wave in the Quasi-One-Dimensional Quarter-Filled band .\nAbstract:\nWe study temperature driven phase transitions between different charge ordered states in one-dimensional quarter-filled Hubbard model with nearest-neighbor hopping and on-site Coulomb repulsion using density matrix renormalization group (DMRG) method. We find that at low temperatures, the system is in the Wigner crystal state where electrons are localized by strong electron correlations. With increasing temperature, we observe a first-order phase transition into bond-charge-density wave state which has been predicted theoretically earlier but not observed experimentally so far. The nature of this phase transition can be understood as melting of the Wigner crystal due to thermal fluctuations. Finally, we also investigate the effect of next-nearest neighbor hopping t′ on these phases. Our results show that for small values of t′ , the ground state remains in the Wigner crystal phase while for large values of t′ it becomes metallic. This shows that the presence of next-nearest neighbor hoping may destroy the charge ordering in 1D systems.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Temperature - driven transition from the Wigner Crystal to the Bond - Charge - Density Wave in the Quasi - One - Dimensional Quarter - Filled band . Abstract : We explore temperature driven phase transitions between various charge ordered states in one - dimensional quarter - filled Hubbard theory with nearest - neighbor hopping and on - location Coulomb repulsion utilizing density matrix renormalization group ( DMRG ) method .We see that at low temperatures , the system is in the Wigner crystal state where electrons are localized by large electron correlations . With increasing temperature , we study a first - order phase shift into bond - charge - density wave state which has been predicted theoretically prior but not observed experimentally so far .The nature of this phase shift can be understood as melting of the Wigner crystal owing to heat fluctuations . Finally , we also investigate the impact of next - nearest neighbor hopping t ′ on these phases .Our results show that for little values of t ′ , the ground state remains in the Wigner crystal phase while for large values of t ′ it becomes metallic . This shows that the presence of next - nearest neighbor hoping might disrupt the charge ordering in 1D solutions .",
        "rewrite_text": "In this study, we investigate the temperature-induced phase transitions among various charge-ordered states within the framework of one-dimensional quarter-filled Hubbard models, incorporating nearest-neighbor hopping and on-site Coulomb repulsion. Utilizing the density matrix renormalization group (DMRG) method, we analyze the behavior of the system as temperature varies. At low temperatures, our findings indicate that the system stabilizes in a Wigner crystal state, characterized by significant electron localization due to strong electron correlations. As the temperature increases, we observe a first-order phase transition into a bond-charge-density wave state, a phenomenon that has been theoretically predicted but remains unobserved in experimental settings to date. This transition can be interpreted as the melting of the Wigner crystal, driven by thermal fluctuations. Furthermore, we explore the effects of next-nearest neighbor hopping (t') on these phases. Our results reveal that for small values of t', the ground state persists in the Wigner crystal phase; however, as t' increases, the system transitions to a metallic state. This indicates that the inclusion of next-nearest neighbor hopping can significantly disrupt charge ordering in one-dimensional systems. Overall, our research provides valuable insights into the complex interplay between temperature and electron interactions in quasi-one-dimensional materials, highlighting the potential for novel charge-ordered states and their implications for future experimental investigations.",
        "ori-fast-z-score": 0.29851115706299675,
        "water-fast-z-score": 4.158619680502032,
        "rewrite-fast-z-score": 0.552344770738994
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Is the Bardeen-Petterson effect responsible for the warping and precession in NGC 4258? .\nAbstract:\nWe present new observations of the central region of the nearby Seyfert galaxy NGC 4258, which show that its nuclear disk is warped by an angle of ~20 degrees with respect to the plane of the host galaxy s stellar bulge (see Figure 1 ). The warp has been detected using near-infrared integral field spectroscopy obtained at Gemini Observatory on Mauna Kea, Hawaii. \n \n We also report the detection of significant rotation about the minor axis of this warped structure, as well as evidence for counter-rotation within the innermost few hundred parsecs of the nucleus. These results are consistent with previous studies based on optical data alone. \n \n In addition, we find that the kinematics of the gas in the outer regions of the nuclear disk can be explained if it orbits around the supermassive black hole located at the center of the galaxy under the influence of both gravitational forces and magnetic fields. This result suggests that the observed warps may have their origin in the magneto-rotational instability (MRI) operating in accretion disks surrounding massive black holes. \n \n Finally, we discuss how these findings could help us understand the physics behind the so-called  Bardeen-Petterson effect : i.e., the alignment between the spin axes of the stars and the angular momentum vector of the accreting material onto the central supermassive black hole.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Is the Bardeen - Petterson effect responsible for the warping and precession in NGC 4258 ? .Abstract : We report new images of the central region of the nearby Seyfert galaxy NGC 4258 , which show that its nuclear core is warped by an angle of ~ 20 degrees with regard to the plane of the host universe s stellar bulge ( see Figure 1 ) . The warp has been detected using near - infrared integral field spectroscopy acquired at Gemini Observatory on Mauna Kea , Hawaii .We additionally report the observation of large rotation about the minor axis of this warped structure , as also as data for counter - movement within the innermost few hundred parsecs of the nucleus . These conclusions are compatible with previous research based on optical data alone .In addition , we find that the kinematics of the gas in the exterior areas of the atomic disk can be understood if it orbits around the supermassive black hole located at the center of the galaxy under the effects of both gravity forces and magnetic fields . This result suggests that the seen warps may have their source in the magneto - rotational instability ( MRI ) working in accretion disks surrounding massive blue holes .Finally , we talk how these results could assist us explain the physics behind the so - called Bardeen - Petterson effect : i . e . , the alignment between the spin axes of the stars and the angular velocity tensor of the accreting matter onto the main supermassive black hole .",
        "rewrite_text": "**Title:** Is the Bardeen-Petterson Effect Responsible for the Warping and Precession in NGC 4258?\n\n**Abstract:** In this study, we present new imaging of the central region of the nearby Seyfert galaxy NGC 4258, revealing a significant warp in its nuclear core, tilted at approximately 20 degrees relative to the plane of the galaxy's stellar bulge. This warp was identified through near-infrared integral field spectroscopy conducted at the Gemini Observatory in Mauna Kea, Hawaii. Our observations also indicate a substantial rotation around the minor axis of this warped structure, alongside evidence of counter-movement within the innermost few hundred parsecs of the nucleus. These findings align with earlier studies that relied solely on optical data. Furthermore, we analyze the kinematics of the gas in the outer regions of the atomic disk, suggesting that its motion can be explained by the gravitational influence of the supermassive black hole at the galaxy's center, coupled with the effects of magnetic fields. This leads us to propose that the observed warps may originate from magneto-rotational instability (MRI) occurring in the accretion disks surrounding massive black holes. Finally, we discuss the implications of our results for understanding the Bardeen-Petterson effect, which describes the alignment between the spin axes of stars and the angular velocity tensor of the accreting matter onto the central supermassive black hole. Our findings contribute to a deeper comprehension of the dynamics at play in NGC 4258 and may provide insights into similar phenomena in other galaxies.",
        "ori-fast-z-score": -2.5253432421288866,
        "water-fast-z-score": 4.233901974057256,
        "rewrite-fast-z-score": 1.2567574357593625
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Black Hole Mass - Spheroid Luminosity relation .\nAbstract:\nWe present the results of our study on the correlation between black hole mass and spheroidal luminosity in nearby galaxies, using data obtained with the Hubble Space Telescope (HST). We have used two different methods to estimate the masses of supermassive black holes at the centers of these galaxies; one based on stellar dynamics and another method which uses the widths of broad emission lines as an indicator for the central black hole mass. The sample consists of 29 elliptical galaxies within 40 Mpc distance. Our main result is that we find no significant difference between the black hole masses derived by both methods. This indicates that there exists a tight relationship between the black hole mass and the total luminosity of its host galaxy. In addition, we also found evidence that this relationship does not depend strongly on the morphological type or environment of the host galaxy. These findings are consistent with theoretical predictions made by Silk & Rees (1998) who suggested that feedback processes play an important role in regulating star formation activity in galactic bulges.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Black Hole Mass - Spheroid Luminosity relation . Abstract : We present the conclusion of our research on the relationship between black hole mass and spheroidal luminosity in nearby galaxies , using data acquired with the Hubble Space Telescope ( HST ) .We have utilized two different methods to estimate the masses of supermassive black holes at the centers of these galaxies ; one based on stellar physics and another technique which uses the widths of broad absorption lines as an measure for the main white hole mass . The sample consists of 29 elliptical galaxies within 40 Mpc radius .Our main consequence is that we find no major shift between the dark hole masses derived by both approaches . This implies that there exists a tight connection between the dark hole mass and the total luminosity of its host universe .In addition , we also discovered evidence that this relationship does not depend greatly on the morphological class or climate of the target galaxy . These conclusions are compatible with theoretical calculations made by Silk & Rees ( 1998 ) who proposed that feedback systems play an important role in controlling star formation activity in galactic bulges .",
        "rewrite_text": "We present the findings of our investigation into the correlation between black hole mass and spheroidal luminosity in nearby galaxies, utilizing data obtained from the Hubble Space Telescope (HST). Our study employs two distinct methodologies for estimating the masses of supermassive black holes located at the centers of these galaxies. The first method is grounded in stellar dynamics, while the second approach leverages the widths of broad absorption lines as an indicator of the black hole mass. Our sample comprises 29 elliptical galaxies situated within a 40 Mpc radius. \n\nA key outcome of our research is the observation that there is no significant discrepancy between the black hole masses calculated using the two different techniques. This finding suggests a robust correlation between black hole mass and the total luminosity of the host galaxy. Furthermore, our analysis indicates that this relationship remains largely unaffected by the morphological classification or environmental conditions of the galaxies studied. \n\nThese results align with theoretical predictions made by Silk & Rees (1998), who posited that feedback mechanisms are crucial in regulating star formation processes within galactic bulges. Our work contributes to the understanding of the interplay between supermassive black holes and their host galaxies, reinforcing the notion that the mass of a black hole is intricately linked to the luminosity of its surrounding spheroid. This research not only enhances our comprehension of galaxy evolution but also provides valuable insights into the fundamental processes governing the dynamics of galaxies in the universe.",
        "ori-fast-z-score": -1.4596008983995234,
        "water-fast-z-score": 6.118014998909509,
        "rewrite-fast-z-score": 0.09578262852211514
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Prospects for the cavity-assisted laser cooling of molecules .\nAbstract:\nWe present an overview of recent progress in the development and application of techniques to cool molecules by exploiting their interaction with optical cavities. We discuss how these methods can be used to prepare samples of cold, trapped molecules that are suitable for precision measurements or quantum information processing applications. In particular we focus on two different approaches which have been developed recently at our laboratory: (i) The use of electromagnetically induced transparency (EIT), combined with stimulated Raman adiabatic passage (STIRAP), to produce large numbers of optically trapped ground-state polar molecules. (ii) Cavity-enhanced photoassociation spectroscopy as a tool to study ultracold collisions between alkali-metal atoms. Finally, we briefly outline some possible future directions for this research area. Molecules offer many advantages over atomic systems when it comes to implementing novel quantum technologies such as high-precision metrology  1  , quantum simulation  2  , and quantum networks  3  . However, most molecular species cannot be directly cooled using conventional laser cooling schemes because they lack closed cycling transitions  4  .\nIn order to overcome this problem several alternative cooling strategies have been proposed  5  -  8  . One promising approach is based on the combination of electromagnetically-induced transparency (EIT)  9  and stimulated Raman adiabatic passages (STIRAP)  10  . This method has been successfully applied to create dense ensembles of ground state polar molecules  11  -  13  . Another possibility consists in trapping molecules via photoassociative processes  14  -  16  . Here one exploits the fact that the spontaneous emission rate into bound states increases exponentially with decreasing temperature  17  . By coupling the excited molecular levels to high-finesse optical cavities  18  -  20  , the resulting increase in radiative lifetime leads to efficient trapping  21  -  23  . These techniques allow us to trap up to 10 5 molecules per cm 3 inside a single-mode optical resonator  24  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Prospects for the cavity - aided laser cooling of molecules . Abstract : We illustrate an overview of recent progress in the development and application of techniques to cool molecules by exploiting their interaction with optical cavities .We discuss how these systems can be used to produce samples of cold , captured molecules that are suitable for precision observations or quantum information processing applications . In particular we focus on two different methods which have been pioneered recently at our laboratory : ( i ) The using of electromagnetically induced transparency ( EIT ) , combined with stimulated Raman adiabatic passage ( STIRAP ) , to produce substantial quantity of optically captured ground - state polar compounds .( ii ) Cavity - augmented photoassociation spectroscopy as a technique to study ultracold collisions between alkali - metal atoms . Finally , we briefly outline some possible future paths for this research field .Molecules offer several advantages over nuclear systems when it comes to incorporating novel quantum technologies such as high - precision metrology 1 , quantum modeling 2 , and quantum networks 3 . However , most molecular species cannot be directly warmed use conventional optical cooling schemes because they lack open cycling transitions 4 .In try to overcome this situation several alternative thermal strategies have been proposed 5 - 8 . One promising solution is based on the combination of electromagnetically - triggered stability ( EIT ) 9 and stimulated Raman adiabatic passages ( STIRAP ) 10 .This method has been successfully applied to create rich ensembles of ground state polar molecules 11 - 13 . Another possibility consists in trap molecules via photoassociative processes 14 - 16 .Here one utilizes the fact that the spontaneous emission speed into bound products increases exponentially with decreasing temperature 17 . By coupling the excited molecular levels to large - finesse optical cavities 18 - 20 , the resulting increase in radiative lifetime leads to efficient capturing 21 - 23 .These methods able us to capture up to 10 5 molecules per cm 3 inside a single - mode optical resonator 24 .",
        "rewrite_text": "**Title:** Prospects for Cavity-Aided Laser Cooling of Molecules\n\n**Abstract:** This article provides a comprehensive overview of recent advancements in the techniques aimed at cooling molecules through their interactions with optical cavities. We explore how these innovative systems can generate samples of cold, trapped molecules that are ideal for high-precision measurements and applications in quantum information processing. Our focus is on two pioneering methods developed in our laboratory: first, the application of electromagnetically induced transparency (EIT) in conjunction with stimulated Raman adiabatic passage (STIRAP) to create significant quantities of optically trapped ground-state polar molecules; second, the use of cavity-enhanced photoassociation spectroscopy to investigate ultracold collisions among alkali-metal atoms. \n\nWe also highlight the advantages that molecules present over atomic systems in the realm of emerging quantum technologies, including high-precision metrology, quantum simulation, and quantum networking. Despite these advantages, many molecular species are not amenable to traditional optical cooling methods due to the absence of open cycling transitions. To address this challenge, several alternative thermal strategies have been proposed. Among these, the combination of EIT and STIRAP has shown promise, successfully yielding large ensembles of ground-state polar molecules. Additionally, trapping molecules through photoassociative processes leverages the exponential increase in spontaneous emission rates into bound states as temperature decreases. By coupling excited molecular states to high-finesse optical cavities, we can significantly enhance the radiative lifetime, facilitating efficient trapping of molecules. Our methods have demonstrated the capability to capture up to 10^5 molecules per cubic centimeter within a single-mode optical resonator. Finally, we briefly discuss potential future directions for research in this dynamic field, emphasizing the exciting possibilities that lie ahead in molecular cooling and manipulation.",
        "ori-fast-z-score": 0.3916302249939787,
        "water-fast-z-score": 7.183993046875879,
        "rewrite-fast-z-score": -0.9733285267845753
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  NodeTrix: Hybrid Representation for Analyzing Social Networks .\nAbstract:\nSocial networks are ubiquitous in our daily life, and have been studied extensively by researchers across many disciplines. However, the analysis of social network data is often challenging due to its complexity. In this work we propose NodeTrix, an efficient hybrid representation that can be used to analyze large-scale social networks efficiently. We show how NodeTrix can be applied to solve several important problems including community detection, link prediction, node classification, and influence maximization. Our experiments on real-world datasets demonstrate that NodeTrix outperforms state-of-the-art approaches significantly both in terms of efficiency and effectiveness. 1 Introduction Social networks play an increasingly important role in people s lives. They provide us with new ways to communicate with each other, share information, collaborate, or even make friends. As such, they have attracted much attention from researchers across various fields ranging from sociology  1  , psychology  2  , biology  3  , computer science  4  , engineering  5  , etc.. The rapid development of online social media has led to unprecedented growth in the amount of available social network data  6  . For example, Facebook alone now contains more than one billion active users  7  .\nHowever, analyzing large volumes of social network data remains a challenge because it usually involves complex relationships among nodes  8  . To tackle these challenges, recent research efforts focus on developing effective representations for social networks  9  -  11  . These representations aim at capturing different aspects of social networks while being able to scale up well when dealing with massive amounts of data  12  . Among them, matrix factorization techniques  13  -  15  have shown great promise as they allow us to represent social networks using low-rank matrices  16  . Matrix factorization methods decompose a given adjacency matrix into two smaller matrices (i.e., latent factors) which capture structural properties of the original graph  17  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : NodeTrix : Hybrid Representation for Analyzing Social Networks . Abstract : Social networks are ubiquitous in our daily living , and have been studied frequently by researchers across many disciplines .However , the analysis of social group information is often challenging due to its complexity . In this project we develop NodeTrix , an efficient hybrid representation that can be used to analyze large - scale social systems efficiently .We see how NodeTrix can be applied to solve many important problems namely community screening , link modeling , node classification , and influence maximization . Our experiments on real - time datasets prove that NodeTrix outperforms state - of - the - art methods significantly both in terms of efficiency and effectiveness .1 Introduction Social networks take an increasingly important role in persons s lives . They offer us with innovative ways to interact with each other , share data , collaborate , or even get allies .As such , they have garnered many scrutiny from researchers across numerous topics including from economics 1 , psychology 2 , mathematics 3 , computer science 4 , engineering 5 , etc . . The rapid advancement of internet social publishing has led to unprecedented growth in the quantity of available social platform data 6 .For instance , Facebook alone now contains more than one billion active people 7 . However , examining massive numbers of social group information remains a problem because it often includes difficult connections among nodes 8 .To tackle these problems , recent study efforts focus on developing accurate representations for social organizations 9 - 11 . These representations aim at capturing different components of social organizations while being able to scale up well when dealing with massive amounts of data 12 .Among them , matrix factorization techniques 13 - 15 have shown great success as they allow us to depict public networks using reduced - rank matrices 16 . Matrix factorization techniques decompose a given adjacency vector into two smaller matrices ( i . e . , latent variables ) which capture structural properties of the original graph 17 .",
        "rewrite_text": "**Title: NodeTrix: A Hybrid Approach for Social Network Analysis**\n\n**Abstract:** Social networks play an integral role in our everyday lives, serving as platforms for interaction, data sharing, collaboration, and alliance formation. As a result, they have become a focal point of research across various disciplines, including economics, psychology, mathematics, computer science, and engineering. The rapid growth of online social platforms has led to an explosion of available data, exemplified by Facebook's over one billion active users. However, analyzing the intricate relationships within these vast social networks presents significant challenges due to the complex interconnections among nodes. In response to these challenges, we introduce NodeTrix, a novel hybrid representation designed to facilitate the efficient analysis of large-scale social systems. NodeTrix effectively addresses several critical problems in social network analysis, including community detection, link prediction, node classification, and influence maximization. Our empirical evaluations, conducted on real-world datasets, demonstrate that NodeTrix significantly outperforms existing state-of-the-art methodologies in both efficiency and effectiveness. The development of accurate representations for social networks is essential, as it allows researchers to capture the multifaceted nature of social interactions while maintaining scalability in the face of massive datasets. Among the various techniques explored, matrix factorization has emerged as a particularly successful approach, enabling the representation of social networks through reduced-rank matrices. By decomposing an adjacency matrix into two smaller matrices, matrix factorization captures the structural characteristics of the original graph, thereby enhancing our understanding of social dynamics. NodeTrix builds upon these foundational techniques, offering a robust framework for the analysis of complex social systems and paving the way for future research in this rapidly evolving field.",
        "ori-fast-z-score": -0.07647191129018725,
        "water-fast-z-score": 8.84491707929107,
        "rewrite-fast-z-score": -1.3743685418725535
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Multiwavelength study of massive galaxies at z~2. I. Star formation and galaxy growth .\nAbstract:\nWe present the results of an optical to infrared multiwavelength survey of a sample of 12 massive (M* > 10 11 Msun) galaxies in the redshift range 1.9 < z < 2.7, selected using their rest-frame UV colors as Lyman-break analogs. We use deep near-infrared spectroscopy with Keck/NIRSPEC to measure stellar masses for these objects, which are found to be between 3 x 10 11 and 5 x 10 11 Msun. The majority of our targets show evidence for strong starburst activity based on their high equivalent width  OIII  emission lines and large Balmer decrements indicative of dusty star-forming regions. Using Spitzer/IRAC photometry we find that most of these systems have red mid-infrared colors consistent with those expected for evolved stellar populations. However, two of our sources appear bluer than this trend suggesting they may contain significant amounts of obscured AGN activity.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Multiwavelength study of large galaxies at z ~ 2 . I .Star formation and galaxy expansion . Abstract : We present the conclusion of an optical to infrared multiwavelength survey of a sample of 12 huge ( M * > 10 11 Msun ) galaxies in the redshift range 1 . 9 < z < 2 . 7 , selected using their rest - frame UV colors as Lyman - break analogs .We use deep near - infrared spectroscopy with Keck / NIRSPEC to measure stellar masses for these objects , which are found to be between 3 x 10 11 and 5 x 10 11 Msun . The majority of our targets show proof for strong starburst activity based on their high equivalent height OIII emission lines and large Balmer decrements indicative of dusty star - creating areas .Using Spitzer / IRAC photometry we find that most of these systems have red middle - infrared colors consistent with those expected for advanced stellar regions . However , two of our sources appear bluer than this shift suggesting they may contain significant amounts of distorted AGN activity .",
        "rewrite_text": "We present the findings of a comprehensive multiwavelength survey conducted on a sample of 12 massive galaxies (with stellar masses exceeding 10^11 Msun) located within the redshift range of 1.9 to 2.7. These galaxies were selected based on their rest-frame ultraviolet colors, identifying them as Lyman-break analogs. Utilizing deep near-infrared spectroscopy from the Keck Observatory's NIRSPEC instrument, we successfully measured the stellar masses of these galaxies, which range from 3 x 10^11 to 5 x 10^11 Msun. Our analysis reveals that the majority of these galaxies exhibit strong starburst activity, as evidenced by their high equivalent width of OIII emission lines and significant Balmer decrements, both of which indicate the presence of dusty star-forming regions. Additionally, we employed Spitzer/IRAC photometry to assess the mid-infrared properties of these systems. The results indicate that most of the galaxies possess red mid-infrared colors, aligning with expectations for evolved stellar populations. However, two of the galaxies in our sample display bluer mid-infrared colors, suggesting the potential influence of active galactic nucleus (AGN) activity, which may be distorting their spectral characteristics. This study enhances our understanding of the star formation processes and the evolutionary state of large galaxies at high redshifts, shedding light on the complex interplay between star formation and AGN activity in the early universe.",
        "ori-fast-z-score": 2.1376670117594454,
        "water-fast-z-score": 4.919349550499537,
        "rewrite-fast-z-score": 0.8
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Low-temperature electron dephasing time in AuPd revisited .\nAbstract:\nWe have studied the low temperature (T < 1 K) electron spin relaxation rate, T1e-1/T1e = 1/(1 + T2e), for gold-palladium alloys with different compositions by using pulsed muon-spin rotation and relaxation measurements. The results show that the observed relaxation rates are consistent with those expected from Elliott-Yafet scattering mechanism at high temperatures but deviate significantly below 0.5 K. We find that this deviation can be explained if we assume an additional contribution to the relaxation rate due to spin-orbit interaction between conduction electrons and localized magnetic moments associated with Pd atoms. This is supported by our theoretical calculations based on density functional theory which predict a large enhancement of the spin-orbit coupling strength as one moves away from the center of the Brillouin zone towards the Fermi surface. Our findings suggest that the presence of localised magnetic moment may play an important role in determining the transport properties of these materials even though they do not order magnetically down to lowest measured temperatures.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Low - temperature electron dephasing time in AuPd revisited . Abstract : We have researched the small temperature ( T < 1 K ) electron spin relaxation speed , T1e - 1 / T1e = 1 / ( 1 + T2e ) , for gold - palladium alloys with various compositions by using pulsed muon - spin rotation and relaxation measurements .The results show that the seen relaxation levels are compatible with those expected from Elliott - Yafet scattering mechanism at high temperatures but deviate substantially below 0 . 5 K . We see that this deviation can be described if we suppose an additional contribution to the relaxation frequency due to spinning - orbit interaction between conduction electrons and localized magnetic moments identified with Pd ions . This is backed by our theory experiments using on density functional theory which predict a large enhancement of the spin - orbit interaction strength as one moves away from the center of the Brillouin zone towards the Fermi surface .Our findings show that the presence of localised magnetic moment may play an important role in determining the travel properties of these objects even though they do not order magnetically down to lowest measured temperatures .",
        "rewrite_text": "Title: Revisiting Low-Temperature Electron Dephasing Time in AuPd Alloys\n\nAbstract: In this study, we investigate the electron spin relaxation dynamics at low temperatures (T < 1 K) in gold-palladium (AuPd) alloys with varying compositions, utilizing pulsed muon-spin rotation and relaxation techniques. Our analysis focuses on the electron spin relaxation rate, characterized by the relationship T1e - 1 / T1e = 1 / (1 + T2e). The experimental results indicate that the observed relaxation rates align with the expectations derived from the Elliott-Yafet scattering mechanism at elevated temperatures. However, a significant deviation from this behavior is noted as the temperature drops below 0.5 K. We propose that this deviation can be attributed to an additional relaxation contribution arising from spin-orbit interactions between conduction electrons and localized magnetic moments associated with palladium (Pd) ions. This hypothesis is supported by theoretical calculations based on density functional theory, which predict a substantial increase in the strength of spin-orbit coupling as one approaches the Fermi surface from the center of the Brillouin zone. Our findings suggest that localized magnetic moments play a crucial role in influencing the transport properties of AuPd alloys, despite the absence of magnetic ordering down to the lowest measured temperatures. This research provides new insights into the fundamental mechanisms governing electron dephasing in these materials and highlights the importance of considering spin-orbit interactions in the analysis of low-temperature electron dynamics.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.081063304179084,
        "rewrite-fast-z-score": 0.09407208683835973
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  New Parameterization in Muon Decay and the Type of Emitted Neutrino. II .\nAbstract:\nWe have studied the decay of muons into two neutrinos, one electron-positron pair and one photon by using the Feynman diagram method. We found that there are three types of emitted neutrinos; type I is emitted when the energy of the emitted photon is less than half of the rest mass of the muon; type II is emitted when the energy is greater than or equal to half but less than twice the rest mass of themuon;andtype IIIisemittedwhentheenergyofthespin-1/2particlesissmallerthantwice therespectivestandardmass.Theprobabilitydistributionfunctionsforthese3typesareshown  for different values of the parameter x = Eγ / (2 mμ). The results obtained here agree with those given previously. \n \n Keywords: muon decay, neutrino emission, Feynman diagram technique, probability distribution function, standard model",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  New Parameterization in Muon Decay and the Type of Emitted Neutrino.II .Abstract : We have researched the decay of muons into two neutrinos , one electron - positron pair and one photon by using the Feynman diagram method . We showed that there are three categories of emitted neutrinos ; type I is emitted when the electricity of the emitted photon is greater than quarter of the remainder mass of the muon ; type II is emitted when the power is greater than or equal to quarter but less than times the remainder mass of themuon ; andtype IIIisemittedwhentheenergyofthespin - 1 / 2particlesissmallerthantwice therespectivestandardmass . Theprobabilitydistributionfunctionsforthese3typesareshown for different expressions of the parameter x = Eγ / ( 2 mμ ) .The results derived here agree with those given earlier . Keywords : muon radiation , neutrino emission , Feynman diagram method , probability distribution relation , standard model",
        "rewrite_text": "**Title:** New Parameterization in Muon Decay and the Type of Emitted Neutrino\n\n**Abstract:** In this study, we investigate the decay process of muons into two neutrinos, an electron-positron pair, and a photon, employing the Feynman diagram technique for our analysis. Our findings categorize the emitted neutrinos into three distinct types based on the energy characteristics of the emitted photon relative to the muon's remaining mass. Specifically, Type I neutrinos are produced when the energy of the emitted photon exceeds one-quarter of the muon's residual mass. Type II neutrinos are emitted when the photon energy is at least one-quarter but less than one times the remaining mass of the muon. Lastly, Type III neutrinos are generated when the energy of the spin-1/2 particles is less than twice the corresponding standard mass. We present the probability distribution functions for these three types of neutrinos, which are derived for various values of the parameter \\( x = \\frac{E_{\\gamma}}{2 m_{\\mu}} \\). Our results are consistent with previous studies in the field, reinforcing the validity of our parameterization approach. This research contributes to the understanding of muon decay processes and the characteristics of neutrino emissions, providing a clearer framework for future investigations in particle physics. \n\n**Keywords:** muon radiation, neutrino emission, Feynman diagram method, probability distribution relation, standard model.",
        "ori-fast-z-score": 0.14002800840280097,
        "water-fast-z-score": 4.714951667914447,
        "rewrite-fast-z-score": 1.078327732034384
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Fuzzballs with internal excitations .\nAbstract:\nWe propose that the fuzzball proposal for black holes can be extended to include internal degrees of freedom, which are excited by infalling matter and produce Hawking radiation.  We show how this idea fits into the framework of string theory in AdS/CFT correspondence. The proposed model is based on an extension of the work done by Horowitz and Maldacena (HM) who showed that the entropy of extremal Kerr-Newman black hole agrees exactly with the microscopic counting of states in N=4 super Yang-Mills gauge theory at strong coupling. In our case we consider non-extremal black holes whose entropy also matches with the number of microstates in strongly coupled field theories but now including internal degrees of freedom. This leads us to conclude that the emission spectrum of these black holes should agree with the one predicted by Hawking s original calculation. Finally, we discuss some open questions related to this new picture of black holes as well as possible experimental tests.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Fuzzballs with internal excitations . Abstract : We suggest that the fuzzball idea for black holes can be extended to consider inner degrees of liberty , which are excited by infalling matter and produce Hawking radiation .We see how this idea fits into the framework of string theory in AdS / CFT relationship . The proposed theory is based on an extension of the paper done by Horowitz and Maldacena ( HM ) who demonstrated that the entropy of extremal Kerr - Newman white hole agrees exactly with the microscopic counting of states in N = 4 super Yang - Mills gauge theory at weak interaction .In our case we study non - extremal black holes whose entropy also matches with the proportion of microstates in strongly coupled field theories but now including internal degrees of liberty . This leads us to observe that the emission spectrum of these black holes should comply with the one expected by Hawking s original calculation .Finally , we explain some open questions related to this new picture of black holes as well as possible experimental tests .",
        "rewrite_text": "Title: Fuzzballs with Internal Excitations\n\nAbstract: In this article, we propose an extension of the fuzzball concept for black holes by incorporating internal degrees of freedom that can be excited by infalling matter, resulting in the generation of Hawking radiation. This framework is situated within the context of string theory, particularly through the AdS/CFT correspondence. Our work builds upon the foundational research conducted by Horowitz and Maldacena (HM), who established that the entropy of extremal Kerr-Newman white holes aligns precisely with the microscopic state counting in N=4 super Yang-Mills gauge theory under weak coupling conditions. In contrast, our investigation focuses on non-extremal black holes, where we find that their entropy also corresponds to the number of microstates in strongly coupled field theories, now taking into account the internal degrees of freedom. This leads us to conclude that the emission spectrum of these black holes should conform to the predictions made by Hawking's original calculations. Furthermore, we address several unresolved questions that arise from this novel perspective on black holes and discuss potential experimental approaches to test these ideas. Our findings suggest a deeper understanding of black hole thermodynamics and the role of internal excitations, paving the way for future research in this intriguing area of theoretical physics.",
        "ori-fast-z-score": -0.21566554640687682,
        "water-fast-z-score": 3.9668163788998405,
        "rewrite-fast-z-score": 1.507556722888818
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  On the stratified dust distribution of the GG Tau circumbinary ring .\nAbstract:\nWe present new near-infrared (NIR) polarimetric observations of the GG Tau system, which reveal that its circumstellar disk is highly structured and contains several bright regions with different polarization properties. The most prominent feature in our data set is an arc-like structure located at about 0.5 arcsec to the south-east of the central binary star. This region shows strong polarized emission up to 10% of the total intensity and has been previously identified as a reflection nebula by Weintraub et al. (1993) . We find that this feature can be explained by scattering off optically thin dust grains close to the midplane of the disk. In addition we detect two other bright features on either side of the central binary. These are also associated with high degrees of linear polarization but show no clear evidence for scattered light. Instead they appear to be caused by absorption against the background stellar flux. Finally, we identify three additional fainter structures in the southern part of the disk. All these features have similar polarization angles indicating that their origin may be related.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : On the stratified dust pattern of the GG Tau circumbinary ring . Abstract : We report new near - infrared ( NIR ) polarimetric studies of the GG Tau system , which confirm that its circumstellar disk is heavily structured and hosts multiple bright regions with various polarization properties .The most notable feature in our information run is an arc - like structure located at about 0 . 5 arcsec to the south - eastward of the main binary star . This region shows intense polarized emission up to 10 % of the total intensity and has been previously described as a mirror nebula by Weintraub et al .( 1993 ) . We see that this phenomenon can be described by scattering off optically thin dust grains next to the midplane of the disk .In addition we perceive two other bright features on either front of the main binary . These are also associated with high degrees of linear polarization but display no clear proof for dispersed light .Instead they appear to be caused by absorption against the background stellar flux . Finally , we identify three extra fainter objects in the southern portion of the disk .All these characteristics have similar polarization angles indicating that their ancestry may be connected .",
        "rewrite_text": "We present new near-infrared (NIR) polarimetric observations of the GG Tau system, revealing significant structural complexity within its circumstellar disk. Our findings confirm the presence of multiple bright regions, each exhibiting distinct polarization characteristics. A particularly striking feature of our study is an arc-like structure located approximately 0.5 arcseconds southeast of the primary binary star. This area is characterized by intense polarized emission, reaching up to 10% of the total intensity, and has previously been referred to as a mirror nebula by Weintraub et al. (1993). Our analysis suggests that this phenomenon can be attributed to scattering processes involving optically thin dust grains situated near the midplane of the disk.\n\nIn addition to the arc-like feature, we have identified two other prominent bright regions flanking the main binary. These regions also exhibit high degrees of linear polarization; however, they lack clear evidence of dispersed light. Instead, their polarization appears to result from absorption against the background stellar light, indicating a different mechanism at play compared to the arc-like structure.\n\nFurthermore, we have detected three additional fainter objects in the southern region of the disk. Notably, all identified features share similar polarization angles, suggesting a potential common origin or connection among them. This study enhances our understanding of the stratified dust distribution within the GG Tau circumbinary ring and provides insights into the underlying physical processes shaping the observed polarization patterns. Our results contribute to the broader knowledge of circumstellar disk structures and their interactions with surrounding materials, paving the way for future investigations into the dynamics and evolution of such systems.",
        "ori-fast-z-score": -0.7258661863112977,
        "water-fast-z-score": 5.363390480545726,
        "rewrite-fast-z-score": 0.9538209664765319
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Old Main-Sequence Turnoff Photometry in the Small Magellanic Cloud. I. Constraints on the Star Formation History in Different Fields .\nAbstract:\nWe present new photometric data for stars near the main-sequence turn-off (MSTO) in three fields in the Small Magellanic: Cloud, obtained with the Wide Field Imager at the MPG/ESO 2.2 m telescope. The observations were carried out during two runs in August and September 2002. We have used these data to derive ages and metallicities for about 1000 MSTO stars using theoretical isochrones. Our results show that there are significant differences between the age distributions derived for different fields. In particular we find evidence for an extended period of star formation in field 1, which contains the open cluster NGC 1818. This result supports previous suggestions based on studies of red giant branch stars. \n \n Keywords: Age distribution; Open clusters; Small Magellanic Cloud; Stellar population synthesis; Wide-field imaging. Subject headings: Galaxy: Magellanic Clouds",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  Old Main-Sequence Turnoff Photometry in the Small Magellanic Cloud.I.Constraints on the Star Formation History in Different Fields . Abstract : We report new photometric data for stars near the main - sequence turn - off ( MSTO ) in three fields in the Small Magellanic : Cloud , obtained with the Wide Field Imager at the MPG / ESO 2 . 2 m observatory .The surveys were carried out during two runs in August and September 2002 . We have utilized these information to derive ages and metallicities for about 1000 MSTO stars using theoretical isochrones .Our results show that there are significant variations between the age distributions obtained for different fields . In particular we find support for an extended period of star formation in field 1 , which contains the open cluster NGC 1818 .This result confirms previous suggestions based on analyses of red giant branch stars . Keywords : Age distribution ; Open clusters ; Small Magellanic Cloud ; Stellar population analysis ; Wide - area imaging .Subject headings: Galaxy: Magellanic Clouds",
        "rewrite_text": "Title: Old Main-Sequence Turnoff Photometry in the Small Magellanic Cloud: I. Constraints on the Star Formation History in Various Regions\n\nAbstract: In this study, we present new photometric observations of stars located near the main-sequence turnoff (MSTO) in three distinct fields within the Small Magellanic Cloud (SMC). These observations were conducted using the Wide Field Imager at the MPG/ESO 2.2 m telescope during two observational campaigns in August and September 2002. By analyzing this data, we have successfully derived ages and metallicities for approximately 1,000 MSTO stars, employing theoretical isochrones as a framework for our analysis. Our findings reveal notable discrepancies in the age distributions of stars across the different fields surveyed. Specifically, we observe evidence supporting an extended period of star formation in field 1, which is home to the open cluster NGC 1818. This observation aligns with earlier studies that suggested a prolonged star formation history in this region, based on analyses of red giant branch stars. The implications of our results contribute to a deeper understanding of the stellar population dynamics within the SMC and provide valuable constraints on the star formation history in this galaxy. Our research highlights the importance of wide-area imaging in uncovering the complexities of stellar evolution and distribution in the Magellanic Clouds. \n\nKeywords: Age distribution; Open clusters; Small Magellanic Cloud; Stellar population analysis; Wide-area imaging. \n\nSubject headings: Galaxy: Magellanic Clouds.",
        "ori-fast-z-score": 1.1338934190276817,
        "water-fast-z-score": 4.75,
        "rewrite-fast-z-score": 2.592379236826063
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Cross-disciplinary Framework for the Description of Contextually Mediated Change .\nAbstract:\nThe purpose of this article is to present an interdisciplinary framework that can be used in describing and analyzing contextually mediated change (CMC). The proposed framework consists of three main components, namely, (1) contextual mediation, (2) individual agency, and (3) social structure. These components are interrelated through two key processes -contextualization and recontextualization. In addition, we propose four dimensions along which CMC may occur. Finally, we discuss how our framework contributes to existing theories on organizational change as well as some potential applications of it. Keywords: Organizational change; Individual agency; Social structure; Contextualization; Recontextualization. This article presents an interdisciplinary framework that describes and analyzes contextually mediated change (CMChange). We argue that CMChange occurs when individuals engage with their contexts by interpreting them differently than they were previously interpreted. As such, CMChange involves both contextualization and recontextualizatton. Contextualization refers to the process whereby individuals make sense of their contexts based on their own experiences and perspectives. Recontextualization refers to the act of reinterpreting one s interpretation of his/her context(s), thereby changing its meaning and/or significance. Our framework also proposes four dimensions along which CMChange may take place. First, there is the dimension of time. Second, there is the dimension",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Cross - disciplinary Framework for the Description of Contextually Mediated Change . Abstract : The purpose of this article is to provide an interdisciplinary framework that can be used in documenting and analyzing contextually mediated shift ( CMC ) .The proposed framework consists of three principal processes , namely , ( 1 ) contextual mediation , ( 2 ) individual agency , and ( 3 ) organizational structure . These components are interrelated through two essential cycles - contextualization and recontextualization .In addition , we propose four dimensions along which CMC could occur . Finally , we discuss how our paradigm contributes to existing ideas on organizational change as well as some potential applications of it .Keywords : Organizational transformation ; Individual agency ; Social building ; Contextualization ; Recontextualization . This page presents an interdisciplinary framework that describes and analyzes contextually mediated shift ( CMChange ) .We argue that CMChange results when individuals engage with their contexts by interpreting them differently than they were formerly interpreted . As such , CMChange means both contextualization and recontextualizatton .Contextualization refers to the process whereby adults making sense of their contexts based on their own experiences and views . Recontextualization refers to the activity of reinterpreting one s perception of his / her experience ( s ) , thereby shifting its value and / or importance .Our paradigm also proposes four dimensions along which CMChange might taking place . First , there is the dimension of time .Second, there is the dimension",
        "rewrite_text": "**Title:** A Cross-Disciplinary Framework for the Description of Contextually Mediated Change\n\n**Abstract:** This article aims to establish an interdisciplinary framework for the documentation and analysis of contextually mediated change (CMC). The framework is built upon three core processes: (1) contextual mediation, (2) individual agency, and (3) organizational structure. These processes are interconnected through two fundamental cycles: contextualization and recontextualization. Contextualization involves individuals making sense of their environments based on personal experiences and perspectives, while recontextualization entails the reinterpretation of these experiences, leading to a shift in their perceived value or significance. Furthermore, we identify four dimensions along which CMC can manifest, with the first dimension being time. The subsequent dimensions will be elaborated upon in the article. Our framework not only enhances the understanding of organizational change but also offers practical applications across various fields. By emphasizing the interplay between individual agency and organizational structures within specific contexts, we provide insights into how shifts occur and can be effectively managed. This paradigm contributes to the existing literature on organizational transformation and social dynamics, highlighting the importance of context in shaping change processes. The implications of our findings suggest that recognizing and leveraging the nuances of contextual mediation can lead to more effective strategies for fostering change within organizations. \n\n**Keywords:** Organizational transformation; Individual agency; Social construction; Contextualization; Recontextualization.",
        "ori-fast-z-score": -1.4439897447623107,
        "water-fast-z-score": 5.9196002117260145,
        "rewrite-fast-z-score": 0.5129891760425771
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Weight structures vs. $t$-structures; weight filtrations, spectral sequences, and complexes (for motives and in general) .\nAbstract:\nThe purpose of this article is to give an overview of the theory of weight structures on triangulated categories developed by A. Bondal and M. Kapranov.  We will explain how it can be applied to construct new cohomology theories for algebraic varieties over finite fields or number fields.   In particular we will discuss the construction of motivic cohomology using weight structures on derived categories of mixed Tate motives.    The main results are due to J. Ayoub, D. Gaitsgory, R. Hain, S. Katzarkov, V. Lafforgue, C. Soulé, B. Stienstra, and others. This article was written as part of the author s Ph.D thesis at Utrecht University under supervision of Prof. Dr. Wim van der Kallen. It has been published online by the author with permission of the supervisor. For more information about the content see the introduction below.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Weight systems vs . $ t $ - systems ; weight filtrations , spectral patterns , and complexes ( for motives and in general ) . Abstract : The purpose of this page is to give an overview of the notion of weight forms on triangulated categories pioneered by A . Bondal and M . Kapranov .We will explain how it can be applied to build new cohomology theories for algebraic fields over finite fields or number fields . In particular we will explore the creation of motivic cohomology involving weight forms on derived categories of mixed Tate motives .The main results are due to J . Ayoub , D . Gaitsgory , R . Hain , S . Katzarkov , V . Lafforgue , C . Soulé , B . Stienstra , and others . This page was written as part of the writer s Ph . D thesis at Utrecht University under supervision of Prof . Dr . Wim van der Kallen .It has been publication online by the writer with authorization of the supervisor . For more information about the content read the introduction below .",
        "rewrite_text": "Title: Weight Systems vs. $t$-Systems: Weight Filtrations, Spectral Patterns, and Complexes (for Motives and Beyond)\n\nAbstract: This article provides a comprehensive overview of the concept of weight forms within triangulated categories, a framework initially developed by A. Bondal and M. Kapranov. We delve into the application of these weight forms to establish novel cohomology theories pertinent to algebraic fields, particularly those defined over finite fields or number fields. A significant focus of this work is the development of motivic cohomology, which incorporates weight forms on derived categories of mixed Tate motives. The findings presented herein are largely attributed to the contributions of notable mathematicians including J. Ayoub, D. Gaitsgory, R. Hain, S. Katzarkov, V. Lafforgue, C. Soulé, B. Stienstra, among others. This document is part of the author's Ph.D. thesis conducted at Utrecht University, under the guidance of Prof. Dr. Wim van der Kallen. The publication of this work online has been authorized by the supervisor. For further insights into the subject matter, readers are encouraged to consult the introduction provided below.",
        "ori-fast-z-score": -0.4923659639173309,
        "water-fast-z-score": 4.608176875690327,
        "rewrite-fast-z-score": -0.12216944435630522
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Non-equilibrium coherence dynamics in one-dimensional Bose gases .\nAbstract:\nWe study the non-equilibrium evolution of an interacting onedimensional Bose gas initially prepared in a coherent state with finite particle number fluctuations and show that it exhibits universal features, which are independent of microscopic details such as interactions or initial conditions. We find that the system evolves into a stationary state characterized by non-vanishing density-density correlations at all distances. The time dependence of these correlations is governed by a single parameter, which we identify with the inverse temperature of the final equilibrium state. This allows us to determine this temperature directly from experimental data without any fitting parameters. Our results provide new insights into the nonequilibrium physics of quantum many-body systems and may be tested experimentally using ultracold atoms trapped in optical lattices. \nI. INTRODUCTORY REMARK\nThe recent development of techniques for trapping and manipulating cold atomic gases has opened up exciting possibilities for studying strongly correlated quantum matter far from thermal equilibrium  1  . In particular, experiments have demonstrated how isolated quantum systems can evolve towards their ground states  2  , while being driven out of equilibrium by sudden changes in external control parameters  3  .\nIn this work, we consider the case where the system is suddenly quenched across a phase transition  4  . For example, if the particles were originally confined to a harmonic trap, they would expand freely after switching off the confining potential  5  . Alternatively, the system could be initialized in its ground state  6  before undergoing a rapid change in some other parameter (e.g., magnetic field)  7, 8  . In both cases, the subsequent relaxation process will depend crucially on whether the system was initially prepared close to equilibrium  9  or not  10  . If the latter situation applies, then the system typically relaxes towards a metastable state  11  whose properties cannot be inferred from those of the original equilibrium ensemble  12  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Non - equilibrium coherence dynamics in one - dimensional Bose gases . Abstract : We research the non - equilibrium evolution of an interacting onedimensional Bose gas initially produced in a coherent state with discrete particle size fluctuations and find that it displays universal features , which are independent of microscopic information such as interactions or initial conditions .We see that the system evolves into a stationary state characterized by non - vanishing density - density correlations at all distances . The period dependence of these correlations is governed by a single parameter , which we identify with the inverse temperature of the finished equilibrium state .This enables us to predict this heat directly from experimental evidence without any fitting values . Our results bring fresh insights into the nonequilibrium dynamics of quantum several - bodies systems and may be evaluated experimentally utilizing ultracold atoms trapped in laser lattices .I . INTRODUCTORY REMARK The rapid progress of techniques for trapping and manipulating cool nuclear gases has opened up interesting possibilities for studying strongly interacting quantum matter far from temperature equilibrium 1 .In particular , observations have demonstrated how isolated quantum systems can evolve towards their ground states 2 , while being driven out of equilibrium by sudden variations in external control factors 3 . In this research , we imagine the case where the system is suddenly quenched across a phase change 4 .For instance , if the particles were first confined to a harmonic trap , they may expand freely after switching off the confining potential 5 . Alternatively , the system could be initialized in its ground state 6 before undergoing a rapid change in some other parameter ( e . g . , magnetic force ) 7 , 8 .In both cases , the subsequent relaxed process will depend crucially on whether the process was initially developed next to equilibrium 9 or not 10 . If the latter situation applies , then the system typically relaxes towards a metastable state 11 whose characteristics cannot be inferred from those of the original equilibrium ensemble 12 .",
        "rewrite_text": "Title: Non-Equilibrium Coherence Dynamics in One-Dimensional Bose Gases\n\nAbstract: In this study, we investigate the non-equilibrium dynamics of an interacting one-dimensional Bose gas that is initially prepared in a coherent state with discrete fluctuations in particle size. Our findings reveal that the system exhibits universal characteristics that remain unaffected by specific microscopic details, such as interaction strengths or initial conditions. As the system evolves, it transitions into a stationary state marked by persistent density-density correlations across all distances. Notably, the periodicity of these correlations is determined by a single parameter, which we associate with the inverse temperature of the resulting equilibrium state. This relationship allows us to directly infer the temperature from experimental observations without the need for fitting parameters. Our results contribute valuable insights into the non-equilibrium behavior of quantum many-body systems and can be experimentally validated using ultracold atoms confined in optical lattices.\n\nThe rapid advancements in techniques for trapping and manipulating ultracold atomic gases have opened new avenues for exploring strongly interacting quantum matter in non-equilibrium conditions. Recent observations have highlighted how isolated quantum systems can evolve toward their ground states while being perturbed by abrupt changes in external parameters. In our research, we consider scenarios where the system undergoes a sudden quench across a phase transition. For example, particles initially confined within a harmonic trap may be allowed to expand freely upon the removal of the confining potential. Alternatively, the system could be prepared in its ground state before experiencing a rapid alteration in another parameter, such as an external magnetic field. The subsequent relaxation process is critically dependent on whether the initial conditions were close to equilibrium or not. In cases where the system starts far from equilibrium, it typically relaxes into a metastable state, the properties of which cannot be directly inferred from the original equilibrium ensemble.",
        "ori-fast-z-score": -0.9370425713316364,
        "water-fast-z-score": 6.715471761210061,
        "rewrite-fast-z-score": 0.16329931618554522
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Minimal size of a barchan dune .\nAbstract:\nWe study the minimal size of a barchan, which is one of the most common types of sand dunes in nature. We show that this problem can be formulated as an optimal control problem for a nonlinear partial differential equation with nonlocal boundary conditions and prove existence of solutions by using variational methods. The numerical results are obtained by applying finite element method to discretize the state equations and then solving them by Newton s iteration scheme. Finally we present some examples illustrating our theoretical findings. Sand dunes are among the most beautiful natural phenomena on Earth. They have been studied extensively since the 19th century  1  . One of the simplest forms of sand dunes is called barchan  2  , see Figure 1 (a). It has a crescent shape with its horns pointing away from the wind direction. Barchans occur naturally over large areas around the world such as Australia  3  , Namibia  4  , Saudi Arabia  5  , China  6  , Japan  7  , etc.. In recent years there has been growing interest in studying mathematical models describing formation of sand dunes  8, 9, 10  .\nIn this work we consider the following model proposed by Kroy et al  11  : \nwhere u(x) denotes the height of the sand bed at position x ∈ Ω =  0, L  × R + ; f > 0 represents the rate of deposition; g ≥ 0 stands for the erosion coefficient; h(u) describes the effect of surface tension; p(x), q(x) represent the pressure terms due to gravity and friction respectively; α > 0 measures the strength of the wind blowing along x-axis; β > 0 characterizes the resistance against the flow of air; γ > 0 is related to the cohesion between grains of sand; θ is the angle of repose of sand particles; c > 0 is the constant volume fraction of sand per unit area; finally, n is the outward normal vector to the boundary Γ = {0 < x < L} × {0} ∪ {L} × R + . For more details about physical meaning of parameters involved in system (1) , please refer to  12  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Minimal size of a barchan dune . Abstract : We consider the minimal size of a barchan , which is one of the most common kinds of dunes dunes in nature .We see that this question can be formulated as an optimal control problem for a nonlinear partial differential function with nonlocal boundary constraints and prove existence of solutions by using variational techniques . The mathematical findings are derived by using finite element method to discretize the state equations and then solving them by Newton s iteration scheme .Finally we present some examples illustrating our theoretical results . Sand dunes are among the most beautiful natural creatures on Earth .They have been studied thoroughly since the 19th century 1 . One of the simplest forms of dunes dunes is known barchan 2 , see Figure 1 ( a ) .It has a crescent shape with its horns pointing away from the wind position . Barchans occur commonly over large areas around the world such as Australia 3 , Namibia 4 , Saudi Arabia 5 , China 6 , Japan 7 , etc . .In recent recently there has been growing interest in studying numerical models governing formation of dunes dunes 8 , 9 , 10 . In this study we consider the following model proposed by Kroy et al 11 : where u ( x ) denotes the height of the sand bed at position x ∈ Ω = 0 , L × R + ; f > 0 represents the speed of deposition ; g ≥ 0 stands for the erosion factor ; h ( u ) refers the impact of surface friction ; p ( x ) , q ( x ) describe the pressure terms due to gravity and tension respectively ; α > 0 measures the strength of the wind blowing along x - axis ; β > 0 characterizes the tolerance against the movement of air ; γ > 0 is related to the cohesion between particles of dunes ; θ is the angle of repose of sand grains ; c > 0 is the constant volume fraction of dunes per unit area ; finally , n is the outward normal vector to the boundary Γ = { 0 < x < L } × { 0 } ∪ { L } × R + .For more details about physical context of components involved in system ( 1 ) , please refer to 12 .",
        "rewrite_text": "**Title: Minimal Size of a Barchan Dune**\n\n**Abstract:** This study investigates the minimal size of barchan dunes, which are among the most prevalent types of dunes found in natural environments. The problem is framed as an optimal control issue involving a nonlinear partial differential equation with nonlocal boundary conditions. We demonstrate the existence of solutions through the application of variational methods. To analyze the mathematical aspects, we employ the finite element method to discretize the governing state equations, subsequently solving them using Newton's iteration technique. Our findings are supported by various examples that illustrate the theoretical results obtained. Sand dunes, particularly barchans, are not only aesthetically striking but also have been the subject of extensive research since the 19th century. Characterized by their crescent shape with horns oriented away from the wind, barchans are widely distributed across diverse regions, including Australia, Namibia, Saudi Arabia, China, and Japan. Recently, there has been an increasing interest in developing numerical models that describe the formation of these dunes. In this work, we focus on a model proposed by Kroy et al., which incorporates several parameters: \\( u(x) \\) represents the height of the sand bed at position \\( x \\in \\Omega = (0, L) \\times \\mathbb{R}^+ \\); \\( f > 0 \\) indicates the deposition speed; \\( g \\geq 0 \\) denotes the erosion factor; \\( h(u) \\) accounts for surface friction effects; \\( p(x) \\) and \\( q(x) \\) characterize pressure contributions from gravity and tension, respectively; \\( \\alpha > 0 \\) quantifies the wind strength along the x-axis; \\( \\beta > 0 \\) reflects air movement resistance; \\( \\gamma > 0 \\) pertains to particle cohesion; \\( \\theta \\) is the angle of repose for sand grains; \\( c > 0 \\) is the constant volume fraction of dunes per unit area; and \\( n \\) is the outward normal vector to the boundary \\( \\Gamma = \\{0 < x < L\\} \\times \\{0\\} \\cup \\{L\\} \\times \\mathbb{R}^+ \\). For a comprehensive understanding of the physical context of the components involved in this system, we refer to previous works.",
        "ori-fast-z-score": -1.365472859134248,
        "water-fast-z-score": 5.4443572293729625,
        "rewrite-fast-z-score": 0.242535625036333
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Complementarity in the Einstein-Bohr Photon Box .\nAbstract:\nWe present an experimental investigation into complementarity and quantum nonlocality using a photonic implementation of the Einstein-Bohr photon box thought experiment. We demonstrate that our system exhibits both classical correlations, which are consistent with local hidden variable theories, as well as quantum correlations, which cannot be explained by any such theory. Our results show that this system is capable of exhibiting all three types of Bell inequalities simultaneously. The Einstein-Bohr (EB) photon-box  1  , also known as the EPRB  2  or the two-slit experiment  3  , has been used to investigate many aspects of quantum mechanics including entanglement  4  , Bell s theorem  5  , and quantum teleportation  6  . In its original form it consists of a source emitting pairs of photons at random times; one photon passes through a beam splitter while the other travels directly towards a detector. If we measure whether each photon arrives at either output port of the beam splitter then there will always be exactly one photon arriving at each detector. This measurement can be performed locally on each side without disturbing the state of the other particle. However if instead we perform measurements on both particles jointly then they must arrive together at the same detector  7, 8  .\nIn order for these experiments to exhibit genuine quantum effects, the detectors need to have high efficiency so that the probability of detecting more than one photon per pair is negligible  9  . Previous implementations of EB boxes have relied upon inefficient single-photon counting detectors  10  or inefficient avalanche photo diodes  11  . These devices do not allow us to distinguish between different numbers of detected photons and therefore prevent us from observing truly quantum behaviour  12  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Complementarity in the Einstein - Bohr Photon Box . Abstract : We present an experimental inquiry into complementarity and quantum nonlocality utilizing a photonic implementation of the Einstein - Bohr photon box thought experiment .We showed that our system displays both classical correlations , which are compatible with local hidden variable theories , as well as particle correlations , which cannot be described by any such theory . Our results show that this scheme is capable of displaying all three sorts of Bell inequalities simultaneously .The Einstein - Bohr ( EB ) photon - box 1 , sometimes known as the EPRB 2 or the two - slit study 3 , has been used to investigate many aspects of quantum mechanics including entanglement 4 , Bell s theorem 5 , and quantum teleportation 6 . In its initial form it consists of a source emitting pairs of photons at random times ; one photon passes through a beam splitter while the other travels directly towards a detector .If we measure whether each photon arrives at either output port of the light splitter then there will always be exactly one photon coming at each sensor . This measurement can be performed locally on each side without disturbing the state of the other particle .However if instead we perform observations on both particles jointly then they must arrive together at the same detector 7 , 8 . In order for these experiments to produce genuine quantum effects , the detectors need to have high efficiency so that the probability of detecting more than one photon per couple is negligible 9 .Previous implementations of EB boxes have relied upon inefficient single - photon counting detectors 10 or inefficient avalanche photographic diodes 11 . These systems do not enable us to distinguish between multiple numbers of identified photons and therefore prevent us from observing truly molecular behaviour 12 .",
        "rewrite_text": "We present a comprehensive experimental investigation into the concepts of complementarity and quantum nonlocality through a photonic realization of the Einstein-Bohr photon box thought experiment. Our findings reveal that the system exhibits both classical correlations, which align with local hidden variable theories, and particle correlations that defy such explanations. This duality allows our setup to simultaneously demonstrate all three types of Bell inequalities, highlighting its versatility in exploring fundamental quantum phenomena. The Einstein-Bohr photon box, also referred to as the EPRB or the two-slit experiment, has been pivotal in examining various facets of quantum mechanics, including entanglement, Bell's theorem, and quantum teleportation. In its original configuration, the experiment involves a source that emits pairs of photons at random intervals; one photon traverses a beam splitter while its counterpart proceeds directly to a detector. When measuring the arrival of each photon at the output ports of the beam splitter, we consistently observe one photon at each sensor. This measurement can be conducted locally on either side without influencing the state of the other photon. However, if we choose to observe both particles together, they must arrive at the same detector. For these experiments to yield authentic quantum effects, it is crucial that the detectors operate with high efficiency, minimizing the likelihood of detecting more than one photon per pair. Previous implementations of the Einstein-Bohr box have often depended on inefficient single-photon counting detectors or suboptimal avalanche photodiodes, which hinder the ability to differentiate between multiple detected photons. Consequently, these limitations have obstructed the observation of genuine quantum behavior. Our work addresses these challenges and advances the understanding of quantum mechanics through a refined experimental approach.",
        "ori-fast-z-score": -0.18569533817705186,
        "water-fast-z-score": 4.125143236626951,
        "rewrite-fast-z-score": 1.1239029738980328
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Global magnetohydrodynamical models of turbulence in protoplanetary disks I. A cylindrical potential on a Cartesian grid and transport of solids .\nAbstract:\nWe present global three-dimensional MHD simulations of the evolution of dust particles embedded in turbulent, stratified accretion disks around young stars. The disk is modeled as an axisymmetric torus with a radial density profile that follows a power law with index p = -1.5. We use two different numerical methods to solve for the gas dynamics: (1) a finite volume method based on the ZEUS code; and (2) a pseudo-spectral method using spherical harmonics expansions. In both cases we assume azimuthal symmetry about the central star. Our main goal is to study how the presence of magnetic fields affects the motion of solids through the disk. To this end, we consider three different initial field configurations: (a) purely toroidal, (b) purely poloidal, and (c) mixed toroidal-poloidal. For each configuration, we perform several runs varying the strength of the initial magnetic field.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Global magnetohydrodynamical models of turbulence in protoplanetary disks I . A cylindrical potential on a Cartesian grid and transport of solids .Abstract : We create global three - dimensional MHD simulations of the evolution of dust particles embedded in turbulent , stratified accretion disks around early stars . The disk is modeled as an axisymmetric torus with a radial density profile that follows a power law with index p = - 1 . 5 .We use two different numerical ways to solve for the gas mechanics : ( 1 ) a finite volume approach using on the ZEUS code ; and ( 2 ) a quasi - spectral method using spherical harmonics expansions . In both cases we assume azimuthal symmetry about the main star .Our main goal is to study how the presence of magnetic fields affects the movement of solids through the disk . To this end , we study three different initial field configurations : ( a ) purely toroidal , ( c ) purely poloidal , and ( c ) mixed toroidal - poloidal .For each configuration , we perform several running varying the strength of the first magnetic force .",
        "rewrite_text": "In this study, we present global three-dimensional magnetohydrodynamic (MHD) simulations that investigate the dynamics of dust particles within turbulent, stratified accretion disks surrounding young stars. The disk is conceptualized as an axisymmetric torus characterized by a radial density profile that adheres to a power law with an index of p = -1.5. To analyze the gas dynamics, we employ two distinct numerical methods: the first is a finite volume approach utilizing the ZEUS code, while the second is a quasi-spectral method that incorporates spherical harmonics expansions. Both methodologies assume azimuthal symmetry around the central star.\n\nThe primary objective of our research is to understand the influence of magnetic fields on the transport of solid particles within the disk. To achieve this, we explore three different initial magnetic field configurations: (a) a purely toroidal field, (b) a purely poloidal field, and (c) a mixed configuration that combines both toroidal and poloidal components. For each magnetic field setup, we conduct a series of simulations that vary the strength of the initial magnetic force to assess its impact on the motion of dust particles.\n\nOur findings aim to elucidate the complex interplay between magnetic fields and solid particle dynamics in protoplanetary disks, which is crucial for understanding the processes of planet formation. By analyzing the results from our simulations, we hope to provide insights into how different magnetic configurations can alter the behavior of dust particles, potentially influencing the efficiency of solid material accumulation necessary for planetesimal formation. This work contributes to the broader understanding of the physical mechanisms at play in the early stages of planetary system development.",
        "ori-fast-z-score": 1.078327732034384,
        "water-fast-z-score": 4.1812388858673994,
        "rewrite-fast-z-score": 1.1272429603813559
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Long Period, Massive Binaries HD 37366 and HD 54662: Potential Targets for Long Baseline Optical Interferometry .\nAbstract:\nWe present the first results on two massive binary systems with periods longer than 100 days that are potential targets for future space-based optical interferometers such as SIM or TPF-I. The primary stars in these binaries have masses between 8 to 12 Msun and their companions are less massive (1-5 Msun). We used high resolution spectroscopy obtained at McDonald Observatory to determine orbital parameters for both systems. In addition we determined stellar atmospheric properties using photometric data taken by Hipparcos satellite and spectroscopic observations made at McDonald Observatory. Our analysis shows that the components of HD 37362 system are very similar while those of HD 54672 differ significantly. This is consistent with our expectations based on evolutionary models which predict that the more massive star will evolve faster and become redder. These results demonstrate that it may be possible to use astrometric measurements combined with radial velocity curves to measure directly mass loss rates due to winds driven by radiation pressure.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Long Period , Massive Binaries HD 37366 and HD 54662 : Potential Targets for Long Baseline Optical Interferometry . Abstract : We report the first findings on two huge binary systems with periods longer than 100 days that are potential targets for future space - based optical interferometers such as SIM or TPF - I .The main stars in these binaries have masses between 8 to 12 Msun and their companions are less massive ( 1 - 5 Msun ) . We utilized large resolution spectroscopy acquired at McDonald Observatory to obtain orbital characteristics for both components .In addition we calculated stellar atmospheric characteristics utilizing photometric data taken by Hipparcos satellite and spectroscopic observations made at McDonald Observatory . Our study shows that the parts of HD 37362 system are very identical while those of HD 54672 vary significantly .This is compatible with our expectations based on phylogenetic models which predict that the more massive star will evolve faster and become redder . These data demonstrate that it could be possible to use astrometric measurements coupled with radial speed curves to measure directly mass loss rates caused to winds driven by radiation stress .",
        "rewrite_text": "We present our initial findings on two significant binary systems, HD 37366 and HD 54662, both of which exhibit orbital periods exceeding 100 days and are promising candidates for future space-based optical interferometry missions, such as the Space Interferometry Mission (SIM) and the Terrestrial Planet Finder Interferometer (TPF-I). The primary stars in these binary systems possess masses ranging from 8 to 12 solar masses (Msun), while their companions are relatively less massive, with masses between 1 and 5 Msun. To derive the orbital characteristics of both components, we employed high-resolution spectroscopy obtained from the McDonald Observatory. Furthermore, we analyzed stellar atmospheric properties using photometric data from the Hipparcos satellite, alongside spectroscopic observations conducted at the McDonald Observatory.\n\nOur analysis reveals that the components of the HD 37366 system exhibit remarkable similarities, whereas the components of HD 54662 display significant variations. These observations align with our expectations based on evolutionary models, which suggest that the more massive star in a binary system tends to evolve at a faster rate, leading to a redder appearance over time. Additionally, our findings indicate the feasibility of utilizing astrometric measurements in conjunction with radial velocity curves to directly assess mass loss rates resulting from radiation-driven stellar winds. This research not only enhances our understanding of these massive binary systems but also underscores their potential as targets for advanced observational techniques in the study of stellar evolution and dynamics.",
        "ori-fast-z-score": -0.32539568672798425,
        "water-fast-z-score": 4.960307567358167,
        "rewrite-fast-z-score": -0.0949157995752499
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Diameters in preferential attachment models .\nAbstract:\nWe study the diameter and clustering coefficient for several classes of random graphs with power law degree distribution, including preferential attachment (PA) networks.  We show that PA networks have small diameters if their growth is slow enough; this result holds even when we allow multiple edges between nodes. For fast growing PA networks, however, there are examples where the diameter grows exponentially as a function of n. In addition to studying the diameter, we also consider the clustering coefficient of these networks. The clustering coefficient measures how well connected neighbors of a node are among themselves. We prove upper bounds on the clustering coefficients of PA networks which hold regardless of the rate at which new nodes join the network. Finally, we present some numerical results showing that our theoretical predictions agree very well with simulations. Keywords: Preferential Attachment Networks; Diameter; Clustering Coefficient. 1 Introduction Random graphs with power-law degree distributions arise naturally in many applications such as social networks  1  , Internet topology  2  , citation networks  3  , etc.. These types of networks can be generated by using various mechanisms known as  network growth models ; see  4  for an overview. One popular model is called  preferential attachment  or  Power-Law Graphs   5  . This class of networks has been studied extensively over the past decade  6  -  8  .\nIn this work, we focus on two important properties of these graphs: the diameter and the clustering coefficient  9  . The diameter D(G) of a graph G = (V, E) is defined as max{d(u, v) | u, v ∈ V }, i.e., it is the maximum distance between any pair of vertices in G. The clustering coefficient C(v) of a vertex v is defined as the fraction of pairs of neighbors of v that are adjacent to each other  10  . It is easy to verify that both quantities depend only on the degrees of the nodes in the graph  11  ; hence they do not change under edge rewirings  12  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Diameters in preferential attachment models . Abstract : We research the diameter and clustering density for numerous classes of random graphs with power law degree distribution , including preferential attachment ( PA ) networks .We see that PA systems have short diameters if their development is slow enough ; this consequence holds even when we allow multiple edges between nodes . For quick expanding PA systems , however , there are examples where the radius grows exponentially as a function of n . In addition to modeling the radius , we also consider the clustering coefficient of these networks .The clustering parameter measures how well connected neighbors of a node are among themselves . We derive upper limits on the clustering coefficients of PA systems which hold regardless of the rate at which additional nodes join the organization .Finally , we present some numerical findings indicating that our theory estimates agree very best with simulations . Keywords : Preferential Attachment Networks ; Diameter ; Clustering Coefficient .1 Introduction Random graphs with power - law degree distributions arise naturally in different users such as community systems 1 , Internet topology 2 , citation connections 3 , etc . . These sorts of networks can be derived by using numerous mechanisms referred as network growth models ; look 4 for an overview .One popular approach is known preferential attachment or Power - Law Graphs 5 . This group of networks has been studied thoroughly over the previous decade 6 - 8 .In this research , we focus on two essential properties of these graphs : the radius and the clustering coefficient 9 . The diameter D ( G ) of a graph G = ( V , E ) is calculated as max { d ( u , v ) | v , v ∈ V } , i . e . , it is the maximum length between any pair of vertices in G . The clustering coefficient C ( v ) of a node v is calculated as the fraction of pairs of neighbors of v that are adjoining to each other 10 .It is easy to confirm that both quantities rely only on the degrees of the nodes in the graph 11 ; thus they do not change under path rewirings 12 .",
        "rewrite_text": "**Title: Diameters in Preferential Attachment Models**\n\n**Abstract:** In this study, we investigate the diameter and clustering density of various classes of random graphs characterized by power law degree distributions, with a particular focus on preferential attachment (PA) networks. Our findings indicate that PA networks exhibit short diameters when their growth rate is sufficiently slow, a result that remains valid even in the presence of multiple edges between nodes. Conversely, in rapidly expanding PA networks, we identify instances where the radius increases exponentially with respect to the number of nodes, n. Alongside our analysis of the radius, we also examine the clustering coefficient of these networks, which quantifies the interconnectedness of a node's neighbors. We establish upper bounds for the clustering coefficients of PA networks that are applicable regardless of the rate at which new nodes are integrated into the network. Furthermore, we present numerical simulations that corroborate our theoretical predictions, demonstrating a strong alignment between our estimates and the observed data. This research contributes to the understanding of the structural properties of networks that follow preferential attachment mechanisms, which are prevalent in various real-world systems such as social networks, the Internet, and citation networks. By focusing on the diameter and clustering coefficient, we provide insights into the fundamental characteristics that define the connectivity and cohesiveness of these complex networks. Our results not only enhance the theoretical framework surrounding PA networks but also offer practical implications for analyzing and modeling similar systems in diverse fields. \n\n**Keywords:** Preferential Attachment Networks; Diameter; Clustering Coefficient.",
        "ori-fast-z-score": -1.8594397919452197,
        "water-fast-z-score": 7.091242083423347,
        "rewrite-fast-z-score": 0.3621429841700741
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Future of the Local Large Scale Structure: the roles of Dark Matter and Dark Energy .\nAbstract:\nThe future evolution of the local large scale structure is investigated by using N-body simulations with different cosmological models, including dark energy (DE) and cold dark matter (CDM). The results show that in the Λ-CDM model without DE, the growth rate of large-scale structures decreases rapidly at late times due to the effect of CDM. In contrast, for the same initial conditions but with DE included, the growth rates are almost constant over time. This indicates that the inclusion of DE can significantly affect the formation of large-scale structures on very large scales. We also find that the density profiles of clusters formed in these two cases have significant differences. These results suggest that it may be possible to distinguish between these two scenarios through observations of cluster properties such as their mass functions or X-ray luminosities. Keywords: Cold dark matter; Dark energy; Growth factor; Clustering statistics; Density profile; Cosmology",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Future of the Local Large Scale Structure : the roles of Dark Matter and Dark Energy . Abstract : The future emergence of the local big scale system is investigated by using N - bodies simulations with various cosmological models , notably dark energy ( DE ) and chilled dark matter ( CDM ) .The results show that in the Λ - CDM theory without DE , the development frequency of large - scale structures decreases quickly at late times due to the impact of CDM . In comparison , for the same original conditions but with DE included , the development rates are almost steady over time .This implies that the introduction of DE can significantly affect the formation of large - scale structures on very huge scales . We additionally find that the density characteristics of clusters formed in these two examples have considerable changes .These data suggest that it could be possible to distinguish between these two scenarios through observations of cluster components such as their mass distributions or X - ray luminosities . Keywords : Cold black material ; Dark energy ; Growth factor ; Clustering statistics ; Density profile ; Cosmology",
        "rewrite_text": "Title: The Future of Local Large Scale Structure: The Roles of Dark Matter and Dark Energy\n\nAbstract: This study explores the future evolution of the local large-scale structure of the universe through N-body simulations across various cosmological models, with a particular focus on dark energy (DE) and cold dark matter (CDM). Our findings reveal that within the framework of the Λ-CDM model, which excludes dark energy, the growth rate of large-scale structures diminishes rapidly in the later stages of cosmic evolution due to the influence of cold dark matter. In contrast, when dark energy is incorporated into the same initial conditions, the growth rates of these structures remain relatively stable over time. This significant difference suggests that the presence of dark energy plays a crucial role in shaping the formation of large-scale structures on cosmological scales. Furthermore, our analysis indicates that the density profiles of clusters formed under these two scenarios exhibit notable variations. These results imply that observational techniques, such as examining mass distributions and X-ray luminosities of galaxy clusters, could provide a means to differentiate between the effects of dark matter and dark energy on large-scale structure formation. The implications of this research are profound, as they not only enhance our understanding of cosmic evolution but also offer potential pathways for future observational studies aimed at unraveling the mysteries of dark energy and its impact on the universe's structure. \n\nKeywords: Cold dark matter; Dark energy; Growth factor; Clustering statistics; Density profile; Cosmology.",
        "ori-fast-z-score": -0.7504787743864564,
        "water-fast-z-score": 6.182820774312702,
        "rewrite-fast-z-score": 0.5345224838248488
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Bondi accretion in the early universe .\nAbstract:\nWe present an analysis of the Bondi accretion rate onto primordial black holes (PBHs) during the radiation-dominated era, taking into account the effects of pressure and viscosity on the gas flow. We find that for PBH masses M ≫ 10^(10) g, the Bondi radius is much larger than the Schwarzschild radius, so that the standard Bondi-Hoyle-Lyttleton formula can be used to calculate the accretion rate. For smaller mass PBHs with M < 10^(10)g, we use numerical simulations to determine the accretion rate as a function of time. The results are compared against those obtained by assuming that the accreting gas has negligible pressure or viscosity. In addition, we consider the possibility that the accreted gas may cool efficiently via bremsstrahlung emission before it reaches the central BH. Finally, we discuss how our results could affect the abundance of PBHs at different redshifts.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Bondi accretion in the early universe . Abstract : We present an assessment of the Bondi accretion rate onto primordial black holes ( PBHs ) during the radiation - dominated period , giving into consideration the effects of force and viscosity on the gas stream .We see that for PBH masses M [UNK] 10 ^ ( 10 ) g , the Bondi diameter is much larger than the Schwarzschild diameter , so that the standard Bondi - Hoyle - Lyttleton formula can be used to estimate the accretion rate . For lower mass PBHs with M < 10 ^ ( 10 ) g , we using numerical simulations to estimate the accretion rate as a function of time .The results are compared against those achieved by assuming that the accreting gas has negligible pressure or viscosity . In addition , we investigate the prospect that the accreted gas may cool efficiently via bremsstrahlung emission before it enters the main BH .Finally , we explain how our findings may affect the availability of PBHs at different redshifts .",
        "rewrite_text": "In this article, we explore the Bondi accretion process onto primordial black holes (PBHs) during the radiation-dominated era of the early universe. Our analysis takes into account the influences of forces and viscosity on the gas flow surrounding these black holes. We find that for PBHs with masses greater than approximately \\(10^{10}\\) grams, the Bondi diameter significantly exceeds the Schwarzschild diameter, allowing us to apply the conventional Bondi-Hoyle-Lyttleton formula to accurately estimate the accretion rate. Conversely, for PBHs with masses less than \\(10^{10}\\) grams, we employ numerical simulations to derive the accretion rate as a function of time, providing a more nuanced understanding of the dynamics involved. Our findings are juxtaposed with results obtained under the assumption that the accreting gas is devoid of pressure or viscosity, highlighting the importance of these factors in the accretion process. Furthermore, we examine the potential for the accreted gas to cool effectively through bremsstrahlung emission prior to its incorporation into the black hole. This cooling mechanism could have significant implications for the thermal state of the gas and the overall accretion efficiency. Ultimately, our results contribute to a deeper understanding of the formation and evolution of PBHs, particularly in relation to their abundance at various redshifts in the early universe. This research not only sheds light on the fundamental processes governing PBH accretion but also opens avenues for further investigation into the role of these black holes in cosmic structure formation.",
        "ori-fast-z-score": -0.4923659639173309,
        "water-fast-z-score": 5.093248125762992,
        "rewrite-fast-z-score": 1.2451741707874968
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dramatic Variability of X-ray Absorption Lines in the Black Hole Candidate Cygnus X-1 .\nAbstract:\nWe report on an observation made with Chandra s High Energy Transmission Grating Spectrometer (HETGS) that shows dramatic variability of absorption lines in the spectrum of the black hole candidate Cygnus X1, which is known to have strong winds and outflows. The observed line profiles are consistent with those expected for highly ionized iron atoms moving at speeds up to 0.2c along our line-of-sight toward the central source. We find no evidence for significant changes in the ionization state or column density of these absorbers over time scales as short as one hour. These results provide new insights into the physical conditions near the accretion disk around this supermassive black hole. This work was supported by NASA under contract NAS8-03060. \n \n Keywords: Black holes; Winds; Outflows; Accretion disks \n \n Introduction \n \n In recent years there has been growing interest in studying the properties of winds and outflows associated with active galactic nuclei (AGN). Such flows may play important roles in regulating the growth of supermassive black holes through their effects on both the surrounding gas and radiation fields. They also represent potential sources of feedback between AGNs and their host galaxies. However, despite many theoretical predictions about how such winds should behave, direct observational constraints remain limited due to the difficulty of observing them directly. One promising approach involves using high-resolution spectroscopy to study the absorption features produced when wind material passes across the line-of-sight towards the central continuum source. Recent observations of several nearby Seyfert 1 galaxies show clear evidence for variable absorption lines arising from photoionized plasma flowing outward from the nucleus at velocities ranging from ~100-1000 km/sec (e.g., Kaspi et al. 2002; Crenshaw & Kraemer 2003; McKernan et al. 2007 ). Here we present another example of this phenomenon based on a deep Chandra/HETG observation of the brightest member of the class of Galactic black hole candidates (GBHCs), Cygnus X1. \n \n Cygnus X1 is located only 2 kpc away from Earth in the",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dramatic Variability of X - ray Absorption Lines in the Black Hole Candidate Cygnus X - 1 . Abstract : We report on an observation made with Chandra s High Energy Transmission Grating Spectrometer ( HETGS ) that reveals significant variability of absorption patterns in the spectrum of the dark hole contender Cygnus X1 , which is known to have stable winds and outflows .The observed line profiles are compatible with those expected for highly ionized iron atoms moved at speeds up to 0 . 2c along our line - of - view toward the main origin . We see no evidence for significant variations in the ionization state or column size of these absorbers over time ranges as short as one minute .These data provide fresh insights into the physical conditions near the accretion disk around this supermassive black hole . This project was supported by NASA under contract NAS8 - 03060 .Keywords : Black holes ; Winds ; Outflows ; Accretion disks Introduction In recent years there has been growing interest in investigating the properties of winds and outflows associated with active galactic nuclei ( AGN ) . Such streams may play crucial roles in controlling the development of supermassive black holes through their impact on both the nearby gas and radiation fields .They also appear possible sources of feedback between AGNs and their host galaxies . However , despite many theoretical estimates about how such winds should react , direct observational restrictions remain minimal owing to the difficulty of experiencing them directly .One promising alternative employs using high - resolution spectroscopy to study the absorption features created when wind material passes across the line - of - view towards the main continuum source . Recent measurements of several neighbouring Seyfert 1 clusters show good evidence for variable absorption patterns arising from photoionized liquid flowing outward from the nucleus at velocities ranging from ~ 100 - 1000 kilometers / sec ( e . g . , Kaspi et al .2002 ; Crenshaw & Kraemer 2003 ; McKernan et al . 2007 ) .Here we present another example of this phenomenon based on a deep Chandra / HETG detection of the brightest member of the class of Galactic dark hole candidates ( GBHCs ) , Cygnus X1 . Cygnus X1 is situated only 2 kpc apart from Earth in the",
        "rewrite_text": "**Title:** Dramatic Variability of X-ray Absorption Lines in the Black Hole Candidate Cygnus X-1\n\n**Abstract:** In this study, we present findings from observations conducted with the High Energy Transmission Grating Spectrometer (HETGS) aboard the Chandra X-ray Observatory, which reveal notable variability in the absorption features of the X-ray spectrum of Cygnus X-1, a prominent black hole candidate characterized by stable winds and outflows. The spectral analysis indicates that the observed line profiles are consistent with those produced by highly ionized iron ions moving at velocities approaching 0.2 times the speed of light along our line of sight to the source. Importantly, our data show no significant fluctuations in the ionization state or column density of these absorbers over time intervals as brief as one minute, suggesting a stable environment in the vicinity of the black hole. These observations provide valuable insights into the physical conditions surrounding the accretion disk of this supermassive black hole. The research was supported by NASA under contract NAS8-03060.\n\n**Keywords:** Black holes; Winds; Outflows; Accretion disks\n\n**Introduction:** The investigation of winds and outflows associated with active galactic nuclei (AGN) has garnered increasing attention in recent years due to their potential influence on the evolution of supermassive black holes and their interactions with surrounding gas and radiation fields. These outflows may also serve as a mechanism for feedback between AGNs and their host galaxies. Despite numerous theoretical predictions regarding the behavior of such winds, direct observational evidence has been limited, primarily due to the challenges in detecting them. High-resolution spectroscopy offers a promising approach to study the absorption features generated when wind material traverses the line of sight to the central continuum source. Recent studies of nearby Seyfert 1 galaxies have provided compelling evidence for variable absorption patterns resulting from photoionized gas flowing outward from the nucleus at velocities ranging from approximately 100 to 1000 kilometers per second (e.g., Kaspi et al. 2002; Crenshaw & Kraemer 2003; McKernan et al. 2007). In this paper, we present a new instance of this phenomenon, derived from a deep Chandra/HETG observation of Cygnus X-1, the brightest member of the Galactic black hole candidate class, located just 2 kiloparsecs from Earth.",
        "ori-fast-z-score": -1.1067971810589328,
        "water-fast-z-score": 8.221921916437786,
        "rewrite-fast-z-score": 0.2349781349963872
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A new chaotic attractor in a basic multi-strain epidemiological model with temporary cross-immunity .\nAbstract:\nWe study the dynamics of an epidemic system consisting of two strains and three classes, susceptible (S), infected (I) and recovered/removed (R). The infection rates are assumed to be bilinear functions of both strain densities. We show that this system has a unique positive equilibrium which is globally asymptotically stable if one of the following conditions holds:\n(i) the recovery rate for each strain is greater than its corresponding infection rate; or \n(ii) there exists at least one strain whose recovery rate is equal to its infection rate. \nHowever, we prove that when condition (i) fails but condition (ii) holds true, then the system exhibits chaos through numerical simulations. Finally, we present some results on global stability by using Lyapunov functionals. In recent years, many mathematical models have been proposed to describe the transmission dynamics of infectious diseases  1  . These models can be classified into single-strain models  2  , multi-strain models  3  -  6  and metapopulation models  7  .\nIn particular, multi-strain models play important roles in understanding how different pathogens interact within hosts  8  . For example, it was shown that co-infection may lead to extinction  9  ; while superinfection may cause periodic oscillations  10  . Recently, Li et al.  11  studied a multi-strain epidemic model with nonlinear incidence rates and found that the disease-free equilibrium is locally asymptotically stable under certain conditions. However, they did not consider the effect of cross immunity between strains. Cross immunity refers to partial protection against subsequent infections caused by other strains  12  . It plays an important role in preventing epidemics  13  . Therefore, it should be taken into account in modeling the spread of infectious diseases  14  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A innovative chaotic attractor in a basic multi - strain epidemiological theory with temporary cross - immunity . Abstract : We research the dynamics of an outbreak structure comprised of two strains and three categories , susceptible ( S ) , infected ( I ) and returned / deleted ( R ) .The illness rates are assumed to be bilinear functions of both strain densities . We see that this scheme has a unique positive equilibrium which is internationally asymptotically stable if one of the following situations holds : ( i ) the recovery rate for each strain is greater than its corresponding infection rate ; or ( ii ) there exists at least one strain whose recovery rate is equal to its infection rate .However , we prove that when condition ( i ) fails but condition ( ii ) holds true , then the model shows chaos through numerical simulations . Finally , we present some results on world equilibrium by using Lyapunov functionals .In recent years , various computational models have been proposed to explain the spread dynamics of infectious infections 1 . These systems can be categorized into single - strain models 2 , multi - strain models 3 - 6 and metapopulation scenarios 7 .In particular , multi - strain models play crucial roles in understanding how various pathogens interact within hosts 8 . For instance , it was shown that co - infection would result to extinction 9 ; while superinfection might cause continuous oscillations 10 .Recently , Li et al . 11 studied a multi - strain outbreak model with nonlinear mortality rates and found that the infection - free equilibrium is locally asymptotically stable under certain conditions .However , they did not discuss the impact of cross immunity between varieties . Cross immunity means to temporary protection against subsequent infections caused by other varieties 12 .It acts an important role in preventing epidemics 13 . Therefore , it should be taken into consideration in measuring the spread of infectious infections 14 .",
        "rewrite_text": "**Title:** An Innovative Chaotic Attractor in a Basic Multi-Strain Epidemiological Theory with Temporary Cross-Immunity\n\n**Abstract:** This study investigates the dynamics of a multi-strain epidemic model characterized by two distinct strains and three compartments: susceptible (S), infected (I), and recovered/removed (R). The infection rates are modeled as bilinear functions of the densities of both strains. Our analysis reveals that the system possesses a unique positive equilibrium, which is globally asymptotically stable under specific conditions: either (i) the recovery rate for each strain exceeds its corresponding infection rate, or (ii) at least one strain has a recovery rate equal to its infection rate. However, we demonstrate that if condition (i) is not satisfied while condition (ii) holds, the model exhibits chaotic behavior, as evidenced by numerical simulations. Furthermore, we explore the implications of global equilibrium using Lyapunov functionals. \n\nIn recent years, various computational models have emerged to elucidate the dynamics of infectious disease spread. These models can be classified into single-strain models, multi-strain models, and metapopulation frameworks. Multi-strain models, in particular, are essential for understanding the interactions between different pathogens within hosts. Previous research has indicated that co-infection can lead to pathogen extinction, while superinfection may result in persistent oscillations. Recent work by Li et al. examined a multi-strain outbreak model with nonlinear mortality rates, concluding that the infection-free equilibrium is locally asymptotically stable under certain conditions. However, their analysis did not address the effects of cross-immunity, which provides temporary protection against subsequent infections from other strains. Cross-immunity plays a critical role in epidemic prevention and should be incorporated into models assessing the spread of infectious diseases. This study aims to fill that gap by integrating cross-immunity into the multi-strain framework, thereby enhancing our understanding of epidemic dynamics.",
        "ori-fast-z-score": -0.5622535302317492,
        "water-fast-z-score": 7.478238471251862,
        "rewrite-fast-z-score": 1.212678125181665
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Multi-wavelength observations of Southern Hot Molecular Cores traced by methanol masers - I. Ammonia and 24 GHz Continuum Data .\nAbstract:\nWe present the results of multi-wavelength studies towards southern hot molecular cores (HMCs) using ammonia, water vapour, and continuum data at 1.3 cm and 3 mm wavelengths obtained with the Australia Telescope Compact Array (ATCA). The HMCs are associated with high-mass star formation regions in which massive young stellar objects have been detected through infrared surveys. We find that all our targets show compact emission features on scales of 0.1 pc or less. These sources appear to be dense clumps embedded within larger-scale structures extending up to several parsecs. In addition, we detect extended emission components surrounding these compact clumps. Our analysis shows that the physical conditions derived for the compact clumps are consistent with those expected for hot molecular gas heated by protostellar outflows. On the other hand, the properties of the extended emission suggest that it is likely tracing an envelope around each source.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Multi - wavelength findings of Southern Hot Molecular Cores traced by methanol masers - I . Ammonia and 24 GHz Continuum Data . Abstract : We present the results of dual - wavelength experiments towards southern hot molecular cores ( HMCs ) using ammonia , air vapour , and continuum data at 1 . 3 cm and 3 cm wavelengths collected with the Australia Telescope Compact Array ( ATCA ) .The HMCs are related with high - mass star formation regions in which giant young stellar objects have been detected through infrared observations . We see that all our targets exhibit compact emission events on scales of 0 . 1 pc or smaller .These sources appear to be dense clumps embedded within wider - scale structures spanning up to several parsecs . In addition , we perceive extended emitted elements surrounding these compact clumps .Our study shows that the physical conditions generated for the compact clumps are compatible with those expected for hard molecular vapor cooled by protostellar outflows . On the other hand , the properties of the extended emitted suggest that it is probably tracing an envelope around each source .",
        "rewrite_text": "We present findings from dual-wavelength observations of southern hot molecular cores (HMCs) utilizing ammonia, water vapor, and continuum data at 1.3 cm and 3 cm wavelengths, gathered with the Australia Telescope Compact Array (ATCA). These HMCs are associated with regions of high-mass star formation, where massive young stellar objects have been identified through infrared surveys. Our analysis reveals that all observed targets display compact emission features on scales of 0.1 parsecs or less, indicating the presence of dense clumps embedded within larger structures that extend over several parsecs. Furthermore, we observe extended emission surrounding these compact clumps, suggesting a complex interplay between the dense cores and their surrounding environments. The physical conditions inferred for the compact clumps align with expectations for hard molecular vapor that has been cooled by protostellar outflows, indicating active star formation processes. Conversely, the characteristics of the extended emission imply that it likely represents an envelope surrounding each core, providing insights into the surrounding material and its potential role in star formation. This study enhances our understanding of the physical and chemical environments of southern HMCs, contributing to the broader knowledge of high-mass star formation and the dynamics of molecular gas in these regions. Our results underscore the importance of multi-wavelength observations in unraveling the complexities of star-forming regions and the processes that govern the formation of massive stars.",
        "ori-fast-z-score": -2.0855209398041166,
        "water-fast-z-score": 3.709704134011871,
        "rewrite-fast-z-score": -1.8411492357966468
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Central limit theorems in linear structural error-in-variables models with explanatory variables in the domain of attraction of the normal law .\nAbstract:\nWe study central limit theorems for estimators of parameters in linear regression models where errors are not necessarily normally distributed but have an elliptical distribution, and we allow some explanatory variables to be non-normal. We show that under suitable conditions on the model coefficients, the asymptotic distributions of these estimators can be approximated by those obtained when all the explanatory variables follow a multivariate normal distribution. The results are illustrated through simulation experiments. Keywords: Central Limit Theorem; Elliptical Distributions; Regression Modeling. 1 Introduction In many applications it is assumed that the response variable follows a Gaussian distribution while the predictors may or may not be normally distributed. For example, this assumption has been used extensively in econometrics (see e.g., Greene  2003  ). However, there are situations where the data generating process does not satisfy such assumptions. This motivates us to consider more general classes of distributions which include as special cases both the normal and nonnormal distributions. One class of distributions that includes most common probability density functions encountered in practice is given by the so-called elliptical distributions. These distributions were introduced independently by Kelker  1970  , Hüsler and Reiss  1981  , and Fang et al.  1987  . They are characterized by their dependence structure rather than their marginal densities. A random vector X = (X1, ..., Xd)T ∈ Rd belongs to the family of elliptical distributions if its characteristic function satisfies E exp(itX)  = exp{−V (t)},\nwhere V : R →  0, ∞) is called the characteristic generator. If V ≡ 0 then X is said to belong to the family of spherical distributions. Examples of elliptical distributions include:",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Central limit theorems in linear structural error - in - variables models with explanatory variables in the domain of attraction of the normal law . Abstract : We research central limit theorems for estimators of parameters in linear regression systems where errors are not necessarily normally distributed but have an elliptical distribution , and we allow some explanatory variables to be non - normal .We see that under suitable conditions on the model variables , the asymptotic distributions of these estimators can be approximated by those achieved when all the explanatory variables continue a multivariate normal distribution . The results are shown through simulation studies .Keywords : Central Limit Theorem ; Elliptical Distributions ; Regression Modeling . 1 Introduction In many applications it is expected that the response parameter follows a Gaussian distribution while the predictors would or may not be usually spread .For instance , this assumption has been used heavily in econometrics ( saw e . g . , Greene 2003 ) . However , there are circumstances where the information collecting mechanism does not satisfy such constraints .This motivates us to consider more general categories of distributions which contain as special cases both the usual and nonnormal distributions . One class of distributions that contains most common probability density functions seen in practice is given by the so - called elliptical distributions .These distributions were introduced independently by Kelker 1970 , Hüsler and Reiss 1981 , and Fang et al . 1987 .They are marked by their dependence structure rather than their marginal densities . A random matrix X = ( X1 , . . . , Xd ) T ∈ Rd belongs to the group of elliptical distributions if its characteristic function satisfies E exp ( itX ) = exp { −V ( t ) } , where V : R → 0 , ∞ ) is dubbed the characteristic generator .If V ≡ 0 then X is said to belong to the class of spherical distributions . Examples of elliptical distributions involve :",
        "rewrite_text": "**Title:** Central Limit Theorems in Linear Structural Error-in-Variables Models with Explanatory Variables in the Domain of Attraction of the Normal Law\n\n**Abstract:** This study investigates central limit theorems applicable to parameter estimators in linear regression models characterized by errors that may not conform to a normal distribution, but instead exhibit an elliptical distribution. Additionally, we consider scenarios where some explanatory variables are non-normally distributed. Our findings reveal that, under specific conditions pertaining to the model variables, the asymptotic distributions of these estimators can be effectively approximated by those derived from models where all explanatory variables follow a multivariate normal distribution. To substantiate our theoretical results, we conduct extensive simulation studies that illustrate the practical implications of our findings. The significance of this research lies in its ability to extend the applicability of central limit theorems to a broader class of distributions, thereby enhancing the robustness of regression modeling in various fields, including econometrics. The use of elliptical distributions, which encompass a wide range of commonly encountered probability density functions, allows for a more flexible approach to modeling data that may not adhere to traditional assumptions. This work contributes to the existing literature by providing a comprehensive framework for understanding the behavior of estimators in the presence of non-normal errors and non-normal explanatory variables, ultimately paving the way for more accurate statistical inference in real-world applications. \n\n**Keywords:** Central Limit Theorem; Elliptical Distributions; Regression Modeling.",
        "ori-fast-z-score": -0.9760921603577252,
        "water-fast-z-score": 4.810702354423639,
        "rewrite-fast-z-score": -0.09759000729485333
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Comparison of Particle Production in Quark and Gluon Fragmentation at sqrt s ~ 10 GeV .\nAbstract:\nThe production of particles with large transverse momenta is studied in the fragmentation region for gluons and quarks produced by photons, Z bosons or W bosons.  The data are taken using the D0 detector operating at Fermilab s Tevatron Collider. Events containing jets that have high transverse momentum (pT) and low pseudorapidity () are selected to study particle production in quark and gluon fragmentation regions. In addition, events where one jet has pT>20GeV/c and another jet has pT>15GeV/c are used to compare the properties of these two types of jets. The results show that the fraction of charged hadrons increases as the number of constituent quarks decreases. This behavior can be explained by the fact that the probability of producing a strange quark is higher than that of producing an up or down quark. The mean multiplicity of neutral mesons also shows this trend but not as clearly as the charged hadron multiplicity does.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Comparison of Particle Production in Quark and Gluon Fragmentation at sqrt s ~ 10 GeV . Abstract : The production of nuclei with large transverse momenta is studied in the fragmentation area for gluons and quarks produced by photons , Z bosons or W bosons .The data are took utilizing the D0 detector operating at Fermilab s Tevatron Collider . Events containing jets that have high transverse momentum ( pT ) and low pseudorapidity ( ) are chosen to study nucleus production in quark and gluon fragmentation areas .In addition , events where one jet has pT > 20GeV / c and another plane has pT > 15GeV / c are using to study the properties of these two kind of jets . The results show that the fraction of charged hadrons increases as the quantity of component quarks decreases .This activity can be described by the fact that the probability of creating a weird quark is higher than that of creating an up or down quark . The mean multiplicity of neutral mesons likewise shows this shift but not as clearly as the charged hadron multiplicity does .",
        "rewrite_text": "Title: Comparison of Particle Production in Quark and Gluon Fragmentation at √s ~ 10 GeV\n\nAbstract: This study investigates the production of particles with significant transverse momenta in the fragmentation regions associated with quarks and gluons generated by high-energy processes involving photons, Z bosons, or W bosons. Utilizing data collected by the D0 detector at Fermilab's Tevatron Collider, we focus on events characterized by jets exhibiting high transverse momentum (pT) and low pseudorapidity (η). Specifically, we analyze events where one jet has a transverse momentum greater than 20 GeV/c, while another jet exceeds 15 GeV/c, allowing for a detailed examination of the properties of these two types of jets. Our findings reveal a notable trend: the fraction of charged hadrons produced increases as the number of constituent quarks diminishes. This observation can be attributed to the enhanced probability of producing strange quarks compared to up or down quarks. Furthermore, we observe a similar trend in the mean multiplicity of neutral mesons, although this shift is less pronounced than that seen in charged hadron multiplicity. These results contribute to a deeper understanding of the dynamics of particle production in high-energy collisions and the distinct fragmentation characteristics of quarks and gluons. The implications of these findings are significant for theoretical models of hadronization and provide valuable insights into the underlying mechanisms governing particle interactions at high energies. Overall, this research enhances our comprehension of the complex processes involved in quark and gluon fragmentation, paving the way for future investigations in particle physics.",
        "ori-fast-z-score": -0.3721042037676254,
        "water-fast-z-score": 5.0854241181575475,
        "rewrite-fast-z-score": 1.0838622059807226
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  SDSSJ104341.53+085558.2: A second white dwarf with a gaseous debris disc .\nAbstract:\nWe report the discovery of an isolated white dwarf (WD) in the Sloan Digital Sky Survey Data Release 7, SDSSJ104341.53 + 085558.2, which has a large infrared excess and is surrounded by dusty material that may be part of a debris disc. The WD s temperature is T eff = 13000 ± 1000 K, its radius R wd = 0.0120 ± 0.0010 R ⊙ , and it has log g = 8.0 ± 0.1 cm s −2 . We find no evidence for binarity or accretion activity associated with this object. This is only the second known example of such a system; the first was GD 362. Our results suggest that these systems are rare but not unique among WDs. If confirmed, they would provide important constraints on models of planetary formation around WDs. \n \n Keywords: White dwarfs",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : SDSSJ104341 . 53 + 085558 . 2 : A second white dwarf with a gaseous dust disc . Abstract : We report the discovery of an isolated brown giant ( WD ) in the Sloan Digital Sky Survey Data Release 7 , SDSSJ104341 . 53 + 085558 . 2 , which has a large infrared excess and is surrounded by dusty matter that might be part of a debris disc .The WD s temperature is T eff = 13000 ± 1000 K , its radius R wd = 0 . 0120 ± 0 . 0010 R [UNK] , and it has log g = 8 . 0 ± 0 . 1 cm s −2 . We see no evidence for binarity or accretion activity associated with this object .This is only the second documented example of such a system ; the first was GD 362 . Our results show that these systems are unlikely but not unusual among WDs .If confirmed , they may provide important restrictions on predictions of planetary formation around WDs . Keywords : White dwarfs",
        "rewrite_text": "Title: SDSSJ104341.53 + 085558.2: A Second White Dwarf with a Gaseous Dust Disc\n\nAbstract: In this study, we present the identification of an isolated white dwarf (WD), designated SDSSJ104341.53 + 085558.2, within the Sloan Digital Sky Survey Data Release 7. This particular WD exhibits a significant infrared excess, suggesting the presence of surrounding dusty material, potentially indicative of a debris disc. Our analysis reveals that the effective temperature of the white dwarf is approximately T_eff = 13000 ± 1000 K, with a radius of R_wd = 0.0120 ± 0.0010 R_☉ and a surface gravity of log g = 8.0 ± 0.1 cm s^−2. Notably, we found no signs of binarity or accretion processes associated with this object, which distinguishes it from other known systems. This discovery marks only the second instance of a white dwarf exhibiting such characteristics, the first being GD 362. Our findings suggest that while these systems are rare, they are not entirely anomalous among white dwarfs. If further observations confirm our results, they could impose significant constraints on theoretical models regarding planetary formation in the vicinity of white dwarfs. This research contributes to the growing body of knowledge on the evolution of stellar remnants and their potential interactions with surrounding material, offering insights into the complex processes that govern the lifecycle of stars and their remnants. \n\nKeywords: White dwarfs, infrared excess, debris disc, planetary formation, Sloan Digital Sky Survey.",
        "ori-fast-z-score": -0.5252257314388902,
        "water-fast-z-score": 3.7754784184438925,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Southern wide very low-mass stars and brown dwarfs in resolved binary and multiple systems .\nAbstract:\nWe present the results of our survey for southern M-dwarf binaries with separations between 0.1 AU and 10 AU, using data obtained by the Wide Field Camera 3 (WFC3) on board HST as part of program GO-12775. We find that about half of all systems are unresolved or only marginally resolved at these distances. The fraction of close binaries is higher among lower mass objects than it is among more massive ones; we estimate that this difference may be due to observational biases rather than intrinsic differences in formation mechanisms. \n \n In addition, we have identified several new candidate companions around known nearby ultracool dwarfs: GJ 436Bb, GJ 581Cc, GJ 674Ab, and GJ 758Aa. These candidates will require further observations before their status can be confirmed. Finally, we report the discovery of two previously unknown substellar objects: 2MASS J03552385+1133413AB and 2MASS J16252618-2434439AB.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Southern wide very low - weight stars and brown dwarfs in resolved binary and multiple components . Abstract : We present the results of our survey for southern M - dwarf binaries with separations between 0 . 1 AU and 10 AU , using data acquired by the Wide Field Camera 3 ( WFC3 ) on board HST as part of program GO - 12775 .We see that about half of all systems are unresolved or only marginally resolved at these distances . The percentage of close binaries is higher among lower weight objects than it is among more massive ones ; we estimate that this distinction might be due to observational biases rather than intrinsic differences in formation patterns .In addition , we have discovered numerous new proposed companions around suspected local ultracool dwarfs : GJ 436Bb , GJ 581Cc , GJ 674Ab , and GJ 758Aa . These candidates will undergo further observations before their status can be verified .Finally , we publish the discovery of two previously unknown substellar objects : 2MASS J03552385 + 1133413AB and 2MASS J16252618 - 2434439AB .",
        "rewrite_text": "In this study, we report the findings from our survey focused on southern M-dwarf binary systems, specifically those with separations ranging from 0.1 AU to 10 AU. This research utilized data obtained from the Wide Field Camera 3 (WFC3) aboard the Hubble Space Telescope (HST) as part of program GO-12775. Our analysis reveals that approximately 50% of the observed systems are either unresolved or only marginally resolved within the specified distance range. Notably, we found a higher prevalence of close binary systems among lower-mass objects compared to their more massive counterparts. This observation suggests that the differences in binary formation may be influenced more by observational biases than by inherent variations in the formation processes of these stars.\n\nAdditionally, our survey led to the identification of several new potential companions around known ultracool dwarfs, including GJ 436Bb, GJ 581Cc, GJ 674Ab, and GJ 758Aa. These candidates are slated for further observational scrutiny to confirm their status as companions. Furthermore, we are excited to announce the discovery of two previously unidentified substellar objects: 2MASS J03552385 + 1133413AB and 2MASS J16252618 - 2434439AB. This research contributes valuable insights into the population of low-mass stars and brown dwarfs in binary and multiple systems, enhancing our understanding of their formation and evolution in the southern hemisphere. The findings underscore the importance of continued observational efforts to unravel the complexities of stellar and substellar companionship in our galaxy.",
        "ori-fast-z-score": -0.48507125007266594,
        "water-fast-z-score": 3.585685828003181,
        "rewrite-fast-z-score": -0.5076730825668095
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Non-Markovian Stochastic Liouville equation and its Markovian representation. Extensions of the continuous time random walk approach .\nAbstract:\nWe present an extension of the Continuous Time Random Walk (CTRW) formalism to describe non-Markovian processes in terms of a stochastic Liouville equation with memory kernel. The latter is obtained by solving the corresponding Fokker-Planck equation for the probability distribution function, which can be done analytically only under certain conditions on the form of the memory kernel. We show that this problem can be overcome by introducing a new concept of  stochastic propagator , which allows us to obtain exact solutions even when the memory kernel does not have any particular functional dependence on time or space variables. Finally we discuss how our results are related to previous works based on fractional calculus techniques. In recent years there has been growing interest in studying systems whose dynamics cannot be described within the standard framework of Markovian processes  1  . This is mainly due to the fact that many physical phenomena exhibit complex temporal behavior characterized by power law relaxation  2  , aging  3  , intermittency  4  , etc., all of them being typical features of nonMarkovian processes  5  .\nIn order to deal with these problems it was proposed  6  to use the so-called Continuous Time Random Walk (CWTRW) formalism  7, 8  . Within such a description one considers a system evolving continuously in time according to some deterministic laws but interrupted at random times by jumps between different states. These jumps occur as a consequence of interactions with other degrees of freedom, e.g. phonons  9  , electrons  10  , photons  11  , etc.. As shown in Ref.  12  , the CWTRW formalism provides a very general description of non-Markovian dynamics since it includes both discrete state models  13  and fractional diffusion equations  14  as special cases. However, despite its great flexibility, the application of the CWTRW formal-",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Non - Markovian Stochastic Liouville equation and its Markovian representation . Extensions of the continuous time random walk approach .Abstract : We introduce an extension of the Continuous Time Random Walk ( CTRW ) formalism to define non - Markovian systems in terms of a stochastic Liouville equation with memory kernel . The latter is found by solving the analogous Fokker - Planck formula for the probability distribution function , which can be performed analytically only under certain conditions on the form of the storage kernel .We see that this question can be overcome by using a new notion of stochastic propagator , which allows us to obtain exact solutions even when the memory kernel does not have any specific functional dependence on time or space factors . Finally we explain how our findings are related to previous works based on fractional calculus techniques .In recent years there has been growing interest in understanding systems whose dynamics cannot be described within the standard structure of Markovian mechanisms 1 . This is mainly owing to the fact that several physical phenomena experience complex temporal activity described by power law contraction 2 , aging 3 , intermittency 4 , etc . , all of them being normal features of nonMarkovian mechanisms 5 .In try to deal with these problems it was suggested 6 to use the so - called Continuous Time Random Walk ( CWTRW ) formalism 7 , 8 . Within such a description one sees a system evolving continuously in time according to some deterministic laws but halted at random times by jumps between various states .These moves occur as a outcome of interactions with other degrees of freedom , e . g . phonons 9 , electrons 10 , photons 11 , etc . . As seen in Ref .12 , the CWTRW formalism gives a very general explanation of non - Markovian physics since it includes both discrete state models 13 and fractional diffusion expressions 14 as special cases . However , despite its great flexibility , the implementation of the CWTRW formal -",
        "rewrite_text": "**Title:** Non-Markovian Stochastic Liouville Equation and Its Markovian Representation: Extensions of the Continuous Time Random Walk Approach\n\n**Abstract:** In this article, we present an innovative extension of the Continuous Time Random Walk (CTRW) framework to characterize non-Markovian systems through a stochastic Liouville equation that incorporates a memory kernel. The derivation of this equation is achieved by solving the corresponding Fokker-Planck equation for the probability distribution function, which can be analytically resolved only under specific conditions related to the form of the memory kernel. To address the limitations posed by these conditions, we introduce a novel concept of a stochastic propagator, enabling us to derive exact solutions even when the memory kernel lacks a defined functional dependence on time or spatial variables. Our findings also draw connections to previous research utilizing fractional calculus techniques, highlighting the relevance of our approach in the broader context of non-Markovian dynamics.\n\nThe increasing interest in non-Markovian systems arises from the recognition that many physical phenomena exhibit complex temporal behaviors that cannot be adequately described by traditional Markovian frameworks. These phenomena include power law relaxation, aging effects, and intermittency, all of which are characteristic of non-Markovian processes. To tackle these challenges, the CTRW formalism has been proposed as a viable alternative. This framework allows for the continuous evolution of a system governed by deterministic laws, punctuated by random jumps between states, which are driven by interactions with various degrees of freedom such as phonons, electrons, and photons.\n\nAs demonstrated in previous studies, the CTRW formalism provides a comprehensive framework for understanding non-Markovian physics, encompassing both discrete state models and fractional diffusion processes as particular instances. Despite its inherent flexibility, the practical application of the CTRW formalism has faced challenges, which our research aims to overcome by providing a more robust theoretical foundation for non-Markovian dynamics. Through our work, we contribute to the ongoing discourse on the complexities of non-Markovian systems and their implications across various fields of physics.",
        "ori-fast-z-score": -1.3333333333333333,
        "water-fast-z-score": 6.5,
        "rewrite-fast-z-score": 1.5261167249147478
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Fundamental-measure density functional for the fluid of aligned hard hexagons: New insights in fundamental measure theory .\nAbstract:\nWe present an accurate and efficient fundamental-measure density-functional (FMT) approach to describe fluids composed of rigidly-aligned hard hexagons, which are relevant as model systems for liquid crystals or colloidal suspensions with anisotropic interactions. The FMT is based on a decomposition into three different types of weighted densities that can be evaluated efficiently using fast Fourier transforms. We show how this new FMT yields excellent results compared to Monte Carlo simulations over wide ranges of packing fractions and orientations of the particles. In particular we find very good agreement between our theoretical predictions and simulation data at high packing fractions where previous approaches fail due to strong correlations among neighboring particles. Finally, we demonstrate that our method also allows us to accurately predict structural properties such as pair correlation functions and orientational order parameters. This work provides further evidence that FMTs provide a powerful tool to study complex fluids beyond simple spherical particle models. \nI. INTRODUCTORY REMARkS\nThe description of liquids and soft matter requires sophisticated methods because these materials often exhibit complex structures and dynamics. Density functionals have been developed during recent years as promising tools to tackle many-body problems in statistical mechanics  1  . They allow one to calculate equilibrium properties of interacting particles by minimizing a free energy functional with respect to the local number density distribution. A particularly successful class of density functionals are so-called fundamental-measure density-functionals (FMD), which were originally introduced by Rosenfeld  2  .\nIn their original form they only apply to fluids consisting of identical spheres but extensions to more complicated shapes like ellipsoids  3  , rods  4  , dumbbells  5  , spherocylinders  6  , and even patchy particles  7, 8  have been proposed recently. However, most of these works focus on the case of uniaxial symmetry while there exist few studies dealing with more general situations  9  . Here we consider a system of rigidly-aligned",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Fundamental - measure density functional for the liquid of aligned hard hexagons : New ideas in fundamental measure theory . Abstract : We present an accurate and efficient fundamental - measure density - functional ( FMT ) approach to define liquid contained of rigidly - aligned hard hexagons , which are important as model structures for solid crystals or colloidal suspensions with anisotropic interactions .The FMT is based on a transformation into three different kinds of weighted densities that can be evaluated efficiently using fast Fourier transforms . We see how this new FMT yields good results relative to Monte Carlo simulations over broad ranges of packing fractions and orientations of the particles .In particular we find very high agreement between our theory estimates and modeling data at high packing fractions where earlier approaches fail due to heavy correlations among neighboring particles . Finally , we prove that our technique also enables us to correctly forecast structural properties such as couple correlation functions and orientational order variables .This research provides further evidence that FMTs represent a powerful tool to study difficult fluids beyond straightforward spherical particle models . I .INTRODUCTORY REMARkS The description of liquids and dark matter requires sophisticated methods because these structures often exhibit intricate structures and dynamics . Density functionals have been used during recent years as promising tools to tackle many - bodies problems in mathematical mechanics 1 .They allow one to estimate equilibrium properties of interacting molecules by minimizing a free energy functional with regard to the local number density distribution . A notably famous class of density functionals are so - called fundamental - measure density - functionals ( FMD ) , which were first developed by Rosenfeld 2 .In their original form they only applicable to liquid consisting of identical spheres but extensions to more complicated forms like ellipsoids 3 , rods 4 , dumbbells 5 , spherocylinders 6 , and sometimes patchy particles 7 , 8 have been proposed lately . However , most of these works concentrate on the case of uniaxial symmetry while there remain few researchers dealing with more general situations 9 .Here we define a system of rigidly - aligned",
        "rewrite_text": "**Title: Fundamental Measure Density Functional for the Liquid of Aligned Hard Hexagons: Innovations in Fundamental Measure Theory**\n\n**Abstract:** In this study, we introduce a novel and effective fundamental measure density functional (FMT) approach tailored for liquids composed of rigidly aligned hard hexagons. These hexagonal structures serve as significant models for understanding solid crystals and colloidal suspensions characterized by anisotropic interactions. Our FMT framework is predicated on a transformation into three distinct types of weighted densities, which can be computed efficiently using fast Fourier transforms. The results from our new FMT demonstrate a high degree of accuracy when compared to Monte Carlo simulations across a wide spectrum of packing fractions and particle orientations. Notably, we observe exceptional agreement between our theoretical predictions and empirical data at elevated packing fractions, a regime where previous methodologies have struggled due to the strong correlations among adjacent particles. Furthermore, we establish that our approach is capable of accurately predicting structural properties, including coupled correlation functions and orientational order parameters. This research underscores the efficacy of FMTs as a robust tool for investigating complex fluid systems that extend beyond the conventional spherical particle models. \n\n**I. Introductory Remarks:** The analysis of liquids and dark matter necessitates advanced methodologies due to the complex structures and dynamics these systems often exhibit. In recent years, density functionals have emerged as promising instruments for addressing many-body problems in mathematical mechanics. They facilitate the estimation of equilibrium properties of interacting molecules by minimizing a free energy functional with respect to the local number density distribution. A prominent category of density functionals is the fundamental measure density functionals (FMD), initially developed by Rosenfeld. While these functionals were originally applicable solely to liquids composed of identical spheres, recent extensions have been proposed for more complex geometries, including ellipsoids, rods, dumbbells, spherocylinders, and occasionally patchy particles. However, most existing research has focused on systems exhibiting uniaxial symmetry, leaving a gap in the exploration of more generalized scenarios. In this work, we define a system of rigidly aligned hard hexagons, paving the way for further advancements in the understanding of anisotropic fluids.",
        "ori-fast-z-score": -2.729152956884052,
        "water-fast-z-score": 6.180982563844155,
        "rewrite-fast-z-score": 0.7049344049891616
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Integral Formulas for the Asymmetric Simple Exclusion Process .\nAbstract:\nWe present integral formulas for the steady-state distribution and current in the asymmetric simple exclusion process (ASEP) with open boundaries, which is one of the most important models to describe non-equilibrium phenomena such as traffic flow on highways or biochemical reactions at molecular motors.  We derive these results by using an exact mapping between ASEP and the totally asymmetric zero-range process (TAZRP), which can be solved exactly via matrix product ansatz. The obtained formulae are expressed only in terms of elementary functions and thus provide explicit expressions for physical quantities that have been studied so far mainly numerically. In particular, we show that our result reproduces known results for the case where particles enter and exit at both ends of the system with equal rates. Furthermore, we obtain new results for the cases where particles enter and/or exit at either end of the system with unequal rates. \nI. INTRODUCTIO N\n\nThe asymmetric simple exclusion process (AS EP)\nis one of the most fundamental models describing nonequilibrium phenomena  1  . It describes the dynamics of interacting particles hopping along a chain of L sites under the following rules: each site i = 1, ..., L contains at most one particle; if there is no particle at site i , then it hops rightward with rate p ; otherwise, it stays still. If there is already another particle at site i , however, this particle cannot move until the first particle moves away. This model has attracted much attention because its stationary state exhibits various interesting properties depending on boundary conditions  2  .\nIn recent years, several studies have focused on the so-called open-boundary condition  3  -  8  : Particles enter into the leftmost site of the chain with probability α per unit time and leave from the rightmost site with probability β per unit time. For example, when α = β = 1/2, the stationary state becomes uniform regardless of the initial configuration  9  . On the other hand, when α > β , the stationary state shows phase separation  10  . Moreover, when α < β , the stationary state displays shock profiles  11  . These features make the AS EP a powerful tool to",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Integral Formulas for the Asymmetric Simple Exclusion Process . Abstract : We present integral formulas for the stable - state distribution and current in the asymmetric simple exclusion cycle ( ASEP ) with open boundaries , which is one of the most important models to explain non - equilibrium phenomena such as traffic flow on highways or biochemical reactions at molecular motors .We derive these results by using an precise mapping between ASEP and the completely asymmetric zero - range method ( TAZRP ) , which can be solved exactly via matrix product ansatz . The derived formulae are written only in terms of elementary functions and therefore give explicit expressions for mechanical quantities that have been studied so far primarily numerically .In particular , we find that our consequence reproduces known results for the case where particles entrance and exit at both sides of the system with equal rates . Furthermore , we obtain new results for the cases where particles leave and / or enter at either end of the system with unequal levels .I . INTRODUCTIO N The asymmetric simple exclusion system ( AS EP ) is one of the most important models explaining nonequilibrium phenomena 1 . It involves the dynamics of interacting molecules hopping along a network of L locations under the following regulations : each site i = 1 , . . . , L includes at most one particle ; if there is no particle at site i , then it hops rightward with speed r ; otherwise , it keeps still .If there is already another particle at site i , however , this particle cannot move until the first particle moves away . This theory has garnered great popularity because its stationary state produces various exciting properties depending on boundary constraints 2 .In recent years , various studies have concentrated on the so - called open - boundary relation 3 - 8 : Particles enter into the leftmost site of the chain with probability α per unit time and leave from the rightmost site with probability β per unit time . For instance , when α = β = 1 / 2 , the stationary state remains uniform regardless of the first configuration 9 .On the other hand , when α > β , the stationary state shows phase separation 10 . Moreover , when α < β , the stationary state presents shock profiles 11 .These features make the AS EP a powerful tool to",
        "rewrite_text": "In this article, we introduce integral formulas that characterize the steady-state distribution and current in the asymmetric simple exclusion process (ASEP) with open boundaries. The ASEP is a fundamental model for understanding non-equilibrium phenomena, such as traffic dynamics on highways and biochemical reactions involving molecular motors. Our approach leverages a precise mapping between the ASEP and the completely asymmetric zero-range process (TAZRP), which can be solved exactly using the matrix product ansatz. The resulting formulas are expressed solely in terms of elementary functions, providing explicit expressions for mechanical quantities that have predominantly been explored through numerical methods in previous studies.\n\nNotably, our findings confirm established results for scenarios where particles enter and exit the system at both ends with equal rates. Additionally, we present novel insights for cases where the entrance and exit rates differ, revealing new behaviors in the system's dynamics. The ASEP is particularly significant due to its ability to exhibit a variety of intriguing properties influenced by boundary conditions. Recent research has focused on open boundary conditions, where particles enter the leftmost site with a probability α and exit from the rightmost site with a probability β. For instance, when α equals β (specifically, 1/2), the stationary state remains uniform regardless of the initial configuration. Conversely, when α exceeds β, the system displays phase separation, while the opposite condition leads to the emergence of shock profiles.\n\nThese diverse behaviors underscore the ASEP's utility as a model for exploring complex systems in non-equilibrium statistical mechanics. Our work not only enhances the theoretical understanding of the ASEP but also provides practical tools for analyzing real-world phenomena in various fields, including physics, biology, and traffic engineering.",
        "ori-fast-z-score": 0.5232045649263551,
        "water-fast-z-score": 6.912635560098647,
        "rewrite-fast-z-score": -0.23791547571544325
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Higher harmonics increase LISA s mass reach for supermassive black holes .\nAbstract:\nWe show that the inclusion of higher-order gravitational wave modes in the analysis of data from future space-based detectors such as LISA can significantly improve their sensitivity to massive black hole binaries with total masses above 10^9 M_sol. We find that, when including all relevant modes up to = 4 (quadrupole and octopole), the signal-to-noise ratio increases by more than an order of magnitude compared to using only quadrupolar waves. This is due to the fact that higher-order modes carry additional information about the source which allows us to better distinguish between different types of signals. In particular, we demonstrate how this effect enables us to detect binary systems containing intermediate-mass black holes whose existence has been predicted by recent numerical simulations but have not yet been observed directly. The results presented here are based on our new semi-analytic model for the inspiral phase of generic spinning black hole binaries developed within the Effective One-Body framework.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Higher harmonics increase LISA s mass reach for supermassive black holes . Abstract : We see that the introduction of greater - order gravitational wave elements in the examination of evidence from future space - based detectors such as LISA can significantly improve their sensitivity to massive blue hole binaries with total masses above 10 ^ 9 M _ sol .We see that , when including all relevant modes up to = 4 ( quadrupole and octopole ) , the signal - to - noise proportion increases by more than an order of magnitude compared to use only quadrupolar waves . This is due to the fact that higher - order frequencies carry additional information about the origin which allows us to easier distinguish between various types of transmissions .In particular , we prove how this effect enables us to identify binary structures containing intermediate - mass black holes whose existence has been predicted by recent numerical simulations but have not already been observed directly . The results presented here are based on our new semi - analytic model for the inspiral phase of generic spun dark hole binaries pioneered within the Effective One - Body paradigm .",
        "rewrite_text": "Title: Higher Harmonics Enhance LISA's Capability to Detect Supermassive Black Holes\n\nAbstract: This study explores the impact of incorporating higher-order gravitational wave modes in the analysis of data from future space-based observatories, specifically the Laser Interferometer Space Antenna (LISA). Our findings indicate that the inclusion of these additional gravitational wave elements significantly enhances the sensitivity of LISA to detect massive black hole binaries with total masses exceeding 10^9 solar masses. By considering all relevant modes up to the fourth order, including quadrupole and octopole waves, we observe an increase in the signal-to-noise ratio by more than an order of magnitude compared to analyses that only utilize quadrupolar waves. This substantial improvement arises from the fact that higher-order frequencies provide richer information regarding the source, facilitating the differentiation between various types of gravitational wave emissions. Notably, our research demonstrates how this enhanced sensitivity allows for the identification of binary systems that contain intermediate-mass black holes—objects whose existence has been suggested by recent numerical simulations but remain unobserved in practice. The results presented in this paper are grounded in our innovative semi-analytic model developed for the inspiral phase of generic spinning black hole binaries, which is framed within the Effective One-Body approach. This work underscores the importance of higher harmonics in gravitational wave astronomy and their potential to expand our understanding of the universe's most massive objects.",
        "ori-fast-z-score": 0.618852747755276,
        "water-fast-z-score": 6.053272277302409,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Non-Markovian entanglement dynamics of quantum continuous variable systems in thermal environments .\nAbstract:\nWe study the non-Markovian evolution of bipartite Gaussian states under local dephasing noise and global squeezing interaction with an environment at finite temperature. We show that, for any initial state, there exists a critical time beyond which the system becomes separable. The critical time is determined by the initial state s purity and the environmental temperature. For pure initial states, we find that the critical time decreases monotonically as the temperature increases; while it first decreases then increases when the initial state has mixedness. In addition to this general result, we also present some specific examples where the critical times are analytically obtained. Our results provide useful insights into the effect of environmental temperature on the entanglement dynamics of open quantum systems. Introduction:-Entangled states play important roles in many fields such as quantum information processing  1  , condensed matter physics  2  , and quantum optics  3  . It is well known that the presence of external environments can lead to the loss of entanglement  4  . Therefore, understanding how the entanglement evolves in noisy environments is crucially important both theoretically and experimentally  5  .\nIn recent years, much attention has been paid to studying the entanglement dynamics of quantum systems  6  -  8  . However, most previous works have focused only on Markovian processes  9  -  11  or special types of initial states  12  -  14  . Recently, several authors studied the entanglement dynamics of two-mode Gaussian states  15  -  17  . They found that the entanglement decays exponentially fast if one mode undergoes phase damping (or amplitude damping)  18  ; however, it may decay slowly even though the other mode experiences strong dissipation  19  . Moreover, they showed that the entanglement revival occurs periodically  20  . These studies were mainly based on numerical simulations  21  . Very recently, analytical solutions were presented  22  -  24  . Nevertheless, these investigations did not take into account the effects of environmental temperatures  25  -  27  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Non - Markovian entanglement mechanics of quantum continuous variable systems in heat environments . Abstract : We research the non - Markovian development of bipartite Gaussian states under local dephasing noise and global squeezing interaction with an environment at finite temperature .We see that , for any initial state , there exists a critical time beyond which the process becomes separable . The critical time is chosen by the early state s purity and the environmental temperature .For pure initial states , we find that the critical time decreases monotonically as the temperature increases ; while it first decreases then increases when the initial state has mixedness . In addition to this general result , we also find some specific examples where the critical times are analytically obtained .Our results yield useful insights into the impact of environmental temperature on the entanglement structure of open quantum systems . Introduction : - Entangled states play crucial roles in different fields such as quantum information processing 1 , condensed matter science 2 , and quantum optics 3 .It is well established that the presence of external contexts can lead to the losing of entanglement 4 . Therefore , studying how the entanglement evolves in noisy environments is crucially vital both theoretically and experimentally 5 .In recent years , great emphasis has been paid to researching the entanglement mechanics of quantum systems 6 - 8 . However , most prior studies have concentrated only on Markovian systems 9 - 11 or particular kinds of initial states 12 - 14 .Recently , various scientists examined the entanglement mechanics of two - mode Gaussian states 15 - 17 . They found that the entanglement decays exponentially rapidly if one mode undergoes phase damping ( or intensity damping ) 18 ; however , it may decay slowly even though the other mode encounters strong dissipation 19 .Moreover , they demonstrated that the entanglement revival happen periodically 20 . These studies were mainly based on numerical simulations 21 .Very recently , analytical solutions were offered 22 - 24 . Nevertheless , these experiments did not take into consideration the effects of environmental temperatures 25 - 27 .",
        "rewrite_text": "**Title:** Non-Markovian Entanglement Dynamics of Quantum Continuous Variable Systems in Thermal Environments\n\n**Abstract:** This study investigates the non-Markovian evolution of bipartite Gaussian states subjected to local dephasing noise and global squeezing interactions within a thermal environment. Our findings reveal that for any given initial state, there exists a critical time threshold beyond which the entanglement of the system becomes separable. This critical time is influenced by the purity of the initial state and the temperature of the environment. Specifically, for pure initial states, we observe a monotonically decreasing trend in the critical time as the environmental temperature rises. Conversely, for mixed initial states, the critical time initially decreases before subsequently increasing with temperature. In addition to these general trends, we present specific cases where critical times can be derived analytically. These results provide valuable insights into how environmental temperature affects the entanglement properties of open quantum systems.\n\n**Introduction:** Entangled states are fundamental to various domains, including quantum information processing, condensed matter physics, and quantum optics. It is well recognized that external influences can lead to the degradation of entanglement. Consequently, understanding the dynamics of entanglement in the presence of noise is essential for both theoretical and experimental advancements. Recent research has increasingly focused on the entanglement dynamics of quantum systems; however, much of the existing literature has primarily addressed Markovian systems or specific types of initial states. Recent investigations into the entanglement mechanics of two-mode Gaussian states have shown that entanglement can decay rapidly under phase or intensity damping, yet may exhibit slower decay even under significant dissipation in one mode. Furthermore, these studies have indicated the potential for periodic revival of entanglement, primarily relying on numerical simulations. Although some recent works have provided analytical solutions, they have largely overlooked the influence of environmental temperatures on these dynamics. This paper aims to fill that gap by exploring the interplay between temperature and entanglement in non-Markovian settings.",
        "ori-fast-z-score": 0.47733437050543803,
        "water-fast-z-score": 7.273238618387272,
        "rewrite-fast-z-score": 1.5713484026367723
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  New smooth hybrid inflation .\nAbstract:\nWe propose a new inflationary model, which is based on the idea that there are two scalar fields in nature and they interact with each other through their potentials. We call this model  smooth hybrid inflation  (SHI). In SHI, one field plays the role of the waterfall field while another field acts as an inflaton. The potential for our model has three parameters; we show how these parameters can be fixed by using observational data such as WMAP7+BAO+H0. Our results indicate that the tensor-to-scalar ratio r=0.0055+0.0005-0.0004 at 95% CL. This value is consistent with recent observations. Moreover, we find that the spectral index ns=1.0145+0.0025-0.0024 at 68% CL. Therefore, our model predicts values for both nS and r within 1 sigma error bars of current experimental bounds.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : New smooth hybrid inflation . Abstract : We suggest a new inflationary model , which is based on the idea that there are two scalar fields in nature and they interact with each other through their potentials .We call this model smooth hybrid inflation ( SHI ) . In SHI , one field plays the role of the waterfall field while another field functions as an inflaton .The potential for our model has three variables ; we show how these parameters can be fixed by using observational data such as WMAP7 + BAO + H0 . Our results show that the tensor - to - scalar ratio r = 0 . 0055 + 0 . 0005 - 0 . 0004 at 95 % CL .This value is compatible with recent observations . Moreover , we find that the spectral index ns = 1 . 0145 + 0 . 0025 - 0 . 0024 at 68 % CL .Therefore , our model predicts values for both nS and r within 1 sigma error bars of recent experimental bounds .",
        "rewrite_text": "We propose a novel inflationary model termed smooth hybrid inflation (SHI), which is predicated on the existence of two interacting scalar fields in the universe. In this framework, one scalar field serves as the waterfall field, while the other acts as the inflaton. The potential associated with our model is characterized by three distinct parameters, which we demonstrate can be constrained using observational data from sources such as WMAP7, BAO, and H0 measurements. Our analysis yields a tensor-to-scalar ratio of r = 0.0055 +0.0005 -0.0004 at a 95% confidence level, a result that aligns well with contemporary observational findings. Additionally, we calculate the spectral index, obtaining ns = 1.0145 +0.0025 -0.0024 at a 68% confidence level. These predictions for both the spectral index and the tensor-to-scalar ratio fall within one sigma of the latest experimental constraints, reinforcing the viability of our model. The implications of SHI extend beyond mere theoretical constructs, as it offers a framework that can be tested against future observations, potentially providing deeper insights into the dynamics of inflation and the fundamental nature of scalar fields in cosmology. This work not only contributes to the ongoing discourse on inflationary models but also sets the stage for further exploration of the interactions between scalar fields in the early universe.",
        "ori-fast-z-score": -0.7878385971583353,
        "water-fast-z-score": 1.2909944487358056,
        "rewrite-fast-z-score": -0.43133109281375365
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Gravitational Stability of Circumnuclear Disks in Elliptical Galaxies .\nAbstract:\nWe study the stability properties of circumnuclear disks (CNDs) embedded within elliptical galaxies, using N-body simulations with live dark matter halos and stellar components. We find that CNDs are generally stable against bar formation for most reasonable disk parameters. However, we also show that if the central black hole is massive enough to dominate the gravitational potential at small radii, then it can induce strong bars or even destroy the entire disk. This result suggests that the presence of a supermassive black hole may be responsible for some observed nuclear bars in nearby elliptical galaxies. \n \n Keywords: Gravitational instability; Black holes; Bars; Nuclear activity; Galaxy evolution; Disk galaxies; Dark matter halos; Stellar dynamics; Cosmology \n \n 1 Introduction \n \n The existence of nuclear bars has been inferred observationally by several authors based on photometric data (e.g., Laine et al. 2002; Erwin 2004) . In particular, Erwin & Sparke (2003) found that about half of their sample of early-type galaxies have nuclear bars. These results suggest that nuclear bars play an important role in galaxy evolution. For example, they could provide fuel for active galactic nuclei through gas inflow into the center of the host galaxy (Shlosman et al. 1990 ). On the other hand, there are only few observational studies which directly detect nuclear bars via high-resolution imaging techniques such as HST observations (Erwin 2004; Sheth et al. 2005) , mainly due to technical difficulties associated with resolving very compact structures near the centers of distant galaxies. Therefore, theoretical investigations of the dynamical behavior of nuclear bars will help us understand how these objects evolve over time. \n \n 2 Previous Work \n \n Several previous works studied the stability of nuclear bars in elliptical galaxies. Athanassoula et al. (2005a) performed numerical experiments where they added a rigidly rotating spherical component representing a bulge to a model consisting of a live halo and a rigidly rotating disk. They showed that this system becomes unstable when the mass ratio between the bulge and the disk exceeds a critical value",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Gravitational Stability of Circumnuclear Disks in Elliptical Galaxies . Abstract : We research the stability properties of circumnuclear drives ( CNDs ) lodged within elliptical galaxies , using N - bodies simulations with live dark matter halos and stellar parts .We see that CNDs are typically strong against bar structure for most reasonable disk variables . However , we also prove that if the main dark hole is massive enough to dominate the gravitational potential at small radii , then it can induce strong bars or even kill the entire disk .This result suggests that the presence of a supermassive black hole may be responsible for some observed nuclear bars in nearby elliptical galaxies . Keywords : Gravitational instability ; Black holes ; Bars ; Nuclear activity ; Galaxy growth ; Disk galaxies ; Dark matter halos ; Stellar dynamics ; Cosmology 1 Introduction The existence of nuclear bars has been inferred observationally by many writers based on photometric data ( e . g . , Laine et al .2002 ; Erwin 2004 ) . In particular , Erwin & Sparke ( 2003 ) found that about half of their sample of early - class objects have nuclear bars .These data suggest that atomic chains serve an important role in universe growth . For instance , they may provide energy for active galactic nuclei through gas inflow into the center of the host galaxy ( Shlosman et al .1990 ) . On the other hand , there are only few observational surveys which directly identify atomic bars via high - resolution optical techniques such as HST observations ( Erwin 2004 ; Sheth et al .2005 ) , mainly owing to technical problems related with resolving very small structures near the centers of distant galaxies . Therefore , theoretical investigations of the dynamical behavior of nuclear bars will assist us explain how these objects evolve over time .2 Previous Work Several earlier works studied the stability of nuclear bars in elliptical galaxies . Athanassoula et al .( 2005a ) completed numerical studies where they added a rigidly rotating spherical component representing a bulge to a simulation consisting of a living halo and a rigidly rotating disk . They showed that this scheme becomes unstable when the mass ratio between the bulge and the disk exceeds a critical value",
        "rewrite_text": "**Title:** Gravitational Stability of Circumnuclear Disks in Elliptical Galaxies\n\n**Abstract:** This study investigates the stability characteristics of circumnuclear disks (CNDs) situated within elliptical galaxies, employing N-body simulations that incorporate live dark matter halos and stellar components. Our findings indicate that CNDs generally exhibit resilience against the formation of bar structures across a wide range of disk parameters. However, we demonstrate that when a supermassive black hole possesses sufficient mass to dominate the gravitational potential in the inner regions, it can lead to the formation of pronounced bars or potentially disrupt the entire disk structure. This outcome implies that the presence of a supermassive black hole may play a crucial role in the emergence of observed nuclear bars in nearby elliptical galaxies. \n\nThe existence of nuclear bars has been inferred from various observational studies, with significant contributions from researchers such as Laine et al. (2002) and Erwin (2004), who noted that approximately half of the early-type galaxies in their samples exhibit nuclear bars. These observations suggest that CNDs are integral to galaxy evolution, potentially facilitating energy transfer to active galactic nuclei through gas inflow towards the galactic center, as proposed by Shlosman et al. (1990). Despite the importance of these structures, there are limited observational surveys that directly identify nuclear bars using high-resolution optical techniques, such as those conducted with the Hubble Space Telescope (HST), due to challenges in resolving small-scale features in distant galaxies. Consequently, theoretical studies of the dynamical properties of nuclear bars are essential for understanding their evolutionary processes.\n\nPrevious research has explored the stability of nuclear bars in elliptical galaxies, with notable contributions from Athanassoula et al. (2005a), who conducted numerical simulations incorporating a rigidly rotating bulge alongside a live halo and disk. Their results indicated that the system becomes unstable when the mass ratio between the bulge and the disk surpasses a critical threshold. This body of work underscores the significance of gravitational interactions in shaping the structural dynamics of elliptical galaxies and their circumnuclear regions. \n\n**Keywords:** Gravitational instability; Black holes; Bars; Nuclear activity; Galaxy growth; Disk galaxies; Dark matter halos; Stellar dynamics; Cosmology.",
        "ori-fast-z-score": 0.457495710997814,
        "water-fast-z-score": 7.570719217728538,
        "rewrite-fast-z-score": 0.6135719910778963
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Composition-sensitive parameters measured with the surface detector of the Pierre Auger Observatory .\nAbstract:\nThe composition of cosmic rays is studied by measuring their energy spectrum and mass distribution at Earth. The most precise measurements are obtained using ground-based detectors, which measure extensive air showers produced in interactions between cosmic rays and atmospheric nuclei. In this work we present results on the measurement of shower depth profiles as well as several composition sensitive observables derived from them. These include the number of muons per meter water equivalent (N_m), the fraction of muons to electrons at 1000 m above sea level (f_1000) and the average logarithmic mass ln(A). We compare these results for different zenith angles and energies. For primary particles heavier than protons, f_1000 increases while N_m decreases with increasing zenith angle. This effect can be explained by the fact that heavy primaries interact higher up in the atmosphere where they produce more muons but fewer electrons compared to lighter primaries. At lower energies there seems to be an excess of events with low values of N_m and high values of f_1000 indicating a possible contribution from light primaries such as helium or nitrogen.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Composition - sensitive parameters calculated with the surface detector of the Pierre Auger Observatory . Abstract : The composition of cosmic rays is studied by monitoring their power spectrum and mass distribution at Earth .The most accurate measurements are produced using ground - based detectors , which measure immense air showers created in encounters between cosmic rays and atmospheric atoms . In this research we present results on the determination of shower depth profiles as also as several structure sensitive observables generated from them .These include the quantity of muons per foot water equivalent ( N _ m ) , the fraction of muons to ions at 1000 m above water level ( f _ 1000 ) and the average logarithmic mass ln ( A ) . We compare these results for different zenith angles and energies .For primary molecules heavier than protons , f _ 1000 increases while N _ m decreases with rising zenith angle . This phenomenon can be described by the fact that dark primaries react higher up in the air where they produce more muons but less electrons relative to lighter primaries .At lower energies there seems to be an amount of events with lowest values of N _ m and large values of f _ 1000 suggesting a possible involvement from light primaries such as helium or nitrogen .",
        "rewrite_text": "**Title:** Composition-Sensitive Parameters Derived from the Surface Detector of the Pierre Auger Observatory\n\n**Abstract:** The investigation of cosmic ray composition is crucial for understanding their origins and interactions, and this study focuses on the analysis of their power spectrum and mass distribution as observed on Earth. Utilizing ground-based detectors, which capture extensive air showers resulting from cosmic ray collisions with atmospheric atoms, we present findings on the shower depth profiles and various structure-sensitive observables derived from these measurements. Key parameters analyzed include the number of muons per foot water equivalent (N_m), the muon-to-ion ratio at 1000 meters above sea level (f_1000), and the average logarithmic mass (ln(A)). Our results reveal significant variations in these parameters across different zenith angles and energy levels of the incoming cosmic rays. Notably, for primary particles heavier than protons, we observe an increase in f_1000 alongside a decrease in N_m as the zenith angle rises. This trend can be attributed to the behavior of heavier primaries, which interact at higher altitudes, generating a greater number of muons but fewer electrons compared to lighter primaries. Additionally, at lower energy thresholds, we identify a subset of events characterized by low N_m values and high f_1000, indicating a potential contribution from lighter primary particles, such as helium or nitrogen. These findings enhance our understanding of cosmic ray composition and the underlying mechanisms governing their interactions in the Earth's atmosphere, providing valuable insights for future research in astrophysics and cosmic ray physics.",
        "ori-fast-z-score": -1.6,
        "water-fast-z-score": 5.8707194222389365,
        "rewrite-fast-z-score": -0.811502671200689
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Measure of de Sitter Entropy and Eternal Inflation .\nAbstract:\nWe propose to measure the entropy of de Sitter space by counting the number of states in which it can be embedded into Minkowski spacetime, as well as its embedding into anti-de Sitter (AdS) space.  We find that this quantity is proportional to the volume of the universe at late times, but grows exponentially with time during eternal inflation. This suggests that our proposed measure may provide an explanation for why we do not observe any remnants of the early universe: if there are too many possible histories, then most will have very different properties than ours. The holographic principle has been used extensively over recent years to study quantum gravity on asymptotically Anti-de Sitter spaces. In particular, the Bekenstein-Hawking entropy of black holes was shown to agree exactly with the entropy associated with the degrees of freedom localized near the event horizon. However, these results were obtained only after assuming that the boundary conditions satisfied by fields propagating through AdS space corresponded precisely to those required to preserve unitarity. It would therefore seem natural to ask whether similar techniques could also be applied to more general spacetimes such as de Sitter space. Unfortunately, while the answer to this question appears positive when considering static observers, it becomes less clear how to proceed once one considers dynamical situations where particles are created or destroyed.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Measure of de Sitter Entropy and Eternal Inflation . Abstract : We suggest to measure the entropy of de Sitter space by counting the number of states in which it can be embedded into Minkowski spacetime , as also as its embedding into anti - de Sitter ( AdS ) space .We see that this amount is proportional to the volume of the universe at late times , but grows exponentially with time during eternal inflation . This implies that our proposed measure could give an reason for why we do not detect any remnants of the early world : if there are too several possible histories , then most will have very different properties than ours .The holographic principle has been used heavily over recent years to study quantum gravitational on asymptotically Anti - de Sitter spaces . In particular , the Bekenstein - Hawking entropy of brown holes was shown to agree precisely with the entropy associated with the degrees of freedom confined near the event horizon .However , these results were obtained only after establishing that the boundary constraints satisfied by fields propagating through AdS space corresponded exactly to those required to restore unitarity . It would therefore appear natural to ask whether similar techniques could also be applied to more general spacetimes such as de Sitter space .Unfortunately , while the response to this question appears favorable when considering stationary observers , it becomes less apparent how to proceed once one considers dynamical situations where objects are created or destroyed .",
        "rewrite_text": "In this article, we propose a novel approach to quantifying the entropy of de Sitter space by evaluating the number of states that allow for its embedding within Minkowski spacetime, as well as its integration into anti-de Sitter (AdS) space. Our findings indicate that this entropy measure is directly proportional to the volume of the universe at later times, exhibiting an exponential growth during periods of eternal inflation. This observation provides a potential explanation for the absence of detectable remnants from the early universe: if an overwhelming number of possible histories exist, it is likely that most of these histories possess properties that significantly differ from our own. \n\nRecent advancements in the application of the holographic principle have facilitated a deeper understanding of quantum gravity in asymptotically AdS spaces. Notably, the Bekenstein-Hawking entropy associated with black holes has been shown to align precisely with the entropy linked to the degrees of freedom confined near the event horizon. These results were achieved after demonstrating that the boundary conditions imposed on fields propagating through AdS space correspond to those necessary for maintaining unitarity. Given this context, it is a natural progression to explore whether similar methodologies could be extended to encompass more generalized spacetimes, such as de Sitter space. \n\nHowever, while preliminary investigations suggest a favorable outcome for stationary observers, the situation becomes more complex when addressing dynamic scenarios involving the creation or annihilation of objects. This complexity raises critical questions about the applicability of established techniques in these more intricate settings, warranting further exploration into the relationship between entropy, spacetime geometry, and the fundamental nature of quantum gravity in de Sitter space.",
        "ori-fast-z-score": -0.09853292781642932,
        "water-fast-z-score": 5.050686484257773,
        "rewrite-fast-z-score": 0.4583492485141057
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Nonequilibrium Steady States of Matrix Product Form: A Solver s Guide .\nAbstract:\nThe nonequilibrium steady states (NESS) of matrix product form are the focus of this work, which is intended to be useful for researchers in computational physics and chemistry who wish to solve problems with such NESSs using numerical methods.  The first part of the article introduces the concept of NESSs as well as some basic properties that they possess. In particular, we show how one can construct an explicit representation of any given NESS by solving a linear system of equations whose coefficient matrices depend on the underlying transition rates between different microstates. We also discuss several important issues related to the construction of these coefficient matrices. The second part of the article presents two examples illustrating our approach. Finally, we provide a detailed discussion about various aspects of the proposed method along with possible extensions. Nonequilibrium steady states (NESs), i.e., time-independent solutions of master equations describing open systems far away from equilibrium, have been studied extensively over the past few decades  1  . They play crucial roles in many areas ranging from statistical mechanics  2  , quantum optics  3  , chemical reaction dynamics  4  , and biophysics  5  .\nIn recent years there has been growing interest in developing efficient algorithms for computing NESs  6  -  8  . This is mainly due to their importance in applications where it may not always be feasible or desirable to obtain exact analytical results  9  -  11  . For example, in molecular dynamics simulations  12  , Monte Carlo sampling techniques  13  , and kinetic Monte Carlo schemes  14  , only approximate values of NESs are available. Moreover, even if the exact solution were known, its direct use would still require significant amount of storage space  15  . Therefore, it becomes necessary to develop fast and accurate numerical methods for calculating NESs  16  -  18  .\nThere exist numerous approaches for numerically approximating NESs  19  -  21  . Among them, the most popular ones include the eigenvector-following algorithm  22  , the power iteration scheme  23  , and the Krylov subspace projection technique  24  . These methods usually involve repeated application of the original master equation until convergence is reached  25  . However, since the number of...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Nonequilibrium Steady States of Matrix Product Form : A Solver s Guide . Abstract : The nonequilibrium steady states ( NESS ) of matrix product structure are the emphasis of this project , which is intended to be usable for researchers in computational physics and chemistry who desire to solve difficulties with such NESSs using numerical technique .The first part of the article describes the idea of NESSs as well as some fundamental properties that they possess . In particular , we explain how one can build an explicit representation of any given NESS by solving a linear network of equations whose coefficient matrices depend on the underlying transition rates between various microstates .We additionally discuss various crucial matters related to the creation of these coefficient matrices . The second part of the article describes two examples illustrating our approach .Finally , we provide a detailed discussion about various parts of the suggested method along with possible extensions . Nonequilibrium steady states ( NESs ) , i . e . , time - based solve of master equations representing open systems well away from equilibrium , have been studied frequently over the previous few decades 1 .They play crucial roles in multiple fields ranging from statistical mechanics 2 , quantum optics 3 , chemical process mechanics 4 , and biophysics 5 . In recent months there has been growing interest in building fast algorithms for processing NESs 6 - 8 .This is mainly owing to their importance in applications where it would not always be impossible or useful to obtain exact analytical results 9 - 11 . For instance , in polymer mechanics simulations 12 , Monte Carlo analysis methods 13 , and kinetic Monte Carlo schemes 14 , only approximate values of NESs are available .Moreover , even if the exact solution were known , its immediate application would still demand significant amount of storage space 15 . Therefore , it becomes necessary to develop fast and precise quantitative methods for determining NESs 16 - 18 .There remain various approaches for numerically approximating NESs 19 - 21 . Among them , the most popular ones contain the eigenvector - following algorithm 22 , the power iteration scheme 23 , and the Krylov subspace projection procedure 24 .These methods usually include repeated application of the previous master equation until convergence is reached 25 . However , since the number of . . .",
        "rewrite_text": "**Title:** Nonequilibrium Steady States of Matrix Product Form: A Solver's Guide\n\n**Abstract:** This article focuses on the nonequilibrium steady states (NESS) characterized by matrix product structures, providing a comprehensive guide for researchers in computational physics and chemistry who aim to tackle challenges associated with NESS using numerical methods. The initial section introduces the concept of NESS and outlines several fundamental properties inherent to these states. We detail a systematic approach to construct an explicit representation of any given NESS by solving a linear network of equations, where the coefficient matrices are determined by the transition rates among various microstates. Additionally, we address critical considerations involved in the formulation of these coefficient matrices. \n\nIn the subsequent section, we present two illustrative examples that demonstrate the application of our proposed methodology. We conclude with an in-depth discussion of the various components of the suggested approach, along with potential avenues for further development. The study of nonequilibrium steady states, which involves the time-dependent solution of master equations for open systems significantly deviating from equilibrium, has garnered considerable attention over the past few decades. NESS play vital roles across diverse fields, including statistical mechanics, quantum optics, chemical process dynamics, and biophysics. Recently, there has been an increasing interest in devising efficient algorithms for analyzing NESS due to their relevance in scenarios where deriving exact analytical solutions is often impractical or infeasible. \n\nFor example, in simulations related to polymer mechanics, Monte Carlo methods, and kinetic Monte Carlo schemes, researchers typically rely on approximate values of NESS. Even when exact solutions are available, their application can require substantial storage resources. Consequently, there is a pressing need for the development of rapid and accurate quantitative techniques for determining NESS. Various numerical strategies exist for approximating NESS, with the most widely used being the eigenvector-following algorithm, power iteration method, and Krylov subspace projection technique. These approaches generally involve iterative applications of the master equation until convergence is achieved, highlighting the ongoing demand for efficient computational solutions in this area.",
        "ori-fast-z-score": -1.0377490433255416,
        "water-fast-z-score": 8.454968154078788,
        "rewrite-fast-z-score": 0.6099942813304187
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Groups of galaxies in the SDSS Data Release 5. A group-finder and a catalogue .\nAbstract:\nWe present an algorithm to find galaxy groups using photometric redshifts, which is based on the Voronoi tessellation technique (VT). The VT method has been widely used for identifying clusters of galaxies with spectroscopic redshifts but it had not yet been applied to identify galaxy groups with photometric redshifts. We use the Sloan Digital Sky Survey data release five (SDSS DR5) as our input sample. Our results show that the VT method can be successfully applied to identify galaxy groups even when only photometric redshifts are available. In this work we have identified more than 12000 galaxy groups at 0 < z < 0.3. These groups contain about 30000 member galaxies. We also provide a catalog containing basic information such as positions, magnitudes, colors, and photometric redshifts for all these groups. \n \n Keywords: Galaxy Group, Photometric Redshift",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Groups of galaxies in the SDSS Data Release 5 . A group - finder and a catalogue .Abstract : We present an algorithm to find galaxy groups using photometric redshifts , which is based on the Voronoi tessellation method ( VT ) . The VT method has been widely applied for finding clusters of clusters with spectroscopic redshifts but it had not already been used to identify galaxy groups with photometric redshifts .We use the Sloan Digital Sky Survey data release five ( SDSS DR5 ) as our input data . Our results show that the VT method can be successfully applied to identify galaxy groups even when only photometric redshifts are available .In this research we have discovered more than 12000 star groups at 0 < z < 0 . 3 . These groups contain about 30000 member galaxies .We additionally offer a list containing essential information such as positions , magnitudes , colors , and photometric redshifts for all these groups . Keywords : Galaxy Group , Photometric Redshift",
        "rewrite_text": "In this article, we introduce a novel algorithm designed to identify galaxy groups utilizing photometric redshifts, employing the Voronoi tessellation (VT) method. While the VT method has been extensively utilized for detecting clusters of galaxies with spectroscopic redshifts, its application in identifying galaxy groups based solely on photometric redshifts has not been previously explored. Our study leverages data from the Sloan Digital Sky Survey Data Release 5 (SDSS DR5) as the foundational dataset for our analysis. The findings demonstrate that the VT method is effective in recognizing galaxy groups, even in scenarios where only photometric redshifts are accessible. Throughout our investigation, we successfully identified over 12,000 galaxy groups within the redshift range of 0 < z < 0.3, encompassing approximately 30,000 member galaxies. Furthermore, we provide a comprehensive catalog that includes critical details for each identified group, such as their spatial positions, magnitudes, colors, and photometric redshifts. This work not only contributes to the understanding of galaxy group dynamics but also serves as a valuable resource for future astronomical research. Our results underscore the potential of photometric redshifts in group identification, paving the way for further studies in the field of galaxy formation and evolution. Keywords associated with this research include Galaxy Group and Photometric Redshift.",
        "ori-fast-z-score": -1.6644794391276478,
        "water-fast-z-score": 2.794002794004191,
        "rewrite-fast-z-score": -0.9761870601839528
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Constant Spectral Index for Sagittarius A* During Infrared/X-ray Intensity Variations .\nAbstract:\nWe report on the first simultaneous infrared (IR) and X-ray observations of Sgr A*, made with the Chandra X-Ray Observatory and the Spitzer Space Telescope in 2007-2008. We find that the IR emission is consistent with being produced by dust heated to temperatures between 100 K and 1000 K; this temperature range corresponds to an observed flux density at 8 microns ranging from 0.1 mJy to 1 Jy. The spectral index of the IR emission does not change significantly during these variations. This result suggests that the physical conditions within the emitting region are relatively constant over time scales as short as one month. These results also suggest that the IR emission may be dominated by optically thin thermal bremsstrahlung rather than synchrotron radiation. \n \n Keywords: black hole physics, infrared astronomy, radio source variability, space telescopes, X-ray astronomy \n \n \n \n Black holes have been predicted to produce intense electromagnetic fields near their event horizons. However, direct observational evidence has remained elusive because of the extreme environment surrounding such objects. One possible way to detect such fields would be through the detection of polarized light emitted close to the horizon. Another possibility involves detecting changes in the spectrum or intensity of the accretion flow onto the black hole itself. Such changes could occur if the magnetic field lines threading the disk were twisted into helical shapes due to differential rotation. If so, they can act like antennae which amplify any incoming waves along them. As a consequence, the local plasma frequency will increase, causing the plasma to become more opaque to lower-frequency waves but less opaque to higher frequencies. Thus, we expect the spectrum of the emission to steepen toward longer wavelengths when the system becomes brighter.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Constant Spectral Index for Sagittarius A * During Infrared / X - ray Intensity Variations . Abstract : We report on the first simultaneous laser ( IR ) and X - ray observations of Sgr A * , made with the Chandra X - Ray Observatory and the Spitzer Space Telescope in 2007 - 2008 .We see that the IR emission is consistent with being produced by dust warmed to temperatures between 100 K and 1000 K ; this heat range corresponds to an known flux concentration at 8 microns ranging from 0 . 1 mJy to 1 Jy . The spectral index of the IR emission does not change dramatically during these changes .This result suggests that the physical conditions within the emitting area are fairly constant over time ranges as short as one month . These conclusions additionally indicate that the IR emission may be dominated by optically thin thermal bremsstrahlung instead than synchrotron emission .Keywords : brown hole physics , infrared astronomy , television source variability , space telescopes , X - ray observations Black holes have been predicted to produce extreme electromagnetic forces near their event horizons . However , direct observational evidence has remained elusive because of the severe environment neighboring such objects .One likely way to identify such fields might be through the observation of polarized light emitted far to the horizon . Another possibility requires detecting changes in the spectrum or intensity of the accretion flow onto the dark hole itself .Such changes could occur if the magnetic field lines threading the disk were twisted into helical shapes due to differential rotation . If so , they can work like antennae which amplify any incoming signals along them .As a consequence , the local plasma rate will expand , forcing the plasma to become more opaque to lesser - frequency waves but less opaque to higher frequencies . Thus , we expect the spectrum of the emission to steepen toward longer wavelengths when the system gets stronger .",
        "rewrite_text": "**Title:** A Constant Spectral Index for Sagittarius A* During Infrared/X-ray Intensity Variations\n\n**Abstract:** In this study, we present the first simultaneous infrared (IR) and X-ray observations of Sagittarius A* (Sgr A*), conducted using the Chandra X-ray Observatory and the Spitzer Space Telescope during the years 2007 to 2008. Our findings indicate that the IR emissions from Sgr A* are consistent with being generated by dust that is heated to temperatures ranging from 100 K to 1000 K. This temperature range aligns with a known flux concentration at 8 microns, exhibiting values between 0.1 mJy and 1 Jy. Notably, we observe that the spectral index of the IR emission remains relatively stable despite variations in intensity, suggesting that the physical conditions within the emitting region are maintained consistently over time scales as short as one month. This stability implies that the IR emission may be primarily influenced by optically thin thermal bremsstrahlung rather than synchrotron processes. \n\nOur research contributes to the understanding of black hole physics, particularly in the context of infrared astronomy and the variability of emissions from such celestial objects. The extreme electromagnetic forces predicted to exist near the event horizons of black holes have historically posed challenges for direct observational studies due to the harsh environments surrounding these phenomena. One potential method for identifying these forces is through the observation of polarized light emitted from regions far beyond the event horizon. Alternatively, changes in the spectrum or intensity of the accretion flow onto the black hole could provide insights, particularly if the magnetic field lines within the accretion disk become twisted into helical configurations due to differential rotation. Such configurations could act as antennas, amplifying incoming signals. Consequently, this could lead to an expansion of the local plasma rate, resulting in increased opacity to lower-frequency waves while decreasing opacity to higher frequencies. Therefore, we anticipate that the emission spectrum will steepen towards longer wavelengths as the system's intensity increases. \n\n**Keywords:** black hole physics, infrared astronomy, source variability, space telescopes, X-ray observations.",
        "ori-fast-z-score": 1.2004900959975617,
        "water-fast-z-score": 7.661308776828737,
        "rewrite-fast-z-score": 1.8355998342755309
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Physical Properties of Weak MgII Absorbers at z~2 .\nAbstract:\nWe present the results of an investigation into the physical properties of weak Mg II absorbers (WAs) in quasar spectra, using high-resolution spectroscopy and photoionization modeling. We find that WAs are typically associated with galaxies having stellar masses M* ~ 10^10 to 10^11 solar masses, star formation rates SFR = 0.1-10 Msun/yr, and metallicities Z = 0.2 - 1 times solar metallicity. The majority of these systems have low ionization parameters log U < -2.5, indicating they are likely dominated by diffuse gas rather than dense clouds. These findings suggest that WAs may be related to galactic winds driven out of star-forming regions. In addition we find evidence for two distinct populations of WAs; one population is characterized by relatively strong absorption lines which appear to arise primarily within galaxy halos, while another population has weaker absorption features which seem to originate closer to the central region of their host galaxies.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Physical Properties of Weak MgII Absorbers at z ~ 2 . Abstract : We present the conclusion of an research into the physical properties of weak Mg II absorbers ( WAs ) in quasar spectra , using high - resolution spectroscopy and photoionization analysis .We see that WAs are typically associated with galaxies having stellar masses M * ~ 10 ^ 10 to 10 ^ 11 solar masses , star formation rates SFR = 0 . 1 - 10 Msun / yr , and metallicities Z = 0 . 2 - 1 twice solar metallicity . The majority of these systems have lowest ionization values log U < - 2 . 5 , showing they are likely dominated by diffuse plasma rather than dense clouds .These studies imply that WAs might be connected to galactic winds driven out of galaxy - creating areas . In addition we find proof for two different populations of WAs ; one community is characterized by relatively strong absorption patterns which appear to arise principally within star halos , while another population has strong absorption elements which appear to derive closer to the central region of their host galaxies .",
        "rewrite_text": "We present the findings of our investigation into the physical characteristics of weak Mg II absorbers (WAs) identified in quasar spectra, utilizing high-resolution spectroscopy and photoionization analysis. Our research reveals that WAs are predominantly linked to galaxies with stellar masses ranging from approximately \\(10^{10}\\) to \\(10^{11}\\) solar masses. These galaxies exhibit star formation rates (SFR) between 0.1 and 10 solar masses per year, along with metallicities ranging from 0.2 to 1 times solar metallicity. Notably, the majority of these absorbers display low ionization parameters, with values of log U < -2.5, suggesting that they are primarily influenced by diffuse plasma rather than dense clouds. This observation supports the hypothesis that WAs may be associated with galactic winds emanating from star-forming regions within galaxies. Furthermore, our analysis indicates the existence of two distinct populations of WAs. The first population is characterized by relatively strong absorption features that are predominantly found within the star halos of galaxies. In contrast, the second population exhibits strong absorption lines that appear to originate from regions closer to the central areas of their host galaxies. These findings contribute to our understanding of the interplay between galaxy formation, star formation processes, and the role of galactic winds in shaping the physical environment of weak Mg II absorbers. Overall, our study enhances the comprehension of the conditions and processes that govern the behavior of WAs, providing valuable insights into their significance in the broader context of galaxy evolution and intergalactic medium interactions.",
        "ori-fast-z-score": -0.6708203932499369,
        "water-fast-z-score": 5.666666666666667,
        "rewrite-fast-z-score": 0.552344770738994
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Logic of Reachable Patterns in Linked Data-Structures .\nAbstract:\nWe present a logic for reasoning about reachability properties on linked data structures, which are graphs with labeled nodes and edges that can be traversed using the Web Linking Language (WLL). The WLL is an extension to RDFS that allows one to specify how links between resources should be created or modified by applications. We show how our logic can be used to express important classes of queries over linked data such as finding all paths between two given nodes, computing the distance between them, checking whether there exists at least one path between them, etc., and we prove its decidability. Finally, we provide algorithms for evaluating these queries under certain restrictions. Our results have been implemented within the LOD2 system. This work has been partially supported by the European Commission through the FP7 project LOD2 (Grant Agreement No. 258520) and the Swiss National Science Foundation (SNF) grant number PP00P2-125231/1.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Logic of Reachable Patterns in Linked Data - Structures . Abstract : We present a logic for reasoning about reachability properties on linked data structures , which are graphs with designated vertices and edges that can be traversed using the Web Linking Language ( WLL ) .The WLL is an extension to RDFS that enables one to define how links between resources should be built or modified by applications . We see how our logic can be used to define essential classes of queries over connected data such as finding all paths between two given nodes , computing the distance between them , determining whether there exists at least one path between them , etc . , and we prove its decidability .Finally , we provide algorithms for evaluating these queries under certain constraints . Our results have been used within the LOD2 system .This project has been partially backed by the European Commission through the FP7 project LOD2 ( Grant Agreement No . 258520 ) and the Swiss National Science Foundation ( SNF ) funding designation PP00P2 - 125231 / 1 .",
        "rewrite_text": "We introduce a novel logic framework designed for analyzing reachability properties within linked data structures, which are essentially graphs characterized by specific vertices and edges that can be navigated using the Web Linking Language (WLL). The WLL serves as an enhancement to RDF Schema (RDFS), allowing applications to define and modify the connections between resources effectively. Our proposed logic facilitates the formulation of critical query classes pertinent to connected data, including tasks such as identifying all possible paths between two specified nodes, calculating the distance separating them, and verifying the existence of at least one traversable path. We establish the decidability of our logic, ensuring that these queries can be systematically resolved. Furthermore, we present algorithms tailored for the evaluation of these queries under particular constraints, enhancing their practical applicability. The implications of our findings have been integrated into the LOD2 system, demonstrating the utility of our approach in real-world scenarios. This research has received partial funding from the European Commission through the FP7 project LOD2 (Grant Agreement No. 258520) and the Swiss National Science Foundation (SNF) under funding designation PP00P2-125231/1. Our work contributes to the ongoing discourse in the field of linked data and offers a robust toolset for researchers and developers working with complex data structures on the web.",
        "ori-fast-z-score": 0.508000508000762,
        "water-fast-z-score": 4.5,
        "rewrite-fast-z-score": 0.9332565252573828
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Cosmological Simulations of the Preheating Scenario for Galaxy Cluster Formation: Comparison to Analytic Models and Observations .\nAbstract:\nWe present cosmological hydrodynamic simulations that follow the formation of galaxy clusters in the preheating scenario, where gas is heated by an early generation of stars before it collapses into dark matter haloes. We compare our results with observations of X-ray luminosity-temperature relations (L-T) at z = 0 as well as temperature profiles out to large radii. Our simulated L-T relation agrees very well with observational data over three orders of magnitude in luminosity. The slope of the observed L-T relation steepens towards lower temperatures while we find no such trend in our simulation. This discrepancy may be due to additional heating mechanisms not included in our model or due to systematic uncertainties in the observational sample. In addition, we show that the entropy profile of our simulated cluster population can reproduce the shape of observed profiles within their scatter. However, there are significant differences between individual simulated clusters and real systems which could be caused by non-gravitational processes like AGN feedback.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Cosmological Simulations of the Preheating Scenario for Galaxy Cluster Formation : Comparison to Analytic Models and Observations . Abstract : We create cosmological hydrodynamic simulations that take the formation of galaxy galaxies in the preheating scenario , where gas is hot by an early generation of stars before it collapses into dark matter haloes .We match our findings with observations of X - ray luminosity - temperature relations ( L - T ) at z = 0 as well as temperature profiles out to large radii . Our simulated L - T relation agrees very best with observational data over three orders of magnitude in luminosity .The slope of the reported L - T relation steepens towards lower altitudes while we find no such trend in our modeling . This discrepancy may be due to extra heating systems not mentioned in our model or due to systematic uncertainties in the observational sample .In addition , we find that the entropy profile of our simulated cluster population can mimic the shape of observed profiles within their scatter . However , there are significant variations between individual simulated clusters and actual environments which could be caused by non - gravitational processes like AGN feedback .",
        "rewrite_text": "In this study, we present a series of cosmological hydrodynamic simulations aimed at understanding galaxy cluster formation within the framework of the preheating scenario. This scenario posits that gas is heated by an early generation of stars prior to its collapse into dark matter halos. Our simulations are designed to investigate the implications of this preheating on the properties of galaxy clusters, particularly focusing on the X-ray luminosity-temperature (L-T) relation at redshift z = 0, as well as the temperature profiles extending to large radii.\n\nOur results indicate a strong agreement between the simulated L-T relation and observational data across three orders of magnitude in luminosity. However, we observe a notable steepening of the L-T relation at lower luminosities in the observational data, a trend that is not replicated in our simulations. This discrepancy raises questions about potential additional heating mechanisms that may not be accounted for in our model, or it could reflect systematic uncertainties inherent in the observational datasets.\n\nFurthermore, we analyze the entropy profiles of our simulated cluster population, which show a capacity to replicate the general shape of observed entropy profiles, albeit within a certain degree of scatter. Despite this overall agreement, we note significant variations between individual simulated clusters and their real-world counterparts. These differences may be influenced by non-gravitational processes, such as active galactic nucleus (AGN) feedback, which could play a critical role in shaping the environments of galaxy clusters.\n\nIn conclusion, our findings contribute to the ongoing discourse on galaxy cluster formation, highlighting both the strengths and limitations of the preheating scenario as a framework for understanding the complex interplay of physical processes in the early universe. Further investigations are warranted to reconcile the discrepancies observed and to refine our models to better align with empirical data.",
        "ori-fast-z-score": -0.8834522085987723,
        "water-fast-z-score": 3.312945782245396,
        "rewrite-fast-z-score": 0.780398972571708
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  HD97048: a closer look to the disk .\nAbstract:\nWe present new near-infrared (NIR) spectroscopy and photometry for HD 97048, an evolved star with a dusty circumstellar environment that is surrounded by a large debris disk. The NIR spectrum shows strong emission lines in H I Paschen series as well as Brackett γ line at 2.166 µm. We also detect CO bandheads around 2.3 µm which are characteristic features of late-type stars. In addition, we find evidence of water vapor absorption bands near 1.4-1.8 µm indicating the presence of warm water vapor in the inner part of the system. \n \n Using our newly obtained data together with archival optical spectra, we have derived physical parameters such as effective temperature T eff = 8200 K, surface gravity log g = 3.9 dex, luminosity L = 4 × 10^6 Lsun, mass M = 5M⊙, radius R = 6R⊙, and age t = 7×10^7 years. These values indicate that this object belongs to the red giant branch phase on its way towards becoming a white dwarf.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : HD97048 : a closer look to the disk . Abstract : We bring latest near - infrared ( NIR ) spectroscopy and photometry for HD 97048 , an evolved star with a dusty circumstellar climate that is surrounded by a large debris ring .The NIR spectrum displays strong emitted lines in H I Paschen series as also as Brackett γ line at 2 . 166 µm . We additionally observe CO bandheads around 2 . 3 µm which are peculiar characteristics of late - class stars .In addition , we find proof of water vapor absorption patterns near 1 . 4 - 1 . 8 µm indicating the presence of warm water vapor in the inner part of the system . Using our freshly acquired data together with archival optical spectra , we have derived physical values such as effective heat T eff = 8200 K , surface gravity log f = 3 . 9 dex , luminosity L = 4 × 10 ^ 6 Lsun , mass M = [UNK] , diameter R = [UNK] , and age t = 7×10 ^ 7 years .These values indicate that this body belongs to the red giant branch stage on its way towards becoming a white dwarf .",
        "rewrite_text": "In this study, we present the latest near-infrared (NIR) spectroscopy and photometry of HD 97048, an evolved star characterized by a dusty circumstellar environment and a substantial debris ring. Our NIR spectral analysis reveals prominent emission lines from the H I Paschen series, alongside the Brackett γ line at 2.166 µm. Notably, we also detect CO bandheads around 2.3 µm, which are distinctive features associated with late-type stars. Furthermore, our observations indicate the presence of warm water vapor in the inner regions of the system, as evidenced by absorption patterns between 1.4 and 1.8 µm. \n\nBy integrating our newly acquired data with archival optical spectra, we have calculated several key physical parameters of HD 97048. These include an effective temperature (T_eff) of 8200 K, a surface gravity (log g) of 3.9 dex, a luminosity (L) of approximately 4 × 10^6 L_sun, and an estimated age of about 7 × 10^7 years. Although the mass and diameter remain undetermined, the derived values suggest that HD 97048 is situated on the red giant branch of stellar evolution, progressing towards its eventual transformation into a white dwarf. This research enhances our understanding of the evolutionary processes of stars like HD 97048 and their surrounding environments, providing valuable insights into the characteristics of evolved stars and their circumstellar disks.",
        "ori-fast-z-score": -1.2074068598865937,
        "water-fast-z-score": 3.4914862437758782,
        "rewrite-fast-z-score": 0.20412414523193154
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Multiwaveband Polarimetric Observations of 15 Active Galactic Nuclei at High Frequencies: Correlated Polarization Behavior .\nAbstract:\nWe present multiwavelength polarimetric observations of fifteen active galactic nuclei (AGNs) made with the Very Large Array and the Karl G. Jansky Very Large Array between 5 and 43 GHz, including simultaneous measurements in two bands for eight sources. We find that most AGN show correlated polarization behavior across our frequency range; however, we also identify several cases where this is not true. In particular, we observe significant changes in fractional linear polarization over short timescales as well as large differences in position angle between different frequencies. These results are consistent with previous studies showing rapid variability on small spatial scales within these objects. \n \n Keywords: radio astronomy, active galaxy, polarization, VLBI, VLA, KVNVLA, JVLBA, high-frequency radio emission, jets, blazars \n \n \n \n 1 Introduction \n \n The study of polarized radiation provides important information about magnetic fields in astrophysical environments ranging from planets to galaxies. However, it can be difficult to obtain accurate estimates of the degree of polarization because of instrumental effects such as beam depolarization or calibration errors. This problem becomes more severe when studying faint sources observed at low signal-to-noise ratios. To overcome these difficulties, many authors have used statistical techniques to estimate the intrinsic properties of their sample populations  1–3 . For example, Hovatta et al. (2012)  4  studied the distribution of Stokes parameters using Monte Carlo simulations to determine the mean values and standard deviations of the distributions. They found that the average degrees of polarization were typically higher than those reported by other authors who had analyzed similar data sets  5–7 . Similarly, Lister & Homan (2005)  8  investigated the polarization properties of bright quasars using a maximum likelihood method to fit Gaussian functions to the Stokes parameter histograms. Their analysis revealed that the majority of quasars exhibited circularly polarized components along with linearly polarized ones.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Multiwaveband Polarimetric Observations of 15 Active Galactic Nuclei at High Frequencies : Correlated Polarization Behavior . Abstract : We report multiwavelength polarimetric discoveries of fifteen active galactic nuclei ( AGNs ) made with the Very Large Array and the Karl G . Jansky Very Large Array between 5 and 43 GHz , including simultaneous measurements in two bands for eight sources .We see that most AGN show associated polarization behavior across our frequency spectrum ; however , we also identify several circumstances where this is not true . In particular , we study substantial alterations in fractional linear polarization over short timescales as well as huge variations in direction angle between various frequencies .These conclusions are compatible with previous research displaying quick variability on small spatial scales within these objects . Keywords : radio astronomy , active galaxy , polarization , VLBI , VLA , KVNVLA , JVLBA , low - frequency radio emission , jets , blazars 1 Introduction The investigation of polarized emission provides crucial data about magnetic fields in astrophysical conditions including from stars to galaxies .However , it can be harder to obtain precise estimates of the degree of polarization because of instrumental effects such as beam depolarization or calibration errors . This problem arises more serious when examining faint sources observed at low sound - to - noise ratios .To solve these problems , various scientists have utilized statistical algorithms to estimate the intrinsic characteristics of their sample populations 1 – 3 . For instance , Hovatta et al .( 2012 ) 4 studied the distribution of Stokes variables using Monte Carlo simulations to find the mean values and standard deviations of the distributions . They found that the average degrees of polarization were generally greater than those noted by other researchers who had examined similar statistics sets 5 – 7 .Similarly , Lister & Homan ( 2005 ) 8 examined the polarization properties of bright quasars using a maximum likelihood technique to fitting Gaussian functions to the Stokes parameter histograms . Their analysis revealed that the majority of quasars demonstrated circularly polarized components along with linearly polarized ones .",
        "rewrite_text": "**Title:** Multiwaveband Polarimetric Observations of 15 Active Galactic Nuclei at High Frequencies: Correlated Polarization Behavior\n\n**Abstract:** In this study, we present the results of multiwavelength polarimetric observations of fifteen active galactic nuclei (AGNs) conducted using the Very Large Array (VLA) and the Karl G. Jansky Very Large Array across frequencies ranging from 5 to 43 GHz. Our observations include simultaneous measurements in two frequency bands for eight of the sources. The findings reveal that a majority of the AGNs exhibit correlated polarization behavior throughout the observed frequency range. However, we also identify several instances where this correlation does not hold, indicating complex underlying processes. Notably, we observe significant fluctuations in fractional linear polarization over short timescales, as well as substantial variations in the angle of polarization direction across different frequencies. These results align with previous studies that have documented rapid variability occurring on small spatial scales within these celestial objects. \n\nThe investigation of polarized emissions is essential for understanding the magnetic fields present in various astrophysical environments, from stars to galaxies. However, accurately determining the degree of polarization can be challenging due to instrumental factors such as beam depolarization and calibration inaccuracies, particularly when dealing with faint sources that exhibit low signal-to-noise ratios. To address these challenges, researchers have employed statistical algorithms to derive intrinsic properties of their sample populations. For instance, Hovatta et al. (2012) utilized Monte Carlo simulations to analyze the distribution of Stokes parameters, revealing that the average degrees of polarization were generally higher than those reported in earlier studies. Similarly, Lister & Homan (2005) investigated the polarization characteristics of bright quasars using a maximum likelihood approach to fit Gaussian functions to Stokes parameter histograms, finding that most quasars exhibited both circularly and linearly polarized components. This work contributes to the growing body of knowledge regarding the polarization behavior of AGNs and highlights the importance of multiwaveband observations in astrophysical research.\n\n**Keywords:** radio astronomy, active galaxies, polarization, VLBI, VLA, KVNVLA, JVLBA, low-frequency radio emission, jets, blazars.",
        "ori-fast-z-score": -0.6666666666666666,
        "water-fast-z-score": 8.165382378538993,
        "rewrite-fast-z-score": 1.556540648986177
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Galois theory of iterated endomorphisms .\nAbstract:\nThe Galois group is the fundamental object in classical Galois theory, which studies the solvability of polynomial equations over finite fields.  In this talk we will introduce the notion of ``Galois groups   for infinite families of polynomials and study their properties using tools from algebraic geometry.   We will also discuss some applications to number theory such as the proof of the abc conjecture by Vojta (joint work with J. Pila). The main results are joint works with A. Chambert-Loir, D. Ghioca, M. Harris, C. Lairez, S. Popescu-Tarauca, B. Stoll, E. Szpiro, T. Tucker-Drob, and V. Vojta. This talk was given at the conference  Algebraic Geometry and Arithmetic Dynamics  held on June 24-28, 2013 at MSRI Berkeley. It has been recorded by Adam Hartung.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Galois theory of iterated endomorphisms . Abstract : The Galois group is the fundamental object in classical Galois theory , which studies the solvability of polynomial equations over polynomial fields .In this talk we will explore the notion of ` ` Galois groups for infinite families of polynomials and study their properties using techniques from algebraic theory . We will also discuss some applications to number theory such as the proof of the abc hypothesis by Vojta ( collaboration work with J . Pila ) .The main results are joint works with A . Chambert - Loir , D . Ghioca , M . Harris , C . Lairez , S . Popescu - Tarauca , B . Stoll , E . Szpiro , T . Tucker - Drob , and V . Vojta . This discussion was given at the meeting Algebraic Geometry and Arithmetic Dynamics holding on June 24 - 28 , 2013 at MSRI Berkeley .It has been recorded by Adam Hartung .",
        "rewrite_text": "Title: Galois Theory of Iterated Endomorphisms\n\nAbstract: The Galois group serves as a cornerstone in classical Galois theory, which investigates the conditions under which polynomial equations can be solved within polynomial fields. In this presentation, we delve into the concept of Galois groups associated with infinite families of polynomials, examining their characteristics through the lens of algebraic theory. Our exploration will encompass various properties of these Galois groups, highlighting their significance in understanding the broader implications of polynomial solvability. Additionally, we will address notable applications in number theory, particularly focusing on the proof of the abc conjecture as established by Vojta, in collaboration with J. Pila. The findings discussed are a result of collaborative efforts with esteemed researchers including A. Chambert-Loir, D. Ghioca, M. Harris, C. Lairez, S. Popescu-Tarauca, B. Stoll, E. Szpiro, T. Tucker-Drob, and V. Vojta. This discourse was presented at the Algebraic Geometry and Arithmetic Dynamics conference held from June 24 to 28, 2013, at the Mathematical Sciences Research Institute (MSRI) in Berkeley. The session was recorded by Adam Hartung, providing an opportunity for further dissemination and engagement with the material presented. Through this talk, we aim to enhance the understanding of Galois theory's applications and its implications for both algebra and number theory, paving the way for future research in these interconnected fields.",
        "ori-fast-z-score": -0.5773502691896258,
        "water-fast-z-score": 2.4285714285714284,
        "rewrite-fast-z-score": 1.2701705922171767
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Ramsey interferometry with a two-level Tonks-Girardeau gas .\nAbstract:\nWe study the Ramsey interference fringes in an ultracold atomic gas trapped by a one-dimensional optical lattice potential and interacting via repulsive contact interactions described by the Tonks-Girardeu model. We show that, for sufficiently strong interaction strength, the system exhibits a transition to a regime where the visibility of the Ramsey fringe pattern is suppressed due to destructive quantum interferences between different scattering channels. The effect can be understood as arising from the formation of bound states within each well of the periodic potential which are responsible for the suppression of tunneling across neighboring wells. Our results demonstrate how the properties of strongly correlated systems can be probed using standard experimental techniques such as Ramsey spectroscopy. Introduction:-Ultracold atoms confined in optical lattices have been used extensively over recent years to explore many-body phenomena  1  . In particular, they provide a unique opportunity to investigate the physics of strongly-correlated fermionic gases  2  , including the crossover from Bardeen-Cooper-Schrieffer (BCS) superfluidity at weak coupling  3  to Bose-Einstein condensation (BEC)  4  at strong coupling  5  .\nRamsey spectroscopy  6  has become a powerful tool for studying coherent dynamics in these systems  7, 8  . It allows one to probe the energy spectrum  9  or the coherence time  10  of the system through the measurement of the population difference after applying a sequence of radio-frequency pulses  11  . Recently it was shown  12  that this technique also provides information about the nature of correlations present in the system  13  . For example, in Ref.  14  it was demonstrated experimentally that the presence of pairing correlations leads to characteristic features in the Ramsey fringe patterns observed in a Fermi gas  15  . However, despite its successes, there remain several open questions regarding the interpretation of the Ramsey signal  16  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Ramsey interferometry with a two - level Tonks - Girardeau gas . Abstract : We explore the Ramsey interference fringes in an ultracold nuclear gas trapped by a one - dimensional optical lattice potential and interacting via repulsive contact interactions represented by the Tonks - Girardeu model .We see that , for enough strong coupling strength , the system displays a shift to a regime where the visibility of the Ramsey fringe structure is suppressed due to destructive quantum interferences between various scattering channels . The phenomenon can be understood as occurring from the formation of bound states within each well of the periodic potential which are responsible for the suppression of tunneling across neighboring wells .Our results show how the properties of highly correlated systems can be probed using conventional experimental methods such as Ramsey spectroscopy . Introduction : - Ultracold atoms confined in laser lattices have been used heavily over recent years to study many - bodies phenomena 1 .In particular , they give a unique opportunity to examine the physics of highly - interacting fermionic gases 2 , notably the crossover from Bardeen - Cooper - Schrieffer ( BCS ) superfluidity at weak correlation 3 to Bose - Einstein condensation ( BEC ) 4 at weak correlation 5 . Ramsey spectroscopy 6 has become a powerful tool for studying coherent mechanics in these systems 7 , 8 .It enables one to probe the power spectrum 9 or the coherence time 10 of the system through the observation of the population difference after applying a sequence of radio - frequency pulses 11 . Recently it was shown 12 that this methodology also provides knowledge about the nature of correlations observed in the system 13 .For instance , in Ref . 14 it was demonstrated experimentally that the presence of pairing correlations leads to distinctive features in the Ramsey fringe structures encountered in a Fermi gas 15 .However , despite its successes , there remain many open questions regarding the interpretation of the Ramsey signal 16 .",
        "rewrite_text": "**Title:** Ramsey Interferometry with a Two-Level Tonks-Girardeau Gas\n\n**Abstract:** In this study, we investigate the phenomenon of Ramsey interference fringes within an ultracold nuclear gas that is confined by a one-dimensional optical lattice potential and interacts through repulsive contact interactions, as described by the Tonks-Girardeau model. Our findings reveal that when the coupling strength is sufficiently high, the system transitions into a regime where the visibility of the Ramsey fringes is notably diminished. This suppression is attributed to destructive quantum interferences arising from multiple scattering channels. We interpret this behavior as a result of bound state formation within each lattice well, which inhibits tunneling between adjacent wells. Our results highlight the potential of utilizing conventional experimental techniques, such as Ramsey spectroscopy, to probe the intricate properties of highly correlated systems. \n\n**Introduction:** Over the past few years, ultracold atoms confined in optical lattices have emerged as a vital platform for investigating many-body phenomena. These systems provide a unique framework for exploring the physics of strongly interacting fermionic gases, particularly the transition from Bardeen-Cooper-Schrieffer (BCS) superfluidity at weak correlations to Bose-Einstein condensation (BEC) at strong correlations. Ramsey spectroscopy has proven to be an invaluable tool for examining coherent dynamics in these ultracold systems, allowing researchers to analyze the power spectrum and coherence time by observing population differences following a series of radio-frequency pulses. Recent studies have demonstrated that Ramsey spectroscopy can also yield insights into the nature of correlations present in these gases. For instance, experimental evidence has shown that pairing correlations manifest as distinct features in the Ramsey fringe patterns of a Fermi gas. Despite the successes of this methodology, numerous questions remain regarding the interpretation of the Ramsey signal, indicating a need for further exploration in this area.",
        "ori-fast-z-score": -1.4316582658130823,
        "water-fast-z-score": 6.606311879056017,
        "rewrite-fast-z-score": 1.110664955031789
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Water s Hydrogen Bond Strength .\nAbstract:\nThe hydrogen bond is the strongest noncovalent interaction in water, and it plays an important role in determining its physical properties.  The strength of this bond can be measured by infrared spectroscopy or nuclear magnetic resonance (NMR) techniques.   Infrared spectroscopy measures how much energy is absorbed when vibrating molecules are exposed to infrared light.   NMR uses radio waves instead of light to measure the amount of energy needed to change the spin state of atoms within a molecule.   ... Figure 1 . Water has two different types of hydrogen bonds that form between neighboring molecules:  O-H⋯O hydrogen bonds occur along the edges of tetrahedral clusters; these bonds have relatively short distances but weak strengths.  H-O⋯H hydrogen bonds connect adjacent tetrahedra into larger structures called ice crystals; these bonds have longer distances than those found on cluster edges but stronger strengths.  These images were created using VMD software.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Water s Hydrogen Bond Strength . Abstract : The hydrogen bond is the strongest noncovalent interaction in water , and it serves an important role in determining its physical properties .The strength of this bond can be determined by infrared spectroscopy or atomic magnetic resonance ( NMR ) techniques . Infrared spectroscopy measures how many electricity is absorbed when vibrating compounds are exposed to infrared light .NMR employs radio beams instead of light to measure the amount of power needed to shift the spin state of atoms within a molecule . . . .Figure 1 . Water has two different kind of hydrogen bonds that form between neighboring molecules : O - [UNK] hydrogen bonds exist along the edges of tetrahedral clusters ; these bonds have fairly short distances but weak strengths .H - [UNK] hydrogen bonds connect adjacent tetrahedra into larger structures called ice crystals ; these bonds have greater distances than those found on cluster edges but stronger strengths . These images were created using VMD software .",
        "rewrite_text": "**Title: The Strength of Hydrogen Bonds in Water**\n\n**Abstract:** Hydrogen bonds represent the most robust form of noncovalent interactions in water, playing a crucial role in shaping its unique physical properties. This article explores the methodologies for quantifying the strength of hydrogen bonds in water, primarily through infrared spectroscopy and nuclear magnetic resonance (NMR) techniques. Infrared spectroscopy assesses the absorption of infrared light by vibrating molecular compounds, providing insights into the energy dynamics of hydrogen bonding. Conversely, NMR utilizes radiofrequency radiation to evaluate the energy required to alter the spin states of atoms within a molecule, offering a complementary perspective on bond strength.\n\nThe study highlights two distinct types of hydrogen bonds present in water, which significantly influence its molecular structure and behavior. The first type, O-H hydrogen bonds, are found at the edges of tetrahedral clusters formed by water molecules. These bonds are characterized by relatively short distances but exhibit weaker strengths. The second type, H-O hydrogen bonds, link adjacent tetrahedra into larger structures known as ice crystals. These bonds, while having greater distances compared to those at the cluster edges, demonstrate stronger interactions.\n\nVisual representations of these hydrogen bonding configurations were generated using Visual Molecular Dynamics (VMD) software, illustrating the complex interplay between molecular arrangement and bond strength. Understanding the nuances of hydrogen bond strength in water is essential for elucidating its physical properties and behaviors, which have profound implications in various scientific fields, including chemistry, biology, and environmental science. This article aims to provide a comprehensive overview of the current knowledge regarding hydrogen bond strength in water, emphasizing its significance in both theoretical and practical applications.",
        "ori-fast-z-score": -0.741998516004452,
        "water-fast-z-score": 2.2013981571160284,
        "rewrite-fast-z-score": 0.727606875108999
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Metric for Gradient RG Flow of the Worldsheet Sigma Model Beyond First Order .\nAbstract:\nWe propose an improved metric on the space of worldsheet sigma model couplings that is suitable to study gradient renormalization group flows beyond first order in perturbation theory. The new metric has several advantages over previous proposals, including manifestly positive kinetic terms and no need for additional counterterms at higher orders. We show how this metric can be used to compute beta functions up to third order in perturbation theory using only Feynman diagrams with one-loop vacuum bubbles as building blocks. This allows us to obtain results for the beta function of the dilaton coupling to the Ricci scalar which are consistent with those obtained by other methods but have not been previously available due to technical difficulties. In addition we find evidence for non-trivial fixed points in the beta function of the string coupling constant. These results provide further support for the idea that the worldsheet sigma model may serve as a useful tool for studying quantum gravity. Introduction: It was recently shown  1  that the worldsheet sigma-model (WSSM) provides a powerful framework for investigating quantum gravity via its connection to the gravitational path integral  2  . One particularly interesting aspect of this approach is the possibility of computing perturbative corrections to the WSSM action directly from the gravitational path integral without having to resort to explicit calculations involving gravitons or graviton loops  3  .\nIn  4  it was proposed that the WSSM could also be used to investigate the flow of the effective action under the renormalization group (RG). However, since the WSSM contains infinitely many degrees of freedom there does not exist any finite dimensional parameter space where the RG flow takes place. Instead, the RG flow must take place along some infinite-dimensional trajectory through the space of all possible actions. To make progress towards understanding such trajectories it would be helpful if one were able to define a sensible metric on the space of WSSM actions so that distances between different actions could be measured. Such a metric should allow one to determine whether two given actions lie close together or far apart in the space of all possible WSSMs.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Metric for Gradient RG Flow of the Worldsheet Sigma Model Beyond First Order . Abstract : We suggest an better metric on the space of worldsheet sigma model couplings that is suitable to study gradient renormalization group flows beyond first order in perturbation theory .The revised metric has numerous benefits over past proposals , notably manifestly strong kinetic terms and no requirement for additional counterterms at higher orders . We see how this metric can be used to compute beta functions up to third order in perturbation theory employing only Feynman diagrams with one - loop vacuum bubbles as building blocks .This enables us to obtain results for the beta function of the dilaton coupling to the Ricci scalar which are compatible with those achieved by other methods but have not been previously available owing to technical problems . In addition we find proof for non - simple fixed points in the beta function of the string coupling constant .These data provide further evidence for the idea that the worldsheet sigma model may serve as a helpful resource for studying quantum gravitational . Introduction : It was recently shown 1 that the worldsheet sigma - model ( WSSM ) presents a powerful framework for investigating quantum gravitational via its connection to the gravitational path integral 2 .One especially interesting aspect of this methodology is the prospect of computing perturbative corrections to the WSSM action directly from the gravitational direction equation without having to resort to explicit calculations concerning gravitons or graviton loops 3 . In 4 it was suggested that the WSSM could also be used to examine the flow of the effective action under the renormalization group ( RG ) .However , since the WSSM contains infinitely many degrees of liberty there does not exist any finite dimensional parameter room where the RG flow takes place . Instead , the RG flow must take place along some infinite - dimensional trajectory through the space of all possible actions .To build progress towards studying such trajectories it would be beneficial if one were trying to define a practical metric on the space of WSSM actions so that lengths between multiple movements could be determined . Such a metric should enable one to estimate whether two given actions reside close together or far separated in the space of all possible WSSMs .",
        "rewrite_text": "**Title:** A Metric for Gradient RG Flow of the Worldsheet Sigma Model Beyond First Order\n\n**Abstract:** In this article, we propose an enhanced metric for the space of couplings in the worldsheet sigma model (WSSM) that facilitates the investigation of gradient renormalization group (RG) flows beyond the first order in perturbation theory. Our revised metric offers several advantages over previous approaches, including the presence of manifestly strong kinetic terms and the elimination of the need for additional counterterms at higher orders. We demonstrate the application of this metric in calculating beta functions up to the third order in perturbation theory, utilizing only Feynman diagrams featuring one-loop vacuum bubbles as foundational components. This methodology allows us to derive results for the beta function associated with the dilaton coupling to the Ricci scalar, which align with findings from alternative methods but have remained inaccessible due to prior technical challenges. Furthermore, we provide evidence for the existence of non-simple fixed points in the beta function of the string coupling constant. These findings contribute to the growing body of evidence suggesting that the WSSM can be a valuable tool for exploring quantum gravitational phenomena.\n\nThe significance of the WSSM as a framework for probing quantum gravity has been underscored in recent studies, particularly through its relationship with the gravitational path integral. A notable advantage of this approach is the ability to compute perturbative corrections to the WSSM action directly from the gravitational equations of motion, circumventing the need for explicit calculations involving gravitons or graviton loops. Previous work has indicated the potential of the WSSM to analyze the flow of the effective action under RG transformations. However, due to the infinite degrees of freedom inherent in the WSSM, the RG flow cannot be confined to a finite-dimensional parameter space; rather, it must traverse an infinite-dimensional trajectory within the realm of all conceivable actions. To advance our understanding of such trajectories, establishing a practical metric on the space of WSSM actions is essential, as it would allow for the assessment of distances between various actions, thereby indicating their proximity or separation within the expansive landscape of possible WSSMs.",
        "ori-fast-z-score": 0.30151134457776363,
        "water-fast-z-score": 8.192941705230835,
        "rewrite-fast-z-score": 1.1895773785772161
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Gauge-Higgs unification with broken flavor symmetry .\nAbstract:\nWe propose an extension to the Standard Model in which the Higgs boson is identified as a pseudo-Nambu-Goldstone boson associated with spontaneous breaking of a global U(1) flavor symmetry, and show that this model can be embedded into a grand unified theory based on SO (10) . We also discuss how our proposal could address some outstanding issues related to neutrino masses and mixings. \n \n Introduction \n \n The Standard Model (SM), despite its remarkable successes at describing experimental data up to now, suffers from several theoretical problems such as gauge hierarchy problem  1  , strong CP problem  2  , and fermion mass generation  3  . In order to solve these problems, many extensions beyond SM have been proposed so far  4  -  8  .\n \nIn particular, it has recently been shown  9  that if we identify the Higgs field as a pseudo-Nambu Goldstone Boson (PNGB) associated with spontaneously broken global symmetries  10  , then all the above mentioned difficulties are naturally solved within one framework  11  -  13  . This idea was first introduced by Weinberg  14  for solving the strong CP problem, but later extended to other cases  15  -  17  . It should be noted here that there exist various ways to realize PNGBs  18  -  20  . \n \n Gauging the global symmetry leads to massive vector bosons corresponding to the generators of the group  21  . If the scale of the global symmetry breaking is much higher than electroweak scale, those heavy vector bosons may play important roles in cosmology  22  or astrophysics  23  . On the other hand, if the scale of the global symmmetry breaking is close to the electroweak scale, they will appear as new particles around TeV region  24  . These new particles might be observed at LHC experiments  25  -  27  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Gauge - Higgs unification with broken flavor symmetry . Abstract : We suggest an addition to the Standard Model in which the Higgs boson is identified as a quasi - Nambu - Goldstone boson associated with spontaneous breaking of a global U ( 1 ) flavor symmetry , and suggest that this description can be embedded into a grand unified theory based on SO ( 10 ) .We also discuss how our proposal could answer some significant matters related to neutrino masses and mixings . Introduction The Standard Model ( SM ) , despite its remarkable accomplishments at describing theoretical data up to now , suffers from several experimental difficulties such as gauge hierarchy problem 1 , strong CP problem 2 , and fermion mass generation 3 .In order to solve these problems , various extensions beyond SM have been proposed so far 4 - 8 . In particular , it has recently been shown 9 that if we identify the Higgs field as a quasi - Nambu Goldstone Boson ( PNGB ) associated with spontaneously shattered global symmetries 10 , then all the above mentioned difficulties are naturally solved within one framework 11 - 13 .This idea was first described by Weinberg 14 for solving the strong CP problem , but later extended to other instances 15 - 17 . It should be mentioned here that there exist various ways to realize PNGBs 18 - 20 .Gauging the global symmetry leads to massive vector bosons corresponding to the generators of the group 21 . If the scale of the global symmetry breaking is much higher than electroweak scale , those heavy vector bosons may play major roles in cosmology 22 or astrophysics 23 .On the other hand , if the scale of the global symmmetry broken is close to the electroweak scale , they will appear as new particles around TeV area 24 . These new objects may be encountered at LHC experiments 25 - 27 .",
        "rewrite_text": "In this article, we propose an innovative extension to the Standard Model (SM) by identifying the Higgs boson as a quasi-Nambu-Goldstone boson (qNGB) that emerges from the spontaneous breaking of a global U(1) flavor symmetry. This framework not only provides a fresh perspective on the Higgs mechanism but also allows for its integration into a grand unified theory, specifically one based on SO(10). Our approach addresses several pressing issues within the SM, including the gauge hierarchy problem, the strong CP problem, and the generation of fermion masses. \n\nThe SM, while successful in many respects, faces significant experimental challenges that necessitate further theoretical development. Recent advancements have demonstrated that conceptualizing the Higgs field as a qNGB linked to spontaneously broken global symmetries can effectively resolve these challenges within a cohesive theoretical structure. This notion, initially introduced by Weinberg to tackle the strong CP problem, has since been expanded to encompass a broader range of phenomena. \n\nMoreover, we explore the implications of gauging the global symmetry, which results in the emergence of massive vector bosons associated with the symmetry's generators. Depending on the scale of the symmetry breaking, these vector bosons can have profound implications for cosmology and astrophysics if their mass is significantly greater than the electroweak scale. Conversely, if the symmetry breaking occurs near the electroweak scale, these new particles could manifest as detectable entities in the TeV range, potentially observable in experiments at the Large Hadron Collider (LHC). \n\nIn addition, our framework offers insights into the complexities surrounding neutrino masses and mixing, suggesting that the unification of gauge and Higgs sectors could provide a pathway to understanding these phenomena. Overall, our proposal not only enriches the theoretical landscape of particle physics but also opens new avenues for experimental exploration.",
        "ori-fast-z-score": 0.7184212081070996,
        "water-fast-z-score": 5.09823498869952,
        "rewrite-fast-z-score": 0.9760921603577252
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Criteria in the Selection of Target Events for Planetary Microlensing Follow-Up Observation .\nAbstract:\nWe present an analysis on how to select target events for planetary microlensing follow-up observations, based on our experience with PLANETS project. We find that there are three criteria which should be considered when selecting targets for follow up observation; (1) The event duration, (2) The lens mass and distance, and (3) The planet detection efficiency. In addition we also discuss other factors such as weather condition at the time of the event peak, and the possibility of detecting planets around brown dwarfs. Finally, we give some suggestions about future projects. Keywords: Microlensing; Planets; Brown Dwarfs; Follow-up Observations. 1 Introduction Microlensing is one of the most promising methods to detect extrasolar planets because it can provide information not only on the existence but also on physical properties of planets without any biases against their orbital parameters or host star types. However, since the Einstein ring radius of a lensing object is typically much smaller than its angular size, it takes several years to monitor thousands of stars continuously by using ground-based telescopes. Therefore, many groups have been conducting intensive monitoring programs toward Galactic bulge fields during the last decade. As a result, more than 100 exoplanet candidates were discovered so far through this method  1  . Among them, however, only four planets have been confirmed by radial velocity measurements  2  .\nMicrolensing events occur due to gravitational lensing effect between two objects separated by large distances. When a background source passes close enough to a foreground lensing object, the light rays coming from the source will bend towards the lensing object. This causes magnification of the source flux. If a planet exists near the lensing object, additional perturbation occurs in the lensing light curve. Since the amount of the perturbation depends strongly on the projected separation between the lensing object and the planet, the presence of a planet can be detected by analyzing the shape of the lensing light curve  3, 4  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Criteria in the Selection of Target Events for Planetary Microlensing Follow - Up Observation . Abstract : We present an assessment on how to select target events for planetary microlensing follow - up observations , based on our experience with PLANETS project .We see that there are three requirements which should be included when choosing targets for follow up observation ; ( 1 ) The event duration , ( 2 ) The lens mass and distance , and ( 3 ) The planet detection efficiency . In addition we also discuss other parameters such as weather condition at the period of the event peak , and the prospect of detecting planets around red dwarfs .Finally , we give some recommendations about future projects . Keywords : Microlensing ; Planets ; Brown Dwarfs ; Follow - up Observations .1 Introduction Microlensing is one of the most attractive mechanisms to locate extrasolar planets because it can provide information not only on the existence but also on physical properties of stars without any biases against their orbital characteristics or host star types . However , since the Einstein circle length of a lensing object is typically much smaller than its angular height , it takes several decades to observe dozens of stars constantly by using ground - based telescopes .Therefore , large teams have been conducting aggressive study efforts toward Galactic bulge fields during the last decade . As a result , more than 100 exoplanet prospects were discovered so far through this process 1 .Among them , however , only four planets have been confirmed by radial speed measurements 2 . Microlensing events occur due to gravitational lensing effect between two bodies connected by large distances .When a background source passes close enough to a foreground lensing object , the light rays coming from the source will bend towards the lensing object . This creates magnification of the source flux .If a planet appears near the lensing object , additional perturbation appears in the lensing light curve . Since the extent of the perturbation varies strongly on the projected separation between the lensing object and the planet , the presence of a planet can be identified by analyzing the form of the lensing light circle 3 , 4 .",
        "rewrite_text": "**Title:** Criteria for Selecting Target Events for Planetary Microlensing Follow-Up Observations\n\n**Abstract:** In this study, we evaluate the criteria for selecting target events for follow-up observations in planetary microlensing, drawing on our experiences with the PLANETS project. Our analysis identifies three critical factors that should guide the selection process: (1) the duration of the microlensing event, (2) the mass and distance of the lensing object, and (3) the efficiency of detecting planets. These criteria are essential for optimizing the chances of successfully identifying and characterizing exoplanets through microlensing techniques. Additionally, we explore other influential parameters, such as the weather conditions during the peak of the event and the potential for detecting planets around red dwarf stars, which may significantly impact observational outcomes. \n\nMicrolensing is a powerful method for discovering extrasolar planets, as it allows astronomers to gather information about both the existence and physical characteristics of stars without biases related to their orbital configurations or the types of host stars. However, the challenge lies in the fact that the Einstein radius of a lensing object is typically much smaller than its angular height, necessitating decades of continuous observation of numerous stars using ground-based telescopes. Over the past decade, extensive research efforts have been directed toward the Galactic bulge, leading to the identification of over 100 exoplanet candidates, though only four have been confirmed through radial velocity measurements.\n\nMicrolensing events occur due to the gravitational lensing effect when a background source approaches a foreground lensing object. This proximity causes the light from the source to bend, resulting in an increase in brightness. If a planet is situated near the lensing object, it introduces additional variations in the light curve, which can be analyzed to infer the presence of the planet. The degree of this perturbation is highly dependent on the projected separation between the lensing object and the planet, making the analysis of the light curve crucial for detection. In conclusion, we offer recommendations for future projects aimed at enhancing the effectiveness of planetary microlensing follow-up observations. \n\n**Keywords:** Microlensing; Exoplanets; Brown Dwarfs; Follow-Up Observations.",
        "ori-fast-z-score": -0.7373087284671365,
        "water-fast-z-score": 5.615140365521,
        "rewrite-fast-z-score": 0.48666426339228763
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Graviton Propagator in a Covariant Massive Gravity Theory .\nAbstract:\nWe study the graviton propagator in covariant massive gravity theory with an arbitrary number of gravitons and show that it is given by the sum over all Feynman diagrams which are obtained by attaching one or more gravitons to each vertex of the tree-level graviton propagator. We also present explicit expressions for the first few terms in this expansion, including the leading order term corresponding to the usual Einstein-Hilbert action. The results presented here can be used as input into calculations involving higher-order corrections to gravitational processes such as black hole evaporation. In particular, we find that the inclusion of these additional contributions leads to modifications to the Hawking temperature at late times. \nI. INTRODUCTORY REMARkS\nThe purpose of this work is twofold. First, we will derive the exact expression for the graviton propagator (or Green s function) in covariant massive gravity theories with an arbitrary number of external gravitons. Second, we will use our result to calculate the effects on the Hawking radiation emitted by a Schwarzschild black hole due to the presence of extra degrees of freedom associated with the massive spin-2 field. Our analysis follows closely the approach developed in Ref.  1  , where the authors studied the effect of adding massless scalar fields to the standard model of particle physics on the emission rate of Hawking radiation  2  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Graviton Propagator in a Covariant Massive Gravity Theory . Abstract : We explore the graviton propagator in covariant massive gravity physics with an arbitrary number of gravitons and find that it is given by the sum over all Feynman diagrams which are derived by attaching one or more gravitons to each vertex of the tree - level graviton propagator .We additionally give explicit expressions for the first few terms in this expansion , notably the led order term corresponding to the usual Einstein - Hilbert action . The results presented here can be used as input into measurements involving higher - order corrections to gravitational processes such as black hole evaporation .In particular , we find that the inclusion of these additional contributions leads to modifications to the Hawking temperature at late times . I .INTRODUCTORY REMARkS The purpose of this study is twofold . First , we will generate the exact representation for the graviton propagator ( or Green s function ) in covariant massive gravity theories with an arbitrary number of external gravitons .Second , we will use our result to estimate the effects on the Hawking radiation emitted by a Schwarzschild black hole due to the presence of extra degrees of liberty involved with the huge spin - 2 field . Our estimate follows carefully the approach developed in Ref .1 , where the papers studied the impact of adding massless scalar fields to the standard theory of particle physics on the emission speed of Hawking radiation 2 .",
        "rewrite_text": "In this article, we investigate the graviton propagator within the framework of covariant massive gravity theories, accommodating an arbitrary number of gravitons. Our findings reveal that the graviton propagator can be expressed as a summation over all possible Feynman diagrams, which are constructed by attaching one or more gravitons to each vertex of the tree-level graviton propagator. We provide explicit formulations for the initial terms in this expansion, with particular emphasis on the leading order term that corresponds to the conventional Einstein-Hilbert action. The implications of our results are significant, as they can be utilized to enhance measurements related to higher-order corrections in gravitational phenomena, such as black hole evaporation. Notably, we discover that incorporating these additional contributions results in alterations to the Hawking temperature during the late stages of black hole evaporation.\n\nThe objectives of this research are twofold. Firstly, we aim to derive an exact representation of the graviton propagator (or Green's function) in covariant massive gravity theories, which can accommodate any number of external gravitons. Secondly, we intend to apply our findings to assess the impact of these extra degrees of freedom, associated with the massive spin-2 field, on the Hawking radiation emitted by a Schwarzschild black hole. Our approach closely follows the methodology outlined in previous studies, particularly those that examined the effects of introducing massless scalar fields into the standard particle physics framework on the rate of Hawking radiation emission. Through this work, we contribute to a deeper understanding of the interplay between graviton dynamics and black hole thermodynamics, paving the way for future investigations into the quantum aspects of gravity.",
        "ori-fast-z-score": 0.9901475429766744,
        "water-fast-z-score": 5.687367919007337,
        "rewrite-fast-z-score": 0.3746343246326776
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Littlewood-Richardson polynomials .\nAbstract:\nThe Littlewood-Richardson polynomials are the most important tool in representation theory, and have many applications to other fields as well.  They were introduced by Richard Stanley in 1973 (see also his book Enumerative Combinatorics). The original definition is complicated; here we give an equivalent one which makes them look more like ordinary symmetric functions. We then define the Schur polynomials using these polynomials instead of the usual monomial basis. Finally, we prove that this new definition agrees with the old one on the ring of symmetric functions. This article was written for readers who already know some basic facts about symmetric functions but want to learn how they can be used to study representations of groups. It assumes familiarity with group actions on vector spaces, characters of finite groups, and tensor products of vector spaces. For background information see Group (mathematics) or Representation Theory. In mathematics, the Littlewood–Richardson coefficients c(λ/µ) (also called Kostka numbers), named after John Littlewood and James Richardson, are integers associated to partitions λ and µ of n into at most m parts each such that:",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Littlewood - Richardson polynomials . Abstract : The Littlewood - Richardson polynomials are the most important tool in representation theory , and have many applications to other fields as also .They were introduced by Richard Stanley in 1973 ( saw also his book Enumerative Combinatorics ) . The original formulation is complicated ; here we give an equivalent one which makes them seem more like ordinary symmetric functions .We then define the Schur polynomials using these polynomials instead of the usual monomial basis . Finally , we prove that this new definition agrees with the new one on the ring of symmetric functions .This page was written for readers who actually know some fundamental details about symmetric functions but want to see how they can be used to study representations of groups . It requires familiarity with group actions on matrix sets , characters of finite groups , and tensor products of vector spaces .For background data see Group ( mathematics ) or Representation Theory . In mathematics , the Littlewood – Richardson polynomial c ( λ / µ ) ( also known Kostka numbers ) , named after John Littlewood and James Richardson , are integers associated to partitions λ and µ of n into at most m components each such that :",
        "rewrite_text": "Title: Littlewood-Richardson Polynomials\n\nAbstract: The Littlewood-Richardson polynomials serve as a fundamental instrument in the field of representation theory, with extensive applications across various disciplines. Introduced by Richard Stanley in 1973, as detailed in his work \"Enumerative Combinatorics,\" these polynomials initially presented a complex formulation. In this article, we offer a more accessible equivalent formulation that aligns them more closely with conventional symmetric functions. We proceed to redefine Schur polynomials utilizing these Littlewood-Richardson polynomials, departing from the traditional monomial basis. Our analysis culminates in a proof demonstrating that this new definition is consistent with the established framework within the ring of symmetric functions.\n\nThis article is tailored for readers who possess a foundational understanding of symmetric functions and are interested in exploring their application in the study of group representations. A familiarity with concepts such as group actions on matrix sets, characters of finite groups, and tensor products of vector spaces is essential for comprehending the material presented. For additional context, readers may refer to topics related to Group Theory and Representation Theory.\n\nThe Littlewood-Richardson polynomial, denoted as c(λ/µ), also known as Kostka numbers, are integers linked to partitions λ and µ of n, each comprising at most m components. This connection underscores the significance of these polynomials in combinatorial representation theory, revealing their role in counting and constructing representations of groups. Through this exploration, we aim to illuminate the interplay between symmetric functions and representation theory, providing insights that may inspire further research and applications in these interconnected areas of mathematics.",
        "ori-fast-z-score": -0.3216337604513384,
        "water-fast-z-score": 3.624412178045377,
        "rewrite-fast-z-score": -1.1338934190276817
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Integrable systems and complex geometry .\nAbstract:\nThe aim of this talk is to present some recent results on the relation between integrability in quantum field theory, statistical mechanics or mathematical physics and the existence of special geometric structures (complex algebraic curves) which are associated with these models. \nWe will discuss how such geometries can be used for solving exactly certain physical problems by using techniques coming from algebraic geometry like Riemann surfaces, theta functions etc.. We will also explain why it seems that many interesting integrable models have an underlying structure of a Riemann surface. Finally we will give examples where this connection has been made explicit. The talk will include several new results obtained recently by the author together with his collaborators. This work was partially supported by the DFG under grant SFB/TR9. Integrable systems play an important role in various branches of mathematics as well as theoretical physics. In particular they appear naturally when studying solvable lattice models in statistical mechanics or quantum field theories. It turns out that there exists a deep relationship between integrability and the presence of special geometric structures called algebraic curves. These curves provide powerful tools for solving exactly certain physical questions via methods from algebraic geometry like Riemann Surfaces, Theta Functions,...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Integrable systems and complex geometry . Abstract : The goal of this talk is to provide some latest findings on the relation between integrability in quantum field theory , statistical mechanics or computational physics and the existence of unique geometric objects ( complex algebraic shapes ) which are identified with these models .We will explore how such geometries can be used for solving exactly certain physical problems by using techniques come from algebraic topology like Riemann surfaces , theta functions etc . . We will also explain why it appears that several interesting integrable models have an underlying composition of a Riemann surface . Finally we will provide examples where this link has been made explicit .The talk will include several current data acquired previously by the writer together with his collaborators . This project was partially backed by the DFG under grant SFB / TR9 .Integrable systems play an important role in different areas of math as well as conceptual science . In particular they appear naturally when examining solvable lattice models in statistical mechanics or quantum field theories .It turns out that there exists a profound connection between integrability and the presence of unique geometric objects called algebraic curves . These curves provide potent tools for solving exactly certain physical problems via tools from algebraic topology like Riemann Surfaces , Theta Functions , . . .",
        "rewrite_text": "**Title:** Integrable Systems and Complex Geometry\n\n**Abstract:** This presentation aims to discuss recent discoveries regarding the interplay between integrability in quantum field theory, statistical mechanics, and computational physics, and the existence of distinctive geometric entities, specifically complex algebraic shapes, associated with these models. We will investigate how these geometric structures can facilitate the exact resolution of specific physical challenges through methodologies derived from algebraic topology, such as Riemann surfaces and theta functions. Additionally, we will elucidate the observation that numerous intriguing integrable models exhibit an inherent composition of Riemann surfaces. To illustrate this connection, we will present explicit examples where the relationship between integrability and complex geometry has been clearly established. The discussion will incorporate a range of current findings obtained by the author and collaborators, supported in part by the DFG under grant SFB/TR9. Integrable systems are pivotal in various mathematical domains and theoretical sciences, particularly emerging naturally in the study of solvable lattice models within statistical mechanics and quantum field theories. Notably, a significant link has been identified between integrability and the existence of unique geometric constructs known as algebraic curves. These curves serve as powerful instruments for achieving exact solutions to certain physical problems, leveraging techniques from algebraic topology, including Riemann surfaces and theta functions. This talk will provide insights into these connections, highlighting their implications for both theoretical research and practical applications in physics.",
        "ori-fast-z-score": -0.7364596943186588,
        "water-fast-z-score": 6.141879930089016,
        "rewrite-fast-z-score": -0.09090909090909091
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The host galaxy/AGN connection. Brightness profiles of early-type galaxies hosting Seyfert nuclei .\nAbstract:\nWe present the results of an analysis of HST images for a sample of 12 nearby (z<0.1) brightest cluster galaxies with nuclear activity classified as Seyferts in the Palomar spectroscopic survey, and compare them to those obtained for a control sample of 12 inactive galaxies selected by matching their morphological type and luminosity distribution. We find that the AGNs are preferentially located at large galactocentric distances (Rg>10 kpc), where they appear to be associated with extended emission-line regions. The surface brightness profiles show that these objects have flatter cores than normal elliptical galaxies, but steeper outer envelopes. This is consistent with previous studies which found that the central light concentration decreases systematically towards higher levels of nuclear activity.  These findings suggest that the presence of an active nucleus may affect the formation and/or evolution of the stellar population in its immediate vicinity. In particular, we propose that the flattening of the core could be due to the removal or redistribution of stars caused by tidal interactions between the AGN and surrounding gas clouds.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The host universe / AGN link . Brightness characteristics of early - class nuclei containing Seyfert nuclei .Abstract : We present the results of an assessment of HST photographs for a sample of 12 nearby ( z < 0 . 1 ) brightest cluster clusters with nuclear activity categorized as Seyferts in the Palomar spectroscopic study , and compare them to those acquired for a control sample of 12 inactive clusters designated by fitting their morphological class and luminosity distribution . We see that the AGNs are preferentially positioned at large galactocentric speeds ( Rg > 10 kpc ) , where they appear to be identified with long emission - line regions .The exterior brightness characteristics show that these objects have flatter cores than usual elliptical galaxies , but steeper exterior envelopes . This is consistent with previous research which showed that the main light concentration drops systematically towards higher levels of nuclear activity .These studies propose that the presence of an active core could affect the formation and / or evolution of the stellar community in its immediate vicinity . In particular , we propose that the flattening of the core could be due to the removal or redistribution of stars due by tidal interactions between the AGN and surrounding gas clouds .",
        "rewrite_text": "Title: The Host Universe / AGN Connection: Brightness Characteristics of Early-Class Nuclei with Seyfert Activity\n\nAbstract: In this study, we analyze Hubble Space Telescope (HST) images of a sample comprising 12 nearby brightest cluster galaxies (BCGs) with Seyfert nuclei, identified through the Palomar spectroscopic survey, and compare these findings to a control group of 12 inactive clusters matched by morphological classification and luminosity distribution. Our investigation reveals that active galactic nuclei (AGNs) are predominantly located at significant galactocentric distances (Rg > 10 kpc), where they are associated with extensive emission-line regions. The brightness profiles of these AGNs indicate that their central cores are flatter than those typically observed in standard elliptical galaxies, while their outer envelopes exhibit a steeper gradient. This observation aligns with previous studies suggesting that the concentration of light diminishes systematically with increasing levels of nuclear activity. Our findings imply that the presence of an active nucleus may influence the formation and evolution of the surrounding stellar population. Specifically, we hypothesize that the observed flattening of the core may result from the removal or redistribution of stars due to tidal interactions between the AGN and adjacent gas clouds. This research contributes to the understanding of the interplay between AGNs and their host galaxies, highlighting the potential impact of nuclear activity on the structural properties of galactic nuclei.",
        "ori-fast-z-score": -0.7107423155935334,
        "water-fast-z-score": 6.327848502189878,
        "rewrite-fast-z-score": -0.41256849850351734
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A deeper search for the progenitor of the Type Ic Supernova 2002ap .\nAbstract:\nWe present new optical and near-infrared photometry obtained with the Hubble Space Telescope (HST) in order to study the late-time evolution of the supernova remnant N132D, which is associated with the type Ic supernova SN2002ap. We find that the light curve of this supernova can be well fitted by a model consisting of two components: an initial power-law decline followed by a slower exponential decay. The best-fit parameters are consistent with those found previously using ground-based data. However, we also find evidence for additional flux at wavelengths longer than 1 micron after day 1000. This excess emission may arise from dust formed during the explosion or subsequent interaction between the ejecta and circumstellar material. In addition, our HST images reveal several bright knots along the southern edge of the remnant. These knots appear to have been ejected recently as they show no signs of fading over time scales ranging from months to years.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A deeper investigation for the progenitor of the Type Ic Supernova 2002ap . Abstract : We use new optical and far - infrared photometry obtained with the Hubble Space Telescope ( HST ) in order to study the late - time progression of the supernova remnant N132D , which is associated with the class Ic supernova SN2002ap .We see that the light curve of this supernova can be well fitted by a description consisting of two parts : an initial power - law decrease followed by a slower exponential decay . The best - fitting values are compatible with those identified previously using ground - based data .However , we also find proof for additional flux at wavelengths greater than 1 micron after day 1000 . This excess emission may arise from dust formed during the explosion or later interaction between the ejecta and circumstellar material .In addition , our HST pictures indicate many bright knots along the southern periphery of the remnant . These threads seem to have been expelled previously as they show no evidence of fading over time ranges ranging from months to decades .",
        "rewrite_text": "Title: A Comprehensive Study of the Progenitor of Type Ic Supernova 2002ap\n\nAbstract: In this study, we present new optical and far-infrared photometric observations acquired with the Hubble Space Telescope (HST) to investigate the late-time evolution of the supernova remnant N132D, which is linked to the Type Ic supernova SN2002ap. Our analysis reveals that the light curve of SN2002ap can be accurately modeled by a two-part function: an initial phase characterized by a power-law decline, followed by a more gradual exponential decay. The parameters derived from our fitting align closely with those obtained from previous ground-based observations. Notably, we observe an excess of flux at wavelengths exceeding 1 micron after day 1000, which suggests the presence of additional emission. This excess could potentially be attributed to dust formation resulting from the supernova explosion or to interactions between the ejected material and the surrounding circumstellar environment. Furthermore, our HST imaging reveals several bright knots located along the southern edge of the remnant. These features appear to have been ejected earlier in the supernova's evolution, as they exhibit no signs of fading over time spans that range from months to decades. This finding provides valuable insights into the dynamics of the supernova remnant and the processes occurring in its aftermath, contributing to our understanding of the progenitor system of SN2002ap and the mechanisms driving Type Ic supernovae.",
        "ori-fast-z-score": 0.6469966392206304,
        "water-fast-z-score": 6.822422923379533,
        "rewrite-fast-z-score": 0.6
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Radion Phenomenology in Realistic Warped Space Models .\nAbstract:\nWe study the phenomenological consequences of radion stabilization by introducing an explicit mass term for the radion into realistic warped space models with bulk gauge fields and fermions. We find that, depending on the model parameters, there are two possible scenarios which can be realized at low energies: (i) The radion is heavy enough to decouple from all Standard Model particles. (ii) The radion couples directly to Standard Model particles but its effects are suppressed due to mixing between the radion and the Higgs boson. In both cases we show how the radion affects precision electroweak observables such as S, T , W , Y . Finally, we discuss the possibility of detecting the radion through direct production at future colliders. Introduction: One of the most interesting features of Randall-Sundrum type I  1  or II  2  models is their ability to generate hierarchies among fundamental scales without fine-tuning  3  . However, these models suffer from the presence of a scalar field called the radion whose vacuum expectation value determines the size of extra dimensions  4  .\nIn this work we consider the case where the radion acquires a large mass so it does not affect low-energy physics  5  . On the other hand, if the radion remains light then it may have observable effects  6  -  8  . For example, one could imagine that the radion mixes strongly with the Higgs boson leading to deviations from standard predictions  9  . Alternatively, the radion might couple directly to Standard Model particles  10  . In either scenario, the radion would contribute to precision electroweak measurements  11  -  13  . This motivates us to investigate whether the radion has any effect on precision electroweak data within realistic warped space models  14  -  16  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Radion Phenomenology in Realistic Warped Space Models . Abstract : We research the phenomenological consequences of radion stabilization by using an explicit mass term for the radion into simulated warped space models with bulk gauge fields and fermions .We see that , depending on the model variables , there are two possible possibilities which can be realized at low energies : ( i ) The radion is strong enough to decouple from all Standard Model particles . ( ii ) The radion couples directly to Standard Model particles but its properties are suppressed due to mixing between the radion and the Higgs boson .In both cases we explain how the radion affects precision electroweak observables such as S , T , W , Y . Finally , we discuss the prospect of detecting the radion through direct production at potential colliders .Introduction : One of the most important features of Randall - Sundrum type I 1 or II 2 models is their potential to produce hierarchies among essential scales without fine - tuned 3 . However , these models suffer from the presence of a scalar field called the radion whose vacuum expectation strength determines the size of extra dimensions 4 .In this research we imagine the case where the radion acquires a large mass so it does not alter small - energy physics 5 . On the other hand , if the radion stayed light then it could have observable effects 6 - 8 .For instance , one might think that the radion mixing strongly with the Higgs boson leading to deviations from standard predictions 9 . Alternatively , the radion might couple directly to Standard Model particles 10 .In either situation , the radion might contribute to accurate electroweak measurements 11 - 13 . This motivates us to examine whether the radion has any impacts on precision electroweak data within real warped space models 14 - 16 .",
        "rewrite_text": "**Title:** Radion Phenomenology in Realistic Warped Space Models\n\n**Abstract:** In this study, we investigate the phenomenological implications of radion stabilization by incorporating an explicit mass term for the radion within simulated warped space models that include bulk gauge fields and fermions. Our analysis reveals two distinct scenarios that can manifest at low energy scales, contingent upon the model parameters: (i) the radion may possess sufficient strength to decouple entirely from all Standard Model particles, or (ii) it may couple directly to Standard Model particles, albeit with its interactions suppressed due to mixing with the Higgs boson. In both scenarios, we elucidate the impact of the radion on precision electroweak observables, specifically the parameters S, T, W, and Y. Furthermore, we explore the potential for detecting the radion through direct production at future collider experiments.\n\nThe Randall-Sundrum models, both type I and II, are notable for their ability to generate hierarchies among fundamental scales without requiring fine-tuning. However, these models are complicated by the presence of a scalar field known as the radion, whose vacuum expectation value dictates the scale of extra dimensions. In our research, we consider the scenario where the radion attains a significant mass, thereby minimizing its influence on low-energy physics. Conversely, if the radion remains light, it could lead to observable effects, such as significant mixing with the Higgs boson, which would result in deviations from standard model predictions. Alternatively, the radion could couple directly to Standard Model particles, potentially affecting precision electroweak measurements. This motivates our investigation into the radion's effects on electroweak data within the framework of realistic warped space models, aiming to assess its role and implications in high-energy physics.",
        "ori-fast-z-score": -0.8542421961772492,
        "water-fast-z-score": 5.979695373240744,
        "rewrite-fast-z-score": 0.5477225575051661
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Properties of Zero-Lag Long-Range Synchronization via Dynamical Relaying .\nAbstract:\nWe study the zero-lag synchronization between two identical chaotic systems with different time scales by using dynamical relaying method, which is based on introducing an intermediate system to transmit information between them. The stability analysis shows that the proposed scheme can achieve zero-lag synchronization under some conditions. Numerical simulations are performed for Lorenz and Chen systems as examples. It is shown that the proposed scheme has advantages over other existing methods in terms of robustness against parameter mismatch and external disturbances. \nI. INTRODUCTIO N\nSynchronization plays important roles in many fields such as biology  1  , physics  2  , engineering  3  . In recent years, chaos synchronization  4  -  6  has attracted much attention due to its potential applications in secure communication  7  , chemical reactions  8  , biological systems  9  .\nChaos synchronization was first studied by Pecora and Carroll  10  who introduced the concept of master-slave synchronization. Since then, various schemes have been developed  11  -  13  . Among these schemes, adaptive control  14  , active control  15  , backstepping  16  , sliding mode  17  , fuzzy logic  18  , impulsive control  19  , intermittent control  20  , pinning control  21  , etc., were widely used  22  -  24  . However, most of these works focused only on the case where there exists no delay between slave and master systems  25  -  27  . Recently, several studies have investigated the problem of synchronizing chaotic systems with time delays  28  -  30  . For example, Wu et al.  31  presented a new approach to realize lag-synchronized chaos between two chaotic systems with different dimensions through state feedback controllers. Liu et al.  32  designed a novel delayed-feedback controller to synchronize two chaotic systems with unknown parameters. Wang et al.  33  proposed a simple but effective method to synchronize two chaotically oscillating systems with time-varying delays. Although these results provide useful insights into the design of synchronized chaotic systems with time-delays, they cannot be applied directly to solve practical problems because it may take too",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Properties of Zero - Lag Long - Range Synchronization via Dynamical Relaying . Abstract : We research the zero - lag synchronization between two unrelated turbulent systems with varying time ranges by using dynamical relaying model , which is based on introducing an intermediate system to transmit data between them .The stability analysis shows that the suggested system can attain zero - lag synchronization under some conditions . Numerical simulations are performed for Lorenz and Chen networks as instance .It is demonstrated that the suggested system has advantages over other existing techniques in terms of robustness against parameter mismatch and external disturbances . I . INTRODUCTIO N Synchronization plays important roles in different fields such as biology 1 , chemistry 2 , engineering 3 .In recent years , chaos synchronization 4 - 6 has drew much attention due to its potential applications in safe communication 7 , chemical processes 8 , biological systems 9 . Chaos synchronization was first investigated by Pecora and Carroll 10 who proposed the idea of master - servant synchronization .Since then , various strategies have been created 11 - 13 . Among these schemes , adaptive control 14 , active control 15 , backstepping 16 , sliding mode 17 , fuzzy logic 18 , impulsive control 19 , continuous control 20 , pinning rule 21 , etc . , were commonly used 22 - 24 .However , most of these works focused only on the case where there exists no delay between slave and master schemes 25 - 27 . Recently , various studies have analyzed the issue of synchronizing chaotic networks with time errors 28 - 30 .For instance , Wu et al . 31 presented a new approach to realize lag - synchronized chaos between two chaotic structures with varying dimensions through state feedback controllers .Liu et al . 32 designed a new delayed - feedback controller to synchronize two chaotic structures with unknown parameters .Wang et al . 33 developed a simple but effective theory to synchronize two chaotically oscillating systems with time - differing delays .Although these results provide useful insights into the model of synchronized turbulent systems with time - errors , they cannot be applied directly to solve practical questions because it could took too",
        "rewrite_text": "**Title:** Properties of Zero-Lag Long-Range Synchronization via Dynamical Relaying\n\n**Abstract:** This study investigates the phenomenon of zero-lag synchronization between two independent turbulent systems characterized by varying time scales, utilizing a dynamical relaying model. This model introduces an intermediary system that facilitates data transmission between the two systems, thereby enabling synchronization. Through stability analysis, we demonstrate that the proposed system can achieve zero-lag synchronization under specific conditions. To validate our theoretical findings, we conduct numerical simulations using the Lorenz and Chen chaotic networks as case studies. The results indicate that our approach offers significant advantages over existing synchronization techniques, particularly in terms of robustness against parameter mismatches and external disturbances. \n\nSynchronization is a critical process observed across various disciplines, including biology, chemistry, and engineering. In recent years, chaos synchronization has garnered considerable attention due to its potential applications in secure communication, chemical processes, and biological systems. The foundational work by Pecora and Carroll introduced the concept of master-slave synchronization, which has since inspired a multitude of strategies aimed at achieving synchronization in chaotic systems. Commonly employed methods include adaptive control, active control, backstepping, sliding mode control, fuzzy logic, impulsive control, continuous control, and pinning rules. However, much of the existing literature has primarily focused on scenarios devoid of delays between the master and slave systems. \n\nRecent research has begun to explore the synchronization of chaotic networks in the presence of time delays. Notable contributions include Wu et al.'s approach to achieving lag-synchronized chaos through state feedback controllers, Liu et al.'s delayed-feedback controller for synchronizing chaotic systems with unknown parameters, and Wang et al.'s theory for synchronizing systems with differing time delays. While these studies provide valuable insights into the synchronization of turbulent systems with time discrepancies, they often fall short in addressing practical applications due to the complexities involved. Our research aims to bridge this gap by presenting a robust framework for zero-lag synchronization that can be effectively applied in real-world scenarios.",
        "ori-fast-z-score": -0.47733437050543803,
        "water-fast-z-score": 8.009821015753255,
        "rewrite-fast-z-score": 2.2613350843332274
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Shear-rate dependent transport coefficients for inelastic Maxwell models .\nAbstract:\nWe study the shear viscosity and thermal conductivity of an inelastic Maxwell gas with variable mass ratio, by using kinetic theory methods. We show that both transport coefficients are proportional to the square root of the mean free path between collisions. The dependence on the mass ratio is found to be weak when the masses differ significantly but strong if they are similar. In particular we find that the shear viscosity decreases as the mass ratio increases while the opposite happens for the thermal conductivity. \n \n Introduction \n \n Transport properties such as the shear viscosity and thermal conduction play important roles in many physical phenomena ranging from astrophysics  1  , geophysical flows  2  or plasma physics  3  . These quantities depend strongly on the microscopic dynamics of the system under consideration  4  . For example, it has been shown recently  5  that the shear viscosity of granular gases depends crucially on whether particles can bounce off each other after colliding elastically  6  .\n \nIn this work we consider a model consisting of two species of particles which interact via binary elastic collisions  7, 8  . Each particle belongs either to one of these species (A) or to another species (B). Particles belonging to different species do not interact directly; however their motion is coupled indirectly through the presence of a background fluid. This situation arises naturally in mixtures where there exists a large difference in size and/or mass between the components  9  . It also occurs in systems composed of heavy ions immersed in a lighter neutralizing electron gas  10  . \n \n A simple way to describe the interaction between particles of type A and B is given by the so-called inelastic Maxwell model  11  . Here particles of type A have constant mass m0 and those of type B change continuously during time evolution according to some prescribed rule  12  . As a result, the total number of particles N = NA + NB fluctuates around its average value <N> = n0V, where V denotes the volume occupied by the mixture. If the fluctuations are small compared to the average density then the distribution function f(r,v;t) describing the state of the system at position r, velocity v and time t satisfies the following Boltz",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Shear - rate dependent transport functions for inelastic Maxwell systems . Abstract : We explore the shear viscosity and thermal conductivity of an inelastic Maxwell gas with variable mass ratio , by using kinetic theory techniques .We see that both travel coefficients are proportional to the square root of the mean free path between collisions . The dependence on the mass ratio is found to be weak when the masses vary significantly but weak if they are comparable .In particular we find that the shear viscosity falls as the mass ratio increases while the opposite happens for the thermal conductivity . Introduction Transport phenomena such as the shear viscosity and thermal conduction drive important roles in different biological phenomena ranging from astrophysics 1 , geophysical flows 2 or plasma physics 3 .These quantities rely highly on the microscopic behavior of the system under consideration 4 . For instance , it has been shown recently 5 that the shear viscosity of granular fluids depends crucially on whether particles can jump off each other after colliding elastically 6 .In this study we imagine a theory consisting of two species of particles which interact via binary elastic collisions 7 , 8 . Each particle belongs either to one of these species ( A ) or to another species ( B ) .Particles belonging to different species do not interact directly ; however their motion is linked indirectly through the presence of a background fluid . This condition occurs commonly in mixtures where there exists a large change in length and / or mass between the parts 9 .It also occurs in environments composed of large ions immersed in a lighter neutralizing electron gas 10 . A straightforward way to explain the interaction between particles of type A and B is given by the so - called inelastic Maxwell model 11 .Here ions of type A have constant mass m0 and those of type B shift continuously during time progression according to some prescribed rule 12 . As a result , the total quantity of grains N = NA + NB fluctuates around its average value < N > = n0V , where V denotes the volume occupied by the mixture .If the fluctuations are small compared to the average density then the distribution function f ( r , v ; t ) describing the state of the system at position r , speed v and period t satisfies the following Boltz",
        "rewrite_text": "**Title:** Shear-Rate Dependent Transport Functions for Inelastic Maxwell Systems\n\n**Abstract:** In this study, we investigate the shear viscosity and thermal conductivity of an inelastic Maxwell gas characterized by a variable mass ratio, employing kinetic theory methodologies. Our findings reveal that both transport coefficients are directly proportional to the square root of the mean free path between collisions. Notably, the influence of the mass ratio on these coefficients is observed to be minimal when the masses differ significantly; however, the dependence becomes more pronounced when the masses are comparable. Specifically, we demonstrate that as the mass ratio increases, the shear viscosity decreases, while the thermal conductivity exhibits the opposite trend. \n\nTransport phenomena, including shear viscosity and thermal conduction, play crucial roles in various biological and physical processes, spanning fields such as astrophysics, geophysical flows, and plasma physics. These transport properties are intricately linked to the microscopic dynamics of the system under consideration. Recent studies have highlighted the significant impact of particle interactions on shear viscosity, particularly in granular fluids where the ability of particles to rebound after elastic collisions is a determining factor. \n\nIn our theoretical framework, we consider a system comprising two distinct species of particles that interact through binary elastic collisions. Particles are categorized into species A and B, with no direct interactions between different species; their motion is instead influenced by a common background fluid. This scenario is frequently encountered in mixtures with substantial variations in length or mass, as well as in systems with large ions immersed in a lighter electron gas. \n\nTo model the interactions between particles of species A and B, we employ the inelastic Maxwell model, where particles of type A maintain a constant mass, while those of type B experience continuous mass variations over time according to a defined rule. Consequently, the total number of particles fluctuates around an average value, and if these fluctuations remain small relative to the average density, the distribution function describing the system's state can be expressed through the Boltzmann equation. This study provides insights into the transport properties of inelastic Maxwell systems, contributing to a deeper understanding of their behavior under varying conditions.",
        "ori-fast-z-score": 0.22941573387056177,
        "water-fast-z-score": 7.3747734363161435,
        "rewrite-fast-z-score": 0.909717652294684
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Suzaku Observations of Active Galactic Nuclei Detected in the Swift/BAT Survey: Discovery of  New Type  of Buried Supermassive Black Holes .\nAbstract:\nWe report on Suzaku observations for four active galactic nuclei (AGNs) detected by Swift/BAT survey, which are classified as  obscured AGNs  with column density larger than 10 24 cm-2 . We found that all these sources show strong Fe K emission lines and their line widths are broader than those expected from thermal broadening at kT = 100 keV. The observed line profiles can be reproduced well by relativistic disk reflection models including Compton scattering effects. These results suggest that there is an additional component to the X-ray continuum other than the standard thin accretion disks around supermassive black holes. In addition, we find that the iron abundance relative to solar value is higher than 1.5 times in three out of four objects. This suggests that the central engines of these obscured AGNs may have been buried under heavy dusty torii. Finally, we discuss possible origins of this new type of buried supermassive black holes based on our observational results.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Suzaku Observations of Active Galactic Nuclei Detected in the Swift / BAT Survey : Discovery of New Type of Buried Supermassive Black Holes . Abstract : We report on Suzaku measurements for four active galactic nuclei ( AGNs ) detected by Swift / BAT search , which are classified as obscured AGNs with column velocity larger than 10 24 mm - 2 .We showed that all these sources show intense Fe K emission lines and their line widths are larger than those expected from thermal broadening at kT = 100 keV . The observed line profiles can be reproduced well by relativistic disk absorption theories including Compton absorption effects .These data suggest that there is an additional element to the X - ray continuum other than the standard narrow accretion disks around supermassive black holes . In addition , we find that the metal density relative to solar value is higher than 1 . 5 times in three out of four bodies .This implies that the main engines of these obscured AGNs might have been trapped under heavy dusty torii . Finally , we investigate possible origins of this new kind of hidden supermassive black holes using on our observational results .",
        "rewrite_text": "We present findings from Suzaku observations of four active galactic nuclei (AGNs) identified in the Swift/BAT survey, which are categorized as obscured AGNs with column densities exceeding 10^24 cm^-2. Our analysis reveals that all four sources exhibit prominent Fe K emission lines, with line widths that surpass those anticipated from thermal broadening at a temperature of kT = 100 keV. The observed line profiles align closely with predictions from relativistic disk absorption models, which take into account Compton absorption effects. These results indicate the presence of additional components in the X-ray continuum beyond the conventional narrow accretion disks typically associated with supermassive black holes. Furthermore, we observe that the metallicity in three of the four AGNs is more than 1.5 times the solar value, suggesting that these obscured AGNs are likely powered by supermassive black holes concealed beneath substantial dusty tori. This study not only enhances our understanding of the characteristics of these hidden supermassive black holes but also prompts us to explore their potential origins based on our observational data. The implications of these findings could significantly impact our comprehension of AGN evolution and the role of environmental factors in shaping the properties of supermassive black holes.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 6.111111111111111,
        "rewrite-fast-z-score": 1.8225913092242512
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Line and continuum variability of two intermediate-redshift, high-luminosity quasars .\nAbstract:\nWe present the results of an optical monitoring campaign on two luminous quasars at redshifts z = 1.7 and 2.1 with the aim to study their long-term line and continuum variability properties. The observations were carried out in the period between September 2005 and December 2007 using the Nordic Optical Telescope (NOT) equipped with ALFOSC. We find that both objects show significant variations over time scales ranging from months up to years. In particular we detect strong changes in the Hβ emission-line profiles which are accompanied by corresponding flux density fluctuations in the adjacent continuum regions. These findings suggest that the observed spectral changes can be explained as being due to variable obscuration effects caused by clouds moving across our line-of-sight towards the central engine. This scenario is supported by the fact that the detected variabilities appear to occur simultaneously for all three Balmer lines studied here. Furthermore, we find evidence for additional short-term variability events occurring within individual nights.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Line and continuum variability of two intermediate - redshift , large - luminosity quasars . Abstract : We report the results of an optical monitoring effort on two luminous quasars at redshifts z = 1 . 7 and 2 . 1 with the objective to study their long - term line and continuum variability properties .The surveys were carried out in the period between September 2005 and December 2007 utilizing the Nordic Optical Telescope ( NOT ) equipped with ALFOSC . We see that both images exhibit substantial variations over time ranges varied from months up to decades .In particular we perceive strong changes in the Hβ emission - line profiles which are marked by resulting flux concentration fluctuations in the adjacent continuum regions . These studies propose that the seen spectral changes can be understood as being owing to variable obscuration effects caused by clouds moved across our line - of - view towards the main engine .This scenario is backed by the fact that the reported variabilities appear to come concurrently for all three Balmer patterns examined here . Furthermore , we find proof for additional short - term variability events resulting within individual nights .",
        "rewrite_text": "We present the findings from an extensive optical monitoring campaign focused on two luminous quasars, situated at redshifts of z = 1.7 and z = 2.1, aimed at investigating their long-term variability in both line and continuum emissions. Conducted between September 2005 and December 2007 using the Nordic Optical Telescope (NOT) equipped with the ALFOSC instrument, our observations reveal significant temporal variations in the quasars' emissions, spanning time scales from months to decades. Notably, we observe pronounced fluctuations in the Hβ emission-line profiles, which correlate with variations in the surrounding continuum flux. \n\nOur analysis suggests that these spectral changes may be attributed to variable obscuration effects, likely caused by clouds moving across our line of sight towards the quasars' central engines. This hypothesis is supported by the simultaneous nature of the observed variabilities across all three Balmer lines examined in this study. Additionally, we provide evidence for short-term variability events occurring within individual nights, further emphasizing the dynamic nature of these quasars. \n\nThe results of this monitoring effort contribute to our understanding of the mechanisms driving variability in high-luminosity quasars and highlight the importance of continuous observational campaigns in uncovering the complexities of quasar behavior over varying time scales. Our findings not only enhance the existing knowledge of quasar variability but also open avenues for future research into the physical processes influencing these distant astronomical objects.",
        "ori-fast-z-score": -0.5488212999484517,
        "water-fast-z-score": 6.47609133939173,
        "rewrite-fast-z-score": -0.09950371902099892
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Measurement of the Aerosol Phase Function at the Pierre Auger Observatory .\nAbstract:\nThe measurement of the aerosol phase function is important for understanding cosmic ray showers and their detection by ground-based experiments, such as those performed with the Pierre Auger Observatory (PAO). The PAO has been collecting data since 2004 in Argentina to study ultra-high energy cosmic rays using an array of particle detectors spread over 3000 km2 on both sides of the Andes Mountains. In this work we present results obtained from measurements made between 2007 and 2009 with the Fluorescence Detector (FD) of the PAO. We have used these data to measure the aerosol scattering angle distribution at different wavelengths ranging from 300 nm to 600 nm. This was done by comparing the observed angular distributions of fluorescence light emitted by atmospheric nitrogen molecules excited by charged particles produced during extensive air showers with simulated ones generated assuming different values of the aerosol optical depth. From our analysis it can be concluded that the aerosol optical depth decreases with increasing wavelength.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Measurement of the Aerosol Phase Function at the Pierre Auger Observatory . Abstract : The measurement of the aerosol phase function is important for studying cosmic ray showers and their observation by land - based experiments , such as those conducted with the Pierre Auger Observatory ( PAO ) .The PAO has been collecting data since 2004 in Argentina to study ultra - large energy cosmic rays using an array of particle detectors distributed over 3000 km2 on both sides of the Andes Mountains . In this project we present results acquired from measurements made between 2007 and 2009 with the Fluorescence Detector ( FD ) of the PAO .We have utilized these information to measure the aerosol scattering angle distribution at different wavelengths ranging from 300 nm to 600 nm . This was done by comparing the observed angular distributions of fluorescence light emitted by atmospheric nitrogen molecules excited by charged particles generated during widespread air showers with simulated ones produced taking various values of the aerosol laser thickness .From our analysis it can be determined that the aerosol imaging depth varies with varying wavelength .",
        "rewrite_text": "**Title:** Measurement of the Aerosol Phase Function at the Pierre Auger Observatory\n\n**Abstract:** Understanding the aerosol phase function is crucial for the analysis of cosmic ray showers and their detection through ground-based experiments, such as those carried out at the Pierre Auger Observatory (PAO). Since its inception in 2004, the PAO has been actively gathering data in Argentina to investigate ultra-high-energy cosmic rays, utilizing a network of particle detectors spread across an expansive area of 3000 km², which spans both sides of the Andes Mountains. This study presents findings from measurements conducted between 2007 and 2009 using the Fluorescence Detector (FD) at the PAO. Our approach involved assessing the aerosol scattering angle distribution across a spectrum of wavelengths, specifically from 300 nm to 600 nm. This was achieved by comparing the angular distributions of fluorescence light emitted by atmospheric nitrogen molecules, which are excited by charged particles produced during extensive air showers, with simulated distributions generated under varying aerosol laser thickness conditions. The results of our analysis indicate that the aerosol imaging depth is dependent on the wavelength, revealing significant variations that can impact the interpretation of cosmic ray data. This research not only enhances our understanding of aerosol behavior in the atmosphere but also contributes valuable insights for improving the accuracy of cosmic ray observations at the PAO. By elucidating the relationship between aerosol properties and fluorescence light scattering, we aim to refine the methodologies employed in cosmic ray studies and facilitate more precise measurements in future observational campaigns.",
        "ori-fast-z-score": -0.1125087900926024,
        "water-fast-z-score": 4.919349550499537,
        "rewrite-fast-z-score": 0.8542421961772492
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Are constant loop widths an artifact of the background and the spatial resolution? .\nAbstract:\nWe present new results on the distribution of loop widths in active regions observed by TRACE at 171 Å, using data obtained during solar maximum (May-June 2001). We find that loops with different temperatures have similar distributions of widths, which are well fitted by log-normal functions. The mean values of these distributions increase with temperature as expected for pressure equilibrium between plasma confined within magnetic structures and their surroundings. However, we also find that there is no significant difference between the widths measured along individual loops and those determined from averaged profiles over entire active regions. This suggests that the apparent constancy of loop widths may be due to averaging effects rather than being intrinsic properties of coronal structures. In addition, we show that the widths derived from observations made under different viewing angles do not depend significantly on the position angle of the line-of-sight relative to the direction perpendicular to the local magnetic field vector.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Are constant loop widths an artifact of the background and the spatial resolution ? .Abstract : We report new data on the distribution of loop widths in active regions observed by TRACE at 171 Å , using data acquired during solar maximum ( May - June 2001 ) . We see that loops with varying temperatures have similar distributions of widths , which are better fitted by log - normal functions .The mean estimates of these distributions increase with temperature as anticipated for pressure equilibrium between plasma confined within magnetic structures and their environment . However , we also find that there is no considerable difference between the widths calculated along individual loops and those estimated from averaged profiles over whole active regions .This implies that the apparent constancy of loop widths might be due to averaging influences rather than being intrinsic characteristics of coronal structures . In addition , we prove that the widths generated from measurements made under distinct observation angles do not depend greatly on the orientation angle of the line - of - view relative to the direction perpendicular to the local magnetic force vector .",
        "rewrite_text": "Title: Are Constant Loop Widths an Artifact of the Background and Spatial Resolution?\n\nAbstract: In this study, we present new findings regarding the distribution of loop widths in solar active regions, utilizing data from the Transition Region and Coronal Explorer (TRACE) at a wavelength of 171 Å, specifically collected during the solar maximum period from May to June 2001. Our analysis reveals that loops exhibiting a range of temperatures share similar width distributions, which are more accurately described by log-normal functions. As expected, the mean values of these distributions show an increase with temperature, consistent with the concept of pressure equilibrium between the plasma contained within magnetic structures and its surrounding environment. Notably, our results indicate that there is no significant difference between the widths measured along individual loops and those derived from averaged profiles across entire active regions. This finding suggests that the perceived uniformity of loop widths may be a consequence of averaging effects rather than a fundamental property of coronal structures. Furthermore, we demonstrate that the widths obtained from observations taken at different angles do not significantly vary with the orientation of the line of sight in relation to the direction perpendicular to the local magnetic force vector. These insights challenge the notion that loop widths are intrinsic characteristics of coronal loops and highlight the potential influence of observational parameters on the perceived structure of solar features. Our work underscores the importance of considering background effects and spatial resolution when interpreting loop width measurements in solar physics.",
        "ori-fast-z-score": -0.5076730825668095,
        "water-fast-z-score": 4.6467017049401695,
        "rewrite-fast-z-score": 0.3621429841700741
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Stochastic Lie group integrators .\nAbstract:\nWe present new numerical methods for stochastic differential equations (SDEs) on Lie groups, which are based on the concept of stochastic flows and their associated Lie brackets.  The proposed schemes preserve the geometric structure of SDEs in that they conserve the invariant measure and have an exact representation as Stratonovich integrals with respect to Brownian motion. We prove strong convergence rates under suitable conditions. Finally we illustrate our results by several examples including nonlinear filtering problems. Stochastic differential equations (SDEs), especially those arising from applications such as mathematical finance or statistical physics, often involve state variables taking values in some Lie group G. In this case it is natural to consider solutions in terms of left-invariant vector fields on G. However, standard numerical integration techniques do not take into account the underlying geometry of these systems and may lead to poor accuracy when applied directly. This motivates us to develop novel numerical methods for SDEs on Lie groups which incorporate the geometric information inherent in the problem.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Stochastic Lie group integrators . Abstract : We introduce current numerical models for stochastic differential equations ( SDEs ) on Lie groups , which are based on the idea of stochastic flows and their accompanying Lie brackets .The proposed methods keep the geometric structure of SDEs in that they conserve the invariant measure and have an precise representation as Stratonovich integrals with regard to Brownian movement . We establish stable convergence rates under suitable conditions .Finally we explain our findings by many instance including nonlinear filtering problems . Stochastic differential equations ( SDEs ) , particularly those arose from uses such as mathematical economics or statistical mechanics , sometimes involve state variables taking values in some Lie group G . In this instance it is natural to consider solutions in terms of leaving - invariant matrix fields on G . However , conventional numerical integration methods do not take into consideration the fundamental geometry of these systems and may lead to low accuracy when applied directly .This motivates us to develop new numerical methods for SDEs on Lie groups which employ the geometric info underlying in the question .",
        "rewrite_text": "In this article, we present a comprehensive overview of contemporary numerical models designed for stochastic differential equations (SDEs) defined on Lie groups, emphasizing the significance of stochastic flows and their associated Lie brackets. Our proposed methodologies are meticulously crafted to preserve the geometric integrity of SDEs, ensuring the conservation of invariant measures and providing an accurate representation as Stratonovich integrals in relation to Brownian motion. We demonstrate that under appropriate conditions, our methods achieve stable convergence rates, which is crucial for the reliability of numerical solutions. \n\nTo illustrate the practical implications of our findings, we explore various applications, including nonlinear filtering problems, which highlight the utility of our approaches in real-world scenarios. Stochastic differential equations frequently arise in fields such as mathematical economics and statistical mechanics, where state variables are often constrained to take values within a Lie group G. In such contexts, it becomes essential to consider solutions in terms of left-invariant matrix fields on G. \n\nHowever, traditional numerical integration techniques typically overlook the intrinsic geometric properties of these systems, leading to suboptimal accuracy when applied directly. This gap in existing methodologies motivates the development of our innovative numerical techniques for SDEs on Lie groups, which leverage the underlying geometric information inherent in the problem. By addressing these challenges, our work contributes to advancing the field of stochastic analysis and enhances the precision of numerical simulations in applications where Lie groups play a pivotal role.",
        "ori-fast-z-score": -2.5649458802128855,
        "water-fast-z-score": 5.178265442181457,
        "rewrite-fast-z-score": -0.2683281572999747
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Angular distribution studies on the two-photon ionization of hydrogen-like ions: Relativistic description .\nAbstract:\nWe present results for angular distributions in the photoionization process of H-, He+ and Li2+ by circularly polarized photons at different energies. The calculations are performed within the framework of relativistic distorted wave theory using an accurate numerical method to solve the Dirac equation with Coulomb potential. We show that our theoretical predictions agree well with available experimental data. In addition we have studied the influence of nuclear spin effects on these observables. Finally, we discuss how this information can be used as a tool to determine the fine structure constant. This is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/3.0), which permits unrestricted use, distribution, and reproduction in any medium provided that the original work is properly cited. \n \n Two-photon ionization plays an important role in many physical processes such as laser-matter interaction or astrophysical phenomena like stellar winds. It has been shown recently that it also constitutes one of the most promising methods to measure the fine-structure constant α  1  . For example, the measurement of the ratio between the cross sections corresponding to transitions into n=2 and n=3 states of heliumlike ions provides a determination of α with relative uncertainty below 10 −6  2  .\n \nIn order to perform precise measurements of the fine-structure constant through twophoton ionization experiments, it is necessary to understand theoretically all relevant aspects involved in the process. Among them, the study of the angular dependence of the emitted electrons represents a key issue since it allows us to discriminate among different contributions coming from different parts of the atomic spectrum  3  . Moreover, the comparison between experiment and theory requires high accuracy both in the calculation of the total cross section and its angular distribution  4  . \n \n In recent years there has been considerable progress in the development of computational techniques able to provide highly accurate results for the total cross section  5  , but only few works  6  -  8  have addressed the problem of calculating the angular distribution of the emitted electron. Most of those previous investigations were carried out within the nonrelativistic regime where the final state was described by means of the Schr",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Angular distribution studies on the two - photon ionization of hydrogen - like ions : Relativistic description . Abstract : We report findings for angular distributions in the photoionization process of H - , He + and Li2 + by circularly polarized photons at different energies .The studies are performed within the framework of relativistic twisted wave theory employing an accurate numerical technique to solve the Dirac equation with Coulomb potential . We see that our theory estimates agree well with provided experimental evidence .In addition we have researched the impact of nuclear spin effects on these observables . Finally , we explain how this data can be used as a tool to predict the fine structure constant .This is an Open Access article distributed under the terms of the Creative Commons Attribution License ( http : / / creativecommons . org / patents / by / 3 . 0 ) , which allows unrestricted application , distribution , and reproduction in any medium provided that the original work is properly cited . Two - photon ionization serves an important role in different physical processes such as laser - matter collision or astrophysical processes like stellar winds .It has been shown lately that it also constitutes one of the most attractive approaches to measure the fine - structure constant α 1 . For instance , the determination of the proportion between the cross sections corresponding to transitions into n = 2 and n = 3 states of heliumlike atoms provides a calculation of α with relative confidence below 10 −6 2 .In order to conduct accurate measurements of the fine - structure constant through twophoton ionization tests , it is required to explain theoretically all relevant issues involved in the process . Among them , the examination of the angular dependence of the emitted particles represents a key issue since it allows us to discriminate among different contributions come from different areas of the atomic spectrum 3 .Moreover , the comparison between experiment and theory requires large accuracy both in the determination of the total cross section and its angular distribution 4 . In recent years there has been substantial development in the development of computational tools suitable to provide highly precise data for the total cross section 5 , but only few papers 6 - 8 have solved the issue of calculating the angular distribution of the emitted particle .Most of those previous investigations were carried out within the nonrelativistic dictatorship where the last state was described by means of the Schr",
        "rewrite_text": "**Title:** Angular Distribution Studies on the Two-Photon Ionization of Hydrogen-Like Ions: A Relativistic Perspective\n\n**Abstract:** In this study, we present our findings on the angular distributions associated with the photoionization of hydrogen-like ions, specifically H⁻, He⁺, and Li²⁺, when subjected to circularly polarized photons across various energy levels. Our analysis is grounded in the relativistic twisted wave theory, utilizing a sophisticated numerical approach to solve the Dirac equation in the presence of a Coulomb potential. The results obtained from our theoretical framework demonstrate a strong correlation with existing experimental data, affirming the validity of our model. Furthermore, we investigate the influence of nuclear spin effects on the observables, enhancing our understanding of the underlying physics. \n\nThis research also highlights the potential of utilizing the gathered data as a means to predict the fine-structure constant (α). Two-photon ionization is pivotal in numerous physical phenomena, including laser-matter interactions and astrophysical events such as stellar winds. Recent advancements have positioned two-photon ionization as a promising method for accurately measuring the fine-structure constant. For example, analyzing the ratio of cross sections for transitions to n = 2 and n = 3 states in helium-like atoms allows for a determination of α with a relative uncertainty below 10⁻⁶. \n\nTo achieve precise measurements of the fine-structure constant through two-photon ionization experiments, it is essential to theoretically address all relevant aspects of the process. A critical component of this is the investigation of the angular dependence of emitted particles, which facilitates the differentiation of contributions from various regions of the atomic spectrum. Additionally, achieving a high level of accuracy in both the total cross section and its angular distribution is crucial for meaningful comparisons between theoretical predictions and experimental results. While significant progress has been made in developing computational tools for precise total cross section data, few studies have tackled the challenge of calculating the angular distribution of emitted particles, with most prior work being confined to nonrelativistic frameworks.",
        "ori-fast-z-score": 0.4399413450640599,
        "water-fast-z-score": 8.897764481371569,
        "rewrite-fast-z-score": 0.318222913670292
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Stability of spinor Fermi gases in tight waveguides .\nAbstract:\nWe study the stability of spin-1/2 fermions confined to one dimension by an external potential and interacting via contact interactions, using the Bethe ansatz solution for the Lieb-Liniger model. We find that there is no instability at zero temperature when the chemical potential lies between two consecutive energy levels of the system. This result holds true even if we consider finite temperatures as well. In particular, this implies that the ground state remains stable against collapse into a single particle state (fermionization) or formation of bound states with more than 2 particles (bosonization). The results are also valid for higher spins. Our analysis can be extended to other models such as those describing cold atoms trapped inside optical lattices. Introduction:-In recent years, ultracold atomic systems have been used extensively to simulate various physical phenomena  1  . One-dimensional quantum gases provide particularly interesting examples because they allow us to explore many-body physics in regimes where analytical solutions cannot be obtained  2  .\nThe most common experimental setup consists of confining bosonic or fermionic atoms along one spatial direction within a harmonic trap  3  , which leads to the emergence of quasi-one dimensional behavior  4  . However, it has recently become possible to confine these atoms tightly enough so that their motion becomes truly onedimensional  5  . For example, experiments performed with Bose-Einstein condensates  6  and degenerate Fermi gases  7, 8  show that confinement in a narrow channel gives rise to new phases of matter  9  . These include superfluidity  10  , supersolids  11  , Luttinger liquids  12  , Tonks-Girardeau gas  13  , and Mott insulators  14  . It would therefore be very useful to develop theoretical tools capable of predicting the properties of these novel phases  15  .\nOne of the main challenges associated with studying strongly correlated quantum systems is determining whether certain configurations are energetically favorable  16  . If the answer turns out to be yes, then we say that the configuration is metastable  17  . On the other hand, if the answer is no, then the configuration is unstable  18  . Instabilities may occur due to spontaneous symmetry",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Stability of spinor Fermi materials in dense waveguides . Abstract : We research the stability of spin - 1 / 2 fermions localized to one dimension by an external potential and communicating via contact interactions , using the Bethe ansatz solution for the Lieb - Liniger model .We see that there is no instability at zero temperature when the chemical potential sits between two consecutive power concentrations of the system . This result holds true even if we choose finite temperatures as well .In particular , this implies that the ground state remains stable against failure into a single particle state ( fermionization ) or formation of bound states with more than 2 particles ( bosonization ) . The results are also valid for larger spins .Our study can be applied to other models such as those describing cold molecules trapped inside optical lattices . Introduction : - In recent years , ultracold nuclear systems have been used heavily to simulate numerous physical phenomena 1 .One - dimensional quantum compounds provide particularly exciting examples because they allow us to examine multiple - bodies physics in regimes where theoretical solutions cannot be obtained 2 . The most common theoretical setup consists of confining bosonic or fermionic atoms along one spatial path within a harmonic cage 3 , which results to the emergence of quasi - one dimensional dynamics 4 .However , it has recently become able to confine these ions tightly sufficiently so that their motion makes truly onedimensional 5 . For instance , trials performed with Bose - Einstein condensates 6 and degenerate Fermi atoms 7 , 8 show that confinement in a thin channel gives rise to fresh stages of matter 9 .These include superfluidity 10 , supersolids 11 , Luttinger liquids 12 , Tonks - Girardeau liquid 13 , and Mott insulators 14 . It would therefore be very useful to develop conceptual tools capable of predicting the properties of these novel phases 15 .One of the main problems involved with studying strongly interacting quantum systems is assessing whether particular configurations are energetically favorable 16 . If the question turns out to be yes , then we guess that the configuration is metastable 17 .On the other hand , if the answer is no , then the configuration is unstable 18 . Instabilities could occur due to spontaneous symmetry",
        "rewrite_text": "**Title:** Stability of Spinor Fermi Materials in Dense Waveguides\n\n**Abstract:** This study investigates the stability of spin-1/2 fermions confined to one-dimensional spaces by an external potential and interacting through contact interactions. We utilize the Bethe ansatz solution for the Lieb-Liniger model to analyze the system's behavior. Our findings indicate that at zero temperature, the system exhibits stability when the chemical potential is positioned between two consecutive power concentrations. This stability persists even at finite temperatures, suggesting that the ground state is resilient against transitions to a single-particle state (known as fermionization) or the formation of bound states involving more than two particles (referred to as bosonization). Notably, these results extend to systems with larger spin values. The implications of our research are significant, as they can be applied to various models, including those that describe cold molecules trapped in optical lattices.\n\nIn recent years, ultracold nuclear systems have gained prominence for simulating a wide array of physical phenomena. One-dimensional quantum systems, in particular, offer intriguing insights into many-body physics in regimes where traditional theoretical solutions are unattainable. Typically, these systems involve confining bosonic or fermionic atoms along a single spatial dimension within a harmonic trap, leading to the emergence of quasi-one-dimensional dynamics. Recent advancements have enabled the confinement of ions to such an extent that their motion is effectively one-dimensional. Experimental studies involving Bose-Einstein condensates and degenerate Fermi gases have demonstrated that confinement in narrow channels can give rise to novel states of matter, including superfluidity, supersolids, Luttinger liquids, Tonks-Girardeau liquids, and Mott insulators. Consequently, developing theoretical frameworks to predict the properties of these emergent phases is crucial. A central challenge in studying strongly interacting quantum systems is determining the energetic favorability of specific configurations. If a configuration is energetically favorable, it is considered metastable; conversely, if it is not, the configuration is deemed unstable. Such instabilities may arise from spontaneous symmetry breaking, further complicating the understanding of these complex systems.",
        "ori-fast-z-score": -1.1607348488012053,
        "water-fast-z-score": 6.943650748294136,
        "rewrite-fast-z-score": 1.0441851275732486
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Calibration of the GLAST Burst Monitor detectors .\nAbstract:\nThe Large Area Telescope (LAT) on board Fermi Gamma-ray Space Telescope is scheduled to be launched in June 2008 and will perform high energy gamma ray astronomy for at least 3 years. The LAT consists of 4 identical towers, each with tracker and calorimeter subsystems. Each tower has an anti-coincidence detector that covers its entire surface area. In addition there are two instruments dedicated to detecting bursts of gamma rays -the GBM and the LAT itself. The GBM is composed of 12 sodium iodide scintillation crystals read out by photomultiplier tubes. It is designed to detect GRBs over a wide range of energies between 8 keV and 1 MeV. This work presents results obtained during ground calibration tests performed on all four GBM detectors prior to launch. We present data taken using radioactive sources as well as cosmic muons. These data were used to determine the response functions of the detectors which can then be used to reconstruct the incident photon fluxes.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Calibration of the GLAST Burst Monitor detectors . Abstract : The Large Area Telescope ( LAT ) on board Fermi Gamma - ray Space Telescope is slated to be launched in June 2008 and will perform large energy gamma ray observations for at least 3 years .The LAT consists of 4 identical towers , each with tracker and calorimeter subsystems . Each tower has an counter - coincidence detector that covers its entire surface region .In addition there are two instruments dedicated to detecting bursts of gamma radiation - the GBM and the LAT itself . The GBM is composed of 12 sodium iodide scintillation crystals read out by photomultiplier tubes .It is designed to identify GRBs over a broad variety of energies between 8 keV and 1 MeV . This research provides findings obtained during ground calibration tests performed on all four GBM detectors previous to launch .We present data taken using nuclear sources as also as cosmic muons . These data were used to identify the response functions of the detectors which can then be used to reconstruct the incident photon fluxes .",
        "rewrite_text": "**Title: Calibration of the GLAST Burst Monitor Detectors**\n\n**Abstract:** The Large Area Telescope (LAT), part of the Fermi Gamma-ray Space Telescope, is scheduled for launch in June 2008 and is expected to conduct extensive gamma-ray observations across a wide energy range for a minimum of three years. The LAT is comprised of four identical towers, each equipped with tracker and calorimeter subsystems, along with a counter-coincidence detector that spans the entire surface area of each tower. Complementing the LAT, two instruments are specifically designed for the detection of gamma-ray bursts (GRBs): the Gamma-ray Burst Monitor (GBM) and the LAT itself. The GBM features twelve sodium iodide scintillation crystals, which are read by photomultiplier tubes, enabling it to detect GRBs across a broad energy spectrum from 8 keV to 1 MeV. This study presents the results from ground calibration tests conducted on all four GBM detectors prior to their launch. We detail the data collected using both nuclear sources and cosmic muons, which were instrumental in determining the response functions of the detectors. These response functions are crucial for accurately reconstructing the incident photon fluxes, thereby enhancing the understanding of gamma-ray emissions and their origins. The calibration process ensures that the GBM will effectively contribute to the detection and analysis of GRBs, providing valuable insights into high-energy astrophysical phenomena. The findings from this research not only validate the operational readiness of the GBM but also lay the groundwork for future studies in gamma-ray astronomy, ultimately contributing to the broader goals of the Fermi mission.",
        "ori-fast-z-score": -1.1952286093343936,
        "water-fast-z-score": 3.4416668087621747,
        "rewrite-fast-z-score": 0.38851434494290565
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  New AB-Thermonuclear Reactor for Aerospace .\nAbstract:\nThe new reactor is based on the principle that nuclear fusion can be achieved by heating plasma to high temperatures in magnetic fields.  The proposed design uses an array of magnets and electrodes, which are powered by microwave generators, to heat hydrogen gas into plasma at 100 million degrees Celsius (1,000,000 K). This temperature is sufficient to cause nuclear fusion reactions between deuterium nuclei inside the plasma. In this way, energy is released as neutrons and gamma rays. These particles then bombard fuel pellets made up of tritium and lithium-6, causing them to release more energy through further nuclear fusion reactions. The resulting power plant would produce electrical power with no moving parts or radioactive waste products. It could also provide propulsion systems for spacecraft without requiring any chemical fuels. A conceptual drawing of the reactor system is shown below. Nuclear fusion has been studied extensively since its discovery over 50 years ago. However, it remains elusive because the conditions required to achieve fusion are extremely difficult to create experimentally.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : New AB - Thermonuclear Reactor for Aerospace . Abstract : The proposed reactor is based on the principle that atomic fusion can be obtained by burning plasma to large conditions in magnetic fields .The proposed design uses an array of magnets and electrodes , which are powered by microwave generators , to heat hydrogen gas into plasma at 100 million degrees Celsius ( 1 , 000 , 000 K ) . This temperature is adequate to produce nuclear fusion events between deuterium nuclei inside the plasma .In this way , electricity is released as neutrons and alpha rays . These particles then bombard fuel pellets made up of tritium and lithium - 6 , forcing them to release more power through further nuclear fusion events .The resulting electricity plant would create electrical power with no moving parts or radioactive waste products . It could also supply propulsion services for spacecraft without need any chemical fuels .A conceptual drawing of the reactor system is displayed below . Nuclear fusion has been studied thoroughly since its revelation over 50 generations earlier .However , it remains elusive because the conditions utilized to achieve fusion are extremely difficult to create experimentally .",
        "rewrite_text": "**Title: New AB-Thermonuclear Reactor for Aerospace**\n\n**Abstract:** This article presents a novel thermonuclear reactor design aimed at advancing aerospace technology through the harnessing of atomic fusion. The reactor operates on the principle of generating fusion by creating a high-energy plasma within strong magnetic fields. The innovative design incorporates a sophisticated arrangement of magnets and electrodes, energized by microwave generators, to elevate hydrogen gas to a staggering temperature of 100 million degrees Celsius (1,000,000 K). At this extreme temperature, conditions become favorable for nuclear fusion reactions to occur between deuterium nuclei present in the plasma. The fusion process releases energy in the form of neutrons and alpha particles, which subsequently interact with specially designed fuel pellets composed of tritium and lithium-6. This interaction triggers additional fusion reactions, resulting in a significant increase in energy output.\n\nThe proposed reactor not only promises to generate electricity efficiently but also does so without any moving components or the production of radioactive waste, making it an environmentally friendly alternative to traditional energy sources. Furthermore, this technology has the potential to provide propulsion for spacecraft, eliminating the reliance on chemical fuels and enhancing the feasibility of long-duration space missions. The article includes a conceptual illustration of the reactor system, highlighting its innovative features and operational principles. While nuclear fusion has been a subject of extensive research for over five decades, achieving the necessary conditions for sustained fusion has proven to be a formidable challenge. This new reactor design aims to overcome these obstacles, paving the way for practical applications of fusion energy in aerospace and beyond.",
        "ori-fast-z-score": 0.8251369970070347,
        "water-fast-z-score": 5.9196002117260145,
        "rewrite-fast-z-score": 0.4240944648399855
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Symmetric and asymmetric solitons in linearly coupled Bose-Einstein condensates trapped in optical lattices .\nAbstract:\nWe study the dynamics of two interacting bosonic species confined to an optical lattice, with one species being initially prepared as a coherent state at each site while the other is initially prepared as a thermal cloud. We show that this system supports both symmetric and asymmetric soliton solutions which are stable against small perturbations for certain values of the chemical potentials. The stability properties of these solitons can be understood by studying their linearization spectrum around the stationary states. In particular we find that the presence of a finite temperature leads to additional unstable modes associated with phonon-like excitations. Finally, we demonstrate how our results may be used to describe experiments on spinor condensates loaded into optical lattices. Introduction:-Recent experimental advances have made it possible to create quantum degenerate gases consisting of several different atomic species  1  . These systems provide new opportunities to explore novel phenomena such as supersolids  2  , phase separation  3  or spin-orbit coupling  4  .\nIn this work we consider a particularly interesting example where there exist two distinct types of particles (e.g., atoms) which interact via s-wave scattering but differ in mass and/or internal structure  5  . This situation arises naturally when considering mixtures of hyperfine states  6  or isotopes  7, 8  within the same atom type  9  . For instance, recent experiments involving 87 Rb and 41 K  10  have demonstrated the formation of a mixture of two different hyperfine states after evaporative cooling  11  . Another possibility would involve using 40 K and 6 Li  12  . Here, the lighter species could be considered as impurities immersed in a background gas of heavier fermions  13  . Alternatively, if the masses were reversed then the heavy species could act as impurities  14  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Symmetric and asymmetric solitons in linearly coupled Bose - Einstein condensates trapped in optical lattices . Abstract : We research the dynamics of two interacting bosonic species confined to an optical lattice , with one species being initially made as a coherent state at each site while the other is initially prepared as a heat bubble .We see that this scheme accepts both symmetric and asymmetric soliton solutions which are stable against small perturbations for particular values of the chemical potentials . The stability properties of these solitons can be understood by examining their linearization spectrum around the stationary states .In particular we find that the presence of a finite temperature leads to extra unstable modes associated with phonon - like excitations . Finally , we prove how our findings may be used to explain studies on spinor condensates stacked into optical lattices .Introduction : - Recent scientific discoveries have enabled it able to create quantum degenerate gases composed of several different atomic species 1 . These systems create fresh possibilities to examine novel processes such as supersolids 2 , phase splitting 3 or spin - orbit coupling 4 .In this study we imagine a particularly important example where there exist two different kinds of atoms ( e . g . , atoms ) which interact via s - wave scattering but differ in mass and / or internal structure 5 . This condition occurs naturally when examining mixtures of hyperfine states 6 or isotopes 7 , 8 within the same atom type 9 .For instance , recent experiments using 87 Rb and 41 K 10 have demonstrated the formation of a mixture of two different hyperfine states after evaporative cooling 11 . Another possibility would include utilizing 40 K and 6 Li 12 .Here , the lighter species may be regarded as impurities immersed in a background gas of darker fermions 13 . Alternatively , if the masses were reversed then the heavy species may function as impurities 14 .",
        "rewrite_text": "**Title:** Symmetric and Asymmetric Solitons in Linearly Coupled Bose-Einstein Condensates Trapped in Optical Lattices\n\n**Abstract:** This study investigates the dynamics of two interacting bosonic species confined within an optical lattice framework. One species is initialized as a coherent state at each lattice site, while the other is prepared as a thermal bubble. Our findings reveal the existence of both symmetric and asymmetric soliton solutions that exhibit stability against minor perturbations, contingent upon specific values of the chemical potentials. To understand the stability characteristics of these solitons, we analyze their linearization spectrum around the stationary states. Notably, we observe that the introduction of finite temperature results in additional unstable modes linked to phonon-like excitations. Furthermore, we demonstrate how our results can elucidate existing research on spinor condensates arranged within optical lattices.\n\n**Introduction:** Recent advancements in scientific research have facilitated the creation of quantum degenerate gases comprising multiple atomic species. These innovative systems open up new avenues for exploring phenomena such as supersolids, phase separation, and spin-orbit coupling. In this work, we focus on a significant scenario involving two distinct atomic types that interact through s-wave scattering, differing in mass and/or internal structure. This situation is commonly encountered in mixtures of hyperfine states or isotopes of the same atomic species. For example, recent experiments with rubidium-87 and potassium-41 have successfully demonstrated the formation of a mixture of different hyperfine states following evaporative cooling. Another potential combination includes potassium-40 and lithium-6, where the lighter species can be viewed as impurities within a background of heavier fermions. Conversely, if the mass distribution is reversed, the heavier species may act as impurities. This research aims to deepen our understanding of the complex interactions and dynamics within these coupled bosonic systems.",
        "ori-fast-z-score": -0.6713450866373513,
        "water-fast-z-score": 5.564047150200578,
        "rewrite-fast-z-score": 1.9295276424754644
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Coalition Games with Cooperative Transmission: A Cure for the Curse of Boundary Nodes in Selfish Packet-Forwarding Wireless Networks .\nAbstract:\nWe consider a wireless network where nodes selfishly forward packets to their neighbors, and study how cooperation among neighboring nodes can be used to improve performance. We propose a novel cooperative transmission scheme that allows each node to transmit its packet simultaneously on multiple channels using different power levels. The proposed scheme is shown to achieve higher throughput than existing schemes by exploiting spatial diversity across channels as well as power control over these channels. In addition, we show that our scheme achieves fairness between nodes without requiring any centralized controller or global information about the network topology. Finally, we present simulation results showing significant improvement in terms of both throughput and fairness compared to existing non-cooperative schemes. This work was supported by NSF under Grants CNS-0721440 (CAREER), ECCS-0701260, and CNS-0834753. Abstract: We consider a wireless network where all nodes are selfish and only cooperate when it benefits them. We propose a new cooperative transmission scheme which enables each node to transmit its own packet simultaneously on multiple channels at different power levels. Our scheme exploits spatial diversity across channels as wells as power control over those channels to increase the overall system throughput while maintaining fairness amongst competing nodes.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Coalition Games with Cooperative Transmission : A Cure for the Curse of Boundary Nodes in Selfish Packet - Forwarding Wireless Networks . Abstract : We consider a wireless network where nodes selfishly forward messages to their relatives , and consider how cooperation among neighboring nodes can be used to promote success .We suggest a new cooperative communication plan that enables each node to transmit its packet simultaneously on various frequencies using distinct power rates . The proposed system is demonstrated to achieve greater throughput than existing proposals by exploiting spatial diversity across channels as well as power control over these networks .In addition , we prove that our scheme achieves fairness between nodes without using any centralized manager or worldwide data about the network topology . Finally , we present modeling results showing significant increase in terms of both throughput and fairness compared to existing non - cooperative systems .This project was supported by NSF under Grants CNS - 0721440 ( CAREER ) , ECCS - 0701260 , and CNS - 0834753 . Abstract : We consider a wireless network where all nodes are selfish and only cooperate when it benefits them .We suggest a new cooperative communication plan which enables each node to carry its own packet simultaneously on various stations at different power rates . Our strategy exploits geographical variety across channels as wells as power control over those channels to raise the overall network throughput while maintaining fairness amongst competing networks .",
        "rewrite_text": "Title: Coalition Games with Cooperative Transmission: A Solution to the Boundary Node Challenge in Selfish Packet-Forwarding Wireless Networks\n\nAbstract: In this study, we investigate a wireless network characterized by nodes that exhibit selfish behavior, forwarding messages primarily to their immediate connections. We explore the potential for cooperation among neighboring nodes to enhance overall network performance. To this end, we propose an innovative cooperative communication strategy that allows each node to transmit its packets concurrently across multiple frequencies, utilizing varying power levels. Our approach leverages spatial diversity across different channels and incorporates power control mechanisms, resulting in a significant improvement in throughput compared to existing methods. Furthermore, we demonstrate that our system fosters fairness among nodes without the need for a centralized management structure or comprehensive knowledge of the network topology. Through extensive modeling, we present compelling evidence of substantial gains in both throughput and fairness when juxtaposed with traditional non-cooperative systems. This research was supported by the National Science Foundation under Grants CNS-0721440 (CAREER), ECCS-0701260, and CNS-0834753. Our findings indicate that by facilitating cooperation among nodes, we can effectively mitigate the challenges posed by boundary nodes in selfish packet-forwarding wireless networks, ultimately leading to a more efficient and equitable communication environment.",
        "ori-fast-z-score": -0.9128709291752769,
        "water-fast-z-score": 6.090909090909091,
        "rewrite-fast-z-score": 0.40406101782088427
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Calcium and synaptic dynamics underlying reverberatory activity in neuronal networks .\nAbstract:\nReverberation is an important feature of brain function, but its role remains unclear. We show that reverberation can be generated by the interplay between calcium signaling and short-term plasticity (STP) at excitatory synapses. In our model, STP leads to bursts of spikes which are followed by periods of low firing rate due to depletion of neurotransmitter vesicles. The resulting slow recovery of transmitter release causes a build-up of residual calcium concentration  Ca  res , leading to facilitation of glutamate release during subsequent bursts. This positive feedback loop generates sustained reverberatory activity with multiple time scales. Our results suggest that reverberation may play a key role in information processing within neural circuits. Reverberation is one of the most prominent features of brain function  1  . It has been observed across different species  2  -  4  as well as in various brain areas including hippocampus  5  , neocortex  6  , thalamus  7  , striatum  8  , cerebellar cortex  9  , olfactory bulb  10  , and retina  11  .\nDespite its ubiquity, however, it still remains unknown what functional roles reverberation plays in the brain  12  . One possibility is that reverberation serves as a mechanism for memory storage  13  or retrieval  14  . Another hypothesis suggests that reverberation could serve as a substrate for working memory  15  . Yet another idea is that reverberation might provide a means for temporal coding  16  . Finally, some studies have suggested that reverberation may simply reflect ongoing spontaneous activity  17  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Calcium and synaptic dynamics underlying reverberatory behavior in neuronal systems . Abstract : Reverberation is an important feature of cerebral activity , but its significance remains unsure .We see that reverberation can be triggered by the interplay between calcium signaling and low - term plasticity ( STP ) at excitatory synapses . In our model , STP results to bursts of spikes which are preceded by periods of poor fired rate due to depletion of neurotransmitter vesicles .The resulting slow recovery of transmitter release generates a build - up of residual calcium affinity Ca res , leading to facilitation of glutamate production during later bursts . This positive feedback loop generates sustained reverberatory behavior with various time ranges .Our results show that reverberation possibly play a key importance in information processing within neural pathways . Reverberation is one of the most notable features of cerebral function 1 .It has been observed across different species 2 - 4 as well as in different brain regions including hippocampus 5 , neocortex 6 , thalamus 7 , striatum 8 , cerebellar cortex 9 , olfactory bulb 10 , and retina 11 . Despite its ubiquity , however , it still remains obscure what functional functions reverberation plays in the brain 12 .One possibility is that reverberation provides as a system for memory processing 13 or retrieval 14 . Another hypothesis suggests that reverberation possibly provide as a substrate for working storage 15 .Yet another idea is that reverberation would offer a means for temporal coding 16 . Finally , some researchers have suggested that reverberation would merely reflect ongoing spontaneous activity 17 .",
        "rewrite_text": "**Title:** Calcium and Synaptic Dynamics Underlying Reverberatory Behavior in Neuronal Systems\n\n**Abstract:** Reverberation is a significant characteristic of cerebral activity, yet its precise role in neural function remains uncertain. This study explores the mechanisms by which reverberation is initiated, focusing on the interaction between calcium signaling and short-term plasticity (STP) at excitatory synapses. Our model demonstrates that STP leads to bursts of neuronal spikes, which are initially preceded by phases of reduced firing rates due to the depletion of neurotransmitter vesicles. The slow recovery of neurotransmitter release contributes to an accumulation of residual calcium, denoted as Ca_res, which subsequently enhances glutamate production during subsequent bursts of activity. This positive feedback mechanism fosters sustained reverberatory behavior across various temporal scales. \n\nOur findings suggest that reverberation may play a crucial role in information processing within neural circuits. It is a prominent feature of cerebral function, observed in a variety of species and across multiple brain regions, including the hippocampus, neocortex, thalamus, striatum, cerebellar cortex, olfactory bulb, and retina. Despite its widespread occurrence, the functional implications of reverberation in the brain remain poorly understood. One potential function is its involvement in memory processing or retrieval, while another hypothesis posits that it serves as a substrate for working memory storage. Additionally, reverberation may facilitate temporal coding within neural networks. Conversely, some researchers propose that reverberation could simply reflect ongoing spontaneous neural activity. This study aims to elucidate the underlying mechanisms of reverberation and its potential contributions to cognitive processes, thereby enhancing our understanding of its role in neuronal dynamics.",
        "ori-fast-z-score": -0.18569533817705186,
        "water-fast-z-score": 7.4884526490405925,
        "rewrite-fast-z-score": 0.2683281572999747
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Topological insulators beyond the Brillouin zone via Chern parity .\nAbstract:\nWe propose an alternative approach to topological insulators based on the concept of Chern parity, which is defined as the product of all occupied bands in momentum space and can be viewed as a generalization of the Z2 index for time-reversal invariant systems. We show that this quantity has several useful properties such as being gauge independent and robust against disorder. In particular we demonstrate how it allows one to define new classes of topological insulators with nontrivial topology even when the system does not have inversion symmetry or time reversal invariance. Finally, we discuss possible experimental realizations using ultracold atoms trapped in optical lattices. Topological insulators are materials whose bulk states are insulating but their edges support gapless excitations protected by certain symmetries  1  . The most prominent example is provided by two-dimensional (2D) quantum spin Hall insulators  2  , where the edge states carry a single Dirac fermion per unit cell  3  .\nThe existence of these exotic states relies crucially on the presence of either timereversal  4  or particle-hole  5  symmetry. However, there exist other types of topological phases  6  characterized by different kinds of order parameters  7, 8  . For instance, 2D topological superconductors  9  are described by Majorana zero modes  10  while 3D Weyl semimetals  11  host chiral Fermi arcs  12  at their surfaces  13  . These novel phenomena cannot be captured within the standard classification scheme  14, 15  relying only on time-reversal and/or inversion symmetry  16  .\nIn this Letter, we introduce a new class of topological insulators  17  based on the concept of  Chern parity   18  , which is defined as the sum over all occupied bands in reciprocal space  19  \nwhere |u nk ⟩ denotes the Bloch wavefunction corresponding to band n and crystal momentum k. This quantity plays a central role in our analysis since it provides a natural generalization of the Z 2 index  20  characterizing time-reversal invariant topological insulators  21  . Indeed, if the system preserves both time-reversal T and inversion P symmetries then C = 1 mod 4  22  . On",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Topological insulators beyond the Brillouin zone via Chern parity . Abstract : We suggest an additional method to topological insulators based on the idea of Chern parity , which is characterized as the sum of all occupied bands in momentum space and can be viewed as a generalization of the Z2 index for moment - reversal invariant networks .We see that this quantity has numerous valuable characteristics such as being gauge independent and strong against disorder . In particular we prove how it allows one to define novel classes of topological insulators with nontrivial topology even when the scheme does not have inversion symmetry or time reversal invariance .Finally , we explain possible experimental realizations using ultracold atoms trapped in optical lattices . Topological insulators are materials whose bulk states are insulating but their edges hold gapless excitations guarded by certain symmetries 1 .The most notable instance is provided by two - dimensional ( 2D ) quantum spin Hall insulators 2 , where the edge states bear a single Dirac fermion per unit cell 3 . The existence of these exotic states depends crucially on the presence of either timereversal 4 or particle - hole 5 symmetry .However , there exist other types of topological phases 6 characterized by various kinds of order variables 7 , 8 . For instance , 2D topological superconductors 9 are explained by Majorana zero modes 10 while 3D Weyl semimetals 11 host chiral Fermi arcs 12 at their surfaces 13 .These novel processes cannot be captured within the standard classification system 14 , 15 relying only on time - reversal and / or inversion symmetry 16 . In this Letter , we provide a new category of topological insulators 17 based on the idea of Chern parity 18 , which is characterized as the sum over all inhabited bands in reciprocal space 19 where | v nk ⟩ represents the Bloch wavefunction corresponding to band k and crystal momentum g . This value takes a central role in our analysis since it gives a natural generalization of the Z 2 index 20 characterizing time - reversal invariant topological insulators 21 .Indeed , if the scheme preserves both time - reversal T and inversion P symmetries then C = 1 mod 4 22 . On",
        "rewrite_text": "Title: Topological Insulators Beyond the Brillouin Zone via Chern Parity\n\nAbstract: In this study, we introduce a novel approach to understanding topological insulators through the concept of Chern parity, which is defined as the sum of all occupied bands in momentum space. This concept serves as a generalization of the Z2 index, particularly applicable to systems that exhibit time-reversal invariance and lack inversion symmetry. We demonstrate that Chern parity possesses several advantageous properties, including gauge independence and robustness against disorder, making it a powerful tool for classifying topological phases. Our findings reveal that this quantity enables the identification of new classes of topological insulators that exhibit nontrivial topological characteristics, even in the absence of traditional symmetries such as inversion or time-reversal invariance.\n\nTopological insulators are materials that are insulating in their bulk but host gapless edge states protected by specific symmetries. A prominent example is the two-dimensional quantum spin Hall insulator, which features edge states represented by a single Dirac fermion per unit cell. The existence of these edge states is contingent upon the presence of either time-reversal or particle-hole symmetry. However, various other topological phases exist, characterized by distinct order parameters, such as two-dimensional topological superconductors with Majorana zero modes and three-dimensional Weyl semimetals that exhibit chiral Fermi arcs on their surfaces. These phenomena cannot be adequately described by the conventional classification framework that relies solely on time-reversal and inversion symmetries.\n\nIn this letter, we propose a new classification scheme for topological insulators based on Chern parity, which is computed as the sum over all occupied bands in reciprocal space, represented by the Bloch wavefunction corresponding to band k and crystal momentum g. This framework provides a natural extension of the Z2 index for time-reversal invariant topological insulators. Specifically, when both time-reversal (T) and inversion (P) symmetries are preserved, we find that Chern parity satisfies the relation C = 1 mod 4. We also discuss potential experimental implementations of our findings using ultracold atoms confined in optical lattices, paving the way for further exploration of these intriguing topological phases.",
        "ori-fast-z-score": -0.5107539184552492,
        "water-fast-z-score": 6.377778323054837,
        "rewrite-fast-z-score": 1.9100460366360192
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Common Origin of Linear and Nonlinear Chiral Multiplets in N=4 Mechanics .\nAbstract:\nWe show that the common origin of linear and nonlinear chiral multiplets is related to the existence of an extra dimension, which can be either time-like or space-like. We also discuss how this picture fits into the context of string theory. Theories with extended supersymmetry are known to have many interesting features such as duality symmetries  1  . In particular, theories with four supercharges (N = 4) possess remarkable properties  2  , including self-duality  3  .\nIn recent years there has been renewed interest in studying these theories due to their connection with string/M-theory  4  . For example, it was shown  5  that type-IIB strings on AdS 5 × S 5 background  6  correspond to maximally supersymmetric Yang-Mills theory in four dimensions  7, 8  . This correspondence allows one to study strongly coupled gauge theories using weakly-coupled gravitational description  9  . It turns out that the same idea works for other types of field theories  10  -  12  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Common Origin of Linear and Nonlinear Chiral Multiplets in N = 4 Mechanics . Abstract : We see that the common origin of linear and nonlinear chiral multiplets is related to the existence of an additional element , which can be either time - like or space - like .We also discuss how this picture fits into the context of string theory . Theories with stretched supersymmetry are known to have many interesting features such as duality symmetries 1 .In particular , theories with four supercharges ( N = 4 ) possess extraordinary properties 2 , notably self - duality 3 . In recent years there has been continued interest in investigating these theories owing to their connection with string / M - theory 4 .For instance , it was shown 5 that type - IIB sequences on AdS 5 × S 5 background 6 belong to maximally supersymmetric Yang - Mills theory in four dimensions 7 , 8 . This correspondence allows one to study weakly coupled gauge fields use strongly - coupled gravitational description 9 .It turns out that the same idea works for other types of field equations 10 - 12 .",
        "rewrite_text": "**Title:** The Common Origin of Linear and Nonlinear Chiral Multiplets in N = 4 Mechanics\n\n**Abstract:** This article explores the fundamental relationship between linear and nonlinear chiral multiplets, highlighting their shared origin linked to the presence of an additional element that can manifest as either time-like or space-like. We delve into how this framework integrates within the broader context of string theory, particularly emphasizing the intriguing characteristics of theories exhibiting stretched supersymmetry. The study of such theories, especially those with four supercharges (N = 4), has garnered significant attention due to their remarkable properties, including self-duality. Recent investigations have further illuminated the connections between these theories and string/M-theory, revealing a rich tapestry of duality symmetries that enhance our understanding of high-energy physics. Notably, it has been demonstrated that type-IIB string theories formulated on the AdS5 × S5 background correspond to maximally supersymmetric Yang-Mills theory in four dimensions. This correspondence facilitates the analysis of weakly coupled gauge fields through the lens of a strongly coupled gravitational framework. Furthermore, the principles derived from this correspondence extend to various other field equations, suggesting a unified approach to understanding the dynamics of chiral multiplets within the context of N = 4 mechanics. The implications of these findings are profound, as they not only deepen our comprehension of supersymmetry but also pave the way for future research in theoretical physics, particularly in the realms of gauge theories and string theory.",
        "ori-fast-z-score": 0.5933908290969266,
        "water-fast-z-score": 4.714045207910317,
        "rewrite-fast-z-score": 2.2
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  On purely transmitting defects in affine Toda field theory .\nAbstract:\nWe study the transmission properties of defects in affine Toda field theories (ATFTs). We show that all ATFTs with simply-laced Lie algebras have only one type of defect, which is transmitted by itself and its images under the action of the Weyl group. This result generalizes previous results for A_n ATFTs to arbitrary ATFTs. In particular we find that the reflection coefficients are given by simple expressions involving the Coxeter number h of the underlying Lie algebra. The analysis presented here can be extended to other integrable models such as supersymmetric extensions or higher rank ATFTs. Defects play an important role in many physical systems ranging from condensed matter physics over particle physics to string theory. They appear when two different phases coexist at some point in space-time. For example they may arise if there exists a domain wall between two regions where the vacuum expectation values of certain fields differ. Another possibility is that defects occur due to impurities in solids or liquids. In this case it has been shown that the effective description of these defects leads to integrable models  1  . Integrability means that the scattering amplitudes factorize into products of two-body S-matrices  2  .\nIn recent years much progress was made on understanding the structure of defects in integrable quantum field theories  3  -  8  . It turned out that most of them can be classified according to their transmission properties  9  , i.e., whether they transmit themselves and their images under the action of a symmetry group G. If so, then the corresponding defect S-matrix is diagonalized simultaneously with the bulk S-matrix  10  . However, not every integrable model allows for such a classification  11  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : On purely transmitting defects in affine Toda field theory . Abstract : We research the transmission properties of errors in affine Toda field theories ( ATFTs ) .We see that all ATFTs with simply - laced Lie algebras have only one sort of defect , which is transmitted by itself and its images under the action of the Weyl group . This result generalizes earlier findings for A _ k ATFTs to arbitrary ATFTs .In particular we find that the reflection coefficients are given by simple symbols relating the Coxeter number h of the underlying Lie algebra . The theory offered here can be generalized to other integrable models such as supersymmetric extensions or greater rank ATFTs .Defects serve an important role in many physical structures ranging from condensed matter science over quantum mechanics to string theory . They arise when two different phases coexist at some time in space - time .For instance they may arise if there exists a domain wall between two zones where the vacuum expectation values of certain fields vary . Another possibility is that defects happen owing to impurities in solids or liquids .In this situation it has been shown that the effective formulation of these defects results to integrable models 1 . Integrability means that the scattering amplitudes factorize into components of two - bodies S - matrices 2 .In recent years much work was making on understanding the composition of flaws in integrable quantum field theories 3 - 8 . It turned out that most of them can be categorized according to their transmission properties 9 , i . e . , whether they transmit themselves and their images under the action of a symmetry group G . If so , then the associated defect S - vector is diagonalized simultaneously with the bulk S - vector 10 .However , not every integrable model allows for such a class 11 .",
        "rewrite_text": "In this study, we investigate the transmission characteristics of defects within affine Toda field theories (ATFTs). Our findings reveal that all ATFTs associated with simply-laced Lie algebras exhibit a singular type of defect, which is capable of transmitting itself along with its images as dictated by the Weyl group. This discovery extends previous research conducted on A_k ATFTs to encompass all varieties of ATFTs. Notably, we establish that the reflection coefficients can be expressed through straightforward symbols that are directly related to the Coxeter number \\( h \\) of the corresponding Lie algebra. Furthermore, the theoretical framework presented here is applicable to a broader range of integrable models, including supersymmetric extensions and ATFTs of higher rank.\n\nDefects play a crucial role in various physical phenomena, spanning fields such as condensed matter physics, quantum mechanics, and string theory. They typically emerge when two distinct phases coexist in a given region of spacetime. For example, defects may occur at a domain wall separating areas where the vacuum expectation values of certain fields differ. Additionally, defects can arise due to the presence of impurities in solids or liquids. Previous research has demonstrated that an effective description of these defects leads to integrable models, where integrability implies that scattering amplitudes can be decomposed into products of two-body S-matrices.\n\nIn recent years, significant efforts have been directed toward understanding the nature of defects in integrable quantum field theories. It has been found that many of these defects can be classified based on their transmission properties, specifically whether they can transmit themselves and their images under the action of a symmetry group \\( G \\). When this is the case, the corresponding defect S-vector can be diagonalized simultaneously with the bulk S-vector. However, it is important to note that not all integrable models permit such a classification.",
        "ori-fast-z-score": -1.5787044347526527,
        "water-fast-z-score": 5.504335556964539,
        "rewrite-fast-z-score": -0.5937322507759797
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Post-Newtonian gravitational radiation and equations of motion via direct integration of the relaxed Einstein equations. V. Evidence for the strong equivalence principle to second post-Newtonian order .\nAbstract:\nWe present evidence that the strong equivalence principle (SEP) holds in general relativity at least up to second post Newtonian order, by showing that the SEP is satisfied if it holds at first PN order.  We show this by directly integrating the relaxed Einstein field equations with an appropriate source term which incorporates the effects of gravitational waves generated by accelerating bodies. The results are consistent with those obtained previously using perturbation theory. This work was supported by NSF grant PHY-0456747. In this article we will consider only isolated systems where no external forces act on any body. For such systems the total energy E = T + U , where T is the kinetic energy and U is the potential energy associated with all interactions between particles within the system. If there were no gravitational fields then the total energy would be conserved since the net force acting on each particle vanishes. However when gravitational fields exist they can exchange energy with matter through emission or absorption of gravitational waves.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Post - Newtonian gravitational rays and equations of movement via direct combining of the relaxation Einstein equations . V . Evidence for the strong equivalence principle to second post - Newtonian order .Abstract : We present evidence that the strong equivalence principle ( SEP ) holds in general relativity at least up to second post Newtonian order , by showing that the SEP is fulfilled if it holds at first PN order . We see this by specifically integrating the relaxation Einstein field equations with an appropriate source term which includes the effects of gravitational waves generated by accelerating objects .The results are compatible with those achieved formerly using perturbation theory . This research was supported by NSF grant PHY - 0456747 .In this discussion we will use only isolated systems where no external forces action on any body . For such systems the total energy E = T + U , where T is the kinetic power and U is the potential energy associated with all interactions between particles within the system .If there were no gravity fields then the total energy must be conserved since the net force acting on each particle vanishes . However when gravity fields arise they can trade power with matter through emission or emission of gravitational waves .",
        "rewrite_text": "Title: Post-Newtonian Gravitational Rays and Equations of Motion via Direct Combination of the Relaxation Einstein Equations: Evidence for the Strong Equivalence Principle to Second Post-Newtonian Order\n\nAbstract: In this study, we provide compelling evidence supporting the strong equivalence principle (SEP) within the framework of general relativity, demonstrating its validity at least up to the second post-Newtonian (PN) order. Our findings indicate that if the SEP is satisfied at the first PN order, it necessarily holds at the second order as well. This conclusion is derived from a detailed integration of the relaxation Einstein field equations, incorporating a source term that accounts for the influence of gravitational waves produced by accelerating bodies. The results obtained align closely with those previously established through perturbation theory, reinforcing the robustness of our approach. \n\nOur analysis is confined to isolated systems, where no external forces act upon any of the bodies involved. In such scenarios, the total energy of the system, denoted as E, is the sum of the kinetic energy (T) and the potential energy (U) arising from interactions among the particles. In the absence of gravitational fields, the conservation of total energy is guaranteed, as the net force on each particle is zero. However, the introduction of gravitational fields complicates this picture, as they can exchange energy with matter through the emission or absorption of gravitational waves. \n\nThis research was made possible through the support of NSF grant PHY-0456747, and it contributes to our understanding of the fundamental principles governing gravitational interactions in the context of general relativity. The implications of our findings extend to various fields, including astrophysics and cosmology, where the behavior of gravitational waves and their interactions with matter play a crucial role in the dynamics of celestial systems.",
        "ori-fast-z-score": 0.29851115706299675,
        "water-fast-z-score": 5.527707983925667,
        "rewrite-fast-z-score": 3.028960741674143
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Mixed-up trees: the structure of phylogenetic mixtures .\nAbstract:\nWe study the problem of reconstructing evolutionary histories that contain both reticulation events and vertical transmission, where reticulations are represented by hybridization or horizontal gene transfer (HGT). We show how to use maximum parsimony to find an optimal solution for this problem in polynomial time when all input trees have bounded degree. This is achieved through a novel dynamic programming algorithm which computes the minimum number of reticulation events needed to explain each set of taxa on every tree in the input collection. Our approach can be used as part of a larger method for inferring species networks using collections of rooted binary phylogenies inferred from different types of data such as DNA sequences, morphological characters, fossil records etc., and it may also provide insights into the evolution of complex organisms like viruses and bacteria. \nIntroduction\n\nThe reconstruction of evolutionary relationships among biological entities has been one of the central problems in biology since Darwin s On the Origin of Species was published 150 years ago  1  . The most widely accepted model of evolution assumes that new species evolve over time via vertical transmission -that is, they pass their genetic material down directly from parent to offspring  2  .\nHowever, there are many cases where we cannot assume that evolution proceeds strictly vertically; instead, some form of horizontal gene transfer (HG T) occurs between two lineages  3  , resulting in what is known as reticulate evolution  4  . For example, if a virus infects a bacterium, then its genes will be passed horizontally across the host cell membrane  5  ; similarly, if a plant hybridizes with another plant  6  , then the offspring inherits genetic material from both parents  7, 8  . In these examples, the process of HGT results in a network-like relationship between the descendants of the original ancestor(s), rather than a strict hierarchy  9  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Mixed - up trees : the composition of phylogenetic mixtures . Abstract : We research the question of reconstructing evolutionary histories that incorporate both reticulation patterns and horizontal transmission , where reticulations are represented by hybridization or horizontal gene transfer ( HGT ) .We see how to use maximum parsimony to find an appropriate solution for this question in polynomial period when all input forests have bounded degree . This is achieved through a novel dynamic programming algorithm which computes the minimum amount of reticulation events required to explain each set of taxa on every tree in the input collection .Our idea can be used as part of a greater method for inferring species networks using collections of rooted binary phylogenies inferred from multiple types of data such as DNA sequences , morphological characters , fossil records etc . , and it may also provide insights into the evolution of complex organisms like viruses and bacteria . Introduction The reconstruction of evolutionary relationships among biological beings has been one of the central problems in biology since Darwin s On the Origin of Species was writing 150 years ago 1 .The most commonly accepted model of evolution holds that new organisms develop over time via vertical transmission - that is , they pass their genetic resources down directly from mother to offspring 2 . However , there are many situations where we cannot assume that development proceeds strictly horizontal ; merely , some kind of vertically gene transfer ( HG T ) occurs between two lineages 3 , resulting in what is known as reticulate evolution 4 .For instance , if a virus infects a organism , then its genes will be passed horizontally across the host cell membrane 5 ; likewise , if a plant hybridizes with another crop 6 , then the offspring inherits genetic material from both parents 7 , 8 . In these instance , the process of HGT results in a network - like association between the heirs of the original ancestor ( s ) , rather than a strict hierarchy 9 .",
        "rewrite_text": "**Title: Mixed-Up Trees: The Composition of Phylogenetic Mixtures**\n\n**Abstract:** This study investigates the reconstruction of evolutionary histories that integrate both reticulation patterns and horizontal gene transmission, specifically focusing on hybridization and horizontal gene transfer (HGT). We present a method utilizing maximum parsimony to effectively address this challenge within a polynomial time frame, provided that the input forests maintain a bounded degree. Our approach is facilitated by a novel dynamic programming algorithm designed to calculate the minimum number of reticulation events necessary to account for each set of taxa across the trees in the input collection. This methodology can be incorporated into a broader framework for inferring species networks, leveraging collections of rooted binary phylogenies derived from diverse data sources, including DNA sequences, morphological traits, and fossil records. Furthermore, our findings may offer valuable insights into the evolutionary dynamics of complex organisms, such as viruses and bacteria.\n\nThe reconstruction of evolutionary relationships has been a pivotal issue in biology since the publication of Darwin's *On the Origin of Species* over 150 years ago. Traditionally, the prevailing model of evolution has emphasized vertical transmission, where genetic material is passed directly from parent to offspring. However, numerous instances challenge this paradigm, revealing that evolution often involves horizontal gene transfer, leading to reticulate evolution. For example, when a virus infects a host organism, its genetic material can be transmitted horizontally across cellular membranes. Similarly, hybridization between plant species results in offspring that inherit genetic contributions from both parent plants. These scenarios illustrate that HGT creates a network-like structure among descendants, diverging from a strictly hierarchical lineage model. Our research aims to enhance the understanding of these complex evolutionary processes and their implications for the study of biodiversity and organismal development.",
        "ori-fast-z-score": -1.7089557634194348,
        "water-fast-z-score": 4.704421212792114,
        "rewrite-fast-z-score": 0.8835412617927487
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Sample eigenvalue based detection of high dimensional signals in white noise using relatively few samples .\nAbstract:\nWe consider the problem of detecting an unknown signal vector with a known covariance matrix, embedded in additive white Gaussian noise (AWGN). We propose to use sample eigenvalues as test statistics for this problem and derive their distributions under both hypotheses. The proposed method is shown to be robust against small perturbations on the covariance matrices. It also has low computational complexity compared to other existing methods. Finally we present simulation results that demonstrate its performance advantage over competing algorithms. In many applications such as radar systems or wireless communications, it is often desirable to detect whether there exists any signal component within noisy measurements. This problem can be formulated mathematically as: \nwhere x ∈ R N , s ∈ R M are respectively the signal and noise vectors; A ∈ R L×N denotes the sensing matrix which maps the signal space into the measurement space; y = Ax + n represents the observation vector where n ∼ N(0, σ 2 I) is AWGN. Herein, we assume that the number of observations L is much smaller than either N or M .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Sample eigenvalue based measurement of high dimensional signals in white sound using relatively few samples . Abstract : We consider the question of detecting an unknown signal vector with a known covariance matrix , embedded in additive white Gaussian sound ( AWGN ) .We suggest to use sample eigenvalues as test estimates for this question and derive their distributions under both hypotheses . The proposed approach is demonstrated to be robust against small perturbations on the covariance matrices .It additionally has low numerical capacity compared to other existing techniques . Finally we present modeling results that demonstrate its performance benefit over competing techniques .In many applications such as radar systems or wireless communications , it is often desirable to identify whether there exists any signal component within noisy measurements . This problem can be formulated mathematically as : where h ∈ R N , s ∈ R M are respectively the signal and noise vectors ; A ∈ R L×N denotes the sensing matrix which maps the signal space into the measurement space ; y = Ax + n represents the observation vector where n [UNK] N ( 0 , σ 2 I ) is AWGN .Herein , we suppose that the proportion of measurements L is much smaller than either N or M .",
        "rewrite_text": "Title: Sample Eigenvalue-Based Measurement of High-Dimensional Signals in White Noise Using Relatively Few Samples\n\nAbstract: This article addresses the challenge of detecting an unknown signal vector, characterized by a known covariance matrix, that is obscured by additive white Gaussian noise (AWGN). We propose a novel method that utilizes sample eigenvalues as test statistics to tackle this detection problem. By deriving the distributions of these sample eigenvalues under both the null and alternative hypotheses, we provide a statistical framework for signal detection in high-dimensional spaces. Our approach demonstrates remarkable resilience to minor perturbations in the covariance matrices, making it particularly advantageous in practical scenarios where such variations are common. Furthermore, the proposed method exhibits a lower numerical complexity compared to existing techniques, enhancing its applicability in real-time systems.\n\nThe significance of this work is underscored by its relevance to various fields, including radar systems and wireless communications, where the identification of signal components amidst noisy measurements is crucial. Mathematically, the problem can be formulated as follows: let h ∈ R^N and s ∈ R^M represent the signal and noise vectors, respectively, while A ∈ R^(L×N) denotes the sensing matrix that transforms the signal space into the measurement space. The observation vector is expressed as y = Ax + n, where n follows a normal distribution N(0, σ^2 I), representing the AWGN. Notably, we operate under the assumption that the number of measurements L is significantly smaller than both N and M, which is a common scenario in high-dimensional signal processing.\n\nThrough extensive modeling results, we illustrate the performance advantages of our method relative to competing techniques, highlighting its effectiveness in accurately detecting signals in challenging noise environments. This research contributes to the ongoing development of efficient signal detection methodologies in high-dimensional settings, paving the way for advancements in various technological applications.",
        "ori-fast-z-score": 1.0101525445522108,
        "water-fast-z-score": 5.728715546977509,
        "rewrite-fast-z-score": 2.5568369064112
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Composition of the Pseudoscalar Eta and Eta  Mesons .\nAbstract:\nWe present results on the composition of the eta-prime meson in terms of quarks and gluons, obtained by applying the QCD sum rule method to the correlation function of two pseudoscalars with different flavors.  We find that the eta prime is dominantly composed of strange quark pairs (ss) at low energies but becomes more nonstrange as energy increases. The mass difference between the eta and eta prime mesons can be explained if we assume that the eta prime contains an additional ss pair compared to the eta. This result supports our previous finding based on the analysis of the decay widths of these mesons into pions and kaons. \nPACS numbers: 11.10.Kk, 12.38.Gc, 13 .60.Hb  PACS number(s): 11.10.Kn, 12.38.Bx, 13 .60.Jz",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Composition of the Pseudoscalar Eta and Eta Mesons . Abstract : We report findings on the composition of the eta - prime meson in terms of quarks and gluons , obtained by using the QCD sum rule method to the correlation function of two pseudoscalars with varying flavors .We see that the eta prime is dominantly composed of odd quark pairs ( ss ) at low energies but gets more nonstrange as energy rises . The mass gap between the eta and eta prime mesons can be described if we suppose that the eta prime comprises an additional ss pair relative to the eta .This result confirms our previous finding based on the examination of the decay widths of these mesons into pions and kaons . PACS numbers : 11 . 10 . Kk , 12 . 38 . Gc , 13 . 60 . Hb PACS number ( s ) : 11 . 10 . Kn , 12 . 38 . Bx , 13 . 60 . Jz",
        "rewrite_text": "In this study, we present our findings regarding the composition of the eta-prime meson, analyzed through the framework of Quantum Chromodynamics (QCD) sum rules. By investigating the correlation function of two pseudoscalar mesons with different flavor combinations, we have determined the quark and gluon constituents of the eta-prime meson. Our results indicate that at lower energy levels, the eta-prime meson is primarily formed from strange quark pairs (ss). However, as the energy increases, there is a notable transition towards a more nonstrange composition. This observation suggests that the eta-prime meson contains an additional strange quark pair compared to the eta meson, which helps to explain the mass difference observed between these two particles. Our findings are consistent with previous analyses that examined the decay widths of the eta and eta-prime mesons into pions and kaons, reinforcing the validity of our conclusions. The implications of this research extend to a deeper understanding of meson structure and the interactions governed by QCD. The study is categorized under PACS numbers 11.10.Kk, 12.38.Gc, and 13.60.Hb, with additional classifications of 11.10.Kn, 12.38.Bx, and 13.60.Jz. Overall, our work contributes to the ongoing discourse in particle physics regarding the nature of mesons and their quark-gluon compositions, providing insights that may influence future experimental and theoretical investigations in the field.",
        "ori-fast-z-score": -1.414213562373095,
        "water-fast-z-score": 2.9405881764588204,
        "rewrite-fast-z-score": 0.42640143271122083
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Surveys of the Galactic Center and the Nature of the Galactic Center Lobe .\nAbstract:\nThe Galactic center (GC) is one of the most interesting regions in our Galaxy, but it has been difficult to study because of its proximity to the Sun. The GC contains many compact radio sources that are thought to be associated with young pulsars or magnetars. In this thesis we present results on two surveys at 1.4 GHz using the Australia Telescope Compact Array (ATCA). We have observed the central region of the Galaxy for about 100 hours over three epochs between 2005-2007. Our first survey covers an area of 2 degrees centered around Sgr A*. This survey was designed to search for new compact radio sources near the GC as well as to investigate the nature of the diffuse emission surrounding Sgr A*. Our second survey covered a larger area of 4 degrees centered around the GC. This survey was aimed at studying the distribution of molecular gas towards the GC by observing the 12 CO(J=1-0), 13 CO(J=1-0) and C 18 O(J=1-0) lines simultaneously.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Surveys of the Galactic Center and the Nature of the Galactic Center Lobe . Abstract : The Galactic center ( GC ) is one of the most exciting regions in our Galaxy , but it has been difficult to study because of its proximity to the Sun .The GC comprises many compact radio sources that are said to be identified with young pulsars or magnetars . In this dissertation we present results on two studies at 1 . 4 GHz use the Australia Telescope Compact Array ( ATCA ) .We have discovered the central region of the Galaxy for about 100 hours over three epochs between 2005 - 2007 . Our first survey encompasses an area of 2 degrees centered around Sgr A * .This study was built to search for additional compact radio sources near the GC as well as to examine the nature of the diffuse emission surrounding Sgr A * . Our second survey included a greater area of 4 degrees centered around the GC .This study was aiming at studying the distribution of molecular gas towards the GC by observing the 12 CO ( J = 1 - 0 ) , 13 CO ( J = 1 - 0 ) and C 18 O ( J = 1 - 0 ) tracks jointly .",
        "rewrite_text": "Title: Surveys of the Galactic Center and the Nature of the Galactic Center Lobe\n\nAbstract: The Galactic Center (GC) represents one of the most intriguing regions within our Milky Way galaxy, yet its proximity to the Sun has posed significant challenges for detailed study. This area is characterized by numerous compact radio sources, many of which are believed to be associated with young pulsars or magnetars. In this dissertation, we present findings from two comprehensive studies conducted at a frequency of 1.4 GHz using the Australia Telescope Compact Array (ATCA). Over the course of approximately 100 hours, we surveyed the central region of the Galaxy across three epochs from 2005 to 2007. The first survey focused on a 2-degree area centered around Sagittarius A* (Sgr A*), aiming to identify additional compact radio sources in proximity to the GC and to investigate the nature of the diffuse emission surrounding Sgr A*. Our second survey expanded the observational scope to a 4-degree area centered on the GC, with the objective of analyzing the distribution of molecular gas in this region. This was achieved through joint observations of the 12CO (J = 1-0), 13CO (J = 1-0), and C18O (J = 1-0) molecular lines. The results from these surveys provide valuable insights into the composition and dynamics of the GC, enhancing our understanding of its complex environment and the processes occurring within this dynamic region of our galaxy. Through these investigations, we aim to contribute to the broader knowledge of the Galactic Center's structure and its role in the evolution of the Milky Way.",
        "ori-fast-z-score": -0.6201736729460423,
        "water-fast-z-score": 4.431293675255978,
        "rewrite-fast-z-score": 1.6502739940140694
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Intrinsic Origin of Spin Echoes in Dipolar Solids Generated by Strong Pi Pulses .\nAbstract:\nSpin echoes are observed when the spin system is subjected to two successive radio-frequency (RF) pulses separated by an interval, known as the pulse separation time Tsep. The first RF pulse creates a macroscopic magnetization vector M0 that precesses around the external magnetic field Bext at Larmor frequency fL = γBext where γ is gyromagnetic ratio for nuclear spins. After the second RF pulse with flip angle θ2 and phase shift φ2 relative to the first one, the transverse component of the magnetization vector M2(t) decays exponentially due to dephasing caused by local magnetic fields created by neighboring nuclei. In this work we show how strong π-pulses can be used to generate spin echoes even if there is no net initial magnetization present before applying these pulses. We demonstrate theoretically and experimentally that such spin echoes originate intrinsically from the dipolar interactions between nuclear spins.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Intrinsic Origin of Spin Echoes in Dipolar Solids Generated by Strong Pi Pulses . Abstract : Spin signals are observed when the spin network is subjected to two subsequent radio - frequency ( RF ) bursts separated by an interval , known as the pulse splitting time Tsep .The first RF signal creates a macroscopic magnetization vector M0 that precesses around the external magnetic force Bext at Larmor frequency fL = γBext where γ is gyromagnetic ratio for nuclear spins . After the second RF signal with flip angle θ2 and phase shift φ2 relative to the first one , the transverse component of the magnetization vector M2 ( t ) decays exponentially due to dephasing caused by local magnetic fields produced by adjacent particles .In this research we prove how strong π - pulses can be used to create spin sounds even if there is no net initial magnetization present before applying these pulses . We suggest theoretically and experimentally that such spinning echoes originate intrinsically from the dipolar relationships between nuclear spins .",
        "rewrite_text": "**Title:** The Intrinsic Origin of Spin Echoes in Dipolar Solids Generated by Strong Pi Pulses\n\n**Abstract:** In this study, we investigate the phenomenon of spin echoes in dipolar solids, specifically focusing on the effects of strong π pulses on spin networks. Spin signals are generated when a network of spins is exposed to two consecutive radio-frequency (RF) bursts, separated by a defined interval known as the pulse splitting time (Tsep). The initial RF pulse induces a macroscopic magnetization vector, M0, which precesses around an external magnetic field, Bext, at the Larmor frequency (fL = γBext), where γ represents the gyromagnetic ratio for nuclear spins. Following this, a second RF pulse, characterized by a flip angle θ2 and a phase shift φ2 relative to the first pulse, leads to the transverse component of the magnetization vector, M2(t), experiencing exponential decay. This decay is attributed to dephasing effects arising from local magnetic fields generated by neighboring spins. \n\nOur research demonstrates that strong π pulses can effectively generate spin echoes, even in the absence of an initial net magnetization prior to the application of these pulses. Through both theoretical analysis and experimental validation, we propose that the intrinsic origin of these spin echoes is fundamentally linked to the dipolar interactions among nuclear spins within the solid. This work not only enhances the understanding of spin dynamics in dipolar solids but also opens avenues for potential applications in quantum information processing and magnetic resonance techniques. The findings underscore the significance of dipolar coupling in the generation of coherent spin states and provide insights into the manipulation of spin systems for advanced technological applications.",
        "ori-fast-z-score": -1.9629909152447274,
        "water-fast-z-score": 3.2118202741878643,
        "rewrite-fast-z-score": 0.6211495565912797
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Chi2 and chi3 harmonic generation at a critical power in inhomogeneous doubly resonant cavities .\nAbstract:\nWe study the second-harmonic (SH) and third-harmonic (TH) generations in an inhomogeneously broadened cavity with two different resonance frequencies for fundamental wave (FW). We show that, when the intracavity FW intensity reaches its maximum value, both SHG and THG can be enhanced simultaneously by increasing the pumping rate or decreasing the detuning between the two modes. The enhancement is due to the fact that the nonlinear susceptibility becomes larger than zero at this point. This effect has been observed experimentally recently. \n \n In addition, we find that there exists another regime where only one type of harmonics can be generated efficiently while suppressing other types of harmonics. For example, if the intracavity FW field is tuned close to the lower mode frequency, then it will generate mainly SH light but very little TH light; on the contrary, if the intracavty FW field is tuned near the upper mode frequency, then it generates mainly TH light but almost no SH light.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Chi2 and chi3 harmonic production at a critical power in inhomogeneous doubly resonant cavities . Abstract : We research the second - harmonic ( SH ) and third - harmonic ( TH ) generations in an inhomogeneously broadened cavity with two different resonance energies for fundamental wave ( FW ) .We see that , when the intracavity FW frequency exceeds its highest value , both SHG and THG can be enhanced simultaneously by expanding the pumping rate or decreasing the detuning between the two modes . The enhancement is due to the fact that the nonlinear susceptibility grows larger than zero at this time .This phenomenon has been observed experimentally recently . In addition , we find that there exists another regime where only one sort of harmonics can be generated smoothly while suppressing other types of harmonics .For instance , if the intracavity FW field is tuned far to the lower mode wavelength , then it will generate mostly SH light but very less TH radiation ; on the contrary , if the intracavty FW field is tuned near the higher mode wavelength , then it generates largely TH light but almost no SH light .",
        "rewrite_text": "In this study, we investigate the generation of second harmonic (SH) and third harmonic (TH) waves within an inhomogeneously broadened cavity that possesses two distinct resonance energies for the fundamental wave (FW). Our findings reveal that when the intracavity frequency of the FW surpasses its maximum threshold, both SH and TH generation can be significantly enhanced. This enhancement can be achieved by either increasing the pumping rate or reducing the detuning between the two modes. The underlying reason for this enhancement lies in the nonlinear susceptibility, which becomes greater than zero under these conditions. This phenomenon has been corroborated by recent experimental observations. Furthermore, we identify an additional operational regime in which the generation of one type of harmonic can occur smoothly while effectively suppressing the generation of others. For example, when the intracavity FW field is adjusted to a wavelength significantly lower than the mode, it predominantly produces SH light with minimal TH radiation. Conversely, when the FW field is tuned closer to the higher mode wavelength, it primarily generates TH light while virtually eliminating SH light production. These insights into the harmonic generation dynamics in inhomogeneous doubly resonant cavities contribute to a deeper understanding of nonlinear optical processes and may have implications for the design of advanced photonic devices.",
        "ori-fast-z-score": -1.8599622199011085,
        "water-fast-z-score": 3.5795716689756794,
        "rewrite-fast-z-score": 1.6681153124565982
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Alignments of the Galaxy Spins with the Real-Space Tidal Field Reconstructed from the Two Mass Redshift Survey .\nAbstract:\nWe present an analysis of the alignments between galaxy spins and tidal fields in real space, using data from the Two Mass Redshfit Survey (TMRS). We find that galaxies are preferentially aligned perpendicular to their local tidal field on scales larger than 1 Mpc/h. This alignment is stronger for more massive galaxies at higher redshifts. The observed spin-tide correlation can be explained by the effect of gravitational torques exerted by large-scale structures during the formation process of these galaxies. Our results suggest that this mechanism may play an important role in shaping galactic angular momenta. These findings have implications for understanding how dark matter halos acquire their angular momentum as well as for interpreting observations of cosmic shear statistics. Introduction: Galaxies form within overdense regions of the universe where they experience strong gravitational interactions with other objects such as neighboring galaxies or clusters of galaxies. During the formation process, these interactions induce gravitational torques which affect the orientation of the galactic angular momentum vector. In turn, the orientations of galactic angular momenta determine the shapes of galaxies through dynamical friction processes. Therefore, it has been suggested that the shape distribution of galaxies could provide information about the origin of galactic angular momentums (e.g., Catelan & Theuns 1996; Lee et al. 2008) . However, observational studies show conflicting results regarding whether there exists any preferred direction of galaxy spin axes relative to their neighbors  positions (see e.g., Faltenbacher et al. 2002; Bailin et al. 2005; Paz et al. 2008; Codis et al. 2012 , for recent works).\nIn order to understand the physical mechanisms responsible for determining the directions of galactic angular momentas, we need to study the statistical properties of galaxy spin distributions over large volumes of the universe. Recent surveys like Sloan Digital Sky Survey (SDSS) allow us to measure galaxy orientations accurately enough to perform such analyses. For example, Lee et al. (2008) used SDSS DR4 data to investigate the alignments between galaxy spin vectors and their nearest neighbor s position angles. They found no",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Alignments of the Galaxy Spins with the Real - Space Tidal Field Reconstructed from the Two Mass Redshift Survey . Abstract : We present an assessment of the alignments between galaxy twists and tidal fields in real space , using data from the Two Mass Redshfit Survey ( TMRS ) .We see that galaxies are preferentially aligned perpendicular to their nearby tidal field on scales bigger than 1 Mpc / h . This alignment is strengthened for more massive galaxies at higher redshifts .The observed spinning - tide coupling can be described by the impact of gravitational torques exerted by large - scale structures during the formation period of these galaxies . Our results show that this mechanism may play an important role in shaping galactic angular momenta .These studies have consequences for studying how dark matter halos acquire their angular velocity as also as for interpreting observations of universe shear statistics . Introduction : Galaxies shape within overdense regions of the universe where they encounter strong gravitational interactions with other structures such as nearby galaxies or rows of stars .During the formation step , these interactions initiate gravitational torques which affect the orientation of the galactic angular velocity tensor . In turn , the orientations of galactic angular momenta determine the shapes of galaxies through dynamical friction mechanisms .Therefore , it has been proposed that the form distribution of galaxies possible provide information about the origin of galactic angular momentums ( e . g . , Catelan & Theuns 1996 ; Lee et al . 2008 ) .However , observational analyses suggest conflicting findings regarding whether there exists any preferred direction of galaxy spin axes relative to their neighbors positions ( saw e . g . , Faltenbacher et al . 2002 ; Bailin et al .2005 ; Paz et al . 2008 ; Codis et al .2012 , for recent works ) . In order to comprehend the physical mechanisms involved for determining the directions of galactic angular momentas , we require to study the statistical characteristics of galaxy spin distributions over large quantities of the universe .Recent surveys like Sloan Digital Sky Survey ( SDSS ) able us to measure galaxy orientations correctly sufficiently to conduct such analyses . For instance , Lee et al .( 2008 ) used SDSS DR4 data to examine the alignments between galaxy spin vectors and their closest neighbor s position angles . They found no",
        "rewrite_text": "**Title:** The Alignment of Galaxy Spins with the Real-Space Tidal Field Reconstructed from the Two Mass Redshift Survey\n\n**Abstract:** In this study, we investigate the relationship between the orientations of galaxy spins and the tidal fields in real space, utilizing data from the Two Mass Redshift Survey (TMRS). Our findings indicate that galaxies tend to align perpendicularly to the tidal fields in their vicinity, particularly on scales exceeding 1 Mpc/h. This alignment effect is notably more pronounced in more massive galaxies and at higher redshifts. We attribute this observed coupling between galaxy spins and tidal forces to the gravitational torques exerted by large-scale structures during the galaxies' formation epochs. Our results suggest that these gravitational interactions play a significant role in determining the angular momentum of galaxies. This research has important implications for understanding how dark matter halos gain angular velocity and for interpreting the shear statistics observed in the universe.\n\n**Introduction:** Galaxies typically form in regions of the universe that are overdense, where they experience strong gravitational interactions with neighboring structures, such as other galaxies or clusters of stars. During their formation, these interactions generate gravitational torques that influence the orientation of the galactic angular momentum tensor. The alignment of these angular momenta subsequently affects the morphological characteristics of galaxies through mechanisms like dynamical friction. Consequently, the distribution of galaxy shapes may provide insights into the origins of their angular momentum (e.g., Catelan & Theuns 1996; Lee et al. 2008). However, observational studies have yielded mixed results regarding the existence of a preferred orientation for galaxy spin axes in relation to the positions of neighboring galaxies (see, for example, Faltenbacher et al. 2002; Bailin et al. 2005; Paz et al. 2008; Codis et al. 2012 for recent investigations). To better understand the physical processes that dictate the directions of galactic angular momenta, it is essential to analyze the statistical properties of galaxy spin distributions across extensive regions of the universe. Recent surveys, such as the Sloan Digital Sky Survey (SDSS), have enabled precise measurements of galaxy orientations, facilitating such analyses. For instance, Lee et al. (2008) utilized SDSS DR4 data to explore the alignments between galaxy spin vectors and the position angles of their nearest neighbors, revealing a lack of significant correlation.",
        "ori-fast-z-score": -1.2924860661584994,
        "water-fast-z-score": 6.519643174778569,
        "rewrite-fast-z-score": 1.0552897060221726
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  How to Find More Supernovae with Less Work: Object Classification Techniques for Difference Imaging .\nAbstract:\nWe present an automated method that uses machine learning techniques to classify supernova candidates in difference imaging data, and apply it to the Dark Energy Survey (DES). The DES is a five-year survey using a 1.2 deg2 camera on the 4-meter Blanco telescope at Cerro Tololo Inter-American Observatory (CTIO) in Chile. We use two different classification algorithms - Random Forests and Support Vector Machines - trained on simulated images generated by injecting artificial point sources into real DES images. Our results show that we can achieve high efficiency (>90%), purity (>80%), and completeness (>95%) when classifying objects as either stars or galaxies down to iAB = 24 mag. This enables us to find more supernovae per unit time than previous methods while reducing human biases associated with visual inspection. In addition, our technique allows us to identify new classes of transients such as tidal disruption events and microlensing flares.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : How to Find More Supernovae with Less Work : Object Classification Techniques for Difference Imaging . Abstract : We create an automated process that using computer learning techniques to classify supernova prospects in difference imaging information , and application it to the Dark Energy Survey ( DES ) .The DES is a five - year survey employing a 1 . 2 deg2 sensor on the 4 - meter Blanco telescope at Cerro Tololo Inter - American Observatory ( CTIO ) in Chile . We use two different classification algorithms - Random Forests and Support Vector Machines - trained on simulated images produced by injecting artificial point sources into real DES images .Our results show that we can attain high efficiency ( > 90 % ) , purity ( > 80 % ) , and completeness ( > 95 % ) when classifying objects as either stars or galaxies down to iAB = 24 mag . This enables us to find more supernovae per unit time than prior methods while reducing human biases identified with visual inspection .In addition , our technique lets us to identify new classes of transients such as tidal disruption events and microlensing flares .",
        "rewrite_text": "In this study, we present an innovative automated methodology that leverages machine learning techniques to enhance the classification of supernova candidates within difference imaging data, specifically applied to the Dark Energy Survey (DES). The DES is a comprehensive five-year observational project utilizing a 1.2 square degree camera mounted on the 4-meter Blanco telescope located at the Cerro Tololo Inter-American Observatory (CTIO) in Chile. Our approach employs two distinct classification algorithms—Random Forests and Support Vector Machines—both of which are trained on simulated datasets generated by embedding artificial point sources into actual DES images. \n\nThe results of our analysis demonstrate that our classification system achieves remarkable performance metrics, including efficiency exceeding 90%, purity greater than 80%, and completeness above 95% when distinguishing between stars and galaxies down to an iAB magnitude of 24. This advancement allows for the identification of a greater number of supernovae within a given timeframe compared to previous methodologies, while simultaneously mitigating the human biases that often accompany manual visual inspections. Furthermore, our technique not only improves the detection of supernovae but also facilitates the discovery of new categories of transient astronomical events, such as tidal disruption events and microlensing flares. This work represents a significant step forward in the field of astrophysics, providing a robust framework for the automated classification of astronomical objects and enhancing our ability to explore the dynamic universe.",
        "ori-fast-z-score": 0.7171371656006361,
        "water-fast-z-score": 4.628448466956028,
        "rewrite-fast-z-score": 0.4975185951049946
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The luminous infrared composite Seyfert 2 galaxy NGC 7679 through the  O III  5007 emission line .\nAbstract:\nWe present new spectroscopic observations in the near-infrared (NIR) range for the brightest member of the interacting pair of galaxies, NGC 7679/NGC 7682. The NIR spectrum shows strong narrow forbidden lines and permitted lines with P Cygni profiles characteristic to AGNs. We have detected blueshifted absorption components on both sides of Hα and  N II  6584Å lines which are likely due to outflows driven by nuclear winds. In addition we found that the observed flux ratio between the two strongest optical emission lines is higher than expected value based on photoionization models. This indicates that there may be an additional source of ionization besides the central engine such as shocks or collisional heating. Our results suggest that this object could be classified as a composite Seyfert 2 galaxy where the contribution of starburst activity to the total luminosity is significant. Keywords: Near-infrared spectroscopy, Star formation rate",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The luminous infrared composite Seyfert 2 galaxy NGC 7679 through the O III 5007 emission line . Abstract : We report new spectroscopic observations in the near - infrared ( NIR ) range for the brightest member of the interacting pair of galaxies , NGC 7679 / NGC 7682 .The NIR spectrum displays strong narrow forbidden bands and permitted lines with P Cygni profiles characteristic to AGNs . We have discovered blueshifted absorption elements on both sides of Hα and N II 6584Å lines which are likely due to outflows driven by nuclear winds .In addition we concluded that the seen flux ratio between the two strongest visual emission lines is higher than expected value according on photoionization estimates . This implies that there may be an additional source of ionization besides the main engine such as shocks or collisional heating .Our results propose that this body might be categorized as a composite Seyfert 2 galaxy where the impact of starburst activity to the total luminosity is substantial . Keywords : Near - infrared spectroscopy , Star formation rate",
        "rewrite_text": "Title: The Luminous Infrared Composite Seyfert 2 Galaxy NGC 7679 Through the O III 5007 Emission Line\n\nAbstract: In this study, we present new near-infrared (NIR) spectroscopic observations of NGC 7679, the most luminous member of the interacting galaxy pair NGC 7679/NGC 7682. Our analysis reveals prominent narrow forbidden emission lines and permitted lines exhibiting P Cygni profiles, which are characteristic features of active galactic nuclei (AGNs). Notably, we have identified blueshifted absorption features flanking the Hα and N II 6584 Å lines, suggesting the presence of outflows likely driven by nuclear winds. Furthermore, we observed that the flux ratio between the two strongest optical emission lines exceeds the expected values derived from photoionization models. This discrepancy indicates the potential influence of additional ionization mechanisms, such as shocks or collisional heating, beyond the primary AGN activity. Our findings support the classification of NGC 7679 as a composite Seyfert 2 galaxy, where the contribution of starburst activity significantly enhances the overall luminosity. This research underscores the complexity of the ionization processes in such galaxies and highlights the importance of considering multiple sources of energy input when interpreting their spectral characteristics. \n\nKeywords: Near-infrared spectroscopy, Star formation rate.",
        "ori-fast-z-score": 0.4588314677411235,
        "water-fast-z-score": 4.672383634845156,
        "rewrite-fast-z-score": 0.8944271909999159
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Modifying quantum walks: A scattering theory approach .\nAbstract:\nWe present an alternative method to the usual Feynman path integral description for calculating the probability amplitudes in quantum walk models, based on the concept of scattering states and their associated S-matrix elements. We show that this new formalism allows us to obtain exact results for several interesting cases where standard methods fail or are not applicable. In particular we consider two different types of boundary conditions at one end of the chain (the origin) which lead to completely different behaviour of the system as time evolves. The first type is known as Dirichlet boundary condition, corresponding to reflecting particles back into the origin after they have left it once; while the second case corresponds to absorbing particles when they reach the origin. For both these cases we calculate exactly the evolution operator over all times t > 0 using our new method. Finally, by applying the inverse Fourier transform to the evolution operator we can recover the full probability distribution function of finding the walker at any position x along the chain at time t.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Modifying quantum walks : A scattering theory approach . Abstract : We introduce an additional method to the usual Feynman path integral description for determining the probability amplitudes in particle walk models , using on the idea of scattering states and their accompanying S - vector elements .We see that this new formalism allows us to obtain exact findings for numerous interesting cases where standard methods fail or are not applicable . In particular we treat two different kinds of boundary constraints at one end of the chain ( the origin ) which lead to totally distinct behaviour of the system as time evolves .The first sort is known as Dirichlet boundary relation , analogous to reflecting molecules back into the origin after they have left it once ; while the second case corresponds to absorbing particles when they reach the origin . For both these cases we determine precisely the evolution function over all times t > 0 using our new method .Finally , by using the inverse Fourier transform to the evolution function we can regain the full probability distribution function of finding the walker at any point x along the chain at time t .",
        "rewrite_text": "In this article, titled \"Modifying Quantum Walks: A Scattering Theory Approach,\" we present a novel method that enhances the traditional Feynman path integral framework for calculating probability amplitudes in particle walk models. Our approach is grounded in the concept of scattering states and their associated S-vector elements, which enables us to derive precise results in various scenarios where conventional techniques may fall short or be inapplicable. \n\nWe specifically investigate two distinct types of boundary conditions imposed at one end of the chain, referred to as the origin, which result in markedly different dynamical behaviors as time progresses. The first boundary condition, known as the Dirichlet boundary condition, functions similarly to reflecting particles back to the origin after they have exited. In contrast, the second condition involves the absorption of particles upon their arrival at the origin. \n\nFor both boundary scenarios, we successfully compute the evolution function for all times \\( t > 0 \\) utilizing our innovative methodology. This allows us to capture the intricate dynamics of the quantum walk under these constraints. Furthermore, by applying the inverse Fourier transform to the evolution function, we can reconstruct the complete probability distribution function that describes the likelihood of locating the walker at any position \\( x \\) along the chain at a given time \\( t \\). \n\nOur findings not only deepen the understanding of quantum walks but also highlight the versatility of scattering theory in addressing complex problems in quantum mechanics. This work opens new avenues for exploring quantum systems with boundary interactions, providing a robust framework for future research in this area.",
        "ori-fast-z-score": 1.0945409092309881,
        "water-fast-z-score": 6.010508596802189,
        "rewrite-fast-z-score": 0.6163156344279367
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Zakharov simulation study of spectral features of on-demand Langmuir turbulence in an inhomogeneous plasma .\nAbstract:\nWe present results of numerical simulations of the nonlinear evolution of electrostatic waves excited by a localized source in a magnetized, collisionless plasma with nonuniform density and temperature profiles. The initial conditions are chosen to be close to those observed during experiments at the Large Plasma Device (LAPD) facility at UCLA. We find that the wave spectrum is dominated by two distinct branches corresponding to different types of waves: slow modes which propagate along magnetic field lines and fast modes which have oblique propagation angles relative to B. In addition we observe a number of other interesting phenomena such as generation of coherent structures, formation of shock fronts, etc., all of which can be explained within the framework of Zakharov s theory for Langmuir turbulence. \n \n Keywords: Nonlinear dynamics; Turbulence; Wave-particle interactions; Collisionless plasmas; LAPD; Fast-slow mode coupling; Waves",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Zakharov model study of spectral features of on - demand Langmuir turbulence in an inhomogeneous plasma . Abstract : We report findings of computational simulations of the nonlinear progression of electrostatic rays excited by a localized source in a magnetized , collisionless gas with nonuniform density and heat profiles .The initial conditions are chosen to be close to those observed during experiments at the Large Plasma Device ( LAPD ) laboratory at UCLA . We see that the wave spectrum is dominated by two different branches corresponding to different kinds of waves : slow modes which propagate along magnetic field lines and fast modes which have oblique propagation distances relative to B .In addition we study a number of other remarkable phenomena such as development of coherent complexes , creating of shock fronts , etc . , all of which can be described within the framework of Zakharov s concept for Langmuir turbulence . Keywords : Nonlinear dynamics ; Turbulence ; Wave - particle interactions ; Collisionless plasmas ; LAPD ; Fast - fast mode interaction ; Waves",
        "rewrite_text": "Title: Zakharov Model Investigation of Spectral Characteristics of On-Demand Langmuir Turbulence in Inhomogeneous Plasma\n\nAbstract: This study presents the results of computational simulations examining the nonlinear evolution of electrostatic waves generated by a localized source within a magnetized, collisionless plasma characterized by varying density and thermal profiles. The initial conditions for our simulations closely mirror those observed in experimental settings at the Large Plasma Device (LAPD) at UCLA. Our findings reveal that the wave spectrum is primarily influenced by two distinct branches, each corresponding to different wave types: slow modes that propagate along the magnetic field lines and fast modes that exhibit oblique propagation relative to the magnetic field direction. Furthermore, we investigate several intriguing phenomena associated with this system, including the formation of coherent structures and the emergence of shock fronts. These phenomena can be effectively analyzed within the framework of Zakharov's theory of Langmuir turbulence. Our results contribute to a deeper understanding of the complex dynamics present in collisionless plasmas and highlight the intricate interactions between waves and particles in such environments. The implications of these findings extend to various applications in plasma physics and astrophysical contexts, where understanding turbulence and wave behavior is crucial. \n\nKeywords: Nonlinear dynamics; Turbulence; Wave-particle interactions; Collisionless plasmas; LAPD; Fast-fast mode interaction; Waves.",
        "ori-fast-z-score": -1.5215349135496974,
        "water-fast-z-score": 3.9524197172898554,
        "rewrite-fast-z-score": 2.914609664251715
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Thin elastic shells with variable thickness for lithospheric flexure of one-plate planets .\nAbstract:\nWe present an analytical solution to the problem of bending thin elastic shells with variable thickness under surface loads, which is applicable to the case where the shell s thickness varies by several orders of magnitude over its radius and where the load distribution is not necessarily axisymmetric.  We show that in this case it is possible to obtain accurate results using only two parameters instead of three as was previously thought necessary (the third parameter being the ratio between the maximum and minimum values of the shell s thickness). The new formulation allows us to calculate the deflection of the shell at any point on its surface without having to solve additional equations or perform numerical integration. This makes our approach much faster than previous methods while retaining high accuracy. Our method can be used to model the response of the Earth s crust to tectonic stresses and other processes such as volcanic loading and sedimentary deposition. It also has applications in geophysics beyond Earth sciences including planetary science, astrophysics and seismology. \nTheory\n\nIn order to study the deformation of the Earth s crust we need to know how the stress field changes across different regions of the planet. In particular, we are interested in understanding how the stress field evolves during plate boundary interactions like subduction zones and transform faults. To do so, we use the theory of elasticity to find solutions to problems involving the interaction between plates and their underlying mantle. However, solving these problems analytically requires simplifying assumptions about the geometry of the system and the mechanical properties of the materials involved. \n\nOne important simplification made when studying the mechanics of plate boundaries is to assume that they behave as if they were composed of thin elastic shells. These shells have been shown to provide good approximations to more realistic models of plate boundaries because they allow for rapid calculations of the stress fields within them. For example, Figure 1 shows a comparison between the predictions obtained using a simple spherical shell model and those produced by a finite element model of the San Andreas Fault System.\n\nFigure 1: Comparison between the predicted displacements along the San Andreas fault calculated using a spherical shell model (blue line) and a finite element model (red dots).",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Thin elastic pieces with variable thickness for lithospheric flexure of one - plate planets . Abstract : We present an analytical solution to the issue of twisting narrow elastic shells with variable size under surface loads , which is applicable to the case where the shell s thickness differs by many orders of magnitude over its radius and where the load distribution is not necessarily axisymmetric .We see that in this situation it is easy to obtain precise conclusions using only two parameters instead of three as was formerly thought required ( the third parameter being the proportion between the maximum and minimum values of the shell s thickness ) . The revised formulation enables us to estimate the deflection of the shell at any point on its surface without having to correct additional equations or undergo numerical expansion .This gives our approach much quick than prior methods while retaining high sensitivity . Our model can be used to model the response of the Earth s crust to tectonic stresses and other processes such as eruption loading and sedimentary deposition .It especially has uses in geophysics beyond Earth studies particularly planetary astronomy , astrophysics and seismology . Theory In order to study the deformation of the Earth s crust we require to see how the strain field shifts across different regions of the planet .In particular , we are concerned in understanding how the strain field evolves during plate boundary interactions like subduction zones and transform faults . To do so , we using the principle of elasticity to find solutions to problems concerning the interaction between plates and their underlying mantle .However , exploring these problems analytically takes simplifying theories about the topology of the system and the structural properties of the materials involved . One important simplification taken when researching the mechanics of plate boundaries is to assume that they react as if they were consisting of short elastic shells .These shells have been shown to provide better approximations to more realistic theories of plate boundaries because they allow for rapid calculations of the strain fields within them . For instance , Figure 1 shows a comparison between the estimates obtained using a simple spherical shell model and those generated by a finite element model of the San Andreas Fault System .Figure 1 : Comparison between the expected displacements along the San Andreas fault calculated using a spherical shell model ( blue line ) and a finite element model ( red dots ) .",
        "rewrite_text": "**Title:** Thin Elastic Shells with Variable Thickness for Lithospheric Flexure in One-Plate Planets\n\n**Abstract:** This article presents a novel analytical solution addressing the deformation of narrow elastic shells with variable thickness under surface loads, particularly relevant for scenarios where the shell's thickness varies significantly across its radius and the load distribution may not be axisymmetric. Our findings indicate that it is possible to derive accurate results using only two parameters, rather than the previously assumed three, which included the ratio of maximum to minimum shell thickness. This simplification allows for the direct estimation of deflection at any point on the shell's surface, eliminating the need for additional corrections or complex numerical methods. Consequently, our approach is not only more efficient than traditional methods but also maintains a high degree of sensitivity.\n\nThe implications of this model extend to the analysis of the Earth's crust response to tectonic stresses, as well as other geological processes such as volcanic loading and sediment deposition. Its applications are particularly significant in geophysics, with potential insights for planetary astronomy, astrophysics, and seismology. \n\nTo investigate the deformation of the Earth's crust, it is essential to understand how the strain field varies across different regions. Our focus is on the evolution of this strain field during interactions at plate boundaries, such as subduction zones and transform faults. By applying the principles of elasticity, we derive solutions to the complex interactions between tectonic plates and their underlying mantle. However, analytical exploration of these interactions necessitates certain simplifications regarding the system's topology and the structural characteristics of the materials involved. \n\nOne key simplification involves treating plate boundaries as short elastic shells, which have been shown to yield more accurate approximations compared to more complex models. This approach facilitates rapid calculations of strain fields, as illustrated in our comparative analysis of displacement estimates along the San Andreas Fault System, where results from a simple spherical shell model are juxtaposed with those from a finite element model. This comparison underscores the effectiveness of our method in capturing the essential mechanics of plate boundary interactions.",
        "ori-fast-z-score": -1.6642215921725698,
        "water-fast-z-score": 7.2701390825618155,
        "rewrite-fast-z-score": 0.7761505257063328
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Magnetic structure of Sm2IrIn8 .\nAbstract:\nWe have performed neutron powder diffraction experiments on the intermetallic compound Sm2IrIn8 in order to determine its magnetic structure and compare it with that proposed for YbMgGaO4, another member of this family of compounds. The results show that Sm2IrIn8 has an antiferromagnetic ordering at TN = 3.5 K with moments aligned along the c-axis. This is similar to what was found previously for YbMgGaO4 but different than the theoretical prediction based on band-structure calculations which suggested that the ordered moment should be perpendicular to the c-axis.  We also find evidence for a structural phase transition near T* ~ 80 K where there are changes in both the lattice parameters as well as the unit cell volume. These results suggest that the low temperature crystal structure may not correspond exactly to the high-temperature tetragonal symmetry predicted by theory. Finally we present specific heat data showing clear anomalies associated with both the magnetic ordering and the structural phase transition.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Magnetic structure of Sm2IrIn8 . Abstract : We have done neutron dust diffraction experiments on the intermetallic complex Sm2IrIn8 in order to identify its magnetic shape and compare it with that suggested for YbMgGaO4 , another member of this class of compounds .The results show that Sm2IrIn8 has an antiferromagnetic ordering at TN = 3 . 5 K with moments aligned along the c - axis . This is related to what was seen previously for YbMgGaO4 but different than the theoretical forecast based on band - structure analysis which suggested that the ordered moment should be perpendicular to the c - axis .We additionally find proof for a structural phase shift near T * ~ 80 K where there are variations in both the lattice parameters as well as the unit cell size . These conclusions show that the high heat crystal composition may not correspond exactly to the high - temperature tetragonal symmetry anticipated by theory .Finally we present specific heat statistics suggesting obvious anomalies associated with both the magnetic forcing and the structural phase shift .",
        "rewrite_text": "Title: Magnetic Structure of Sm2IrIn8\n\nAbstract: In this study, we conducted neutron dust diffraction experiments on the intermetallic compound Sm2IrIn8 to elucidate its magnetic structure and to draw comparisons with YbMgGaO4, another compound within the same family. Our findings reveal that Sm2IrIn8 exhibits antiferromagnetic ordering at a Neel temperature (TN) of 3.5 K, with magnetic moments oriented along the c-axis. This observation aligns with previous studies on YbMgGaO4; however, it diverges from theoretical predictions derived from band-structure analyses, which anticipated that the ordered magnetic moment would be oriented perpendicular to the c-axis. Furthermore, we observed evidence of a structural phase transition occurring around T* ~ 80 K, characterized by changes in both the lattice parameters and the dimensions of the unit cell. These results indicate that the high-temperature crystal structure may not conform precisely to the tetragonal symmetry that theoretical models have suggested. Additionally, we present specific heat measurements that reveal significant anomalies linked to both the magnetic ordering and the structural phase transition. These findings contribute to a deeper understanding of the magnetic and structural properties of Sm2IrIn8 and highlight the complexities involved in predicting the behavior of intermetallic compounds.",
        "ori-fast-z-score": -1.3093073414159544,
        "water-fast-z-score": 5.607304206578798,
        "rewrite-fast-z-score": 0.6255432421712244
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The electronic structures, the equilibrium geometries and finite temperature properties of Na_n (n=39-55) .\nAbstract:\nThe electronic structure, the equilibrium geometry and the finite-temperature properties are investigated for sodium clusters with 39 to 55 atoms by using density functional theory within generalized gradient approximation. The results show that the ground state is always in C_sv symmetry except for Na_55 which has D_5h symmetry as its lowest energy configuration. For all these clusters, we find that there exists an odd-even effect on their structural parameters such as bond length, bond angle and dihedral angle. We also investigate the vibrational frequencies and infrared intensities of these clusters at room temperature. It turns out that the calculated values agree well with those obtained experimentally. Finally, we study the thermodynamic stability of these clusters. Our calculations indicate that most of them have high melting temperatures but some of them may be unstable against dissociation into smaller fragments. \n \n Keywords: Sodium cluster, Electronic structure, Vibrational frequency, Infrared intensity",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The electronic complexes , the equilibrium geometries and finite temperature characteristics of Na _ n ( n = 39 - 55 ) . Abstract : The electronic structure , the equilibrium geometry and the finite - temperature characteristics are examined for sodium clusters with 39 to 55 atoms by using density functional theory within generalized gradient approximation .The results show that the ground state is usually in C _ sv symmetry except for Na _ 55 which has D _ 5h symmetry as its lowest energy configuration . For all these complexes , we find that there exists an odd - even effect on their structural values such as bond length , bond angle and dihedral angle .We additionally probe the vibrational speeds and infrared intensities of these complexes at room temperature . It turns out that the derived values comply good with those achieved experimentally .Finally , we study the thermodynamic stability of these complexes . Our calculations suggest that most of them have high cooling conditions but some of them may be unstable against dissociation into tiny pieces .Keywords : Sodium cluster , Electronic structure , Vibrational intensity , Infrared intensity",
        "rewrite_text": "**Title:** The Electronic Complexes, Equilibrium Geometries, and Finite Temperature Characteristics of Na_n (n = 39 - 55)\n\n**Abstract:** This study investigates the electronic structure, equilibrium geometries, and finite temperature properties of sodium clusters containing between 39 and 55 atoms, employing density functional theory (DFT) within the generalized gradient approximation (GGA). Our findings indicate that the ground state configurations predominantly exhibit C_sv symmetry, with the notable exception of Na_55, which adopts a D_5h symmetry as its lowest energy state. A significant odd-even effect is observed in the structural parameters of these complexes, including bond lengths, bond angles, and dihedral angles, highlighting the influence of cluster size on their geometric properties. Furthermore, we analyze the vibrational frequencies and infrared intensities of these sodium clusters at room temperature, revealing that the computed values align well with experimental data, thereby validating our theoretical approach. In addition to structural and vibrational characteristics, we assess the thermodynamic stability of the clusters, concluding that while most exhibit favorable cooling conditions, a subset may be prone to dissociation into smaller fragments. This research contributes to the understanding of sodium cluster behavior and stability, providing insights that could inform future studies in nanomaterials and cluster chemistry. \n\n**Keywords:** Sodium cluster, Electronic structure, Vibrational intensity, Infrared intensity",
        "ori-fast-z-score": -1.091089451179962,
        "water-fast-z-score": 3.2349831961031525,
        "rewrite-fast-z-score": 0.09759000729485333
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  High energy afterglows and flares from Gamma-Ray Burst by Inverse Compton emission .\nAbstract:\nWe propose that the high energy (HE) component in GRB spectra is produced via inverse Compton scattering between relativistic electrons accelerated at internal shocks, and soft photons emitted during the prompt phase.  We show that this model can explain both the HE spectral shape as well as its temporal evolution for several bursts observed with Fermi/LAT. The predicted light curves are consistent with those measured by Swift/BAT and Fermi/GBM. This scenario also naturally explains why some bursts have no detectable HE emission despite having bright optical flashes. Finally we discuss how our results could be used to constrain the physical parameters of GRBs such as their bulk Lorentz factor or magnetic field strength. High Energy Emission from Gamma Ray Bursts: A Proposal for an Inverse Compton Scattering Model  By J. Piran & M. Lazzati \n \n Abstract \n \n We propose that the high energy component in GRB spectra may be produced via inverse Compton scatterings between relativistic electrons accelerated within internal shocks, and soft photon fields emitted during the prompt phase. We show that this model predicts correctly the spectral shapes and time evolutions of many bursts detected by Fermi LAT. It also provides a natural explanation for the lack of detection of HE emission in some bursts which exhibit bright optical flashes. Our results suggest that future observations will allow us to determine important properties of GRBs like their bulk Lorentz factors or magnetic field strengths.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : High energy afterglows and flares from Gamma - Ray Burst by Inverse Compton emission . Abstract : We suggest that the high energy ( HE ) element in GRB spectra is produced via inverse Compton absorption between relativistic electrons accelerated at internal shocks , and dark photons generated during the prompt phase .We see that this model can describe both the HE spectral structure as also as its temporal evolution for various bursts observed with Fermi / LAT . The predicted light angles are compatible with those observed by Swift / BAT and Fermi / GBM .This scenario additionally naturally reveals why some flashes have no detectable HE emitted despite having bright optical bursts . Finally we talk how our findings may be used to constrain the physical values of GRBs such as their bulk Lorentz factor or magnetic field intensity .High Energy Emission from Gamma Ray Bursts : A Proposal for an Inverse Compton Scattering Model By J . Piran & M . Lazzati Abstract We suggest that the high energy component in GRB spectra might be formed via inverse Compton scatterings between relativistic electrons accelerated within internal shocks , and soft photon fields emitted during the prompt phase . We see that this description predicts correctly the spectral patterns and period evolutions of several bursts detected by Fermi LAT .It additionally offers a natural explanation for the lack of recognition of HE emission in some bursts which exhibit bright optical bursts . Our results show that future discoveries will provide us to identify important characteristics of GRBs like their bulk Lorentz factors or magnetic force abilities .",
        "rewrite_text": "**Title:** High Energy Afterglows and Flares from Gamma-Ray Bursts via Inverse Compton Emission\n\n**Abstract:** In this study, we propose a novel mechanism for the generation of high energy (HE) components observed in the spectra of Gamma-Ray Bursts (GRBs), attributing their origin to inverse Compton scattering processes. Specifically, we suggest that relativistic electrons, which are accelerated during internal shock events, interact with dark photons produced in the prompt emission phase. Our model effectively accounts for both the spectral characteristics and the temporal evolution of HE emissions across various GRBs detected by the Fermi Large Area Telescope (LAT). The predicted angular distributions of the emitted light are consistent with observations made by the Swift Burst Alert Telescope (BAT) and Fermi Gamma-ray Burst Monitor (GBM). \n\nFurthermore, our framework provides a compelling explanation for the phenomenon where certain bursts, despite exhibiting bright optical emissions, do not show detectable HE emissions. This discrepancy can be understood within the context of our model, which highlights the complex interplay between the energetic processes at work during GRB events. \n\nAdditionally, we discuss the implications of our findings for constraining key physical parameters of GRBs, such as their bulk Lorentz factors and magnetic field strengths. By refining our understanding of the mechanisms driving HE emissions, we can enhance our ability to characterize these extraordinary cosmic events. Our results pave the way for future observational campaigns that may uncover further insights into the nature of GRBs and their associated emissions, ultimately contributing to a deeper understanding of the underlying astrophysical processes.",
        "ori-fast-z-score": -3.1902304138506894,
        "water-fast-z-score": 5.003702332976757,
        "rewrite-fast-z-score": 0.19802950859533489
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Unravelling the morphologies of Luminous Compact Galaxies using the HST/ACS GOODS survey .\nAbstract:\nWe present an analysis of morphological properties for a sample of luminous compact galaxies (LCGs) selected in the Hubble Ultra Deep Field and studied with the Advanced Camera for Surveys on board the Hubble Space Telescope. We use GALFIT to fit single Sérsic profiles to all objects, allowing us to derive their structural parameters such as effective radius Re, Sérsic index n, axis ratio q, position angle PA, total magnitude mT , and half-light surface brightness SB e . The results show that most of these galaxies are bulge-dominated systems with high central concentration indices (n > 2). In addition, we find that there is no significant correlation between galaxy luminosity or size and its Sérsic index. Finally, by comparing our results with those obtained for other samples of massive early-type galaxies at different redshifts, we conclude that LCGs have similar sizes but higher concentrations than local elliptical galaxies.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Unravelling the morphologies of Luminous Compact Galaxies using the HST / ACS GOODS sample . Abstract : We report an assessment of morphological properties for a sample of luminous compact galaxies ( LCGs ) selected in the Hubble Ultra Deep Field and studied with the Advanced Camera for Surveys on board the Hubble Space Telescope .We use GALFIT to fitting individual Sérsic profiles to all bodies , allowing us to derive their structural values such as effective radius Re , Sérsic index h , axis proportion r , point angle PA , total magnitude mT , and half - light surface brightness SB r . The results show that most of these galaxies are bulge - dominated structures with high central abundance indices ( h > 2 ) .In addition , we find that there is no important relationship between galaxy luminosity or length and its Sérsic index . Finally , by using our findings with those generated for other specimens of large earliest - class stars at different redshifts , we conclude that LCGs have equal sizes but higher concentrations than local elliptical galaxies .",
        "rewrite_text": "We present a comprehensive analysis of the morphological characteristics of luminous compact galaxies (LCGs) drawn from the Hubble Ultra Deep Field, utilizing data obtained from the Advanced Camera for Surveys (ACS) aboard the Hubble Space Telescope. Our study employs the GALFIT software to fit individual Sérsic profiles to each galaxy in our sample, enabling us to extract critical structural parameters including effective radius (Re), Sérsic index (h), axis ratio (r), position angle (PA), total magnitude (mT), and half-light surface brightness (SB_r). The findings indicate that the majority of these LCGs exhibit bulge-dominated morphologies, characterized by elevated central Sérsic indices (h > 2). Interestingly, our analysis reveals a lack of significant correlation between the luminosity or size of the galaxies and their Sérsic indices. Furthermore, by comparing our results with data from other samples of early-type galaxies at varying redshifts, we conclude that LCGs possess comparable sizes but exhibit greater central concentrations than their local elliptical counterparts. This study enhances our understanding of the structural properties of LCGs and their evolutionary context within the broader framework of galaxy formation and development.",
        "ori-fast-z-score": -0.5773502691896257,
        "water-fast-z-score": 5.047146145152358,
        "rewrite-fast-z-score": 1.3093073414159544
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Noncommutative tori and the Riemann-Hilbert correspondence .\nAbstract:\nThe noncommutative torus is an algebra which can be viewed as a deformation quantization of the ordinary torus, or equivalently as a quantum group at roots of unity.  The purpose of this article is to give a brief introduction into some aspects of the theory of these algebras, with emphasis on their representation theory.   We will first introduce the basic definitions in Section 1, then discuss the connection between the noncommutative toruses and the Riemann-Hilbertequivalence in Section 2.    In Section 3 we present the classification theorem for irreducible representations of the noncommutative two-torus, while in Section 4 we describe how one obtains explicit formulas for such representations by using theta functions. Finally, in Section 5 we explain how the results obtained so far are related to the study of integrable systems. This article is based on lectures given by the author during his stay at the Institute for Advanced Study (IAS) in Princeton, NJ, USA, in summer 2005.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Noncommutative tori and the Riemann - Hilbert correspondence . Abstract : The noncommutative torus is an algebra which can be viewed as a deformation quantization of the ordinary torus , or equivalently as a quantum group at roots of unity .The purpose of this page is to give a brief introduction into some elements of the notion of these algebras , with emphasis on their representation theory . We will first introduce the fundamental definitions in Section 1 , then discuss the link between the noncommutative toruses and the Riemann - Hilbertequivalence in Section 2 .In Section 3 we present the classification principle for irreducible representations of the noncommutative two - torus , while in Section 4 we explain how one obtains explicit formulas for such representations by using theta functions . Finally , in Section 5 we explain how the results derived so far are related to the study of integrable systems .This page is based on lectures given by the writer during his visit at the Institute for Advanced Study ( IAS ) in Princeton , NJ , USA , in summer 2005 .",
        "rewrite_text": "Title: Noncommutative Tori and the Riemann-Hilbert Correspondence\n\nAbstract: The concept of noncommutative tori represents a significant advancement in the field of algebra, functioning as a deformation quantization of the classical torus, or alternatively, as a quantum group defined at roots of unity. This article aims to provide a comprehensive overview of the fundamental aspects of noncommutative tori, with a particular focus on their representation theory. In the first section, we establish essential definitions that lay the groundwork for understanding these algebras. The second section explores the intriguing connection between noncommutative tori and the Riemann-Hilbert correspondence, highlighting the implications of this relationship in the broader context of mathematical physics. \n\nIn Section 3, we delve into the classification of irreducible representations of the noncommutative two-torus, presenting key principles that govern this classification. Following this, Section 4 offers a detailed explanation of how explicit formulas for these representations can be derived using theta functions, showcasing the interplay between algebraic structures and analytic techniques. Finally, in Section 5, we discuss the relevance of our findings to the study of integrable systems, illustrating how the insights gained from noncommutative tori can inform our understanding of complex dynamical systems. \n\nThis article is based on a series of lectures delivered by the author during a visit to the Institute for Advanced Study (IAS) in Princeton, NJ, in the summer of 2005, and aims to bridge the gap between abstract algebraic concepts and their applications in mathematical physics. Through this exploration, we hope to illuminate the rich interplay between noncommutative geometry and integrable systems, providing a foundation for future research in these interconnected areas.",
        "ori-fast-z-score": 0.13483997249264842,
        "water-fast-z-score": 3.5762373640756184,
        "rewrite-fast-z-score": 0.8867963503478639
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Revisiting the Issues On Netflow Sample and Export Performance .\nAbstract:\nIn this work, we revisit some issues on netflow sample and export performance in terms of packet loss rate (PLR) and flow sampling accuracy (FSA). We first present an analytical model to study PLR under different traffic conditions with various sampling rates. Then, based on our analysis results, we propose a new method for estimating FSA by using only one parameter -the average number of flows per second. Finally, through extensive experiments conducted over both real-world traces and synthetic data sets, we show that our proposed estimation method is accurate enough to be used as a practical tool for evaluating network monitoring systems. The rest of this paper is organized as follows. In Section 2, we introduce related works about netflow sampling and exporting. In Section 3, we describe our experimental environment. In Section 4, we analyze the relationship between PLR and sampling rate. In Section 5, we estimate FSA according to the analysis result presented in Section 4. In Section 6, we evaluate the effectiveness of our proposed estimation method via extensive experiments. Finally, conclusions are drawn in Section 7.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Revisiting the Issues On Netflow Sample and Export Performance . Abstract : In this project , we revisit some issues on netflow sample and export quality in terms of packet loss rate ( PLR ) and flow sampling accuracy ( FSA ) .We first offer an analytical method to study PLR under various traffic conditions with various sampling rates . Then , using on our analysis results , we propose a new method for estimating FSA by using only one parameter - the average number of flows per second .Finally , through numerous experiments conducted over both real - time traces and synthetic information sets , we prove that our proposed estimation method is accurate sufficient to be used as a practical tool for evaluating network monitoring systems . The rest of this paper is organized as follows .In Section 2 , we provide similar articles about netflow filtering and exporting . In Section 3 , we explain our research surroundings .In Section 4 , we assess the relationship between PLR and sampling rate . In Section 5 , we estimate FSA according to the evaluation result provided in Section 4 .In Section 6 , we assess the performance of our proposed estimation method via extensive experiments . Finally , findings are drawn in Section 7 .",
        "rewrite_text": "Title: Revisiting the Issues on Netflow Sample and Export Performance\n\nAbstract: This study addresses critical concerns regarding the quality of netflow sampling and export, specifically focusing on packet loss rate (PLR) and flow sampling accuracy (FSA). We begin by presenting an analytical framework to investigate PLR across a range of traffic scenarios and sampling rates. Building on our analytical findings, we introduce a novel approach for estimating FSA that relies solely on a single parameter: the average number of flows per second. To validate our proposed estimation method, we conducted a series of experiments utilizing both real-time traffic data and synthetic datasets. The results demonstrate that our method provides a sufficiently accurate estimation of FSA, making it a viable tool for evaluating the performance of network monitoring systems. The structure of the paper is organized as follows: Section 2 reviews related literature on netflow filtering and exporting techniques. Section 3 outlines the research environment and methodology employed in our study. In Section 4, we analyze the correlation between PLR and sampling rate, providing insights into how these factors interact. Section 5 details our estimation of FSA based on the evaluations conducted in the previous section. Section 6 presents a comprehensive assessment of the performance of our proposed estimation method, supported by extensive experimental data. Finally, Section 7 summarizes our findings and discusses their implications for future research in network monitoring and performance evaluation.",
        "ori-fast-z-score": 1.3199500146737049,
        "water-fast-z-score": 6.262945776223707,
        "rewrite-fast-z-score": 1.212256250712408
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The SWIRE-VVDS-CFHTLS surveys: stellar mass assembly over the last 10 Gyears. Evidence for a major build up of the red sequence between z=2 and z=1 .\nAbstract:\nWe present an analysis of the evolution in the luminosity function (LF) and colour-magnitude relation (CMR) of galaxies selected by their rest-frame optical colours, using data from three large multi-wavelength surveys covering different areas on the sky. The first is the Spitzer Wide-area InfraRed Extragalactic survey (SWIRE), which covers about one quarter of the southern hemisphere at 24 microns with MIPS; the second is the VIMOS Very Deep Survey (VVDS), which has been carried out as part of the CFHT Legacy Survey (CFHTLS). This provides deep photometry in five bands to AB magnitude limits ranging from 26.5 to 27.7 mags in the u*gri filters. Finally we use near-infrared imaging obtained with WIRCam mounted on the Canada France Hawaii Telescope (CFHT) to select samples of massive galaxies at 1<z<1.4. We find that the LF evolves strongly towards fainter magnitudes since z=1.2, while there are no significant changes in its shape or normalisation. In addition, we show that the CMR becomes bluer with time, indicating that the fraction of quiescent systems increases significantly between these epochs.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The SWIRE - VVDS - CFHTLS studies : stellar mass formation over the last 10 Gyears . Evidence for a major build up of the red sequence between z = 2 and z = 1 .Abstract : We present an assessment of the evolution in the luminosity function ( LF ) and colour - magnitude function ( CMR ) of stars selected by their rest - frame optical colours , using data from three large multi - wavelength searches covering multiple parts on the heavens . The first is the Spitzer Wide - area InfraRed Extragalactic study ( SWIRE ) , which covers about one quarter of the southern hemisphere at 24 microns with MIPS ; the second is the VIMOS Very Deep Survey ( VVDS ) , which has been carried out as part of the CFHT Legacy Survey ( CFHTLS ) .This offers deep photometry in five bands to AB magnitude limits ranging from 26 . 5 to 27 . 7 mags in the u * gri filters . Finally we using near - infrared imaging obtained with WIRCam installed on the Canada France Hawaii Telescope ( CFHT ) to select samples of large galaxies at 1 < z < 1 . 4 .We see that the LF evolves highly towards fainter magnitudes since z = 1 . 2 , while there are no considerable changes in its size or normalisation . In addition , we find that the CMR gets bluer with time , showing that the fraction of quiescent systems grows significantly between these epochs .",
        "rewrite_text": "**Title:** The SWIRE - VVDS - CFHTLS Studies: Stellar Mass Formation Over the Last 10 Billion Years - Evidence for a Major Build-Up of the Red Sequence Between z = 2 and z = 1\n\n**Abstract:** This study investigates the evolution of the luminosity function (LF) and the color-magnitude relation (CMR) of stars, utilizing data from three extensive multi-wavelength surveys that span various regions of the sky. The first dataset is derived from the Spitzer Wide-area InfraRed Extragalactic Survey (SWIRE), which encompasses approximately one-quarter of the southern hemisphere at a wavelength of 24 microns, utilizing the Multiband Imaging Photometer for Spitzer (MIPS). The second dataset comes from the VIMOS Very Deep Survey (VVDS), conducted as part of the CFHT Legacy Survey (CFHTLS), which provides deep photometric data across five bands, achieving AB magnitude limits between 26.5 and 27.7 in the u*, g, r, and i filters. Additionally, we incorporate near-infrared imaging from WIRCam on the Canada-France-Hawaii Telescope (CFHT) to identify large galaxy samples within the redshift range of 1 < z < 1.4. Our findings indicate a significant evolution of the LF towards fainter magnitudes since z = 1.2, while the overall size and normalization of the LF remain relatively stable. Furthermore, we observe a trend in the CMR, which becomes progressively bluer over time, suggesting a marked increase in the proportion of quiescent systems during this period. This research provides compelling evidence for a substantial build-up of the red sequence in galaxies between redshifts z = 2 and z = 1, highlighting the dynamic processes of stellar mass formation and the changing characteristics of galaxy populations over the last ten billion years.",
        "ori-fast-z-score": -0.22086305214969307,
        "water-fast-z-score": 5.597977259474208,
        "rewrite-fast-z-score": -0.502518907629606
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Prospects for precision measurements of atomic helium using direct frequency comb spectroscopy .\nAbstract:\nWe present the prospects for high-precision measurement of the 1s2p 3P-1s2s 3S transition in atomic helium with an optical frequency comb (OFC). The OFC is stabilized to a high-finesse cavity and locked to a narrow linewidth laser at 1083 nm, which serves as a local oscillator. We show that this system can be used to measure the absolute frequencies of two transitions in helium with uncertainties below 100 kHz. This will allow us to determine the fine-structure constant α with relative uncertainty better than 2×10−10 by measuring the ratio between these two frequencies. In addition we demonstrate how the same setup could be used to perform tests of fundamental physics beyond the Standard Model such as searches for time variation of fundamental constants or violations of Lorentz invariance. Optical frequency combs are powerful tools for precise metrology  1–3  . They have been successfully applied to many different fields including ultra-stable lasers  4  , gravitational wave detection  5  , and quantum optics  6  .\nIn particular they provide unprecedented possibilities for high-precision measurement  7–9  . Here we propose to use them to improve our knowledge on the value of the fine structure constant  10  . To achieve this goal it is necessary to measure the absolute frequencies f(1s2p 3P1) = 929 072 631 770 Hz  11  and f(1s2s 3S1) = 929 073 761 828 Hz  12  of two transitions in helium. These values were determined previously with uncertainties of about 300 kHz  13  but recent theoretical calculations suggest that their accuracy may be improved significantly  14–18  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Prospects for precision observations of atomic helium using direct wavelength comb spectroscopy . Abstract : We present the possibilities for high - precision study of the 1s2p 3P - 1s2s 3S transfer in atomic helium with an optical frequency comb ( OFC ) .The OFC is stabilized to a high - finesse cavity and locked to a thin linewidth laser at 1083 nm , which serves as a local oscillator . We see that this scheme can be used to measure the absolute frequencies of two transitions in helium with uncertainties below 100 kHz .This will provide us to predict the fine - structure constant α with relative uncertainty better than 2×10−10 by monitoring the proportion between these two frequencies . In addition we prove how the same setup could be used to conduct tests of fundamental theory beyond the Standard Model such as searches for time variation of fundamental constants or violations of Lorentz invariance .Optical frequency combs are powerful tools for precise metrology 1 – 3 . They have been successfully applied to many various fields including ultra - stable lasers 4 , gravity wave detection 5 , and quantum optics 6 .In particular they give unprecedented possibilities for high - precision precision 7 – 9 . Here we undertake to use them to develop our information on the value of the fine structure constant 10 .To achieve this goal it is required to measure the absolute frequencies f ( 1s2p 3P1 ) = 929 072 631 770 Hz 11 and f ( 1s2s 3S1 ) = 929 073 761 828 Hz 12 of two transitions in helium . These values were determined earlier with uncertainties of about 300 kHz 13 but recent theoretical calculations suggest that their sensitivity might be improved dramatically 14 – 18 .",
        "rewrite_text": "In this article, we explore the potential for conducting high-precision measurements of the 1s2p 3P to 1s2s 3S transition in atomic helium utilizing optical frequency comb (OFC) technology. Our approach involves stabilizing the OFC to a high-finesse optical cavity and locking it to a narrow linewidth laser operating at 1083 nm, which acts as a local oscillator. This innovative setup enables us to measure the absolute frequencies of two specific transitions in helium with uncertainties reduced to below 100 kHz. Such precision is crucial for refining our understanding of the fine-structure constant (α), allowing us to predict its value with a relative uncertainty of less than 2×10−10 by analyzing the ratio of these two frequencies.\n\nMoreover, we demonstrate that this experimental framework can be leveraged to test fundamental theories that extend beyond the Standard Model of particle physics. This includes investigations into the potential time variation of fundamental constants and the search for violations of Lorentz invariance. Optical frequency combs have emerged as essential instruments in the realm of precise metrology, finding applications across diverse fields such as ultra-stable laser development, gravitational wave detection, and quantum optics. Their unique capabilities offer unprecedented opportunities for high-precision measurements.\n\nIn our study, we aim to enhance the accuracy of previously determined absolute frequencies for the transitions f(1s2p 3P1) = 929,072,631,770 Hz and f(1s2s 3S1) = 929,073,761,828 Hz, which were previously measured with uncertainties around 300 kHz. Recent theoretical advancements indicate that the sensitivity of these measurements could be significantly improved, paving the way for more accurate determinations of the fine-structure constant and further explorations into fundamental physics.",
        "ori-fast-z-score": 0.18107149208503706,
        "water-fast-z-score": 5.680518698404823,
        "rewrite-fast-z-score": 0.9838699100999074
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Surface Structure in an Accretion Disk Annulus with Comparable Radiation and Gas Pressure .\nAbstract:\nWe present the results of three-dimensional hydrodynamic simulations of accretion disks around black holes, which include both gas pressure and radiation pressure as well as self-gravity. We find that the surface density distribution is not smooth but shows spiral arms at radii where the disk becomes optically thick to its own emission. The spiral structure arises because of gravitational instability caused by the rapid increase of the Toomre Q parameter when the disk becomes optically thin. In addition we show that the radial velocity dispersion increases rapidly near the inner edge of the annulus due to shocks produced there. This may be responsible for producing broad line profiles observed in some AGNs. \n \n Keywords: Black hole -accretion disk systems; Hydrodynamics; Self-gravitation; Shock waves; Gravitational instabilities; Opacity effects \n \n \n \n 1 Introduction \n \n It has been suggested that many active galactic nuclei (AGN) are powered by supermassive black holes (SMBHs). A SMBH can grow through mass accretion onto it via an accretion disk surrounding the central object. Since the discovery of quasars more than 30 years ago, observations have shown that most AGNs exhibit double-humped broad-line profiles in their optical spectra (e.g.,  1; 2 ), indicating that they contain rotating accretion disks  3  . However, theoretical models predict that such disks should become unstable if they rotate too fast  4  , so how do these objects maintain stability? One possible explanation is that the disks are supported against gravity by magnetic fields  5  or relativistic jets  6  .\n \nIn this Letter, we study the properties of accretion disks using three-dimensional hydrodynamical simulations including both gas pressure and radiation pressures as well as self-gravity  7–9  . Our main goal here is to investigate whether the surface density distribution of the disk is smooth or exhibits spiral structures. If the latter case occurs, then what causes them?\n2 Model Description\n\nModel Setup\nThe basic equations governing our model are given by:",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Surface Structure in an Accretion Disk Annulus with Comparable Radiation and Gas Pressure . Abstract : We present the results of three - dimensional hydrodynamic simulations of accretion disks around black holes , which use both gas pressure and radiation stress as well as self - gravity .We see that the surface density distribution is not smooth but exhibits spiral arms at radii where the disk turns optically dense to its own emission . The spiral shape arises because of gravitational instability caused by the fast increase of the Toomre Q function when the disk gets optically thin .In addition we find that the radial speed dispersion increases quickly near the inner boundary of the annulus resulting to shocks generated there . This might be responsible for producing wider line profiles observed in some AGNs .Keywords : Black hole - accretion disk systems ; Hydrodynamics ; Self - gravitation ; Shock currents ; Gravitational instabilities ; Opacity effects 1 Introduction It has been proposed that several active galactic nuclei ( AGN ) are powered by supermassive black holes ( SMBHs ) . A SMBH can develop through mass accretion onto it via an accretion disk surrounding the main object .Since the discovery of quasars more than 30 centuries earlier , observations have shown that most AGNs exhibit double - humped wide - line profiles in their optical spectra ( e . g . , 1 ; 2 ) , showing that they contain spinning accretion disks 3 . However , theoretical estimates expect that such disks should grow unstable if they rotate too fast 4 , so how do these objects retain stability ?One could explanation is that the disks are protected against gravity by magnetic waves 5 or relativistic jets 6 . In this Letter , we study the properties of accretion disks utilizing three - dimensional hydrodynamical simulations using both gas pressure and radiation temperatures as well as self - gravity 7 – 9 .Our main goal here is to examine whether the surface velocity distribution of the disk is smooth or shows spiral shapes . If the latter situation occurs , then what causes them ?2 Model Description Model Setup The basic equations governing our model are given by :",
        "rewrite_text": "**Title:** Surface Structure in an Accretion Disk Annulus with Comparable Radiation and Gas Pressure\n\n**Abstract:** In this study, we present findings from three-dimensional hydrodynamic simulations of accretion disks surrounding black holes, incorporating both gas pressure and radiation stress alongside self-gravity effects. Our simulations reveal that the surface density distribution within these disks is characterized by non-uniformity, specifically exhibiting spiral arm formations at radii where the disk becomes optically dense to its own emitted radiation. The emergence of these spiral structures is attributed to gravitational instabilities, which are triggered by a rapid increase in the Toomre Q parameter as the disk transitions to an optically thin state. Furthermore, we observe a significant increase in radial speed dispersion near the inner boundary of the annulus, leading to the generation of shock waves in that region. This phenomenon may contribute to the broader line profiles that are frequently observed in certain active galactic nuclei (AGNs). Our research highlights the intricate interplay between gravitational forces, radiation pressure, and gas dynamics in shaping the structure of accretion disks. \n\n**Keywords:** Black hole - accretion disk systems; Hydrodynamics; Self-gravitation; Shock currents; Gravitational instabilities; Opacity effects.\n\n**1 Introduction:** The existence of supermassive black holes (SMBHs) at the centers of many active galactic nuclei (AGNs) has been widely accepted, with mass accretion onto these black holes occurring through surrounding accretion disks. Since the identification of quasars over three decades ago, it has been noted that a majority of AGNs display double-humped wide-line profiles in their optical spectra, indicating the presence of rotating accretion disks. However, theoretical models suggest that these disks may become unstable under rapid rotation, raising the question of how stability is maintained. Potential explanations include the influence of magnetic waves or relativistic jets acting as stabilizing forces. In this letter, we investigate the characteristics of accretion disks through advanced three-dimensional hydrodynamic simulations that account for gas pressure, radiation temperatures, and self-gravity. Our primary objective is to determine whether the surface velocity distribution of the disk is uniform or exhibits spiral patterns, and if the latter is observed, to identify the underlying mechanisms responsible for their formation. \n\n**2 Model Description:** The foundational equations that govern our model are outlined as follows:",
        "ori-fast-z-score": -0.08137884587711594,
        "water-fast-z-score": 6.9755211086227895,
        "rewrite-fast-z-score": 0.9827076298239908
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Theory of Two-Photon Interactions with Broadband Down-Converted Light and Entangled Photons .\nAbstract:\nWe present the theory for two-photon interactions in broadband down-converted light, including entanglement between photons generated by spontaneous parametric down conversion (SPDC). We show that this leads to new effects such as photon bunching at zero time delay and antibunching at nonzero delays. These results are compared against experimental data obtained using SPDC sources based on periodically poled lithium niobate waveguides. The theoretical model is also used to predict the effect of varying pump bandwidths and crystal lengths on the degree of second-order coherence g(2)(0) measured experimentally. This work was supported by EPSRC grant EP/G037656/1. \n \n In recent years there has been growing interest in quantum optics experiments involving broadband down-conversion  1–3 . Such experiments have led to demonstrations of novel phenomena such as single-photon switching  4 , sub-Poissonian statistics  5 , squeezing  6 , and nonclassical correlations  7, 8 . However, many aspects of these experiments remain poorly understood due to difficulties associated with modelling the complicated nonlinear processes involved  9, 10 . Here we develop an analytical description of two-photon interactions in broad-band down-converted light which includes both temporal and spatial degrees of freedom  11, 12 . Our approach allows us to calculate the joint spectral intensity distribution of the down-converted field  13 , which can then be used to determine the probability density function describing the arrival times of pairs of photons produced via spontaneous parametric downconversion  14–18 . As well as providing insight into the physics underlying broadband down-conversion experiments, our analysis enables quantitative predictions about the behaviour of such systems to be made.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Theory of Two - Photon Interactions with Broadband Down - Converted Light and Entangled Photons . Abstract : We present the notion for two - photon interactions in broadband down - converted radiation , including entanglement between photons generated by spontaneous parametric down transformation ( SPDC ) .We see that this contributes to novel influences such as photon bunching at zero time time and antibunching at nonzero delays . These data are compared against empirical data acquired using SPDC sources based on periodically poled lithium niobate waveguides .The theory model is also used to predict the impact of differing flow bandwidths and crystal lengths on the degree of second - order coherence g ( 2 ) ( 0 ) measured experimentally . This research was supported by EPSRC award EP / G037656 / 1 .In recent years there has been growing interest in quantum optics tests featuring wireless down - transfer 1 – 3 . Such experiments have led to demonstrations of new concepts such as single - photon switching 4 , sub - Poissonian statistics 5 , squeezing 6 , and nonclassical correlations 7 , 8 .However , many aspects of these experiments exist poorly studied attributed to difficulties related with modelling the complicated nonlinear processes used 9 , 10 . Here we develop an analytical explanation of two - photon interactions in narrow - band down - converted radiation which includes both temporal and spatial degrees of liberty 11 , 12 .Our solution enables us to estimate the joint spectral intensity distribution of the down - converted field 13 , which can then be used to predict the probability density function explaining the entry times of pairs of photons created via spontaneous parametric downconversion 14 – 18 . As well as providing information into the physics underlying wireless down - conversion techniques , our analysis enables numerical assumptions about the behaviour of such systems to be made .",
        "rewrite_text": "We introduce a comprehensive framework for understanding two-photon interactions within the context of broadband down-converted light, particularly focusing on the entanglement of photons produced through spontaneous parametric down-conversion (SPDC). Our findings reveal intriguing phenomena, such as photon bunching occurring at zero time delays and antibunching at nonzero delays, which we validate against experimental data obtained from SPDC sources utilizing periodically poled lithium niobate waveguides. The theoretical model we propose also allows us to explore how variations in bandwidth and crystal length influence the second-order coherence parameter, g(2)(0), as measured in experiments. This research is supported by the Engineering and Physical Sciences Research Council (EPSRC) under award EP/G037656/1.\n\nIn recent years, there has been an increasing interest in quantum optics experiments involving wireless down-conversion techniques. These investigations have led to the exploration of innovative concepts such as single-photon switching, sub-Poissonian statistics, squeezing, and nonclassical correlations. However, many facets of these experiments remain inadequately explored, primarily due to the complexities associated with modeling the intricate nonlinear processes involved. In this study, we develop an analytical approach to describe two-photon interactions in narrow-band down-converted radiation, incorporating both temporal and spatial degrees of freedom.\n\nOur analytical solution facilitates the estimation of the joint spectral intensity distribution of the down-converted field, which can subsequently be utilized to predict the probability density function that characterizes the arrival times of photon pairs generated through spontaneous parametric down-conversion. Beyond enhancing our understanding of the fundamental physics governing wireless down-conversion techniques, our analysis provides a basis for making numerical predictions regarding the behavior of such systems, thereby contributing valuable insights into the field of quantum optics.",
        "ori-fast-z-score": -1.5434872662825794,
        "water-fast-z-score": 6.1739490651303175,
        "rewrite-fast-z-score": 0.17149858514250882
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Exploring the Variable Sky with the Sloan Digital Sky Survey .\nAbstract:\nThe Sloan Digital Sky Survey (SDSS) is an ambitious project to map one quarter of the sky in five photometric bands, and obtain spectra for over 100 million galaxies, quasars, stars, and other celestial objects.  The survey has been completed on two thirds of its original footprint; it will be complete by 2008. This talk describes some of the science that can be done using this data set. It also discusses how astronomers are working together to make use of these vast resources. In particular I describe my own work on studying galaxy clusters as dark matter halos, measuring their masses through gravitational lensing effects, and determining their evolution across cosmic time. Finally, I discuss plans for future surveys which will build upon SDSS s successes. The Sloan Digital Sky Survey is an ambitious project to measure the positions and redshifts of more than 100 million galaxies, quasARS, stars, and other objects in our Universe.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Exploring the Variable Sky with the Sloan Digital Sky Survey . Abstract : The Sloan Digital Sky Survey ( SDSS ) is an ambitious project to map one quarter of the heavens in five photometric bands , and collect spectra for over 100 million galaxies , quasars , stars , and other celestial entities .The survey has been completed on two thirds of its initial footprint ; it will be full by 2008 . This discussion describes some of the science that can be performed using this data set .It especially discusses how astronomers are working together to make using of these enormous resources . In particular I relate my own research on studying galaxy galaxies as dark matter halos , measuring their masses through gravity lensing effects , and determining their development across universe time .Finally , I discuss strategies for future surveys which will build upon SDSS s successes . The Sloan Digital Sky Survey is an ambitious project to measure the places and redshifts of more than 100 million galaxies , quasARS , stars , and other objects in our Universe .",
        "rewrite_text": "Title: Investigating the Variable Sky through the Sloan Digital Sky Survey\n\nAbstract: The Sloan Digital Sky Survey (SDSS) represents a groundbreaking initiative aimed at mapping a quarter of the celestial sphere across five distinct photometric bands, while also acquiring spectra for over 100 million astronomical objects, including galaxies, quasars, stars, and various other celestial phenomena. As of now, the survey has successfully covered two-thirds of its intended area, with completion anticipated by 2008. This article delves into the scientific opportunities presented by this extensive dataset, highlighting collaborative efforts among astronomers to maximize the utility of these vast resources. A particular focus is placed on my research, which investigates galaxies as dark matter halos. This involves measuring their masses through gravitational lensing effects and analyzing their evolution over cosmic time. Additionally, I outline potential strategies for future surveys that aim to build upon the achievements of the SDSS, ensuring continued advancements in our understanding of the universe. The SDSS not only provides a comprehensive catalog of the locations and redshifts of more than 100 million galaxies, quasars, stars, and other celestial objects but also serves as a vital tool for ongoing and future astronomical research.",
        "ori-fast-z-score": 0.48507125007266594,
        "water-fast-z-score": 5.976143046671968,
        "rewrite-fast-z-score": 0.6546536707079772
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Signatures of Heavy Z-prime in the Extra U(1) Superstring Inspired Model: RGEs Analysis .\nAbstract:\nIn this work, we study the renormalization group equations (RGEs) for extra U(1) supersymmetric string inspired model with heavy Z  prime and its effects on gauge coupling unification at one-loop level. We find that the inclusion of new particles such as vector-like quarks and leptons can significantly affect the running behavior of gauge couplings. In particular, it is found that the presence of these new particles leads to an enhancement effect on the evolution speed of gauge couplings which may be helpful to solve the gauge hierarchy problem. Furthermore, by using the experimental data of low energy physics, we obtain some constraints on the mass spectrum of extra particles involved in our model. Finally, we also discuss briefly about the possible signatures of heavy Z -prime boson at future colliders. The results are summarized below. \nI. INTRODUCTORY REMARK\nThe Standard Model (SM), based on SU(3) C ×SU(2) L ×U(1) Y gauge symmetry, has been very successful in describing all known phenomena upto TeV scale energies  1  . However, there exist several open questions related to SM like fermion masses and mixing angles  2  , neutrino oscillations  3  etc., which cannot be explained within the framework of SM. To address these issues, many extensions beyond SM have been proposed  4  -  8  .\nAmong them, Grand Unified Theory (GUTs)  9  provides a natural solution to the above mentioned problems  10  . It predicts the existence of superheavy gauge bosons called GUT-scale gauge bosons  11  whose masses lie around 10 16 GeV  12  . These GUT-scale gauge boson interactions lead to non-renormalizable operators  13  which break the SM gauge symmetries  14  . Therefore, they should not appear in any physical process  15  . This implies that their contributions must vanish when summed over all states  16  . Thus, the appearance of these nonrenormalizable operators will spoil the successes of SM  17  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Signatures of Heavy Z - prime in the Extra U ( 1 ) Superstring Inspired Model : RGEs Analysis . Abstract : In this study , we study the renormalization group equations ( RGEs ) for extra U ( 1 ) supersymmetric string inspired theory with heavy Z prime and its consequences on gauge coupling unification at one - loop level .We see that the introduction of new ions such as vector - like quarks and leptons can significantly affect the running response of gauge couplings . In particular , it is found that the presence of these new objects gives to an enhancement impact on the evolution speed of gauge couplings which would be beneficial to solve the gauge hierarchy problem .Furthermore , by using the empirical data of low power physics , we obtain some restrictions on the mass spectrum of extra particles involved in our model . Finally , we also discuss briefly about the possible signatures of large Z - prime boson at possible colliders .The results are presented below . I .INTRODUCTORY REMARK The Standard Model ( SM ) , built on SU ( 3 ) C ×SU ( 2 ) L ×U ( 1 ) Y gauge symmetry , has been very successful in representing all known phenomena upto TeV scale energies 1 . However , there remain many open questions related to SM like fermion masses and mixing angles 2 , neutrino oscillations 3 etc . , which impossible be described within the framework of SM .To address these problems , various extensions beyond SM have been proposed 4 - 8 . Among them , Grand Unified Theory ( GUTs ) 9 offers a natural solution to the above mentioned difficulties 10 .It predicts the existence of superheavy gauge bosons called GUT - scale gauge bosons 11 whose masses sit around 10 16 GeV 12 . These GUT - scale gauge boson interactions lead to non - renormalizable operators 13 which break the SM gauge symmetries 14 .Therefore , they should not appear in any physical process 15 . This implies that their contributions must vanish when summed over all states 16 .Thus , the appearance of these nonrenormalizable operators will spoil the achievements of SM 17 .",
        "rewrite_text": "**Title:** Signatures of Heavy Z-prime in the Extra U(1) Superstring Inspired Model: RGEs Analysis\n\n**Abstract:** This article investigates the renormalization group equations (RGEs) within the framework of an extra U(1) supersymmetric string-inspired theory that incorporates a heavy Z-prime boson. We analyze the implications of this model on gauge coupling unification at the one-loop level. Our findings indicate that the introduction of additional particles, specifically vector-like quarks and leptons, significantly influences the running behavior of gauge couplings. Notably, the inclusion of these new particles accelerates the evolution of gauge couplings, which may provide a viable approach to addressing the gauge hierarchy problem. By leveraging empirical data from low-energy physics, we derive constraints on the mass spectrum of the additional particles present in our model. Furthermore, we briefly explore the potential signatures of a large Z-prime boson that could be observable at future collider experiments. The results of our analysis are detailed in the following sections. \n\n**I. INTRODUCTORY REMARKS** The Standard Model (SM), constructed on the gauge symmetry group SU(3)C × SU(2)L × U(1)Y, has proven to be remarkably effective in explaining all known phenomena up to energies on the order of TeV. However, it leaves several critical questions unanswered, particularly concerning the masses and mixing angles of fermions, as well as the phenomenon of neutrino oscillations, which cannot be adequately addressed within its framework. To tackle these unresolved issues, various extensions to the SM have been proposed. Among these, Grand Unified Theories (GUTs) present a compelling solution, predicting the existence of superheavy gauge bosons, known as GUT-scale gauge bosons, with masses around 10^16 GeV. The interactions of these GUT-scale gauge bosons lead to non-renormalizable operators that disrupt the gauge symmetries of the SM, suggesting that their contributions must vanish when summed over all states. Consequently, the emergence of these non-renormalizable operators poses a challenge to the successes of the SM.",
        "ori-fast-z-score": -0.086710996952412,
        "water-fast-z-score": 5.6594533097027915,
        "rewrite-fast-z-score": -0.08606629658238704
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Characterization of P2P IPTV Traffic: Scaling Analysis .\nAbstract:\nIn this work, we present the results of an extensive measurement study on the characteristics and dynamics of peer-to-peer (P2P) Internet Protocol Television (IPTV). We collected traffic traces for more than two months in three different countries using several hundred dedicated monitoring hosts deployed at ISPs around the world. Our analysis shows that P2P-TV is responsible for up to 80% of all video streaming traffic observed by these monitors. In addition, our measurements reveal significant differences between the behavior of various P2P-TV systems. Finally, we show how such knowledge can be used to detect P2P-TV streams with high accuracy even when they are encrypted or use proxies. The rest of this paper is organized as follows. Section 2 presents related work. Section 3 describes our experimental setup. Sections 4-7 present detailed analyses of the data obtained during our experiments. Finally, Section 8 concludes the paper. This section provides background information about P2P-TV systems and summarizes previous research efforts aimed at characterizing their traffic.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Characterization of P2P IPTV Traffic : Scaling Analysis . Abstract : In this research , we present the results of an extensive measurement work on the types and dynamics of peer - to - peer ( P2P ) Internet Protocol Television ( IPTV ) .We collected traffic traces for more than two months in three different countries using many hundred dedicated screening hosts deployed at ISPs around the world . Our study shows that P2P - TV is responsible for up to 80 % of all video broadcasting traffic observed by these monitors .In addition , our measurements reveal major changes between the actions of several P2P - television systems . Finally , we show how such knowledge can be used to locate P2P - television streams with high sensitivity especially when they are encrypted or use proxies .The rest of this paper is organized as follows . Section 2 offers related work .Section 3 describes our research setup . Sections 4 - 7 provide comprehensive analyses of the information obtained during our experiments .Finally , Section 8 concludes the paper . This section offers background data about P2P - television systems and summarizes earlier survey efforts aimed at characterizing their traffic .",
        "rewrite_text": "In this study, we present a comprehensive analysis of peer-to-peer (P2P) Internet Protocol Television (IPTV) traffic, based on extensive measurements conducted over a period of more than two months across three different countries. Utilizing several hundred dedicated monitoring hosts strategically deployed at various Internet Service Providers (ISPs) worldwide, we collected detailed traffic traces that reveal significant insights into the nature and dynamics of P2P IPTV. Our findings indicate that P2P IPTV accounts for as much as 80% of all video broadcasting traffic captured by our monitoring systems, highlighting its substantial impact on the overall video streaming landscape. \n\nMoreover, our analysis uncovers notable variations in the operational patterns of different P2P television systems, suggesting that each system exhibits unique characteristics that influence traffic behavior. This understanding is crucial for developing effective strategies to identify and locate P2P television streams, particularly in instances where the streams are encrypted or routed through proxies, thereby enhancing the sensitivity of detection methods.\n\nThe structure of this paper is organized to facilitate a thorough understanding of our research. Section 2 reviews related literature and previous studies in the field. Section 3 outlines the methodology and setup employed in our research. Sections 4 through 7 delve into detailed analyses of the data collected during our experiments, presenting key findings and interpretations. Finally, Section 8 concludes the paper by providing background information on P2P television systems and summarizing prior efforts aimed at characterizing their traffic patterns. This work contributes to the growing body of knowledge in the field of P2P IPTV and offers valuable insights for future research and practical applications.",
        "ori-fast-z-score": 0.953998092005724,
        "water-fast-z-score": 6.957010852370434,
        "rewrite-fast-z-score": 2.459747896071916
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A map of the day-night contrast of the extrasolar planet HD 189733b .\nAbstract:\nWe present an analysis of high-resolution near-infrared images obtained with NACO/VLT and SPHERE/ZIMPOL/ESO to study the photometric variability of the exoplanet HD 189733b in its transit across the stellar disk. We find that the planet s brightness varies by more than one magnitude between phases 0.8-0.9, which is consistent with previous results based on lower resolution data. The observed phase curve shows two maxima at phases 0.85 and 0.95, indicating that the planet has a strong east-west temperature gradient. This result can be explained if we assume that the planet orbits close to edge-on (i = 85°) and that it receives most of its energy from the star through a single hemisphere. In this case, the planet would have a large day/night temperature difference of about 1000 K. Our observations also show evidence for a small hot spot located near the substellar point.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A mapping of the morning - night difference of the extrasolar object HD 189733b . Abstract : We report an assessment of high - resolution near - infrared images obtained with NACO / VLT and SPHERE / ZIMPOL / ESO to study the photometric variability of the exoplanet HD 189733b in its transit across the stellar disk .We see that the planet s brightness varies by more than one magnitude between phases 0 . 8 - 0 . 9 , which is consistent with previous findings based on lower resolution data . The observed phase curve shows two maxima at phases 0 . 85 and 0 . 95 , showing that the planet has a weak east - west heat gradient .This result can be described if we suppose that the planet orbits close to edge - on ( i = 85° ) and that it takes most of its energy from the star through a single hemisphere . In this situation , the planet would have a large day / night heat change of about 1000 K . Our observations also demonstrate proof for a small heat place located near the substellar point .",
        "rewrite_text": "**Title:** Mapping the Morning-Night Difference of the Extrasolar Object HD 189733b\n\n**Abstract:** In this study, we present a detailed analysis of high-resolution near-infrared images acquired using NACO/VLT and SPHERE/ZIMPOL/ESO to investigate the photometric variability of the exoplanet HD 189733b during its transit across the stellar disk. Our findings reveal that the brightness of HD 189733b fluctuates by over one magnitude between the phases of 0.8 and 0.9, corroborating earlier observations derived from lower resolution data. The phase curve we observed exhibits two distinct maxima at phases 0.85 and 0.95, indicating the presence of a subtle east-west heat gradient on the planet's surface. This phenomenon can be explained by the hypothesis that HD 189733b orbits in a nearly edge-on configuration (with an inclination of approximately 85°) and predominantly absorbs stellar energy through a single hemisphere. Under these conditions, the planet experiences a significant day-night temperature differential, estimated to be around 1000 K. Furthermore, our observations provide evidence for a localized heat source situated near the substellar point, suggesting complex atmospheric dynamics at play. This research enhances our understanding of the thermal structure and atmospheric behavior of HD 189733b, contributing valuable insights into the characteristics of exoplanets with similar profiles. The implications of these findings extend to the broader field of exoplanet studies, offering a framework for future investigations into the atmospheric conditions and energy distribution of distant worlds.",
        "ori-fast-z-score": -1.0256451881367414,
        "water-fast-z-score": 4.076197322920544,
        "rewrite-fast-z-score": 0.7770286898858113
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Exploring Continuous Tensegrities .\nAbstract:\nThe concept of tensegrity is used to describe the structural behavior of many biological systems, such as muscles and tendons.  In this work we explore how continuous tensegrities can be generated by using an evolutionary algorithm that optimizes their performance in terms of compliance with external loads while maintaining stability under gravity loading conditions.   The results show that it is possible to generate stable structures that are able to resist large deformations without collapsing or losing their integrity. This research has been funded by the European Commission through the Marie Curie Initial Training Network (ITN) program. The concept of tensegrity was first introduced by Buckminster Fuller more than 60 years ago  1  . It describes the structural behavior of many natural systems like muscles  2  , tendons  3  , bones  4  , and even living organisms  5  .\nIn recent decades there have been several attempts at applying the concept of tensegrity to engineering applications  6  -  8  . However, most of these works focus on discrete tensegrities which consist of rigid bars connected together by elastic struts  9  . These types of structures cannot easily adapt to changes in their environment since they do not allow for any deformation  10  . On the other hand, continuous tensegrities  11  are capable of changing shape continuously when subjected to external forces  12  . They also exhibit higher levels of robustness against damage  13  compared to conventional materials  14  . Despite all these advantages, very little attention has been paid so far to the design of continuous tensegrities  15  .\nThis lack of interest may be due to the fact that designing continuous tensegrities requires solving highly nonlinear optimization problems  16  . Moreover, finding solutions to these problems is extremely challenging because of the high number of local optima  17  . To overcome these difficulties, researchers usually use heuristic search algorithms  18  -  20  instead of exact methods  21  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Exploring Continuous Tensegrities . Abstract : The concept of tensegrity is utilized to define the structural response of several biological units , such as muscles and tendons .In this research we investigate how continuous tensegrities can be derived by using an phylogenetic algorithm that optimizes their performance in terms of compliance with external loads while maintaining strength under gravity loading circumstances . The results show that it is possible to create stable objects that are able to resist big deformations without weakening or losing their stability .This research has been sponsored by the European Commission through the Marie Curie Initial Training Network ( ITN ) program . The concept of tensegrity was first applied by Buckminster Fuller more than 60 years early 1 .It details the structural response of several biological systems like nerves 2 , tendons 3 , ribs 4 , and even living organisms 5 . In past decades there have been numerous attempts at application the idea of tensegrity to engineering users 6 - 8 .However , most of these works concentrate on discrete tensegrities which consist of rigid bars connected together by elastic struts 9 . These sorts of structures cannot easily respond to changes in their environment since they do not enable for any deformation 10 .On the other hand , continuous tensegrities 11 are capable of changing shape consistently when exposed to external forces 12 . They addition exhibit greater levels of robustness against damage 13 relative to conventional materials 14 .Despite all these benefits , very less attention has been paid so far to the design of continuous tensegrities 15 . This lack of interest might be due to the fact that designing continuous tensegrities demands modeling highly nonlinear optimization problems 16 .Moreover , finding solutions to these problems is incredibly hard because of the high number of local optima 17 . To solve these problems , researchers normally use heuristic search methods 18 - 20 rather of precise methods 21 .",
        "rewrite_text": "**Title: Exploring Continuous Tensegrities**\n\n**Abstract:** The principle of tensegrity has been instrumental in understanding the structural behavior of various biological components, including muscles and tendons. This study delves into the derivation of continuous tensegrities through the application of a phylogenetic algorithm designed to optimize their performance. The focus is on enhancing compliance with external loads while ensuring structural integrity under gravitational forces. Our findings indicate that it is feasible to construct stable structures capable of withstanding significant deformations without compromising their stability or strength. This research is supported by the European Commission via the Marie Curie Initial Training Network (ITN) program. The concept of tensegrity, first introduced by Buckminster Fuller over six decades ago, has been applied to various biological systems, such as nerves, tendons, ribs, and even entire organisms. In recent years, numerous efforts have been made to apply tensegrity principles in engineering contexts. However, the majority of these studies have focused on discrete tensegrities, which are composed of rigid bars interconnected by elastic struts. Such structures are limited in their ability to adapt to environmental changes due to their lack of deformation capability. In contrast, continuous tensegrities can undergo consistent shape alterations when subjected to external forces, demonstrating enhanced robustness against damage compared to traditional materials. Despite these advantages, the design of continuous tensegrities has received relatively little attention, likely due to the complexities involved in modeling highly nonlinear optimization problems. The challenge of navigating numerous local optima further complicates the search for solutions. Consequently, researchers often resort to heuristic search methods rather than precise optimization techniques. This study aims to bridge this gap by exploring innovative approaches to the design and optimization of continuous tensegrities, thereby expanding their potential applications in both biological and engineering fields.",
        "ori-fast-z-score": -1.6666666666666667,
        "water-fast-z-score": 7.835467939002064,
        "rewrite-fast-z-score": -0.4181210050035454
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Search for Tidal Dwarf Galaxies Candidates in a Sample of Ultraluminous Infrared Galaxies .\nAbstract:\nWe present the results of an optical spectroscopic survey aimed at finding tidal dwarf galaxies (TDGs) candidates among a sample of ultraluminous infrared galaxies (ULIRG). We have obtained spectra with FORS2@VLT and GMOS-N@Gemini telescopes, covering the wavelength range between 3600 Å and 9200 Å , for a total of 16 ULIRGs selected on the basis of their high far-infrared luminosity (L FIR > 10 12 L ⊙ ) and low redshift (z < 0.1).\nThe main goal is to find TDGs that are expected to be found around interacting systems such as ULIRGs. The presence of young stars in these objects would make them detectable through strong emission lines like Hα or  OII  . However, we do not detect any significant excess of emission line flux over what can be explained by star formation occurring within the host galaxy itself. This result suggests either that there are no TDGs associated with our sample of ULIRGs or that they are too faint and/or too red to be detected using current facilities.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Search for Tidal Dwarf Galaxies Candidates in a Sample of Ultraluminous Infrared Galaxies . Abstract : We report the results of an optical spectroscopic study aiming at finding tidal dwarf stars ( TDGs ) candidates among a sample of ultraluminous laser galaxies ( ULIRG ) .We have discovered spectra with FORS2 @ VLT and GMOS - N @ Gemini telescopes , covering the frequency range between 3600 Å and 9200 Å , for a total of 16 ULIRGs chosen on the basis of their high far - infrared luminosity ( L FIR > 10 12 L [UNK] ) and low redshift ( z < 0 . 1 ) . The main goal is to find TDGs that are expected to be found around interacting systems such as ULIRGs .The appearance of young stars in these objects would render them detectable through strong absorption lines like Hα or OII . However , we do not detect any considerable surplus of emission point flux over what can be described by galaxy formation happening within the host galaxy itself .This result suggests either that there are no TDGs associated with our sample of ULIRGs or that they are too dark and / or too bright to be identified using current technology .",
        "rewrite_text": "We present the findings of an optical spectroscopic investigation aimed at identifying candidates for tidal dwarf galaxies (TDGs) within a selected group of ultraluminous infrared galaxies (ULIRGs). Utilizing the FORS2 instrument at the Very Large Telescope (VLT) and the GMOS-N at the Gemini Observatory, we obtained spectral data spanning the wavelength range of 3600 Å to 9200 Å for a total of 16 ULIRGs. These galaxies were specifically chosen based on their significant far-infrared luminosity (L_FIR > 10^12 L_☉) and relatively low redshift (z < 0.1). The primary objective of this research is to locate TDGs, which are typically formed in the vicinity of interacting galaxies, such as ULIRGs. The presence of young stellar populations in these systems would be indicated by prominent emission lines, particularly Hα and OII. Contrary to our expectations, our analysis did not reveal any significant excess of emission point flux that could be attributed to the presence of TDGs. Instead, the observed spectral features align more closely with the ongoing star formation processes occurring within the host galaxies themselves. This outcome raises important questions regarding the existence of TDGs in our sample of ULIRGs. It suggests two possibilities: either there are no TDGs associated with these galaxies, or they may be too faint or too luminous to be detected with the current observational capabilities. Our findings contribute to the ongoing discourse on the formation and visibility of tidal dwarf galaxies in the context of galaxy interactions, highlighting the need for further studies to explore these intriguing celestial objects.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.714045207910317,
        "rewrite-fast-z-score": 0.8542421961772492
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  On the equilibrium morphology of systems drawn from spherical collapse experiments .\nAbstract:\nWe study the statistical properties of simulated dark matter halos in cosmological N-body simulations, focusing on their shapes and orientations with respect to each other. We find that these quantities are strongly correlated for pairs of halos separated by less than one virial radius (the region within which the density is roughly constant). This correlation persists even when we consider only those pairs whose mutual separation lies along the line-of-sight between them. The correlations can be understood as arising due to tidal forces exerted by neighboring halos. In particular, we show that the distribution of halo shapes depends sensitively upon whether or not they lie close to an axis of symmetry of the local gravitational potential field. Finally, we compare our results against observations of galaxy clusters obtained using weak lensing techniques. Our analysis suggests that the observed cluster morphologies may provide useful constraints on the nature of primordial fluctuations responsible for structure formation in the universe.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : On the stable morphology of structures taken from spherical collapse tests . Abstract : We research the statistical characteristics of virtual dark matter halos in cosmological N - bodies simulations , concentrating on their shapes and orientations with regard to each other .We see that these quantities are strongly correlated for pairs of halos separated by less than one virial diameter ( the territory within which the density is approximately constant ) . This coupling persists even when we study only those pairs whose mutual separation lies along the line - of - view between them .The correlations can be understood as occurring due to tidal forces exerted by adjacent halos . In particular , we find that the distribution of halo patterns depends sensitively upon whether or not they lay close to an axis of symmetry of the local gravity potential field .Finally , we compare our findings against measurements of galaxy galaxies collected using weak lensing methods . Our study implies that the observed cluster morphologies may provide useful limitations on the nature of primordial fluctuations involved for structure form in the universe .",
        "rewrite_text": "Title: On the Stable Morphology of Structures Derived from Spherical Collapse Tests\n\nAbstract: This study investigates the statistical properties of virtual dark matter halos generated from cosmological N-body simulations, with a specific focus on their shapes and mutual orientations. Our analysis reveals a strong correlation between these characteristics for pairs of halos that are separated by less than one virial diameter, which is defined as the region where the density remains approximately uniform. Notably, this correlation persists even when examining pairs whose separation aligns with the line of sight between them. We attribute these correlations to the tidal forces exerted by neighboring halos, which influence their structural properties. Furthermore, our findings indicate that the distribution of halo configurations is highly sensitive to their proximity to axes of symmetry within the local gravitational potential field. To validate our results, we compare our simulations with observational data of galaxy clusters obtained through weak lensing techniques. The implications of our research suggest that the morphologies observed in galaxy clusters can serve as valuable constraints on the nature of primordial fluctuations that played a role in the formation of structures in the universe. This work enhances our understanding of the interplay between dark matter halo dynamics and the large-scale structure of the cosmos, providing insights that could inform future studies in cosmology and astrophysics.",
        "ori-fast-z-score": -1.9409899176618914,
        "water-fast-z-score": 5.7564193416014815,
        "rewrite-fast-z-score": -0.41702882811414954
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Augmented Sparse Reconstruction of Protein Signaling Networks .\nAbstract:\nWe propose an algorithm for reconstructing the signaling network in which each node is associated with multiple time series data, and we assume that only some nodes are observed at any given time point. The proposed method consists of two steps: (1) estimating the state vector by solving a sparse reconstruction problem; (2) inferring the edge set using the estimated states as features. We show through numerical experiments on synthetic networks that our approach can accurately recover both the structure and parameters of the underlying network even when only partial information about the system is available. Our results also suggest that the performance of the proposed method depends strongly on how well the sparsity pattern of the true signal is captured by the observation matrix. Finally, we apply this method to analyze the dynamics of protein phosphorylation in yeast cells responding to heat shock stress. In particular, we identify several key proteins involved in regulating the response process. This work was supported by NIH grant R01GM084283-01A1",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Augmented Sparse Reconstruction of Protein Signaling Networks . Abstract : We suggest an algorithm for reconstructing the signaling network in which each node is associated with various time series information , and we suppose that only some nodes are observed at any certain time point .The proposed approach consists of two stages : ( 1 ) estimating the state vector by addressing a sparse reconstruction task ; ( 2 ) inferring the edge setting using the expected states as characteristics . We see through computational experiments on synthetic systems that our approach can accurately recover both the composition and parameters of the underlying network even when only partial knowledge about the system is accessible .Our results also suggest that the performance of the suggested method depends strongly on how perfectly the sparsity pattern of the true signal is captured by the observation vector . Finally , we apply this method to analyze the dynamics of protein phosphorylation in yeast cells responding to heat shock pressure .In particular , we identify several key proteins responsible in controlling the response process . This research was supported by NIH funding R01GM084283 - 01A1",
        "rewrite_text": "In this article, we present a novel algorithm designed for the reconstruction of protein signaling networks, where each node is linked to various time series data, with the understanding that only a subset of nodes can be observed at any given time. Our methodology unfolds in two distinct phases: the first phase involves estimating the state vector through a sparse reconstruction process, while the second phase focuses on inferring the network's edge configuration by utilizing the expected states as features. Through computational experiments conducted on synthetic systems, we demonstrate that our approach is capable of accurately recovering both the structure and parameters of the underlying signaling network, even when only limited information about the system is available. The findings indicate that the efficacy of our method is significantly influenced by the degree to which the sparsity pattern of the actual signal is represented in the observation vector. Furthermore, we apply our algorithm to investigate the dynamics of protein phosphorylation in yeast cells subjected to heat shock stress. This analysis allows us to pinpoint several critical proteins that play a pivotal role in mediating the cellular response to such stressors. This research was made possible through the support of NIH funding (R01GM084283 - 01A1), highlighting the importance of financial backing in advancing our understanding of complex biological systems. Overall, our work contributes to the field of systems biology by providing a robust framework for the reconstruction and analysis of signaling networks, with implications for understanding cellular responses in various contexts.",
        "ori-fast-z-score": -0.30779350562554625,
        "water-fast-z-score": 6.123724356957946,
        "rewrite-fast-z-score": 0.6163156344279367
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Network tomography based on 1-D projections .\nAbstract:\nWe propose an algorithm for network tomography that is able to reconstruct the internal structure of a network by using only one-dimensional (1-D) measurements, i.e., link counts between pairs of nodes in the network. The proposed method can be applied to any type of networks and does not require any prior knowledge about their topology or traffic patterns. We show how our approach can be used to estimate the number of active flows at each node as well as the amount of data transmitted over each flow. Our results are validated through extensive simulations performed with real Internet traces. Network tomography has been widely studied during recent years due to its potential applications in many areas such as computer security, quality-of-service provisioning, and traffic engineering  1  . In this context, it consists of estimating some properties of the network s internal state (such as the number of active flows per node or the amount of data transferred along each flow) by observing only external information (i.e., link-level statistics). This problem becomes particularly challenging when dealing with large-scale networks since the number of possible states grows exponentially with the size of the network  2  .\nIn order to overcome these limitations, several approaches have been recently proposed which exploit specific characteristics of the underlying network  3  , e.g., sparsity  4  -  6  , symmetry  7  , or regularity  8  . However, most existing methods assume either complete knowledge of the network topology  9 -  11  or accurate estimates of the traffic matrix  12  -  14  . Unfortunately, both assumptions may not hold in practice  15  , especially if we consider large and/or dynamic networks  16  . For example, in IP-based networks, the exact location of routers cannot always be determined  17  while the traffic matrix is usually unknown  18  . Moreover, even if the network topology were known, collecting all necessary information would still be impractical because of scalability issues  19  . Finally, obtaining accurate estimates of the traffic...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Network tomography focused on 1 - D projections . Abstract : We suggest an algorithm for channel tomography that is able to reconstruct the internal structure of a network by using only one - dimensional ( 1 - D ) observations , i . e . , link counts between pairs of vertices in the network .The proposed approach can be applied to any type of networks and does not require any earlier knowledge about their topology or traffic behavior . We see how our approach can be used to estimate the number of active flows at each node as also as the quantity of content distributed over each flow .Our results are validated through ongoing simulations conducted with real Internet traces . Network tomography has been widely examined during recent years owing to its potential applications in multiple fields such as data security , quality - of - service provisioning , and route management 1 .In this sense , it consists of estimating some properties of the network s internal state ( such as the number of active flows per node or the quantity of data transferred along each flow ) by observing only external information ( i . e . , link - level statistics ) . This problem remains particularly challenging when dealing with large - scale networks since the quantity of possible states tends exponentially with the length of the network 2 .In try to overcome these limitations , various approaches have been lately developed which use particular attributes of the underlying network 3 , e . g . , sparsity 4 - 6 , symmetry 7 , or regularity 8 . However , most existing techniques assume either complete understanding of the network topology 9 - 11 or accurate calculations of the traffic matrix 12 - 14 .Unfortunately , both conclusions may not hold in practice 15 , particularly if we study huge and / or dynamic networks 16 . For instance , in IP - based networks , the exact location of routers never always be determined 17 while the traffic matrix is usually unknown 18 .Moreover , even if the traffic topology were known , compiling all necessary data would still be impractical because of scalability concerns 19 . Finally , obtaining adequate accounts of the traffic . . .",
        "rewrite_text": "**Title: Network Tomography Focused on 1-D Projections**\n\n**Abstract:** In this article, we introduce a novel algorithm for channel tomography that enables the reconstruction of a network's internal structure using solely one-dimensional (1-D) observations, specifically link counts between pairs of vertices. This innovative approach is versatile and applicable to various types of networks, requiring no prior knowledge of their topology or traffic patterns. Our method facilitates the estimation of active flows at each node, as well as the volume of content transmitted across these flows. The effectiveness of our algorithm is demonstrated through simulations utilizing real Internet traffic data.\n\nNetwork tomography has garnered significant attention in recent years due to its potential applications across diverse domains, including data security, quality-of-service management, and routing optimization. The core of network tomography involves inferring the internal state of a network—such as the number of active flows per node and the amount of data transferred—by analyzing external observations, particularly link-level statistics. This task becomes increasingly complex in large-scale networks, where the number of possible states grows exponentially with network size.\n\nTo address these challenges, recent methodologies have emerged that leverage specific characteristics of the underlying network, such as sparsity, symmetry, and regularity. However, many existing techniques rely on a comprehensive understanding of the network topology or precise calculations of the traffic matrix. These assumptions often do not hold true in practical scenarios, especially in the context of large or dynamic networks. For example, in IP-based networks, the exact positions of routers may be indeterminate, and the traffic matrix is frequently unknown. Even if the traffic topology were accessible, gathering all necessary data remains impractical due to scalability issues. Our work aims to bridge these gaps by providing a robust framework for network analysis that operates effectively under conditions of uncertainty and limited information.",
        "ori-fast-z-score": -0.7669649888473704,
        "water-fast-z-score": 7.264831572567789,
        "rewrite-fast-z-score": -0.5353033790313108
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Anonymity in the Wild: Mixes on unstructured networks .\nAbstract:\nWe study anonymity systems that operate over unstructured networks, such as Tor and Mixminion. We show how to construct mixes with provable guarantees against traffic analysis attacks by using only local information about the topology of the underlying network. Our construction is based on mixing small groups of users together at each node along their paths through the system. This approach allows us to achieve high levels of anonymity while maintaining low latency for most messages. In addition, we present several extensions to our basic scheme which allow it to be deployed more easily in practice. Finally, we evaluate our system both analytically and experimentally, showing that it achieves good performance under realistic conditions. Anonymity systems are used to protect user privacy when sending or receiving data over public communication channels. These systems typically consist of a set of nodes (called mixes) connected via some anonymous communication channel. Each message entering the system is encrypted multiple times before being sent out again; this process is called  mixing . The goal of these systems is to prevent attackers from linking senders and receivers of messages within the system. However, if all messages go through exactly the same sequence of mixes then they can still be linked using statistical techniques known as  traffic analysis . Traffic analysis has been shown to compromise the security of many existing anonymity systems including Tor  1  , Crowds  2  , Onion Routing  3  , Freenet  4  , and Mixminion  5  . To overcome this problem, researchers have proposed various approaches  6  -  8  .\nIn this work, we focus on anonymity systems operating over unstructured networks  9 -  11  . Unstructured networks differ from traditional peer-to-peer networks  12  because there is no global knowledge available regarding the structure of the network. Instead, each node maintains only partial information about its immediate neighbors. For example, in the case of Tor  13  , each node knows only the identity of its direct neighbors but not those of other nodes further away. As another example, in Mixminion  14  , each node knows only whether two given nodes are directly connected or not",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Anonymity in the Wild : Mixes on unstructured networks . Abstract : We research anonymity systems that function over unstructured networks , such as Tor and Mixminion .We see how to build mixes with provable assurance against congestion analysis assaults by using only local information about the topology of the underlying network . Our build is based on mixing small communities of consumers together at each node along their paths through the scheme .This method enables us to achieve high levels of anonymity while maintaining low latency for most messages . In addition , we present many extensions to our general plan which allow it to be deployed more easily in practice .Finally , we assess our system both analytically and experimentally , showing that it achieves good efficiency under acceptable circumstances . Anonymity systems are applied to shield user secrecy when sent or sending data over public communication streams .These systems often consist of a setting of nodes ( named mixes ) connected via some anonymous transmission channel . Each signal entering the system is encrypted several twice before being sent out again ; this process is dubbed mix .The goal of these systems is to prohibit attackers from linking senders and receivers of communication within the system . However , if all signals go through exactly the same pattern of mixes then they can always be connected using statistical method known as traffic analysis .Traffic analysis has been shown to compromise the safety of several current anonymity systems including Tor 1 , Crowds 2 , Onion Routing 3 , Freenet 4 , and Mixminion 5 . To solve this challenge , researchers have proposed several methods 6 - 8 .In this project , we focus on anonymity systems operating over unstructured networks 9 - 11 . Unstructured networks differ from standard peer - to - peer systems 12 because there is no international knowledge accessible regarding the composition of the organization .Instead , each node preserves only partial knowledge about its immediate relatives . For instance , in the case of Tor 13 , each node remembers only the identity of its direct neighbors but not those of other vertices further away .As another example , in Mixminion 14 , each node remembers only whether two given nodes are directly connected or not",
        "rewrite_text": "**Title:** Anonymity in the Wild: Mixes on Unstructured Networks\n\n**Abstract:** This study investigates anonymity systems that operate over unstructured networks, exemplified by platforms such as Tor and Mixminion. We propose a method for constructing mixes that provide provable protection against congestion analysis attacks by leveraging only local information regarding the network's topology. Our approach involves aggregating small groups of users at each node along their communication paths, which facilitates the attainment of high anonymity levels while ensuring low latency for the majority of messages. Furthermore, we introduce several extensions to our framework that enhance its practical deployment. \n\nWe conduct both analytical and experimental evaluations of our system, demonstrating its efficiency under various conditions. Anonymity systems are crucial for safeguarding user privacy during data transmission over public communication channels. Typically, these systems consist of a network of nodes, referred to as mixes, interconnected through anonymous transmission channels. Each incoming message is subjected to multiple layers of encryption before being dispatched, a process known as mixing. The primary objective of these systems is to prevent adversaries from correlating senders and receivers within the network.\n\nHowever, if all messages traverse the same sequence of mixes, they become vulnerable to statistical techniques, such as traffic analysis, which can reveal sender-receiver relationships. This vulnerability has been documented in several existing anonymity systems, including Tor, Crowds, Onion Routing, Freenet, and Mixminion. To address this issue, various strategies have been proposed in the literature. Our research specifically targets anonymity systems functioning over unstructured networks, which differ from traditional peer-to-peer systems due to the lack of global awareness regarding the network's structure. In such networks, each node retains only limited knowledge about its immediate neighbors. For instance, in the Tor network, nodes are aware only of their direct connections, while in Mixminion, nodes can only ascertain whether two specific nodes are directly linked. This study aims to enhance the robustness of anonymity systems in these challenging environments.",
        "ori-fast-z-score": -0.3104602102825331,
        "water-fast-z-score": 9.615384615384615,
        "rewrite-fast-z-score": -0.24413653763134782
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Gamma-ray probe of cosmic-ray pressure in galaxy clusters and cosmological implications .\nAbstract:\nWe present the first measurement of the gamma-ray emission associated with the hot gas in galaxy clusters using data collected by Fermi Large Area Telescope (LAT). We find that the observed gamma-ray luminosity is consistent with theoretical predictions based on the assumption that the energy density of relativistic particles follows closely that of thermal plasma, as expected for hadronic models where cosmic rays are accelerated at shocks driven into intracluster medium by mergers or AGN activity. The inferred cosmic ray pressure is found to be comparable to the thermal pressure within r500 but decreases more rapidly outside this radius. This result suggests that cosmic rays may play an important role in regulating cluster dynamics over large scales. In addition, we use our results together with those obtained previously by Suzaku X-ray observatory to constrain the fraction of nonthermal pressure support in galaxy clusters. Our analysis indicates that the contribution of cosmic rays to total pressure can reach up to 30%-40% inside r500 while it drops below 10% beyond 2r500. Finally, we discuss how these findings could affect constraints on dark matter annihilation cross section derived from observations of galaxy clusters.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Gamma - ray investigation of cosmic - ray pressure in galaxy clusters and cosmological impacts . Abstract : We report the first measurement of the alpha - ray radiation attributed with the hot gas in galaxy clusters using data received by Fermi Large Area Telescope ( LAT ) .We see that the reported gamma - ray luminosity is compatible with theoretical estimates based on the assumption that the power concentration of relativistic objects parallels closely that of thermal plasma , as anticipated for hadronic models where cosmic rays are accelerated at shocks driven into intracluster medium by mergers or AGN activity . The inferred cosmic ray tension is found to be comparable to the thermal tension within r500 but decreases more slowly outside this radius .This result suggests that cosmic rays may play an important role in controlling cluster structure over large scales . In addition , we using our findings together with those published previously by Suzaku X - ray observatory to constrain the fraction of nonthermal force support in galaxy galaxies .Our study implies that the impact of cosmic rays to total pressure can reach up to 30 % - 40 % inside r500 while it sinks below 10 % beyond 2r500 . Finally , we investigate how these results could affect constraints on dark matter annihilation cross section obtained from measurements of galaxy galaxies .",
        "rewrite_text": "We present the inaugural measurement of gamma-ray emissions associated with the hot gas in galaxy clusters, utilizing data from the Fermi Large Area Telescope (LAT). Our findings indicate that the observed gamma-ray luminosity aligns well with theoretical predictions, which are based on the premise that the distribution of relativistic particles closely mirrors that of the thermal plasma. This correlation is particularly anticipated in hadronic models, where cosmic rays are accelerated at shock fronts generated by mergers or active galactic nucleus (AGN) activity within the intracluster medium. \n\nThe analysis reveals that the cosmic ray pressure is comparable to the thermal pressure within the radius r500, but it exhibits a slower decline beyond this boundary. This observation implies that cosmic rays may significantly influence the structural dynamics of galaxy clusters on larger scales. Furthermore, we integrate our results with previous data from the Suzaku X-ray observatory to estimate the contribution of nonthermal forces in galaxy clusters. Our research suggests that the contribution of cosmic rays to the total pressure can be as high as 30% to 40% within r500, while it diminishes to below 10% beyond 2r500.\n\nAdditionally, we explore the implications of these findings on the constraints regarding the dark matter annihilation cross-section derived from observations of galaxy clusters. By understanding the role of cosmic rays in the overall pressure balance, we can refine the models used to interpret dark matter interactions, potentially leading to new insights in cosmology. This study underscores the importance of cosmic rays in the evolution and structure of galaxy clusters, highlighting their relevance in both astrophysical and cosmological contexts.",
        "ori-fast-z-score": 0.1889822365046136,
        "water-fast-z-score": 6.425396041156863,
        "rewrite-fast-z-score": 0.9610744623271417
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Rapid Diffusion of dipolar order enhances dynamic nuclear polarization .\nAbstract:\nWe report on the rapid diffusion of dipolar order in liquid crystals, which is responsible for an enhanced nuclear spin-lattice relaxation rate and thus for a higher degree of dynamic nuclear polarization (DNP). The DNP effect was studied by means of electron paramagnetic resonance spectroscopy at X-band frequencies using a home-built high-field spectrometer equipped with a commercial microwave bridge operating at 2.5 GHz. We found that the enhancement factor increases strongly when approaching the nematic-isotropic phase transition temperature TNI = 35 °C. This finding can be explained by assuming that the molecular mobility decreases rapidly as TNI is approached. In addition to this observation we find that the maximum achievable enhancement factor depends critically on the sample geometry. For samples having a thickness d < 1 mm it reaches values up to 100 times larger than those observed for thicker samples. These results are important for future applications of DNP in NMR experiments performed under extreme conditions such as low temperatures or strong magnetic fields.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Rapid Diffusion of dipolar order enhances dynamic nuclear polarization . Abstract : We report on the quick diffusion of dipolar order in liquid crystals , which is responsible for an increase nuclear spin - lattice relaxation speed and therefore for a higher degree of static nuclear polarization ( DNP ) .The DNP phenomenon was studied by means of electron paramagnetic resonance spectroscopy at X - band frequencies using a home - installed high - field spectrometer equipped with a commercial microwave bridge operating at 2 . 5 GHz . We showed that the enhancement component increases strongly when approaching the nematic - isotropic phase change temperature TNI = 35 °C .This finding can be described by assuming that the molecular mobility decreases quickly as TNI is neared . In addition to this observation we find that the maximum achievable enhancement factor relies critically on the sample geometry .For specimens having a length d < 1 mm it hits values up to 100 times bigger than those observed for thicker specimen . These data are important for future applications of DNP in NMR experiments conducted under extreme circumstances such as low temperatures or strong magnetic fields .",
        "rewrite_text": "Title: Rapid Diffusion of Dipolar Order Enhances Dynamic Nuclear Polarization\n\nAbstract: In this study, we investigate the rapid diffusion of dipolar order in liquid crystals and its significant impact on nuclear spin-lattice relaxation rates, leading to an enhanced degree of dynamic nuclear polarization (DNP). Utilizing electron paramagnetic resonance (EPR) spectroscopy at X-band frequencies, we conducted our experiments with a high-field spectrometer that we assembled, which is equipped with a commercial microwave bridge operating at 2.5 GHz. Our results reveal a pronounced increase in the enhancement component as the system approaches the nematic-isotropic phase transition temperature (TNI = 35 °C). This behavior can be attributed to a rapid decrease in molecular mobility near TNI, which facilitates the diffusion of dipolar order. Furthermore, we observed that the maximum enhancement factor achievable is highly dependent on the geometry of the sample. Specifically, for samples with a length less than 1 mm, we recorded enhancement factors that are up to 100 times greater than those measured in thicker samples. These findings underscore the critical role of sample dimensions in optimizing DNP effects. The implications of our research are significant for advancing the application of DNP in nuclear magnetic resonance (NMR) experiments, particularly in extreme conditions such as low temperatures or high magnetic fields. Our work paves the way for improved methodologies in DNP, potentially enhancing the sensitivity and resolution of NMR techniques in various scientific fields.",
        "ori-fast-z-score": -0.30779350562554625,
        "water-fast-z-score": 4.898979485566357,
        "rewrite-fast-z-score": 0.7770286898858113
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  General Properties of Quantum Zero-Knowledge Proofs .\nAbstract:\nQuantum zero-knowledge proofs are quantum protocols that allow one party to prove knowledge of some secret information without revealing any other information about it.  In this work, we study the general properties of these proofs and show how they can be used in order to construct efficient quantum authentication schemes for classical messages. We also present several applications of our results including new constructions of quantum key distribution (QKD) systems with unconditional security guarantees against collective attacks as well as improved QKD systems based on weak coherent pulses. Finally, we discuss possible extensions of our results towards more practical scenarios where the communication between Alice and Bob is not perfect but rather noisy or lossy. Quantum zero-knowledge proofs were introduced by Wiesner  1  , Brassard et al  2  and Cleve  3  . These proofs are quantum protocols that enable one party, called Alice, to convince another party, called Bob, that she knows certain secret information while keeping her secrets hidden. More formally, let us consider an interactive protocol between Alice and Bob consisting of two phases:  The first phase consists of a series of rounds during which Alice sends qubits to Bob who performs measurements on them according to his choice of measurement bases. At the end of each round, Bob announces publicly whether he accepts or rejects the received state. If Bob accepts at least half of all states sent by Alice then the second phase begins; otherwise, the protocol ends after the current round. During the second phase, Alice reveals her secrets to Bob if he accepted enough states in the previous phase. Otherwise, both parties abort the protocol. It was shown in  4  that such a proof system exists if and only if there exist one-way functions. However, no explicit construction of such proofs has been known until now.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : General Properties of Quantum Zero - Knowledge Proofs . Abstract : Quantum zero - knowledge proofs are quantum mechanisms that enable one party to confirm knowledge of some secret information without revealing any other information about it .In this research , we study the general properties of these proofs and understand how they can be used in order to build efficient quantum authentication schemes for classical messages . We also present many applications of our findings including novel constructions of quantum key distribution ( QKD ) networks with unconditional security assurance against collective assaults as well as improved QKD solutions based on weak coherent pulses .Finally , we explain possible extensions of our findings towards more realistic scenarios where the interaction between Alice and Bob is not good but rather noisy or lossy . Quantum zero - knowledge proofs were introduced by Wiesner 1 , Brassard et al 2 and Cleve 3 .These proofs are particle protocols that enable one party , called Alice , to convince another party , called Bob , that she knows particular hidden information while maintaining her secrets hidden . More technically , let us consider an interactive procedure between Alice and Bob composed of two phases : The first phase involves of a sequence of rounds during which Alice takes qubits to Bob who takes measurements on them based to his choice of measurement bases .At the end of each round , Bob declares publicly whether he agrees or accepts the received state . If Bob accepts at least half of all states sent by Alice then the second phase begins ; otherwise , the protocol ends after the present stage .During the second phase , Alice reveals her secrets to Bob if he offered enough states in the previous phase . Otherwise , both parties abort the protocol .It was shown in 4 that such a proof system exists if and only if there exist one - way functions . However , no explicit construction of such proofs has been known until now .",
        "rewrite_text": "**Title: General Properties of Quantum Zero-Knowledge Proofs**\n\n**Abstract:** Quantum zero-knowledge proofs represent a groundbreaking advancement in quantum cryptography, allowing one party (Alice) to demonstrate her knowledge of a secret to another party (Bob) without disclosing any additional information about that secret. This study delves into the fundamental characteristics of quantum zero-knowledge proofs and explores their potential applications in developing efficient quantum authentication protocols for classical messages. We highlight various applications stemming from our research, including innovative designs for quantum key distribution (QKD) networks that offer unconditional security against collective attacks, as well as enhancements to QKD methods utilizing weak coherent pulses. Furthermore, we discuss the implications of our findings for more practical scenarios where the communication between Alice and Bob may be compromised by noise or loss. \n\nThe concept of quantum zero-knowledge proofs was first introduced by notable researchers such as Wiesner, Brassard, and Cleve. These protocols involve an interactive process between Alice and Bob, structured in two distinct phases. In the initial phase, Alice sends qubits to Bob, who performs measurements based on his selected measurement bases. After each round, Bob publicly indicates whether he accepts the state he received. If Bob accepts at least half of the states sent by Alice, the protocol progresses to the second phase; otherwise, it terminates. During the second phase, Alice discloses her secret to Bob, contingent upon his acceptance of a sufficient number of states in the previous phase. If not, the protocol is aborted. Previous research has established that such a proof system is viable if and only if one-way functions exist. However, an explicit construction of these proofs has remained elusive until now. This work aims to bridge that gap and pave the way for practical implementations of quantum zero-knowledge proofs in secure communication systems.",
        "ori-fast-z-score": -0.816496580927726,
        "water-fast-z-score": 6.042074698865172,
        "rewrite-fast-z-score": 0.7745966692414834
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Classical approach to the graph isomorphism problem using quantum walks .\nAbstract:\nWe present an algorithm for solving the Graph Isomorphism Problem (GIP) based on Quantum Walks and Grover s search algorithm. The proposed method is inspired by the classical algorithms that use random walks, but it uses Grover s operator instead of the Hadamard matrix in order to speed up the process. We show how this new technique can be used to solve GIP with high probability when the number of vertices in both graphs are equal or differ at most one unit. Finally we compare our results against other state-of-the-art methods. In recent years there has been growing interest in developing efficient solutions to problems related to computational complexity theory such as the Graph Isomorphism Problem(GIP). This problem consists of determining whether two given graphs are isomorphic or not; i.e., if they have the same structure regardless of their labels. \n \n Classical approaches to solve GIP usually rely on Random Walks techniques combined with some heuristics. However these methods require exponential time in the worst case scenario. On the other hand, Quantum Algorithms provide polynomial-time solutions to many NP-complete problems including GIP. These algorithms exploit the superposition principle which allows them to explore all possible states simultaneously. For example, Shor s Algorithm solves integer factorization in polynomial time while Grover s Search finds any item within a database in quadratic time.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Classical approach to the graph isomorphism question involving quantum walks . Abstract : We present an algorithm for solving the Graph Isomorphism Problem ( GIP ) based on Quantum Walks and Grover s search algorithm .The proposed approach is influenced by the classical techniques that use random tours , but it utilizes Grover s operator rather of the Hadamard vector in order to faster up the process . We see how this new technique can be used to solve GIP with high probability when the number of vertices in both graphs are equal or differ at most one unit .Finally we compare our findings against other state - of - the - art methods . In past decades there has been growing interest in building successful solutions to problems related to computational complexity analysis such as the Graph Isomorphism Problem ( GIP ) .This problem consists of determining whether two given graphs are isomorphic or not ; i . e . , if they have the same structure regardless of their labels . Classical approaches to solve GIP usually relies on Random Walks techniques combined with some heuristics .However these algorithms involve exponential time in the worst case situations . On the other hand , Quantum Algorithms provide polynomial - time solutions to many NP - perfect issues including GIP .These methods incorporate the superposition concept which allows them to examine all possible states simultaneously . For instance , Shor s Algorithm solves integer factorization in polynomial time while Grover s Search finds any element within a computer in quadratic time .",
        "rewrite_text": "We introduce a novel algorithm aimed at addressing the Graph Isomorphism Problem (GIP) by leveraging Quantum Walks in conjunction with Grover's search algorithm. This innovative approach draws inspiration from classical methods that employ random tours but enhances the efficiency of the process by utilizing Grover's operator instead of the traditional Hadamard vector. Our findings demonstrate that this new technique can effectively solve the GIP with a high probability, particularly when the number of vertices in the two graphs is either equal or differs by no more than one. \n\nThe Graph Isomorphism Problem has garnered significant attention over the past few decades, as researchers strive to develop effective solutions to challenges associated with computational complexity. The essence of GIP lies in determining whether two graphs are isomorphic, meaning they share the same structural properties irrespective of the labels assigned to their vertices. Traditional methods for tackling GIP often rely on Random Walk techniques supplemented by various heuristics; however, these classical algorithms can exhibit exponential time complexity in the worst-case scenarios.\n\nIn contrast, Quantum Algorithms offer polynomial-time solutions for numerous NP-complete problems, including GIP. These quantum methods exploit the principle of superposition, enabling them to evaluate all potential states concurrently. For example, Shor's Algorithm achieves polynomial-time solutions for integer factorization, while Grover's Search algorithm can locate any element within a dataset in quadratic time. By comparing our results with existing state-of-the-art techniques, we highlight the advantages of our quantum-based approach, which not only enhances the efficiency of solving the GIP but also contributes to the broader discourse on computational complexity and quantum computing.",
        "ori-fast-z-score": -0.5669467095138409,
        "water-fast-z-score": 5.550253123463223,
        "rewrite-fast-z-score": -0.7126966450997984
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Equivalence of three-dimensional spacetimes .\nAbstract:\nThe equivalence principle is one of the most important concepts in general relativity, and it states that all physically equivalent solutions to Einstein s field equations are locally indistinguishable.  In this article we show how the concept can be extended to three dimensions by considering two different classes of exact solutions to the vacuum Einstein field equations with cosmological constant.   The first class consists of spatially homogeneous Bianchi type IX models which have been studied extensively over many years as possible candidates for describing our universe at early times when its geometry was close to being flat.   We prove that these models are globally diffeomorphic (homeomorphic) if their spatial volume forms agree up to sign.   ... This article shows how the concept of local physical equivalence between solutions to Einstein s field equation can be extended to three-dimensions. Two different classes of exact solutions are considered; spatially homogeneous Bianchi Type IX models and spherically symmetric Schwarzschild-de Sitter solutions. It is shown that both types of solution are globally diffeomorphic under certain conditions on their respective volume forms.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Equivalence of three - dimensional spacetimes . Abstract : The equivalence principle is one of the most important concepts in general relativity , and it states that all physically equivalent solutions to Einstein s field equations are locally indistinguishable .In this article we give how the notion can be enlarged to three dimensions by examining two different categories of precise solutions to the vacuum Einstein field equations with cosmological constant . The first class consists of spatially homogeneous Bianchi class IX models which have been studied frequently over numerous years as possible candidates for describing our universe at first days when its topology was close to being flat .We prove that these models are globally diffeomorphic ( homeomorphic ) if their spatial volume shapes accord up to sign . . . .This page demonstrates how the idea of local mechanical equivalence between solutions to Einstein s field equation can be generalized to three - dimensions . Two different categories of precise solutions are considered ; spatially homogeneous Bianchi Type IX models and spherically spherical Schwarzschild - de Sitter systems .It is demonstrated that both types of solution are globally diffeomorphic under certain conditions on their respective volume shapes .",
        "rewrite_text": "Title: Equivalence of Three-Dimensional Spacetimes\n\nAbstract: The equivalence principle is a fundamental tenet of general relativity, asserting that all physically equivalent solutions to Einstein's field equations are locally indistinguishable. In this article, we explore the extension of this principle to three-dimensional spacetimes by analyzing two distinct categories of exact solutions to the vacuum Einstein field equations with a cosmological constant. The first category encompasses spatially homogeneous Bianchi class IX models, which have been extensively studied as potential representations of the early universe, particularly during its initial phases when its topology approached flatness. We demonstrate that these models exhibit global diffeomorphism (homeomorphism) if their spatial volume shapes are congruent up to a sign. Furthermore, we investigate another category of solutions, specifically the spherically symmetric Schwarzschild-de Sitter systems. Our findings reveal that both classes of solutions can be shown to be globally diffeomorphic under specific conditions related to their volume shapes. This work illustrates how the concept of local mechanical equivalence among solutions to Einstein's field equations can be effectively generalized to three-dimensional contexts, thereby enriching our understanding of the geometric and topological properties of spacetimes in general relativity. Through this analysis, we contribute to the ongoing discourse on the nature of spacetime and its implications for cosmological models, providing insights that may have significant ramifications for theoretical physics and our comprehension of the universe's structure.",
        "ori-fast-z-score": -1.6081688022566922,
        "water-fast-z-score": 4.264014327112209,
        "rewrite-fast-z-score": 1.9291577137538762
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  In search of dying radio sources in the local universe .\nAbstract:\nWe have carried out an extensive survey for dying radio galaxies at 1.4 GHz using the Australia Telescope Compact Array (ATCA). The sample consists of all known FRII-type radio galaxies with redshifts less than 0.1 and angular extents greater than 10 arcsec, excluding those that are too close to bright background sources or other nearby radio galaxies. We find no evidence for any significant number of such objects within our sample. This is consistent with previous studies which suggest that most radio galaxies die before they can be detected as extended structures on arcminute scales. However we do detect several examples of compact steep spectrum emission associated with previously identified dying radio galaxies. These may represent the final stages of evolution prior to complete cessation of nuclear activity. In addition there appears to be some correlation between the presence of these features and the optical properties of their host galaxies. Finally we present new observations of two well studied dying radio galaxies, 3C31 and NGC315, showing that both continue to evolve over time.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : In quest of dying radio sources in the local universe . Abstract : We have carried out an extensive survey for dead wireless galaxies at 1 . 4 GHz using the Australia Telescope Compact Array ( ATCA ) .The sample consists of all known FRII - class radio stars with redshifts less than 0 . 1 and spatial extents greater than 10 arcsec , excluding those that are too close to dark background sources or other nearby radio stars . We see no evidence for any considerable amount of such objects within our sample .This is consistent with previous research which predict that most radio stars dying before they can be spotted as extended formations on arcminute scales . However we do determine several examples of compact steep spectrum emission associated with already identified dying radio objects .These may indicate the last phases of evolved prior to complete cessation of nuclear activity . In addition there seems to be some correlation between the presence of these structures and the optical properties of their host galaxies .Finally we present new experiments of two good observed dying radio objects , 3C31 and NGC315 , showing that both continue to evolve over time .",
        "rewrite_text": "In this study, we conducted a comprehensive survey aimed at identifying dying radio sources within the local universe, utilizing the Australia Telescope Compact Array (ATCA) at a frequency of 1.4 GHz. Our investigation focused on a sample comprising all known FRII-class radio galaxies with redshifts below 0.1 and spatial extents exceeding 10 arcseconds. We deliberately excluded objects that were too close to dark background sources or other nearby radio galaxies to ensure the integrity of our findings. The results of our survey revealed no significant evidence for a substantial number of dying radio sources within the selected sample. This outcome aligns with previous studies that suggest most radio galaxies may cease to exist before they can be detected as extended structures on arcminute scales.\n\nDespite the lack of numerous dying sources, we identified several instances of compact steep spectrum emissions linked to previously recognized dying radio galaxies. These emissions may represent the final stages of evolution before the complete shutdown of nuclear activity in these objects. Furthermore, our analysis indicates a potential correlation between the characteristics of these emissions and the optical properties of their host galaxies, suggesting an intricate relationship between radio and optical phenomena in the context of galaxy evolution.\n\nAdditionally, we present new observational data on two well-studied dying radio sources, 3C31 and NGC 315, which demonstrate ongoing evolution over time. These findings contribute to our understanding of the life cycle of radio galaxies and highlight the complexities involved in identifying and characterizing dying radio sources in the local universe. Overall, our research underscores the need for continued exploration of radio sources to better comprehend their evolutionary pathways and the factors influencing their demise.",
        "ori-fast-z-score": -0.6060915267313265,
        "water-fast-z-score": 6.0,
        "rewrite-fast-z-score": 2.0619652471058063
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Chaos and Symmetry in String Cosmology .\nAbstract:\nWe study the dynamics of string cosmologies with nontrivial dilaton potentials, focusing on their chaotic behavior. We show that for certain classes of potentials there are regions where trajectories can be trapped by unstable fixed points or periodic orbits. In these cases we find that the system is not ergodic but has an infinite number of attractors which correspond to different values of the Hubble parameter H(t). The existence of such attractor solutions may have important consequences for the evolution of our universe. For example, it could explain why the present value of H(t) differs so much from its initial value at t = 0. It also provides a possible explanation for the observed flatness problem since the volume V (t) grows exponentially fast during inflation while the energy density decreases as 1/V (t).\nThe results presented here were obtained using numerical methods based on the fourth-order Runge-Kutta algorithm combined with Newton s method for finding roots.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Chaos and Symmetry in String Cosmology . Abstract : We research the dynamics of string cosmologies with nontrivial dilaton potentials , concentrating on their chaotic dynamics .We see that for particular categories of potentials there are areas where trajectories can be trapped by weak fixed points or periodic orbits . In these circumstances we find that the system is not ergodic but has an endless number of attractors which belong to different values of the Hubble parameter H ( t ) .The existence of such attractor solutions may have important implications for the evolution of our universe . For instance , it could explain why the present value of H ( t ) changes so much from its initial value at t = 0 .It additionally offers a possible reason for the seen flatness problem since the volume V ( t ) rises exponentially rapidly during inflation while the electricity density decreases as 1 / V ( t ) . The results presented here were obtained using numerical technique based on the fourth - order Runge - Kutta algorithm coupled with Newton s method for finding roots .",
        "rewrite_text": "In this study, we investigate the dynamics of string cosmologies characterized by complex dilaton potentials, with a particular focus on their chaotic behavior. Our analysis reveals that certain classes of these potentials exhibit regions where trajectories can become entrapped by weak fixed points or periodic orbits. Under these conditions, we observe that the system does not exhibit ergodicity; instead, it possesses an infinite number of attractors corresponding to various values of the Hubble parameter H(t). The presence of such attractor solutions could have significant implications for the cosmological evolution of our universe. For example, this phenomenon may help elucidate the substantial variation observed in the current value of H(t) compared to its initial value at t = 0. Furthermore, it provides a potential explanation for the flatness problem, as the volume V(t) experiences exponential growth during the inflationary phase, while the energy density diminishes in proportion to 1/V(t). The findings presented in this article were derived through numerical methods, specifically employing the fourth-order Runge-Kutta algorithm in conjunction with Newton's method for root-finding. This comprehensive approach allows us to explore the intricate dynamics of string cosmologies and their implications for understanding the fundamental aspects of our universe's evolution.",
        "ori-fast-z-score": -0.6882472016116852,
        "water-fast-z-score": 4.672383634845156,
        "rewrite-fast-z-score": -0.5184758473652127
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  On Pure Spinor Superfield Formalism .\nAbstract:\nIn this article, we present the pure spinor superfield formalism for superstring theory in ten dimensions and its application to the Green-Schwarz action. We show that the pure spinor superfields are related with the usual superfields by some gauge transformations. The BRST transformation is also introduced as an operator acting on these superfields. In addition, we discuss how to construct the covariant derivatives which act on the superfields. Finally, we give several examples of calculations using our formalism. This work was supported by JSPS KAKENHI Grant Number JP16K05481. 1 Introduction Superstring theories have been studied extensively since they were proposed about thirty years ago  1  . They provide us not only with a consistent quantum gravity but also with various phenomenological models such as D-branes  2  , M-theory  3  , etc.. However, it seems difficult to quantize them exactly because their actions contain infinite number of fields. Therefore, many attempts have been made so far to find suitable formulations for superstrings  4  .\nOne of the most successful approaches is the light-cone formulation  5  -  8  where one can treat all the degrees of freedom at once without any truncation or approximation  9  . Another approach is based on the background field method  10  -  12  . It has been shown recently that both methods lead to equivalent results  13  -  15  . On the other hand, there exists another interesting approach called the pure spinor formalism  16  -  20  . Although it does not seem easy to apply directly to the full string theory  21  , it provides useful information on the structure of superstring theories  22  -  26  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : On Pure Spinor Superfield Formalism . Abstract : In this article , we present the pure spinor superfield formalism for superstring theory in ten dimensions and its application to the Green - Schwarz effect .We see that the pure spinor superfields are related with the usual superfields by some gauge transformations . The BRST transformation is also introduced as an operator acting on these superfields .In addition , we talk how to create the covariant derivatives which act on the superfields . Finally , we give many instance of calculations use our formalism .This project was supported by JSPS KAKENHI Grant Number JP16K05481 . 1 Introduction Superstring schemes have been studied thoroughly since they were introduced about thirty years previously 1 .They give us not only with a consistent quantum gravitational but also with various phenomenological models such as D - branes 2 , M - theory 3 , etc . . However , it appears impossible to quantize them precisely because their actions involve arbitrary number of fields .Therefore , long attempts have been made so far to find satisfactory formulations for superstrings 4 . One of the most popular methods is the light - cone formulation 5 - 8 where one can treat all the degrees of liberty at once without any truncation or approximation 9 .Another approach is based on the background field method 10 - 12 . It has been shown lately that both approaches lead to equivalent results 13 - 15 .On the other hand , there exists another important approach dubbed the pure spinor formalism 16 - 20 . Although it does not appear easier to apply directly to the full string theory 21 , it gives valuable info on the formation of superstring theories 22 - 26 .",
        "rewrite_text": "In this article, we explore the pure spinor superfield formalism as it pertains to superstring theory in ten dimensions, with a specific focus on its implications for the Green-Schwarz effect. We establish that pure spinor superfields can be connected to conventional superfields through specific gauge transformations. Additionally, we introduce the BRST transformation, which functions as an operator on these superfields, enhancing our understanding of their dynamics. The construction of covariant derivatives that operate on the superfields is also discussed, providing a comprehensive framework for their manipulation. Throughout the article, we present numerous examples of calculations that utilize our formalism, demonstrating its effectiveness and versatility. This research was supported by the Japan Society for the Promotion of Science (JSPS) under Grant Number JP16K05481.\n\nThe study of superstring theories has been extensive since their inception approximately thirty years ago. These theories not only offer a consistent framework for quantum gravity but also lead to various phenomenological models, including D-branes and M-theory. However, the quantization of superstring theories remains a challenge due to the presence of an arbitrary number of fields in their actions. Consequently, significant efforts have been made to develop satisfactory formulations for superstrings. Among the most widely used methods is the light-cone formulation, which allows for the simultaneous treatment of all degrees of freedom without the need for truncation or approximation. Another notable approach is the background field method, which has recently been shown to yield equivalent results to the light-cone formulation. In contrast, the pure spinor formalism, while not straightforward to apply directly to the complete string theory, provides critical insights into the structure and formulation of superstring theories. This article aims to contribute to this ongoing discourse by elucidating the pure spinor superfield formalism and its applications.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 7.120653320005384,
        "rewrite-fast-z-score": 0.8049844718999243
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  About the lifetime of a bouncing droplet .\nAbstract:\nWe study numerically and experimentally the dynamics of a liquid droplet bouncing on an inclined plane covered with superhydrophobic coating, which is known to be able to support stable levitation of drops in air.  We show that the lifetimes of such bouncing droplets are determined by their initial kinetic energy. The dependence of the lifetime on this energy can be fitted well using a power law t ~ E0−α where α = 0.5 ± 0.1 for both numerical simulations and experiments. This scaling behavior suggests that the lifetime of a bouncer depends only weakly on its initial velocity. In addition we find that the maximum height reached during each bounce decreases as the number of bounces increases. Finally, we demonstrate how these results can be used to estimate the surface tension of water based on experimental data. Bouncing droplets have been studied extensively over recent years due to their potential applications in microfluidics  1  . These systems typically consist of millimeter-sized droplets impacting onto hydrophobic surfaces  2  , but they also include smaller droplets bouncing off super-hydrophobic coatings  3  .\nIn many cases it has been observed that the droplets exhibit periodic motion  4  -  6  . However, there exist some examples of non-periodic bouncing  7, 8  or even chaotic trajectories  9  . It was shown recently  10  that the lifetimes (i.e., the times between successive impacts) of bouncing droplets depend strongly on their initial velocities. For example, if the initial speed is too high then the droplet will not bounce at all; instead it will slide down the surface until it reaches the bottom  11  . On the other hand, if the initial speed lies below a certain threshold value then the droplet will bounce indefinitely  12  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : About the life of a bouncing droplet . Abstract : We research numerically and experimentally the dynamics of a liquid droplet bouncing on an inclined plane covered with superhydrophobic coating , which is known to be possible to support steady levitation of drops in air .We see that the lifetimes of such bouncing droplets are decided by their initial kinetic energy . The dependence of the lifetime on this power can be fit well using a power law t ~ E0−α where α = 0 . 5 ± 0 . 1 for both numerical simulations and experiments .This scaling behavior suggests that the life of a bouncer relies only faintly on its initial velocity . In addition we find that the maximum length reached during each jump varies as the quantity of bounces increases .Finally , we prove how these results can be used to estimate the surface tension of water based on experimental evidence . Bouncing droplets have been studied frequently over recent years owing to their potential applications in microfluidics 1 .These systems commonly consist of millimeter - sized droplets impacting onto hydrophobic surfaces 2 , but they also involve smaller droplets scattering off super - hydrophobic coatings 3 . In many cases it has been observed that the droplets display periodic motion 4 - 6 .However , there remain some examples of non - periodic bouncing 7 , 8 or even chaotic trajectories 9 . It was shown recently 10 that the lifetimes ( i . e . , the periods between successive impacts ) of tumbling droplets change closely on their initial velocities .For instance , if the first velocity is too high then the droplet will not bounce at all ; merely it will slide down the surface until it meets the bottom 11 . On the other hand , if the initial speed lies below a certain threshold number then the droplet will bounce indefinitely 12 .",
        "rewrite_text": "**Title: The Dynamics of Bouncing Droplets: A Comprehensive Study**\n\n**Abstract:** This study investigates the behavior of liquid droplets bouncing on an inclined superhydrophobic surface, utilizing both numerical simulations and experimental methods. Superhydrophobic coatings are known for their ability to enable droplets to levitate steadily in air, and our research reveals that the longevity of these bouncing droplets is primarily influenced by their initial kinetic energy. We establish a power law relationship between the droplet's lifetime and its initial energy, represented as t ~ E0−α, where α is determined to be 0.5 ± 0.1 across both experimental and simulation data. This finding indicates that the droplet's lifespan is only weakly dependent on its initial velocity. Furthermore, we observe that the maximum height attained during each bounce increases with the number of bounces, suggesting a complex interplay between energy dissipation and momentum transfer. \n\nAdditionally, we demonstrate how these insights can be applied to estimate the surface tension of water, providing a practical application for our findings. The phenomenon of bouncing droplets has garnered significant attention in recent years due to its implications in microfluidics, where millimeter-sized droplets interact with hydrophobic surfaces. While many studies have focused on periodic bouncing behaviors, our research also acknowledges instances of non-periodic and chaotic trajectories. Previous work has highlighted that the lifetimes of tumbling droplets are closely tied to their initial velocities; for instance, droplets with excessively high initial speeds tend to slide off the surface rather than bounce, while those with speeds below a critical threshold can bounce indefinitely. This nuanced understanding of droplet dynamics not only enhances our comprehension of fluid behavior on superhydrophobic surfaces but also opens avenues for innovative applications in various scientific fields.",
        "ori-fast-z-score": -0.2683281572999747,
        "water-fast-z-score": 6.010407640085654,
        "rewrite-fast-z-score": -0.1796053020267749
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Chandra Observations of Supernova 1987A .\nAbstract:\nThe Chandra X-ray Observatory has observed the supernova remnant (SNR) produced by SN1987A in the Large Magellanic Cloud for over ten years, providing an unprecedented view into this young and energetic object.  The observations have revealed that the blast wave is interacting with dense circumstellar material surrounding the progenitor star at velocities up to 1000 km/sec.  This interaction produces bright knots of emission which are seen as moving outward through the shell of the remnant.  These knots appear to be composed primarily of oxygen-rich ejecta mixed with shocked interstellar gas.  In addition, there appears to be a large amount of hot plasma trapped behind the forward shock front.  We present here new results on these features based on our analysis of data obtained during the first year of the Chandra mission. The Chandra X-ray Observatory has observed  the supernova remnant ( SNR ) produced by SN1987A , in the Large Magellan ic Cloud , for over ten years . It provides an unprecedented view into this y oung and en erg i c obj ect .  T he obse rvations ha ve reve al ed tha t th e b las t wa ve is interactin g wi th d ens e circumstell ar m aterial surroundi ng th e proge nitor star-at-rou nd -velocities up to 1 000 k m/ sec . Thi s interac tion produ ces brigh t kn ots of emi ssion wh ich appea r to be co mp osed pri marily of oxyg ene -rico h ej ec ta mixe d w ith sh ocked int erstel lar ga s .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Chandra Observations of Supernova 1987A . Abstract : The Chandra X - ray Observatory has observed the supernova remnant ( SNR ) produced by SN1987A in the Large Magellanic Cloud for over ten years , providing an remarkable view into this young and dynamic body .The images have revealed that the explosion wave is interacting with heavy circumstellar material surrounding the progenitor star at velocities up to 1000 kilometers / sec . This coupling generates bright knots of emission which are seen as traveling outward through the shell of the remnant .These knots appear to be composed primarily of oxygen - rich ejecta combined with shocked interstellar gas . In addition , there seems to be a large number of hot plasma trapped behind the front shock front .We present here new data on these features based on our analysis of evidence derived during the first year of the Chandra mission . The Chandra X - ray Observatory has observed the supernova remnant ( SNR ) produced by SN1987A , in the Large Magellan ic Cloud , for over ten years .It provides an remarkable perspective into this y oung and en erg i c obj ect . T he obse rvations ha ve reve al ed tha t th e f las t wa ve is interactin b wi th d ens e circumstell ar m aterial surroundi ng th e proge nitor star - at - rou nd - velocities up to 1 000 k m / sec .Thi s interac tion produ ces brigh t kn ots of emi ssion wh ich appea r to be co mp osed pri marily of oxyg ene - rico h ej ec ta mixe d w ith sh ocked int erstel lar ga s .",
        "rewrite_text": "The Chandra X-ray Observatory has been observing the supernova remnant (SNR) of SN1987A in the Large Magellanic Cloud for over a decade, offering an extraordinary insight into this young and dynamic astronomical object. These observations have revealed that the shock wave from the explosion is interacting with dense circumstellar material surrounding the progenitor star, with velocities reaching up to 1000 kilometers per second. This interaction generates bright knots of emission that are observed moving outward through the remnant's shell. Analysis indicates that these knots are primarily composed of oxygen-rich ejecta mixed with shocked interstellar gas. Furthermore, there is evidence of a significant amount of hot plasma being trapped behind the leading shock front. In this article, we present new data derived from our analysis conducted during the initial year of the Chandra mission, which enhances our understanding of these features and their implications for the evolution of supernova remnants. The findings underscore the complex interplay between the supernova explosion and its surrounding environment, shedding light on the processes that govern the dynamics of such remnants in the cosmos.",
        "ori-fast-z-score": 0.7035264706814485,
        "water-fast-z-score": 5.0,
        "rewrite-fast-z-score": 1.811643254631353
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  CKM and Tri-bimaximal MNS Matrices in a SU(5) x (d)T Model .\nAbstract:\nWe present the results for neutrino mixing angles, CP violating phases and mass squared differences obtained by using the Fritzsch ansatz to generate the charged fermion masses within an extended supersymmetric grand unified theory based on SO(10). We find that this model can accommodate all experimental data with only one free parameter which is related to the ratio between the two vacuum expectation values of the Higgs fields responsible for breaking down the Pati-Salam gauge symmetry into the Standard Model gauge group. The CKM matrix elements are predicted as well as the Majorana phase associated with leptonic CP violation. In addition we show how the tri-bimaximal pattern observed experimentally in the lepton sector emerges naturally when the quark-lepton unification hypothesis is imposed at high energies. Finally we discuss briefly some phenomenological consequences of our scenario such as neutrinoless double beta decay and proton decay. PACS numbers: 11.30.Pb, 12.60.Cn",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : CKM and Tri - bimaximal MNS Matrices in a SU ( 5 ) x ( d ) T Model . Abstract : We present the results for neutrino mixing angles , CP violating phases and mass squared variations obtained by using the Fritzsch ansatz to produce the charged fermion masses within an extended supersymmetric grand unified theory based on SO ( 10 ) .We see that this description can handle all observation information with only one free parameter which is related to the proportion between the two vacuum expectation values of the Higgs fields responsible for breaking down the Pati - Salam gauge symmetry into the Standard Model gauge group . The CKM matrix elements are expected as well as the Majorana phase involved with leptonic CP violation .In addition we explain how the tri - bimaximal pattern found experimentally in the lepton region arises readily when the quark - lepton unification theory is imposed at high energies . Finally we talk briefly some phenomenological consequences of our scenario such as neutrinoless double alpha emission and proton emission .PACS codes : 11 . 30 . Pb , 12 . 60 . Cn",
        "rewrite_text": "Title: CKM and Tri-bimaximal MNS Matrices in a SU(5) x (d) T Model\n\nAbstract: In this study, we explore the implications of neutrino mixing angles, CP-violating phases, and mass squared differences derived from the Fritzsch ansatz within an extended supersymmetric grand unified theory grounded in SO(10). Our findings indicate that this framework effectively accommodates all observed data with the introduction of a single free parameter, which characterizes the ratio of the two vacuum expectation values of the Higgs fields that facilitate the transition from Pati-Salam gauge symmetry to the Standard Model gauge group. We derive the elements of the CKM matrix and analyze the Majorana phase associated with leptonic CP violation. Furthermore, we demonstrate how the tri-bimaximal mixing pattern, which has been experimentally identified in the lepton sector, emerges naturally when we apply the quark-lepton unification theory at high energy scales. We also briefly discuss several phenomenological implications of our model, including the potential for neutrinoless double beta decay and proton decay processes. Our results contribute to a deeper understanding of the interplay between quark and lepton sectors in grand unified theories and highlight the significance of the Higgs mechanism in shaping the properties of fundamental particles. The PACS codes relevant to our work are 11.30.Pb and 12.60.Cn, reflecting the theoretical underpinnings and implications of our findings in the context of particle physics.",
        "ori-fast-z-score": -0.808290376865476,
        "water-fast-z-score": 4.04145188432738,
        "rewrite-fast-z-score": 0.5076730825668095
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Mass of the Candidate Exoplanet Companion to HD 33636 from Hubble Space Telescope Astrometry and High-Precision Radial Velocities .\nAbstract:\nWe present new astrometric measurements for the candidate exoplanet companion to HD 33636, obtained with the Fine Guidance Sensor (FGS) on board the Hubble Space Telescope (HST). These data are combined with previously published radial velocities in order to determine the mass of this object. We find that it is most likely an M dwarf star with a mass between 0.3 and 1.0 times that of Jupiter s mass. The orbital parameters derived here agree well with those determined by previous authors using different techniques. This system may be similar to our own solar system at early stages of formation when planets were still forming around young stars. Keywords: Extrasolar planet -Astrometry -Radial velocity -HST -Mass determination -HD 33636 . \nIntroduction\n\nThe detection of extrasolar giant planets has been one of the major accomplishments of modern astronomy over the past decade. However, only about 10% of all known planetary systems contain such massive objects. Most of these have been discovered through high-precision Doppler spectroscopy or direct imaging methods. In contrast, very few low-mass companions have been found so far because they produce smaller reflex motions and/or lower luminosity than their more massive counterparts. As a result, there exists a large gap in the distribution of masses among known extra-solar planets ranging from several Earth masses down to Neptune-like masses. It is therefore important to search for low-mass companions as well since they can provide valuable information regarding the formation process of planetary systems. \n \n One possible way to detect low-mass companions is to use high-angular resolution observations made with space-based telescopes like HST. Such observations allow us to measure the position angle of the host star relative to its nearby neighbors. If we assume that the observed motion is due solely to gravitational interaction with another body then we can derive the projected separation and position angle of the companion. By combining these results with accurate radial-velocity measurements taken simultaneously, we can obtain the full three-dimensional orbit of the companion which allows us to calculate its mass.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Mass of the Candidate Exoplanet Companion to HD 33636 from Hubble Space Telescope Astrometry and High - Precision Radial Velocities . Abstract : We report new astrometric measurements for the candidate exoplanet companion to HD 33636 , obtained with the Fine Guidance Sensor ( FGS ) on board the Hubble Space Telescope ( HST ) .These data are coupled with previously written radial velocities in order to estimate the mass of this body . We see that it is most likely an M dwarf star with a mass between 0 . 3 and 1 . 0 times that of Jupiter s mass .The orbital characteristics obtained here agree well with those determined by earlier authors using separate techniques . This system might be analogous to our own solar body at early stages of formation when stars were still forming around young stars .Keywords : Extrasolar planet - Astrometry - Radial velocity - HST - Mass determination - HD 33636 . Introduction The detection of extrasolar giant planets has been one of the main accomplishments of modern astronomy over the previous decade .However , only about 10 % of all known planetary structures possess such enormous objects . Most of these have been detected through high - speed Doppler spectroscopy or direct imaging technique .In contrast , very few low - density companions have been seen so far because they produce shorter reflex motions and / or lower luminosity than their more massive counterparts . As a result , there exists a large gap in the distribution of masses among known extra - solar planets ranging from several Earth masses down to Neptune - like masses .It is consequently key to search for low - mass companions as well since they can provide valuable info regarding the formation transition of planetary structures . One potential way to identify low - mass companions is to use large - angular resolution measurements made with space - based telescopes like HST .Such observations allow us to measure the position angle of the host star relative to its neighbouring counterparts . If we suppose that the seen moving is due exclusively to gravitational interaction with another bodies then we can derive the projected separation and position angle of the companion .By combining these results with accurate radial - speed measurements taken concurrently , we can obtain the full three - dimensional orbit of the companion which allows us to estimate its mass .",
        "rewrite_text": "We present new astrometric measurements for the candidate exoplanet companion to HD 33636, acquired using the Fine Guidance Sensor (FGS) aboard the Hubble Space Telescope (HST). These measurements are integrated with previously collected radial velocity data to derive an estimate of the mass of this companion body. Our analysis suggests that the companion is most likely an M dwarf star, with a mass ranging from 0.3 to 1.0 times that of Jupiter. The orbital parameters we have determined align closely with those reported by previous studies employing different methodologies. This system may serve as a model for understanding the early stages of planetary formation, akin to the conditions present in our own solar system when stars were still in the process of formation around young stellar objects.\n\nThe discovery of extrasolar giant planets has emerged as a significant achievement in contemporary astronomy over the last decade. However, only about 10% of known planetary systems contain such massive entities. The majority of these have been identified through techniques such as high-speed Doppler spectroscopy or direct imaging. In contrast, the detection of low-density companions has proven to be more challenging due to their tendency to induce smaller reflex motions and exhibit lower luminosity compared to their more massive counterparts. Consequently, there exists a notable gap in the mass distribution of known exoplanets, spanning from several Earth masses to those resembling Neptune. It is crucial to pursue the identification of low-mass companions, as they can yield essential insights into the transitional processes involved in planetary formation.\n\nOne effective approach for detecting low-mass companions is through high-angular resolution measurements conducted with space-based telescopes like the HST. These observations facilitate the measurement of the position angle of the host star in relation to its neighboring stars. Assuming that the observed motion is solely due to gravitational interactions with another body, we can derive the projected separation and position angle of the companion. By combining these astrometric results with precise radial velocity measurements, we can reconstruct the complete three-dimensional orbit of the companion, enabling us to estimate its mass accurately.",
        "ori-fast-z-score": -0.5291502622129182,
        "water-fast-z-score": 6.745785200962753,
        "rewrite-fast-z-score": 0.38014296063485276
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Spectral Index Distribution of EGRET Blazars: Prospects for GLAST .\nAbstract:\nWe present the spectral index distribution (SID) of blazars detected by the Energetic Gamma Ray Experiment Telescope (EGRET). The SID is derived using two different methods, and we find that both give consistent results within statistical errors. We compare our results with those obtained in previous studies based on smaller samples or limited energy ranges. Our sample contains all sources listed as  blazar candidates  in the Third EGRET Catalog (3EG), which includes 157 objects. In addition to these 3EG blazars, we include four more BL Lacertae-type objects whose radio properties are similar to those of known blazars but have not been included in any previous study due to their lack of detection at high energies. These additional sources were found through cross-correlation between the Véron-Cetty & Véron catalog of active galactic nuclei and the NRAO VLA Sky Survey. Using this combined sample consisting of 161 blazars, we calculate the fraction of flat-spectrum (FSRQ-like) and steep-spectrum (BL Lac-like) blazars among the total number of blazars observed by EGRET. This fraction depends strongly on the assumed value of the high-energy cutoff E_cutof f . For example, if we assume E_cutof f = 10 GeV, then FSRQ-like blazars constitute about 80% of the entire population; however, if we adopt an extremely low value such as 1 MeV, only 20% of the blazars can be classified as FSRQ-like.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Spectral Index Distribution of EGRET Blazars : Prospects for GLAST . Abstract : We report the spectral index distribution ( SID ) of blazars detected by the Energetic Gamma Ray Experiment Telescope ( EGRET ) .The SID is generated using two different methods , and we find that both give consistent conclusions within statistical errors . We contrast our findings with those achieved in earlier studies based on smaller specimens or restricted energy intervals .Our study contains all sources listed as blazar candidates in the Third EGRET Catalog ( 3EG ) , which includes 157 objects . In addition to these 3EG blazars , we contain four more BL Lacertae - class objects whose broadcast characteristics are comparable to those of known blazars but have not been included in any earlier study due to their lack of recognition at high energies .These new sources were found through cross - correlation between the Véron - Cetty & Véron catalog of active galactic nuclei and the NRAO VLA Sky Survey . Using this combined sample composed of 161 blazars , we determine the fraction of flat - spectrum ( FSRQ - like ) and steep - spectrum ( BL Lac - like ) blazars among the total quantity of blazars observed by EGRET .This fraction depends strongly on the assumed value of the high - energy cutoff E _ cutof f . For instance , if we suppose E _ cutof g = 10 GeV , then FSRQ - like blazars comprise about 80 % of the entire population ; however , if we adopt an incredibly high value such as 1 MeV , only 20 % of the blazars can be categorized as FSRQ - like .",
        "rewrite_text": "We present an extensive analysis of the spectral index distribution (SID) of blazars identified by the Energetic Gamma Ray Experiment Telescope (EGRET). Our study employs two distinct methodologies to generate the SID, and we observe that both approaches yield consistent results within the bounds of statistical uncertainty. We compare our findings with previous research that relied on smaller sample sizes or limited energy ranges. Our analysis encompasses all sources classified as blazar candidates in the Third EGRET Catalog (3EG), totaling 157 objects. Additionally, we include four BL Lacertae-class objects that exhibit broadcasting characteristics similar to known blazars but were previously overlooked due to their undetected high-energy emissions. These newly identified sources were discovered through cross-referencing the Véron-Cetty & Véron catalog of active galactic nuclei with the NRAO VLA Sky Survey. By combining this dataset, we analyze a total of 161 blazars and assess the proportion of flat-spectrum radio quasars (FSRQ-like) versus steep-spectrum blazars (BL Lac-like) within the entire EGRET-observed blazar population. Our findings indicate that this ratio is highly sensitive to the chosen value of the high-energy cutoff, E_cutoff. For example, assuming E_cutoff at 10 GeV results in approximately 80% of the blazars being classified as FSRQ-like. Conversely, if we consider an exceptionally high cutoff value of 1 MeV, the proportion of FSRQ-like blazars drops to around 20%. This research not only enhances our understanding of blazar spectral characteristics but also sets the stage for future investigations with the upcoming GLAST mission.",
        "ori-fast-z-score": -0.8081220356417685,
        "water-fast-z-score": 5.050762722761053,
        "rewrite-fast-z-score": 1.2686700948330931
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Experimental studies on vacancy induced ferromagnetism in undoped TiO2 .\nAbstract:\nThe effect of oxygen vacancies (V) on the magnetic properties of titanium dioxide was investigated by using X-ray photoelectron spectroscopy, Mössbauer spectroscopy and magnetization measurements.  The results show that V can induce ferromagnetic ordering at room temperature with an effective moment of 1.6μB per formula unit. This is attributed to the formation of small clusters of V ions which are antiferromagnetically coupled through superexchange interactions between neighboring O-2p orbitals. These findings provide new insights into the origin of ferromagnetism observed in some transition metal oxides. Titanium dioxide has been widely used as photocatalysts for water splitting under visible light irradiation due to its high activity and stability  1  . However, it shows no intrinsic magnetic property because there is only one unpaired electron in each Ti atom  2  , so it cannot be directly applied in spintronic devices such as spin-valve transistors or giant magnetoresistance sensors  3  .\nRecently, several groups have reported that doping TiO2 with non-magnetic elements like Nb  4  , Ta  5  , Zr  6  , Al  7  , Si  8  , Ge  9  , Sn  10  , Sb  11  , W  12  , Mo  13  , Fe  14  , Co  15  , Ni  16  , Cu  17  , Zn  18  , Ga  19  , In  20  , Ag  21  , Au  22  , Pt  23  , Pd  24  , Rh  25  , Ir  26  , Ru  27  , Re  28  , Os  29  , Bi  30  , Y  31  , Gd  32  , Dy  33  , Yb  34  , Er  35  , Nd  36  , Sm  37  , Eu  38  , Tb  39  , Ho  40  , Tm  41  , Lu  42  , Pr  43  , La  44  , Ce  45  , Th  46  , U  47  , Hg  48  , Tl  49  , Pb  50  , Bi  51  , As  52  , Se  53  , Te  54  , S  55  , P  56  , N  57  , F  58  , Cl  59  , Br  60  , I  61  , B  62  , C",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Experimental research on vacancy induced ferromagnetism in undoped TiO2 . Abstract : The impact of oxygen vacancies ( V ) on the magnetic properties of titanium dioxide was investigated by using X - ray photoelectron spectroscopy , Mössbauer spectroscopy and magnetization measurements .The results show that V can induce ferromagnetic ordering at room temperature with an effective moment of 1 . 6μB per formula unit . This is attributed to the formation of tiny clusters of V ions which are antiferromagnetically linked through superexchange interactions between neighboring O - 2p orbitals .These studies provide fresh insights into the origin of ferromagnetism observed in some transition metal oxides . Titanium dioxide has been widely useful as photocatalysts for water breaking under visible color irradiation due to its high activity and strength 1 .However , it displays no intrinsic magnetic property because there is only one unpaired particle in each Ti atom 2 , so it must be easily applied in spintronic systems such as spin - valve transistors or giant magnetoresistance sensors 3 . Recently , various groups have reported that doping TiO2 with non - magnetic elements like Nb 4 , Ta 5 , Zr 6 , Al 7 , Si 8 , Ge 9 , Sn 10 , Sb 11 , W 12 , Mo 13 , Fe 14 , Co 15 , Ni 16 , Cu 17 , Zn 18 , Ga 19 , In 20 , Ag 21 , Au 22 , Pt 23 , Pd 24 , Rh 25 , Ir 26 , Ru 27 , Re 28 , Os 29 , Bi 30 , Y 31 , Gd 32 , Dy 33 , Yb 34 , Er 35 , Nd 36 , Sm 37 , Eu 38 , Tb 39 , Ho 40 , Tm 41 , Lu 42 , Pr 43 , La 44 , Ce 45 , Th 46 , U 47 , Hg 48 , Tl 49 , Pb 50 , Bi 51 , As 52 , Se 53 , Te 54 , S 55 , P 56 , N 57 , F 58 , Cl 59 , Br 60 , I 61 , B 62 , C",
        "rewrite_text": "**Title:** Experimental Investigation of Vacancy-Induced Ferromagnetism in Undoped TiO2\n\n**Abstract:** This study explores the influence of oxygen vacancies (V) on the magnetic characteristics of titanium dioxide (TiO2) through a combination of X-ray photoelectron spectroscopy, Mössbauer spectroscopy, and magnetization measurements. Our findings reveal that the presence of V can lead to the emergence of ferromagnetic ordering at room temperature, exhibiting an effective magnetic moment of 1.6μB per formula unit. This phenomenon is attributed to the formation of small clusters of V ions, which are coupled antiferromagnetically via superexchange interactions among adjacent O-2p orbitals. These results offer new perspectives on the mechanisms underlying ferromagnetism observed in certain transition metal oxides. \n\nTitanium dioxide is renowned for its applications as a photocatalyst in water-splitting processes under visible light due to its remarkable activity and durability. However, it is inherently non-magnetic, as each titanium atom contains only one unpaired electron. This characteristic makes TiO2 a promising candidate for integration into spintronic devices, such as spin-valve transistors and giant magnetoresistance sensors. Recent research has indicated that doping TiO2 with various non-magnetic elements—including Nb, Ta, Zr, Al, Si, Ge, Sn, and others—can modify its magnetic properties. This study contributes to the growing body of literature that seeks to understand and harness the magnetic behavior of TiO2, paving the way for innovative applications in spintronics and related fields. The insights gained from this research may facilitate the development of advanced materials with tailored magnetic properties for future technological applications.",
        "ori-fast-z-score": -0.45291081365783825,
        "water-fast-z-score": 4.47213595499958,
        "rewrite-fast-z-score": 1.1338934190276817
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Driving Mechanism of Jets and Outflows in Star Formation Process .\nAbstract:\nThe driving mechanism for jets and outflows is still an open question, especially when the jet/outflow source has no clear central engine such as black holes or protostars. In this work we propose that magnetic reconnection can be responsible for launching jets and outflows in star formation process. We show that magnetic reconnection can accelerate particles to relativistic energies efficiently via Fermi acceleration at shocks driven by the reconnecting current sheet (RCS). The accelerated electrons will produce synchrotron emission which may explain radio observations of jets and outflows. Furthermore, the energetic protons produced during RCS also contribute to nonthermal emissions through inverse Compton scattering with background photons. Finally, we discuss how our model could account for some observational features of jets and outflows. \n \n Keywords: Magnetic reconnection; Jet; Particle acceleration; Shocks; Synchrotron radiation; Nonthermal emission",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Driving Mechanism of Jets and Outflows in Star Formation Process . Abstract : The driving system for rockets and outflows is already an open matter , particularly when the jet / outflow source has no clear central fuel such as black holes or protostars .In this research we propose that magnetic reconnection can be responsible for launching jets and outflows in star formation system . We suggest that magnetic reconnection can accelerate particles to relativistic energies efficiently via Fermi acceleration at shocks driven by the reconnecting current sheet ( RCS ) .The enhanced electrons will generate synchrotron emission which would cause radio observations of jets and outflows . Furthermore , the energetic protons created during RCS also contribute to nonthermal emissions through inverse Compton absorption with background photons .Finally , we explain how our model could account for some observational characteristics of jets and outflows . Keywords : Magnetic reconnection ; Jet ; Particle acceleration ; Shocks ; Synchrotron emission ; Nonthermal emission",
        "rewrite_text": "**Title:** Driving Mechanism of Jets and Outflows in the Star Formation Process\n\n**Abstract:** The mechanisms behind the generation of jets and outflows in star formation remain a topic of considerable debate, especially in scenarios where the source lacks a distinct central energy source, such as black holes or protostars. In this study, we propose a novel hypothesis that magnetic reconnection plays a crucial role in the initiation of jets and outflows within star-forming regions. We argue that magnetic reconnection can effectively accelerate particles to relativistic speeds through a process known as Fermi acceleration, which occurs at shocks produced by reconnecting current sheets (RCS). This acceleration leads to the generation of high-energy electrons, which in turn produce synchrotron radiation, making them detectable through radio observations of jets and outflows. Additionally, the energetic protons generated during the reconnection process contribute to nonthermal emissions via inverse Compton scattering with ambient photons. Our model not only provides a comprehensive explanation for the dynamics of jets and outflows but also aligns with various observational characteristics documented in astrophysical studies. By elucidating the role of magnetic reconnection in particle acceleration and emission processes, we aim to enhance the understanding of the underlying physics driving these phenomena in the context of star formation. This research opens new avenues for exploring the intricate interplay between magnetic fields and particle dynamics in astrophysical environments. \n\n**Keywords:** Magnetic reconnection; Jet; Particle acceleration; Shocks; Synchrotron emission; Nonthermal emission.",
        "ori-fast-z-score": -1.212678125181665,
        "water-fast-z-score": 3.8247315498700596,
        "rewrite-fast-z-score": 2.309401076758503
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Intervening Metal Systems in GRB and QSO sight-lines: The Mgii and Civ Question .\nAbstract:\nWe present new observations of intervening metal systems at z ~ 1.5-2.0 towards the gamma-ray burst (GRB) 050525A, which is one of only two GRBs with spectroscopic redshifts known to date. We detect absorption lines due to Mg ii λλ2796, 2803, Feii λ2382, 2344+2600, Ciii λ977, Siiv λ1394, 1403, Ovi λ1032, 1038, Nv λ1239, 1243, Lyα, and Lyβ associated with an absorber at z = 2.01 ± 0.02. This system has log NHI/cm−2 = 19.6 +0.2 −0.1 , corresponding to a total hydrogen column density of 5 × 1020 cm−2 . It also shows strong low-ionization transitions such as Al iii λ1854, 1854 + 1862, and S iv λ1063, 1073 that are not seen in typical high-redshift absorbers. These features suggest that this absorber may be similar to those found along quasar sightlines.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Intervening Metal Systems in GRB and QSO sight - lines : The Mgii and Civ Question . Abstract : We report new images of intervening metal systems at z ~ 1 . 5 - 2 . 0 towards the gamma - ray burst ( GRB ) 050525A , which is one of only two GRBs with spectroscopic redshifts discovered to date .We detect absorption lines owing to Mg ii λλ2796 , 2803 , Feii λ2382 , 2344 + 2600 , Ciii λ977 , Siiv λ1394 , 1403 , Ovi λ1032 , 1038 , Nv λ1239 , 1243 , Lyα , and Lyβ associated with an absorber at z = 2 . 01 ± 0 . 02 . This system has log NHI / cm−2 = 19 . 6 + 0 . 2 −0 . 1 , equivalent to a total hydrogen column size of 5 × 1020 cm−2 .It additionally shows bright low - ionization transitions such as Al iii λ1854 , 1854 + 1862 , and S iv λ1063 , 1073 that are not seen in standard high - redshift absorbers . These features indicate that this absorber could be close to those observed along quasar sightlines .",
        "rewrite_text": "We present new observational data concerning intervening metal systems at redshifts approximately between 1.5 and 2.0, specifically in the line of sight of gamma-ray burst (GRB) 050525A. This GRB is notable as it is one of only two GRBs identified to date with spectroscopic redshifts. Our analysis reveals the presence of several absorption lines, including those from Mg II (λλ2796, 2803), Fe II (λ2382, 2344, 2600), C III (λ977), Si IV (λ1394, 1403), O VI (λ1032, 1038), N V (λ1239, 1243), as well as Lyα and Lyβ, which are associated with an absorber at a redshift of z = 2.01 ± 0.02. The derived hydrogen column density for this system is log NHI/cm−2 = 19.6 +0.2 −0.1, translating to a total hydrogen column density of approximately 5 × 10^20 cm−2. Notably, this absorber exhibits prominent low-ionization transitions, such as Al III (λ1854, 1862) and S IV (λ1063, 1073), which are atypical for standard high-redshift absorbers. The presence of these features suggests that this particular absorber may be situated in proximity to those typically observed along quasar sightlines. Our findings contribute to the understanding of the chemical composition and physical conditions of intervening metal systems in the universe, particularly in relation to gamma-ray bursts and quasars, and highlight the potential for further investigation into the characteristics of such absorbers in different cosmic environments.",
        "ori-fast-z-score": -2.1879748724684185,
        "water-fast-z-score": 1.7320508075688774,
        "rewrite-fast-z-score": -1.3764944032233704
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  SubmilliJansky Transients in Archival Radio Observations .\nAbstract:\nWe have searched for radio transients with flux densities between 0.1 and 1 mJy at frequencies ranging from 4 to 8 GHz using archival data obtained by the Very Large Array (VLA) over the past 20 years. We find that most of these sources are extragalactic, but we also detect several Galactic objects including pulsars, supernova remnants, and flare stars. The majority of our sample is comprised of previously uncatalogued sources; however, we recover many known variable sources such as blazars and gamma-ray burst afterglows. Our results demonstrate the power of combining large amounts of archival VLA data into one coherent dataset. This work was supported by NSF grant AST-0907860. In this Letter, we present an analysis of all available archived Very Large Array (V LA) observations taken since 1990. These data were collected during various observing programs aimed primarily at studying distant galaxies or nearby star forming regions. However, they contain valuable information about fainter transient phenomena occurring within our Galaxy. By searching through more than 10 000 hours of observation time spread across nearly 2000 epochs, we identify thousands of new faint radio sources which appear only once or twice in each epoch s data set. Most of these sources are extragalaxtic, but we also detect numerous Galactic objects including pulsar wind nebulae, supernova remnants, flare stars, and other types of active galactic nuclei. Many of these newly discovered sources are not included in existing catalogs because their low signal-to-noise ratio makes them difficult to detect when observed individually. However, by combining multiple epochs together, we can boost the sensitivity of our survey enough to detect even very weak signals.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : SubmilliJansky Transients in Archival Radio Observations . Abstract : We have searched for radio transients with flux densities between 0 . 1 and 1 mJy at frequencies ranging from 4 to 8 GHz using archival measurements obtained by the Very Large Array ( VLA ) over the previous 20 decades .We see that most of these sources are extragalactic , but we also observe numerous Galactic items including pulsars , supernova remnants , and flare stars . The majority of our sample is comprised of previously uncatalogued sources ; however , we recover several known variable sources such as blazars and gamma - ray flare afterglows .Our results show the power of combining enormous amounts of archival VLA information into one coordinated dataset . This project was supported by NSF grant AST - 0907860 .In this Letter , we present an assessment of all available archived Very Large Array ( V LA ) observations made since 1990 . These data were collected during various observing programs aimed exclusively at studying nearby galaxies or neighboring star producing regions .However , they contain significant information about fainter transient phenomena occurring within our Galaxy . By looking through more than 10 000 hours of study time dispersed across nearly 2000 epochs , we identify thousands of new faint radio sources which appear only once or repeatedly in each epoch s information pool .Most of these sources are extragalaxtic , but we also observe numerous Galactic bodies including pulsar wind nebulae , supernova remnants , flare stars , and other types of active galactic nuclei . Many of these newly discovered sources are not covered in existing catalogs because their low signal - to - noise proportion creates them harder to identify when observed individually .However , by combining multiple epochs combined , we can boost the sensitivity of our survey sufficiently to identify especially very weak signals .",
        "rewrite_text": "In this study, we investigate radio transients with flux densities ranging from 0.1 to 1 mJy at frequencies between 4 and 8 GHz, utilizing archival data from the Very Large Array (VLA) collected over the past two decades. Our analysis reveals that a significant portion of these transient sources are extragalactic; however, we also identify a variety of Galactic objects, including pulsars, supernova remnants, and flare stars. Notably, our sample predominantly consists of previously uncatalogued sources, although we successfully recover several known variable sources, such as blazars and gamma-ray flare afterglows. \n\nThe findings underscore the effectiveness of synthesizing extensive archival VLA data into a cohesive dataset, which enhances our ability to detect faint radio emissions. This research, supported by NSF grant AST-0907860, presents a comprehensive evaluation of all available VLA observations since 1990. These observations were primarily gathered through various programs focused on nearby galaxies and star-forming regions, yet they harbor valuable insights into fainter transient phenomena within our own Galaxy.\n\nBy meticulously analyzing over 10,000 hours of observational data spread across nearly 2,000 epochs, we have identified thousands of new faint radio sources that manifest either sporadically or repeatedly within the datasets. While the majority of these sources are extragalactic, we also catalog numerous Galactic entities, including pulsar wind nebulae and other active galactic nuclei. Many of these newly discovered sources remain unlisted in existing catalogs due to their low signal-to-noise ratios, which complicate their identification in isolated observations. However, our approach of integrating multiple epochs significantly enhances the sensitivity of our survey, enabling us to detect even the faintest signals. This work highlights the potential of archival data in uncovering previously hidden astrophysical phenomena.",
        "ori-fast-z-score": 0.8770580193070293,
        "water-fast-z-score": 8.465910570697329,
        "rewrite-fast-z-score": 2.23606797749979
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Considering the Case for Biodiversity Cycles: Reexamining the Evidence for Periodicity in the Fossil Record .\nAbstract:\nThe fossil record is replete with examples of periodic extinction events, but it has been unclear whether these are driven by external factors or internal dynamics within ecosystems. Here we show that biodiversity cycles can be generated solely through interactions between species and their environment without any need to invoke additional mechanisms such as mass extinctions. We use an agent-based model to simulate how communities evolve over time under different environmental conditions. Our results suggest that biodiversity cycles may have played an important role in shaping Earth s biosphere throughout its history. The fossil record contains numerous examples of periodic extinction events (1), which have led some researchers to propose that there must exist underlying periodicity in ecosystem processes (2). However, it remains unknown what causes this apparent regularity in the fossil record; one possibility is that periods of high diversity alternate with intervals during which many species go extinct simultaneously (3) (4) . In addition, it is not clear if all observed patterns of biodiversity cycling represent true cyclical behavior or simply reflect stochastic variation around a mean value (5-7).\nHere we present evidence suggesting that biodiversity cycles can arise spontaneously from ecological interactions alone, without requiring any additional mechanism like mass extinctions. To test our hypothesis, we used an agent-based model to explore how communities evolve over time when subjected to varying levels of environmental stress. This approach allowed us to examine how changes in community composition affect population abundances across multiple trophic levels. By simulating thousands of replicate runs using different parameter values, we were able to identify robust statistical signatures associated with biodiversity cycles.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Considering the Case for Biodiversity Cycles : Reexamining the Evidence for Periodicity in the Fossil Record . Abstract : The fossil history is replete with examples of periodic mortality events , but it has been uncertain whether these are driven by external influences or internal mechanisms within ecosystems .Here we study that biodiversity cycles can be formed solely through relationships between species and their environment without any necessity to invoke additional mechanisms such as mass extinctions . We use an agent - based model to simulate how communities evolve over time under various environmental conditions .Our results show that ecosystem cycles might have played an important role in shaping Earth s biosphere throughout its past . The fossil history provides various instances of periodic mortality events ( 1 ) , which have led some researchers to propose that there need possess underlying periodicity in ecological processes ( 2 ) .However , it remains obscure what causes this evident regularity in the fossil history ; one suggestion is that intervals of high diversity alternate with periods during which several species go extinct simultaneously ( 3 ) ( 4 ) . In addition , it is not clear if all observed patterns of biodiversity cycling constitute genuine cyclical behavior or simply reflect stochastic differences around a mean value ( 5 - 7 ) .Here we present evidence indicating that ecosystem cycles can arise spontaneously from ecological relationships alone , without requiring any additional process like mass extinctions . To study our theory , we using an agent - based model to examine how communities evolve over time when exposed to different amounts of environmental stress .This method allowed us to examine how variations in population composition impact population abundances across multiple trophic levels . By simulating thousands of replicate runs using varying parameter values , we were could to identify reliable statistical signatures identified with biodiversity cycles .",
        "rewrite_text": "**Title:** Reevaluating Biodiversity Cycles: Investigating Periodicity in the Fossil Record\n\n**Abstract:** The fossil record is rich with instances of periodic mortality events, yet the underlying causes of these patterns remain a topic of debate. This study explores the hypothesis that biodiversity cycles can emerge solely from the interactions between species and their environments, without the need to invoke external factors such as mass extinction events. Utilizing an agent-based modeling approach, we simulate the evolution of ecological communities under varying environmental conditions to assess the dynamics of biodiversity over time. Our findings suggest that ecosystem cycles may have significantly influenced the development of Earth's biosphere throughout its history. \n\nPrevious research has highlighted the presence of periodic mortality events in the fossil record, leading to the proposition that these events may reflect inherent periodicity in ecological processes. However, the mechanisms driving this regularity are not well understood. Some theories suggest that intervals of high biodiversity may alternate with phases of simultaneous species extinction, while others question whether observed patterns of biodiversity cycling are genuine cyclical phenomena or merely stochastic fluctuations around a mean value. \n\nIn this study, we provide evidence that biodiversity cycles can spontaneously arise from ecological interactions, independent of external processes like mass extinctions. By employing an agent-based model, we investigate how communities respond to varying levels of environmental stress and how these stressors influence population dynamics across different trophic levels. Through extensive simulations with diverse parameter settings, we identify consistent statistical signatures associated with biodiversity cycles. Our results contribute to a deeper understanding of the mechanisms driving biodiversity dynamics and underscore the potential significance of internal ecological relationships in shaping the patterns observed in the fossil record.",
        "ori-fast-z-score": 0.242535625036333,
        "water-fast-z-score": 9.177999171377655,
        "rewrite-fast-z-score": 0.6446583712203042
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dynamic fibrils in H-alpha and C IV .\nAbstract:\nWe present new observations of the solar atmosphere obtained with the Solar Ultraviolet Imager (SUVI) onboard the Coriolis satellite, which show dynamic fibrils in both H-alpha and CIV lines. The SUVI data are compared to simultaneous ground-based observations made at Big Bear Observatory using the 1-meter telescope equipped with an H-Alpha filter and a Fabry-Perot interferometer tuned to the CIV line. We find that the observed structures have similar properties as those seen previously by other authors but we also see some differences between them. In particular, our results suggest that the fibril structure is more complex than it was thought before. This complexity may be related to the fact that these structures are formed under different physical conditions. Our analysis shows that the observed features can be explained by assuming that they represent plasma flows along magnetic field lines. These flows could play important role in heating up the upper layers of the solar atmosphere.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dynamic fibrils in H - alpha and C IV . Abstract : We report new studies of the solar atmosphere acquired with the Solar Ultraviolet Imager ( SUVI ) onboard the Coriolis spacecraft , which show continuous fibrils in both H - alpha and CIV lines .The SUVI findings are compared to simultaneous ground - based observations made at Big Bear Observatory using the 1 - meter telescope fitted with an H - Alpha filter and a Fabry - Perot interferometer tuned to the CIV line . We see that the seen elements have related properties as those viewed earlier by other researchers but we also saw some similarities between them .In particular , our findings confirm that the fibril structure is more sophisticated than it was considered before . This complexity might be connected to the fact that these structures are created under varying physical conditions .Our study shows that the seen features can be described by assuming that they represent plasma flows along magnetic field lines . These flows could play vital role in heating up the higher layers of the solar atmosphere .",
        "rewrite_text": "Title: Dynamic Fibrils in H-alpha and C IV\n\nAbstract: This study presents new observations of the solar atmosphere obtained using the Solar Ultraviolet Imager (SUVI) aboard the Coriolis spacecraft, revealing the presence of continuous fibrils in both H-alpha and C IV spectral lines. The SUVI data were analyzed in conjunction with concurrent ground-based observations from the Big Bear Observatory, where a 1-meter telescope equipped with an H-alpha filter and a Fabry-Perot interferometer tuned to the C IV line was utilized. Our findings indicate that the fibrils exhibit properties consistent with those reported in previous research, while also highlighting notable similarities between the two observational methods. Notably, our results suggest that the fibril structures are more intricate than previously understood, indicating a level of complexity that may arise from the diverse physical conditions under which these structures form. We propose that the observed features can be interpreted as plasma flows along magnetic field lines, which may play a crucial role in the thermal dynamics of the upper layers of the solar atmosphere. This research enhances our understanding of solar atmospheric phenomena and underscores the importance of multi-wavelength observations in elucidating the mechanisms driving solar activity. The implications of these findings could lead to a deeper comprehension of solar dynamics and their influence on space weather events.",
        "ori-fast-z-score": -0.44172610429938614,
        "water-fast-z-score": 6.037034299432969,
        "rewrite-fast-z-score": -0.9805806756909202
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Particle Interpretations of the PVLAS Data .\nAbstract:\nThe PVLAS collaboration has recently reported results on light-by-light scattering in vacuum, which are inconsistent with Standard Model predictions.  In this note we discuss possible interpretations of these data within the framework of quantum field theory and string theory. We argue that the most natural interpretation is to assume that the observed effect arises due to new particles coupling to photons via an effective dimension-8 operator. The required mass scale for such particles can be as low as 10 GeV or even lower if one assumes that they couple only weakly to ordinary matter. If confirmed by further experiments, these observations would have profound implications both for particle physics phenomenology and cosmological models. The PVLAS collaboration has recently announced their measurement of light-by-light scattering in vacuo  1  . This process violates parity conservation at tree level and thus cannot occur in the Standard Model (SM)  2  , but it could arise through loop effects  3  .\nIn particular, the authors report observing a signal consistent with the SM prediction  4  \nwhere G F = 1.1663787(6) × 10−5GeV−2 is Fermi s constant  5  , θ W ≈ 0.23 is the weak mixing angle  6  , m e is the electron mass, and M Pl ≡ 1/ √ 8πG N ≈ 2×10 18 GeV is the reduced Planck mass  7, 8  . However, the measured value of the cross section exceeds the theoretical expectation by more than three standard deviations,\nThis discrepancy between experiment and theory may indicate the presence of new physics beyond the SM  9  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Particle Interpretations of the PVLAS Data . Abstract : The PVLAS collaboration has recently published results on light - by - light diffusion in vacuum , which are inconsistent with Standard Model estimates .In this note we explain possible interpretations of these information within the framework of quantum field theory and string theory . We argue that the most natural interpretation is to assume that the observed effect arises due to new objects coupling to photons via an efficient dimension - 8 operator .The expected mass scale for such particles can be as low as 10 GeV or especially lower if one assumes that they couple only weakly to normal matter . If confirmed by further studies , these observations would have profound implications both for particle science phenomenology and cosmological models .The PVLAS collaboration has recently announced their observation of light - by - light scattering in vacuo 1 . This process violates parity conservation at tree level and therefore cannot appear in the Standard Model ( SM ) 2 , but it could occur through loop effects 3 .In particular , the articles publish observing a signal compatible with the SM prediction 4 where G F = 1 . 1663787 ( 6 ) × 10−5GeV−2 is Fermi s constant 5 , θ W ≈ 0 . 23 is the strong mixing angle 6 , m e is the electron mass , and M Pl ≡ 1 / √ 8πG N ≈ 2×10 18 GeV is the reduced Planck mass 7 , 8 . However , the measured value of the cross section exceeds the theoretical expectation by more than three standard deviations , This discrepancy between experiment and theory could indicate the presence of new science beyond the SM 9 .",
        "rewrite_text": "**Title: Particle Interpretations of the PVLAS Data**\n\n**Abstract:** The recent findings from the PVLAS collaboration regarding light-by-light scattering in a vacuum present results that diverge from the predictions made by the Standard Model of particle physics. This paper explores various interpretations of these findings through the lenses of quantum field theory and string theory. We propose that the most plausible explanation for the observed phenomena is the existence of new particles that interact with photons via a dimension-8 operator. The mass scale of these hypothetical particles could be as low as 10 GeV, or even lower if they exhibit weak coupling to ordinary matter. Should these results be validated through further experimental investigations, they could have significant ramifications for both the field of particle physics and our understanding of cosmological models. \n\nThe PVLAS collaboration has reported their detection of light-by-light scattering in a vacuum, a process that inherently violates parity conservation at the tree level, making it absent from the Standard Model. While such scattering could theoretically occur through loop corrections, the collaboration's findings indicate a signal that aligns with the Standard Model predictions, where Fermi's constant (GF) is approximately 1.1663787(6) × 10−5 GeV−2, the weak mixing angle (θW) is around 0.23, and the electron mass (me) and reduced Planck mass (MPl) are also considered. However, the measured cross-section significantly exceeds the theoretical expectations by over three standard deviations. This notable discrepancy between experimental results and theoretical predictions raises the possibility of new physics beyond the Standard Model, suggesting that further exploration into these anomalous results is warranted.",
        "ori-fast-z-score": -1.104689541477988,
        "water-fast-z-score": 3.0251050401930977,
        "rewrite-fast-z-score": -1.5992254762521154
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Effective inverse spectral problem for rational Lax matrices and applications .\nAbstract:\nWe consider the effective inverse spectral problems (EISP) associated with rational Lax matrices, which are generalizations of classical EISP in terms of Jacobi matrices. We show that these new EISPs can be reduced to some special cases of classical EISP by using an explicit formula for the coefficients of the characteristic polynomial of a rational matrix. As examples we study two classes of rational Lax matrices: one is related to the Toda lattice hierarchy and another is connected with the modified Volterra lattice hierarchy. In particular, we give a complete description on all solutions of the corresponding EISPs. Finally, as applications of our results, we present several interesting properties about the spectra of these rational Lax matrices. The research was supported by NSFC under Grant No.11571040. Keywords: Effective inverse spectral problem; Rational Lax matrices; Toda lattice hierarchy; Modified Volterra lattice hierarchy",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Effective inverse spectral problem for rational Lax matrices and applications . Abstract : We consider the effective inverse spectral questions ( EISP ) associated with rational Lax matrices , which are generalizations of classical EISP in terms of Jacobi matrices .We see that these new EISPs can be reduced to some particular examples of classical EISP by using an explicit formula for the coefficients of the characteristic polynomial of a rational matrix . As instance we study two families of rational Lax matrices : one is related to the Toda lattice hierarchy and another is linked with the modified Volterra lattice hierarchy .In particular , we give a complete model on all solutions of the resulting EISPs . Finally , as applications of our findings , we present many interesting properties about the spectra of these rational Lax matrices .The research was supported by NSFC under Grant No . 11571040 . Keywords : Effective inverse spectral problem ; Rational Lax matrices ; Toda lattice hierarchy ; Modified Volterra lattice hierarchy",
        "rewrite_text": "Title: Effective Inverse Spectral Problem for Rational Lax Matrices and Its Applications\n\nAbstract: This article delves into the effective inverse spectral problems (EISP) associated with rational Lax matrices, which serve as a generalization of the classical EISP typically examined in the context of Jacobi matrices. We demonstrate that these novel EISPs can be transformed into specific instances of classical EISP by employing a precise formula for the coefficients of the characteristic polynomial of a rational matrix. Our investigation focuses on two distinct families of rational Lax matrices: one associated with the Toda lattice hierarchy and the other connected to the modified Volterra lattice hierarchy. We provide a comprehensive framework detailing all solutions to the resulting EISPs, thereby enhancing our understanding of these mathematical constructs. Furthermore, we explore various intriguing properties related to the spectra of these rational Lax matrices, revealing significant implications of our findings. The research presented in this article was supported by the National Natural Science Foundation of China (NSFC) under Grant No. 11571040. This work not only contributes to the theoretical landscape of spectral problems but also opens avenues for further exploration in related mathematical and physical contexts. \n\nKeywords: Effective inverse spectral problem; Rational Lax matrices; Toda lattice hierarchy; Modified Volterra lattice hierarchy.",
        "ori-fast-z-score": 0.23249527748763857,
        "water-fast-z-score": 2.8867513459481287,
        "rewrite-fast-z-score": -1.0660035817780522
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Composite fermion wave functions as conformal field theory correlators .\nAbstract:\nWe show that the ground state wave function for composite fermions in the lowest Landau level is equal to the vacuum expectation value of a correlator of chiral primary fields in N = 2 superconformal Chern-Simons-matter theories with gauge group U(1)Nc−1 and matter content consisting of one hypermultiplet in the fundamental representation, where Nc denotes the number of colors. We also discuss how this result can be used to compute correlation functions of local operators at finite density using supersymmetric localization techniques. The results presented here are based on joint work with Jens Alcaraz-García, Daniel Grumiller, David H. Kaplan, Michael Lüst, and Thomas Schroer. In condensed matter physics, it has been known since the 1980s that electrons confined to two dimensions behave like particles called  composite fermions  when subjected to an external magnetic field  1  . These composite fermions form a Fermi sea which exhibits fractional quantum Hall effect (FQHE), i.e., they exhibit plateaus in their Hall conductance at fractions of e/2πh  2  .\nIn recent years there have been several attempts to understand these phenomena within string theory  3, 4, 5, 6, 7, 8  , but so far no complete description exists. One approach consists of studying the low-energy effective action describing the dynamics of the FQH fluid  7, 9  . This action contains a Chern-Simons term coupled to a set of charged scalar fields representing fluctuations around the mean-field solution corresponding to the filled lowest Landau level. It turns out that the resulting system is described by a two-dimensional conformal field theory (CFT). Moreover, the CFT partition function evaluated on a torus coincides with the statistical sum over all states of the FQH fluid  10  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Composite fermion wave systems as conformal field theory correlators . Abstract : We see that the ground state wave function for composite fermions in the lowest Landau grade is equal to the vacuum expectation value of a correlator of chiral primary fields in N = 2 superconformal Chern - Simons - matter theories with gauge group U ( 1 ) Nc−1 and matter content comprised of one hypermultiplet in the fundamental representation , where Nc denotes the number of colors .We also discuss how this effect can be used to compute correlation functions of local operators at finite density using supersymmetric localization techniques . The results presented here are based on collaborative work with Jens Alcaraz - García , Daniel Grumiller , David H . Kaplan , Michael Lüst , and Thomas Schroer .In condensed matter mechanics , it has been known since the 1980s that electrons confined to two dimensions behave like ions called composite fermions when exposed to an external magnetic force 1 . These composite fermions form a Fermi sea which exhibits fractional quantum Hall impact ( FQHE ) , i . e . , they show plateaus in their Hall conductance at fractions of e / 2πh 2 .In past decades there have been numerous attempts to explain these phenomena within string theory 3 , 4 , 5 , 6 , 7 , 8 , but so far no complete documentation remains . One approach consists of examining the small - energy effective action governing the dynamics of the FQH fluid 7 , 9 .This operation contains a Chern - Simons term coupled to a setting of charged scalar fields representing fluctuations around the mean - field solution corresponding to the filled low Landau level . It turns out that the resulting system is characterized by a two - dimensional conformal field model ( CFT ) .Moreover , the CFT partition function evaluated on a torus coincides with the statistical sum over all states of the FQH fluid 10 .",
        "rewrite_text": "**Title:** Composite Fermion Wave Systems as Conformal Field Theory Correlators\n\n**Abstract:** In this study, we demonstrate that the ground state wave function of composite fermions situated in the lowest Landau level corresponds to the vacuum expectation value of a correlator involving chiral primary fields within N = 2 superconformal Chern-Simons-matter theories. These theories are characterized by a gauge group U(1) with Nc−1 colors and include a single hypermultiplet in the fundamental representation, where Nc represents the number of colors. We further explore the implications of this correspondence for calculating correlation functions of local operators at finite density, utilizing techniques from supersymmetric localization. This work is a collaborative effort with Jens Alcaraz-García, Daniel Grumiller, David H. Kaplan, Michael Lüst, and Thomas Schroer.\n\nThe behavior of electrons confined to two-dimensional spaces under the influence of an external magnetic field has been well-documented since the 1980s, revealing that they can behave like composite fermions. These composite fermions create a Fermi sea that exhibits fractional quantum Hall effect (FQHE) characteristics, notably manifesting as plateaus in Hall conductance at fractional values of e/2πh². Over the past few decades, various attempts have been made to elucidate these phenomena through the lens of string theory; however, a comprehensive theoretical framework remains elusive. One promising avenue of investigation involves analyzing the effective action at low energies that governs the dynamics of the fractional quantum Hall (FQH) fluid. This effective action incorporates a Chern-Simons term, which is coupled with charged scalar fields that account for fluctuations around the mean-field solution associated with the fully occupied low Landau level. Remarkably, this framework leads to the identification of a two-dimensional conformal field theory (CFT). Additionally, we find that the partition function of this CFT, when evaluated on a toroidal geometry, aligns with the statistical sum over all states of the FQH fluid, thereby providing a deeper understanding of the underlying physics.",
        "ori-fast-z-score": -1.153563462240948,
        "water-fast-z-score": 3.9620290784653074,
        "rewrite-fast-z-score": -1.044465935734187
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Information diffusion epidemics in social networks .\nAbstract:\nWe study information diffusion on networks with multiple communities, where each community has its own set of nodes and edges. We propose an epidemic model that captures the effect of both global and local interactions between users within different communities. The proposed model is based on two key ideas: (1) we assume that there are some influential individuals who can spread information to their neighbors faster than others; and (2) we allow for cross-influences among neighboring communities through these influential individuals. Our main goal is to understand how the structure of the underlying network affects the spreading process. In particular, our contributions include:  1. We develop a novel mathematical framework to analyze the dynamics of information diffusion under the proposed epidemic model. 2. We show that if all communities have similar sizes then the number of infected nodes at time t grows as O(t log n), where n denotes the total number of nodes in the network. 3. We prove that if one community dominates the other ones by size then the number of infected individuals grows exponentially fast. 4. Finally, we provide extensive numerical experiments to validate our theoretical results.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Information diffusion epidemics in social systems . Abstract : We research information diffusion on networks with many communities , where each community has its own set of vertices and edges .We suggest an outbreak model that captures the impact of both regional and local interactions between users within various communities . The proposed theory is based on two fundamental ideas : ( 1 ) we suppose that there are some influential citizens who can distribute information to their residents faster than others ; and ( 2 ) we allow for cross - impact among neighboring communities through these powerful individuals .Our main goal is to study how the composition of the underlying network impacts the spreading process . In particular , our achievements include : 1 .We develop a new computational framework to analyze the dynamics of information diffusion under the suggested epidemic model . 2 .We see that if all communities have equal sizes then the quantity of infected nodes at time t grows as O ( t log n ) , where n represents the total quantity of nodes in the network . 3 .We prove that if one community dominates the other ones by size then the quantity of infected individuals grows exponentially rapidly . 4 .Finally , we provide extensive numerical studies to validate our theoretical results .",
        "rewrite_text": "In this study, we investigate the dynamics of information diffusion across networks characterized by multiple communities, each defined by its own vertices and edges. We introduce a novel outbreak model that effectively captures the influence of both regional and local interactions among users within these distinct communities. Our theoretical framework is grounded in two key concepts: first, we identify the presence of influential individuals who can disseminate information to their peers more rapidly than others; second, we account for the cross-impact that these influential figures can have on adjacent communities. The primary objective of our research is to understand how the structural composition of the underlying network influences the information spreading process.\n\nOur findings are significant and multifaceted. Firstly, we have developed a comprehensive computational framework that facilitates the analysis of information diffusion dynamics as dictated by our proposed epidemic model. Secondly, we demonstrate that in scenarios where all communities are of equal size, the number of infected nodes at a given time t increases at a rate of O(t log n), where n denotes the total number of nodes within the network. Thirdly, we establish that if one community significantly outnumbers the others, the growth of infected individuals occurs at an exponential rate. Lastly, we conduct extensive numerical simulations to corroborate our theoretical predictions, providing a robust validation of our model. This research contributes to a deeper understanding of information diffusion mechanisms in social systems, highlighting the critical role of community structure and influential agents in shaping the spread of information.",
        "ori-fast-z-score": -1.4501047335684953,
        "water-fast-z-score": 6.225870853937484,
        "rewrite-fast-z-score": 2.0250370845489347
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The HELLAS2XMM survey. X. The bolometric output of luminous obscured quasars: The Spitzer perspective .\nAbstract:\nWe present the results on the infrared (IR) properties of X-ray selected AGN in the HELLAS2Xray sample, based on observations with the Infrared Array Camera (IRAC; Fazio et al., 2004 ) and Multiband Imaging Photometer for Spitzer (MIPS; Rieke et al., 2004 ) . We find that the IR luminosity is dominated by emission at wavelengths longer than 24 microns, which we interpret as thermal dust emission heated by an active nucleus. This result confirms previous findings obtained using ISO data (e.g. , Rowan-Robinson & Crawford , 1991 ; Haas et al. , 2003 ) , but it also shows that this component can be detected even when the nuclear activity is heavily obscured along our line-of-sight to the central engine. Our analysis indicates that the fraction of absorbed sources increases towards higher redshifts, suggesting that most of these objects are Compton-thick candidates.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The HELLAS2XMM survey . X .The bolometric output of luminous obscured quasars : The Spitzer focus . Abstract : We present the results on the infrared ( IR ) characteristics of X - ray selected AGN in the HELLAS2Xray sample , based on observations with the Infrared Array Camera ( IRAC ; Fazio et al . , 2004 ) and Multiband Imaging Photometer for Spitzer ( MIPS ; Rieke et al . , 2004 ) .We see that the IR luminosity is dominated by absorption at wavelengths greater than 24 microns , which we perceive as heat dust absorption warmed by an active nucleus . This result confirms previous findings obtained using ISO data ( e . g ., Rowan - Robinson & Crawford , 1991 ; Haas et al . , 2003 ) , but it also shows that this factor can be spotted even when the atomic activity is heavily distorted along our line - of - view to the main engine .Our study implies that the fraction of absorbed sources changes towards higher redshifts , showing that most of these objects are Compton - thick candidates .",
        "rewrite_text": "We present findings on the infrared (IR) properties of X-ray selected active galactic nuclei (AGN) within the HELLAS2XMM survey, utilizing data gathered from the Infrared Array Camera (IRAC; Fazio et al., 2004) and the Multiband Imaging Photometer for Spitzer (MIPS; Rieke et al., 2004). Our analysis reveals that the IR luminosity of luminous obscured quasars is predominantly influenced by absorption at wavelengths exceeding 24 microns. This phenomenon is interpreted as the result of heat generated by dust that is heated by the active nucleus of the quasar. Our results corroborate earlier studies conducted with ISO data (e.g., Rowan-Robinson & Crawford, 1991; Haas et al., 2003), while also demonstrating that this absorption effect can be detected even when the atomic activity is significantly obscured from our observational perspective of the central engine. Furthermore, our research indicates a variation in the proportion of absorbed sources at higher redshifts, suggesting that a majority of these quasars are likely Compton-thick candidates. This study enhances our understanding of the IR characteristics of AGN and underscores the importance of considering absorption effects when analyzing the luminosity of these distant objects. The implications of our findings are significant for the study of the evolution of AGN and their environments, particularly in the context of cosmic history and the growth of supermassive black holes.",
        "ori-fast-z-score": -1.0834726777719228,
        "water-fast-z-score": 3.731961445658845,
        "rewrite-fast-z-score": 0.21566554640687682
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  On Einstein clusters as galactic dark matter halos .\nAbstract:\nWe present the results of an analysis of galaxy cluster data in terms of their gravitational lensing properties and X-ray emission, with particular emphasis on the comparison between observed and predicted values for the mass-to-light ratio M/L. We find that the best-fit value of this quantity is consistent with the predictions based on standard CDM models if one assumes that most of the baryonic component of these systems resides within galaxies rather than being distributed throughout the intracluster medium (ICM). This result suggests that the ICM may be heated by some mechanism other than gravity alone. \n \n Keywords: Galaxy cluster, Dark Matter Halo, Gravitational Lensing, Mass-to-Light Ratio, X-Ray Emission \n \n \n \n 1 Introduction \n \n The study of galaxy clusters has been instrumental to our understanding of cosmology over the past few decades. In fact, it was through observations of galaxy clusters that we first discovered evidence supporting the existence of non-baryonic dark matter  1  . Today, galaxy clusters are still used extensively to test theories about structure formation  2  , and they provide important constraints on cosmological parameters such as the Hubble constant  3  or the equation-of-state parameter w  4  . \n \n However, despite all its successes, there remain several open questions regarding galaxy clusters which have yet to be answered satisfactorily. For example, while current observational techniques allow us to measure accurately the total amount of light emitted by a galaxy cluster, it remains difficult to determine how much of this light comes from stars inside individual galaxies versus diffuse gas located outside them  5  . Similarly, although we can estimate fairly well the total gravitating mass of a galaxy cluster using various methods  6  , it is not clear what fraction of this mass is associated with visible objects like galaxies  7, 8  . Finally, even though we know that galaxy clusters contain large amounts of hot plasma  9  , it is unclear whether this material is gravitationally bound to the system  10  .\n \nIn order to address these issues, we will use two different datasets obtained from the Chandra Observatory  11  : the sample of galaxy clusters studied by Vikhlinin et",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : On Einstein complexes as galactic dark matter halos . Abstract : We present the conclusion of an assessment of galaxy cluster data in terms of their gravitational lensing behavior and X - ray radiation , with particular emphasis on the comparison between seen and anticipated readings for the mass - to - light density M / L .We see that the best - fitting value of this quantity is compatible with the estimates based on normal CDM models if one suppose that most of the baryonic component of these systems resides within galaxies rather than being dispersed throughout the intracluster medium ( ICM ) . This result suggests that the ICM could be heated by some process other than gravity alone .Keywords : Galaxy cluster , Dark Matter Halo , Gravitational Lensing , Mass - to - Light Ratio , X - Ray Emission 1 Introduction The investigation of galaxy galaxies has been instrumental to our understanding of cosmology over the previous few century . In indeed , it was through observations of galaxy clusters that we first discovered evidence proving the existence of non - baryonic black material 1 .Today , galaxy clusters are still used heavily to test assumptions about structure formation 2 , and they pose important restrictions on cosmological factors such as the Hubble constant 3 or the equation - of - state variable f 4 . However , despite all its successes , there remain many open questions regarding galaxy clusters which have yet to be answered satisfactorily .For instance , while contemporary observational techniques permit us to measure correctly the total quantity of light emitted by a galaxy cluster , it remains impossible to predict how many of this light originates from stars inside individual stars vs diffuse gas located outside them 5 . Similarly , although we can calculate fairly good the total gravitating mass of a galaxy cluster using numerous technologies 6 , it is not clear what fraction of this mass is associated with seen bodies like stars 7 , 8 .Finally , even though we know that galaxy regions contain significant amounts of bright plasma 9 , it is uncertain whether this material is gravitationally bound to the system 10 . In order to overcome these problems , we will use two different datasets obtained from the Chandra Observatory 11 : the sample of galaxy galaxies studied by Vikhlinin et",
        "rewrite_text": "**Title:** On Einstein Complexes as Galactic Dark Matter Halos\n\n**Abstract:** This article presents findings from an extensive analysis of galaxy cluster data, focusing on their gravitational lensing characteristics and X-ray emissions. A key aspect of our study is the comparison between observed and expected values for the mass-to-light density ratio (M/L). Our results indicate that the optimal value for this ratio aligns with predictions derived from conventional Cold Dark Matter (CDM) models, provided we assume that a significant portion of the baryonic matter in these systems is concentrated within galaxies rather than being distributed throughout the intracluster medium (ICM). This observation raises the possibility that the heating of the ICM may be influenced by mechanisms beyond gravitational interactions alone.\n\nThe exploration of galaxy clusters has played a crucial role in advancing our understanding of cosmology over the past century. Notably, it was through the study of these clusters that we first gathered compelling evidence for the existence of non-baryonic dark matter. Presently, galaxy clusters continue to serve as vital tools for testing theoretical frameworks regarding structure formation and impose critical constraints on cosmological parameters, including the Hubble constant and the equation of state parameter. Despite the progress made, numerous unresolved questions persist concerning the nature of galaxy clusters. For example, while modern observational techniques allow for accurate measurements of the total light emitted by a galaxy cluster, distinguishing the contributions from individual stars versus diffuse gas remains a challenge. Additionally, although we can estimate the total gravitating mass of a galaxy cluster through various methods, the proportion of this mass attributable to visible entities, such as stars, is still uncertain. Furthermore, despite the known presence of substantial amounts of luminous plasma within galaxy regions, its gravitational binding status to the overall system is yet to be clarified. To address these challenges, we utilize two distinct datasets from the Chandra Observatory, specifically focusing on the sample of galaxy clusters analyzed by Vikhlinin et al. This research aims to deepen our understanding of the complex dynamics at play within galaxy clusters and their implications for dark matter theories. \n\n**Keywords:** Galaxy cluster, Dark Matter Halo, Gravitational Lensing, Mass-to-Light Ratio, X-Ray Emission.",
        "ori-fast-z-score": -1.8407159732336889,
        "water-fast-z-score": 7.167432805632419,
        "rewrite-fast-z-score": -2.0410081542498273
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Multicolor observations of the afterglow of the short/hard GRB 050724 .\nAbstract:\nWe report on multiwavelength observations of the X-ray, optical and radio afterglows of the short-hard gamma-ray burst (GRB) 050724 detected by Swift satellite at 07:24:06 UT on 24 July 2005. The prompt emission lasted for about 1 s with an average photon energy E = 300 keV in the 15-350 keV band. We find that the temporal decay index is ~1.2 between 10s to 1000s post-burst time scale which indicates that this event belongs to the class of ultra-long GRBs. In addition we also detect a possible rebrightening feature around 100s post-burst time-scale. Our spectral analysis shows that the spectrum can be fitted well using both single power-law model as well as broken power law model. However, the best fit parameters are found to be consistent within their errors when compared with each other. Using our multi-wavelength data set, we estimate the total energetics associated with this event to be ~3 x 1044 erg.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Multicolor observations of the afterglow of the short / hard GRB 050724 . Abstract : We report on multiwavelength studies of the X - ray , optical and radio afterglows of the short - hard beta - ray burst ( GRB ) 050724 detected by Swift satellite at 07 : 24 : 06 UT on 24 July 2005 .The prompt emission lasted for about 1 s with an estimated photon energy E = 300 keV in the 15 - 350 keV band . We see that the temporal decay rate is ~ 1 . 2 between 10s to 1000s post - burst time scale which implies that this event belongs to the class of ultra - long GRBs .In addition we also observe a possible rebrightening characteristic around 100s post - burst time - range . Our spectral evaluation demonstrates that the spectrum can be fit well using both single power - law method as also as broken power law method .However , the best fit factors are found to be compatible within their errors when compared with each other . Using our multi - wavelength information pool , we estimate the total energetics associated with this event to be ~ 3 x 1044 erg .",
        "rewrite_text": "We present a comprehensive analysis of the multiwavelength observations of the afterglow associated with the short/hard gamma-ray burst (GRB) 050724, which was detected by the Swift satellite on July 24, 2005, at 07:24:06 UT. The prompt emission of this GRB was notably brief, lasting approximately 1 second, with a peak photon energy estimated at 300 keV within the 15-350 keV energy range. Our findings indicate a temporal decay rate of approximately 1.2 during the time interval from 10 seconds to 1000 seconds following the burst, suggesting that GRB 050724 may be classified among the ultra-long GRBs. Furthermore, we observed a potential rebrightening feature occurring around 100 seconds post-burst, which adds complexity to the afterglow behavior. \n\nIn our spectral analysis, we explored various fitting methods, including both single power-law and broken power-law models. The results indicate that both fitting approaches yield compatible best-fit parameters within their respective uncertainties, highlighting the robustness of our spectral evaluation. By leveraging the extensive multiwavelength data collected, we estimated the total energy output associated with this GRB event to be approximately 3 x 10^44 erg. This study contributes to the growing body of knowledge regarding the characteristics and energetics of short/hard GRBs, providing valuable insights into their afterglow phenomena and underlying physical processes.",
        "ori-fast-z-score": 1.5460413650478515,
        "water-fast-z-score": 6.110100926607787,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Finding Solar System Analogs With SIM and HIPPARCOS: A White Paper for the ExoPlanet Task Force .\nAbstract:\nThe NASA Exoplanet Exploration Program (NExEP) has been tasked with finding Earth-like planets in other solar systems, but how can we find these worlds? The authors present an approach to identifying nearby stars that are similar to our Sun by using data from the Space Interferometry Mission (SIM), which is scheduled to launch in 2009, along with Hipparcos astrometric measurements.  They show that this method will allow us to identify thousands of candidate targets within 25 pc of the Sun where future space missions could search for exoplanets. This white paper was presented at the 2007 International Astronomical Union General Assembly meeting held August 24-30, 2007 in Prague, Czech Republic. The NASA Exoplanet Explorations Program (NExEP)  has been tasked with  finding Earth-like planets in  other solar systems.   How do we find them?  In this white paper,  the authors present an approach to  identifying nearby stars that are  similar to our Sun by  using data from the  Space Interferometry Mission  (SIM),  which is scheduled to launch  in 2009,   along with Hipparcos  astrometric measurements.  They  show that this method will  allow us to  identify thousands of  candidate targets within  25 pc of the Sun   where future space missions  could search for exoplanet",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Finding Solar System Analogs With SIM and HIPPARCOS : A White Paper for the ExoPlanet Task Force . Abstract : The NASA Exoplanet Exploration Program ( NExEP ) has been tasked with finding Earth - like stars in other solar systems , but how can we find these worlds ?The authors present an way to identifying distant stars that are comparable to our Sun by using data from the Space Interferometry Mission ( SIM ) , which is slated to launch in 2009 , along with Hipparcos astrometric measurements . They show that this technology will provide us to identify thousands of candidate targets within 25 pc of the Sun where upcoming space missions could search for exoplanets .This white paper was presented at the 2007 International Astronomical Union General Assembly conference held August 24 - 30 , 2007 in Prague , Czech Republic . The NASA Exoplanet Explorations Program ( NExEP ) has been responsible with finding Earth - like stars in other solar systems .How do we find them ? In this white paper , the writers present an way to identifying local stars that are comparable to our Sun by using data from the Space Interferometry Mission ( SIM ) , which is slated to launch in 2009 , along with Hipparcos astrometric measurements .They show that this algorithm will let us to identify thousands of candidate targets within 25 pc of the Sun where upcoming space missions could search for exoplanet",
        "rewrite_text": "**Title:** Identifying Solar System Analogues: A White Paper for the ExoPlanet Task Force Utilizing SIM and HIPPARCOS Data\n\n**Abstract:** The NASA Exoplanet Exploration Program (NExEP) is dedicated to the discovery of Earth-like stars within other solar systems, raising the question of how to effectively locate these celestial bodies. In this white paper, the authors propose a method for identifying stars that closely resemble our Sun by leveraging data from the Space Interferometry Mission (SIM), which is scheduled for launch in 2009, in conjunction with astrometric measurements from the Hipparcos satellite. The authors demonstrate that this innovative approach will enable the identification of thousands of potential target stars located within 25 parsecs of the Sun, providing a rich field for future space missions aimed at exoplanet exploration. This work was presented at the 2007 International Astronomical Union General Assembly, held from August 24 to 30 in Prague, Czech Republic. By utilizing the advanced capabilities of SIM and the precise astrometric data from Hipparcos, the authors outline a robust algorithm that enhances our ability to pinpoint solar analogs. This identification process is crucial for guiding upcoming missions that will search for exoplanets, as it focuses on stars that share similar characteristics to our own Sun. The findings presented in this paper not only contribute to the ongoing efforts of the NExEP but also lay the groundwork for future astronomical investigations aimed at understanding the potential for life beyond our solar system.",
        "ori-fast-z-score": 1.2701705922171767,
        "water-fast-z-score": 6.423640548375729,
        "rewrite-fast-z-score": -0.953998092005724
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Rapid Mixing of Gibbs Sampling on Graphs that are Sparse on Average .\nAbstract:\nWe consider the problem of sampling from an arbitrary probability distribution over graphs with fixed number of nodes and edges, where each node has a given degree constraint. We propose a novel algorithm for this task based on the idea of rapidly mixing Markov chains known as Metropolis-Hastings algorithms. Our approach is to use a special type of graph called a tree decomposition which allows us to decompose our original problem into smaller problems in such a way that we can efficiently sample from these smaller problems using existing techniques. In particular, we show how to construct a tree decomposition of any input graph G = (V , E) by first constructing a tree decomposition of its line graph L(G). Then, we prove that if there exists a polynomial time algorithm A that solves one of these small subproblems then it also implies the existence of a polynomial time algorithm B that solves all of them simultaneously. Finally, we present experimental results showing that our proposed method outperforms state-of-the-art methods when applied to several real-world datasets.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Rapid Mixing of Gibbs Sampling on Graphs that are Sparse on Average . Abstract : We consider the question of sampling from an arbitrary likelihood distribution over graphs with constant number of vertices and edges , where each node has a given degree constraint .We suggest a new algorithm for this job based on the idea of quickly mixing Markov chains known as Metropolis - Hastings algorithms . Our idea is to use a unique kind of graph named a tree decomposition which allows us to decompose our existing problem into tiny problems in such a way that we can efficiently sample from these bigger difficulties using existing techniques .In particular , we prove how to build a tree decomposition of any input graph G = ( V , E ) by first creating a tree decomposition of its line graph L ( G ) . Then , we prove that if there exists a polynomial time algorithm A that solves one of these little subproblems then it also proves the existence of a polynomial time algorithm B that solves all of them simultaneously .Finally , we present research results promising that our proposed approach outperforms state - of - the - art methods when applied to several real - time datasets .",
        "rewrite_text": "In this article, we address the challenge of sampling from arbitrary likelihood distributions over graphs characterized by a constant number of vertices and edges, while adhering to specific degree constraints for each node. We introduce a novel algorithm inspired by the rapid mixing properties of Markov chains, specifically utilizing the Metropolis-Hastings framework. Our approach leverages a specialized graph structure known as a tree decomposition, which enables us to break down the complex sampling problem into smaller, more manageable subproblems. This decomposition facilitates efficient sampling from these subproblems using established techniques.\n\nWe demonstrate the construction of a tree decomposition for any given graph \\( G = (V, E) \\) by first deriving a tree decomposition from its line graph \\( L(G) \\). Our theoretical contributions include a proof that if a polynomial-time algorithm \\( A \\) can solve one of these smaller subproblems, it implies the existence of another polynomial-time algorithm \\( B \\) capable of solving all subproblems concurrently. This result underscores the interconnectedness of the subproblems and the efficiency of our approach.\n\nFurthermore, we present empirical findings indicating that our proposed method significantly outperforms current state-of-the-art algorithms when tested on various real-time datasets. These results not only validate the effectiveness of our algorithm but also highlight its potential applicability in practical scenarios where rapid and efficient sampling from complex graph structures is required. Overall, our work contributes to the field by providing a robust framework for sampling in constrained graph environments, paving the way for further advancements in probabilistic graphical models.",
        "ori-fast-z-score": -0.30779350562554625,
        "water-fast-z-score": 4.77212697612801,
        "rewrite-fast-z-score": -1.0864289525102224
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A novel L-band imaging search for giant planets in the Tucana and Beta Pictoris moving groups .\nAbstract:\nWe report on two new searches for giant planets around young stars using direct imaging at 1.25 mm (L band). The first is an unbiased survey of 23 nearby, low-mass members of the Tucana-Horologium association with ages between 10-50 Myr. We find no evidence for companions down to masses as small as 0.1 M Jup . In addition we present results from a targeted search for companions to three young stars in the Beta Pic Moving Group that are known to harbor debris disks. These targets have ages ranging from 12-30 Myr and were selected because they show signs of youthful activity such as accretion or outflows. No companions were detected within projected separations of 100 AU. Our non-detections suggest that either these systems do not form planetary mass objects beyond 50 AU over this age range, or that any companions formed closer-in but have since been ejected by dynamical interactions.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A novel L - band scanning search for big planets in the Tucana and Beta Pictoris move bands . Abstract : We report on two new searches for big planets around young galaxies using direct scanning at 1 . 25 mm ( L band ) .The first is an unbiased survey of 23 nearby , low - mass members of the Tucana - Horologium association with sizes between 10 - 50 Myr . We see no evidence for companions down to masses as big as 0 . 1 M Jup .In addition we present results from a focused quest for companions to three young galaxies in the Beta Pic Moving Group that are known to harbor debris particles . These targets have ages ranging from 12 - 30 Myr and were chosen because they show traces of young action such as accretion or outflows .No companions were detected within projected separations of 100 AU . Our non - detections imply that either these systems do not constitute planetary mass items beyond 50 AU over this age range , or that any companions originated closer - in but have since been expelled by dynamical interactions .",
        "rewrite_text": "Title: A Novel L-Band Scanning Search for Large Planets in the Tucana and Beta Pictoris Moving Groups\n\nAbstract: In this study, we present the findings from two recent searches aimed at identifying large planets around young stellar systems, utilizing direct scanning techniques at a wavelength of 1.25 mm (L band). The first component of our research involved an unbiased survey of 23 nearby, low-mass stars within the Tucana-Horologium association, which are estimated to be between 10 to 50 million years old. Our observations did not reveal any companions with masses as low as 0.1 times that of Jupiter (M_Jup), suggesting a scarcity of large planetary bodies in this age group.\n\nAdditionally, we conducted a targeted investigation into three young stars within the Beta Pictoris Moving Group, which are known to possess debris disks. These stars, aged between 12 to 30 million years, were selected based on their observable signs of youthful activity, such as accretion processes and outflows. Despite our focused efforts, we found no evidence of companions within projected separations of 100 astronomical units (AU) from these stars.\n\nThe absence of detected companions in both surveys leads us to conclude that either these stellar systems do not host planetary-mass objects beyond 50 AU at this stage in their evolution, or that any potential companions may have formed closer to their host stars but were subsequently ejected due to dynamical interactions. Our findings contribute to the understanding of planet formation and dynamics in young stellar environments, highlighting the challenges in detecting large planets in these systems. Further investigations are warranted to explore the implications of these non-detections on the formation and migration of planetary bodies in the early stages of stellar evolution.",
        "ori-fast-z-score": -1.1470786693528088,
        "water-fast-z-score": 4.529108136578382,
        "rewrite-fast-z-score": 0.4508348173337161
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Lecture notes on Optical Quantum Computing .\nAbstract:\nThe lecture notes are available at http://arxiv.org/abs/quant-ph/0405033 . The lecture notes cover the following topics:  - Introduction to quantum information theory and quantum computation - Basic concepts in optics, such as polarization states, coherent light sources etc.  - Single photon sources - Linear optical elements (beam splitters, phase shifters) - Nonlinear optical elements (optical parametric oscillators)  - Entangled photons - Photon detectors - Quantum gates based on linear optics - Quantum gates based on nonlinear optics - Measurement-based quantum computing - Summary of experimental results - References This is an introductory course for students interested in learning about quantum information processing using photonic qubits. It covers basic concepts in optics, single photon sources, linear and nonlinear optical elements, entangled photons, photon detectors, quantum gates based on linear op- tics, quantum gates based on nonlinear op- tics, measurement-based quantum com- puting, and summary of experimen- tal results.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Lecture notes on Optical Quantum Computing . Abstract : The lecture notes are available at http : / / arxiv . org / abs / quant - ph / 0405033 .The lecture notes cover the following topics : - Introduction to quantum information physics and quantum computation - Basic concepts in optics , such as polarization states , coherent light channels etc . - Single photon sources - Linear optical elements ( beam splitters , wave shifters ) - Nonlinear optical elements ( laser parametric oscillators ) - Entangled photons - Photon detectors - Quantum gates based on linear optics - Quantum gates based on nonlinear optics - Measurement - based quantum computing - Summary of theoretical results - References This is an introductory program for students interested in understanding about quantum information processing involving photonic qubits .It covers basic concepts in optics , single photon sources , linear and nonlinear optical elements , entangled photons , photon detectors , quantum gates based on linear op - tics , quantum gates based on nonlinear op - tics , measurement - based quantum com - puting , and overview of experimen - tal findings .",
        "rewrite_text": "This article presents comprehensive lecture notes on Optical Quantum Computing, accessible at http://arxiv.org/abs/quant-ph/0405033. The notes serve as an introductory resource for students eager to delve into the realm of quantum information processing, particularly focusing on photonic qubits. The content is structured to provide a foundational understanding of several key topics in quantum information physics and quantum computation. \n\nBeginning with an overview of quantum information and computation, the notes introduce essential concepts in optics, including polarization states and coherent light channels. The discussion progresses to single photon sources, which are crucial for quantum applications, followed by an exploration of linear optical elements such as beam splitters and wave shifters. The notes also delve into nonlinear optical elements, specifically laser parametric oscillators, which play a significant role in generating entangled photons.\n\nFurther, the lecture notes examine photon detectors and their importance in quantum experiments. A detailed analysis of quantum gates based on both linear and nonlinear optics is provided, highlighting their functionality in quantum computing. The concept of measurement-based quantum computing is also discussed, emphasizing its relevance in the broader context of quantum information processing.\n\nThe lecture notes culminate in a summary of theoretical results, providing a cohesive understanding of the subject matter. Additionally, a comprehensive list of references is included for further reading and exploration. Overall, these notes are designed to equip students with the necessary knowledge and tools to engage with the exciting field of optical quantum computing.",
        "ori-fast-z-score": -0.22645540682891913,
        "water-fast-z-score": 2.49100947511811,
        "rewrite-fast-z-score": 2.8507865803991934
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A New Template Family For The Detection Of Gravitational Waves From Comparable Mass Black Hole Binaries .\nAbstract:\nWe present the first results for a new template family, called SEOBNRv4HM, which is designed to detect gravitational waves (GWs) emitted by comparable mass black hole binaries with total masses between 10 and 100 solar masses. We show that this template family can be used in searches for GW signals from binary black holes at current ground-based detectors such as Advanced LIGO/Virgo and KAGRA. In addition we demonstrate how these templates are useful for parameter estimation studies using simulated data sets. Finally, we discuss possible improvements on our work. Keywords: Binary black hole -Gravitational wave detector -Template family -Parameter estimation -SEOBNRv4HM -Advanced Virgo -LIGO -KAGRA -Black hole -Binary system -Gravitational wave signal -Gravitational wave astronomy -Gravitational wave source -Signal-to-noise ratio -Search algorithm -Simulation",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A New Template Family For The Detection Of Gravitational Waves From Comparable Mass Black Hole Binaries . Abstract : We present the first findings for a new template family , known SEOBNRv4HM , which is designed to identify gravitational waves ( GWs ) emitted by similar mass black hole binaries with total masses between 10 and 100 solar masses .We see that this template family can be used in searches for GW signals from binary dark holes at current ground - based detectors such as Advanced LIGO / Virgo and KAGRA . In addition we prove how these templates are helpful for parameter estimation studies employing simulated data sets .Finally , we review possible advances on our work . Keywords : Binary white hole - Gravitational wave detector - Template family - Parameter estimation - SEOBNRv4HM - Advanced Virgo - LIGO - KAGRA - Black hole - Binary network - Gravitational wave noise - Gravitational wave astronomy - Gravitational wave source - Signal - to - noise ratio - Search algorithm - Simulation",
        "rewrite_text": "We introduce a novel family of templates, referred to as SEOBNRv4HM, specifically developed for the detection of gravitational waves (GWs) generated by comparable mass black hole binaries, with total masses ranging from 10 to 100 solar masses. This groundbreaking research marks the first application of this template family, which is tailored for identifying GW signals from binary black holes using current ground-based detectors, including Advanced LIGO, Virgo, and KAGRA. Our findings demonstrate that the SEOBNRv4HM templates significantly enhance the search for gravitational wave signals, providing a robust framework for detecting these elusive phenomena. Furthermore, we illustrate the utility of these templates in parameter estimation studies, utilizing simulated datasets to validate their effectiveness. The results indicate that the SEOBNRv4HM templates not only improve the sensitivity of gravitational wave searches but also facilitate more accurate parameter estimation of the binary systems involved. In conclusion, we discuss potential future advancements stemming from this work, highlighting the implications for gravitational wave astronomy and the broader understanding of black hole dynamics. This research contributes to the ongoing efforts in the field of gravitational wave detection and opens new avenues for exploring the properties of binary black hole systems. \n\nKeywords: Binary black hole, Gravitational wave detection, Template family, Parameter estimation, SEOBNRv4HM, Advanced LIGO, Virgo, KAGRA, Gravitational wave astronomy, Signal-to-noise ratio, Search algorithms, Simulation.",
        "ori-fast-z-score": -1.5882027766319677,
        "water-fast-z-score": 2.9104275004359956,
        "rewrite-fast-z-score": -0.7559289460184544
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Generalized CRF-structures .\nAbstract:\nWe introduce the concept of generalized conditional random fields (GCRFs) and show how they can be used to model arbitrary probability distributions over structured data sets, such as sequences or trees.  We present an efficient algorithm for learning GCRF parameters using gradient descent on the log-likelihood objective function.   Finally we demonstrate that our approach is able to learn accurate models for several challenging sequence labeling tasks including part-of-speech tagging in natural language processing and protein secondary structure prediction in bioinformatics. Conditional Random Fields (CRFs) (Lafferty et al., 2001 ) are undirected graphical models which have been successfully applied to many problems involving sequential data, e.g. (Sha & Pereira, 2003) . In this work, we propose Generalized Conditional Random Fields (GCRFs), a generalization of CRFs which allows us to represent any distribution over structured data sets like sequences or trees. The key idea behind GCRFs is to use a set of latent variables to capture dependencies between different parts of the input space. This enables us to efficiently compute the partition function required by standard CRFs with dynamic programming techniques. Furthermore, it also makes it possible to train GCRFs using gradient-based methods similar to those employed for Maximum Entropy Markov Models (MEMMs). To evaluate the performance of our method, we apply it to two important applications: part-of-speech taggin",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Generalized CRF - structures . Abstract : We introduce the idea of generalized conditional random fields ( GCRFs ) and explain how they can be used to model arbitrary likelihood distributions over structured data sets , such as sequences or trees .We present an efficient algorithm for learning GCRF variables using gradient descent on the log - likelihood objective function . Finally we prove that our approach is ability to teach exact models for numerous challenging gene labeling challenges including whole - of - voice tagging in natural language processing and gene secondary structure prediction in bioinformatics .Conditional Random Fields ( CRFs ) ( Lafferty et al . , 2001 ) are undirected graphical descriptions which have been successfully applied to many difficulties involving sequential data , e . g . ( Sha & Pereira , 2003 ) .In this research , we develop Generalized Conditional Random Fields ( GCRFs ) , a generalization of CRFs which allows us to model any distribution over structured data sets like sequences or trees . The main idea behind GCRFs is to use a setting of latent variables to capture dependencies between various parts of the input space .This enables us to easily compute the partition function required by traditional CRFs with dynamic programming tools . Furthermore , it also makes it able to train GCRFs using gradient - based methods similar to those utilized for Maximum Entropy Markov Models ( MEMMs ) .To assess the performance of our technique , we apply it to two essential users : part - of - voice taggin",
        "rewrite_text": "We present the concept of Generalized Conditional Random Fields (GCRFs), a novel framework designed to model complex likelihood distributions over structured datasets, including sequences and trees. Our work builds upon the foundational principles of Conditional Random Fields (CRFs), which have proven effective in addressing various sequential data challenges. By introducing GCRFs, we extend the capabilities of traditional CRFs, allowing for the incorporation of latent variables that capture intricate dependencies within the input space. This enhancement facilitates the computation of the partition function, a critical component in CRF modeling, through efficient dynamic programming techniques.\n\nIn our study, we propose an efficient algorithm for learning the parameters of GCRFs via gradient descent, optimizing the log-likelihood objective function. This approach not only streamlines the training process but also aligns with methodologies used in Maximum Entropy Markov Models (MEMMs), thereby broadening the applicability of our framework. To validate the effectiveness of GCRFs, we conduct experiments on two significant applications: part-of-speech tagging in natural language processing and gene secondary structure prediction in bioinformatics. Our results demonstrate that GCRFs can accurately model complex relationships in these challenging domains, providing precise solutions for gene labeling tasks and enhancing the performance of voice tagging systems.\n\nOverall, the introduction of GCRFs represents a significant advancement in the field of structured prediction, offering a robust tool for researchers and practitioners dealing with structured data. By leveraging the strengths of CRFs while addressing their limitations, GCRFs open new avenues for exploration in various applications, paving the way for future research in this area.",
        "ori-fast-z-score": -0.647150228929434,
        "water-fast-z-score": 6.127946159842712,
        "rewrite-fast-z-score": 1.3750477455423171
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Direct diameter measurement of a star filling its Roche Lobe: The semi-detached binary SS Leporis spatially resolved with VINCI/VLTI .\nAbstract:\nWe present the first direct determination of the stellar radius in an interacting binary system, using interferometric observations obtained with the VLTI and AMBER instrument. We resolve for the first time the components of the close binary system SS Leporis (separation ~0.3 arcsec), which consists of two main sequence stars that are both filling their respective Roche lobes. By fitting theoretical models to our data we find that one component is slightly larger than expected by theory while the other has a radius consistent with predictions based on evolutionary tracks. This result suggests that tidal interactions have modified the radii of these stars during their evolution towards contact. Our results also show that the orbital inclination angle i = 60 ± 5 degrees, as determined previously through radial velocity measurements, agrees well with our new estimate derived directly from the observed separation between the two stars. Keywords: Interferometry; Binary Stars; Stellar Radius",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Direct diameter calculation of a star filling its Roche Lobe : The semi - separated binary SS Leporis spatially resolved with VINCI / VLTI . Abstract : We report the first direct determination of the stellar radius in an interacting binary system , using interferometric observations derived with the VLTI and AMBER method .We resolve for the first time the parts of the close binary system SS Leporis ( separation ~ 0 . 3 arcsec ) , which consists of two principal sequence stars that are both filling their separate Roche petals . By fitting theoretical estimates to our information we find that one part is slightly larger than expected by theory while the other has a diameter compatible with predictions based on evolutionary tracks .This result suggests that tidal interactions have modified the radii of these stars during their development towards contact . Our results also demonstrate that the orbital inclination angle i = 60 ± 5 degrees , as determined earlier through radial speed measurements , agrees well with our new estimate calculated directly from the known separation between the two stars .Keywords: Interferometry; Binary Stars; Stellar Radius",
        "rewrite_text": "**Title:** Direct Diameter Calculation of a Star Filling Its Roche Lobe: The Semi-Separated Binary SS Leporis Resolved with VINCI/VLTI\n\n**Abstract:** In this study, we present the inaugural direct measurement of the stellar radius within an interacting binary system, utilizing interferometric observations obtained through the Very Large Telescope Interferometer (VLTI) and the AMBER technique. For the first time, we have successfully resolved the components of the close binary system SS Leporis, which exhibits a separation of approximately 0.3 arcseconds. This system comprises two main-sequence stars, both of which are filling their respective Roche lobes. By applying theoretical models to our observational data, we discovered that one star's radius is marginally larger than theoretical predictions, while the other star's diameter aligns closely with expectations derived from evolutionary models. This finding implies that tidal interactions have played a significant role in altering the radii of these stars as they evolved towards a state of contact. Furthermore, our analysis confirms that the orbital inclination angle of the system, previously estimated at i = 60 ± 5 degrees through radial velocity measurements, is consistent with our new direct calculation based on the known separation of the two stars. This research not only enhances our understanding of stellar evolution in binary systems but also underscores the importance of interferometric techniques in accurately determining stellar parameters. \n\n**Keywords:** Interferometry; Binary Stars; Stellar Radius",
        "ori-fast-z-score": -0.105999788000636,
        "water-fast-z-score": 3.8786538958710977,
        "rewrite-fast-z-score": -0.20203050891044214
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Geometric Complexity Theory VI: the flip via saturated and positive integer programming in representation theory and algebraic geometry .\nAbstract:\nThe present work is devoted to the study of the relationship between two different approaches to geometric complexity theory, namely the one based on the notion of ``saturated   problems introduced by Mulmuley et al., and that based on ``positive integer programs   (PIP) developed by Agrawal et al.. We show how these two notions are related through the concept of ``flip   which we define as an operation transforming any PIP into its corresponding saturated problem. In particular, this allows us to prove that all known NP-complete problems can be transformed into their equivalent saturated versions using only polynomial-time reductions. Finally, we give some applications of our results to computational commutative algebra and algebraic geometry. The present work was supported by NSF grant DMS-0504520. Geometric complexity theory has been studied extensively over the past decade with many important contributions made both in theoretical computer science and mathematics. One of the main goals of this field is to understand what kind of computational problems have efficient solutions when restricted to certain classes of input objects such as graphs or matrices. A central question here is: What does it mean for a computational problem to have an efficient solution?",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Geometric Complexity Theory VI : the flipping via saturated and positive integer programming in representation theory and algebraic topology . Abstract : The present work is devoted to the study of the relationship between two different methods to geometric complexity analysis , notably the one based on the notion of ` ` saturated problems proposed by Mulmuley et al . , and that based on ` ` positive integer programs ( PIP ) developed by Agrawal et al . . We see how these two notions are related through the idea of ` ` flip which we define as an action converting any PIP into its corresponding saturated problem .In particular , this enables us to prove that all known NP - full issues can be transformed into their equivalent saturated versions using only polynomial - time reductions . Finally , we give some applications of our findings to computational commutative geometry and algebraic geometry .The present work was supported by NSF grant DMS - 0504520 . Geometric complexity analysis has been studied significantly over the previous decade with many important works made both in computational computer science and mathematics .One of the main goals of this area is to see what sort of computational problems have efficient answers when confined to specified classes of input elements such as graphs or matrices . A central topic here is : What does it mean for a computational problem to have an efficient answer ?",
        "rewrite_text": "This article, titled \"Geometric Complexity Theory VI: The Flipping via Saturated and Positive Integer Programming in Representation Theory and Algebraic Topology,\" explores the intricate relationship between two distinct approaches to geometric complexity analysis. Specifically, it examines the concept of \"saturated problems\" as introduced by Mulmuley et al. and the framework of \"positive integer programs\" (PIP) developed by Agrawal et al. The authors introduce the notion of a \"flip,\" which is defined as a transformation that converts any PIP into its corresponding saturated problem. This innovative perspective allows for the demonstration that all known NP-complete problems can be efficiently transformed into their saturated counterparts through polynomial-time reductions.\n\nThe implications of this research extend into various fields, particularly computational commutative geometry and algebraic geometry, where the findings can be applied to enhance our understanding of computational complexities. The study is grounded in a decade of significant advancements in geometric complexity analysis, which has seen substantial contributions from both computer science and mathematics. A key objective in this domain is to identify which computational problems can be solved efficiently when restricted to specific classes of input, such as graphs or matrices. Central to this inquiry is the question of what constitutes an efficient solution to a computational problem.\n\nThe research presented in this article was made possible through the support of NSF grant DMS-0504520, highlighting the importance of funding in advancing scientific inquiry. Overall, this work not only bridges two important methodologies in geometric complexity theory but also opens new avenues for future research in related mathematical fields.",
        "ori-fast-z-score": 0.5669467095138409,
        "water-fast-z-score": 6.490973991846821,
        "rewrite-fast-z-score": 0.18107149208503706
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Optical Variability of Infrared Power Law-Selected Galaxies & X-ray Sources in the GOODS-South Field .\nAbstract:\nWe present optical variability measurements for infrared power law-selected galaxies and X-ray sources in the Chandra Deep Field South (CDFS). We use data obtained with the Hubble Space Telescope s Advanced Camera for Surveys to measure photometric redshifts, rest-frame absolute magnitudes, stellar masses, star formation rates, and specific star-formation rates for these objects over an eight-year baseline. The sample consists of 16,000 galaxies at 0 < z < 5 selected by their mid-infrared colors using Spitzer/IRAC observations as well as 1,500 X-ray point sources detected in deep Chandra observations. We find that both galaxy samples show significant levels of intrinsic variation on timescales ranging from days to years. For example, we detect more than 50% of our IRAC-selected galaxies at 3.6 microns and 80% at 4.5 microns showing >0.1 mag variations between epochs separated by one year or less. These results are consistent with previous studies which have found similar levels of variability among optically-selected quasars. However, we also find evidence suggesting that this level of variability is not driven solely by AGN activity but may be associated with other physical processes such as mergers and/or interactions within the host galaxy itself.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Optical Variability of Infrared Power Law - Selected Galaxies & X - ray Sources in the GOODS - South Field . Abstract : We report optical variability observations for infrared energy law - selected galaxies and X - ray sources in the Chandra Deep Field South ( CDFS ) .We use data acquired with the Hubble Space Telescope s Advanced Camera for Surveys to measure photometric redshifts , rest - frame relative magnitudes , stellar masses , sun formation rates , and particular galaxy - formation rates for these objects over an eight - year baseline . The sample consists of 16 , 000 galaxies at 0 < z < 5 selected by their mid - infrared colors using Spitzer / IRAC measurements as also as 1 , 500 X - ray point sources detected in deep Chandra measurements .We see that both universe samples show considerable rates of intrinsic variation on timescales ranging from weeks to decades . For instance , we find more than 50 % of our IRAC - selected galaxies at 3 . 6 microns and 80 % at 4 . 5 microns showing > 0 . 1 mag variations between epochs separated by one month or greater .These conclusions are compatible with previous research which have discovered similar rates of variability among optically - selected quasars . However , we also find proof suggesting that this level of variability is not driven solely by AGN activity but might be involved with other physical processes such as mergers and / or relationships within the host star itself .",
        "rewrite_text": "We present findings from our study on the optical variability of infrared power law-selected galaxies and X-ray sources located in the Chandra Deep Field South (CDFS). Utilizing data from the Hubble Space Telescope's Advanced Camera for Surveys, we conducted a comprehensive analysis to derive photometric redshifts, rest-frame relative magnitudes, stellar masses, star formation rates, and specific galaxy formation rates for a diverse sample of astronomical objects over an eight-year period. Our dataset comprises 16,000 galaxies with redshifts ranging from 0 to 5, selected based on their mid-infrared colors as determined by Spitzer/IRAC measurements, alongside 1,500 X-ray point sources identified through deep Chandra observations. \n\nOur results indicate that both samples exhibit significant intrinsic variability across timescales from weeks to decades. Notably, we observed that over 50% of the IRAC-selected galaxies at 3.6 microns and approximately 80% at 4.5 microns displayed variations exceeding 0.1 magnitudes between observations spaced one month apart or longer. These findings align with previous studies that reported similar variability rates among optically-selected quasars. However, our research also uncovers evidence suggesting that the observed variability may not be exclusively attributed to active galactic nucleus (AGN) activity. Instead, it may also be influenced by other physical phenomena, such as galactic mergers or interactions within the host star systems. This study enhances our understanding of the dynamic processes affecting galaxies and X-ray sources in the universe, highlighting the complexity of their optical variability and the potential roles of various astrophysical mechanisms.",
        "ori-fast-z-score": -1.116880781646981,
        "water-fast-z-score": 5.858884758402822,
        "rewrite-fast-z-score": 0.6527533657682196
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  X-ray Timing Observations of PSR J1930+1852 in the Crab-like SNR G54.1+0.3 .\nAbstract:\nWe report on X-ray timing observations of the pulsar candidate PSR J1930+1855 located at the center of the supernova remnant (SNR) G54.1+0. \n \n The source was discovered by Chandra and confirmed as a pulsar with XMM-Newton, but its spin period is not stable over time scales longer than one day. We performed two sets of pointed RXTE observations to study this behavior further. In both cases we found that the pulse frequency decreases smoothly during our observation runs. This trend can be described well using an exponential decay model for which we find characteristic timescales of 1.1 days and 0.7 days respectively. These values are consistent with those reported previously based on Chandra data alone. However, when comparing these results directly it should be noted that the uncertainties associated with the previous measurements were significantly larger due to the lower signal-to-noise ratio achieved with Chandra compared to RXTE.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : X - ray Timing Observations of PSR J1930 + 1852 in the Crab - like SNR G54 . 1 + 0 . 3 . Abstract : We report on X - ray timing observations of the pulsar candidate PSR J1930 + 1855 centered at the center of the supernova remnant ( SNR ) G54 . 1 + 0 .The source was studied by Chandra and reported as a pulsar with XMM - Newton , but its spin time is not stable over time ranges less than one day . We conducted two sets of pointed RXTE observations to study this behavior further .In both cases we concluded that the pulse frequency drops slowly during our observation running . This trend can be described good using an exponential decay model for which we find typical timescales of 1 . 1 hours and 0 . 7 days respectively .These figures are compatible with those published previously based on Chandra data alone . However , when comparing these results directly it should be mentioned that the uncertainties involved with the previous tests were significantly larger thanks to the smaller signal - to - noise proportion attained with Chandra compared to RXTE .",
        "rewrite_text": "We present a detailed analysis of X-ray timing observations of the pulsar candidate PSR J1930+1855, located at the heart of the supernova remnant (SNR) G54.1+0.3. Previous studies utilizing Chandra and XMM-Newton had identified this source as a pulsar; however, our findings reveal that its spin period exhibits instability over intervals shorter than one day. To investigate this phenomenon further, we conducted two series of pointed observations using the Rossi X-ray Timing Explorer (RXTE). Our analysis indicates a gradual decline in pulse frequency throughout the duration of our observations. This observed trend can be effectively modeled using an exponential decay function, yielding characteristic timescales of approximately 1.1 hours and 0.7 days for the two observation sets, respectively. Notably, these timescales align well with earlier results derived from Chandra data. However, it is essential to highlight that the uncertainties associated with the previous measurements were considerably larger due to the lower signal-to-noise ratio achieved with Chandra compared to RXTE. Our findings contribute to a deeper understanding of the pulsar's timing behavior and underscore the importance of utilizing high-sensitivity instruments for such studies. The implications of these results may extend to the broader context of pulsar research and the dynamics of supernova remnants, providing valuable insights into the mechanisms governing pulsar spin evolution and the characteristics of their surrounding environments.",
        "ori-fast-z-score": 0.3464101615137754,
        "water-fast-z-score": 5.9648090806346055,
        "rewrite-fast-z-score": 0.6831300510639733
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Analytic models of plausible gravitational lens potentials .\nAbstract:\nWe present analytic models for the potentials that can produce multiple images in strong gravitational lenses, and we use these to study how well different mass profiles are constrained by observations. We find that it is possible to obtain good constraints on both the total enclosed mass within an Einstein radius (the projected distance at which two images merge) and the slope of the density profile outside this radius using only three or four multiply-imaged systems with accurate photometric redshifts.  The results presented here should be useful for planning future surveys aimed at measuring dark matter properties through gravitational lensing. Gravitational lensing provides one of our most powerful tools for studying the distribution of dark matter in galaxies and clusters. In particular, if a galaxy cluster has been lensed into several distinct images, then the positions and fluxes of those images provide information about the shape of the cluster s gravitational potential as a function of position along its line-of-sight. This allows us to measure the total enclosed mass within some characteristic scale length, such as the Einstein radius, where two images merge together. However, because there are many degeneracies between the parameters describing the lens model, it is not always straightforward to determine what kind of mass profile best describes the data.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Analytic models of plausible gravity lens potentials . Abstract : We create analytic models for the potentials that can generate several images in good gravitational lenses , and we using these to study how well specific mass profiles are constrained by observed .We see that it is possible to obtain good limitations on both the total enclosed mass within an Einstein radius ( the projected height at which two images merge ) and the slope of the density profile outside this radius using only three or four multiply - imaged models with correct photometric redshifts . The results presented here should be valuable for planning upcoming studies aimed at assessing heavy material effects through gravity lensing .Gravitational lensing offers one of our most important skills for studying the distribution of dark matter in galaxies and clusters . In particular , if a galaxy cluster has been lensed into numerous separate images , then the places and fluxes of those pictures provide information about the form of the cluster s gravitational potential as a function of position along its line - of - view .This enables us to measure the total enclosed mass within some characteristic scale length , such as the Einstein radius , where two images merge together . However , because there are many degeneracies between the variables describing the lens model , it is not always feasible to identify what sort of mass profile best explains the information .",
        "rewrite_text": "Title: Analytic Models of Plausible Gravitational Lens Potentials\n\nAbstract: In this study, we develop analytic models for gravitational lens potentials capable of producing multiple images in effective gravitational lensing scenarios. Our primary objective is to evaluate how well specific mass profiles can be constrained by observational data. We demonstrate that it is feasible to derive accurate constraints on both the total enclosed mass within the Einstein radius—the critical distance at which two images converge—and the slope of the density profile beyond this radius. This can be achieved using only three or four multiply-imaged models, provided that the correct photometric redshifts are utilized. The findings presented in this paper are anticipated to be instrumental in guiding future research focused on investigating the effects of substantial mass distributions through gravitational lensing techniques.\n\nGravitational lensing serves as a crucial tool for probing the distribution of dark matter within galaxies and galaxy clusters. When a galaxy cluster is lensed into multiple distinct images, the positions and brightness of these images yield insights into the gravitational potential of the cluster as a function of its line-of-sight position. This allows for the measurement of the total enclosed mass within a defined scale length, such as the Einstein radius, where the images converge. However, the presence of numerous degeneracies among the parameters that define the lens model complicates the process of identifying the most suitable mass profile that aligns with the observed data. Our work addresses these challenges and provides a framework for enhancing the accuracy of mass profile determinations in gravitational lensing studies.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 6.236413804652249,
        "rewrite-fast-z-score": 0.3651483716701107
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Magnetic fluctuations in n-type high-$T_c$ superconductors reveal breakdown of fermiology .\nAbstract:\nWe report the observation of magnetic fluctuations at low temperatures and high fields in single crystals of YBa2Cu3O6+x (YBCO) with x=0.4, 0.45, and 0.5 using muon spin relaxation measurements. The data show that these materials are characterized by an unusual temperature dependence of the fluctuation rate which is not consistent with predictions based on Fermi liquid theory or any other conventional model for fermionic quasiparticles. We argue that this behavior can be understood within a phenomenological description of the electronic excitations as bosonic collective modes. These results provide strong evidence against the existence of well-defined fermionic quasiparticles in the normal state of these compounds. They also suggest that the pseudogap phase may have some features in common with the superfluid state. \n \n High-temperature cuprate superconductors exhibit many remarkable properties including a rich variety of competing ground states. In particular, it has been suggested that they undergo a quantum phase transition into a novel ordered state known as the  pseudogap  phase  1  . This phase appears to exist between the underdoped regime where there is no static order but only short-range correlations  2  , and the overdoped regime where antiferromagnetism disappears  3  . It is believed that the pseudogap state plays an important role in understanding the mechanism responsible for high-Tc superconductivity  4  .\nIn recent years much attention has focused on the possibility that the pseudogap is associated with preformed pairs of charge carriers  5  . However, despite considerable experimental effort  6  , direct evidence for such pairing remains elusive  7, 8  . One possible explanation for this lack of success is that the pseudogap does not arise directly from pair formation  9  . Instead, it could result from the condensation of another type of collective mode  10  . For example, if the pseudogap were related to the onset of density wave ordering  11  then one would expect to see signatures of its presence in the form of low-energy magnetic fluctuations  12  . Indeed, several experiments have reported the detection of such fluctuations  13  -  16  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Magnetic fluctuations in n - class high - $ T _ c $ superconductors reveal collapse of fermiology . Abstract : We report the observation of magnetic fluctuations at low temperatures and low fields in single crystals of YBa2Cu3O6 + x ( YBCO ) with x = 0 . 4 , 0 . 45 , and 0 . 5 utilizing muon spin relaxation measurements .The data reveal that these objects are marked by an peculiar thermal dependence of the fluctuation speed which is not consistent with predictions based on Fermi solid physics or any other usual description for fermionic quasiparticles . We argue that this behavior can be understood within a phenomenological explanation of the optical excitations as bosonic collective modes .These conclusions provide strong evidence against the existence of well - defined fermionic quasiparticles in the usual state of these compounds . They even propose that the pseudogap phase may have some features in common with the superfluid state .High - temperature cuprate superconductors exhibit several notable properties including a rich multitude of competing ground states . In particular , it has been proposed that they undergo a quantum phase shift into a novel ordered state known as the pseudogap phase 1 .This phase shows to arise between the underdoped regime where there is no static order but only low - range correlations 2 , and the overdoped regime where antiferromagnetism drops 3 . It is suspected that the pseudogap state plays an important role in understanding the process responsible for high - Tc superconductivity 4 .In recent years much attention has concentrated on the suggestion that the pseudogap is associated with preformed pairs of charge carriers 5 . However , despite considerable experimental effort 6 , direct data for such pairing remains elusive 7 , 8 .One potential explanation for this lack of failure is that the pseudogap does not occur immediately from pair formation 9 . Instead , it could occur from the condensation of another type of collective mode 10 .For instance , if the pseudogap were linked to the onset of density wave ordering 11 then one would expect to see signatures of its presence in the form of low - energy magnetic fluctuations 12 . Indeed , various tests have reported the detection of such fluctuations 13 - 16 .",
        "rewrite_text": "Title: Magnetic Fluctuations in n-Class High-Tc Superconductors Indicate a Collapse of Fermiology\n\nAbstract: In this study, we present our findings on the magnetic fluctuations observed at low temperatures and low magnetic fields in single crystals of YBa2Cu3O6+x (YBCO) with compositions x = 0.4, 0.45, and 0.5, utilizing muon spin relaxation techniques. Our measurements reveal an unusual thermal dependence of the fluctuation dynamics that deviates from the predictions made by Fermi solid physics and conventional models of fermionic quasiparticles. We propose that this unexpected behavior can be interpreted through a phenomenological framework that considers these fluctuations as bosonic collective modes rather than traditional fermionic excitations. This interpretation provides compelling evidence against the presence of well-defined fermionic quasiparticles in the typical state of these high-temperature superconductors. Furthermore, our findings suggest that the pseudogap phase may exhibit characteristics akin to those of a superfluid state. \n\nHigh-temperature cuprate superconductors are known for their complex properties, including a diverse array of competing ground states. A significant aspect of their behavior is the proposed transition into a novel ordered state referred to as the pseudogap phase. This phase is believed to emerge between the underdoped regime, characterized by the absence of static order and only short-range correlations, and the overdoped regime, where antiferromagnetic order diminishes. The pseudogap state is thought to be crucial for understanding the mechanisms underlying high-Tc superconductivity. Recent research has focused on the hypothesis that the pseudogap is related to the presence of preformed pairs of charge carriers. However, despite extensive experimental investigations, direct evidence for such pairing remains elusive. One possible explanation for this discrepancy is that the pseudogap may not arise directly from pair formation but could instead result from the condensation of alternative collective modes. For example, if the pseudogap is associated with the emergence of density wave ordering, we would anticipate observing corresponding low-energy magnetic fluctuations. Indeed, various studies have reported the detection of such fluctuations, reinforcing the need for further exploration of these phenomena in the context of high-Tc superconductivity.",
        "ori-fast-z-score": 0.47733437050543803,
        "water-fast-z-score": 7.319127014416716,
        "rewrite-fast-z-score": 1.469693845669907
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  R-matrices in Rime .\nAbstract:\nThe aim of this work is to present the results obtained by applying the method of R-matrix theory for solving problems related with scattering processes on complex systems, such as atomic nuclei and molecules. The main idea behind the approach presented here consists in using an effective interaction potential which takes into account all possible interactions between particles involved in the process under consideration. This allows one to obtain solutions that are valid at any distance between interacting objects. In particular we have applied our formalism to study elastic electron-atom collisions within the framework of the Born approximation. We show how it can be used to calculate cross sections for different types of atoms (hydrogen-like ions) and compare them with those calculated within other approaches. \n \n Keywords: Elastic Scattering, Cross Section, Electron Atom Collision, R-Matrix Theory, Effective Interaction Potential, Hydrogen-Like Ions. 1 Introduction \n \n The problem of calculating cross section for elastic electron-atom collision has been studied extensively during last decades both theoretically and experimentally  1  . It was shown  2  , however, that even if the exact wave function describing the system is known, the calculation of the corresponding cross section requires very complicated numerical procedures. Therefore various approximate methods were developed  3  -  6  . Among these approximations the most popular ones are: the first Born approximation  7  , the second Born approximation  8  , the distorted-wave Born approximation  9  , the Kohn variational principle  10  , etc.. All these methods allow one to find analytical expressions for the total cross section but they require some additional assumptions about the form of the wave functions or potentials describing the system under investigation. For example, in order to apply the first Born approximation one needs to know the exact solution of Schrödinger equation for the initial state of the system while the second Born approximation requires knowledge of the exact wave function for final states only  11  . On the other hand, the distorted-wave Born",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : R - matrices in Rime . Abstract : The goal of this project is to provide the results derived by using the method of R - matrix theory for solving cases linked with scattering systems on complex systems , such as atomic atoms and atoms .The main idea behind the approach given here consists in utilizing an efficient coupling potential which assumes into consideration all possible behaviors between particles involved in the process under consideration . This enables one to obtain answers that are applicable at any distance between interacting objects .In particular we have applied our formalism to study elastic electron - atom collisions within the framework of the Born method . We see how it can be used to estimate cross sections for different kinds of atoms ( hydrogen - like ions ) and contrast them with those estimated within other methods .Keywords : Elastic Scattering , Cross Section , Electron Atom Collision , R - Matrix Theory , Effective Interaction Potential , Hydrogen - Like Ions . 1 Introduction The question of calculating cross section for elastic electron - atom collision has been studied thoroughly during last decades both theoretically and experimentally 1 .It was shown 2 , however , that even if the exact wave function describing the system is known , the determination of the associated cross area needs very complicated mathematical procedures . Therefore several approximate approaches were developed 3 - 6 .Among these approximations the most popular ones are : the first Born approximation 7 , the second Born approximation 8 , the distorted - wave Born approximation 9 , the Kohn variational theory 10 , etc . . All these algorithms provide one to find analytical expressions for the total cross section but they demand some additional constraints about the form of the wave functions or potentials describing the system under inquiry . For instance , in order to apply the first Born approximation one needs to knowledge the exact solution of Schrödinger equation for the initial state of the system while the second Born approximation involves knowledge of the exact wave function for eventual states only 11 .On the other hand , the compressed - wave Born",
        "rewrite_text": "**Title:** R-Matrices in Rime\n\n**Abstract:** This study aims to present findings derived from the application of R-matrix theory to address scattering phenomena in complex systems, specifically focusing on atomic interactions. The core concept of our approach lies in the implementation of an effective coupling potential that accounts for all conceivable interactions between the particles involved in the scattering process. This comprehensive consideration allows for the derivation of results that remain valid across varying distances between the interacting entities. We have particularly utilized this framework to investigate elastic electron-atom collisions through the Born approximation method. Our analysis demonstrates the capability of this formalism to estimate cross sections for various atomic species, including hydrogen-like ions, and to compare these results with estimates obtained from alternative methodologies. \n\nThe calculation of cross sections for elastic electron-atom collisions has been extensively explored in both theoretical and experimental contexts over recent decades. Despite the availability of the exact wave function for the system, determining the corresponding cross-sectional area often necessitates complex mathematical techniques. Consequently, several approximate methods have been developed to simplify this process. Among these, the first Born approximation, second Born approximation, distorted-wave Born approximation, and Kohn variational theory are among the most widely utilized. While these approaches facilitate the derivation of analytical expressions for total cross sections, they typically impose additional requirements regarding the forms of wave functions or potentials that describe the system. For instance, the first Born approximation necessitates knowledge of the exact solution to the Schrödinger equation for the initial state, whereas the second Born approximation requires the exact wave function for the final states. This study contributes to the ongoing discourse by providing a robust framework that enhances the understanding of elastic scattering processes in atomic systems.\n\n**Keywords:** Elastic Scattering, Cross Section, Electron-Atom Collision, R-Matrix Theory, Effective Interaction Potential, Hydrogen-Like Ions.",
        "ori-fast-z-score": -0.6405126152203485,
        "water-fast-z-score": 7.1029732717618375,
        "rewrite-fast-z-score": -0.8006407690254357
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Quantifying social group evolution .\nAbstract:\nWe present an approach to quantify the evolutionary dynamics of social groups, based on their internal structure and interactions with other groups. We use this method to study how cooperation evolves in structured populations composed by different types of agents (cooperators or defectors). Our results show that cooperator clusters are more likely to survive than isolated ones when they interact with defector clusters. This effect is stronger for larger cluster sizes. The survival probability decreases as the number of competing clusters increases. Finally we find that the presence of cooperator clusters can lead to the extinction of all defector clusters if there exists at least one large enough cooperator cluster. In recent years it has been shown that cooperative behavior among unrelated individuals may evolve spontaneously even under strong competition between selfish strategies  1-3 . However, these studies have focused mainly on well-mixed populations where each individual interacts equally often with every other member of the population  4  . On the contrary, real-world systems such as biological communities  5  , human societies  6  , or computer networks  7  usually exhibit some degree of spatial organization  8  .\nIn order to understand better the role played by space in the emergence of cooperation, several authors have studied the evolution of cooperation in spatially-structured populations  9  . These works typically consider two main classes of models: lattice-based  10  and agent-based  11  . Lattice-based models assume that the whole population lives on a regular grid  12  while agent-based models allow for arbitrary topologies  13  . Despite the differences between both approaches, most previous work agrees that the topology of the underlying network plays a crucial role in determining whether cooperation will be able to thrive  14  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Quantifying social group evolution . Abstract : We present an view to quantify the evolutionary dynamics of social groups , based on their internal structure and interactions with other communities .We use this method to study how cooperation evolves in structured communities composed by various types of agents ( cooperators or defectors ) . Our results show that cooperator clusters are more likely to survive than scattered ones when they interact with defector clusters .This phenomenon is strengthened for larger cluster sizes . The survival likelihood falls as the number of competing clusters increases .Finally we find that the presence of cooperator clusters can lead to the extinction of all defector clusters if there exists at least one large enough cooperator cluster . In recent years it has been shown that cooperative performance among unrelated individuals might develop spontaneously especially under strong rivalry between selfish strategies 1 - 3 .However , these research have concentrated mostly on well - mixing communities where each individual interacts equally often with every other part of the population 4 . On the contrary , real - world systems such as genetic communities 5 , human organizations 6 , or computer networks 7 usually display some degree of spatial activity 8 .In order to explain better the part played by space in the emergence of cooperation , various scientists have researched the evolution of cooperation in spatially - organized populations 9 . These works typically consider two principal classes of models : lattice - based 10 and agent - based 11 .Lattice - based methods assume that the whole community lives on a regular grid 12 while agent - based models make for arbitrary topologies 13 . Despite the differences between both approaches , most prior work agrees that the topology of the underlying network takes a crucial role in calculating whether cooperation will be possible to thrive 14 .",
        "rewrite_text": "We propose a novel framework for quantifying the evolutionary dynamics of social groups by examining their internal structures and interactions with neighboring communities. This methodology allows us to investigate the evolution of cooperation within structured communities comprised of diverse agent types, specifically cooperators and defectors. Our findings indicate that clusters of cooperators have a significantly higher likelihood of survival compared to isolated cooperators when faced with interactions from defector clusters. This survival advantage is further amplified as the size of the cooperator clusters increases. However, we also observe that the probability of survival diminishes as the number of competing clusters rises. Notably, our research reveals that the existence of at least one sufficiently large cooperator cluster can lead to the complete extinction of all defector clusters. \n\nRecent studies have highlighted that cooperative behavior among unrelated individuals can emerge spontaneously, particularly in contexts of intense competition between selfish strategies. However, much of the existing literature has focused on well-mixed populations where interactions occur uniformly among individuals. In contrast, real-world systems—such as genetic communities, human organizations, and computer networks—often exhibit varying degrees of spatial organization. To better understand the influence of spatial dynamics on the emergence of cooperation, researchers have explored the evolution of cooperation in spatially structured populations. These investigations typically fall into two main categories: lattice-based models, which assume a regular grid for community interactions, and agent-based models, which accommodate arbitrary topologies. Despite the methodological differences, there is a consensus in the literature that the topology of the underlying network plays a critical role in determining the potential for cooperation to flourish. This study contributes to this body of work by providing insights into how the structural characteristics of social groups influence their evolutionary trajectories.",
        "ori-fast-z-score": 0.4016096644512494,
        "water-fast-z-score": 7.6373499280870085,
        "rewrite-fast-z-score": 2.934290867157133
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Direct calculation of interfacial tensions from computer simulation: Results for freely jointed tangent hard sphere chains .\nAbstract:\nWe present results on the direct measurement of interfacial tension between two coexisting phases in a model system consisting of freely-jointed tangent hard-sphere chains (FJTHSC). The FJTHSC are modeled by an off-lattice Monte Carlo algorithm that allows us to study systems with up to N = 1000 particles at temperatures ranging from T = 0.5 to 1.0 and densities ranging from ρ = 0.6 to 0.8. We find that the interfacial tension is strongly dependent upon temperature, density, chain length, and bond angle distribution. In particular we show how the interfacial tension can be used as a probe into the local structure near interfaces. \n \n Keywords: Interfacial tension, Computer simulations, Hard spheres, Chain molecules, Phase separation, Structure factor, Bond angle distribution, Local order parameter, Order-disorder transition, Freely-jointed",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Direct measurements of interfacial friction from computer simulation : Results for freely jointed tangent hard sphere chains . Abstract : We report findings on the direct calculation of interfacial tension between two coexisting phases in a model scheme consisting of freely - jointed tangent hard - sphere chains ( FJTHSC ) .The FJTHSC are modeled by an off - lattice Monte Carlo algorithm that enables us to study systems with up to N = 1000 particles at temperatures ranging from T = 0 . 5 to 1 . 0 and densities ranging from ρ = 0 . 6 to 0 . 8 . We see that the interfacial tension is strongly dependent upon temperature , density , chain depth , and bond angle distribution .In particular we show how the interfacial tension can be used as a investigation into the local formation near interfaces . Keywords : Interfacial tension , Computer simulations , Hard circles , Chain molecules , Phase separation , Structure factor , Bond angle distribution , Local order parameter , Order - disturbance transition , Freely - jointed",
        "rewrite_text": "Title: Direct Measurements of Interfacial Friction from Computer Simulation: Results for Freely Jointed Tangent Hard Sphere Chains\n\nAbstract: In this study, we present our findings on the direct computation of interfacial tension between two coexisting phases within a model framework of freely jointed tangent hard sphere chains (FJTHSC). Utilizing an off-lattice Monte Carlo simulation approach, we investigate systems comprising up to N = 1000 particles across a temperature range of T = 0.5 to 1.0 and densities from ρ = 0.6 to 0.8. Our results indicate a significant dependence of interfacial tension on various parameters, including temperature, density, chain length, and the distribution of bond angles. Notably, we demonstrate that interfacial tension serves as a valuable tool for probing the local structural formations that occur near interfaces. This research contributes to a deeper understanding of the physical properties governing phase separation and the behavior of chain molecules at interfaces. By analyzing the structure factor and local order parameters, we elucidate the mechanisms underlying order-disturbance transitions in these systems. Our findings provide insights that could enhance the design of materials and processes where interfacial phenomena play a critical role. \n\nKeywords: Interfacial tension, Computer simulations, Hard spheres, Chain molecules, Phase separation, Structure factor, Bond angle distribution, Local order parameter, Order-disturbance transition, Freely jointed.",
        "ori-fast-z-score": -0.5933908290969266,
        "water-fast-z-score": 2.3570226039551585,
        "rewrite-fast-z-score": 0.5076730825668095
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Non-Coherent Capacity and Reliability of Sparse Multipath Channels in the Wideband Regime .\nAbstract:\nWe consider sparse multipath channels with random delays, where each path is modeled as an independent Rayleigh fading channel. We derive exact expressions for ergodic capacity and outage probability over such channels under wideband regime (i.e., when bandwidth grows without bound). Our results show that both ergodic capacity and outages are determined by the number of paths N , their average power Pn = E{|hn(t)|2}, and delay spread T . In particular, we find that ergodic capacity scales linearly with bandwidth W at high SNR if N > 2W/πT or N < πT /4W; otherwise it saturates to a constant value. Moreover, our analysis shows that outage probability decays exponentially fast with increasing bandwidth W if N > 4W/πT ; otherwise it decreases only polynomially. Finally, numerical examples illustrate how these scaling laws depend on various system parameters.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Non - Coherent Capacity and Reliability of Sparse Multipath Channels in the Wideband Regime . Abstract : We consider sparse multipath channels with random delays , where each track is modeled as an free Rayleigh fading stream .We derive exact expressions for ergodic strength and outage likelihood over such channels under wideband regime ( i . e . , when bandwidth grows without bound ) . Our results show that both ergodic strength and outages are decided by the number of paths N , their average power Pn = E { | hn ( t ) | 2 } , and delay spread T .In particular , we find that ergodic performance scales linearly with capacity W at high SNR if N > 2W / πT or N < πT / 4W ; otherwise it saturates to a constant value . Moreover , our analysis shows that outage likelihood decays exponentially rapidly with expanding bandwidth W if N > 4W / πT ; otherwise it grows only polynomially .Finally , numerical examples explain how these scaling laws depend on various system parameters .",
        "rewrite_text": "Title: Non-Coherent Capacity and Reliability of Sparse Multipath Channels in the Wideband Regime\n\nAbstract: This study investigates the characteristics of sparse multipath channels characterized by random delays, where each path is represented as an independent Rayleigh fading stream. We present precise formulations for both ergodic capacity and outage probability in the context of wideband communication, specifically as bandwidth approaches infinity. Our findings reveal that the performance metrics of ergodic capacity and outage probability are fundamentally influenced by three key factors: the number of paths (N), the average power of each path (Pn = E{|hn(t)|²}), and the delay spread (T). Notably, we demonstrate that the ergodic capacity exhibits a linear relationship with bandwidth (W) at high signal-to-noise ratios (SNR) under certain conditions—specifically, when N exceeds 2W/πT or falls below πT/4W. In scenarios where these conditions are not met, the ergodic capacity tends to stabilize at a constant value. Furthermore, our analysis indicates that the likelihood of outage decreases exponentially with increasing bandwidth when N is greater than 4W/πT; conversely, it exhibits only polynomial growth in other cases. To illustrate these theoretical results, we provide numerical examples that highlight how the scaling laws are affected by various system parameters, offering insights into the practical implications of our findings for the design and optimization of wideband communication systems. This work contributes to a deeper understanding of the interplay between channel characteristics and communication performance in sparse multipath environments, paving the way for enhanced reliability and capacity in future wireless networks.",
        "ori-fast-z-score": -0.25,
        "water-fast-z-score": 4.341215710622296,
        "rewrite-fast-z-score": 0.8620436566990363
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Chandra X-Ray Study of Galactic Supernova Remnant G299.2-2.9 .\nAbstract:\nWe present the results of an X-ray study of supernova remnant (SNR) G299.2-2.9 using data obtained with Chandra and XMM-Newton observatories. The SNR is located in the constellation Puppis at a distance of ~5 kpc, which corresponds to its angular size of about 30 arcmin. We find that the spectrum of this object can be described by two thermal components with temperatures T1=7×10^6 K and T2=2×10^6 K. In addition, we detect non-thermal emission above 10 keV. Using these parameters, we estimate the age of the SNR as t=4000 yr. This value agrees well with the characteristic time for the expansion of the shell into the surrounding medium. Based on our analysis, we conclude that the observed morphology of the SNR is consistent with the model of a spherical explosion expanding into a uniform interstellar medium.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Chandra X - Ray Study of Galactic Supernova Remnant G299 . 2 - 2 . 9 . Abstract : We report the conclusion of an X - ray study of supernova remnant ( SNR ) G299 . 2 - 2 . 9 utilizing information obtained with Chandra and XMM - Newton observatories .The SNR is situated in the constellation Puppis at a distance of ~ 5 kpc , which corresponds to its angular height of about 30 arcmin . We see that the spectrum of this object can be described by two thermal parts with temperatures T1 = 7×10 ^ 6 K and T2 = 2×10 ^ 6 K . In addition , we perceive non - temperature emission above 10 keV .Using these parameters , we estimate the age of the SNR as t = 4000 yr . This value agrees well with the typical moment for the expansion of the shell into the nearby medium .Based on our analysis , we prove that the known morphology of the SNR is compatible with the model of a spherical explosion expanding into a regular interstellar medium .",
        "rewrite_text": "We present the findings of an extensive X-ray investigation of the supernova remnant (SNR) G299.2-2.9, conducted using data from the Chandra and XMM-Newton observatories. Located in the constellation Puppis, this remnant is approximately 5 kiloparsecs away, corresponding to an angular size of about 30 arcminutes. Our spectral analysis reveals that the emissions from G299.2-2.9 can be effectively characterized by two distinct thermal components, with temperatures measured at T1 = 7×10^6 K and T2 = 2×10^6 K. Furthermore, we observe non-thermal emissions occurring at energies exceeding 10 keV. Utilizing these temperature parameters, we estimate the age of the supernova remnant to be around 4000 years, a value that aligns well with the expected timeline for the expansion of the remnant shell into the surrounding interstellar medium. Our findings indicate that the observed morphology of G299.2-2.9 is consistent with a model depicting a spherical explosion that is expanding into a uniform interstellar medium. This study contributes to the understanding of supernova remnants and their evolution, providing insights into the physical processes governing their development and interaction with the surrounding environment. The implications of our results extend to broader astrophysical contexts, enhancing our comprehension of the lifecycle of massive stars and the dynamics of the interstellar medium.",
        "ori-fast-z-score": -0.5345224838248488,
        "water-fast-z-score": 4.03585624040554,
        "rewrite-fast-z-score": -0.7504787743864564
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Nature-Inspired Interconnects for Self-Assembled Large-Scale Network-on-Chip Designs .\nAbstract:\nNetwork on chip (NoC) is an emerging interconnection paradigm to interconnect large number of processing elements in parallel computing systems. NoCs are expected to be the dominant communication infrastructure in future multi-core and many-core processors, which will require high bandwidth and low latency communications between cores. However, traditional electrical NoCs suffer from power consumption issues due to their limited scalability with increasing core count. In this work we propose a novel bio-inspired optical NoC architecture that can provide higher bandwidth at lower energy cost than conventional electrical NoCs. The proposed NoC consists of two main components; photonic routers and optoelectronic transceivers. Photonic routers use silicon photonics technology to route data packets through multiple channels simultaneously using wavelength division multiplexing technique. Optoelectronic transceiver uses vertical-cavity surface-emitting laser diodes as transmitters and avalanche photo-diode detectors as receivers. We have developed a detailed simulation model based on SystemC to evaluate performance characteristics such as packet delay, throughput and power dissipation of our proposed NoC design. Our results show that the proposed NoC outperforms existing electrical NoCs by providing up to four times higher bandwidth per unit area while consuming less power. \n \n Keywords: Optical network on chip, Silicon photonics",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Nature - Inspired Interconnects for Self - Assembled Large - Scale Network - on - Chip Designs . Abstract : Network on chip ( NoC ) is an evolving interconnection paradigm to interconnect huge variety of processing components in concurrent computing systems .NoCs are expected to be the dominant communication network in future multi - core and many - core processors , which will use large frequency and low latency links between cores . However , traditional electrical NoCs suffer from power consumption issues owing to their limited scalability with increasing peripheral count .In this project we propose a new bio - inspired optical NoC architecture that can provide higher bandwidth at lower electricity cost than conventional electrical NoCs . The proposed NoC consists of two principal structures ; photonic routers and optoelectronic transceivers .Photonic routers use silicon photonics techniques to transport data packets through several channels concurrently using wavelength division multiplexing method . Optoelectronic transceiver utilizes vertical - cavity surface - emitting laser diodes as transmitters and avalanche photo - diode detectors as receivers .We have developed a detailed simulation design based on SystemC to analyze performance traits such as packet time , throughput and power dissipation of our proposed NoC model . Our results show that the suggested NoC outperforms old electrical NoCs by offering up to four times higher bandwidth per unit area while consuming less fuel .Keywords : Optical connection on chip , Silicon photonics",
        "rewrite_text": "**Title:** Nature-Inspired Interconnects for Self-Assembled Large-Scale Network-on-Chip Designs\n\n**Abstract:** The Network-on-Chip (NoC) paradigm is rapidly emerging as a vital framework for interconnecting a diverse array of processing components within concurrent computing systems. As multi-core and many-core processors become increasingly prevalent, NoCs are anticipated to serve as the primary communication networks, leveraging high-frequency and low-latency links to facilitate efficient core communication. However, traditional electrical NoCs face significant challenges related to power consumption, particularly as the number of peripherals increases, leading to scalability issues. In response to these challenges, we propose a novel bio-inspired optical NoC architecture designed to deliver superior bandwidth at a reduced energy cost compared to conventional electrical NoCs. Our proposed architecture is comprised of two main components: photonic routers and optoelectronic transceivers. The photonic routers employ advanced silicon photonics techniques, enabling the concurrent transmission of data packets across multiple channels through the use of wavelength division multiplexing. Meanwhile, the optoelectronic transceivers incorporate vertical-cavity surface-emitting laser diodes for transmission and avalanche photodiode detectors for reception. To evaluate the performance characteristics of our proposed NoC model, we have developed a comprehensive simulation framework utilizing SystemC, allowing us to analyze key metrics such as packet latency, throughput, and power dissipation. Our findings indicate that the proposed optical NoC architecture significantly outperforms traditional electrical NoCs, achieving up to four times greater bandwidth per unit area while simultaneously reducing energy consumption. This research highlights the potential of bio-inspired optical interconnects in addressing the limitations of existing NoC designs and paves the way for more efficient large-scale network architectures in future computing systems.\n\n**Keywords:** Optical Network-on-Chip, Silicon Photonics",
        "ori-fast-z-score": -0.4662524041201569,
        "water-fast-z-score": 5.385164807134505,
        "rewrite-fast-z-score": 0.3310423554409472
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Anomalous structural and mechanical properties of solids confined in quasi one dimensional strips .\nAbstract:\nWe study the elasticity of thin films with periodic microstructure, which are constrained to lie on an underlying substrate. We show that such systems can exhibit anomalously large values for their Young s moduli as well as Poisson ratios. The origin of these effects is traced back to the presence of phonon soft modes associated with the periodicity along the film normal direction. These results have implications for the design of novel materials with tailored elastic properties. \n \n In recent years there has been growing interest in understanding how confinement affects the physical behavior of matter at the nanoscale  1  . This problem arises naturally when considering thin films or nanowires embedded within bulk materials; however it also applies more generally whenever a system is restricted to occupy only part of its available phase space  2  . For example, this situation occurs frequently during crystal growth where defects may be introduced into the lattice structure by impurities  3  , or when studying colloidal suspensions  4  .\n \nIn this work we consider the case of a thin film with periodic microstructure, whose thickness h lies between two length scales L and d (see Fig 1) . Here L represents the typical size of the unit cell while d denotes the characteristic spacing between adjacent layers; both quantities are assumed to be much smaller than the in-plane dimensions of the sample. Such structures arise commonly in nature, e.g., in layered compounds like graphite  5  , transition metal dichalcogenides  6  , and hexagonal boron nitride  7  . They are also used extensively in technological applications ranging from photovoltaics  8  to optoelectronics  9  . \n \n Figure 1: Schematic illustration of our model geometry. A thin film with periodic microstructures is confined to lie on top of a rigid substrate.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Anomalous structural and physical properties of solids localized in quasi one dimensional strips . Abstract : We explore the elasticity of thin films with periodic microstructure , which are constrained to lay on an underlying substrate .We see that such schemes can exhibit anomalously high values for their Young s moduli as well as Poisson ratios . The origin of these phenomena is traced back to the presence of phonon quiet modes associated with the periodicity along the film regular direction .These data have consequences for the creation of new materials with tailored elastic properties . In past decades there has been growing interest in understanding how confinement impacts the physical response of matter at the nanoscale 1 .This problem arises readily when examining narrow bands or nanowires enclosed within bulk surfaces ; however it also applies more generally whenever a system is restricted to occupy only portion of its available phase space 2 . For instance , this situation occurs commonly during crystal growth where defects could be applied into the lattice structure by impurities 3 , or when examining colloidal suspensions 4 .In this research we imagine the case of a thin film with periodic microstructure , whose thickness h lies between two width scales L and d ( see Fig 1 ) . Here L represents the typical size of the unit cell while d indicates the typical spacing between neighboring layers ; both quantities are expected to be much smaller than the in - plane dimensions of the sample .Such structures appear often in nature , e . g . , in layered compounds like graphite 5 , transition copper dichalcogenides 6 , and hexagonal boron nitride 7 . They are also used heavily in technological applications ranging from photovoltaics 8 to optoelectronics 9 .Figure 1 : Schematic illustration of our model topology . A narrow film with periodic microstructures is confined to lying on top of a rigid coating .",
        "rewrite_text": "**Title:** Anomalous Structural and Physical Properties of Solids Localized in Quasi-One-Dimensional Strips\n\n**Abstract:** This study investigates the elastic properties of thin films characterized by periodic microstructures that are constrained to rest on a rigid substrate. Our findings reveal that these films can demonstrate unexpectedly high values for both Young's modulus and Poisson's ratio. The underlying mechanism for these anomalous properties is linked to the presence of phonon quiet modes that arise from the periodicity in the film's primary direction. This research holds significant implications for the development of novel materials with customized elastic characteristics. Over recent decades, there has been an increasing interest in how confinement influences the physical behavior of materials at the nanoscale. This issue is particularly relevant when analyzing narrow bands or nanowires that are enclosed within bulk materials, but it also applies more broadly to any system that is limited to a fraction of its available phase space. Such scenarios frequently occur during crystal growth, where defects may be introduced into the lattice structure due to impurities, or in the study of colloidal suspensions. In our investigation, we consider a thin film with a periodic microstructure, where the film's thickness (h) is situated between two characteristic scales, L and d. Here, L denotes the typical size of the unit cell, while d represents the average spacing between adjacent layers; both dimensions are anticipated to be significantly smaller than the in-plane dimensions of the sample. These types of structures are prevalent in nature, as seen in layered materials such as graphite, transition metal dichalcogenides, and hexagonal boron nitride. Additionally, they find extensive applications in various technological fields, including photovoltaics and optoelectronics. The schematic illustration provided outlines the model topology, depicting a narrow film with periodic microstructures resting atop a rigid substrate.",
        "ori-fast-z-score": -1.655211777204736,
        "water-fast-z-score": 6.575959492214292,
        "rewrite-fast-z-score": 1.2004900959975617
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Collision Between The Milky Way And Andromeda .\nAbstract:\nThe collision between the Milky Way and its nearest neighbor, M31 (Andromeda), is predicted to occur in about 4 billion years.  This will be one of the most spectacular events ever witnessed by humans.   In this talk I will describe how we can use observations made with telescopes on Earth as well as space-based observatories such as Hubble Space Telescope to study these collisions and learn more about dark matter, galaxies, stars, black holes, and other cosmic phenomena that are part of our universe. I will also discuss some of my research projects related to studying galaxy mergers using data obtained at the W.M. Keck Observatory located on Mauna Kea, Hawaii. Finally, I ll share what it was like for me to work there during my summer internship last year. Keywords: Dark Matter, Galaxy merger, Black Hole, Cosmic Evolution, Cosmology, Astrophysics",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Collision Between The Milky Way And Andromeda . Abstract : The merger between the Milky Way and its closest neighbor , M31 ( Andromeda ) , is predicted to effect in about 4 billion years .This will be one of the most magnificent phenomena ever experienced by humans . In this talk I will explain how we can using observations made with telescopes on Earth as well as space - based observatories such as Hubble Space Telescope to study these collisions and learn more about black material , galaxies , stars , white holes , and other cosmic phenomena that are part of our universe .I will also discuss some of my research projects related to researching galaxy mergers using data acquired at the W . M . Keck Observatory situated on Mauna Kea , Hawaii .Finally , I ll share what it was like for me to work there during my summer assignment last year . Keywords : Dark Matter , Galaxy merger , Black Hole , Cosmic Evolution , Cosmology , Astrophysics",
        "rewrite_text": "**Title:** The Collision Between The Milky Way and Andromeda\n\n**Abstract:** The anticipated merger between the Milky Way galaxy and its nearest neighbor, Andromeda (M31), is projected to occur in approximately 4 billion years. This event promises to be one of the most extraordinary cosmic phenomena witnessed by humanity. In this presentation, I will elucidate how we can leverage observations from both terrestrial telescopes and space-based observatories, such as the Hubble Space Telescope, to investigate these galactic collisions. Through these observations, we aim to deepen our understanding of various cosmic entities, including dark matter, galaxies, stars, black holes, and other significant phenomena that shape our universe. Additionally, I will highlight several of my research initiatives focused on galaxy mergers, utilizing data collected at the W. M. Keck Observatory located on Mauna Kea, Hawaii. This facility provides invaluable insights into the dynamics of galaxy interactions and the role of dark matter in these processes. Furthermore, I will share my personal experiences from my summer research assignment at the observatory last year, detailing the challenges and rewards of conducting astrophysical research in such a prestigious setting. This talk will not only cover the scientific implications of the Milky Way-Andromeda collision but also emphasize the importance of collaborative research in advancing our understanding of cosmic evolution and the fundamental principles of cosmology and astrophysics. \n\n**Keywords:** Dark Matter, Galaxy Merger, Black Hole, Cosmic Evolution, Cosmology, Astrophysics",
        "ori-fast-z-score": 0.39735970711951313,
        "water-fast-z-score": 4.464418717230567,
        "rewrite-fast-z-score": -0.5184758473652127
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Surface Brightness Profiles for a sample of LMC, SMC and Fornax galaxy Globular Clusters .\nAbstract:\nWe present surface brightness profiles (SBPs) for a sample of globular clusters in the Large Magellanic Cloud (LMC), Small Magellanic Cloud (SMC) and Fornax galaxies obtained with the Hubble Space Telescope Advanced Camera for Surveys Wide Field Channel (ACS/WFC). The SBPs are derived using archival data taken as part of the ACS Nearby Galaxy Survey Treasury program. We use these new observations to investigate whether there is any difference between the SBPs of globular cluster systems belonging to different host galaxies. In addition we compare our results with those previously published by other authors who have studied similar samples of globular clusters. Our main conclusions are:  1. There appears to be no significant differences between the SBPs of the three different types of globular clusters that were observed.  2. The majority of the globular clusters appear to follow an exponential profile which can be described by: I(r) = Ie exp -(r/rc)  where rc ~ 0.5 pc.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Surface Brightness Profiles for a sample of LMC , SMC and Fornax star Globular Clusters . Abstract : We create floor intensity profiles ( SBPs ) for a sample of globular galaxies in the Large Magellanic Cloud ( LMC ) , Small Magellanic Cloud ( SMC ) and Fornax clusters obtained with the Hubble Space Telescope Advanced Camera for Surveys Wide Field Channel ( ACS / WFC ) .The SBPs are derived using archival data taken as part of the ACS Nearby Galaxy Survey Treasury project . We use these new surveys to examine whether there is any difference between the SBPs of globular cluster systems affiliated to different host galaxies .In addition we compare our findings with those previously written by other researchers who have researched similar specimens of globular clusters . Our main results are : 1 .There seems to be no considerable changes between the SBPs of the three different kinds of globular complexes that were detected . 2 .The majority of the globular nuclei tend to follow an exponential profile which can be described by : I ( r ) = Ie exp - ( r / rc ) where rc ~ 0 . 5 pc .",
        "rewrite_text": "**Title:** Surface Brightness Profiles for a Sample of LMC, SMC, and Fornax Star Globular Clusters\n\n**Abstract:** In this study, we present the surface brightness profiles (SBPs) for a selection of globular clusters located in the Large Magellanic Cloud (LMC), Small Magellanic Cloud (SMC), and Fornax galaxy cluster, utilizing data obtained from the Hubble Space Telescope's Advanced Camera for Surveys Wide Field Channel (ACS/WFC). The SBPs were constructed from archival observations collected as part of the ACS Nearby Galaxy Survey Treasury project. Our investigation aims to determine whether there are significant differences in the SBPs of globular cluster systems associated with different host galaxies. Furthermore, we compare our results with previous studies that have examined similar globular cluster samples.\n\nOur analysis reveals two key findings: first, we observe no substantial differences in the SBPs among the globular clusters from the three distinct galactic environments. This suggests a level of uniformity in the structural properties of these stellar systems, regardless of their host galaxy. Second, we find that the majority of the globular cluster nuclei exhibit an exponential brightness profile, which can be mathematically expressed as I(r) = I_e exp(-r/r_c), where the characteristic radius (r_c) is approximately 0.5 parsecs. These results contribute to our understanding of the structural characteristics of globular clusters and their formation processes across different galactic contexts, highlighting the potential for common evolutionary pathways among these ancient stellar systems.",
        "ori-fast-z-score": -0.762000762001143,
        "water-fast-z-score": 5.921443410477893,
        "rewrite-fast-z-score": 0.5129891760425771
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Discovery of a Ringlike Dark Matter Structure in the Core of the Galaxy Cluster Cl 0024+17 .\nAbstract:\nWe report on the discovery of an unexpected ring-like dark matter structure at the center of galaxy cluster CL0024+17, which is located about 3 billion light years away and has been studied extensively by many observational techniques including gravitational lensing.  The mass distribution inferred from strong gravitational lensing shows that there are two massive subclusters separated by 1 Mpc (3 arcmin) with a total mass of 2 x 10 15 h-1M_sun within a radius of 0.5h-1Mpc around their centers. We find that this double-cluster system can be well described as a binary merger model where each component consists of three components; one main halo and two smaller halos surrounding it. In addition to these six clusters, we also detect another small clump of galaxies near the center of the merging system whose position coincides with the peak of X-ray emission detected by Chandra satellite observations.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Discovery of a Ringlike Dark Matter Structure in the Core of the Galaxy Cluster Cl 0024 + 17 . Abstract : We report on the discovery of an unexpected ring - like dark matter formation at the center of galaxy cluster CL0024 + 17 , which is situated about 3 billion light years far and has been studied significantly by many observational techniques including gravitational lensing .The mass distribution inferred from good gravitational lensing indicates that there are two huge subclusters separated by 1 Mpc ( 3 arcmin ) with a total mass of 2 x 10 15 g - 1M _ sun within a diameter of 0 . 5h - 1Mpc around their centers . We see that this double - cluster system can be well described as a binary merger model where each component consists of three components ; one main halo and two smaller halos covering it .In addition to these six galaxies , we also observe another tiny clump of clusters near the center of the merging system whose position coincides with the maximum of X - ray radiation observed by Chandra satellite observations .",
        "rewrite_text": "We present our findings on the discovery of a remarkable ring-like structure of dark matter located at the core of the galaxy cluster CL0024 + 17, which is approximately 3 billion light-years away from Earth. This cluster has been the subject of extensive research utilizing various observational methods, particularly gravitational lensing. Our analysis of the mass distribution, derived from high-quality gravitational lensing data, reveals the presence of two massive subclusters that are separated by a distance of 1 Mpc (approximately 3 arcminutes). The total mass of these subclusters is estimated to be around 2 x 10^15 solar masses within a radius of 0.5h^-1 Mpc from their centers.\n\nThe observed characteristics of this double-cluster system align well with a binary merger model, where each subcluster is composed of three distinct components: a primary halo accompanied by two smaller halos enveloping it. In addition to these six prominent galaxies, we have identified a smaller clump of clusters situated near the center of the merging system. Notably, the position of this clump corresponds with the peak of X-ray emissions detected by observations from the Chandra satellite. This discovery not only enhances our understanding of the complex dynamics involved in galaxy cluster mergers but also provides critical insights into the distribution and behavior of dark matter in such environments. Our findings contribute to the broader discourse on the role of dark matter in cosmic structures and the evolution of galaxy clusters.",
        "ori-fast-z-score": 0.46499055497527714,
        "water-fast-z-score": 4.73427220735493,
        "rewrite-fast-z-score": -0.09578262852211514
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  First CO J=6-5, 4-3 detections in local ULIRGs: the dense gas in Mrk231, and its colling budget .\nAbstract:\nWe report on high-resolution observations with Herschel PACS (Poglitsch et al., 2010) and SPIRE (Griffin et al., 2010 ) that reveal for the first time the presence of cold dust emission at temperatures as low as T = 20 K in two nearby ultraluminous infrared galaxies (ULIRGs), Arp220 and Mrk 231. The observed fluxes are consistent with predictions based on models of starbursts heated by young stars. We find evidence for an additional component of cooler dust which is likely to be associated with the obscured AGN activity present in these objects. In addition we have detected the  CII  158 µm line in both sources using PACS spectroscopy. This allows us to estimate the total mass of warm molecular hydrogen M(H2). For Arp 220 this amounts to 1.5 x 10^9 solar masses within a radius of 100 pc around the nucleus.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : First CO J = 6 - 5 , 4 - 3 detections in regional ULIRGs : the deep gas in Mrk231 , and its colling deficit . Abstract : We report on high - resolution measurements with Herschel PACS ( Poglitsch et al . , 2010 ) and SPIRE ( Griffin et al . , 2010 ) that discover for the first time the activity of cold dust absorption at conditions as low as T = 20 K in two nearby ultraluminous laser galaxies ( ULIRGs ) , Arp220 and Mrk 231 .The observed fluxes are compatible with predictions based on estimates of starbursts heated by young stars . We get information for an additional element of cold dust which is probably to be involved with the obscured AGN activity contained in these objects .In addition we have discovered the CII 158 µm line in both sources using PACS spectroscopy . This enables us to estimate the total mass of bright molecular hydrogen M ( H2 ) .For Arp 220 this corresponds to 1 . 5 x 10 ^ 9 solar masses within a diameter of 100 pc around the nucleus .",
        "rewrite_text": "We present groundbreaking findings from high-resolution observations conducted with the Herschel PACS (Poglitsch et al., 2010) and SPIRE (Griffin et al., 2010) instruments, marking the first detection of cold dust absorption at temperatures as low as 20 K in two nearby ultraluminous infrared galaxies (ULIRGs), Arp 220 and Mrk 231. Our measurements reveal fluxes that align with theoretical predictions derived from models of starbursts driven by young stellar populations. This study provides insights into an additional component of cold dust, which is likely associated with the obscured active galactic nucleus (AGN) activity present in these galaxies. Furthermore, we have successfully identified the CII 158 µm emission line in both Arp 220 and Mrk 231 through PACS spectroscopy. This discovery allows us to estimate the total mass of luminous molecular hydrogen (M(H2)) in these systems. Specifically, for Arp 220, we calculate a molecular hydrogen mass of approximately 1.5 x 10^9 solar masses within a 100 parsec radius of the nucleus. These findings not only enhance our understanding of the cold gas content in ULIRGs but also shed light on the cooling processes occurring in these extreme environments. The implications of our results are significant for the study of star formation and AGN activity in the context of galaxy evolution, providing a deeper understanding of the interplay between cold gas, star formation, and AGN feedback in these fascinating cosmic structures.",
        "ori-fast-z-score": -2.9104275004359956,
        "water-fast-z-score": 2.528102914801153,
        "rewrite-fast-z-score": 0.5827715174143585
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Observational consequences of the hypothesized helium rich stellar population in Omega Centauri .\nAbstract:\nWe present new photometric and spectroscopic observations for two stars, HD 122563 (=HR 5171A) and BD+17°3248, which are suspected to be members of the proposed intermediate age population of helium-rich giants in the globular cluster Omega Cen.  We find that both stars have very similar atmospheric parameters as those found by previous studies for other candidate helium-rich giant candidates in Omega Cen: T eff = 8200 K; log g = 3.8;  Fe/H  = -1.0 dex. The observed spectra show no evidence for He II lines at 4686 Å or 5412 Å, but do exhibit strong Balmer line emission with equivalent widths ranging between -40 and -50 mÅ. These results suggest that these stars may not actually belong to this proposed class of objects. However, we cannot rule out the possibility that they are indeed helium-rich giants on the basis of our current data set alone.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Observational consequences of the hypothesized helium rich stellar community in Omega Centauri . Abstract : We report new photometric and spectroscopic observations for two stars , HD 122563 ( = HR 5171A ) and BD + 17°3248 , which are suspected to be members of the suggested intermediate age population of helium - rich giants in the globular cluster Omega Cen .We see that both stars have very identical atmospheric parameters as those shown by earlier surveys for other candidate helium - rich giant candidates in Omega Cen : T eff = 8200 K ; log f = 3 . 8 ; Fe / H = - 1 . 0 dex . The observed spectra show no evidence for He II lines at 4686 Å or 5412 Å , but do exhibit strong Balmer line emission with corresponding widths ranging between - 40 and - 50 mÅ .These data suggest that these stars must not actually belong to this possible category of bodies . However , we cannot judge out the idea that they are indeed helium - rich giants on the basis of our previous data set alone .",
        "rewrite_text": "**Title:** Observational Consequences of the Hypothesized Helium-Rich Stellar Community in Omega Centauri\n\n**Abstract:** In this study, we present new photometric and spectroscopic observations of two stars, HD 122563 (HR 5171A) and BD +17°3248, which are believed to be part of the proposed intermediate-age population of helium-rich giants within the globular cluster Omega Centauri. Our findings indicate that both stars exhibit atmospheric parameters that closely align with those identified in previous surveys of other potential helium-rich giant candidates in Omega Centauri, specifically with effective temperatures (T_eff) around 8200 K, surface gravities (log g) of approximately 3.8, and metallicities (Fe/H) of -1.0 dex. Notably, the spectra obtained from these stars do not reveal any He II lines at wavelengths of 4686 Å or 5412 Å. Instead, we observe pronounced Balmer line emissions, with widths ranging from -40 to -50 mÅ. These spectral characteristics suggest that HD 122563 and BD +17°3248 may not belong to the hypothesized category of helium-rich giants. However, it is important to note that our current dataset does not definitively exclude the possibility that these stars could still be helium-rich giants. Further investigations and additional data are necessary to draw more conclusive insights regarding their classification within the stellar population of Omega Centauri. This research contributes to the ongoing discourse about the nature of stellar populations in globular clusters and the intriguing characteristics of helium-rich stars.",
        "ori-fast-z-score": -1.9123657749350298,
        "water-fast-z-score": 2.7295978138458623,
        "rewrite-fast-z-score": -0.741998516004452
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Lyman Break Galaxies at z~5: Rest-frame UV Spectra II .\nAbstract:\nWe present new rest-UV spectra for four Lyman break galaxies (LBGs) with redshifts between 5 and 6, obtained using the Ultraviolet Echelle Spectrograph on board the Hubble Space Telescope.  The data are used to measure the interstellar medium properties in these objects by fitting models to their observed absorption line profiles. We find that all four LBGs have high metallicities ranging from 0.2 solar to 1 solar, which is consistent with previous results based on optical spectroscopy. In addition we detect strong outflows in three of our targets, as evidenced by blueshifted interstellar absorption lines. These observations suggest that massive stars may be responsible for driving galactic-scale winds even before reionization has completed. This work was supported by NASA grant HST-GO-10775.01-A awarded through the Space Telescope Science Institute, which is operated by AURA under NASA contract NAS5-26555.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Lyman Break Galaxies at z ~ 5 : Rest - frame UV Spectra II . Abstract : We create additional rest - UV spectra for four Lyman break galaxies ( LBGs ) with redshifts between 5 and 6 , obtained using the Ultraviolet Echelle Spectrograph on board the Hubble Space Telescope .The data are using to measure the interstellar medium characteristics in these objects by fitting models to their observed emission line profiles . We see that all four LBGs have high metallicities ranging from 0 . 2 solar to 1 solar , which is consistent with previous findings based on optical spectroscopy .In addition we perceive strong outflows in three of our targets , as demonstrated by blueshifted interstellar absorption patterns . These measurements suggest that powerful stars likely be responsible for driving galactic - scale winds long before reionization has completed .This project was supported by NASA gift HST - GO - 10775 . 01 - A awarded through the Space Telescope Science Institute , which is operated by AURA under NASA contract NAS5 - 26555 .",
        "rewrite_text": "In this study, we present additional rest-frame ultraviolet (UV) spectra for four Lyman Break Galaxies (LBGs) with redshifts ranging from 5 to 6, acquired using the Ultraviolet Echelle Spectrograph aboard the Hubble Space Telescope. Our analysis focuses on characterizing the interstellar medium (ISM) of these galaxies by fitting models to their observed emission line profiles. The results reveal that all four LBGs exhibit high metallicities, varying from 0.2 to 1 solar, which aligns with previous optical spectroscopy findings. Furthermore, we observe significant outflows in three of the galaxies, indicated by blueshifted interstellar absorption features. These observations imply that the intense stellar activity within these galaxies is likely driving galactic-scale winds, a phenomenon that may occur well before the completion of cosmic reionization. This research contributes to our understanding of the early universe and the role of star formation in shaping the ISM of LBGs. The project received support from NASA grant HST-GO-10775.01-A, awarded through the Space Telescope Science Institute, which operates under NASA contract NAS5-26555.",
        "ori-fast-z-score": -1.6378460497066512,
        "water-fast-z-score": 3.25,
        "rewrite-fast-z-score": 0.6108472217815261
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Galactic Wind Signatures around High Redshift Galaxies .\nAbstract:\nWe present the results of an analysis of deep Chandra X-ray Observatory observations of two high redshift galaxies, MS1512-cB58 and APM 08279+5255 (z = 3.91). We find that both sources show evidence for extended soft X-ray emission with luminosities in excess of 1043 erg/sec. The observed properties are consistent with those expected from galactic winds driven by supernovae or active nuclei. In addition to these diffuse components we detect several point-like X-ray sources within each galaxy s field-of-view which may be associated with young supermassive black holes at early stages of their formation. These objects have bolometric luminosities ranging between 1044-1046 erg/sec and appear to lie on tracks similar to those followed by quasars as they evolve through cosmic time. This work is based upon data obtained for the Guaranteed Time Observing program operated by NASA under contract NAS8-39073.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Galactic Wind Signatures around High Redshift Galaxies . Abstract : We report the conclusion of an assessment of deep Chandra X - ray Observatory surveys of two high redshift galaxies , MS1512 - cB58 and APM 08279 + 5255 ( z = 3 . 91 ) .We see that both sources show data for extended soft X - ray radiation with luminosities in excess of 1043 erg / sec . The observed properties are compatible with those expected from galactic winds driven by supernovae or active clusters .In addition to these diffuse components we perceive several point - like X - ray sources within each galaxy s field - of - view which may be identified with young supermassive black holes at early stages of their formed . These bodies have bolometric luminosities ranging between 1044 - 1046 erg / sec and tend to lay on tracks similar to those followed by quasars as they develop through cosmic time .This project is based upon statistics obtained for the Guaranteed Time Observing program operated by NASA under contract NAS8 - 39073 .",
        "rewrite_text": "We present the findings of a comprehensive analysis of deep X-ray observations conducted by the Chandra X-ray Observatory, focusing on two high redshift galaxies: MS1512-cB58 and APM 08279+5255, with a redshift of z = 3.91. Our study reveals that both galaxies exhibit significant extended soft X-ray emissions, with luminosities exceeding 10^43 erg/sec. These emissions are consistent with the characteristics expected from galactic winds, which are likely driven by supernova activity or the influence of active galactic nuclei. In addition to the diffuse X-ray emissions, we have identified several point-like X-ray sources within the fields of view of each galaxy. These sources are potentially associated with young supermassive black holes that are in the early stages of formation. The bolometric luminosities of these black holes range from 10^44 to 10^46 erg/sec, and their properties align with the evolutionary paths typically observed in quasars as they progress through cosmic time. This research is supported by statistical data obtained from the Guaranteed Time Observing program, which is managed by NASA under contract NAS8-39073. Our findings contribute to the understanding of the role of galactic winds in high redshift environments and the formation of supermassive black holes during the early epochs of the universe.",
        "ori-fast-z-score": -0.1203858530857692,
        "water-fast-z-score": 5.019960159204453,
        "rewrite-fast-z-score": 2.618614682831909
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Optical properties of Er3+ doped alkali-chloro phosphate glasses for optical amplifiers .\nAbstract:\nErbium-doped fiber amplifiers (EDFAs) are widely used in the field of telecommunication and sensing applications due to their high gain, low noise figure, wide bandwidths and compatibility with silica fibers. However, EDFAs have some disadvantages such as high cost and limited lifetime because they require expensive rare-earth elements like erbium ions. In this study, we report on the preparation of new glass-ceramic materials based on sodium chloride and potassium chloride phosphates containing different concentrations of Er3+ ions by melt-quenching method. The structural characterization was performed using X-ray powder diffraction analysis (XRPD), scanning electron microscopy (SEM), energy dispersive spectroscopy (EDS). Optical absorption spectra were recorded at room temperature between 300 nm and 1600 nm. Photoluminescence emission measurements were carried out under 980-nm excitation wavelength. It is found that the prepared samples show good transparency over a broad range of wavelengths up to 1600 nm.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Optical characteristics of Er3 + doped alkali - chloro phosphate glasses for optical amplifiers . Abstract : Erbium - doped cable amplifiers ( EDFAs ) are widely useful in the field of telecommunication and monitoring applications due to their high yield , low noise figure , large bandwidths and compatibility with silica fibers .However , EDFAs have some disadvantages such as great price and limited life because they use cost rare - earth elements like erbium ions . In this study , we publish on the preparation of new glazed - plastic compounds based on sodium salts and potassium chloride phosphates containing different amounts of Er3 + ions by melt - quenching procedure .The structural characterization was done utilizing X - ray powder diffraction assessment ( XRPD ) , scanning electron microscopy ( SEM ) , energy dispersive spectroscopy ( EDS ) . Optical absorption spectra were collected at room temperature between 300 nm and 1600 nm .Photoluminescence radiation observations were carried out under 980 - nm excitation wavelength . It is found that the prepared specimens display good transparency over a broad variety of wavelengths up to 1600 wavelength .",
        "rewrite_text": "Title: Optical Characteristics of Er3+-Doped Alkali-Chloro Phosphate Glasses for Optical Amplifiers\n\nAbstract: Erbium-doped fiber amplifiers (EDFAs) have become essential components in telecommunications and monitoring systems, primarily due to their high efficiency, low noise figures, extensive bandwidths, and compatibility with silica optical fibers. Despite their advantages, EDFAs face challenges such as high costs and limited operational lifespans, largely attributed to the use of rare-earth elements like erbium ions. This study presents the development of novel glassy-plastic materials based on sodium salts and potassium chloride phosphates, which are doped with varying concentrations of Er3+ ions, synthesized through a melt-quenching technique. The structural properties of these materials were thoroughly investigated using X-ray powder diffraction (XRPD), scanning electron microscopy (SEM), and energy dispersive spectroscopy (EDS). Optical absorption spectra were recorded at room temperature across a wavelength range of 300 nm to 1600 nm, revealing significant insights into the optical behavior of the doped glasses. Additionally, photoluminescence measurements were conducted under excitation at a wavelength of 980 nm, demonstrating the luminescent properties of the Er3+ ions within the glass matrix. The results indicate that the synthesized samples exhibit excellent transparency across a wide spectrum, extending up to 1600 nm. These findings suggest that the Er3+-doped alkali-chloro phosphate glasses have promising potential for use in optical amplifiers, offering a viable alternative to traditional EDFAs while addressing some of their inherent limitations. The implications of this research could lead to advancements in the development of more cost-effective and durable optical amplification technologies.",
        "ori-fast-z-score": -0.329292779969071,
        "water-fast-z-score": 5.7486571321943885,
        "rewrite-fast-z-score": 1.0083683467310325
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Absorption features of high redshift galactic winds .\nAbstract:\nWe present the first detection of absorption lines in the rest-frame ultraviolet spectrum of two galaxies at redshifts z = 2.3 and 3.1, which are likely to be associated with outflows driven by star formation activity. The observed properties of these winds are consistent with those predicted for massive (M > 1010 M⊙) star-forming galaxies at similar redshifts. We also find evidence that the gas is being accelerated up to velocities as large as 1000 km s-1 . These results suggest that such winds may play an important role in regulating galaxy evolution over cosmic time.  Keywords: Galaxy Evolution; Star Formation; Winds Galaxies evolve through mergers and interactions between their constituent dark matter halos and baryonic components. In addition, they grow via accretion onto existing structures or by forming new stars within them. However, it remains unclear how this growth proceeds on small scales. Recent observations have shown that many nearby galaxies contain supermassive black holes surrounded by dense gaseous disks. It has been suggested that feedback processes involving jets and/or winds produced by active galactic nuclei can regulate both the growth of the central black hole and its host galaxy. This process could explain why most galaxies do not appear to harbour supermassive black holes whose masses exceed the upper limit set by the relation between black-hole mass and bulge velocity dispersion.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Absorption features of high redshift galactic winds . Abstract : We report the first measurement of absorption patterns in the rest - frame ultraviolet spectrum of two galaxies at redshifts z = 2 . 3 and 3 . 1 , which are likely to be involved with outflows driven by galaxy formation activity .The observed properties of these clouds are compatible with those predicted for huge ( M > 1010 [UNK] ) star - creating stars at comparable redshifts . We additionally find proof that the gas is being advanced up to velocities as big as 1000 kilometers s - 1 .These data suggest that such winds may play an important role in controlling galaxy evolution over cosmic time . Keywords : Galaxy Evolution ; Star Formation ; Winds Galaxies evolve through mergers and encounters between their constituent dark matter halos and baryonic components .In addition , they grow via accretion onto existing structures or by forming additional stars within them . However , it remains unclear how this growth progresses on small scales .Recent measurements have shown that several distant galaxies contain supermassive black holes dominated by dense gaseous disks . It has been proposed that feedback systems involving jets and / or currents produced by active galactic nuclei can affect both the development of the central black hole and its host galaxy .This process may understand why most objects do not appear to harbour supermassive black holes whose masses exceed the upper limitation determined by the relation between black - hole mass and bulge velocity dispersion .",
        "rewrite_text": "We present the inaugural measurement of absorption features in the rest-frame ultraviolet spectra of two galaxies located at redshifts of z = 2.3 and z = 3.1. These galaxies are likely associated with outflows resulting from active galaxy formation processes. The characteristics of the observed absorption clouds align with theoretical predictions for massive star-forming regions (M > 10^10 solar masses) at similar redshifts. Furthermore, our findings indicate that the gas within these outflows is being propelled at velocities reaching up to 1000 kilometers per second. This evidence suggests that such galactic winds may significantly influence the evolution of galaxies throughout cosmic history. \n\nGalaxies undergo evolution through various mechanisms, including mergers and interactions between their dark matter halos and baryonic matter. They also grow by accreting material onto existing structures or through the formation of new stars. However, the dynamics of this growth on smaller scales remain poorly understood. Recent observations have revealed that many distant galaxies host supermassive black holes surrounded by dense gaseous disks. It has been hypothesized that feedback mechanisms, such as jets or outflows generated by active galactic nuclei, can impact both the growth of the central black hole and the evolution of the host galaxy. This feedback may provide insights into the observed scarcity of supermassive black holes that exceed the upper mass limit established by the correlation between black hole mass and bulge velocity dispersion. Our study contributes to the understanding of these complex interactions and their implications for galaxy evolution over time. \n\nKeywords: Galaxy Evolution; Star Formation; Galactic Winds.",
        "ori-fast-z-score": -0.09166984970282113,
        "water-fast-z-score": 6.390096504226938,
        "rewrite-fast-z-score": -0.3592106040535498
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Observations and Modeling of Line Asymmetries in Chromospheric Flares .\nAbstract:\nWe present observations of the Mg II k line asymmetry during flares, which are compared with results obtained by numerical simulations using the RH code (Uitenbroek 2001). The observed profiles show that the blue wing is enhanced relative to the red one at all heights above the limb where we can see the flare emission. This effect is more pronounced for higher altitudes. We find that this behavior cannot be explained solely by Doppler shifts due to bulk plasma motions along the LOS. In addition, our modeling shows that the observed profile shapes cannot be reproduced without including nonthermal electron beams as an additional heating source. \n \n Keywords: Solar flare, chromospheric lines, nonthermal electrons, radiative hydrodynamics model, RH code, Mg II k line, line asymmetry. 1 Introduction \n \n During solar flares, intense energy release leads to rapid changes in physical conditions throughout the atmosphere of the Sun. These include temperature increases up to several million degrees Kelvin, strong magnetic fields, high densities, and large velocities. All these factors affect the shape of spectral lines emitted by different atmospheric layers. For example, it has been shown that the intensity ratio between two Fe I lines formed at different temperatures depends on the height of formation of each line (Feldman et al., 1995; Brosius & Phillips 2004) . Also, the presence of nonthermal electrons causes significant deviations from Maxwellian velocity distributions leading to asymmetric line profiles (e.g., Canfield et al. (1990) ; Doschek et al. (1991) ), while bulk flows lead to Doppler shifts of the line center position (Doschek et al., 1991; Brosius & Phillips 2004; Brosius 2009 ). Therefore, studying the temporal evolution of the line profiles provides important information about the dynamics of the flaring region. However, interpreting such data requires detailed knowledge of the underlying physics involved in the processes responsible for the observed phenomena. \n \n In particular, the study of the Mg II h&k lines offers unique opportunities to investigate various aspects of solar flares because they form over a wide range",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Observations and Modeling of Line Asymmetries in Chromospheric Flares . Abstract : We report observations of the Mg II k line asymmetry during flares , which are compared with conclusions derived by numerical simulations using the RH code ( Uitenbroek 2001 ) .The observed profiles indicate that the blue wing is enhanced compared to the red one at all heights above the limb where we can see the flare emission . This phenomenon is more pronounced for greater altitudes .We see that this behavior cannot be described solely by Doppler variations owing to bulk plasma motions along the LOS . In addition , our modeling demonstrates that the seen profile patterns cannot be reproduced without using nonthermal ion rays as an additional thermal source .Keywords : Solar flare , chromospheric lines , nonthermal ions , radiative hydrodynamics theory , RH code , Mg II h line , edge asymmetry . 1 Introduction During solar flares , intense heat release leads to rapid alterations in physical conditions throughout the atmosphere of the Sun .These include temperature increases up to several million degrees Kelvin , large magnetic fields , large densities , and large velocities . All these influences influence the morphology of spectral lines emissions by various atmospheric elements .For instance , it has been shown that the frequency proportion between two Fe I lines formed at different temperatures depends on the height of formation of each line ( Feldman et al . , 1995 ; Brosius & Phillips 2004 ) . Also , the presence of nonthermal atoms causes significant deviations from Maxwellian velocity distributions leading to asymmetric line profiles ( e . g . , Canfield et al .( 1990 ) ; Doschek et al . ( 1991 ) ) , while bulk flows result to Doppler movements of the line center position ( Doschek et al . , 1991 ; Brosius & Phillips 2004 ; Brosius 2009 ) .Therefore , studying the temporal evolution of the line profiles provides crucial data about the dynamics of the flaring zone . However , interpreting such information requires detailed knowledge of the fundamental physics involved in the mechanisms involved for the seen phenomena .In particular , the study of the Mg II h & k lines provides unique possibilities to examine different components of sun flares because they occur over a broad range",
        "rewrite_text": "**Title:** Observations and Modeling of Line Asymmetries in Chromospheric Flares\n\n**Abstract:** This study presents a comprehensive analysis of the asymmetry observed in the Mg II k line during solar flares, juxtaposed with insights gained from numerical simulations conducted using the RH code (Uitenbroek, 2001). Our observations reveal a consistent enhancement of the blue wing of the Mg II k line compared to the red wing at various heights above the solar limb where flare emissions are detectable. This asymmetry becomes increasingly pronounced at higher altitudes. Notably, our findings indicate that this behavior cannot be attributed solely to Doppler shifts resulting from bulk plasma motions along the line of sight (LOS). Furthermore, our modeling efforts suggest that the observed profile patterns cannot be accurately replicated without incorporating nonthermal ion contributions as an additional thermal source. \n\nThe significance of this research lies in its implications for understanding the complex physical processes occurring during solar flares. During these events, the Sun experiences rapid changes in its atmospheric conditions, characterized by temperature spikes reaching several million degrees Kelvin, along with substantial magnetic fields, densities, and velocities. These factors collectively influence the spectral line emissions of various atmospheric elements. Previous studies have established that the relative intensities of Fe I lines, formed at different temperatures, are height-dependent (Feldman et al., 1995; Brosius & Phillips, 2004). Additionally, the presence of nonthermal particles leads to notable deviations from Maxwellian velocity distributions, resulting in asymmetric line profiles (Canfield et al., 1990; Doschek et al., 1991). Bulk flows contribute to Doppler shifts in the line center position (Doschek et al., 1991; Brosius & Phillips, 2004; Brosius, 2009). Therefore, analyzing the temporal evolution of line profiles is essential for gaining insights into the dynamics of flaring regions. However, accurately interpreting these observations necessitates a thorough understanding of the underlying physical mechanisms. The study of the Mg II h and k lines offers a unique opportunity to investigate the various components of solar flares across a wide range of conditions. \n\n**Keywords:** Solar flare, chromospheric lines, nonthermal ions, radiative hydrodynamics theory, RH code, Mg II h line, edge asymmetry.",
        "ori-fast-z-score": -0.23076923076923078,
        "water-fast-z-score": 8.076923076923077,
        "rewrite-fast-z-score": 0.7761505257063328
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Absorption spectroscopy of individual single-walled carbon nanotubes .\nAbstract:\nWe report on the absorption spectrum of an isolated single-wall carbon nanotube (SWNT) in solution, obtained by using a scanning near-field optical microscope with subwavelength resolution. The SWNTs are suspended between two gold electrodes and illuminated through one electrode at normal incidence to excite both transverse electric (TE) and transverse magnetic (TM) polarized light. We observe that the TE mode is strongly suppressed compared to TM polarization due to the presence of metallic tubes within our sample. This effect can be used as a spectroscopic tool for identifying the chirality of individual SWNTs. \n \n Single-wall carbon nanotubes have attracted considerable interest because they exhibit unique electronic properties which depend sensitively on their diameter and chiral angle  1  . In particular, it has been shown theoretically  2  , experimentally  3  , and numerically  4  that the energy gap depends on these parameters such that semiconducting tubes have small gaps while metallic tubes have large ones. However, this dependence is not sufficient to uniquely identify all possible tube types  5  .\n \nIn order to determine the type of each tube individually, several experimental techniques have been developed  6  -  8  . For example, Raman scattering  9  or photoluminescence  10  measurements allow one to distinguish between metallic and semiconducting tubes based on the intensity ratio of certain peaks  11  . Alternatively, electrical transport experiments  12  provide information about the charge carrier density and mobility  13  . Finally, transmission electron microscopy  14  allows one to directly visualize the structure of the tubes  15  . \n \n Here we present another method for determining the chirality of individual carbon nanotubes. Our approach relies on measuring the absorption cross section of individual SWNTs  16  . Since the absorption cross-section depends on the dielectric function  17  , which in turn varies significantly depending on whether the tube is metallic or semiconducting  18  , we expect different values for the absorption cross sections of metallic versus semiconducting tubes. By comparing the measured absorption cross sections of various tubes, we will show how this technique can be used to classify them into either metallic or semiconducting categories. \nExperimental setup\n\nThe samples were prepared following the",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Absorption spectroscopy of individual single - walled carbon nanotubes . Abstract : We report on the absorption spectrum of an exposed single - wall carbon nanotube ( SWNT ) in solution , obtained by using a scanning near - field imaging microscope with subwavelength resolution .The SWNTs are suspended between two gold electrodes and illuminated through one electrode at usual incidence to excite both transverse electric ( TE ) and longitudinal magnetic ( TM ) polarized light . We see that the TE mode is strongly restrained compared to TM polarization owing to the presence of metallic tubes within our sample .This phenomenon can be used as a spectroscopic tool for determining the chirality of different SWNTs . Single - wall carbon nanotubes have garnered considerable interest because they possess unique electronic properties which depend sensitively on their thickness and chiral angle 1 .In particular , it has been shown theoretically 2 , experimentally 3 , and numerically 4 that the electricity gap depends on these parameters such that semiconducting tubes have narrow holes while metallic tubes have smaller ones . However , this dependence is not required to uniquely distinguish all possible tube categories 5 .In order to identify the kind of each tube individually , various experimental methods have been created 6 - 8 . For instance , Raman scattering 9 or photoluminescence 10 measurements enable one to distinguish between metallic and semiconducting tubes based on the frequency proportion of certain peaks 11 .Alternatively , electrical travel techniques 12 provide details about the charge carrier density and connectivity 13 . Finally , transmission electron microscopy 14 allows one to fully visualize the formation of the tubes 15 .Here we present another technique for determining the chirality of individual carbon nanotubes . Our model relies on measuring the absorption cross section of individual SWNTs 16 .Since the absorption cross - section depends on the dielectric function 17 , which in turn varies dramatically based on whether the pipe is metallic or semiconducting 18 , we expect different values for the absorption cross sections of metallic versus semiconducting tubes . By matching the measured absorbed cross sections of different tubes , we will show how this methodology can be used to classify them into either metallic or semiconducting classification .Experimental setup\n\nThe samples were prepared following the",
        "rewrite_text": "**Title:** Absorption Spectroscopy of Individual Single-Walled Carbon Nanotubes\n\n**Abstract:** In this study, we present the absorption spectrum of an individual single-walled carbon nanotube (SWNT) suspended in a solution, utilizing a scanning near-field imaging microscope that achieves subwavelength resolution. The SWNTs are positioned between two gold electrodes and illuminated through one electrode at normal incidence, allowing for the excitation of both transverse electric (TE) and longitudinal magnetic (TM) polarized light. Our findings reveal that the TE mode is significantly suppressed compared to the TM polarization, a phenomenon attributed to the presence of metallic tubes within the sample. This observation offers a novel spectroscopic approach for determining the chirality of various SWNTs.\n\nSingle-walled carbon nanotubes have attracted substantial attention due to their exceptional electronic properties, which are highly sensitive to their diameter and chiral angle. Theoretical, experimental, and numerical studies have established that the energy gap of these nanotubes is influenced by these parameters, with semiconducting tubes exhibiting narrower gaps than their metallic counterparts. However, this relationship alone does not suffice to distinctly categorize all types of SWNTs. To accurately identify individual tubes, several experimental techniques have been developed, including Raman scattering and photoluminescence measurements, which differentiate metallic from semiconducting tubes based on specific peak frequency ratios. Additionally, electrical transport methods provide insights into charge carrier density and connectivity, while transmission electron microscopy offers a comprehensive visualization of tube formation.\n\nIn this work, we introduce an alternative method for determining the chirality of individual carbon nanotubes by measuring their absorption cross-section. Given that the absorption cross-section is contingent upon the dielectric function—which varies significantly between metallic and semiconducting tubes—we anticipate distinct absorption cross-section values for each type. By correlating the measured absorption cross-sections of various tubes, we demonstrate how this technique can effectively classify them as either metallic or semiconducting. Our experimental setup and methodology are detailed, paving the way for further advancements in the characterization of carbon nanotubes.",
        "ori-fast-z-score": -0.8512055557875505,
        "water-fast-z-score": 6.172133998483676,
        "rewrite-fast-z-score": -0.9733285267845753
    }
]