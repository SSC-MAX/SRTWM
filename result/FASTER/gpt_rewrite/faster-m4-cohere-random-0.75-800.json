[
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Search for Single-Top Production at CDF .\nAbstract:\nThe D0 and CDF experiments have searched for single-top production in the t-channel, s-channel, and associated Wt channel using data corresponding to an integrated luminosity of 5.4 fb-1 collected by the Fermilab Tevatron Collider between 1992 and 1996.  The results are presented as 95% confidence level upper limits on the cross sections times branching ratios into leptons (electrons or muons) plus jets.  In addition, we present measurements of the top quark mass made with these events. We find no evidence for new physics beyond standard model expectations. These results supersede those previously reported by both collaborations. \nWe thank our colleagues in the CERN accelerator departments for their vital contributions to this work. This research is supported by the U.S. Department of Energy under contract No. DE-AC02-76SF00515. The D0 experiment has also performed searches for single-top quarks produced via the t-channel, s-channnel, and associated Wt channels using 4.3 fb-1 of data taken during Run II of the Fermilab TeVatron collider  1  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Search for Single - Top Production at CDF . Abstract : The D0 and CDF research have searched for single - topped production in the t - channel , s - channel , and related Wt channel using data related to an integrated luminosity of 5 . 4 fb - 1 collected by the Fermilab Tevatron Collider between 1992 and 1996 .The results are presented as 95 % confidence grade upper limits on the cross sections times branching ratios into leptons ( atoms or muons ) plus jets . In addition , we present measurements of the top quark mass made with these events .We see no evidence for new science beyond standard theory expectations . These conclusions supersede those previously reported by both collaborations .We thank our colleagues in the CERN accelerator departments for their vital contributions to this project . This research is backed by the U . S . Department of Energy under contract No .DE - AC02 - 76SF00515 . The D0 study has additionally performed investigations for single - top quarks produced via the t - channel , s - channnel , and related Wt channels using 4 . 3 fb - 1 of evidence gained during Run II of the Fermilab TeVatron collider 1 .",
        "rewrite_text": "Title: Search for Single-Top Production at CDF\n\nAbstract: This article presents the findings from the D0 and CDF collaborations regarding the search for single-top quark production in various channels, specifically the t-channel, s-channel, and associated Wt channel. The analysis is based on data collected from the Fermilab Tevatron Collider, amounting to an integrated luminosity of 5.4 fb⁻¹ during the period from 1992 to 1996. The results are expressed as upper limits on the product of cross sections and branching ratios into leptonic final states, specifically electrons or muons, accompanied by jets. Furthermore, we provide measurements of the top quark mass derived from the events analyzed. Notably, our findings do not indicate any evidence for phenomena beyond the predictions of the Standard Model, reinforcing the established theoretical framework. These results supersede earlier findings reported by both collaborations, highlighting the robustness of our analysis. We extend our gratitude to our colleagues in the CERN accelerator departments for their essential contributions to this research endeavor. This work is supported by the U.S. Department of Energy under contract No. DE-AC02-76SF00515. Additionally, the D0 collaboration has conducted further investigations into single-top quark production through the t-channel, s-channel, and related Wt channels, utilizing 4.3 fb⁻¹ of data collected during Run II of the Fermilab Tevatron Collider. The comprehensive nature of this study underscores the ongoing efforts to explore the properties of top quarks and their interactions, contributing valuable insights to the field of particle physics.",
        "ori-fast-z-score": -1.0256451881367414,
        "water-fast-z-score": 3.396831102433787,
        "rewrite-fast-z-score": -1.584236068762679
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Energy Transfer between Throats from a 10d Perspective .\nAbstract:\nWe study the energy transfer in a network of coupled nonlinear oscillators with time delay, which is motivated by the dynamics of biological systems such as neural networks and genetic regulatory networks. We show that there exists an optimal coupling strength for each individual oscillator to achieve maximum energy transfer efficiency among all other oscillators. The results are obtained through numerical simulations on both small-scale and large-scale networks. In particular, we find that the energy transfer efficiency decreases when the number of nodes increases beyond a certain threshold value. This phenomenon can be explained by the fact that the total amount of available energy per node decreases rapidly due to the increase of the number of nodes. Finally, we discuss possible applications of our findings to real-world problems. Energy transfer plays important roles in many natural phenomena including brain activity  1  , heartbeat  2  , and gene regulation  3  . It has been shown recently that efficient energy transfer may also exist in complex networks  4  -  6  .\nIn this work, we consider a system consisting of N identical nonlinear oscillators connected via delayed couplings (see Fig.   1 ). Each oscillator i = 1; 2; ...; N is described bẏ \nwhere x i 2 R n denotes its state vector, f : R n ! R n describes the local dynamics of each oscillator, c ij > 0 represents the coupling strength between two oscillators i and j, and h(t) is the transmission delay satisfying h max < h(t) < h min ; t 0:",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Energy Transfer between Throats from a 10d Perspective . Abstract : We research the power transfer in a network of coupled nonlinear oscillators with time delay , which is motivated by the dynamics of biological systems such as neural systems and biological regulatory circuits .We see that there exists an appropriate coupling strength for each individual oscillator to achieve maximum energy transfer efficiency among all other oscillators . The results are derived through computational simulations on both large - scale and large - scale networks .In particular , we find that the power transfer efficiency decreases when the number of nodes increases beyond a certain threshold value . This phenomenon can be described by the fact that the total amount of available energy per node reduces rapidly due to the increase of the number of nodes .Finally , we discuss possible applied of our findings to real - time challenges . Energy transfer plays important roles in many natural phenomena including brain activity 1 , heartbeat 2 , and gene control 3 .It has been shown lately that efficient energy flow would also exist in complex networks 4 - 6 . In this research , we study a system consisting of N identical nonlinear oscillators related via slow couplings ( saw Fig .1 ) . Each oscillator i = 1 ; 2 ; . . . ; N is described [UNK] where x i 2 R n denotes its state vector , f : R n !R n represents the local behavior of each oscillator , c ij > 0 represents the interaction strength between two oscillators i and j , and h ( t ) is the propagation delay satisfying h max < h ( t ) < h min ; t 0 :",
        "rewrite_text": "**Title:** Energy Transfer between Throats from a 10D Perspective\n\n**Abstract:** This study investigates the dynamics of energy transfer within a network of coupled nonlinear oscillators that incorporate time delays, drawing inspiration from biological systems such as neural networks and regulatory circuits. Our findings reveal that each oscillator has an optimal coupling strength that maximizes energy transfer efficiency relative to the other oscillators in the network. We employed extensive computational simulations to analyze both large-scale and extensive networks, leading to significant insights. Notably, we observed a decline in power transfer efficiency as the number of nodes in the network exceeded a certain threshold. This reduction can be attributed to the rapid decrease in the total available energy per node as the network expands. Furthermore, we explore the implications of our results for real-time applications, highlighting the critical role of energy transfer in various natural processes, including brain function, cardiac rhythms, and gene regulation. Recent studies have indicated that efficient energy flow is also prevalent in complex networks. Our research focuses on a system comprising N identical nonlinear oscillators interconnected through slow couplings, as illustrated in Figure 1. Each oscillator, indexed by i (where i = 1, 2, ..., N), is characterized by a state vector \\( x_i \\in \\mathbb{R}^n \\), with \\( f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n \\) representing the local dynamics of each oscillator. The interaction strength between oscillators i and j is denoted by \\( c_{ij} > 0 \\), while \\( h(t) \\) signifies the propagation delay, constrained by \\( h_{\\text{max}} < h(t) < h_{\\text{min}} \\) for \\( t \\geq 0 \\). This research contributes to a deeper understanding of energy dynamics in complex systems and offers potential pathways for addressing real-time challenges in various fields.",
        "ori-fast-z-score": -0.9610744623271417,
        "water-fast-z-score": 5.0104368683718885,
        "rewrite-fast-z-score": 0.9330078226479681
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Aharonov-Bohm effect and broken valley-degeneracy in graphene rings .\nAbstract:\nWe study the electronic properties of armchair-edged carbon nanotubes with magnetic flux piercing them, using density functional theory (DFT). We find that for tubes with diameter larger than 1 nm there is no significant difference between the results obtained by DFT and those predicted by the effective-mass approximation. For smaller diameters we observe deviations which are attributed to the breaking of the valley degeneracy due to the curvature effects. The Aharonov-Bohm-effect manifests itself as an oscillatory behavior of the energy gap when varying the applied magnetic field strength. In addition, we show how this effect can be used to tune the bandgap of these structures. Graphene nanorings have been proposed recently as building blocks for novel nanoelectronic devices such as transistors or spintrons  1-3 . These systems exhibit interesting physical phenomena like the quantum Hall effect  4  , persistent currents  5  , and Klein tunneling  6  . Recently it has also been shown that they may serve as efficient single photon sources  7, 8  .\nIn order to understand their transport characteristics one needs to know the dependence of the energy spectrum on various parameters such as the radius R, the number N of hexagons along the circumference, and the external magnetic field B. This problem was addressed theoretically within different approximations  9-13  but only very few studies were performed based on first-principles calculations  14-16 . Here we present a detailed investigation of the influence of the magnetic field on the electronic structure of armchair-edge carbon nanotubes using density functional theory  17  . Our main focus will be on small-diameter tubes where the curvature leads to important modifications compared to large-diameter tubes  18  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Aharonov - Bohm effect and broken valley - degeneracy in graphene rings . Abstract : We research the electronic properties of armchair - edged carbon nanotubes with magnetic flux piercing them , using density functional theory ( DFT ) .We see that for tubes with diameter greater than 1 nm there is no major improvement between the results derived by DFT and those predicted by the effective - mass approximation . For lower diameters we study deviations which are owing to the breaking of the valley degeneracy owing to the curvature impacts .The Aharonov - Bohm - effect manifests itself as an oscillatory behavior of the power gap when varying the applied magnetic force power . In addition , we show how this effect can be used to tune the bandgap of these structures .Graphene nanorings have been proposed lately as building blocks for innovative nanoelectronic equipment such as transistors or spintrons 1 - 3 . These systems exhibit exciting physical phenomena like the quantum Hall phenomenon 4 , persistent currents 5 , and Klein tunneling 6 .Recently it has also been shown that they may serve as efficient single photon sources 7 , 8 . In order to comprehend their transport characteristics one needs to consider the dependence of the power spectrum on various variables such as the radius R , the number N of hexagons along the circumference , and the external magnetic force B .This problem was resolved theoretically within various approximations 9 - 13 but only very few experiments were performed based on first - principles measurements 14 - 16 . Here we present a detailed investigation of the impact of the magnetic force on the electronic stability of armchair - side carbon nanotubes using density functional theory 17 .Our main focus will be on small - radius tubes where the curvature contributes to significant modifications contrast to large - radius tubes 18 .",
        "rewrite_text": "In this study, we investigate the electronic characteristics of armchair-edged carbon nanotubes subjected to magnetic flux, employing density functional theory (DFT) as our primary analytical tool. Our findings indicate that for nanotubes with diameters exceeding 1 nm, the results obtained through DFT closely align with those predicted by the effective-mass approximation, showing no significant discrepancies. However, for nanotubes with smaller diameters, we observe notable deviations attributed to the breaking of valley degeneracy, which is influenced by the curvature of the tubes. The Aharonov-Bohm effect is prominently featured in our analysis, revealing an oscillatory behavior in the power gap as the applied magnetic field strength varies. This phenomenon not only highlights the intricate interplay between magnetic fields and electronic properties but also suggests a potential method for tuning the bandgap of these nanostructures.\n\nRecent advancements have positioned graphene nanorings as promising components for next-generation nanoelectronic devices, including transistors and spintronic applications. These systems are known to exhibit remarkable physical phenomena such as the quantum Hall effect, persistent currents, and Klein tunneling. Furthermore, they have been identified as potential sources of single photons, enhancing their appeal in quantum technology. To fully understand the transport characteristics of graphene nanorings, it is essential to analyze how the power spectrum is influenced by various parameters, including the radius (R), the number of hexagons (N) along the circumference, and the external magnetic field (B). While theoretical approaches have addressed this issue through various approximations, experimental validations based on first-principles measurements remain limited. In this paper, we provide a comprehensive examination of the effects of magnetic fields on the electronic stability of armchair-edged carbon nanotubes, with a particular emphasis on small-radius tubes where curvature plays a significant role in altering their electronic properties.",
        "ori-fast-z-score": -1.3926212476455828,
        "water-fast-z-score": 5.289370814097132,
        "rewrite-fast-z-score": -0.2526455763199557
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The interplay between radio galaxies and cluster environment .\nAbstract:\nWe present the results of an optical spectroscopic survey of radio galaxies in clusters at redshifts z=0.4-0.8, carried out with the William Herschel Telescope (WHT). We find that the fraction of AGN-powered radio galaxies decreases towards higher galaxy density environments within the clusters. This is consistent with previous studies which have found evidence for environmental quenching of star formation activity among massive galaxies. However we also find that there are many examples where powerful radio sources reside in dense regions without any obvious signs of being environmentally suppressed. These objects may be undergoing rapid evolution or they could represent a population of recently accreted field galaxies whose properties are still evolving to resemble those of their local counterparts. The sample consists of 20 radio galaxies selected from the VLA-COSMOS 3 GHz Large Project catalogue (Smolčić et al., 2009) using the following criteria: 1) They lie in one of four X-ray luminous clusters at 0.4 < z < 0.8; 2) Their radio luminosity lies above L(3GHz) = 10 25 W Hz-1; 3) They do not show strong emission lines indicative of ongoing nuclear activity; 4) They were observed during our WHT run on 2010 May 24-25.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The interplay between radio stars and cluster environment . Abstract : We report the conclusion of an optical spectroscopic study of radio nuclei in clusters at redshifts z = 0 . 4 - 0 . 8 , conducted out with the William Herschel Telescope ( WHT ) .We see that the fraction of AGN - powered radio stars reduces towards higher galaxy concentration environments within the clusters . This is consistent with previous research which have discovered evidence for environmental quenching of galaxy formation activity among huge clusters .However we also find that there are many instance where powerful radio sources exist in dense areas without any obvious signs of being environmentally suppressed . These bodies may be experiencing rapid evolution or they may contain a population of newly accreted field galaxies whose properties are still changing to reflect those of their nearby rivals .The sample consists of 20 radio stars selected from the VLA - COSMOS 3 GHz Large Project catalogue ( Smolčić et al . , 2009 ) applying the following standards : 1 ) They sit in one of four X - ray luminous clusters at 0 . 4 < z < 0 . 8 ; 2 ) Their radio luminosity sits above L ( 3GHz ) = 10 25 W Hz - 1 ; 3 ) They do not show weak absorption patterns indicative of ongoing nuclear activity ; 4 ) They were detected during our WHT running on 2010 May 24 - 25 .",
        "rewrite_text": "Title: The Interplay Between Radio Stars and Cluster Environment\n\nAbstract: In this study, we present the findings from an optical spectroscopic investigation of radio nuclei located within galaxy clusters at redshifts ranging from z = 0.4 to 0.8, utilizing the capabilities of the William Herschel Telescope (WHT). Our analysis reveals a notable trend: the fraction of active galactic nuclei (AGN) that power radio stars diminishes in regions of higher galaxy density within these clusters. This observation aligns with prior studies that have suggested environmental quenching of galaxy formation activities in massive clusters. However, our research also uncovers several instances where robust radio sources are present in densely populated areas, exhibiting no clear indications of environmental suppression. These cases may represent rapidly evolving systems or could involve a population of newly accreted field galaxies that are still undergoing changes in their properties to align with those of their neighboring galaxies. \n\nThe sample for our study comprises 20 radio stars, meticulously selected from the VLA-COSMOS 3 GHz Large Project catalog (Smolčić et al., 2009) based on specific criteria: (1) each star is situated within one of four X-ray luminous clusters at redshifts between 0.4 and 0.8; (2) their radio luminosity exceeds L(3GHz) = 10^25 W Hz^-1; (3) they do not exhibit weak absorption features that would suggest ongoing nuclear activity; and (4) they were detected during our observational campaign with the WHT on May 24-25, 2010. This research contributes to our understanding of the complex relationship between radio star activity and the surrounding cluster environment, highlighting both the influence of environmental factors and the potential for unique evolutionary pathways among radio sources in dense galactic settings.",
        "ori-fast-z-score": -1.3199500146737049,
        "water-fast-z-score": 6.934760925288564,
        "rewrite-fast-z-score": 1.7960530202677492
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Observations and Modeling of Line Asymmetries in Chromospheric Flares .\nAbstract:\nWe present observations of the Mg II k line asymmetry during flares, which are compared with results obtained by numerical simulations using the RH code (Uitenbroek 2001). The observed profiles show that the blue wing is enhanced relative to the red one at all heights above the limb where we can see the flare emission. This effect is more pronounced for higher altitudes. We find that this behavior cannot be explained solely by Doppler shifts due to bulk plasma motions along the LOS. In addition, our modeling shows that the observed profile shapes cannot be reproduced without including nonthermal electron beams as an additional heating source. \n \n Keywords: Solar flare, chromospheric lines, nonthermal electrons, radiative hydrodynamics model, RH code, Mg II k line, line asymmetry. 1 Introduction \n \n During solar flares, intense energy release leads to rapid changes in physical conditions throughout the atmosphere of the Sun. These include temperature increases up to several million degrees Kelvin, strong magnetic fields, high densities, and large velocities. All these factors affect the shape of spectral lines emitted by different atmospheric layers. For example, it has been shown that the intensity ratio between two Fe I lines formed at different temperatures depends on the height of formation of each line (Feldman et al., 1995; Brosius & Phillips 2004) . Also, the presence of nonthermal electrons causes significant deviations from Maxwellian velocity distributions leading to asymmetric line profiles (e.g., Canfield et al. (1990) ; Doschek et al. (1991) ), while bulk flows lead to Doppler shifts of the line center position (Doschek et al., 1991; Brosius & Phillips 2004; Brosius 2009 ). Therefore, studying the temporal evolution of the line profiles provides important information about the dynamics of the flaring region. However, interpreting such data requires detailed knowledge of the underlying physics involved in the processes responsible for the observed phenomena. \n \n In particular, the study of the Mg II h&k lines offers unique opportunities to investigate various aspects of solar flares because they form over a wide range",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Observations and Modeling of Line Asymmetries in Chromospheric Flares . Abstract : We report observations of the Mg II k line asymmetry during flares , which are compared with conclusions derived by numerical simulations using the RH code ( Uitenbroek 2001 ) .The observed profiles indicate that the blue wing is enhanced compared to the red one at all heights above the limb where we can see the flare emission . This phenomenon is more pronounced for greater altitudes .We see that this behavior cannot be described solely by Doppler variations owing to bulk plasma motions along the LOS . In addition , our modeling demonstrates that the seen profile patterns cannot be reproduced without using nonthermal ion rays as an additional thermal source .Keywords : Solar flare , chromospheric lines , nonthermal ions , radiative hydrodynamics theory , RH code , Mg II h line , edge asymmetry . 1 Introduction During solar flares , intense heat release leads to rapid alterations in physical conditions throughout the atmosphere of the Sun .These include temperature increases up to several million degrees Kelvin , large magnetic fields , large densities , and large velocities . All these influences influence the morphology of spectral lines emissions by various atmospheric elements .For instance , it has been shown that the frequency proportion between two Fe I lines formed at different temperatures depends on the height of formation of each line ( Feldman et al . , 1995 ; Brosius & Phillips 2004 ) . Also , the presence of nonthermal atoms causes significant deviations from Maxwellian velocity distributions leading to asymmetric line profiles ( e . g . , Canfield et al .( 1990 ) ; Doschek et al . ( 1991 ) ) , while bulk flows result to Doppler movements of the line center position ( Doschek et al . , 1991 ; Brosius & Phillips 2004 ; Brosius 2009 ) .Therefore , studying the temporal evolution of the line profiles provides crucial data about the dynamics of the flaring zone . However , interpreting such information requires detailed knowledge of the fundamental physics involved in the mechanisms involved for the seen phenomena .In particular , the study of the Mg II h & k lines provides unique possibilities to examine different components of sun flares because they occur over a broad range",
        "rewrite_text": "**Title:** Observations and Modeling of Line Asymmetries in Chromospheric Flares\n\n**Abstract:** This study presents an analysis of the asymmetries observed in the Mg II k line during solar flares, juxtaposed with findings from numerical simulations conducted using the RH code (Uitenbroek, 2001). Our observations reveal a consistent enhancement of the blue wing of the Mg II k line relative to the red wing across various heights above the solar limb where flare emissions are detectable. This asymmetry becomes increasingly pronounced at higher altitudes. Notably, our findings indicate that this behavior cannot be attributed solely to Doppler shifts resulting from bulk plasma motions along the line of sight (LOS). Furthermore, our modeling efforts suggest that the observed profile patterns cannot be accurately replicated without incorporating nonthermal ion contributions as an additional thermal source. \n\nThe phenomenon of solar flares is characterized by rapid and intense energy release, leading to significant alterations in the solar atmosphere's physical conditions, including temperature spikes reaching several million degrees Kelvin, the presence of strong magnetic fields, and variations in density and velocity. These factors collectively influence the spectral line emissions of various atmospheric elements. Previous research has established that the relative intensity of two Fe I lines, formed at differing temperatures, is height-dependent (Feldman et al., 1995; Brosius & Phillips, 2004). Additionally, the presence of nonthermal particles can lead to marked deviations from Maxwellian velocity distributions, resulting in asymmetric line profiles (Canfield et al., 1990; Doschek et al., 1991). Bulk plasma flows contribute to Doppler shifts in the line center position (Doschek et al., 1991; Brosius & Phillips, 2004; Brosius, 2009). \n\nThus, investigating the temporal evolution of line profiles is essential for understanding the dynamics of flaring regions. However, accurate interpretation of these observations necessitates a comprehensive understanding of the underlying physical mechanisms. The study of the Mg II h and k lines offers unique insights into the various components of solar flares, as they are formed across a wide range of atmospheric conditions. \n\n**Keywords:** Solar flare, chromospheric lines, nonthermal ions, radiative hydrodynamics theory, RH code, Mg II h line, edge asymmetry.",
        "ori-fast-z-score": -0.23076923076923078,
        "water-fast-z-score": 8.076923076923077,
        "rewrite-fast-z-score": 0.4629100498862757
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Use of ITIL for Process Optimisation in the IT Service Centre of Harz University, exemplified in the Release Management Process .\nAbstract:\nThe purpose of this study is to examine how an organisation can use Information Technology Infrastructure Library (ITIL) as a process optimisation tool within its service centre environment. The research focuses on the release management process at the IT service centre of Harz University and examines whether it complies with all relevant aspects of the ITIL framework. A case study approach was used to collect data through interviews and document analysis. This article presents the results of the investigation into the current state of the release management process at Harz University s IT service centre. It also provides recommendations for improving the process based on the findings. Keywords: information technology infrastructure library, process optimization, release management process, university, IT service centre, case study method. 1 Introduction Today, universities are increasingly relying on their computer systems to support teaching and research activities. As such, they need reliable services that meet user requirements  1  . However, many universities have experienced problems related to poor quality software releases  2  , which may lead to system failures or even loss of data  3  .\nIn order to avoid these issues, organisations should implement appropriate processes  4  . One widely accepted set of guidelines for managing information technology (IT) services is provided by the Information Technology Infrastructure Library  5  . According to  6  , ITIL has been adopted by more than 80% of Fortune 500 companies worldwide since 2003.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Use of ITIL for Process Optimisation in the IT Service Centre of Harz University , exemplified in the Release Management Process . Abstract : The purpose of this study is to examine how an institution can using Information Technology Infrastructure Library ( ITIL ) as a process optimisation platform within its service centre environment .The studies centers on the release control system at the IT service centre of Harz University and examines whether it complies with all relevant parts of the ITIL framework . A case study method was used to collect data through interviews and report study .This page presents the conclusion of the examination into the present state of the release administration process at Harz University s IT network centre . It additionally offers suggestions for improving the process based on the discoveries .Keywords : info technology infrastructure library , process optimization , release administration process , university , IT service centre , case study method . 1 Introduction Today , universities are increasingly relying on their computer machines to support education and scientific activities .As such , they seek reliable services that meet user requirements 1 . However , many universities have experienced difficulties related to bad quality code releases 2 , which sometimes lead to system failures or even loss of documentation 3 .In try to minimize these problems , organisations should implement appropriate processes 4 . One widely accepted set of guidelines for controlling information tech ( IT ) services is provided by the Information Technology Infrastructure Library 5 .According to 6 , ITIL has been used by more than 80 % of Fortune 500 companies globally since 2003 .",
        "rewrite_text": "**Title:** The Use of ITIL for Process Optimization in the IT Service Centre of Harz University: A Case Study on the Release Management Process\n\n**Abstract:** This study investigates the application of the Information Technology Infrastructure Library (ITIL) as a framework for process optimization within the IT service centre at Harz University, focusing specifically on the release management process. The research aims to determine the extent to which the current release control system aligns with the ITIL framework's best practices. Utilizing a case study methodology, data was gathered through interviews and an analysis of relevant documentation. The findings reveal the current state of the release management process at Harz University’s IT service centre, highlighting areas of compliance and non-compliance with ITIL standards. Furthermore, the study provides actionable recommendations for enhancing the release management process, aimed at improving service quality and reliability. The significance of this research lies in its potential to guide universities in adopting ITIL principles to streamline their IT service operations, thereby addressing common challenges such as poor quality code releases, system failures, and documentation issues. As higher education institutions increasingly depend on robust IT systems to support educational and research activities, the implementation of structured processes like ITIL becomes crucial. This study contributes to the growing body of literature on IT service management in academic settings and underscores the importance of process optimization in achieving operational excellence. \n\n**Keywords:** Information Technology Infrastructure Library, process optimization, release management process, university, IT service centre, case study methodology. \n\n**1 Introduction:** In the contemporary educational landscape, universities are heavily reliant on technology to facilitate learning and research. Consequently, there is a pressing need for dependable IT services that align with user expectations. However, many institutions face challenges stemming from inadequate code releases, which can result in system outages and loss of critical documentation. To mitigate these issues, organizations must adopt effective processes. The ITIL framework, recognized as a leading set of guidelines for IT service management, has been embraced by over 80% of Fortune 500 companies worldwide since 2003, making it a valuable resource for universities seeking to enhance their IT service delivery.",
        "ori-fast-z-score": 0.18107149208503706,
        "water-fast-z-score": 8.082238591204872,
        "rewrite-fast-z-score": -0.22677868380553634
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Prompt Emission of High Energy Photons from Gamma Ray Bursts .\nAbstract:\nWe report on the detection by Fermi Large Area Telescope (LAT) of prompt emission in gamma rays with energies above 100 MeV associated with GRB 080916C, one of the brightest bursts ever detected at high energy. The LAT data show that this emission is highly variable and peaks within 1 s after the onset of the burst. We find no evidence for spectral evolution during the first few hundred seconds following the trigger time. A simple power law fit to the LAT spectrum yields an index of -2.2 +/-.1 over the range 100 MeV - 10 GeV. This value is consistent with previous measurements made using Konus-Wind and AGILE satellites but differs significantly from those obtained by other instruments operating below 100 MeV. Our results are inconsistent with models which predict a softening of the photon spectrum as it evolves into the X-ray band. They also rule out scenarios where the high-energy photons originate solely from inverse Compton scattering off relativistic electrons accelerated in internal shocks. \n \n Keywords: Gamma-ray burst",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Prompt Emission of High Energy Photons from Gamma Ray Bursts . Abstract : We report on the discovery by Fermi Large Area Telescope ( LAT ) of prompt emission in gamma particles with energies above 100 MeV attributed with GRB 080916C , one of the brightest bursts ever observed at high energy .The LAT results show that this emission is strongly varied and peaks within 1 s after the beginning of the explosion . We see no evidence for spectral evolution during the first few hundred moments following the trigger time .A straightforward power law suited to the LAT spectrum gives an index of - 2 . 2 + / - . 1 over the range 100 MeV - 10 GeV . This value is consistent with previous measurements made using Konus - Wind and AGILE spacecraft but varies dramatically from those achieved by other instruments working below 100 MeV .Our results are inconsistent with models which predict a softening of the photon spectrum as it evolves into the X - ray band . They even point out situations where the high - energy photons arise solely from inverse Compton absorption off relativistic electrons accelerated in internal shocks .Keywords: Gamma-ray burst",
        "rewrite_text": "**Title:** Prompt Emission of High Energy Photons from Gamma Ray Bursts\n\n**Abstract:** In this study, we present significant findings from the Fermi Large Area Telescope (LAT) regarding the prompt emission of gamma particles exceeding 100 MeV, specifically linked to Gamma Ray Burst (GRB) 080916C, which is recognized as one of the most luminous bursts detected at high energy levels. The LAT observations reveal that the emission exhibits substantial variability, with a pronounced peak occurring within the first second following the onset of the explosion. Notably, our analysis indicates no discernible spectral evolution during the initial few hundred seconds post-trigger. A simple power law model applied to the LAT data yields a spectral index of -2.2 ± 0.1 within the energy range of 100 MeV to 10 GeV. This spectral index aligns with previous findings from the Konus-Wind and AGILE missions but stands in stark contrast to results obtained from other instruments operating below 100 MeV. Our observations challenge existing theoretical models that suggest a softening of the photon spectrum as it transitions into the X-ray domain. Instead, our data support scenarios in which high-energy photons are predominantly produced through inverse Compton scattering involving relativistic electrons that are accelerated during internal shock processes. These insights contribute to a deeper understanding of the mechanisms underlying gamma-ray bursts and the nature of their high-energy emissions. \n\n**Keywords:** Gamma-ray burst, high-energy photons, Fermi Large Area Telescope, spectral index, inverse Compton scattering.",
        "ori-fast-z-score": -0.9058216273156765,
        "water-fast-z-score": 4.837877973981903,
        "rewrite-fast-z-score": 0.38138503569823695
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Shell model study of neutron rich oxygen isotopes .\nAbstract:\nThe shell-model calculations are performed for the ground states and low-lying excited states in 16,17O nuclei using the effective interaction derived by Brown et al.  The results show that the calculated excitation energies agree well with experimental data except for some cases where the observed levels have large mixing between different configurations. In addition to the energy level scheme, we also calculate electromagnetic transition probabilities among these states. It is found that our calculation reproduces fairly well the measured B(E2) values but underestimates the B(M1) values significantly. This may be due to the fact that the M1 operator used here does not include the spin-orbit term which plays an important role in this case. Finally, we discuss briefly about the possible existence of new collective excitations in 17O nucleus. PACS: 21.10.Ks, 22.60.-n Shell Model Study of Neutron Rich Oxygen Isotopes",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Shell theory study of neutron rich oxygen isotopes . Abstract : The shell - model calculations are performed for the ground states and low - lying excited states in 16 , 17O hydrogen utilizing the effective interaction derived by Brown et al .The results show that the adjusted excitation energies agree well with observation information except for some cases where the known levels have huge mixing between various modes . In addition to the power level scheme , we also obtain electromagnetic transition probabilities among these states .It is found that our analysis reproduces reasonably well the measured B ( E2 ) values but underestimates the B ( M1 ) values considerably . This might be due to the fact that the M1 operator used here does not include the spin - orbit term which plays an important role in this instance .Finally , we talk briefly about the possible existence of new collective excitations in 17O nucleus . PACS : 21 . 10 . Ks , 22 . 60 . - n Shell Model Study of Neutron Rich Oxygen Isotopes",
        "rewrite_text": "Title: Shell Theory Investigation of Neutron-Rich Oxygen Isotopes\n\nAbstract: This study presents shell-model calculations focused on the ground and low-lying excited states of the neutron-rich isotopes 16O and 17O, utilizing an effective interaction framework developed by Brown et al. The findings indicate that the adjusted excitation energies align closely with experimental data, with some exceptions observed in cases where significant mixing occurs between different excitation modes. In addition to the energy level scheme, we also calculate the electromagnetic transition probabilities for these states. Our analysis demonstrates a reasonable agreement with the measured B(E2) values, although it significantly underestimates the B(M1) values. This discrepancy may stem from the exclusion of the spin-orbit term in the M1 operator employed in our calculations, which is known to be crucial in this context. Furthermore, we briefly discuss the potential existence of new collective excitations within the 17O nucleus, suggesting avenues for future research. The implications of these findings contribute to a deeper understanding of the nuclear structure and dynamics of neutron-rich oxygen isotopes, highlighting the importance of refining theoretical models to capture the complexities of nuclear interactions. The study is categorized under PACS codes 21.10.Ks and 22.60.-n, reflecting its relevance to shell model studies and nuclear excitations.",
        "ori-fast-z-score": 0.3333333333333333,
        "water-fast-z-score": 5.378448739494827,
        "rewrite-fast-z-score": 0.5940885257860046
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Theory of Two-Photon Interactions with Broadband Down-Converted Light and Entangled Photons .\nAbstract:\nWe present the theory for two-photon interactions in broadband down-converted light, including entanglement between photons generated by spontaneous parametric down conversion (SPDC). We show that this leads to new effects such as photon bunching at zero time delay and antibunching at nonzero delays. These results are compared against experimental data obtained using SPDC sources based on periodically poled lithium niobate waveguides. The theoretical model is also used to predict the effect of varying pump bandwidths and crystal lengths on the degree of second-order coherence g(2)(0) measured experimentally. This work was supported by EPSRC grant EP/G037656/1. \n \n In recent years there has been growing interest in quantum optics experiments involving broadband down-conversion  1–3 . Such experiments have led to demonstrations of novel phenomena such as single-photon switching  4 , sub-Poissonian statistics  5 , squeezing  6 , and nonclassical correlations  7, 8 . However, many aspects of these experiments remain poorly understood due to difficulties associated with modelling the complicated nonlinear processes involved  9, 10 . Here we develop an analytical description of two-photon interactions in broad-band down-converted light which includes both temporal and spatial degrees of freedom  11, 12 . Our approach allows us to calculate the joint spectral intensity distribution of the down-converted field  13 , which can then be used to determine the probability density function describing the arrival times of pairs of photons produced via spontaneous parametric downconversion  14–18 . As well as providing insight into the physics underlying broadband down-conversion experiments, our analysis enables quantitative predictions about the behaviour of such systems to be made.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Theory of Two - Photon Interactions with Broadband Down - Converted Light and Entangled Photons . Abstract : We present the notion for two - photon interactions in broadband down - converted radiation , including entanglement between photons generated by spontaneous parametric down transformation ( SPDC ) .We see that this contributes to novel influences such as photon bunching at zero time time and antibunching at nonzero delays . These data are compared against empirical data acquired using SPDC sources based on periodically poled lithium niobate waveguides .The theory model is also used to predict the impact of differing flow bandwidths and crystal lengths on the degree of second - order coherence g ( 2 ) ( 0 ) measured experimentally . This research was supported by EPSRC award EP / G037656 / 1 .In recent years there has been growing interest in quantum optics tests featuring wireless down - transfer 1 – 3 . Such experiments have led to demonstrations of new concepts such as single - photon switching 4 , sub - Poissonian statistics 5 , squeezing 6 , and nonclassical correlations 7 , 8 .However , many aspects of these experiments exist poorly studied attributed to difficulties related with modelling the complicated nonlinear processes used 9 , 10 . Here we develop an analytical explanation of two - photon interactions in narrow - band down - converted radiation which includes both temporal and spatial degrees of liberty 11 , 12 .Our solution enables us to estimate the joint spectral intensity distribution of the down - converted field 13 , which can then be used to predict the probability density function explaining the entry times of pairs of photons created via spontaneous parametric downconversion 14 – 18 . As well as providing information into the physics underlying wireless down - conversion techniques , our analysis enables numerical assumptions about the behaviour of such systems to be made .",
        "rewrite_text": "In this article, we explore the theoretical framework surrounding two-photon interactions within the context of broadband down-converted light, particularly focusing on the entanglement of photons produced through spontaneous parametric down-conversion (SPDC). Our findings reveal significant phenomena such as photon bunching occurring at zero time delays and antibunching at non-zero delays, which we validate against experimental data obtained from SPDC sources utilizing periodically poled lithium niobate waveguides. Additionally, our theoretical model allows us to investigate how variations in bandwidth and crystal length influence the second-order coherence parameter, g(2)(0), as measured in experiments. This research is supported by the Engineering and Physical Sciences Research Council (EPSRC) under award EP/G037656/1.\n\nThe increasing interest in quantum optics, particularly in experiments involving wireless down-conversion, has led to the exploration of innovative concepts such as single-photon switching, sub-Poissonian statistics, squeezing, and nonclassical correlations. However, many facets of these experiments remain inadequately understood, primarily due to the complexities associated with modeling the intricate nonlinear processes involved. In response, we present an analytical approach to two-photon interactions in narrow-band down-converted radiation, incorporating both temporal and spatial degrees of freedom. \n\nOur solution facilitates the estimation of the joint spectral intensity distribution of the down-converted field, which is instrumental in predicting the probability density function that describes the arrival times of photon pairs generated through spontaneous parametric down-conversion. By shedding light on the underlying physics of wireless down-conversion techniques, our analysis also provides a foundation for making numerical predictions regarding the behavior of such systems, thereby advancing the field of quantum optics.",
        "ori-fast-z-score": -1.5434872662825794,
        "water-fast-z-score": 6.1739490651303175,
        "rewrite-fast-z-score": 0.6163156344279367
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dielectric Relaxation and Phase Transition at Cryogenic Temperatures in 0.65 Pb(Ni_1/3Nb_2/3)O_3 -0.35PbTiO_3 Ceramics .\nAbstract:\nThe dielectric properties, phase transition behavior, and microstructure evolution were investigated for the (0.65 Pb(Ni-1/3Nb-2/3)O_3  -0.35PbTiO3) (PNT) ceramics with different sintering temperatures ranging from 850 to 1100 °C. The PNT samples exhibited high permittivity values up to ~10 4 , low loss tangent below 10 -2 , and large tunability over 30% under an electric field strength of 30 kV/cm at room temperature. With decreasing temperature down to 77 K, the permittivity increased slightly while the loss tangent decreased significantly due to the freezing out of mobile ions. At cryogenic temperatures, two relaxation processes were observed in the frequency range between 1 Hz and 100 kHz. The first process was attributed to the grain boundary effect; it shifted towards higher frequencies as the temperature decreased. The second process was associated with ferroelectric domain wall motion; its relaxation time constant remained almost unchanged when the temperature changed.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dielectric Relaxation and Phase Transition at Cryogenic Temperatures in 0 . 65 Pb ( Ni _ 1 / 3Nb _ 2 / 3 ) O _ 3 - 0 . 35PbTiO _ 3 Ceramics . Abstract : The dielectric characteristics , phase change response , and microstructure behavior were researched for the ( 0 . 65 Pb ( Ni - 1 / 3Nb - 2 / 3 ) O _ 3 - 0 . 35PbTiO3 ) ( PNT ) ceramics with various sintering rates ranging from 850 to 1100 °C .The PNT specimens exhibited high permittivity values up to ~ 10 4 , low loss tangent below 10 - 2 , and large tunability over 30 % under an electric field intensity of 30 kV / cm at room temperature . With decreasing temperature down to 77 K , the permittivity increased somewhat while the gain tangent decreased significantly related to the freezing out of mobile electrons .At cryogenic temperatures , two relaxation processes were detected in the frequency spectrum between 1 Hz and 100 kHz . The first mechanism was due to the grain boundary phenomenon ; it shifted towards higher frequencies as the temperature reduced .The second process was correlated with ferroelectric domain wall motion ; its relax time constant remained nearly unchanged when the temperature changed .",
        "rewrite_text": "**Title:** Dielectric Relaxation and Phase Transition at Cryogenic Temperatures in 0.65 Pb(Ni1/3Nb2/3)O3 - 0.35PbTiO3 Ceramics\n\n**Abstract:** This study investigates the dielectric properties, phase transition behavior, and microstructural characteristics of (0.65 Pb(Ni1/3Nb2/3)O3 - 0.35PbTiO3) (PNT) ceramics subjected to various sintering temperatures ranging from 850 to 1100 °C. The PNT samples demonstrated remarkable dielectric performance, achieving high permittivity values approaching 10,000, a low loss tangent below 0.01, and significant tunability exceeding 30% when exposed to an electric field intensity of 30 kV/cm at room temperature. As the temperature was lowered to 77 K, an increase in permittivity was observed, accompanied by a notable decrease in the loss tangent, which is attributed to the freezing of mobile charge carriers. At cryogenic temperatures, two distinct dielectric relaxation processes were identified within the frequency range of 1 Hz to 100 kHz. The first relaxation mechanism, associated with grain boundary effects, exhibited a shift towards higher frequencies as the temperature decreased. In contrast, the second relaxation process, linked to the motion of ferroelectric domain walls, displayed a relatively stable relaxation time constant, remaining largely unaffected by temperature variations. These findings provide valuable insights into the dielectric behavior and phase transitions of PNT ceramics at low temperatures, highlighting their potential applications in advanced electronic devices operating under cryogenic conditions. The results underscore the importance of sintering temperature in tailoring the dielectric properties and phase behavior of these materials, paving the way for further research into their practical applications in electronic and ferroelectric technologies.",
        "ori-fast-z-score": -0.3464101615137754,
        "water-fast-z-score": 5.505977612893481,
        "rewrite-fast-z-score": 0.8392543274162825
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  NodeTrix: Hybrid Representation for Analyzing Social Networks .\nAbstract:\nSocial networks are ubiquitous in our daily life, and have been studied extensively by researchers across many disciplines. However, the analysis of social network data is often challenging due to its complexity. In this work we propose NodeTrix, an efficient hybrid representation that can be used to analyze large-scale social networks efficiently. We show how NodeTrix can be applied to solve several important problems including community detection, link prediction, node classification, and influence maximization. Our experiments on real-world datasets demonstrate that NodeTrix outperforms state-of-the-art approaches significantly both in terms of efficiency and effectiveness. 1 Introduction Social networks play an increasingly important role in people s lives. They provide us with new ways to communicate with each other, share information, collaborate, or even make friends. As such, they have attracted much attention from researchers across various fields ranging from sociology  1  , psychology  2  , biology  3  , computer science  4  , engineering  5  , etc.. The rapid development of online social media has led to unprecedented growth in the amount of available social network data  6  . For example, Facebook alone now contains more than one billion active users  7  .\nHowever, analyzing large volumes of social network data remains a challenge because it usually involves complex relationships among nodes  8  . To tackle these challenges, recent research efforts focus on developing effective representations for social networks  9  -  11  . These representations aim at capturing different aspects of social networks while being able to scale up well when dealing with massive amounts of data  12  . Among them, matrix factorization techniques  13  -  15  have shown great promise as they allow us to represent social networks using low-rank matrices  16  . Matrix factorization methods decompose a given adjacency matrix into two smaller matrices (i.e., latent factors) which capture structural properties of the original graph  17  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : NodeTrix : Hybrid Representation for Analyzing Social Networks . Abstract : Social networks are ubiquitous in our daily living , and have been studied frequently by researchers across many disciplines .However , the analysis of social group information is often challenging due to its complexity . In this project we develop NodeTrix , an efficient hybrid representation that can be used to analyze large - scale social systems efficiently .We see how NodeTrix can be applied to solve many important problems namely community screening , link modeling , node classification , and influence maximization . Our experiments on real - time datasets prove that NodeTrix outperforms state - of - the - art methods significantly both in terms of efficiency and effectiveness .1 Introduction Social networks take an increasingly important role in persons s lives . They offer us with innovative ways to interact with each other , share data , collaborate , or even get allies .As such , they have garnered many scrutiny from researchers across numerous topics including from economics 1 , psychology 2 , mathematics 3 , computer science 4 , engineering 5 , etc . . The rapid advancement of internet social publishing has led to unprecedented growth in the quantity of available social platform data 6 .For instance , Facebook alone now contains more than one billion active people 7 . However , examining massive numbers of social group information remains a problem because it often includes difficult connections among nodes 8 .To tackle these problems , recent study efforts focus on developing accurate representations for social organizations 9 - 11 . These representations aim at capturing different components of social organizations while being able to scale up well when dealing with massive amounts of data 12 .Among them , matrix factorization techniques 13 - 15 have shown great success as they allow us to depict public networks using reduced - rank matrices 16 . Matrix factorization techniques decompose a given adjacency vector into two smaller matrices ( i . e . , latent variables ) which capture structural properties of the original graph 17 .",
        "rewrite_text": "**Title:** NodeTrix: A Hybrid Representation for Analyzing Social Networks\n\n**Abstract:** Social networks play a pivotal role in our everyday lives, facilitating interactions, data sharing, collaboration, and the formation of alliances. This widespread influence has attracted considerable attention from researchers across various fields, including economics, psychology, mathematics, computer science, and engineering. The rapid growth of social media platforms has resulted in an overwhelming volume of data, exemplified by Facebook's user base exceeding one billion active accounts. However, the analysis of this extensive social data presents significant challenges due to the intricate relationships among nodes within these networks. In response to these challenges, we introduce NodeTrix, an innovative hybrid representation designed to enhance the analysis of large-scale social systems. NodeTrix effectively addresses several critical issues in social network analysis, including community detection, link prediction, node classification, and influence maximization. Our empirical evaluations on real-world datasets demonstrate that NodeTrix significantly outperforms existing state-of-the-art methodologies in both efficiency and effectiveness. The development of accurate representations for social networks is essential, as it enables researchers to capture the complex dynamics of social interactions while maintaining scalability in the face of vast data volumes. Among the various approaches explored in recent studies, matrix factorization techniques have emerged as particularly effective, allowing for the representation of social networks through reduced-rank matrices. These techniques decompose adjacency matrices into smaller latent variable matrices, which encapsulate the structural characteristics of the original graph. By leveraging the strengths of matrix factorization and integrating them into the NodeTrix framework, we provide a robust tool for researchers and practitioners aiming to gain deeper insights into social networks.",
        "ori-fast-z-score": -0.07647191129018725,
        "water-fast-z-score": 8.84491707929107,
        "rewrite-fast-z-score": -0.727606875108999
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Photoionization Model For The Soft X-Ray Spectrum Of NGC 4151 .\nAbstract:\nWe have developed an improved photoionization model for the soft X-ray spectrum of the Seyfert 1 galaxy NGC 4151, based on its optical and UV properties as well as previous observations in the X-ray band.  We find that the observed soft X-ray emission is dominated by line emission from highly ionized species such as O viii-xxii, Ne ix-xiii, Mg xii-xv, Si xiv-xxvi, S xix-xxxi, Ar xxviii-xxxviii, Ca xx-xxxiii, Fe xx-xxxvi, Ni xx-xxxvii, and possibly also C v-vi. In addition to these lines we predict significant contributions from continuum emission due to free-free processes (bremsstrahlung) and recombination radiation. Our best-fit parameters are consistent with those found previously using other methods. However, our results suggest that the gas density may be higher than estimated before, while the ionization parameter appears lower.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Photoionization Model For The Soft X - Ray Spectrum Of NGC 4151 . Abstract : We have developed an better photoionization design for the soft X - ray spectrum of the Seyfert 1 galaxy NGC 4151 , relying on its optical and UV qualities as well as earlier discoveries in the X - ray band .We see that the seen dark X - ray radiation is dominated by line emission from highly ionized species such as O viii - xxii , Ne ix - xiii , Mg xii - xv , Si xiv - xxvi , S xix - xxxi , Ar xxviii - xxxviii , Ca xx - xxxiii , Fe xx - xxxvi , Ni xx - xxxvii , and maybe also C v - vi . In addition to these lines we expect substantial contributions from continuum emission thanks to free - free processes ( bremsstrahlung ) and recombination emission .Our best - fitting characteristics are compatible with those observed previously used other methods . However , our findings show that the gas density might be higher than calculated before , while the ionization variable appears smaller .",
        "rewrite_text": "We present an enhanced photoionization model for the soft X-ray spectrum of the Seyfert 1 galaxy NGC 4151, integrating its optical and ultraviolet properties alongside previous findings in the X-ray regime. Our analysis reveals that the observed soft X-ray emission is predominantly characterized by line emissions from highly ionized elements, including O VIII to XXII, Ne IX to XIII, Mg XII to XV, Si XIV to XXVI, S XIX to XXXI, Ar XXVIII to XXXVIII, Ca XX to XXXIII, Fe XX to XXXVI, Ni XX to XXXVII, and potentially C V to VI. In addition to these prominent spectral lines, we anticipate significant contributions from continuum emissions resulting from free-free processes (bremsstrahlung) and recombination emissions. The parameters derived from our model align well with those obtained through alternative methodologies in prior studies. Notably, our results suggest that the gas density in the region may be higher than previously estimated, while the ionization parameter appears to be lower than earlier calculations indicated. This refined understanding of the soft X-ray spectrum of NGC 4151 not only enhances our comprehension of the physical conditions within this active galactic nucleus but also provides valuable insights into the processes governing the emission mechanisms in similar astronomical objects. The implications of our findings could lead to a reevaluation of the ionization states and densities in other Seyfert galaxies, thereby contributing to the broader field of astrophysics and the study of active galactic nuclei.",
        "ori-fast-z-score": -1.75,
        "water-fast-z-score": 4.923659639173309,
        "rewrite-fast-z-score": 0.9045340337332909
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Ground-state magneto-optical resonances in Cesium vapour confined in an extremely thin cell .\nAbstract:\nWe report on the observation of ground state magneto optical resonance (GMOR) in cesium vapor confined to a sub-micron thickness layer inside a glass micro-cell. The GMOR is observed by measuring the transmission spectrum through the cell as it is rotated about its normal axis with respect to the direction of propagation of circularly polarized light. We show that this effect can be explained using simple classical electrodynamics and we present experimental results which demonstrate the dependence of the GMOR signal strength on various parameters such as the intensity, frequency detuning and polarization angle of the incident laser beam. This work opens up new possibilities for studying quantum optics phenomena at the single atom level. \n \n In recent years there has been considerable interest in developing techniques for trapping atoms or molecules within microscopic volumes  1  . Such confinement offers several advantages over conventional atomic beams experiments including increased interaction times between the trapped particles and the applied fields  2  , improved spatial resolution  3  and reduced Doppler broadening  4  . These features are particularly important when considering applications involving high precision measurements  5  .\nIn addition to these practical benefits, confining neutral matter to small dimensions also provides opportunities for exploring fundamental physics  6  . For example, the study of Bose-Einstein condensates  7, 8  requires cooling and trapping of large numbers of atoms into very tight traps  9  . Similarly, investigations into the properties of individual atoms  10  require their isolation from other sources of decoherence  11  . Finally, studies of macroscopic quantum effects  12  may benefit from the ability to control the number of particles involved  13  . \n \n Here we describe our efforts towards achieving controlled confinement of neutral matter to extremely small dimensions. Specifically, we have developed a technique for producing a thin film of cesium gas inside a glass micro-cell  14  . By exploiting the strong magnetic dipole moment associated with the cesium ground state  15  , we observe a novel form of magneto-optical resonance  16  known as ground state magneto-optical resonance  17  . Our observations suggest that this phenomenon could provide a useful tool for investigating quantum optics processes occurring at the single atom level  18  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Ground - state magneto - optical resonances in Cesium vapour confined in an incredibly thin cell . Abstract : We report on the observation of ground state magneto optical resonance ( GMOR ) in cesium vapor confined to a sub - micron thickness sheet inside a glass micro - cell .The GMOR is observed by monitoring the propagation spectrum through the cell as it is rotated about its regular axis with regard to the direction of propagation of circularly polarized light . We see that this effect can be described using simple classical electrodynamics and we present experimental results which demonstrate the dependence of the GMOR wave strength on various variables such as the frequency , frequency detuning and polarization angle of the incident beam beam .This study opens up new possibilities for studying quantum optics dynamics at the single atom level . In recent years there has been substantial interest in establishing techniques for trapping atoms or compounds within microscopic volumes 1 .Such confinement gives numerous benefits over traditional molecular beams studies namely increased interaction times between the captured particles and the applied fields 2 , enhanced angular sensitivity 3 and lowered Doppler broadening 4 . These features are particularly important when assessing uses requiring high precision observations 5 .In addition to these useful benefits , confining neutral matter to small dimensions additionally offers options for studying basic physics 6 . For instance , the observation of Bose - Einstein condensates 7 , 8 requires freezing and trapping of large numbers of atoms into very close traps 9 .Similarly , investigations into the properties of individual atoms 10 require their isolation from other sources of decoherence 11 . Finally , investigations of macroscopic quantum effects 12 may benefit from the ability to affect the quantity of atoms involved 13 .Here we explain our initiatives towards attain controlled confinement of neutral matter to incredibly small sizes . Specifically , we have developed a technique for producing a thin film of cesium gas inside a glass micro - cell 14 .By exploiting the strong magnetic dipole moment associated with the cesium ground state 15 , we study a new form of magneto - optical resonance 16 known as ground state magneto - optical resonance 17 . Our observations suggest that this phenomenon might give a helpful resource for investigating quantum optics processes resulting at the single atom level 18 .",
        "rewrite_text": "**Title:** Ground-State Magneto-Optical Resonances in Cesium Vapor Confined in an Incredibly Thin Cell\n\n**Abstract:** In this study, we present our findings on the ground state magneto-optical resonance (GMOR) observed in cesium vapor confined within a sub-micron thick layer inside a glass micro-cell. The GMOR phenomenon is investigated by analyzing the propagation spectrum of circularly polarized light as the micro-cell is rotated around its primary axis. Our results indicate that the observed effects can be effectively described using classical electrodynamics principles. We provide experimental data demonstrating the dependence of GMOR wave strength on several parameters, including frequency, frequency detuning, and the polarization angle of the incident light beam. \n\nThis research contributes to the growing interest in techniques for trapping atoms or molecules within microscopic volumes, which offers significant advantages over traditional molecular beam studies. These advantages include prolonged interaction times between trapped particles and external fields, enhanced angular sensitivity, and reduced Doppler broadening, all of which are crucial for high-precision measurements. Furthermore, confining neutral matter to small dimensions allows for the exploration of fundamental physics, such as the observation of Bose-Einstein condensates, which necessitates the trapping of large numbers of atoms in close proximity. \n\nAdditionally, isolating individual atoms is essential for studying their properties while minimizing decoherence from external sources. Our work also highlights the potential for investigating macroscopic quantum effects by manipulating the number of atoms involved in experiments. We detail our efforts to achieve controlled confinement of neutral matter at remarkably small scales, specifically through the development of a technique to create a thin film of cesium gas within a glass micro-cell. By leveraging the strong magnetic dipole moment associated with the cesium ground state, we explore a novel form of magneto-optical resonance, termed ground state magneto-optical resonance. Our observations suggest that this phenomenon could serve as a valuable tool for probing quantum optical processes at the single atom level, paving the way for future advancements in quantum optics research.",
        "ori-fast-z-score": -0.07088812050083358,
        "water-fast-z-score": 8.211790154220079,
        "rewrite-fast-z-score": 1.994108971003163
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Equivalence principle and electromagnetic field: no birefringence, no dilaton, and no axion .\nAbstract:\nWe show that the equivalence principle (EP) is violated in quantum gravity if there are dilatons or axions coupled to photons. The violation can be tested by measuring the polarization rotation angle of light passing through gravitational fields. We find that the EP-violating effect is suppressed for macroscopic distances but enhanced for microscopic ones such as those inside atoms. This suggests that the measurement of atomic clocks may provide an opportunity to test the EP at high precision. \n \n In this work we study how the equivalence principle (EP), which states that all local physical laws should take their same form in any freely falling frame, is modified when one considers quantum gravity effects. It has been shown previously that the EP is violated in general relativity with massive gravitons  1  . Here we consider whether it remains valid in theories beyond Einstein s theory where new degrees of freedom exist. Specifically, we focus on two types of models: scalar-tensor theories  2  , including Brans-Dicke theory  3  , and string-theory inspired models  4  .\n \nIn these theories, dilatons and/or axions appear as additional degrees of freedom besides graviton(s). Dilatons couple directly to photons while axions do so indirectly via coupling to photons and gluons  5  . These couplings lead to violations of the EP  6  . For example, in scalar-tensor theories, the photon acquires a mass term proportional to the strength of the gravitational field  7, 8  . As a result, the speed of light depends on its direction relative to the gravitational field  9  . If the gravitational field varies along the path of propagation, then the speed of light also changes accordingly  10  . Since different polarizations travel at slightly different speeds, they acquire different phases during propagation  11  . Therefore, the polarization state of light will rotate after traveling through a gravitational potential gradient  12  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Equivalence principle and electromagnetic field : no birefringence , no dilaton , and no axion . Abstract : We see that the equivalence principle ( EP ) is violated in quantum gravitational if there are dilatons or axions coupled to photons .The violation can be evaluated by monitoring the polarization rotation angle of light traveling through gravity fields . We see that the EP - violating phenomenon is suppressed for macroscopic distances but improved for microscopic ones such as those inside atoms .This implies that the observation of atomic clocks may provide an ability to test the EP at high precision . In this research we study how the equivalence principle ( EP ) , which says that all local mechanical laws should take their same shape in any freely falling frame , is modified when one discusses quantum gravitational influences .It has been shown previously that the EP is violated in general relativity with massive gravitons 1 . Here we study whether it remains accepted in theories beyond Einstein s principle where new degrees of liberty exist .Specifically , we focus on two kind of models : scalar - vector models 2 , notably Brans - Dicke theory 3 , and string - theory inspired models 4 . In these theories , dilatons and / or axions exist as additional degrees of freedom besides graviton ( s ) .Dilatons couple directly to photons while axions do so indirectly via coupling to photons and gluons 5 . These couplings contribute to violations of the EP 6 .For instance , in scalar - tensor theories , the photon acquires a mass term proportional to the strength of the gravitational field 7 , 8 . As a result , the speed of light changes on its direction relative to the gravitational field 9 .If the gravitational field varies along the path of propagation , then the speed of light still moves accordingly 10 . Since various polarizations move at slightly different speeds , they acquire various phases during propagation 11 .Therefore , the polarization state of light will rotate after moving through a gravitational potential gradient 12 .",
        "rewrite_text": "**Title:** Equivalence Principle and Electromagnetic Field: No Birefringence, No Dilaton, and No Axion\n\n**Abstract:** This study investigates the implications of the equivalence principle (EP) in the context of quantum gravity, particularly in the presence of dilatons and axions that couple with photons. We demonstrate that the EP is compromised when these additional fields are considered, as evidenced by the polarization rotation of light traversing gravitational fields. Notably, while the effects of EP violation are diminished over macroscopic distances, they become more pronounced at microscopic scales, such as those found within atomic structures. This suggests that atomic clock measurements could serve as a high-precision method for testing the EP. \n\nOur analysis delves into the modifications of the EP, which posits that the laws of physics should remain invariant in any freely falling reference frame, when quantum gravitational effects are taken into account. Previous research has indicated that the EP is violated in general relativity with the introduction of massive gravitons. In this work, we explore whether this violation persists in theories that extend beyond Einstein's framework, particularly those that introduce new degrees of freedom. We focus on two specific theoretical models: scalar-vector models, including Brans-Dicke theory, and string-theory inspired frameworks. \n\nIn these models, dilatons and axions emerge as additional fields alongside gravitons. Dilatons interact directly with photons, while axions influence them indirectly through their coupling with both photons and gluons. These interactions lead to observable violations of the EP. For example, in scalar-tensor theories, photons can acquire a mass term that is dependent on the gravitational field's strength, resulting in variations in the speed of light based on its direction relative to the gravitational influence. Consequently, when light propagates through a region where the gravitational field varies, different polarizations of light travel at slightly different velocities, leading to a phase shift. This phenomenon ultimately causes a rotation in the polarization state of light as it passes through a gradient in gravitational potential.",
        "ori-fast-z-score": -0.0842151921066519,
        "water-fast-z-score": 7.442553889063108,
        "rewrite-fast-z-score": 1.611558966391945
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The LISA Gravitational Wave Foreground: A Study of Double White Dwarfs .\nAbstract:\nWe present the results of our study on gravitational wave (GW) foregrounds for the Laser Interferometer Space Antenna (LISA). We focus on double white dwarfs, which are expected to be one of the most important sources in terms of GW energy density and event rate. In particular we investigate how their properties depend on the initial conditions at formation time as well as on the subsequent evolution driven by nuclear burning and orbital decay due to emission of gravitational waves. The latter is studied with detailed numerical simulations using an updated version of the Eggleton code that includes general relativistic effects. \n \n Our main findings can be summarized as follows: \n \n 1. We find that the number of systems detectable within a given volume depends strongly on the assumed distribution function of binary parameters such as mass ratio q = M2/M1 or total system mass Mtot = M1 + M2. This dependence arises because different distributions lead to very different fractions of binaries with favorable orientations relative to the detector s line-of-sight. If all binaries have random orientation then only about 10% of them will produce signals above the detection threshold. On the other hand if they form preferentially face-on this fraction increases up to 50%. Therefore it seems crucially important to determine the true distribution functions of these quantities observationally before making any predictions regarding the number of detections. \n \n 2. We show that there exists a strong correlation between the masses of the two components of a double white dwarf binary. As a result, the majority of systems detected by LISA will consist of nearly equal-mass objects. However, even though the average value of q is close to unity, there still exist many systems where the secondary component has significantly lower mass than its companion. These systems may provide valuable information about the physics of stellar mergers since they allow us to probe the regime of low-q binaries not accessible through observations of single degenerate stars.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The LISA Gravitational Wave Foreground : A Study of Double White Dwarfs . Abstract : We present the results of our research on gravity wave ( GW ) foregrounds for the Laser Interferometer Space Antenna ( LISA ) .We focus on double white dwarfs , which are expected to be one of the most important sources in terms of GW energy density and event frequency . In particular we investigate how their characteristics rely on the first conditions at structure point as well as on the subsequent evolution accelerated by nuclear burning and orbital decay leading to emission of gravitational waves .The last is studied with comprehensive numerical simulations using an updated edition of the Eggleton code that contains general relativistic effects . Our main results can be summarized as follows : 1 .We see that the proportion of systems detectable within a given volume depends strongly on the assumed distribution function of binary parameters such as mass ratio q = M2 / M1 or total system weight Mtot = M1 + M2 . This dependence occurs because different distributions result to very different fractions of binaries with favorable orientations relative to the sensor s line - of - view .If all binaries have random orientation then only about 10 % of them will generate frequencies above the screening threshold . On the other hand if they shape preferentially face - on this fraction increases up to 50 % .Therefore it appears crucially vital to predict the true distribution functions of these quantities observationally before making any observations involving the number of detections . 2 .We see that there exists a powerful correlation between the masses of the two parts of a double white dwarf binary . As a result , the majority of components observed by LISA will consist of almost equal - mass bodies .However , even though the average value of q is close to unity , there still exist many systems where the secondary component has much lower mass than its companion . These systems may provide valuable info about the physics of stars mergers since they allow us to probe the regime of low - q binaries not accessible through observations of single degenerate stars .",
        "rewrite_text": "**Title:** The LISA Gravitational Wave Foreground: An Investigation of Double White Dwarfs\n\n**Abstract:** In this study, we explore the gravitational wave (GW) foregrounds relevant to the Laser Interferometer Space Antenna (LISA), with a particular emphasis on double white dwarf systems. These binaries are anticipated to be significant contributors to the GW energy density and event rates detectable by LISA. Our research delves into how the initial conditions at the structural point of these systems influence their characteristics, as well as how their evolution—driven by nuclear burning and orbital decay—leads to gravitational wave emissions. To achieve this, we employ advanced numerical simulations utilizing an updated version of the Eggleton code, which incorporates general relativistic effects.\n\nOur findings can be summarized in two key points: First, we demonstrate that the detectability of double white dwarf systems within a specified volume is highly sensitive to the assumed distribution functions of binary parameters, such as the mass ratio \\( q = M_2 / M_1 \\) and the total system mass \\( M_{\\text{tot}} = M_1 + M_2 \\). This sensitivity arises because varying distributions yield significantly different fractions of binaries with orientations favorable for detection by LISA. For instance, if the orientations of all binaries are random, only about 10% will produce frequencies exceeding the detection threshold. Conversely, if the binaries are preferentially oriented face-on, this fraction can rise to 50%. Therefore, accurately predicting the true distribution functions of these parameters is essential for interpreting observational data regarding the number of detectable events.\n\nSecond, we identify a strong correlation between the masses of the components in double white dwarf binaries. Consequently, a substantial portion of the systems observed by LISA will consist of nearly equal-mass components. However, despite the average mass ratio \\( q \\) being close to unity, there are numerous systems where the secondary component has a significantly lower mass than its primary. These lower-mass systems are particularly valuable for understanding stellar merger physics, as they allow us to investigate the regime of low-mass ratio binaries, which is not accessible through observations of single degenerate stars.",
        "ori-fast-z-score": -0.15811388300841897,
        "water-fast-z-score": 6.128258770283412,
        "rewrite-fast-z-score": 1.432003111516314
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Point-contact spectroscopy of the antiferromagnetic superconductor HoNi2B2C in the normal and superconducting state .\nAbstract:\nWe report on point contact Andreev reflection (PCAR) measurements performed on single crystals of the heavy fermion compound HoNi2B2C, which is an antiferromagnet with T N = 1.5 K that becomes a type-II superconductor below Tc = 0.8 K. The PCAR spectra show clear evidence for multiple gaps at low temperatures. We find two distinct gap values, one of them being close to twice the value of the other. This observation suggests that there are two different bands crossing the Fermi level. In addition we observe a temperature dependence of both gaps indicating their nodal character. Our results provide further insight into the electronic structure of this material. Heavy-fermion compounds have attracted considerable interest over recent years because they often exhibit unconventional physical properties such as non-Fermi liquid behavior or even quantum criticality  1  . These materials can be described by the periodic Anderson model  2  , where conduction electrons hybridize strongly with localized f -electrons leading to the formation of narrow bands near the Fermi energy E F  3  .\nHoNi 2 B 2 C belongs to the family of so-called borocarbides  4  . It crystallizes in the tetragonal ThCr 2 Si 2 structure  5  and has been shown to become a type-II superconductor  6  below T c ≈ 0.8 K  7, 8  . At ambient pressure it orders magnetically around T N = 1.6 K  9  . Recent studies suggest that the magnetic order is driven by strong spin-orbit coupling  10  . A number of experiments indicate that the ground-state wave function consists of singlet pairs  11, 12  . However, the exact nature of the pairing mechanism remains unclear  13  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Point - touch spectroscopy of the antiferromagnetic superconductor HoNi2B2C in the usual and superconducting state . Abstract : We report on point touch Andreev reflection ( PCAR ) observations performed on single crystals of the heavy fermion compound HoNi2B2C , which is an antiferromagnet with T N = 1 . 5 K that remains a class - II superconductor below Tc = 0 . 8 K . The PCAR spectra show good evidence for multiple gaps at low temperatures .We see two different gap values , one of them being close to double the value of the other . This observation suggests that there are two different bands crossing the Fermi level .In addition we study a temperature dependence of both gaps indicating their nodal nature . Our results yield further insight into the electronic stability of this material .Heavy - fermion compounds have garnered considerable interest over recent history because they frequently exhibit unusual physical properties such as non - Fermi solid behavior or even quantum criticality 1 . These substances can be described by the periodic Anderson model 2 , where conduction electrons hybridize heavily with localized f - ions causing to the formation of broad bands near the Fermi energy E F 3 .HoNi 2 B 2 C belongs to the class of so - called borocarbides 4 . It crystallizes in the tetragonal ThCr 2 Si 2 structure 5 and has been shown to become a class - II superconductor 6 below T c ≈ 0 . 8 K 7 , 8 .At ambient temperature it orders magnetically around T N = 1 . 6 K 9 . Recent research suggest that the magnetic order is generated by strong spin - orbit interaction 10 .A variety of studies imply that the ground - state wave function consists of singlet sets 11 , 12 . However , the exact nature of the pairing structure remains unclear 13 .",
        "rewrite_text": "We present a comprehensive study of point contact Andreev reflection (PCAR) measurements conducted on single crystals of the heavy fermion compound HoNi2B2C, which is characterized as an antiferromagnet with a Néel temperature (T_N) of 1.5 K and transitions into a class-II superconductor below a critical temperature (T_c) of approximately 0.8 K. Our PCAR spectra provide compelling evidence for the presence of multiple superconducting gaps at low temperatures, revealing two distinct gap values, one of which is nearly double the other. This finding implies the existence of two separate bands that intersect the Fermi level, suggesting a complex electronic structure. Furthermore, we investigate the temperature dependence of these gaps, which indicates their nodal characteristics. These insights contribute to a deeper understanding of the electronic stability of HoNi2B2C.\n\nHeavy fermion compounds have attracted significant attention in recent years due to their often unconventional physical properties, including non-Fermi liquid behavior and potential quantum criticality. These materials can be effectively described by the periodic Anderson model, wherein conduction electrons undergo substantial hybridization with localized f-electrons, leading to the formation of broad bands near the Fermi energy (E_F). HoNi2B2C is classified among the borocarbides and crystallizes in the tetragonal ThCr2Si2 structure. It has been established that this compound exhibits superconductivity below T_c ≈ 0.8 K, while at room temperature, it exhibits magnetic ordering around T_N = 1.6 K. Recent studies suggest that the magnetic ordering may arise from strong spin-orbit coupling effects. Various investigations indicate that the ground-state wave function is composed of singlet pairs; however, the precise nature of the pairing mechanism remains an open question. Our findings not only enhance the understanding of the superconducting properties of HoNi2B2C but also contribute to the broader discourse on the intriguing behaviors exhibited by heavy fermion systems.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 6.6996452071463715,
        "rewrite-fast-z-score": 0.6713450866373513
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Possibility of large lifetime differences in neutral B meson systems .\nAbstract:\nWe study the possibility that there are two different lifetimes for neutral B mesons, one corresponding to the standard model and another to new physics beyond it.  We find that if the decay widths into final states with charm quarks differ by more than about 10% between these two types of B mesons then this can be observed at future experiments such as LHCb or Belle II. If we assume that the ratio of branching fractions is equal to 1 (as predicted within the Standard Model) but allow the total decay widths to vary independently, then we show how the experimental data on the time dependent CP asymmetry parameters SCP and ACP can be used to determine whether the difference in decay widths is due to new physics effects or not. Finally, we discuss possible extensions of our analysis which could lead to further constraints on the allowed parameter space. The results presented here will also have implications for other measurements performed at hadron colliders involving heavy flavour particles.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Possibility of large lifetime variations in neutral B meson systems . Abstract : We research the prospect that there are two different lifetimes for neutral B mesons , one corresponding to the standard description and another to new science beyond it .We see that if the decay widths into last states with charm quarks vary by more than about 10 % between these two kind of B mesons then this can be observed at future research such as LHCb or Belle II . If we suppose that the proportion of branching fractions is equal to 1 ( as predicted within the Standard Model ) but allow the total degradation widths to vary independently , then we tell how the empirical data on the period dependent CP asymmetry characteristics SCP and ACP can be used to predict whether the difference in decay widths is due to novel physics factors or not .Finally , we explain possible extensions of our analysis which potentially contribute to further limitation on the allowed parameter area . The results presented here will also have consequences for other tests accomplished at hadron colliders regarding heavy flavour particles .",
        "rewrite_text": "In this study, we investigate the intriguing possibility of differing lifetimes for neutral B mesons, proposing that one lifetime aligns with the conventional understanding while the other may indicate new physics beyond the Standard Model. Our analysis suggests that if the decay widths into final states containing charm quarks exhibit a variation exceeding approximately 10% between these two types of B mesons, such discrepancies could be detectable in upcoming experiments at facilities like LHCb or Belle II. We consider a scenario where the ratio of branching fractions is maintained at unity, as predicted by the Standard Model, while allowing the total decay widths to vary independently. This approach enables us to utilize empirical data related to time-dependent CP asymmetries, specifically SCP and ACP, to discern whether any observed differences in decay widths stem from novel physics or are consistent with established theoretical frameworks. Furthermore, we discuss potential extensions to our analysis that could refine the constraints on the allowed parameter space, enhancing our understanding of the underlying physics. The implications of our findings extend beyond the realm of B meson studies, influencing other experimental tests conducted at hadron colliders that focus on heavy flavor particles. Overall, this research opens new avenues for exploring the fundamental properties of B mesons and their interactions, potentially leading to significant insights into the nature of particle physics.",
        "ori-fast-z-score": -2.457864091118742,
        "water-fast-z-score": 5.196152422706631,
        "rewrite-fast-z-score": 0.30151134457776363
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Circular and non-circular nearly horizon-skimming orbits in Kerr spacetimes .\nAbstract:\nWe study the circular and non-circular motion near the event horizons of rotating black holes by using the Hamilton-Jacobi method, which is an extension of the standard geodesic approach to include higher-order corrections due to gravitational radiation reaction effects. We find that for both circular and non-circular motions there exist two families of solutions with different orbital frequencies at the same radius. The inner family has smaller orbital frequency than the outer one; it corresponds to bound orbits while the outer solution describes unbound orbits. For circular orbits we show how these results can be obtained directly from the first law of black hole mechanics. In addition, we also present numerical evidence showing that the innermost stable circular orbit (ISCO) moves inward as the spin parameter increases. Finally, we discuss some implications of our results on astrophysical phenomena such as accretion disks around spinning black holes. Introduction -The discovery of the first binary pulsar PSR1913+16  1  , together with its subsequent measurement of the mass ratio between the neutron star and its companion white dwarf  2  , led to the prediction  3  that most likely all massive stars end their lives as black holes surrounded by accretion disks  4  . Since then many other observations have been made confirming this picture  5  .\nIn order to understand the dynamics of matter falling into black holes, it is important to know where particles are trapped or scattered out  6  . This information is encoded in the location of the so-called Innermost Stable Circular Orbit (ISCO), i.e., the smallest possible radius r ISCO of a particle s circular orbit  7, 8  . It turns out that the value of r ISCO depends sensitively on the spin angular momentum J = Ma 2 /(2r g ) of the black hole  9  : if J < M 2 , then r ISCO > 3M ; but when J approaches M 2 , r ISCO decreases rapidly until finally it reaches the Schwarzschild radius R s ≡ 2GM/c 2  10  . Therefore, knowing the exact position of the ISCO will help us better understand the physics behind various processes taking place close to",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Circular and non - circular nearly horizon - skimming orbits in Kerr spacetimes . Abstract : We study the circular and non - circular motion near the event horizons of spinning black holes by using the Hamilton - Jacobi method , which is an extension of the standard geodesic approach to use larger - order corrections due to gravitational radiation process effects .We see that for both circular and non - circular movements there exist two families of solutions with various orbital frequencies at the same radius . The outer family has less orbital frequency than the inner one ; it corresponds to bound orbits while the inner solution refers unbound orbits .For circular orbits we prove how these results can be obtained directly from the first law of black hole mechanics . In addition , we also provided quantitative proof showing that the innermost stable spherical orbit ( ISCO ) changes inward as the spin parameter grows .Finally , we talk some implications of our findings on astrophysical processes such as accretion disks around moving black holes . Introduction - The observation of the first binary pulsar PSR1913 + 16 1 , combined with its subsequent calculation of the mass ratio between the neutron star and its companion dark dwarf 2 , leading to the prediction 3 that most likely all large galaxies begin their careers as black holes surrounded by accretion disks 4 .Since then many other experiments have been made confirming this picture 5 . In order to comprehend the dynamics of matter falling into black holes , it is important to consider where objects are captured or scattered out 6 .This information is stored in the location of the so - called Innermost Stable Circular Orbit ( ISCO ) , i . e . , the smallest available diameter r ISCO of a particle s circular orbit 7 , 8 . It turns out that the value of r ISCO relies sensitively on the spin angular velocity J = Ma 2 / ( 2r g ) of the red hole 9 : if J < M 2 , then r ISCO > 3M ; but when J approaches M 2 , r ISCO falls slowly until finally it meets the Schwarzschild diameter R s ≡ 2GM / c 2 10 .Therefore , knowing the exact position of the ISCO will assist us better understand the physics behind several mechanisms taking place nearby to",
        "rewrite_text": "**Title:** Circular and Non-Circular Nearly Horizon-Skimming Orbits in Kerr Spacetimes\n\n**Abstract:** This study investigates the dynamics of circular and non-circular orbits in the vicinity of the event horizons of rotating black holes, utilizing the Hamilton-Jacobi method. This approach extends the conventional geodesic framework by incorporating higher-order corrections that account for the effects of gravitational radiation. Our analysis reveals the existence of two distinct families of solutions for both circular and non-circular orbits, each characterized by different orbital frequencies at identical radii. The outer family exhibits lower orbital frequencies and corresponds to bound orbits, while the inner family pertains to unbound orbits. For circular orbits, we demonstrate that these findings can be derived directly from the first law of black hole mechanics. Furthermore, we provide quantitative evidence indicating that the innermost stable circular orbit (ISCO) shifts inward as the black hole's spin parameter increases. The implications of our results are significant for understanding astrophysical phenomena, particularly in relation to accretion disks surrounding rotating black holes.\n\nThe introduction highlights the historical context of black hole research, beginning with the observation of the binary pulsar PSR1913+16 and the subsequent determination of mass ratios between neutron stars and their companion dark dwarfs. This led to the hypothesis that most large galaxies likely originate as black holes enveloped by accretion disks. Numerous experiments have since corroborated this model. To fully grasp the dynamics of matter interacting with black holes, it is crucial to identify the regions where objects are either captured or ejected. This information is encapsulated in the position of the ISCO, which represents the smallest radius at which a particle can maintain a stable circular orbit. Notably, the radius of the ISCO is highly sensitive to the spin angular momentum of the black hole. Specifically, if the spin parameter is less than a critical value, the ISCO radius exceeds three times the mass of the black hole, while as the spin approaches this critical threshold, the ISCO radius gradually decreases until it coincides with the Schwarzschild radius. Understanding the precise location of the ISCO is essential for elucidating the physical processes occurring in the extreme environments surrounding black holes.",
        "ori-fast-z-score": -2.1226745220209295,
        "water-fast-z-score": 5.577959874688627,
        "rewrite-fast-z-score": -0.6652991438591156
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Derivation of Einstein Gravity without the Axiom of Choice: Topology Hidden in GR .\nAbstract:\nWe present an alternative derivation of Einstein gravity, which does not use the axiom of choice and is based on the concept that topology can be hidden inside general relativity (GR). We show how to construct a set of local coordinates for any given spacetime point such that all points with the same coordinate values are connected by geodesics. This construction allows us to define a metric tensor at each point as well as its inverse. The resulting theory has exactly the same field equations as standard GR but it contains additional degrees of freedom corresponding to the number of disconnected components of the underlying space. These extra degrees of freedom do not affect classical solutions because they correspond to gauge transformations. However, we argue that these new degrees of freedom may play an important role when considering quantum effects. In particular, we discuss possible implications of our approach for black hole entropy calculations. Finally, we comment on some open problems related to this work.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Derivation of Einstein Gravity without the Axiom of Choice : Topology Hidden in GR . Abstract : We present an alternative derivation of Einstein relativity , which does not use the axiom of selection and is based on the idea that geometry can be hidden inside general relativity ( GR ) .We see how to build a setting of local coordinates for any given spacetime point such that all points with the same coordinate parameters are connected by geodesics . This construction provides us to define a metric tensor at each point as well as its inverse .The resulting theory has precisely the same field equations as conventional GR but it contains additional degrees of liberty corresponding to the number of detached components of the underlying space . These added degrees of liberty do not alter classical solutions because they relate to gauge processes .However , we feel that these new degrees of liberty might play an important role when assessing quantum effects . In particular , we issue possible possibilities of our approach for black hole entropy calculations .Finally , we comment on some open problems related to this project .",
        "rewrite_text": "In this article, titled \"A Derivation of Einstein Gravity without the Axiom of Choice: Topology Hidden in GR,\" we introduce a novel derivation of Einstein's theory of relativity that circumvents the use of the axiom of choice. Our approach is predicated on the notion that geometric structures can be embedded within the framework of general relativity (GR). We demonstrate a method for establishing a local coordinate system around any specified point in spacetime, ensuring that all points sharing the same coordinate parameters are interconnected through geodesics. This framework allows us to define a metric tensor at each point, along with its inverse.\n\nThe theory we develop yields field equations that align precisely with those of traditional general relativity, yet it incorporates additional degrees of freedom that correspond to the number of disconnected components within the underlying space. Notably, these extra degrees of freedom do not affect classical solutions, as they pertain to gauge transformations. However, we propose that these new degrees of freedom could be significant when exploring quantum effects in gravitational contexts. \n\nIn particular, we discuss the implications of our framework for calculating black hole entropy, suggesting that our approach may offer fresh insights into this complex area of research. Lastly, we address several unresolved issues that arise from our findings, indicating potential avenues for future investigation. This work not only contributes to the understanding of general relativity but also opens up new possibilities for exploring the interplay between gravity and quantum mechanics.",
        "ori-fast-z-score": -0.8626621856275073,
        "water-fast-z-score": 4.557990884027348,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Stability of the periodic Toda lattice under short range perturbations .\nAbstract:\nWe study stability properties of the periodic Toda lattice with respect to small  time-periodic perturbations. We show that if the perturbation is sufficiently small, then there exists an exponentially stable solution which can be found by solving a linear algebraic system. The proof relies on the Floquet theory and Lyapunov-Schmidt reduction method. This result generalizes previous results obtained in the case when the unperturbed system has only one equilibrium point or when it possesses two equilibria but they are not connected via heteroclinic orbits. In particular we prove that for any number of equilibria greater than 2 (including infinitely many) there exist arbitrarily large perturbations such that all solutions of perturbed systems converge to infinity as t → ∞. \nIntroduction\n\nThe Toda lattice is a classical example of a completely integrable Hamiltonian system introduced by Toda  Tod  . It describes N particles moving along straight lines with pairwise exponential interaction potential between them. For simplicity let us consider the case N = 1. Then the equation describing this motion takes the form \nwhere x(t), y(t) ∈ R n , A :  0, T   × R n → R n×n is continuous matrix-valued function satisfying some additional conditions specified below. If A ≡ 0, i.e., no external forces act upon the particle, then the corresponding solution is called the free Toda flow. It was shown in  KN  that the free Toda flow is globally asymptotically stable provided that the spectrum of the matrix A does not intersect the imaginary axis. Moreover, the authors proved that the set of initial data leading to bounded trajectories coincides with the set of initial data belonging to the basin of attraction of the zero solution. However, these results do not hold true anymore if the matrix A depends on time.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Stability of the periodic Toda lattice under short scale perturbations . Abstract : We explore stability properties of the periodic Toda lattice with regard to small time - periodic perturbations .We see that if the perturbation is sufficiently small , then there exists an exponentially stable solution which can be found by solving a linear algebraic scheme . The proof uses on the Floquet model and Lyapunov - Schmidt elimination theory .This result generalizes earlier findings obtained in the case when the unperturbed model has only one equilibrium point or when it enjoys two equilibria but they are not linked via heteroclinic orbits . In particular we prove that for any number of equilibria greater than 2 ( including infinitely many ) there exist arbitrarily huge perturbations such that all solutions of perturbed systems converge to infinity as t → ∞ .Introduction The Toda lattice is a classical example of a completely integrable Hamiltonian structure developed by Toda Tod . It models N particles moving along straight lines with pairwise exponential interaction potential between them .For simplicity let us consider the case N = 1 . Then the equation explaining this motion takes the form where x ( t ) , y ( t ) ∈ R n , A : 0 , T × R n → R n×n is continuous matrix - valued function satisfying some additional conditions defined below .If A ≡ 0 , i . e . , no external forces action upon the particle , then the equivalent solution is dubbed the free Toda flow . It was shown in KN that the free Toda flow is internationally asymptotically stable given that the spectrum of the matrix A does not intersect the imaginary axis .Moreover , the papers proved that the set of initial data leading to finite trajectories coincides with the set of initial data belonging to the basin of attraction of the zero solution . However , these results do not stand true anymore if the matrix A depends on time .",
        "rewrite_text": "**Title:** Stability of the Periodic Toda Lattice Under Short-Scale Perturbations\n\n**Abstract:** This study investigates the stability characteristics of the periodic Toda lattice in the presence of small, time-periodic perturbations. Our findings indicate that when the perturbation is sufficiently minor, there exists an exponentially stable solution that can be derived through a linear algebraic framework. The proof leverages the Floquet theory and the Lyapunov-Schmidt reduction technique. This work extends previous results that were limited to scenarios where the unperturbed system had either a single equilibrium point or two equilibria that were not connected by heteroclinic orbits. Notably, we demonstrate that for systems with more than two equilibria—potentially an infinite number—there exist arbitrarily large perturbations that cause all solutions of the perturbed systems to diverge to infinity as time progresses towards infinity. \n\nThe Toda lattice serves as a quintessential example of a completely integrable Hamiltonian system, originally formulated by Toda. It describes the dynamics of N particles moving along linear trajectories, interacting through a pairwise exponential potential. For simplicity, we focus on the case where N equals one, leading to a motion equation characterized by variables x(t) and y(t) in R^n, with A: (0, T) × R^n → R^(n×n) being a continuous matrix-valued function subject to specific conditions outlined in the paper. In the absence of external forces, represented by A being identically zero, the resulting motion is referred to as the free Toda flow. Previous research established that this free flow is globally asymptotically stable, provided that the spectrum of matrix A does not intersect the imaginary axis. Furthermore, it was shown that the set of initial conditions leading to finite trajectories aligns with the basin of attraction for the zero solution. However, these stability results are no longer applicable when the matrix A is time-dependent, highlighting the complexities introduced by perturbations in the system.",
        "ori-fast-z-score": -0.08606629658238704,
        "water-fast-z-score": 5.487954724560282,
        "rewrite-fast-z-score": 0.8838834764831843
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Expected Planets in Globular Clusters .\nAbstract:\nGlobular clusters are dense stellar systems that contain thousands to millions of stars, and may be the oldest gravitationally bound objects known.  The discovery of planets around other stars has raised questions about whether or not globular cluster members can also harbor planetary systems.   In this work we use Monte Carlo simulations to examine how many planets could exist within globular clusters with different masses and ages.  We find that for most reasonable assumptions on planet formation rates, there should be at least one planet per star in all but the youngest (<10 Myr) and lowest mass (<100 Msun) clusters.  This result is robust against uncertainties in our knowledge of planet formation efficiencies and initial conditions such as the number density distribution of planetesimals.  Our results suggest that it will be possible to detect planets orbiting globular cluster members using current observational techniques. Keywords: Planetary systems; Stellar evolution; Star clusters; Formation",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Expected Planets in Globular Clusters . Abstract : Globular complexes are dense stellar structures that produce thousands to millions of stars , and may be the earliest gravitationally bound structures known .The observation of stars around other stars has raised questions about whether or not globular cluster groups can also harbor planetary structures . In this project we using Monte Carlo simulations to examine how many worlds could occur within globular complexes with various masses and periods .We see that for most reasonable assumptions on moon formation rates , there should be at least one planet per star in all but the youngest ( < 10 Myr ) and lowest mass ( < 100 Msun ) clusters . This result is robust against uncertainties in our know of planet development efficiencies and original conditions such as the number density distribution of planetesimals .Our results propose that it will be possible to identify planets orbiting globular cluster elements using current observational techniques . Keywords : Planetary systems ; Stellar evolution ; Star clusters ; Formation",
        "rewrite_text": "Title: Expected Planets in Globular Clusters\n\nAbstract: Globular clusters are tightly packed stellar formations that can contain thousands to millions of stars, representing some of the earliest gravitationally bound structures in the universe. The discovery of stars orbiting other stars has prompted inquiries into the potential for planetary systems to exist within these dense environments. This study employs Monte Carlo simulations to investigate the likelihood of planetary formation within globular clusters of varying masses and ages. Our findings suggest that, under most plausible scenarios regarding moon formation rates, there is likely to be at least one planet for every star in globular clusters, with the exception of the youngest clusters (less than 10 million years old) and those with lower masses (under 100 solar masses). This conclusion remains consistent even when accounting for uncertainties related to planetary development efficiencies and initial conditions, such as the distribution of planetesimals. The implications of our results indicate that current observational methods may soon be capable of detecting planets orbiting stars in globular clusters, thereby enhancing our understanding of planetary system formation in these ancient stellar environments. This research contributes to the broader field of stellar evolution and planetary system dynamics, highlighting the potential for rich and diverse planetary architectures within some of the universe's oldest star clusters. \n\nKeywords: Planetary systems; Stellar evolution; Star clusters; Formation.",
        "ori-fast-z-score": -0.22645540682891913,
        "water-fast-z-score": 6.187983455093131,
        "rewrite-fast-z-score": 1.0536089137432665
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  COSMOSOMAS Observations of the CMB and Galactic Foregrounds at 11 GHz: Evidence for anomalous microwave emission at high Galactic Latitude .\nAbstract:\nWe present new observations made with the Cosmosoma experiment, which were designed to search for evidence of an excess in cosmic microwave background (CMB) temperature fluctuations above those predicted by standard cosmological models. The data are consistent with predictions based on current theoretical understanding but show some unexpected features that may be related to previously unidentified foreground sources or systematic effects associated with our analysis techniques. \n \n We have used these results to place limits on possible contributions from primordial gravitational waves and other exotic phenomena such as topological defects. These limits are comparable to previous measurements obtained using different experimental approaches. In addition we report the detection of a significant signal at frequencies below 10GHz, which is not expected within conventional cosmological models. This could represent either a new source of foreground contamination or a novel physical effect. Further investigation will require additional experiments to confirm this result and determine its origin. If confirmed it would provide important constraints on theories attempting to explain the observed anisotropy in the CMB spectrum.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : COSMOSOMAS Observations of the CMB and Galactic Foregrounds at 11 GHz : Evidence for anomalous microwave emission at high Galactic Latitude . Abstract : We address new experiments done with the Cosmosoma study , which were built to search for indication of an accumulation in cosmic microwave background ( CMB ) temperature fluctuations above those predicted by typical cosmological models .The data are compatible with predictions based on current theoretical knowledge but indicate some surprising characteristics that might be connected to formerly unidentified foreground sources or systematic effects involved with our analysis methods . We have utilized these results to place limits on potential contributions from primordial magnetic waves and other exotic processes such as topological errors .These limits are comparable to previous measurements obtained using separate experimental methods . In addition we report the observation of a substantial frequency at speeds below 10GHz , which is not anticipated within conventional cosmological models .This might represent either a new cause of foreground contamination or a novel physical impact . Further investigation will demand additional studies to confirm this effect and establish its identity .If confirmed it would offer important restrictions on theories attempting to explain the observed anisotropy in the CMB spectrum .",
        "rewrite_text": "Title: COSMOSOMAS Observations of the CMB and Galactic Foregrounds at 11 GHz: Evidence for Anomalous Microwave Emission at High Galactic Latitude\n\nAbstract: In this article, we present findings from the recent COSMOSOMAS experiments, which were designed to investigate potential anomalies in cosmic microwave background (CMB) temperature fluctuations that exceed the predictions of standard cosmological models. Our analysis reveals that while the data align with existing theoretical frameworks, they also exhibit unexpected features that may be linked to previously unrecognized foreground sources or systematic biases in our analytical techniques. We have leveraged these observations to impose constraints on the contributions from primordial magnetic waves and other exotic phenomena, such as topological defects. Notably, the limits we established are consistent with earlier measurements derived from different experimental approaches. Furthermore, we report the detection of a significant signal at frequencies below 10 GHz, which is not anticipated by conventional cosmological theories. This finding could indicate either a new source of foreground contamination or a novel physical effect that warrants further exploration. To validate this observation and clarify its nature, additional studies will be necessary. If substantiated, this effect could provide critical insights into the mechanisms underlying the observed anisotropy in the CMB spectrum, thereby enhancing our understanding of the universe's early conditions and the fundamental processes that shaped its evolution.",
        "ori-fast-z-score": -0.8955334711889903,
        "water-fast-z-score": 6.733003292241386,
        "rewrite-fast-z-score": -0.5184758473652127
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Slow wave resonance in periodic stacks of anisotropic layers .\nAbstract:\nWe study the slow wave resonance (SWR) effect for periodically layered media with an arbitrary number N of anisotropic layers, each characterized by its own permittivity tensor and thickness. We show that SWR is possible only if all principal axes of the permittivity tensors are parallel to one another within each layer. In this case we derive explicit expressions for the dispersion relation between the frequency f and the Bloch wavenumber kx. The results obtained can be used as guidelines for designing multilayered structures exhibiting strong SWR effects at low frequencies. \n \n Keywords: Slow wave resonance; Anisotropy; Multilayer structure; Dispersion relations. 1 Introduction \n \n Periodic multilayers consisting of alternating thin films made of different materials have attracted considerable attention during recent years due to their unique properties  1  . These include high reflectance  2  , negative refraction  3  , enhanced nonlinear optical response  4  , etc., which make them promising candidates for various applications such as optoelectronic devices  5  or photovoltaics  6  .\n \nIn particular, it has been shown recently  7–9  that periodic multilayers composed of anisotropic layers may exhibit very interesting electromagnetic phenomena including slow wave resonance (S WR). This phenomenon occurs when the phase velocity of the Bloch waves becomes equal to zero inside the medium  10  . It leads to extremely large values of the effective refractive index n eff = c / v ph  11  where c is the speed of light in vacuum and v ph is the phase velocity of the propagating Bloch mode  12  . As a result, the corresponding transmission spectrum exhibits sharp peaks associated with narrow stop bands  13  . Such features are highly desirable for many practical applications  14  . \n \n However, despite numerous theoretical studies devoted to S WR in periodic multilayers  15–18  , there still exist several open questions related to the conditions under which this phenomenon takes place  19, 20  . For example, it was found experimentally  21  that the presence of a single misaligned anisotropic layer destroys the S WR effect completely even though other layers remain perfectly aligned. On the other hand, numerical simulations  22  suggest that",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Slow wave resonance in periodic piles of anisotropic layers . Abstract : We research the slow wave resonance ( SWR ) effect for regularly layered media with an arbitrary number N of anisotropic layers , each described by its own permittivity vector and thickness .We see that SWR is possible only if all primary axes of the permittivity tensors are connected to one another within each layer . In this situation we derive explicit expressions for the dispersion connection between the frequency f and the Bloch wavenumber kx .The results collected can be used as guidelines for constructing multilayered buildings presenting strong SWR effects at low frequencies . Keywords : Slow wave vibration ; Anisotropy ; Multilayer structure ; Dispersion relations .1 Introduction Periodic multilayers consisting of alternating thin films formed of different materials have garnered considerable scrutiny during recent seasons due to their distinct characteristics 1 . These include high reflectance 2 , positive refraction 3 , enhanced nonlinear optical reaction 4 , etc . , which make them promising candidates for various uses such as optoelectronic technologies 5 or photovoltaics 6 .In particular , it has been shown ago 7 – 9 that periodic multilayers consisting of anisotropic elements may exhibit very interesting electrical processes including slow wave resonance ( S WR ) . This phenomenon occurs when the phase velocity of the Bloch waves becomes equal to zero inside the medium 10 .It results to incredibly large values of the effective refractive index n eff = c / v ph 11 where p is the speed of light in vacuum and v ph is the phase velocity of the propagating Bloch mode 12 . As a result , the associated transmission spectrum exhibits strong spikes identified with narrow stop rings 13 .Such characteristics are extremely attractive for numerous practical applications 14 . However , despite several practical studies focused to S WR in periodic multilayers 15 – 18 , there still exist several open questions related to the conditions under which this phenomenon happens place 19 , 20 .For instance , it was shown experimentally 21 that the presence of a single misaligned anisotropic surface destroys the S WR effect totally even though other layers remain perfectly aligned . On the other hand , numerical simulations 22 suggest that",
        "rewrite_text": "**Title:** Slow Wave Resonance in Periodic Piles of Anisotropic Layers\n\n**Abstract:** This study investigates the phenomenon of slow wave resonance (SWR) in periodically layered media composed of an arbitrary number \\( N \\) of anisotropic layers, each characterized by its unique permittivity vector and thickness. Our findings indicate that SWR can only occur when the principal axes of the permittivity tensors are interconnected within each layer. Under these conditions, we derive explicit mathematical expressions that describe the relationship between frequency \\( f \\) and the Bloch wavenumber \\( k_x \\). The insights gained from this research provide valuable guidelines for the design of multilayered structures that exhibit pronounced SWR effects at low frequencies. \n\nThe interest in periodic multilayers, which consist of alternating thin films made from various materials, has surged due to their unique properties, including high reflectance, positive refraction, and enhanced nonlinear optical responses. These attributes position them as promising candidates for applications in optoelectronic technologies and photovoltaics. Previous studies have demonstrated that multilayers with anisotropic components can exhibit intriguing electrical behaviors, notably SWR, where the phase velocity of Bloch waves approaches zero within the medium. This leads to exceptionally high effective refractive indices, resulting in transmission spectra characterized by pronounced spikes associated with narrow stop bands. \n\nDespite the growing body of research on SWR in periodic multilayers, several questions remain regarding the specific conditions necessary for this phenomenon to manifest. For example, experimental evidence has shown that even a single misaligned anisotropic layer can completely disrupt the SWR effect, despite the alignment of other layers. Conversely, numerical simulations suggest that the interplay of layer alignment and anisotropy is critical to the emergence of SWR. This paper aims to clarify these conditions and contribute to the understanding of SWR in multilayered structures, paving the way for innovative applications in various fields. \n\n**Keywords:** Slow wave resonance; Anisotropic materials; Multilayer structures; Dispersion relations.",
        "ori-fast-z-score": -0.22808577638091165,
        "water-fast-z-score": 7.580980435789034,
        "rewrite-fast-z-score": 2.5
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Evolution of Dark Matter Halo Properties in Clusters, Filaments, Sheets and Voids .\nAbstract:\nWe present the results of cosmological hydrodynamic simulations that follow the evolution of dark matter haloes within different cosmic environments (clusters, filaments, sheets and voids). We find that:\n(i) The mass accretion histories of clusters are dominated by major mergers with other massive systems at high redshifts z > 1.\n(ii) In contrast to clusters, most of the growth of filamentary structures is driven by smooth gas accretion along their length.  This leads to an extended formation history for these objects which can be traced back to early times z < 5. (iii) Sheet-like structures form through the merger of smaller filaments into larger ones. They grow mainly via smooth gas accretion but also experience minor mergers with small groups or galaxies during their lifetime. (iv) Voids evolve almost exclusively due to smooth gas accretion. Their assembly time-scales are typically longer than those of clusters and filaments because they have less dense surroundings.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Evolution of Dark Matter Halo Properties in Clusters , Filaments , Sheets and Voids . Abstract : We present the results of cosmological hydrodynamic simulations that pursue the evolution of bright matter haloes within various cosmic environments ( complexes , filaments , sheets and voids ) .We see that : ( i ) The mass accretion histories of clusters are dominated by major mergers with other large systems at high redshifts z > 1 . ( ii ) In comparison to clusters , most of the development of filamentary structures is caused by smooth gas accretion along their duration .This leads to an extended structure life for these objects which can be traced back to early years z < 5 . ( iii ) Sheet - like structures develop through the merger of tiny filaments into larger ones .They develop principally via smooth gas accretion but also experience minor mergers with little groups or galaxies during their lifetime . ( iv ) Voids evolve virtually primarily due to smooth gas accretion .Their assembly time - scales are typically longer than those of clusters and filaments because they have less dense surroundings .",
        "rewrite_text": "We present findings from cosmological hydrodynamic simulations that investigate the evolution of dark matter halo properties across various cosmic environments, including clusters, filaments, sheets, and voids. Our analysis reveals several key insights into the dynamics of these structures. Firstly, we observe that the mass accretion histories of galaxy clusters are significantly influenced by major mergers with other large systems, particularly at high redshifts (z > 1). This indicates that the formation and growth of clusters are heavily reliant on these substantial interactions during the early universe. \n\nIn contrast, the evolution of filamentary structures is primarily driven by smooth gas accretion over time. This process contributes to the longevity of filaments, allowing them to maintain their structural integrity and trace their origins back to epochs as early as z < 5. Furthermore, we find that sheet-like structures emerge through the merging of smaller filaments into larger configurations. While these sheets predominantly grow through smooth gas accretion, they also undergo minor mergers with smaller groups or galaxies throughout their evolution.\n\nLastly, our study highlights that voids evolve mainly due to smooth gas accretion as well. However, the assembly timescales for voids are generally longer than those for clusters and filaments, a consequence of their less dense environments. These findings enhance our understanding of the complex interplay between dark matter and baryonic matter in shaping the large-scale structure of the universe, providing valuable insights into the processes that govern the formation and evolution of cosmic structures.",
        "ori-fast-z-score": -0.5698028822981898,
        "water-fast-z-score": 3.487772492870674,
        "rewrite-fast-z-score": 0.9205746178983234
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A modified cable formalism for modeling neuronal membranes at high frequencies .\nAbstract:\nWe present an extension to the cable model that allows one to account for membrane dynamics in neurons with active conductances over a wide range of time scales and spatial dimensions, including those relevant to subthreshold electrogenesis. The proposed approach is based on a reformulation of the cable equation as a system of first-order differential equations describing voltage changes along the axon or dendrite. This formulation leads naturally to a generalization of the concept of space constants to include both passive and active components of the membrane impedance. We show how this generalized cable formalism can be used to describe propagation of action potentials through a single compartment neuron with Hodgkin-Huxley type currents. Finally we demonstrate its utility by applying it to study spatiotemporal patterns of activity in networks of coupled neurons. Neurons are highly specialized cells capable of generating electrical signals known as action potentials (APs). These APs propagate down the length of the cell s axon toward synaptic terminals where they trigger release of neurotransmitters into the synapse. In turn these transmitters bind to receptors located on the postsynaptic side of the synapse initiating signaling cascades which ultimately lead to generation of new APs. Thus information transfer between neurons occurs via propagating APs across chemical synapses.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A modified cable formalism for modeling neuronal membranes at high frequencies . Abstract : We present an addition to the cable theory that enables one to explain for membrane dynamics in cells with active conductances over a broad variety of time scales and spatial dimensions , particularly those applicable to subthreshold electrogenesis .The proposed approach is based on a reformulation of the cable formula as a system of first - order differential coefficients relating voltage changes along the axon or dendrite . This formulation leads naturally to a generalization of the idea of space constants to consider both passive and active components of the membrane impedance .We see how this generalized cable formalism can be used to explain propagation of action potentials through a single compartment neuron with Hodgkin - Huxley type currents . Finally we prove its utility by using it to study spatiotemporal patterns of action in networks of coupled neurons .Neurons are extremely specialized cells responsible of transmitting electrical messages termed as action potentials ( APs ) . These APs propagate down the length of the cell s axon toward synaptic terminals where they stimulate release of neurotransmitters into the synapse .In turn these transmitters attach to receptors located on the postsynaptic side of the synapse initiating activation cascades which ultimately contribute to development of new APs . Thus information transfer between neurons occurs via propagating APs across molecular synapses .",
        "rewrite_text": "We introduce an enhancement to the traditional cable theory that facilitates the understanding of membrane dynamics in neuronal cells exhibiting active conductances across a wide range of temporal scales and spatial dimensions, particularly relevant to subthreshold electrogenesis. This innovative approach reformulates the cable equation into a system of first-order differential equations that describe voltage variations along axons or dendrites. This new formulation allows for a comprehensive generalization of the concept of space constants, incorporating both passive and active elements of membrane impedance. \n\nOur modified cable formalism effectively elucidates the propagation of action potentials (APs) in single compartment neurons characterized by Hodgkin-Huxley type currents. By applying this framework, we demonstrate its practical utility in analyzing spatiotemporal patterns of neuronal activity within networks of interconnected neurons. Neurons, as highly specialized cells, are tasked with transmitting electrical signals known as action potentials. These APs travel along the axon's length towards synaptic terminals, where they trigger the release of neurotransmitters into the synaptic cleft. Subsequently, these neurotransmitters bind to receptors on the postsynaptic membrane, initiating activation cascades that ultimately lead to the generation of new action potentials.\n\nConsequently, the transfer of information between neurons is facilitated through the propagation of action potentials across molecular synapses. Our work not only advances the theoretical understanding of neuronal membrane dynamics but also provides a robust framework for investigating complex neuronal interactions and their implications for neural communication and processing. This study opens new avenues for exploring the intricate mechanisms underlying neuronal behavior and their contributions to overall brain function.",
        "ori-fast-z-score": -1.2809280616135812,
        "water-fast-z-score": 4.706787243316417,
        "rewrite-fast-z-score": 0.6311687442672026
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Can dark matter be a Bose-Einstein condensate? .\nAbstract:\nWe consider the possibility that dark matter is made up of bosonic particles, which can condense into a superfluid state at low temperatures. We show how this scenario could explain several puzzling observations in astrophysics and cosmology. In particular we argue that: (i) The observed flat rotation curves of spiral galaxies are explained by the presence of a halo of cold dark matter surrounding each galaxy. (ii) The formation of large-scale structures such as clusters of galaxies proceeds through gravitational collapse of overdensities in the primordial density field seeded by quantum fluctuations during inflation. (iii) Dark energy may arise naturally if the universe contains a large number of weakly interacting massive particles with masses around $10^{22}$ GeV. This article is part of a series on Quantum Matter. For more information see http://arxiv.org/abs/quant-ph/0604070 . \nIntroduction:  Many theories beyond the Standard Model predict new types of elementary particles whose existence has yet to be confirmed experimentally. One particularly interesting class of models involves so-called WIMPZILLAs  1  , i.e., stable relic particles with masses around $10^9$ GeV or higher  2  . These particles would have been produced thermally in the early Universe but their abundance today should still be determined by their annihilation cross section  3  .\nIn this Letter we propose an alternative explanation for the origin of dark matter based on the idea that it consists of self-gravitating bosons  4  . Boson stars  5  are gravitationally bound states of scalar fields  6  predicted by many extensions of the Standard Model  7, 8  . They were first studied in the context of supersymmetric grand unified theories  9  where they play the role of solitonic solutions  10  . More recently, boson stars have also been considered within the framework of string theory  11  . If these objects exist then they will form a population of compact remnants  12  that might constitute all or some fraction of the dark matter  13  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  Can dark matter be a Bose-Einstein condensate?.Abstract : We consider the prospect that dark matter is made up of bosonic particles , which can condense into a superfluid state at low temperatures . We see how this situation could explain several puzzling discoveries in astrophysics and cosmology .In particular we claim that : ( i ) The observed flat rotation curves of spiral galaxies are explained by the presence of a halo of cold dark matter surrounding each galaxy . ( ii ) The formation of large - scale structures such as clusters of stars proceeds through gravity collapse of overdensities in the primordial density field seeded by quantum fluctuations during inflation .( iii ) Dark energy may arise naturally if the universe consists a large number of mildly interacting massive particles with masses around $ 10 ^ { 22 } $ GeV . This page is part of a trilogy on Quantum Matter .For more information see www : / / arxiv . org / abs / quant - ph / 0604070 . Introduction : Many theories beyond the Standard Model predict new types of primary objects whose existence has yet to be verified experimentally .One especially interesting class of models involves so - called WIMPZILLAs 1 , i . e . , stable relic objects with masses around $ 10 ^ 9 $ GeV or greater 2 . These particles might have been created thermally in the early Universe but their density today should still be determined by their annihilation cross section 3 .In this Letter we propose an additional argument for the origin of dark matter based on the idea that it consists of self - gravitating bosons 4 . Boson galaxies 5 are gravitationally bound states of scalar fields 6 expected by many extensions of the Standard Model 7 , 8 .They were first explored in the context of supersymmetric grand unified fields 9 where they hold the part of solitonic answers 10 . More recently , boson stars have also been discussed within the framework of string theory 11 .If these objects exist then they will form a population of compact remnants 12 that might constitute all or some fraction of the dark matter 13 .",
        "rewrite_text": "**Title:** Can Dark Matter Be a Bose-Einstein Condensate?\n\n**Abstract:** This article explores the intriguing hypothesis that dark matter may consist of bosonic particles capable of forming a Bose-Einstein condensate, particularly in superfluid states at low temperatures. We investigate how this theoretical framework could provide explanations for several enigmatic phenomena observed in astrophysics and cosmology. Specifically, we propose that: (i) the flat rotation curves of spiral galaxies can be accounted for by the existence of a halo of cold dark matter enveloping each galaxy; (ii) the development of large-scale structures, such as star clusters, is facilitated by gravitational collapse of density fluctuations in the primordial density field, which are influenced by quantum fluctuations that occurred during the inflationary period of the universe; and (iii) dark energy may emerge naturally from a universe populated by a substantial number of mildly interacting massive particles, each with masses on the order of \\(10^{22}\\) GeV. This paper is part of a trilogy focused on Quantum Matter, and further details can be found at www.arxiv.org/abs/quant-ph/0604070. \n\n**Introduction:** Numerous theories that extend beyond the Standard Model of particle physics predict the existence of new fundamental objects, which have yet to be experimentally validated. Among these, a particularly compelling category includes WIMPZILLAs—stable relic particles with masses around \\(10^9\\) GeV or higher. These particles may have originated thermally in the early universe, yet their present-day density is likely influenced by their annihilation cross-section. In this letter, we introduce a novel perspective on the nature of dark matter, suggesting that it may be composed of self-gravitating bosons. Boson galaxies, which are gravitationally bound states of scalar fields, are anticipated by various extensions of the Standard Model. Initially examined in the context of supersymmetric grand unified theories, these structures have also been considered within string theory frameworks. Should these bosonic entities exist, they could represent a significant portion, if not all, of the dark matter in the universe.",
        "ori-fast-z-score": 1.6222142113076254,
        "water-fast-z-score": 7.137742529753552,
        "rewrite-fast-z-score": 2.301585822275002
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Correlations and sum rules in a half-space for a quantum two-dimensional one-component plasma .\nAbstract:\nWe study the correlations and sum rules in a semi-infinite system with impurities at its surface, which is described by the quantum two-dimensional (2D) one component plasma model. We use the exact diagonalization method to calculate the density-density correlation function and static structure factor as well as their corresponding sum rules. The results show that there are two different regimes depending on whether the temperature T is larger or smaller than the Fermi energy EF . In particular, we find that when T < EF , the behavior of these quantities can be understood within the framework of Landau s Fermi liquid theory. However, if T > EF , our numerical data deviate significantly from this picture. Finally, we also investigate how the presence of impurities affects the above mentioned physical properties. Our findings suggest that the effect of impurities depends strongly on the distance between them. If they are close enough, then the impurity-impurity interaction dominates over other interactions leading to an increase of the effective mass of particles near the surface.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Correlations and sum rules in a half - space for a quantum two - dimensional one - component plasma . Abstract : We explore the correlations and sum rules in a semi - infinite system with impurities at its surface , which is characterized by the quantum two - dimensional ( 2D ) one element plasma model .We use the exact diagonalization technique to estimate the density - density correlation function and static structure parameter as also as their corresponding sum rules . The results show that there are two different regimes depending on whether the temperature T is bigger or smaller than the Fermi energy EF .In particular , we find that when T < EF , the dynamics of these quantities can be understood within the framework of Landau s Fermi liquid theory . However , if T > EF , our numerical information deviate substantially from this picture .Finally , we also investigate how the presence of impurities impacts the above mentioned physical properties . Our findings show that the impact of impurities relies highly on the distance between them .If they are close enough , then the impurity - impurity interaction dominates over other interactions leading to an increase of the effective mass of molecules near the surface .",
        "rewrite_text": "In this study, we investigate the correlations and sum rules within a semi-infinite system containing surface impurities, utilizing the quantum two-dimensional (2D) one-component plasma model as our framework. Employing the exact diagonalization technique, we calculate the density-density correlation function and the static structure factor, along with their associated sum rules. Our analysis reveals the existence of two distinct regimes based on the relationship between temperature (T) and the Fermi energy (EF). Specifically, when the temperature is lower than the Fermi energy (T < EF), the behavior of these physical quantities aligns with the principles of Landau's Fermi liquid theory. Conversely, when the temperature exceeds the Fermi energy (T > EF), our numerical results diverge significantly from this theoretical model. Additionally, we examine the influence of impurities on the aforementioned physical properties. Our results indicate that the effect of impurities is highly dependent on their spatial separation. When impurities are in close proximity, their interactions dominate over other forces, resulting in an increase in the effective mass of the particles located near the surface. This research contributes to a deeper understanding of quantum plasmas in the presence of impurities and highlights the critical role of temperature and impurity interactions in determining the system's physical characteristics.",
        "ori-fast-z-score": 2.465858830126928,
        "water-fast-z-score": 6.041987916036252,
        "rewrite-fast-z-score": 1.3337718577107005
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  An X-ray Survey in SA 57 with XMM-Newton .\nAbstract:\nWe present the results of an observation performed by XMM-Newton on the galaxy cluster Abell S0740 (SA57). The data were taken between December 2004 and January 2005 for a total exposure time of about 100 ks, split into two pointings separated by 1 arcmin. We detect more than 50 sources within the field-of-view of our observations. Most of them are associated to galaxies at different redshifts; we also find several active galactic nuclei (AGN) as well as one background quasar. In order to study their properties, we have extracted spectra for all detected sources using circular regions centered on each source position. For most of these objects, we could fit single power-law models or thermal plasma emission models. From this analysis, we derive luminosities ranging from 1042 erg s-1 up to 1044 erg s-1 . Using the observed fluxes and assuming that they follow a standard candle model, we estimate the number density distribution of clusters per unit volume as a function of redshift. This allows us to calculate the expected number of clusters above a given mass limit as a function of redshift and compare it with the predictions obtained from numerical simulations.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : An X - ray Survey in SA 57 with XMM - Newton . Abstract : We report the results of an observation performed by XMM - Newton on the galaxy cluster Abell S0740 ( SA57 ) .The data were took between December 2004 and January 2005 for a total sensitivity time of about 100 ks , separated into two pointings separated by 1 arcmin . We detect more than 50 sources within the field - of - view of our observations .Most of them are related to galaxies at different redshifts ; we also find several active galactic nuclei ( AGN ) as well as one background quasar . In order to study their characteristics , we have gathered spectra for all detected sources using circular regions centered on each source place .For most of these objects , we may fit single power - law models or thermal plasma radiation estimates . From this analysis , we derive luminosities ranging from 1042 erg s - 1 up to 1044 erg s - 1 .Using the seen fluxes and assuming that they follow a traditional candle model , we estimate the number density spread of clusters per unit volume as a function of redshift . This enables us to estimate the expected number of clusters above a given mass limit as a function of redshift and compare it with the estimates obtained from numerical simulations .",
        "rewrite_text": "We present the findings from an X-ray survey conducted with the XMM-Newton satellite on the galaxy cluster Abell S0740 (SA57). The observations were carried out between December 2004 and January 2005, accumulating a total exposure time of approximately 100 kiloseconds, divided into two separate pointings spaced 1 arcminute apart. Our analysis revealed over 50 X-ray sources within the observed field, predominantly associated with galaxies at varying redshifts. Among these sources, we identified several active galactic nuclei (AGN) and a background quasar. To investigate the properties of these sources, we extracted spectra using circular regions centered on each detected object. For the majority of the sources, we applied single power-law models or thermal plasma radiation models to fit the spectral data. This fitting process allowed us to derive luminosities that range from 10^42 erg s^-1 to 10^44 erg s^-1. By analyzing the observed fluxes and employing a standard candle model, we estimated the number density of clusters per unit volume as a function of redshift. This estimation facilitates the prediction of the expected number of clusters exceeding a specified mass threshold across different redshifts, enabling a comparison with predictions derived from numerical simulations. Our results contribute to a deeper understanding of the X-ray properties of galaxy clusters and their evolution over cosmic time, providing valuable insights for future studies in astrophysics and cosmology.",
        "ori-fast-z-score": 0.21081851067789195,
        "water-fast-z-score": 4.666282626286914,
        "rewrite-fast-z-score": 0.19425717247145283
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Instabilities in the time-dependent neutrino disc in Gamma-Ray Bursts .\nAbstract:\nWe study instabilities that develop in the accretion flow onto black holes during gamma-ray bursts (GRBs). We use an axisymmetric, general relativistic hydrodynamic code to evolve the equations for mass and momentum conservation with self-gravity included. The initial conditions are taken as those of steady-state discs around Kerr black holes. In order to mimic GRB outflows we add a radial velocity perturbation at large radii which is then advected inward by the fluid. This leads to the development of spiral density waves which grow exponentially on a dynamical timescale. These waves can be identified with the Rossby wave instability (RWI) predicted analytically by Lovelace et al. (1999) . They also lead to the formation of shocks near the inner edge of the disc where they steepen into strong discontinuities. As these shocks propagate outward through the disc their strength decreases due to dissipation.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Instabilities in the period - dependent neutrino disc in Gamma - Ray Bursts . Abstract : We research instabilities that develop in the accretion flow onto black holes during gamma - ray bursts ( GRBs ) .We use an axisymmetric , general relativistic hydrodynamic program to evolve the equations for mass and momentum conservation with self - gravity included . The initial conditions are took as those of stable - state discs around Kerr black holes .In order to mimic GRB outflows we create a radial speed perturbation at large radii which is then advected inward by the liquid . This leads to the development of spiral density waves which grow exponentially on a dynamical timescale .These waves can be identified with the Rossby wave disturbance ( RWI ) anticipated analytically by Lovelace et al . ( 1999 ) .They also lead to the formation of shocks near the inner perimeter of the disc where they steepen into deep discontinuities . As these shocks propagate outward through the disc their intensity reduces owing to dissipation .",
        "rewrite_text": "In this study, we investigate the instabilities that arise in the accretion flow onto black holes during gamma-ray bursts (GRBs). Utilizing an axisymmetric, general relativistic hydrodynamic simulation, we evolve the equations governing mass and momentum conservation while accounting for self-gravity. Our initial conditions are based on stable-state discs surrounding Kerr black holes. To simulate the outflows associated with GRBs, we introduce a radial velocity perturbation at large distances, which is subsequently advected inward by the fluid dynamics of the disc. This perturbation initiates the formation of spiral density waves that exhibit exponential growth on a dynamical timescale. These waves can be correlated with the Rossby wave instability (RWI) predicted analytically by Lovelace et al. (1999). Furthermore, the development of these waves leads to the emergence of shocks near the inner edge of the disc, where they steepen into significant discontinuities. As these shocks propagate outward through the disc, their intensity diminishes due to dissipative processes. Our findings contribute to a deeper understanding of the complex dynamics at play during GRBs and highlight the critical role of instabilities in the behavior of accretion discs in extreme astrophysical environments. This research not only enhances our comprehension of GRB phenomena but also provides insights into the fundamental processes governing black hole accretion in general.",
        "ori-fast-z-score": -0.3611575592573076,
        "water-fast-z-score": 3.1075943842694236,
        "rewrite-fast-z-score": 2.620712091804796
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  XMM-Newton X-ray Observations of the Wolf-Rayet Binary System WR 147 .\nAbstract:\nWe present new XMM-Newton observations of the Wolf Rayet binary system WR147, which is one of only two known systems with an O-type companion star and a WN6-7h primary component (the other being WR20a). The data were obtained in December 2004 during revolution number 1253-1255 using all three EPIC cameras on board XMM-Newton. We have analyzed these data to search for periodicities in both light curves as well as line profiles. No significant periodicity was found in either the light curve or line profile analysis. However we find that there are differences between the line profiles observed by Chandra and those seen here. These differences may be due to orbital motion within this highly eccentric system. This work has been supported by NASA grant NAG5-10842. Keywords: Wolf-Rayet stars; binaries; X-rays; XMM-Newton",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : XMM - Newton X - ray Observations of the Wolf - Rayet Binary System WR 147 . Abstract : We report new XMM - Newton discoveries of the Wolf Rayet binary system WR147 , which is one of only two discovered systems with an O - class companion star and a WN6 - 7h primary component ( the other being WR20a ) .The data were obtained in December 2004 during revolution number 1253 - 1255 utilizing all three EPIC sensors on board XMM - Newton . We have analyzed these information to search for periodicities in both light curves as well as line profiles .No considerable periodicity was seen in either the light curve or line profile analysis . However we find that there are variations between the line profiles observed by Chandra and those viewed here .These changes may be due to orbital movement within this strongly eccentric system . This research has been supported by NASA grant NAG5 - 10842 .Keywords : Wolf - Rayet stars ; binaries ; X - rays ; XMM - Newton",
        "rewrite_text": "**Title:** XMM-Newton X-ray Observations of the Wolf-Rayet Binary System WR 147\n\n**Abstract:** In this study, we present new findings from XMM-Newton observations of the Wolf-Rayet binary system WR 147, notable for being one of only two known systems featuring an O-class companion star alongside a WN6-7h primary component, the other being WR20a. The observational data were collected in December 2004 during the satellite's revolutions 1253 to 1255, utilizing all three EPIC sensors onboard XMM-Newton. Our analysis focused on identifying periodicities within the light curves and line profiles of the system. Despite thorough examination, we did not detect any significant periodicity in either the light curves or the line profile analyses. However, we observed notable discrepancies between the line profiles recorded by Chandra and those obtained in our study. These variations may be attributed to the orbital dynamics of this highly eccentric binary system. The implications of these findings contribute to our understanding of the complex interactions within Wolf-Rayet binaries and their evolutionary processes. This research was conducted with the support of NASA grant NAG5-10842. \n\n**Keywords:** Wolf-Rayet stars; binaries; X-rays; XMM-Newton.",
        "ori-fast-z-score": -0.254000254000381,
        "water-fast-z-score": 4.157609203101499,
        "rewrite-fast-z-score": 0.47809144373375745
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Clustering of Lyman alpha emitters at z ~ 4.5 .\nAbstract:\nWe present the results of an optical survey for high redshift galaxies in the field surrounding the radio galaxy PKS 1138-262 (z = 3.9). We have detected over 100 candidate Lyman-alpha emitting galaxies with redshifts between 2 and 5, including several new spectroscopically confirmed members of this cluster. The spatial distribution of these objects is consistent with that expected if they are located within a single dark matter halo centered on the radio source. This result suggests that clusters may be identified by their diffuse emission as well as individual member galaxies. In addition to confirming the existence of a massive cluster around PKS 1138-262 we find evidence for two other overdensities of Lyman-alpha emitting sources near the line-of-sight to the radio source. These structures could represent additional clusters or proto-clusters which will evolve into richer systems like those found today. Finally, our data suggest that there exists a large population of faint Lyman-alpha emitting objects whose properties are similar to those observed locally but whose number density increases rapidly towards higher redshifts.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Clustering of Lyman alpha emitters at z ~ 4 . 5 . Abstract : We present the conclusion of an optical survey for high redshift galaxies in the field surrounding the radio star PKS 1138 - 262 ( z = 3 . 9 ) .We have discovered over 100 candidate Lyman - alpha emitting galaxies with redshifts between 2 and 5 , including several new spectroscopically confirmed members of this cluster . The spatial distribution of these objects is compatible with that expected if they are situated within a single black material halo focused on the radio source .This result suggests that clusters might be identified by their diffuse emission as well as individual member galaxies . In addition to proving the existence of a huge cluster around PKS 1138 - 262 we find proof for two other overdensities of Lyman - alpha emitting sources near the line - of - view to the radio source .These structures could indicate additional complexes or proto - clusters which will evolve into richer complexes like those observed nowadays . Finally , our statistics indicate that there exists a large colony of distant Lyman - alpha emitting objects whose characteristics are comparable to those observed locally but whose number density increases quickly towards higher redshifts .",
        "rewrite_text": "We present the findings of an extensive optical survey targeting high-redshift galaxies in the vicinity of the radio source PKS 1138-262, which has a redshift of 3.9. Our survey has successfully identified over 100 candidate Lyman-alpha emitting galaxies with redshifts ranging from 2 to 5, including several newly confirmed spectroscopic members of this cluster. The spatial arrangement of these galaxies aligns with theoretical expectations, suggesting that they reside within a singular dark matter halo centered around the radio source. This observation implies that clusters of galaxies may be detected not only through their individual members but also via their diffuse emission. In addition to confirming the presence of a substantial cluster surrounding PKS 1138-262, we have also identified two additional regions of Lyman-alpha emitting sources that exhibit significant overdensities along the line of sight to the radio source. These regions may represent further complexes or proto-clusters that are likely to evolve into more developed structures similar to those observed in the present epoch. Our statistical analysis indicates the existence of a vast population of distant Lyman-alpha emitters, whose properties are analogous to those found in the local universe, yet their number density appears to increase sharply at higher redshifts. This research not only enhances our understanding of the clustering behavior of high-redshift galaxies but also contributes to the broader knowledge of cosmic structure formation in the early universe.",
        "ori-fast-z-score": -1.0425720702853738,
        "water-fast-z-score": 4.170288281141495,
        "rewrite-fast-z-score": -0.19069251784911848
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Capillary ordering and layering transitions in two-dimensional hard-rod fluids .\nAbstract:\nWe study the phase behavior of a system of N identical hard rods confined to a square box with periodic boundary conditions, using Monte Carlo simulations at constant pressure P . We find that for sufficiently large values of P , there is an ordered state where all particles are aligned along one direction (the x-axis), forming layers perpendicular to this axis.  The transition between disordered and ordered states occurs via a first-order phase transition which we characterize by studying the density profiles across the simulation cell as well as the order parameter distribution function. For small values of P , however, no such ordered state exists. Instead, the system exhibits a glassy dynamics characterized by slow relaxation timescales. Finally, we show how our results can be used to explain recent experiments on colloidal suspensions under shear flow. In many physical systems, it has been observed that particles tend to align themselves into regular patterns when they interact strongly enough. This phenomenon is known as capillarity  1  or self-assembly  2  .\nIn particular, in two dimensions, particles may form stripes  3  -  5  , squares  6  , hexagons  7  , or even more complex structures  8  depending on their shape  9  , size  10  , interactions  11  , and external fields  12  . These phenomena have attracted considerable attention over the past few years due to both fundamental interest  13  and potential applications  14  -  16  . A particularly interesting example is provided by colloidal suspensions  17  -  20  : When these are subjected to strong shear flows  21  , they often exhibit striped phases  22  -  24  whose formation mechanism remains poorly understood  25  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Capillary ordering and layering transitions in two - dimensional tough - rod liquid . Abstract : We explore the phase response of a system of N identical hard rods confined to a square box with periodic border conditions , using Monte Carlo simulations at constant pressure P .We see that for enough large values of P , there is an ordered state where all particles are aligned along one direction ( the x - axis ) , forming sheets perpendicular to this axis . The switch between disordered and ordered states happens via a first - order phase shift which we characterize by examining the density profiles across the model cell as well as the order parameter distribution function .For small values of P , however , no such ordered state exists . Instead , the system displays a glassy dynamics defined by slow relaxation timescales .Finally , we explain how our findings can be used to explain latest studies on colloidal suspensions under shear flow . In many mechanical systems , it has been observed that particles tend to align themselves into normal patterns when they interact strongly sufficient .This phenomenon is known as capillarity 1 or self - assembly 2 . In particular , in two dimensions , particles may form colors 3 - 5 , circles 6 , hexagons 7 , or especially more complex shapes 8 depending on their shape 9 , size 10 , interactions 11 , and external fields 12 .These phenomena have garnered considerable scrutiny over the previous few years due to both basic concern 13 and possible applications 14 - 16 . A notably important example is provided by colloidal suspensions 17 - 20 : When these are subjected to powerful shear flows 21 , they frequently exhibit striped cycles 22 - 24 whose formation system stays badly explained 25 .",
        "rewrite_text": "**Title:** Capillary Ordering and Layering Transitions in Two-Dimensional Tough-Rod Liquids\n\n**Abstract:** This study investigates the phase behavior of a system comprising N identical hard rods confined within a square box under periodic boundary conditions, utilizing Monte Carlo simulations at a constant pressure (P). Our findings reveal that at sufficiently high pressures, the system transitions into an ordered state where all rods align along a single direction (the x-axis), resulting in the formation of sheets that are perpendicular to this axis. The transition from a disordered to an ordered state is characterized as a first-order phase transition, which we analyze through the examination of density profiles within the simulation cell and the distribution of the order parameter. Conversely, at lower pressures, the system does not exhibit any ordered phase; instead, it demonstrates glassy dynamics characterized by prolonged relaxation times. Furthermore, we discuss the implications of our results in relation to recent studies on colloidal suspensions subjected to shear flow. It has been observed in various mechanical systems that particles tend to align into regular patterns when they experience strong interactions, a phenomenon referred to as capillarity or self-assembly. In two-dimensional systems, the alignment can lead to the formation of diverse structures, including colors, circles, hexagons, and more intricate shapes, influenced by factors such as particle shape, size, interaction strengths, and external fields. These phenomena have attracted significant attention in recent years due to their fundamental importance and potential applications. A particularly noteworthy instance is found in colloidal suspensions, which, when exposed to intense shear flows, often display striped patterns whose underlying formation mechanisms remain poorly understood. This research contributes to the broader understanding of ordering phenomena in complex fluids and their relevance to practical applications in material science and engineering.",
        "ori-fast-z-score": -2.197401062294143,
        "water-fast-z-score": 5.37076069309881,
        "rewrite-fast-z-score": 0.48349377841522817
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Two-loop world-sheet corrections in AdS_5 x S^5 superstring .\nAbstract:\nWe calculate the two-loop beta function for the coupling constant of the AdS5xS5 superstring theory and show that it is proportional to the one-loop result, which implies that there are no non-trivial fixed points at any finite value of the string coupling constant.  We also find that the dilaton field has an imaginary part when we take into account the higher-order terms beyond the leading order approximation. This indicates that our results may be valid only within some limited region of the parameter space where the imaginary part of the dilaton can be neglected. The present work was motivated by the recent study on the gauge/gravity correspondence between N=4 super Yang-Mills (SYM) theories with 16 supercharges and type IIB strings on AdS5xS5G5 backgrounds  1  . In this context, the existence of nontrivial fixed points would correspond to the conformal invariance of the dual SYM theories  2  , while the imaginary part of the dilatonic scalar fields would indicate the instability of the corresponding solutions  3  .\nIn Ref.  4  , the authors have calculated the one-loop beta functions for both the metric tensor and the dilaton field using the Green-Schwarz formalism  5  . They found that these beta functions do not vanish even if they are evaluated at vanishing values of the string coupling constants. However, their calculations were performed under the assumption that all the fermionic contributions vanish identically  6  . It turns out that such an assumption does not hold true  7, 8  . Therefore, it seems necessary to perform more detailed analysis taking into account the effects due to the fermions as well as those coming from the bosons.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Two - loop universe - sheet corrections in AdS _ 5 x S ^ 5 superstring . Abstract : We calculate the two - loop beta function for the coupling constant of the AdS5xS5 superstring theory and find that it is proportional to the one - loop result , which implies that there are no non - trivial fixed points at any finite value of the string coupling constant .We additionally find that the dilaton field has an imaginary part when we took into consideration the higher - order terms beyond the led order approximation . This implies that our findings may be valid only within some restricted region of the parameter space where the imaginary part of the dilaton can be forgotten .The present work was motivated by the recent study on the gauge / gravity correspondence between N = 4 super Yang - Mills ( SYM ) theories with 16 supercharges and class IIB sequences on AdS5xS5G5 backgrounds 1 . In this sense , the existence of nontrivial fixed points would correspond to the conformal invariance of the dual SYM theories 2 , while the imaginary part of the dilatonic scalar fields might suggest the instability of the associated solutions 3 .In Ref . 4 , the authors have calculated the one - loop beta functions for both the metric tensor and the dilaton field using the Green - Schwarz formalism 5 .They found that these beta distributions do not vanish even if they are tested at vanishing values of the string coupling constants . However , their analyses were performed under the assumption that all the fermionic contributions vanish identically 6 .It turns out that such an assumption does not stand true 7 , 8 . Therefore , it appears necessary to conduct more precise analysis taking into consideration the effects due to the fermions as also as those coming from the bosons .",
        "rewrite_text": "**Title:** Two-loop Universe-Sheet Corrections in AdS_5 x S^5 Superstring\n\n**Abstract:** In this study, we investigate the two-loop beta function associated with the coupling constant in the AdS_5 x S^5 superstring theory. Our calculations reveal that this beta function is directly proportional to the one-loop result, indicating the absence of non-trivial fixed points at any finite value of the string coupling constant. Furthermore, we observe that the dilaton field exhibits an imaginary component when higher-order terms beyond the leading order approximation are considered. This finding suggests that our results may only be applicable within a limited region of the parameter space, where the imaginary part of the dilaton can be neglected. \n\nThe motivation for this research stems from recent investigations into the gauge/gravity correspondence involving N = 4 super Yang-Mills (SYM) theories with 16 supercharges and class IIB sequences on AdS_5 x S^5 backgrounds. In this context, the presence of non-trivial fixed points would be indicative of the conformal invariance of the corresponding dual SYM theories. Conversely, the imaginary component of the dilatonic scalar fields may point to potential instabilities within the associated solutions. \n\nPrevious work has calculated the one-loop beta functions for both the metric tensor and the dilaton field using the Green-Schwarz formalism. These analyses revealed that the beta functions do not vanish, even when evaluated at zero values of the string coupling constants. However, it is important to note that these calculations were conducted under the assumption that all fermionic contributions are negligible, a premise that has been shown to be inaccurate. Consequently, it is imperative to perform a more thorough analysis that incorporates the effects of both fermionic and bosonic contributions to obtain a comprehensive understanding of the dynamics at play in this superstring framework.",
        "ori-fast-z-score": -0.6311687442672026,
        "water-fast-z-score": 4.418181209870418,
        "rewrite-fast-z-score": 1.3310347641241707
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Quiet Sun magnetic fields from space-borne observations: simulating Hinode s case .\nAbstract:\nWe present the results of our analysis on quiet sun magnetograms obtained by Hinode/SOT/SP and SDO/HMI instruments, which are compared with those simulated using an MHD model. The observed data were processed to remove instrumental effects such as stray light contamination and spatial smearing due to finite pixel size. We found that the simulated magnetograms reproduce well the main features in the observed ones; however there is still room for improvement especially regarding the fine structures seen in the observed magnetograms. \n \n Keywords: Solar activity, Magnetohydrodynamics (MHD), Quiet Sun, Space weathering, Solar atmosphere, Magnetic field strength, Photospheric plasma flows, Solar surface convection, Solar dynamo, Solar irradiance variability, Solar cycle, Solar corona, Solar flares, Solar prominences, Solar winds, Solar energetics, Solar physics, Solar interior, Solar rotation",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Quiet Sun magnetic fields from space - borne observations : simulating Hinode s case . Abstract : We present the conclusion of our analysis on calm sun magnetograms collected by Hinode / SOT / SP and SDO / HMI instruments , which are compared with those modeled using an MHD model .The observed measurements were processed to remove instrumental impacts such as stray light degradation and spatial smearing due to finite pixel size . We showed that the simulated magnetograms reproduce well the main features in the seen ones ; however there is already room for advancement especially regarding the fine structures visible in the reported magnetograms .Keywords : Solar activity , Magnetohydrodynamics ( MHD ) , Quiet Sun , Space weathering , Solar atmosphere , Magnetic field intensity , Photospheric plasma flows , Solar surface convection , Solar dynamo , Solar irradiance variability , Solar cycle , Solar corona , Solar flares , Solar prominences , Solar storms , Solar energetics , Solar chemistry , Solar interior , Solar rotation",
        "rewrite_text": "Title: Magnetic Fields of the Quiet Sun from Space-Borne Observations: A Simulation of Hinode's Findings\n\nAbstract: In this study, we present the results of our comprehensive analysis of magnetograms depicting the Quiet Sun, obtained from the Hinode Solar Optical Telescope (SOT) and the Solar Dynamics Observatory (SDO) Helioseismic and Magnetic Imager (HMI). Our investigation involved a detailed comparison between the observed magnetograms and those generated through a magnetohydrodynamics (MHD) model. To ensure the accuracy of our analysis, we meticulously processed the observational data to mitigate the effects of instrumental artifacts, including stray light degradation and spatial smearing caused by the finite pixel size of the instruments. Our findings indicate that the simulated magnetograms effectively replicate the primary characteristics observed in the actual data. However, we also identified areas for further improvement, particularly concerning the representation of fine structures that are evident in the reported magnetograms. This research contributes to our understanding of solar activity and the dynamics of the solar atmosphere, emphasizing the importance of accurate modeling in the study of magnetic fields and their implications for space weather phenomena. The keywords associated with this work include Solar activity, Magnetohydrodynamics (MHD), Quiet Sun, Space weathering, Solar atmosphere, Magnetic field intensity, Photospheric plasma flows, Solar surface convection, Solar dynamo, Solar irradiance variability, Solar cycle, Solar corona, Solar flares, Solar prominences, Solar storms, Solar energetics, Solar chemistry, Solar interior, and Solar rotation. Through this analysis, we aim to enhance our comprehension of the Quiet Sun's magnetic environment and its broader implications for solar physics and space weather forecasting.",
        "ori-fast-z-score": -0.47140452079103173,
        "water-fast-z-score": 3.719924439802217,
        "rewrite-fast-z-score": 3.0545207623093087
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dynamic response of Ag monolayers adsorbed on Au(100) upon an oscillatory variation of the chemical potential: A Monte Carlo simulation study .\nAbstract:\nWe have studied by means of molecular dynamics simulations how the dynamic behavior of silver atoms deposited on gold (100) is affected when the substrate temperature and the coverage are varied in such a way that the average number of nearest neighbors per atom changes between 1 to 4. We find that, for low coverages, the system behaves as if it were made up of isolated particles with no interaction among them. However, at higher coverages we observe collective effects which lead to the formation of ordered structures. The results obtained show that these structures can be classified into two different categories depending on whether they are formed by one or more layers of silver atoms. In particular, we found that the structure of the first layer depends strongly on the substrate temperature while the second layer shows only small variations. Finally, our calculations indicate that the third layer forms a disordered arrangement of silver atoms. This work was supported by DGESIC under project PB98-0443-C02-01",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dynamic response of Ag monolayers adsorbed on Au ( 100 ) upon an oscillatory change of the chemical potential : A Monte Carlo simulation study . Abstract : We have researched by means of molecular dynamics simulations how the dynamic behavior of silver atoms deposited on gold ( 100 ) is affected when the substrate temperature and the coverage are varied in such a way that the average number of nearest neighbors per atom shifts between 1 to 4 .We see that , for low coverages , the system behaves as if it were made up of isolated molecules with no interaction among them . However , at higher coverages we encounter collective effects which lead to the formation of ordered systems .The results collected show that these structures can be categorized into two different categories depending on whether they are created by one or more layers of silver atoms . In particular , we reported that the composition of the first layer relies highly on the substrate cold while the second layer exhibits only tiny variations .Finally , our calculations suggest that the third layer makes a disordered arrangement of silver atoms . This project was supported by DGESIC under project PB98 - 0443 - C02 - 01",
        "rewrite_text": "**Title:** Dynamic Response of Ag Monolayers Adsorbed on Au(100) During Oscillatory Chemical Potential Changes: A Monte Carlo Simulation Study\n\n**Abstract:** This study investigates the dynamic behavior of silver (Ag) atoms deposited on a gold (Au) (100) substrate through molecular dynamics simulations, focusing on how variations in substrate temperature and coverage influence atomic interactions. By manipulating these parameters, we observe a transition in the average number of nearest neighbors per Ag atom, ranging from 1 to 4. At low coverage levels, the Ag atoms behave as if they are isolated entities, exhibiting minimal interaction with one another. In contrast, as coverage increases, we identify significant collective phenomena that lead to the emergence of ordered structures within the monolayer. Our findings categorize these structures into two distinct types based on the number of Ag layers formed. Notably, the composition of the first Ag layer is highly dependent on the substrate temperature, while the second layer shows only slight variations in composition. Furthermore, our simulations indicate that the third layer of Ag atoms tends to adopt a disordered arrangement, lacking the order seen in the first two layers. This research enhances our understanding of the atomic-level dynamics of metal monolayers and their dependence on environmental conditions, providing insights that could be valuable for applications in nanotechnology and materials science. The project received funding from DGESIC under project PB98-0443-C02-01, which facilitated the computational resources and support necessary for this investigation.",
        "ori-fast-z-score": 1.0660035817780522,
        "water-fast-z-score": 5.617988764033708,
        "rewrite-fast-z-score": 1.436739427831727
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Modeling Accretion Disk X-ray Continuum of Black Hole Candidates .\nAbstract:\nWe present the results of our investigation into accretion disk continuum emission in black hole candidates (BHCs). We have developed an analytical model for calculating the spectrum emitted by a thin, optically thick accretion disk around a Schwarzschild black hole and applied it to several BHCs with known mass functions. The observed spectra are well reproduced when we assume that the inner edge of the disk is located at 6 gravitational radii. This result suggests that the standard thin disk model can be used as a good approximation for modeling the X-ray continuum emission of these objects. \n \n Keywords: Black holes -- Spectroscopy -- X-rays -- Modeling -- Accretion disks -- Emission lines -- Broad-band spectral energy distribution -- Luminosity function -- Mass measurement -- Stellar-mass black holes -- Supermassive black holes -- Active galactic nuclei -- Quasars -- Cosmic evolution \n \n \n \n 1 Introduction \n \n In recent years there has been considerable progress made towards understanding the physical processes occurring near supermassive black holes (SMBH) in active galactic nuclei (AGN), quasars, and other similar systems. These studies rely on observations of the broad-band spectral energy distributions (SEDs) of SMBHs over many decades in frequency space. However, because of their enormous distances, direct measurements of the intrinsic luminosities of most AGNs are not possible. Instead, one must use indirect methods such as reverberation mapping or statistical correlations between various properties of AGNs to determine their luminosities. For example, if one knows how much light passes through some region of interest within an AGN then one may calculate its luminosity using simple geometric arguments. Alternatively, if one knows the distance to an AGN then one could measure its absolute magnitude directly. Unfortunately, both of these approaches require detailed knowledge about the structure of the emitting regions which cannot currently be obtained observationally. Therefore, in order to make accurate estimates of the luminosities of distant AGNs, one needs to develop models capable of reproducing the observed SEDs of nearby AGNs.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Modeling Accretion Disk X - ray Continuum of Black Hole Candidates . Abstract : We present the conclusion of our inquiry into accretion disk continuum emission in black hole candidates ( BHCs ) .We have developed an analytical model for determining the spectrum emitted by a thin , optically dense accretion disk around a Schwarzschild red hole and applied it to several BHCs with reported mass parameters . The observed spectra are better displayed when we suppose that the inner corner of the disk is situated at 6 gravitational radii .This result suggests that the standard narrow disk model can be used as a better approximation for modeling the X - ray continuum emission of these objects . Keywords : Black holes - - Spectroscopy - - X - rays - - Modeling - - Accretion disks - - Emission lines - - Broad - band spectral power distribution - - Luminosity function - - Mass determination - - Stellar - mass black holes - - Supermassive black holes - - Active galactic nuclei - - Quasars - - Cosmic evolution 1 Introduction In recent years there has been substantial development done towards studying the physical processes arising near supermassive black holes ( SMBH ) in active galactic nuclei ( AGN ) , quasars , and other similar components .These studies relied on observations of the broad - band spectral power distributions ( SEDs ) of SMBHs over numerous years in frequency space . However , because of their huge altitudes , direct measurements of the intrinsic luminosities of most AGNs are not required .Instead , one must use indirect tools such as reverberation projection or statistical correlations between various properties of AGNs to estimate their luminosities . For instance , if one knows how many light passes through some region of interest within an AGN then one may calculate its luminosity using simple geometric arguments .Alternatively , if one knows the distance to an AGN then one might estimate its absolute magnitude simply . Unfortunately , both of these method require precise understanding about the stability of the emitting regions which lacks currently be obtained observationally .Therefore , in order to make accurate calculations of the luminosities of distant AGNs , one needs to develop models capable of reproducing the known SEDs of distant AGNs .",
        "rewrite_text": "We present the findings of our research on the continuum emission from accretion disks in black hole candidates (BHCs). Our study introduces an analytical model designed to calculate the spectrum emitted by a thin, optically thick accretion disk surrounding a Schwarzschild black hole. This model has been applied to various BHCs with established mass parameters. Our analysis indicates that the observed spectra are more accurately represented when we assume that the inner edge of the disk is located at 6 gravitational radii. This outcome implies that the conventional thin disk model serves as a more effective approximation for modeling the X-ray continuum emission from these celestial objects.\n\nIn recent years, significant advancements have been made in understanding the physical processes occurring near supermassive black holes (SMBHs) found in active galactic nuclei (AGNs), quasars, and similar entities. These investigations have primarily relied on long-term observations of the broad-band spectral energy distributions (SEDs) of SMBHs across various frequency ranges. However, due to their immense distances, direct measurements of the intrinsic luminosities of most AGNs are often impractical. Instead, researchers typically employ indirect methods such as reverberation mapping or statistical correlations among different AGN properties to estimate their luminosities. For example, by determining the number of light travel times through a specific region within an AGN, one can derive its luminosity using straightforward geometric principles. Alternatively, knowing the distance to an AGN allows for the estimation of its absolute magnitude. Unfortunately, both approaches necessitate a precise understanding of the stability of the emitting regions, which is currently challenging to obtain through observations. Consequently, to accurately calculate the luminosities of distant AGNs, it is essential to develop models that can replicate the known SEDs of these far-off objects. \n\nKeywords: Black holes, Spectroscopy, X-rays, Modeling, Accretion disks, Emission lines, Broad-band spectral power distribution, Luminosity function, Mass determination, Stellar-mass black holes, Supermassive black holes, Active galactic nuclei, Quasars, Cosmic evolution.",
        "ori-fast-z-score": -1.0309670614335873,
        "water-fast-z-score": 5.910828046793255,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The effect of a fifth large-scale space-time dimension on the conservation of energy in a four dimensional Universe .\nAbstract:\nThe present work is an attempt to show that the concept of time can be extended into a higher-dimensional space, and that this extension may have important consequences for our understanding of physical phenomena.  The author considers the possibility that there are five dimensions of space (four ordinary spatial dimensions plus one extra temporal dimension) which could explain some of the observed properties of matter such as entropy production and irreversibility.   In particular he shows how the existence of these additional dimensions would lead to a violation of the principle of entropy increase with time, and suggests that this might provide a possible explanation for the arrow of time. This article is available from: http://arxiv.org/abs/astro-ph/0403070v1. Introduction:  Time has always been considered by physicists as being fundamentally different from other quantities like position or velocity because it cannot be measured directly but only inferred indirectly through its effects on other measurable quantities.  However, recent developments in theoretical physics suggest that we should consider whether the concept of time itself needs to be modified so that it becomes more closely related to other fundamental concepts such as mass, charge and energy.  For example, string theory predicts that all particles are vibrating strings moving along a multidimensional space called  space-time   1  .    Another approach involves considering the possibility that time is not just another quantity but rather part of a larger structure known as spacetime  2  , where the latter consists of both space and time together  3  .  According to this viewpoint, time is no longer regarded as something separate from space; instead they are viewed as two aspects of the same thing  4  .\nIn fact, many modern theories of quantum gravity predict that the universe contains at least three large scale dimensions - namely length, width and height  5  - while also containing a fourth small-scale dimension  6  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The impact of a fifth wide - scale space - time dimension on the conservation of energy in a four dimensional Universe . Abstract : The present work is an attempt to see that the notion of time can be enlarged into a higher - dimensional space , and that this extension may have important implications for our understanding of natural behavior .The author considers the idea that there are five dimensions of space ( four ordinary spatial dimensions plus one extra temporal dimension ) which could explain some of the seen characteristics of matter such as entropy production and irreversibility . In particular he shows how the existence of these additional dimensions would result to a violation of the principle of entropy increase with time , and suggests that this might give a possible reason for the arrow of time .This section is accessible from : www : / / arxiv . org / abs / astro - ph / 0403070v1 . Introduction : Time has always been regarded by physicists as being fundamentally changed from other quantities like position or speed because it cannot be measured immediately but only inferred indirectly through its consequences on other measurable quantities .However , recent developments in theoretical physics suggest that we should consider whether the notion of time itself needs to be altered so that it becomes more closely related to other fundamental concepts such as mass , charge and energy . For instance , string theory predicts that all atoms are vibrating chords moving along a multidimensional space termed space - time 1 .Another approach requires studying the idea that time is not just another quantity but rather component of a greater formation named as spacetime 2 , where the former consists of both space and time together 3 . According to this viewpoint , time is no longer regarded as something separate from space ; simply they are regarded as two forms of the same thing 4 .In indeed , many contemporary explanations of quantum gravitational suggest that the universe possesses at least three large scale dimensions - notably length , diameter and size 5 - while also containing a fourth short - scale dimension 6 .",
        "rewrite_text": "**Title:** The Impact of a Fifth Wide-Scale Space-Time Dimension on the Conservation of Energy in a Four-Dimensional Universe\n\n**Abstract:** This study explores the hypothesis that the concept of time can be expanded into a higher-dimensional framework, potentially leading to significant insights into the fundamental behaviors of nature. The author posits the existence of five dimensions—comprising four conventional spatial dimensions alongside an additional temporal dimension—which may elucidate certain observed phenomena related to matter, including entropy production and the irreversibility of processes. The analysis reveals that the introduction of these extra dimensions could challenge the established principle of entropy increase over time, offering a novel perspective on the nature of the arrow of time. This research suggests that the interplay between these dimensions might provide a deeper understanding of thermodynamic behavior and the fundamental laws governing energy conservation within our four-dimensional universe.\n\nIn the introduction, the author discusses the traditional view of time as fundamentally distinct from other physical quantities such as position and velocity, primarily because it can only be measured indirectly through its effects on measurable phenomena. However, recent advancements in theoretical physics indicate the necessity of re-evaluating the concept of time, proposing that it may be more closely aligned with other fundamental entities like mass, charge, and energy. For example, string theory posits that all matter consists of vibrating strings that traverse a multidimensional space known as space-time. This perspective encourages a rethinking of time as an integral component of a unified structure, rather than a separate entity. Consequently, contemporary theories in quantum gravity suggest that the universe encompasses at least three macroscopic dimensions—length, width, and height—alongside a fourth, more compact dimension. This framework invites further investigation into the implications of a fifth dimension, potentially reshaping our understanding of the universe's fundamental principles. \n\nFor further details, please refer to the original article available at: www.arxiv.org/abs/astro-ph/0403070v1.",
        "ori-fast-z-score": -1.47026414181486,
        "water-fast-z-score": 6.577497476540163,
        "rewrite-fast-z-score": 0.3965257928590721
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  HUDF-JD2: Mid-infrared Evidence for a z~2 Luminous Infrared Galaxy .\nAbstract:\nWe present new mid-IR photometry and spectroscopy of the HUDF-JD2 galaxy at redshift 2.081, which is one of the most luminous infrared galaxies known to date. The SED shows that it has an extremely red continuum with strong PAH emission features in its rest frame optical spectrum. We find evidence for both star formation activity (from the UV-optical) as well as obscured AGN activity (from X-ray observations). This object may be representative of a population of dusty star-forming galaxies undergoing rapid evolution during this critical epoch when massive black holes are growing rapidly along with their host galaxies. Keywords: Infrared, Redshift, Spectroscopy, Photometry, Black Hole Growth, Star Formation Rate Density, Ultraviolet Background Radiation, Cosmic Evolution, Cosmology, Extragalactic Astronomy, High Energy Astrophysics, Space Science, Nearby Galaxies",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : HUDF - JD2 : Mid - infrared Evidence for a z ~ 2 Luminous Infrared Galaxy . Abstract : We present new mid - IR photometry and spectroscopy of the HUDF - JD2 galaxy at redshift 2 . 081 , which is one of the most luminous infrared galaxies known to date .The SED shows that it has an exceptionally red continuum with powerful PAH emission elements in its rest frame optical spectrum . We get confirmation for both star formation activity ( from the UV - optical ) as well as obscured AGN activity ( from X - ray observations ) .This object may be representative of a population of dusty star - creating stars undergoing fast evolution during this critical epoch when massive blue holes are growing rapidly along with their host galaxies . Keywords : Infrared , Redshift , Spectroscopy , Photometry , Black Hole Growth , Star Formation Rate Density , Ultraviolet Background Radiation , Cosmic Evolution , Cosmology , Extragalactic Astronomy , High Energy Astrophysics , Space Science , Nearby Galaxies",
        "rewrite_text": "We present a comprehensive analysis of the HUDF-JD2 galaxy, located at a redshift of 2.081, which stands out as one of the most luminous infrared galaxies identified to date. Our study incorporates new mid-infrared photometry and spectroscopy, revealing a strikingly red spectral energy distribution (SED) characterized by prominent polycyclic aromatic hydrocarbon (PAH) emission features in its rest-frame optical spectrum. The findings indicate significant star formation activity, as evidenced by ultraviolet and optical observations, alongside indications of obscured active galactic nucleus (AGN) activity derived from X-ray data. This dual activity suggests that HUDF-JD2 may exemplify a class of dusty star-forming galaxies that are experiencing rapid evolutionary changes during a pivotal period in cosmic history. This era is marked by the swift growth of massive black holes in conjunction with their host galaxies, highlighting the intricate relationship between star formation and black hole evolution. Our results contribute to the understanding of cosmic evolution, particularly in the context of star formation rate density and the influence of ultraviolet background radiation. The implications of this research extend to various fields, including cosmology, extragalactic astronomy, high-energy astrophysics, and space science, as we explore the dynamics of nearby galaxies and their role in the broader narrative of the universe's development. This study underscores the importance of mid-infrared observations in uncovering the complexities of galaxy formation and evolution during the early epochs of the universe. \n\nKeywords: Infrared, Redshift, Spectroscopy, Photometry, Black Hole Growth, Star Formation Rate Density, Ultraviolet Background Radiation, Cosmic Evolution, Cosmology, Extragalactic Astronomy, High Energy Astrophysics, Space Science, Nearby Galaxies.",
        "ori-fast-z-score": 0.13245323570650439,
        "water-fast-z-score": 2.888741522913896,
        "rewrite-fast-z-score": 0.9918365981341755
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Spectral Efficiency of Spectrum Pooling Systems .\nAbstract:\nIn this work, we study the spectral efficiency (SE) performance of spectrum pooling systems in which multiple secondary users share the same licensed band with one primary user. We consider two different scenarios for the SE analysis: 1) The case where all secondary users are located at fixed distances away from each other and 2) The case where they are randomly distributed over an area according to some spatial distribution function. In both cases, we assume that there is no direct communication between any pair of secondary users. For scenario 1), we derive closed-form expressions for the ergodic capacity of the system under Rayleigh fading channels as well as Nakagami-m fading channels. Our results show that when the number of secondary users increases, their mutual interference becomes more severe resulting in lower ergodic capacities. However, if the distance between them decreases or equivalently, the density of secondary users increases, then the ergodic capacity improves due to higher signal-to-noise ratio levels.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Spectral Efficiency of Spectrum Pooling Systems . Abstract : In this project , we study the spectral efficiency ( SE ) performance of bandwidth pooling systems in which several secondary users share the same licensed band with one secondary customer .We consider two different scenarios for the SE evaluation : 1 ) The case where all secondary users are situated at fixed distances away from each other and 2 ) The case where they are randomly distributed over an area according to some spatial distribution function . In both cases , we suppose that there is no direct transmission between any pair of secondary users .For scenario 1 ) , we derive closed - form expressions for the ergodic strength of the scheme under Rayleigh fading sources as well as Nakagami - m fading channels . Our results show that when the proportion of secondary users increases , their mutual interference becomes more serious resulting in reduced ergodic capacities .However , if the distance between them decreases or equivalently , the density of secondary users increases , then the ergodic performance improves due to higher sound - to - noise ratio levels .",
        "rewrite_text": "In this study, we investigate the spectral efficiency (SE) of bandwidth pooling systems, where multiple secondary users share the same licensed frequency band alongside a primary user. We analyze two distinct scenarios for evaluating SE: the first scenario involves secondary users positioned at fixed distances from one another, while the second scenario considers their random distribution across a designated area, governed by a specific spatial distribution function. In both scenarios, we assume that there is no direct communication between any pairs of secondary users, which allows us to focus on the impact of mutual interference on spectral efficiency.\n\nFor the first scenario, we derive closed-form expressions for the ergodic capacity of the system under two different fading conditions: Rayleigh fading and Nakagami-m fading channels. Our findings indicate that as the number of secondary users increases, the level of mutual interference intensifies, leading to a decrease in the overall ergodic capacity. Conversely, when the distance between secondary users diminishes—effectively increasing their density—the ergodic performance experiences an enhancement due to improved signal-to-noise ratio (SNR) levels.\n\nThis research highlights the intricate balance between user density and interference in bandwidth pooling systems, providing valuable insights into optimizing spectral efficiency in shared communication environments. The results underscore the importance of considering user distribution and interference dynamics when designing and implementing bandwidth pooling strategies, particularly in scenarios characterized by varying user densities and fading conditions. Overall, our study contributes to the understanding of spectral efficiency in complex communication systems and offers a foundation for future research aimed at enhancing performance in shared spectrum environments.",
        "ori-fast-z-score": 0.6469966392206304,
        "water-fast-z-score": 4.69041575982343,
        "rewrite-fast-z-score": 0.24576957615571215
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Equation of state of atomic systems beyond s-wave determined by the lowest order constrained variational method: Large scattering length limit .\nAbstract:\nWe present an equation of state for atomic systems with large scattering lengths, which is obtained in the framework of the lowest-order constrained variational method (LOCV). The LOCV approach allows one to obtain accurate results for both fermions and bosons at low temperatures. We show that our equation of state agrees well with Monte Carlo simulations performed within the grand canonical ensemble. In particular we find good agreement between theory and experiment on the energy per particle of 4 He-4 He mixtures near the superfluid transition temperature T = Tc. Our results are also compared with those obtained using other theoretical approaches such as the virial expansion or the hypernetted chain approximation. \nI. INTRODUCTORY REMARK\nThe equation of state plays an important role in many areas of physics ranging from nuclear matter  1  , quantum gases  2  , astrophysics  3  , condensed matter  4  , etc.. It describes how various thermodynamic quantities depend on each other under given conditions. For example, it can be used to determine the pressure P , chemical potential µ, entropy S, specific heat Cv, compressibility κT , thermal expansivity αp, sound velocity cs, etc., all of them being functions of density n and/or temperature T . Hereafter we will use the symbol EOS to denote any of these quantities.\nIn this work we consider the case when the scattering length a of two particles becomes very large so that the system behaves like a gas of weakly interacting dimers. This situation occurs e.g. in dilute Bose-Einstein condensates  5  where the scattering length may be tuned via Feshbach resonances  6  .\nII. THEORETICAL APPROACHES\n\nA. Grand Canonical Ensemble\nTo describe the properties of a mixture consisting of Nα atoms of species A and Nβ atoms of species B, we employ the grand-canonical ensemble  7, 8  \nwhere H is the total Hamiltonian of the system, β ≡ 1/kB T denotes inverse temperature, μi is the chemical potential of species i ∈ {A, B}, and Z(Nα,",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Equation of state of nuclear systems beyond s - wave determined by the lowest order constrained variational technique : Large scattering length limit . Abstract : We present an equation of state for atomic systems with large scattering lengths , which is achieved in the framework of the lowest - order constrained variational technique ( LOCV ) .The LOCV method enables one to obtain precise conclusions for both fermions and bosons at low temperatures . We see that our equation of state agrees well with Monte Carlo simulations conducted within the grand canonical ensemble .In particular we find good agreement between theoretical and experiment on the power per particle of 4 He - 4 He mixtures near the superfluid transition rate T = Tc . Our results are also compared with those achieved using other theoretical methods such as the virial expansion or the hypernetted chain method .I . INTRODUCTORY REMARK The equation of state plays an important role in multiple fields of science diverse from nuclear matter 1 , quantum gases 2 , astrophysics 3 , condensed matter 4 , etc . .It gives how various thermodynamic quantities rely on each other under given conditions . For instance , it can be used to predict the pressure P , chemical potential µ , entropy S , specific hot Cv , compressibility κT , thermal expansivity αp , noise speed cs , etc . , all of them being functions of density n and / or temperature T .Hereafter we will use the symbol EOS to indicate any of these quantities . In this study we study the case when the scattering height a of two particles gets very huge so that the system behaves like a gas of weakly interacting dimers .This condition occurs e . g . in dilute Bose - Einstein condensates 5 where the scattering duration must be tuned via Feshbach resonances 6 .II.THEORETICAL APPROACHES\n\nA.Grand Canonical Ensemble To describe the properties of a mixture consisting of Nα atoms of genus A and Nβ ions of genus B , we utilize the grand - canonical ensemble 7 , 8 where H is the total Hamiltonian of the system , β ≡ 1 / kB T denotes inverse temperature , μi is the chemical potential of species i ∈ { A , B } , and Z ( Nα ,",
        "rewrite_text": "**Title:** Equation of State of Nuclear Systems Beyond s-Wave Determined by the Lowest Order Constrained Variational Technique: Large Scattering Length Limit\n\n**Abstract:** In this study, we derive an equation of state for atomic systems characterized by large scattering lengths, employing the lowest-order constrained variational technique (LOCV). This method provides a robust framework for accurately analyzing both fermionic and bosonic systems at low temperatures. Our findings indicate a strong correlation between the LOCV-derived equation of state and results from Monte Carlo simulations performed within the grand canonical ensemble. Notably, we observe a remarkable agreement between our theoretical predictions and experimental data regarding the power per particle in mixtures of helium-4 (4He-4He) close to the superfluid transition temperature (T = Tc). Furthermore, we compare our results with those obtained through alternative theoretical approaches, including the virial expansion and the hypernetted chain method.\n\nThe equation of state is a fundamental concept across various scientific disciplines, including nuclear matter, quantum gases, astrophysics, and condensed matter physics. It elucidates the interdependence of thermodynamic quantities under specific conditions, allowing for the prediction of properties such as pressure (P), chemical potential (µ), entropy (S), specific heat (Cv), compressibility (κT), thermal expansivity (αp), and sound speed (cs), all of which are functions of density (n) and temperature (T). In this investigation, we focus on scenarios where the scattering length (a) between two particles becomes significantly large, resulting in a system that behaves like a gas of weakly interacting dimers. This phenomenon is particularly relevant in dilute Bose-Einstein condensates, where the scattering length can be finely tuned using Feshbach resonances. Our theoretical framework is grounded in the grand canonical ensemble, which effectively describes the properties of mixtures comprising Nα atoms of species A and Nβ ions of species B, incorporating the total Hamiltonian of the system and the relevant thermodynamic parameters.",
        "ori-fast-z-score": -0.40689422938557973,
        "water-fast-z-score": 6.002192581838214,
        "rewrite-fast-z-score": -0.5183210553488161
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Adding Environmental Gas Physics to the Semi-Analytic Method for Galaxy Formation: Gravitational Heating .\nAbstract:\nWe present an improved semi-analytical method (SAM) that includes gravitational heating by dark matter halos and gas cooling in galaxy formation, which is essential to reproduce observed properties of galaxies such as luminosity functions at different redshifts.  We show that our SAM can successfully explain the evolution of the stellar mass function over cosmic time with reasonable parameters. In addition, we find that the inclusion of gravitational heating leads to more realistic predictions on the star formation rate density history than previous models without this effect. Finally, we discuss how the model could be further improved by including other physical processes like supernova feedback or AGN activity. The results presented here are based on observations made with ESO Telescopes at Paranal Observatory under programme ID 085.A-0488(A). This work was supported by JSPS KAKENHI Grant Number JP15K05481. Figure 1 . Predicted number densities of galaxies as a function of their total stellar masses compared with observational data taken from the literature. Red circles represent the predicted number densities using our new SAM code while blue squares indicate those obtained with the original SAM code developed by Nagashima & Yoshii (2004) .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Adding Environmental Gas Physics to the Semi - Analytic Method for Galaxy Formation : Gravitational Heating . Abstract : We introduce an excellent semi - empirical method ( SAM ) that encompasses gravitational heating by black material halos and gas warming in universe formation , which is crucial to reproduce observed properties of stars such as luminosity functions at different redshifts .We see that our SAM can effectively predict the evolution of the stellar mass function over cosmic time with suitable variables . In addition , we find that the introduction of gravitational heating results to more realistic predictions on the star formation rate density history than prior models without this effect .Finally , we talk how the model could be further strengthened by including other physical processes like supernova feedback or AGN activity . The results presented here are based on observations made with ESO Telescopes at Paranal Observatory under programme ID 085 . A - 0488 ( A ) .This project was supported by JSPS KAKENHI Grant Number JP15K05481 . Figure 1 .Predicted number densities of galaxies as a function of their total stellar masses compared with observational data taken from the literature . Red rings represent the expected number densities using our new SAM code while blue squares represents those achieved with the previous SAM code developed by Nagashima & Yoshii ( 2004 ) .",
        "rewrite_text": "Title: Incorporating Environmental Gas Physics into the Semi-Analytic Method for Galaxy Formation: Gravitational Heating\n\nAbstract: In this study, we present a novel semi-empirical method (SAM) that integrates the effects of gravitational heating from dark matter halos and gas warming during the formation of the universe. This approach is essential for accurately reproducing the observed characteristics of stars, including their luminosity functions across various redshifts. Our findings demonstrate that this enhanced SAM effectively models the evolution of the stellar mass function throughout cosmic history by utilizing appropriate variables. Notably, the inclusion of gravitational heating leads to more realistic predictions regarding the history of star formation rate density compared to previous models that did not account for this phenomenon. Furthermore, we discuss potential enhancements to the model by incorporating additional physical processes, such as supernova feedback and active galactic nucleus (AGN) activity, which could further refine our understanding of galaxy formation. The results presented in this paper are based on observational data collected using the ESO Telescopes at the Paranal Observatory, under program ID 085.A-0488(A). This research was supported by the Japan Society for the Promotion of Science (JSPS) KAKENHI Grant Number JP15K05481. Figure 1 illustrates the predicted number densities of galaxies as a function of their total stellar masses, juxtaposed with observational data from existing literature. The red rings indicate the expected number densities derived from our new SAM code, while the blue squares represent the results obtained from the previous SAM code developed by Nagashima & Yoshii (2004).",
        "ori-fast-z-score": -0.6060915267313265,
        "water-fast-z-score": 5.125692857821981,
        "rewrite-fast-z-score": 1.3598002073001698
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Molecular opacities for low-mass metal-poor AGB stars undergoing the Third Dredge Up .\nAbstract:\nWe present new molecular opacity tables that include all relevant molecules in cool, carbon-rich stellar envelopes and are valid over a wide range of temperatures (T = 1000 - 10000 K), densities (ρ = 10 −10 -10 6 g/cm 3 ) and compositions (C/O=0.5-2). The calculations were performed with the state-of-the-art ab initio line-by-line radiative transfer code SPECTRUM using extensive laboratory data on molecular lines as well as theoretical predictions based on quantum chemical methods. We have calculated synthetic spectra for several model atmospheres representative of red giant branch (RGB) and asymptotic giant branch (AGB) stars to demonstrate how our new opacity tables affect their structure and evolution. Our results show that the inclusion of additional species such as SiO, TiO, VO, FeH, MgS, NaCl, CaF, AlO, CrH, MnS, CoO, NiO, ZnS, ZrO, BaO, LaO etc., which are not included in previous studies, leads to significant changes in the atmospheric structure and consequently affects the predicted surface abundances of CNO elements during the third dredge-up phase.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Molecular opacities for low - mass metal - weak AGB stars undergoing the Third Dredge Up . Abstract : We create novel molecular opacity charts that include all relevant molecules in cold , carbon - rich stellar envelopes and are applicable over a broad variety of temperatures ( T = 1000 - 10000 K ) , densities ( ρ = 10 −10 - 10 6 g / cm 3 ) and compositions ( C / O = 0 . 5 - 2 ) .The studies were performed with the state - of - the - art ab initio line - by - line radiative transfer code SPECTRUM using extensive research data on chemical lines as well as theory estimates based on quantum chemical techniques . We have calculated synthetic spectra for various model atmospheres representative of red dwarf branch ( RGB ) and asymptotic giant branch ( AGB ) stars to explain how our new opacity tables affect their structure and evolution .Our results show that the introduction of added species such as SiO , TiO , VO , FeH , MgS , NaCl , CaF , AlO , CrH , MnS , CoO , NiO , ZnS , ZrO , BaO , LaO etc . , which are not mentioned in earlier analyses , leads to significant improvements in the atmospheric composition and consequently affects the expected surface abundances of CNO compounds during the third dredge - up process .",
        "rewrite_text": "We present a comprehensive study on the molecular opacities relevant to low-mass, metal-weak asymptotic giant branch (AGB) stars experiencing the Third Dredge Up. Our research introduces innovative molecular opacity charts that encompass a wide range of molecules found in cold, carbon-rich stellar envelopes. These charts are applicable across a diverse spectrum of temperatures (ranging from 1000 to 10000 K), densities (from 10^-10 to 10^6 g/cm^3), and chemical compositions (with carbon-to-oxygen ratios between 0.5 and 2). Utilizing the advanced ab initio line-by-line radiative transfer code SPECTRUM, we conducted our studies based on extensive empirical data regarding chemical line properties, complemented by theoretical estimates derived from quantum chemical methods. \n\nWe generated synthetic spectra for various model atmospheres that represent both the red giant branch (RGB) and AGB stars, allowing us to assess the impact of our newly developed opacity tables on stellar structure and evolution. Our findings indicate that the inclusion of additional molecular species—such as SiO, TiO, VO, FeH, MgS, NaCl, CaF, AlO, CrH, MnS, CoO, NiO, ZnS, ZrO, BaO, and LaO, which were not considered in previous studies—results in substantial enhancements in the atmospheric composition of these stars. This, in turn, significantly influences the anticipated surface abundances of carbon, nitrogen, and oxygen (CNO) compounds during the Third Dredge Up process. Our work provides critical insights into the role of molecular opacities in the evolution of low-mass AGB stars and highlights the importance of incorporating a broader range of molecular species in astrophysical models to improve our understanding of stellar nucleosynthesis and chemical enrichment in the universe.",
        "ori-fast-z-score": -2.0851441405707476,
        "water-fast-z-score": 3.8367212705025735,
        "rewrite-fast-z-score": -1.5085060660073935
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  WHAM: A WENO-based general relativistic numerical scheme I: Hydrodynamics .\nAbstract:\nWe present the first version of our new code, WHAM (Weno-Hybrid Arbitrary Mesh), which is based on the recently developed weighted essentially non-oscillatory (WENO) schemes for solving hyperbolic conservation laws in one dimension and multi-dimensions.  The main idea behind this method is to use high-order spatial reconstruction with an adaptive mesh refinement technique to achieve high accuracy at low computational cost. We have implemented several different versions of the WENO algorithm into our code including the fifth-order WENO-Z scheme as well as the third- and fifth-order WENO-JS schemes. In addition we also implement the fourth-order Runge-Kutta time integration scheme along with the Harten-Lax-van Leer contact discontinuity capturing scheme to handle discontinuities arising during hydrodynamic evolution. Our results show that all these algorithms are able to produce accurate solutions when compared against exact or reference solutions.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : WHAM : A WENO - based general relativistic numerical system I : Hydrodynamics . Abstract : We introduce the first version of our new code , WHAM ( Weno - Hybrid Arbitrary Mesh ) , which is based on the recently established weighted essentially non - oscillatory ( WENO ) schemes for solving hyperbolic conservation laws in one dimension and multi - dimensions .The main idea behind this algorithm is to use large - order spatial reconstruction with an adaptive mesh refinement technique to achieve high sensitivity at low computational efficiency . We have integrated various different versions of the WENO algorithm into our code including the fifth - order WENO - Z plan as well as the third - and fifth - order WENO - JS schemes .In addition we also incorporate the fourth - order Runge - Kutta time integration scheme along with the Harten - Lax - van Leer contact discontinuity capturing scheme to manage discontinuities resulting during hydrodynamic evolution . Our results show that all these algorithms are able to produce accurate solutions when compared against exact or reference solutions .",
        "rewrite_text": "We present the initial version of our innovative computational code, WHAM (Weno-Hybrid Arbitrary Mesh), designed for general relativistic hydrodynamics. This code leverages the recently developed weighted essentially non-oscillatory (WENO) schemes, which are effective for solving hyperbolic conservation laws in both one-dimensional and multi-dimensional contexts. The core concept of WHAM revolves around employing high-order spatial reconstruction methods in conjunction with an adaptive mesh refinement strategy, enabling the code to achieve remarkable accuracy while maintaining computational efficiency. \n\nIn our implementation, we have integrated several variants of the WENO algorithm, including the fifth-order WENO-Z scheme, as well as the third- and fifth-order WENO-JS schemes. This diversity allows for flexibility in handling various hydrodynamic scenarios. Furthermore, we have incorporated a fourth-order Runge-Kutta method for time integration, complemented by the Harten-Lax-van Leer (HLL) scheme for capturing contact discontinuities that may arise during the hydrodynamic evolution process.\n\nOur extensive testing demonstrates that the algorithms embedded within WHAM yield highly accurate solutions when benchmarked against exact or reference solutions. This capability positions WHAM as a robust tool for researchers in the field of general relativistic hydrodynamics, facilitating the exploration of complex astrophysical phenomena. The combination of high-order accuracy and adaptive mesh refinement makes WHAM particularly suitable for simulations that require detailed resolution of features while managing computational resources effectively. Overall, WHAM represents a significant advancement in numerical methods for hydrodynamic simulations in the context of general relativity.",
        "ori-fast-z-score": 1.9188064472004938,
        "water-fast-z-score": 4.9819900360298925,
        "rewrite-fast-z-score": 0.8838834764831843
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Post-Newtonian gravitational radiation and equations of motion via direct integration of the relaxed Einstein equations. V. Evidence for the strong equivalence principle to second post-Newtonian order .\nAbstract:\nWe present evidence that the strong equivalence principle (SEP) holds in general relativity at least up to second post Newtonian order, by showing that the SEP is satisfied if it holds at first PN order.  We show this by directly integrating the relaxed Einstein field equations with an appropriate source term which incorporates the effects of gravitational waves generated by accelerating bodies. The results are consistent with those obtained previously using perturbation theory. This work was supported by NSF grant PHY-0456747. In this article we will consider only isolated systems where no external forces act on any body. For such systems the total energy E = T + U , where T is the kinetic energy and U is the potential energy associated with all interactions between particles within the system. If there were no gravitational fields then the total energy would be conserved since the net force acting on each particle vanishes. However when gravitational fields exist they can exchange energy with matter through emission or absorption of gravitational waves.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Post - Newtonian gravitational rays and equations of movement via direct combining of the relaxation Einstein equations . V . Evidence for the strong equivalence principle to second post - Newtonian order .Abstract : We present evidence that the strong equivalence principle ( SEP ) holds in general relativity at least up to second post Newtonian order , by showing that the SEP is fulfilled if it holds at first PN order . We see this by specifically integrating the relaxation Einstein field equations with an appropriate source term which includes the effects of gravitational waves generated by accelerating objects .The results are compatible with those achieved formerly using perturbation theory . This research was supported by NSF grant PHY - 0456747 .In this discussion we will use only isolated systems where no external forces action on any body . For such systems the total energy E = T + U , where T is the kinetic power and U is the potential energy associated with all interactions between particles within the system .If there were no gravity fields then the total energy must be conserved since the net force acting on each particle vanishes . However when gravity fields arise they can trade power with matter through emission or emission of gravitational waves .",
        "rewrite_text": "**Title:** Post-Newtonian Gravitational Rays and Equations of Motion via Direct Combination of the Relaxation Einstein Equations: Evidence for the Strong Equivalence Principle to Second Post-Newtonian Order\n\n**Abstract:** In this study, we provide compelling evidence supporting the strong equivalence principle (SEP) within the framework of general relativity, demonstrating its validity at least up to the second post-Newtonian (PN) order. Our findings indicate that if the SEP is satisfied at the first PN order, it necessarily holds at the second order as well. This conclusion is drawn from a detailed integration of the relaxation Einstein field equations, incorporating a source term that accounts for the influence of gravitational waves produced by accelerating bodies. The results obtained align with previous findings derived from perturbation theory, reinforcing the robustness of our approach. \n\nOur investigation is confined to isolated systems, wherein no external forces act upon any of the bodies involved. In such scenarios, the total energy of the system, denoted as E, is the sum of the kinetic energy (T) and the potential energy (U), which encompasses all interactions among the particles within the system. In the absence of gravitational fields, the conservation of total energy is guaranteed, as the net force acting on each particle is zero. However, the introduction of gravitational fields alters this dynamic, allowing for an exchange of energy between matter and gravitational waves through their emission or absorption. \n\nThis research was made possible through the support of the National Science Foundation under grant PHY-0456747. Our findings not only contribute to the theoretical understanding of gravitational interactions but also have implications for future experimental tests of the strong equivalence principle in the context of gravitational wave astronomy and high-energy astrophysical phenomena.",
        "ori-fast-z-score": 0.29851115706299675,
        "water-fast-z-score": 5.527707983925667,
        "rewrite-fast-z-score": 2.5144742283748487
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Crystallization in large wireless networks .\nAbstract:\nWe study the problem of finding an optimal schedule for data transmission over multiple channels with interference constraints, where each channel is assigned to one transmitter-receiver pair and transmissions on different pairs are subject to mutual interference. We consider two models: (i) The first model assumes that all transmitters have fixed power levels. (ii) In the second model we assume that transmitters can adjust their powers dynamically. For both cases, we show how to find an optimal schedule by solving a sequence of linear programs. Our results hold even if there exists only one receiver per transmitter. This work was supported by NSF grant CCF-0430018. 1 Introduction Wireless networks consist of many nodes communicating via radio signals. Each node has limited range and thus cannot communicate directly with every other node. Instead, it communicates indirectly through intermediate nodes called relays or routers. A fundamental question in this context is: How should these relays be placed?",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Crystallization in large wireless networks . Abstract : We research the issue of finding an appropriate schedule for information transmission over multiple channels with interference constraints , where each channel is allotted to one transmitter - receiver set and signals on separate pairs are subject to mutual interference .We consider two models : ( i ) The first theory requires that all transmitters have fixed speed levels . ( ii ) In the second theory we suppose that transmitters can adjust their powers dynamically .For both cases , we prove how to find an appropriate schedule by solving a sequence of linear programs . Our results hold even if there exists only one receiver per transmitter .This work was supported by NSF grant CCF - 0430018 . 1 Introduction Wireless networks consist of many nodes communicating via radio signals .Each node has restricted range and therefore cannot transmit directly with every other node . Instead , it communicates indirectly through intermediate nodes termed relays or routers .A basic issue in this situation is : How should these relays be placed ?",
        "rewrite_text": "Title: Crystallization in Large Wireless Networks\n\nAbstract: This study addresses the challenge of developing an effective scheduling strategy for information transmission across multiple channels while adhering to interference constraints. In our framework, each channel is designated for a specific transmitter-receiver pair, and signals transmitted on different pairs may interfere with one another. We explore two distinct models to tackle this problem: the first model assumes that all transmitters operate at fixed power levels, while the second model allows for dynamic adjustment of transmitter power. For both scenarios, we demonstrate a method for determining an optimal transmission schedule by solving a series of linear programming problems. Notably, our findings remain valid even in cases where there is only one receiver assigned to each transmitter. This research contributes to the understanding of wireless network dynamics and provides a systematic approach to managing interference in large-scale communication systems. The work was supported by NSF grant CCF-0430018. \n\n1. Introduction: Wireless networks are comprised of numerous nodes that communicate through radio signals. Due to the limited transmission range of each node, direct communication between all nodes is not feasible. Instead, nodes rely on intermediate entities known as relays or routers to facilitate indirect communication. A fundamental question arises in this context: What is the optimal placement of these relays to enhance network performance? This inquiry is crucial for improving the efficiency and reliability of wireless communication in complex network environments.",
        "ori-fast-z-score": -1.0660035817780522,
        "water-fast-z-score": 4.133991732024804,
        "rewrite-fast-z-score": -0.5345224838248488
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  X-ray Luminosity Functions of Normal Galaxies in the GOODS .\nAbstract:\nWe present X-ray luminosity functions (XLFs) for normal galaxies at z=0.5-2, based on deep Chandra observations of the Great Observatories Origins Deep Survey-North field. We use photometric redshifts to select galaxy samples with different stellar masses and star formation rates. The XLFs are derived by fitting the observed number counts using a Schechter function plus an exponential cutoff below Lx=1041 erg s-1. Our results show that the faint-end slope is steeper than -1.0 for all mass bins considered here. For massive galaxies (log(M/Msun)>10), we find no significant evolution between z=1.6 and 0.7. However, there appears to be some evidence for evolution towards lower characteristic luminosities as well as flatter slopes at higher redshift. These results suggest that the most massive galaxies may have evolved into less luminous systems over time.  We also compare our results with previous studies which used shallower data or smaller sample sizes.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : X - ray Luminosity Functions of Normal Galaxies in the GOODS . Abstract : We create X - ray luminosity functions ( XLFs ) for normal galaxies at z = 0 . 5 - 2 , built on deep Chandra measurements of the Great Observatories Origins Deep Survey - North field .We use photometric redshifts to select galaxy samples with various stellar masses and galaxy formation rates . The XLFs are derived by fitting the observed number counts utilizing a Schechter function plus an exponential cutoff below Lx = 1041 erg s - 1 .Our results show that the faint - end slope is steeper than - 1 . 0 for all mass bins included here . For huge objects ( log ( M / Msun ) > 10 ) , we find no notable evolve between z = 1 . 6 and 0 . 7 .However , there seems to be some evidence for expansion towards lesser characteristic luminosities as well as flatter slopes at higher redshift . These data suggest that the most large galaxies must have evolution into fewer luminous systems over time .We also compare our findings with previous research which using shallower evidence or smaller data numbers .",
        "rewrite_text": "In this study, we present the X-ray luminosity functions (XLFs) of normal galaxies within the redshift range of z = 0.5 to 2, utilizing extensive observations from the Chandra X-ray Observatory as part of the Great Observatories Origins Deep Survey - North field. Our methodology involves the application of photometric redshifts to curate galaxy samples characterized by varying stellar masses and galaxy formation rates. The XLFs are constructed by fitting the observed number counts with a Schechter function, incorporating an exponential cutoff for luminosities below Lx = 10^41 erg s^-1. \n\nOur analysis reveals that the faint-end slope of the XLF is consistently steeper than -1.0 across all mass bins examined. For massive galaxies, specifically those with log(M/Msun) > 10, we observe no significant evolution in the XLF between redshifts z = 1.6 and 0.7. However, there is suggestive evidence of a shift towards lower characteristic luminosities and a flattening of the slope at higher redshifts. These findings imply that the most massive galaxies have undergone a transformation, evolving into systems with lower luminosities over cosmic time.\n\nFurthermore, we juxtapose our results with previous studies that relied on shallower datasets or smaller sample sizes, highlighting the robustness of our findings. This work contributes to the understanding of galaxy evolution and the role of X-ray emissions in tracing the properties of normal galaxies across different epochs in the universe. Our results underscore the importance of deep observational campaigns in revealing the complexities of galaxy formation and evolution in the context of cosmic history.",
        "ori-fast-z-score": -1.3587324409735149,
        "water-fast-z-score": 5.222222222222222,
        "rewrite-fast-z-score": 1.2809280616135812
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Quasi Equilibrium Grid Algorithm: geometric construction for model reduction .\nAbstract:\nWe present an algorithm to construct the reduced basis space in the context of nonlinear problems with multiple solutions, which is based on the concept of quasi-equilibrium grid (QEG). The QEG method was originally developed by Simo and Armero as a numerical technique for solving rate-independent processes such as plasticity or damage mechanics. We show that this approach can be used to generate snapshots for constructing the reduced basis spaces associated with nonlinear problems with multiple solutions. In particular, we consider two examples arising from structural dynamics and fluid flow computations. Numerical results demonstrate that our proposed method yields accurate approximations at significantly lower computational cost than existing approaches. Keywords: Reduced Basis Method; Quasi-Equilibrium Grids; Nonlinear Problems; Model Order Reduction; Geometric Construction; Snapshot Generation. 1 Introduction.\nThe goal of this work is to develop efficient algorithms for generating snapshots for constructing the RB spaces associated with nonlinear problems having multiple solutions. This problem arises frequently when one solves engineering applications involving complex physical phenomena such as multiphysics coupling, material failure, contact/impact, etc.. For example, in structural dynamics, it may happen that different initial conditions lead to different equilibrium states  19, 20  . Similarly, in fluid flows, there are often many steady-state solutions corresponding to different boundary conditions  7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18  .\nIn order to solve these types of problems efficiently using the reduced basis method (RBM), it is necessary to have a good set of snapshots representing all possible solution behaviors. However, since each snapshot corresponds to a specific solution behavior, it is not easy to obtain them directly through standard finite element analysis. Therefore, various techniques have been developed over the past decade to overcome this difficulty  1, 2, 3, 4, 5, 6, 7, 9, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Quasi Equilibrium Grid Algorithm : geometric design for model reduction . Abstract : We create an algorithm to build the reduced basis set in the context of nonlinear issues with many solutions , which is based on the idea of quasi - equilibrium grid ( QEG ) .The QEG method was originally developed by Simo and Armero as a numerical technique for solving rate - based processes such as plasticity or damage mechanics . We see that this methodology can be used to create snapshots for constructing the reduced basis sets associated with nonlinear issues with many solutions .In particular , we treat two examples arising from functional dynamics and fluid stream computations . Numerical results show that our proposed approach produces accurate approximations at significantly reduced theoretical cost than existing techniques .Keywords: Reduced Basis Method; Quasi-Equilibrium Grids; Nonlinear Problems; Model Order Reduction; Geometric Construction; Snapshot Generation.1 Introduction.The goal of this research is to develop fast algorithms for generating snapshots for constructing the RB spaces related with nonlinear issues having many solutions . This problem arises often when one solves engineering applications requiring complex physical phenomena such as multiphysics coupling , metal failure , touch / explosion , etc . . For instance , in structural physics , it could happen that different initial conditions lead to different equilibrium states 19 , 20 .Similarly , in fluid flows , there are often many steady - state solutions corresponding to different border conditions 7 , 8 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 . In order to solve these kinds of problems easily using the reduced basis method ( RBM ) , it is required to have a good collection of snapshots describing all possible solution behaviors .However , since each snapshot belongs to a certain solve situation , it is not straightforward to obtain them directly through conventional finite element extraction . Therefore , various methods have been used over the previous decade to overcome this challenge 1 , 2 , 3 , 4 , 5 , 6 , 7 , 9 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 , 31 , 32 , 33 , 34 , 35 , 36 , 37 , 38 , 39 , 40",
        "rewrite_text": "**Title:** Quasi-Equilibrium Grid Algorithm: Geometric Design for Model Reduction\n\n**Abstract:** In this study, we introduce a novel algorithm aimed at constructing a reduced basis set tailored for nonlinear problems characterized by multiple solutions. This approach is grounded in the concept of the quasi-equilibrium grid (QEG), a numerical technique initially developed by Simo and Armero for addressing rate-dependent processes, including plasticity and damage mechanics. We demonstrate that the QEG methodology can effectively generate snapshots essential for forming reduced basis sets in the context of nonlinear challenges with numerous potential solutions. To illustrate the applicability of our method, we present two case studies derived from functional dynamics and fluid dynamics simulations. The numerical results indicate that our proposed algorithm yields highly accurate approximations while significantly reducing the computational costs compared to traditional methods. \n\nThe primary objective of this research is to devise efficient algorithms for snapshot generation that facilitate the construction of reduced basis (RB) spaces associated with nonlinear problems exhibiting multiple solutions. Such scenarios frequently arise in engineering applications that involve complex physical phenomena, including multiphysics interactions, material failure, and explosive events. For example, in structural mechanics, varying initial conditions can lead to distinct equilibrium states, while in fluid dynamics, multiple steady-state solutions may exist for different boundary conditions. To effectively tackle these challenges using the reduced basis method (RBM), it is crucial to compile a comprehensive set of snapshots that encapsulate the diverse behaviors of potential solutions. However, since each snapshot corresponds to a specific solving scenario, acquiring them through conventional finite element extraction methods is not straightforward. Consequently, a variety of strategies have been explored over the past decade to address this issue, paving the way for advancements in model order reduction techniques. \n\n**Keywords:** Reduced Basis Method; Quasi-Equilibrium Grids; Nonlinear Problems; Model Order Reduction; Geometric Construction; Snapshot Generation.",
        "ori-fast-z-score": -1.7728105208558367,
        "water-fast-z-score": 8.112515221915238,
        "rewrite-fast-z-score": -1.4974097718542911
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Physical Nature of Polar Broad Absorption Line Quasars .\nAbstract:\nWe present the results of an investigation into the physical nature of polar broad absorption line quasars (BALQSOs). We find that BALQSOs are more likely to be radio-loud than non-BAL QSOs, and that they have higher luminosities at rest-frame ultraviolet wavelengths. The fraction of BALQSOs in our sample is consistent with previous studies; however we do not detect any significant difference between the fractions of BALQSOs found among radio-quiet and radio-loud quasars. In addition, we find no evidence for differences in the properties of BALs observed along different lines-of-sight within individual objects. Finally, we show that there exists a correlation between the strength of the CIV emission-line blueshift and the equivalent width of the associated BAL trough. These findings suggest that BALQSOs may represent a special subclass of radio-loud quasars which are characterized by high accretion rates onto supermassive black holes.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Physical Nature of Polar Broad Absorption Line Quasars . Abstract : We present the conclusion of an investigation into the physical nature of polar wide absorption line quasars ( BALQSOs ) .We see that BALQSOs are more likely to be radio - loud than non - BAL QSOs , and that they have greater luminosities at rest - frame ultraviolet wavelengths . The percentage of BALQSOs in our sample is compatible with previous research ; however we do not detect any large change between the fractions of BALQSOs discovered among radio - quiet and radio - loud quasars .In addition , we find no evidence for variations in the properties of BALs observed along various lines - of - seeing within particular objects . Finally , we prove that there exists a correlation between the strength of the CIV radiation - line blueshift and the equivalent thickness of the associated BAL trough .These studies imply that BALQSOs might represent a unique subclass of radio - loud quasars which are marked by high accretion levels onto supermassive black holes .",
        "rewrite_text": "We present the findings of our study on the physical characteristics of polar broad absorption line quasars (BALQSOs). Our research indicates that BALQSOs exhibit a higher likelihood of being classified as radio-loud compared to their non-BAL counterparts, alongside demonstrating increased luminosity in the rest-frame ultraviolet spectrum. The proportion of BALQSOs identified in our analysis aligns with previous studies; however, we observe no significant variation in the prevalence of BALQSOs among both radio-quiet and radio-loud quasars. Furthermore, our investigation reveals no substantial differences in the properties of broad absorption lines (BALs) when examined across different lines of sight within individual quasars. A noteworthy finding of our research is the established correlation between the strength of the CIV emission line blueshift and the equivalent width of the corresponding BAL trough. These results suggest that BALQSOs may constitute a distinct subclass of radio-loud quasars, characterized by elevated accretion rates onto supermassive black holes. This study enhances our understanding of the unique features and behaviors of BALQSOs, contributing to the broader knowledge of quasar classifications and their underlying physical mechanisms.",
        "ori-fast-z-score": 0.46499055497527714,
        "water-fast-z-score": 6.119912853410033,
        "rewrite-fast-z-score": 0.42640143271122083
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Direct cosmological simulations of the growth of black holes and galaxies .\nAbstract:\nWe present results from direct cosmological hydrodynamic simulations that follow the formation of supermassive black holes (SMBHs) in galactic nuclei, their subsequent evolution through mergers with other SMBHs, and the associated feedback on galaxy properties. We find that:  The simulated SMBH mass function agrees well with observations at z = 0 for M• > 10^7M_solar.  At higher redshifts, our model predicts too many low-mass SMBHs compared to observational estimates based on quasar luminosity functions; this discrepancy may be due to uncertainties in the assumed duty cycle or radiative efficiency of quasars.  Our models predict an average Eddington ratio distribution that is consistent with observed distributions inferred from optical/UV emission lines.  In addition, we show that the predicted relation between BH mass and bulge velocity dispersion agrees reasonably well with observations over four orders of magnitude in BH mass.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Direct cosmological simulations of the development of blue holes and galaxies . Abstract : We report findings from direct cosmological hydrodynamic simulations that track the formation of supermassive black holes ( SMBHs ) in galactic nuclei , their successive evolution through mergers with other SMBHs , and the associated feedback on star dynamics .We see that : The simulated SMBH mass function agrees well with observations at z = 0 for M • > 10 ^ 7M _ solar . At higher redshifts , our model predicts too many small - mass SMBHs compared to observational projections based on quasar luminosity functions ; this discrepancy may be due to uncertainties in the expected duty cycle or radiative efficiency of quasars .Our models predict an estimated Eddington proportion distribution that is compatible with observed distributions inferred from optical / UV absorption lines . In addition , we prove that the expected relation between BH mass and bulge velocity dispersion agrees reasonably well with observations over four orders of magnitude in BH mass .",
        "rewrite_text": "We present the results of direct cosmological hydrodynamic simulations that investigate the formation and evolution of supermassive black holes (SMBHs) within galactic nuclei. Our study focuses on the processes involved in the growth of these black holes, including their mergers with other SMBHs and the subsequent impact on stellar dynamics within their host galaxies. Our findings indicate that the mass function of simulated SMBHs aligns closely with observational data at redshift z = 0 for masses greater than 10^7 solar masses. However, at higher redshifts, our model predicts an excess of low-mass SMBHs when compared to observational estimates derived from quasar luminosity functions. This discrepancy may stem from uncertainties related to the duty cycle or radiative efficiency of quasars, which could affect the observed luminosity and distribution of these black holes. Furthermore, our simulations yield an estimated distribution of Eddington ratios that is consistent with the distributions inferred from optical and ultraviolet absorption line studies. Additionally, we demonstrate that the anticipated correlation between black hole mass and bulge velocity dispersion is in reasonable agreement with empirical observations across a wide range of black hole masses, spanning four orders of magnitude. These results contribute to our understanding of the complex interplay between SMBHs and their host galaxies, shedding light on the mechanisms that govern their growth and the evolution of galactic structures.",
        "ori-fast-z-score": -1.162476387438193,
        "water-fast-z-score": 3.117691453623979,
        "rewrite-fast-z-score": 0.20203050891044214
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Hierarchical Markovian models for hyperspectral image segmentation .\nAbstract:\nWe propose to use hierarchical hidden Markov random fields (HHMRFs) as the underlying model in an unsupervised segmentation algorithm for hyperspectral images. The HHMRFs are constructed by combining several layers of hidden Markov chains, where each layer is associated with one particular spatial scale. We show that this multiscale approach leads to improved performance over single-scale methods and we demonstrate its effectiveness on two different data sets. Finally, we compare our results against those obtained using state-of-the-art algorithms based on Gaussian mixture models or sparse coding techniques. \nIntroduction\n\nHyperspectral imaging has become increasingly popular during recent years due to advances in sensor technology  1  . In contrast to conventional color cameras which capture only three bands per pixel, hyperspectral sensors can record hundreds of narrow spectral bands simultaneously  2  , leading to high-dimensional data volumes. This poses new challenges both in terms of storage requirements and computational complexity  3  .\nIn many applications it would be desirable to perform automatic analysis of such large amounts of data without any prior knowledge about the scene being observed  4  . One important task in this context is the detection of homogeneous regions within the image  5  . These so-called segments may correspond to individual objects  6  , but they could also represent parts of larger structures like buildings  7  or roads  8  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Hierarchical Markovian methods for hyperspectral photograph segmentation . Abstract : We suggest to use hierarchical hidden Markov random fields ( HHMRFs ) as the fundamental model in an unsupervised segmentation algorithm for hyperspectral pictures .The HHMRFs are built by combining several layers of hidden Markov chains , where each layer is associated with one certain spatial scale . We see that this multiscale approach leads to improved performance over multi - scale techniques and we prove its effectiveness on two different data sets .Finally , we compare our findings against those achieved using state - of - the - art algorithms based on Gaussian mixture models or dense coding techniques . Introduction Hyperspectral photography has become especially popular during recent years owing to advances in sensor technology 1 .In comparison to conventional color cameras which capture only three bands per pixel , hyperspectral cameras can display hundreds of narrow spectral patterns simultaneously 2 , leading to large - dimensional data capacities . This poses novel challenges both in terms of storage needs and computational difficulty 3 .In many applications it would be desirable to conduct automatic analysis of such substantial amounts of evidence without any earlier knowledge about the image being detected 4 . One important task in this context is the detection of homogeneous places within the image 5 .These so - called features could belong to individual objects 6 , but they may sometimes reflect parts of bigger structures like houses 7 or roads 8 .",
        "rewrite_text": "In this article, we propose the utilization of hierarchical hidden Markov random fields (HHMRFs) as a core model for an unsupervised segmentation algorithm tailored for hyperspectral images. The HHMRFs are constructed by integrating multiple layers of hidden Markov chains, with each layer corresponding to a specific spatial scale. This multiscale framework demonstrates enhanced performance compared to traditional multi-scale methods, and we substantiate its efficacy through experiments conducted on two distinct datasets. Furthermore, we benchmark our results against those obtained from leading-edge algorithms that employ Gaussian mixture models and dense coding techniques.\n\nHyperspectral imaging has gained significant traction in recent years, primarily due to advancements in sensor technology. Unlike conventional color cameras that capture only three spectral bands per pixel, hyperspectral cameras can acquire hundreds of narrow spectral bands simultaneously, resulting in high-dimensional datasets. This increase in data complexity introduces new challenges related to storage requirements and computational processing. In various applications, there is a pressing need for automated analysis of these extensive datasets without prior knowledge of the images being analyzed. A critical aspect of this analysis is the identification of homogeneous regions within the images. These regions, referred to as features, may correspond to individual objects or may represent components of larger structures, such as buildings or roadways. Our research addresses these challenges by leveraging the hierarchical structure of HHMRFs, which allows for more effective segmentation and analysis of hyperspectral images, ultimately contributing to advancements in the field of remote sensing and image processing.",
        "ori-fast-z-score": -0.4472135954999579,
        "water-fast-z-score": 6.8326451225040765,
        "rewrite-fast-z-score": -0.35355339059327373
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Velocity-Dependent Models for Non-Abelian/Entangled String Networks .\nAbstract:\nWe present new models for non-abelian string networks, which are based on the idea that the network is composed by many entangled strings with different velocities and orientations. We show how to construct such velocity-dependent models in terms of Feynman diagrams. In particular we consider two types of diagrams: (i) ladder-like diagrams corresponding to the exchange of gluons between pairs of strings, (ii) cross-ladder like-diagrams describing interactions among three or more strings. The latter type of diagrams can be interpreted as representing junctions where several strings meet at one point. We discuss some properties of these models and compare them with previous results obtained within the framework of Abelian-Higgs model. Finally, we study numerically the evolution of an initial configuration consisting of a single straight string into a complicated tangle of interacting strings using Monte Carlo simulations. This work was supported by the DFG under contract SFB-TR9  Gravitational Physics",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Velocity - Dependent Models for Non - Abelian / Entangled String Networks . Abstract : We create fresh models for non - abelian string systems , which are based on the idea that the network is composed by many entangled strings with various velocities and orientations .We see how to build such velocity - dependent models in terms of Feynman diagrams . In particular we define two forms of diagrams : ( i ) ladder - like diagrams corresponding to the transfer of gluons between pairs of strings , ( ii ) cross - ladder like - diagrams describing relationships among three or more strings .The latter type of diagrams can be interpreted as representing junctions where many strings join at one point . We discuss some properties of these models and link them with previous findings obtained within the framework of Abelian - Higgs system .Finally , we study numerically the evolution of an initial structure comprised of a single straight string into a detailed tangle of interacting strings using Monte Carlo simulations . This research was supported by the DFG under contract SFB - TR9 Gravitational Physics",
        "rewrite_text": "In this article, we present innovative models for non-Abelian string systems, focusing on the intricate nature of networks formed by multiple entangled strings exhibiting diverse velocities and orientations. Our approach utilizes Feynman diagrams to construct these velocity-dependent models, allowing for a systematic representation of interactions within the string network. We introduce two distinct types of diagrams: the first type consists of ladder-like diagrams that illustrate the transfer of gluons between pairs of strings, while the second type features cross-ladder diagrams that capture the interactions among three or more strings. These cross-ladder diagrams can be interpreted as junctions where multiple strings converge at a single point, highlighting the complex interrelations within the network.\n\nWe delve into the properties of these models and draw connections to previous research conducted within the context of the Abelian-Higgs system, thereby situating our findings within the broader landscape of string theory. To further investigate the dynamics of these non-Abelian string networks, we employ Monte Carlo simulations to numerically analyze the evolution of an initial configuration consisting of a single straight string. This simulation reveals the transformation of the initial structure into a complex tangle of interacting strings, providing insights into the behavior and characteristics of such networks over time.\n\nOur research contributes to the understanding of non-Abelian string dynamics and is supported by the Deutsche Forschungsgemeinschaft (DFG) under contract SFB-TR9 in Gravitational Physics. Through this work, we aim to enhance the theoretical framework surrounding string networks and their interactions, paving the way for future explorations in this fascinating area of theoretical physics.",
        "ori-fast-z-score": 0.47140452079103173,
        "water-fast-z-score": 5.114896104728048,
        "rewrite-fast-z-score": 1.4485719366802965
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Feedback Heating by Cosmic Rays in Clusters of Galaxies .\nAbstract:\nWe present the results of our analysis of the effects of cosmic rays on galaxy clusters, using cosmological hydrodynamic simulations with and without CRs. We find that the presence of CRs leads to an increase in gas temperature at large radii (r > 0.5Rvir) due to adiabatic compression of the ICM during cluster formation. The effect is more pronounced for higher values of the initial magnetic field strength. In addition we find that there are significant differences between the radial profiles of X-ray surface brightness obtained with and without CRs which can be used as observational signatures of their presence. Finally, we show that the inclusion of CRs has little impact on the global properties such as total mass or luminosity but does affect the distribution of metals within the cluster. This work was supported by NASA grant NAG5-9998. Cosmological hydrodynamical simulations were performed using Enzo developed by the Laboratory for Computational Astrophysics at the University of California, San Diego.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Feedback Heating by Cosmic Rays in Clusters of Galaxies . Abstract : We present the conclusion of our analysis of the effects of cosmic rays on star clusters , using cosmological hydrodynamic simulations with and without CRs .We see that the presence of CRs leads to an increase in gas temperature at large radii ( r > 0 . 5Rvir ) due to adiabatic compression of the ICM during cluster structure . The phenomenon is more pronounced for greater values of the early magnetic force power .In addition we find that there are significant variations between the radial profiles of X - ray surface brightness produced with and without CRs which can be used as observational signatures of their presence . Finally , we prove that the inclusion of CRs has little impact on the global properties such as total mass or luminosity but does affect the distribution of metals within the cluster .This work was supported by NASA grant NAG5 - 9998 . Cosmological hydrodynamical simulations were performed using Enzo developed by the Laboratory for Computational Astrophysics at the University of California , San Diego .",
        "rewrite_text": "In this study, we investigate the influence of cosmic rays (CRs) on star clusters by employing cosmological hydrodynamic simulations, both with and without the inclusion of CRs. Our findings reveal that the presence of cosmic rays significantly elevates the gas temperature at larger radii (r > 0.5Rvir) due to the adiabatic compression of the intracluster medium (ICM) during the formation of cluster structures. This effect is notably enhanced when the initial magnetic force power is increased. Furthermore, we observe substantial differences in the radial profiles of X-ray surface brightness between simulations that account for CRs and those that do not. These variations can serve as observational signatures indicating the presence of cosmic rays within clusters. Importantly, our analysis demonstrates that while the inclusion of CRs does not substantially alter the global properties of the clusters, such as their total mass or luminosity, it does influence the distribution of metals throughout the cluster environment. This research was supported by NASA grant NAG5-9998, and the cosmological hydrodynamic simulations were conducted using the Enzo code, developed by the Laboratory for Computational Astrophysics at the University of California, San Diego. Our results contribute to a deeper understanding of the role cosmic rays play in the thermal dynamics and chemical enrichment of galaxy clusters, highlighting their potential impact on observational astrophysics.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 2.9104275004359956,
        "rewrite-fast-z-score": 1.1659976680069961
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Demographics of Transition Objects .\nAbstract:\nWe present the demographics and properties of transition objects in SDSS DR7, which are defined as galaxies with both emission lines (ELGs) and absorption features (AGNs). We find that there is an excess number of ELG-AGN pairs at small separations compared to random distributions. The fraction of AGNs among all ELGs increases towards lower luminosities. There appears to be no significant difference between the fractions of AGNs found within different types of ELGs. These results suggest that some ELGs may harbor hidden AGNs. This work was supported by NASA grant NNX10AD65G. We thank the anonymous referee for helpful comments on this manuscript. In recent years, it has been shown that many active galactic nuclei (AGNs), especially those with low luminosity or obscured by dusty torii, have strong emission line components (see e.g., Ho et al. (1997) , Hao et al. (2005) ), making them appear like normal star-forming galaxies when observed through optical spectroscopic surveys such as Sloan Digital Sky Survey (SDSS; York et al. (2000) ) .\nIn order to identify these  transition objects , we use two criteria based on their spectral energy distribution (SED): 1) they must show both emission lines (ELGs; see Section 2.1 below) and absorption features (Section 2.2) simultaneously; and 2) they should not be classified as quasars according to the BPT diagram (Baldwin et al. 1981 , Kewley et al. 2001 . By applying these selection criteria to the entire sample of galaxies in the seventh data release (DR7; Abazajian et al. 2009 ) of the SDSS, we obtain a total of 16,082 transition objects out of a parent sample of 3,962,843 galaxies.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Demographics of Transition Objects . Abstract : We present the demographics and features of transfer objects in SDSS DR7 , which are specified as galaxies with both emission lines ( ELGs ) and emission elements ( AGNs ) .We see that there is an excess amount of ELG - AGN pairs at small separations compared to random distributions . The percentage of AGNs among all ELGs increases towards less luminosities .There seems to be no major variation between the fractions of AGNs discovered within various types of ELGs . These data suggest that some ELGs might harbor hidden AGNs .This project was supported by NASA grant NNX10AD65G . We thank the anonymous referee for helpful remarks on this manuscript .In recent years , it has been shown that several active galactic nuclei ( AGNs ) , particularly those with poor luminosity or obscured by dusty torii , have strong emitted path constituents ( saw e . g . , Ho et al . ( 1997 ) , Hao et al .( 2005 ) ) , making them seem like usual star - creating galaxies when observed through optical spectroscopic studies such as Sloan Digital Sky Survey ( SDSS ; York et al . ( 2000 ) ) .In order to identify these transition objects , we using two requirements depending on their spectral power distribution ( SED ) : 1 ) they must show both emission lines ( ELGs ; seeing Section 2 . 1 below ) and emission elements ( Section 2 . 2 ) simultaneously ; and 2 ) they should not be categorized as quasars according to the BPT diagram ( Baldwin et al . 1981 , Kewley et al .2001 . By applying these selection categories to the entire sample of galaxies in the seventh data update ( DR7 ; Abazajian et al .2009 ) of the SDSS , we obtain a total of 16 , 082 transition objects out of a parent sample of 3 , 962 , 843 galaxies .",
        "rewrite_text": "**Title: Demographics of Transition Objects**\n\n**Abstract:** In this study, we investigate the demographics and characteristics of transition objects identified in the Sloan Digital Sky Survey Data Release 7 (SDSS DR7). Transition objects are defined as galaxies exhibiting both emission lines (ELGs) and emission elements indicative of active galactic nuclei (AGNs). Our analysis reveals a significant overabundance of ELG-AGN pairs at close separations compared to what would be expected from random distributions. Notably, the proportion of AGNs among all ELGs tends to increase with decreasing luminosity. Furthermore, we observe minimal variation in the fraction of AGNs across different types of ELGs, suggesting that a subset of ELGs may contain hidden AGNs that are not immediately apparent. This research was made possible through the support of NASA grant NNX10AD65G, and we extend our gratitude to the anonymous referee for their constructive feedback on this manuscript.\n\nRecent findings indicate that many AGNs, particularly those with lower luminosities or those obscured by dusty tori, exhibit strong emission line features, which can lead to their misclassification as typical star-forming galaxies in optical spectroscopic surveys like the SDSS. To accurately identify these transition objects, we employed two criteria based on their spectral energy distribution (SED): first, they must display both emission lines and emission elements simultaneously; second, they should not be classified as quasars according to the Baldwin-Phillips-Terlevich (BPT) diagram. By applying these selection criteria to the comprehensive galaxy sample from SDSS DR7, we identified a total of 16,082 transition objects from an initial sample of 3,962,843 galaxies. This work enhances our understanding of the complex relationship between star formation and AGN activity in the universe.",
        "ori-fast-z-score": -3.2863353450309964,
        "water-fast-z-score": 4.164644317955852,
        "rewrite-fast-z-score": 0.09245003270420485
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  4U 0115+63 from RXTE and INTEGRAL Data: Pulse Profile and Cyclotron Line Energy .\nAbstract:\nWe have analyzed the pulse profiles in two energy bands (3-20 keV, 20-100 keV) for 4U0115+63 using data obtained by the Rossi X-ray Timing Explorer (RXTE). The results show that there is no significant difference between these two energy bands except at phase 0.7-0.9 where we see an excess emission above 100 keV. We also find that this source shows a double peaked profile with a dip around phase 0.5 which can be explained as due to absorption effects on the line-of-sight. In addition, we have used data from the International Gamma-Ray Astrophysics Laboratory (INTEGRAL), which has detected cyclotrons lines near 30 keV and 60 keV respectively. Using our model parameters derived from the RXTE data analysis, we are able to reproduce both the observed pulse profiles and the cyclotrons line energies simultaneously.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : 4U 0115 + 63 from RXTE and INTEGRAL Data : Pulse Profile and Cyclotron Line Energy . Abstract : We have analyzed the signal profiles in two electricity bands ( 3 - 20 keV , 20 - 100 keV ) for 4U0115 + 63 utilizing information obtained by the Rossi X - ray Timing Explorer ( RXTE ) .The results show that there is no major variation between these two energy bands except at phase 0 . 7 - 0 . 9 where we saw an excess emission above 100 keV . We additionally find that this source shows a double peaked profile with a dip around phase 0 . 5 which can be understood as owing to absorption effects on the line - of - seeing .In addition , we have utilized information from the International Gamma - Ray Astrophysics Laboratory ( INTEGRAL ) , which has detected cyclotrons lines near 30 keV and 60 keV respectively . Using our model variables derived from the RXTE information study , we are able to predict both the seen beam profiles and the cyclotrons line energies concurrently .",
        "rewrite_text": "In this study, we present a comprehensive analysis of the pulse profiles of the X-ray binary system 4U 0115 + 63, utilizing data from the Rossi X-ray Timing Explorer (RXTE) across two energy bands: 3-20 keV and 20-100 keV. Our investigation reveals that the pulse profiles exhibit minimal variation between these energy ranges, with a notable exception occurring between phase 0.7 and 0.9, where we observe an excess emission above 100 keV. This finding suggests a significant feature in the high-energy emission of the source. Furthermore, our analysis indicates that the pulse profile is characterized by a double-peaked structure, with a pronounced dip around phase 0.5. This dip can be interpreted as a result of absorption effects impacting the line-of-sight, which may influence the observed emission characteristics.\n\nIn addition to the RXTE data, we incorporated observations from the International Gamma-Ray Astrophysics Laboratory (INTEGRAL), which has successfully detected cyclotron lines at approximately 30 keV and 60 keV. By employing model parameters derived from our RXTE analysis, we are able to concurrently predict both the observed beam profiles and the energies of the cyclotron lines. This dual approach not only enhances our understanding of the emission mechanisms at play in 4U 0115 + 63 but also provides insights into the underlying physics governing the behavior of X-ray pulsars. Our findings contribute to the broader knowledge of high-energy astrophysical phenomena and underscore the importance of multi-wavelength observations in unraveling the complexities of such systems.",
        "ori-fast-z-score": -0.12803687993289598,
        "water-fast-z-score": 3.9691432779197755,
        "rewrite-fast-z-score": 0.2873478855663454
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Measurement of Ultra-Low Potassium Contaminations with Accelerator Mass Spectrometry .\nAbstract:\nThe measurement of ultra-low potassium contaminations in silicon is important for the development and production of semiconductor devices, especially solar cells. The detection limit of conventional methods such as flame photometry or atomic absorption spectroscopy (AAS) is not sufficient to meet the requirements set by industry standards. In this work we present an alternative method based on accelerator mass spectrometry (AMS). We show that AMS can be used to measure potassium concentrations down to 10(-12) at% K in Si samples. This corresponds to a sensitivity improvement by three orders of magnitude compared to standard techniques like flame photometry. Furthermore, our results demonstrate that AMS has no significant matrix effects when measuring low-potassium doped Si wafers. Finally, we discuss possible applications of AMS beyond its use as a highly sensitive analytical tool. Keywords: Silicon wafer, Accelerator mass spectrometry, Flame photometry",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Measurement of Ultra - Low Potassium Contaminations with Accelerator Mass Spectrometry . Abstract : The measurement of ultra - low potassium contaminations in silicon is important for the development and production of semiconductor devices , particularly solar devices .The diagnostic limit of typical techniques such as flame photometry or atomic absorption spectroscopy ( AAS ) is not reasonable to meet the requirements set by industry standards . In this research we present an additional method based on accelerator mass spectrometry ( AMS ) .We see that AMS can be used to measure potassium levels down to 10 ( - 12 ) at % K in Si samples . This corresponds to a sensitivity change by three orders of magnitude compared to standard methods like fire photometry .Furthermore , our findings show that AMS has no considerable matrix impacts when examining low - potassium doped Si wafers . Finally , we explain possible use of AMS beyond its use as a highly sensitive characterization technique .Keywords : Silicon wafer , Accelerator mass spectrometry , Flame photometry",
        "rewrite_text": "Title: Measurement of Ultra-Low Potassium Contaminations with Accelerator Mass Spectrometry\n\nAbstract: The accurate measurement of ultra-low potassium contaminations in silicon is crucial for the advancement and manufacturing of semiconductor devices, especially in the realm of solar technology. Traditional analytical methods, such as flame photometry and atomic absorption spectroscopy (AAS), often fall short of the stringent detection limits required by industry standards. In this study, we introduce an innovative approach utilizing accelerator mass spectrometry (AMS) to address this challenge. Our results demonstrate that AMS can effectively quantify potassium concentrations as low as 10^(-12) at % K in silicon samples, achieving a sensitivity enhancement of three orders of magnitude compared to conventional techniques like flame photometry. Additionally, our investigation reveals that AMS exhibits minimal matrix effects when analyzing low-potassium doped silicon wafers, further validating its reliability as a measurement tool. Beyond its application as a highly sensitive characterization method, we discuss the potential of AMS in various other contexts within semiconductor research and production. This work not only highlights the advantages of AMS in detecting ultra-low potassium levels but also opens avenues for its broader application in the semiconductor industry. \n\nKeywords: Silicon wafer, Accelerator mass spectrometry, Flame photometry",
        "ori-fast-z-score": -0.23570226039551587,
        "water-fast-z-score": 5.266851623825876,
        "rewrite-fast-z-score": -0.6324555320336759
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Dynamical State fo the Starless Dense Core FeSt 1-457: A Pulsating Globule? .\nAbstract:\nWe present new near-infrared (NIR) and millimeter-wave observations of the starless dense core FeSt 1-457, which is located in the Taurus molecular cloud complex at a distance of 140 pc. The NIR data were obtained with the Subaru telescope using the SofI instrument on 2005 May 24-25 UT. We detected two sources within the central 0.5 arcmin region; one source was found to be associated with an infrared dark cloud (IRDC), while another source was not. Both sources are embedded deeply inside the dusty envelope surrounding the dense core. In addition, we observed this object simultaneously with the Nobeyama 45 m radio telescope at 1 mm wavelength during the same night as our NIR observation. No significant emission line features were seen in either spectrum. Using these observational results, we discuss possible scenarios for the formation of stars in such a young dense core.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  The Dynamical State fo the Starless Dense Core FeSt 1-457: A Pulsating Globule?.Abstract : We report new near - infrared ( NIR ) and millimeter - wave images of the starless dense core FeSt 1 - 457 , which is situated in the Taurus molecular mist complex at a distance of 140 pc . The NIR data were obtained with the Subaru observatory using the SofI instrument on 2005 May 24 - 25 UT .We observed two sources within the inner 0 . 5 arcmin region ; one source was reported to be involved with an infrared shadow cloud ( IRDC ) , while another source was not . Both sources are lodged deeply inside the dusty envelope surrounding the dense core .In addition , we studied this body simultaneously with the Nobeyama 45 m radio telescope at 1 mm frequency during the same night as our NIR observation . No notable emission line characteristics were witnessed in either spectrum .Using these observational results , we explain possible strategies for the formation of stars in such a young dense core .",
        "rewrite_text": "Title: The Dynamical State of the Starless Dense Core FeSt 1-457: A Pulsating Globule?\n\nAbstract: In this study, we present new near-infrared (NIR) and millimeter-wave imaging of the starless dense core FeSt 1-457, located within the Taurus molecular cloud complex at a distance of 140 parsecs. The NIR observations were conducted at the Subaru Observatory using the SofI instrument over the nights of May 24-25, 2005. Our findings reveal two distinct sources within a 0.5 arcminute radius of the core; one of these sources is associated with an infrared dark cloud (IRDC), while the other is not. Both sources are embedded within the dense, dusty envelope that characterizes the core. Concurrently, we utilized the Nobeyama 45 m radio telescope to observe the core at a frequency of 1 mm during the same observational period. Notably, our spectral analysis did not reveal any significant emission line features. These observational results provide critical insights into the dynamical state of FeSt 1-457 and contribute to our understanding of star formation processes in such nascent dense cores. We discuss potential mechanisms that may facilitate star formation in this young core, considering the implications of our findings in the broader context of stellar evolution within molecular clouds. This research enhances our comprehension of the conditions necessary for star formation and the role of dense cores in the lifecycle of molecular clouds.",
        "ori-fast-z-score": -1.212678125181665,
        "water-fast-z-score": 3.8247315498700596,
        "rewrite-fast-z-score": 0.21320071635561041
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Neutronic performances of the MEGAPIE target .\nAbstract:\nThe Megapie (Megavoltage Ionization Projection Imaging Experiment) is an experiment designed to study the feasibility and performance of proton radiography for medical applications. The main goal of this project was to develop a compact, high intensity ion source based on laser-plasma interaction in order to produce protons with energies up to several hundred MeV. In addition, it has been shown that such sources can be used as targets for neutron production by spallation reactions induced by energetic ions. This work presents results obtained during experiments performed at GSI Darmstadt using a pulsed deuteron beam accelerated by the SIS-18 synchrotron accelerator. Neutrons produced by the D+D reaction were detected by means of two fission chambers placed around the target chamber. A detailed analysis of these data allowed us to determine the number of neutrons emitted per incident deuteron particle and their energy distribution.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Neutronic performances of the MEGAPIE objective . Abstract : The Megapie ( Megavoltage Ionization Projection Imaging Experiment ) is an project meant to study the feasibility and performance of proton radiography for medical uses .The main goal of this project was to develop a compact , large intensity ion supply based on laser - plasma interaction in order to produce protons with energies up to several hundred MeV . In addition , it has been shown that such sources can be used as targets for neutron production by spallation reactions generated by energetic ions .This research provides findings obtained during experiments conducted at GSI Darmstadt involving a pulsed deuteron light accelerated by the SIS - 18 synchrotron accelerator . Neutrons created by the D + D process were detected by means of two fission chambers put around the target chamber .A precise analysis of these information helped us to predict the quantity of neutrons emitted per incident deuteron particle and their power distribution .",
        "rewrite_text": "**Title:** Neutronic Performance of the MEGAPIE Project\n\n**Abstract:** The MEGAPIE (Megavoltage Ionization Projection Imaging Experiment) initiative aims to investigate the feasibility and efficacy of employing proton radiography for medical applications. The primary objective of this project is to create a compact, high-intensity ion source utilizing laser-plasma interactions to generate protons with energies reaching several hundred MeV. Furthermore, this research demonstrates that such ion sources can effectively serve as targets for neutron production through spallation reactions induced by high-energy ions. This paper presents findings from experiments conducted at GSI Darmstadt, where a pulsed deuteron beam was accelerated using the SIS-18 synchrotron accelerator. Neutrons produced via the D + D fusion process were detected using two fission chambers strategically positioned around the target chamber. A thorough analysis of the collected data enabled us to accurately predict the neutron yield per incident deuteron and to characterize their energy distribution. These insights are crucial for understanding the potential applications of proton radiography in medical settings and for optimizing neutron production techniques. The results highlight the promising capabilities of the MEGAPIE project in advancing both medical imaging technologies and neutron generation methodologies, paving the way for future research and development in these fields.",
        "ori-fast-z-score": 0.35603449745815596,
        "water-fast-z-score": 6.128258770283413,
        "rewrite-fast-z-score": 0.20412414523193154
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Is Thick Brane Model Consistent with the Recent Observations? .\nAbstract:\nWe have studied the thick brane model in which our universe is embedded into an extra dimension and found that it can explain some recent observations such as cosmic microwave background anisotropy, supernovae Ia data and baryon acoustic oscillations without introducing any new physics beyond standard model. \n \n In this model, we assume that there exists a scalar field whose potential has two degenerate minima at different values of the vacuum expectation value (VEV). The VEVs are determined by the parameters of the potential. We find that if the difference between these VEVs is large enough to satisfy the condition for the existence of stable domain walls then the thickness of the wall becomes larger than the Hubble radius today. This means that the domain walls cannot be produced during inflationary epoch. On the other hand, if the difference between these vacua is small compared to the Hubble scale, the domain walls will be created after inflation but they decay before nucleosynthesis era due to their extremely high tension.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Is Thick Brane Model Consistent with the Recent Observations ? .Abstract : We have researched the deep brane model in which our universe is embedded into an additional dimension and found that it can describe some latest measurements such as cosmic microwave background anisotropy , supernovae Ia data and baryon acoustic oscillations without exploring any new science beyond standard model . In this description , we suppose that there exists a scalar field whose potential has two degenerate minima at different values of the vacuum expectation value ( VEV ) .The VEVs are decided by the variables of the potential . We see that if the difference between these VEVs is huge enough to meet the condition for the existence of stable domain barriers then the length of the wall grows larger than the Hubble diameter today .This implies that the domain barriers cannot be formed during inflationary epoch . On the other hand , if the difference between these vacua is tiny relative to the Hubble scale , the domain barriers will be formed after inflation but they decay before nucleosynthesis era due to their extremely raised tension .",
        "rewrite_text": "Title: Is the Thick Brane Model Compatible with Recent Observations?\n\nAbstract: In this study, we investigate the thick brane model, which posits that our universe is situated within an additional spatial dimension. Our findings indicate that this model can effectively account for several recent astronomical observations, including cosmic microwave background (CMB) anisotropies, Type Ia supernova data, and baryon acoustic oscillations, all without necessitating any new physics beyond the established framework of the standard model. Central to our analysis is the introduction of a scalar field characterized by a potential featuring two degenerate minima, each corresponding to distinct vacuum expectation values (VEVs). The specific values of these VEVs are determined by the parameters of the potential. \n\nWe demonstrate that if the disparity between the two VEVs is sufficiently large to satisfy the criteria for stable domain walls, the resulting wall length exceeds the current Hubble diameter. This condition suggests that the formation of domain walls is precluded during the inflationary phase of the universe. Conversely, if the difference between the VEVs is minimal in comparison to the Hubble scale, the domain walls may emerge post-inflation. However, due to their significantly elevated tension, these walls would decay prior to the nucleosynthesis epoch. Our results provide insights into the viability of the thick brane model in light of contemporary cosmological data, highlighting its potential to reconcile theoretical frameworks with observational evidence while remaining within the bounds of established physics. This research contributes to the ongoing discourse on the nature of our universe and the implications of higher-dimensional theories in cosmology.",
        "ori-fast-z-score": -2.943920288775949,
        "water-fast-z-score": 3.2627549126854696,
        "rewrite-fast-z-score": 0.8392543274162825
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The effect of the relative orientation between the coronal field and new emerging flux: I Global Properties .\nAbstract:\nWe study the global properties of solar active regions by using high-resolution magnetograms, vector magnetic fields (VMB), and photospheric velocity maps obtained with Hinode/SOT/SP. We find that there is an anti-correlation between the inclination angle of the coronal field lines at the PILs and the amount of newly emerged flux in ARs. The correlation coefficient decreases as we go to higher latitudes. This suggests that the emergence of new flux plays an important role for determining the structure of the coronal field above the PILs. In addition, we found that the distribution of the inclination angles of the coronal field line depends on their distance from the center of the sunspot group. The results suggest that the evolution of the coronal field can be understood if one takes into account both the emergence of new flux and the differential rotation. Keywords: Coronal field, Active region",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The impact of the relative attitude between the coronal field and new emerging flux : I Global Properties . Abstract : We research the global properties of solar active regions by using high - resolution magnetograms , vector magnetic fields ( VMB ) , and photospheric velocity maps obtained with Hinode / SOT / SP .We see that there is an counter - correlation between the inclination angle of the coronal field lines at the PILs and the quantity of newly emerged flux in ARs . The correlation coefficient drops as we went to higher latitudes .This implies that the emergence of new flux serves an important role for determining the composition of the coronal field above the PILs . In addition , we learned that the spread of the inclination angles of the coronal field line determines on their distance from the center of the sunspot group .The results propose that the evolution of the coronal field can be understood if one takes into consideration both the emergence of new flux and the differential rotation . Keywords : Coronal field , Active region",
        "rewrite_text": "**Title:** The Impact of the Relative Attitude Between the Coronal Field and Newly Emerging Flux: I. Global Properties\n\n**Abstract:** This study investigates the global characteristics of solar active regions by analyzing high-resolution magnetograms, vector magnetic fields (VMB), and photospheric velocity maps acquired from the Hinode Solar Optical Telescope (SOT) and Spectro-Polarimeter (SP). Our findings reveal a counter-correlation between the inclination angles of coronal field lines at the polarity inversion lines (PILs) and the amount of newly emerged flux within active regions (ARs). Notably, this correlation coefficient decreases at higher latitudes, suggesting that the emergence of new magnetic flux plays a crucial role in shaping the coronal field structure above the PILs. Furthermore, we observed that the distribution of inclination angles of the coronal field lines is influenced by their proximity to the center of the sunspot group. These results indicate that a comprehensive understanding of coronal field evolution necessitates consideration of both the emergence of new flux and the effects of differential rotation. Our research contributes to the broader understanding of solar magnetic dynamics and highlights the intricate relationship between emerging flux and coronal field configurations in active solar regions. \n\n**Keywords:** Coronal field, Active region",
        "ori-fast-z-score": 0.8944271909999159,
        "water-fast-z-score": 4.777777777777778,
        "rewrite-fast-z-score": 2.1081851067789197
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Efficient Multidimensional Data Redistribution for Resizable Parallel Computations .\nAbstract:\nWe present an efficient data redistribution scheme that allows the parallel execution of computations on dynamically resized computational domains in distributed memory environments. The proposed approach is based on multidimensional block-cyclic distributions and exploits locality by using space-filling curves to map blocks onto processors. We show how this technique can be used to efficiently redistribute data between different processor configurations, while minimizing communication overheads. Our experimental results demonstrate significant performance improvements over existing approaches when executing applications with dynamic load balancing requirements. In particular, we achieve speedups up to 3Â compared to state-of-the-art techniques such as the one presented in  1  . \nIntroduction\n\nParallel computing has become increasingly important due to its ability to solve large problems faster than serial computers  2  , but it also presents new challenges related to the distribution of work among multiple processing units  3  .\nIn order to take advantage of parallelism, many algorithms are designed so that they can run simultaneously on several processors  4  . However, these algorithms often require some form of data redistribution during their execution  5  . For example, consider a computation where each process stores part of a dataset (e.g., matrix)  6  . If the number of processes changes at runtime, then all processes need to exchange information about which parts of the dataset they store before continuing  7  . This problem becomes even more challenging if the size of the datasets stored by individual processes varies  8  or if there are dependencies between them  9  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Efficient Multidimensional Data Redistribution for Resizable Parallel Computations . Abstract : We create an efficient information redistribution system that enables the parallel execution of computations on dynamically resized computational regions in distributed storage systems .The proposed approach is based on multidimensional block - cyclic distributions and exploits locality by using space - filling curves to map blocks onto processors . We see how this methodology can be used to easily redistribute information between various processor configurations , while minimizing communication overheads .Our research results show considerable performance improvements over existing techniques when executing tasks with dynamic load balancing requirements . In particular , we accomplish speedups up to 3Â relative to state - of - the - art methods such as the one illustrated in 1 .Introduction Parallel logic has become rapidly crucial due to its able to solve huge problems quicker than serial machines 2 , but it also provides new obstacles linked to the spread of work among multiple processing facilities 3 . In order to take advantage of parallelism , various algorithms are built so that they can run simultaneously on various processors 4 .However , these algorithms often demand some kind of file redistribution during their execution 5 . For instance , consider a computation where each system stores portion of a dataset ( e . g . , matrix ) 6 .If the number of processes changes at runtime , then all processes must to exchange knowledge about which portions of the dataset they contain before continuing 7 . This problem arises even more challenging if the length of the datasets contained by individual processes vary 8 or if there are dependencies between them 9 .",
        "rewrite_text": "**Title:** Efficient Multidimensional Data Redistribution for Resizable Parallel Computations\n\n**Abstract:** In this study, we introduce a novel information redistribution framework designed to facilitate the parallel execution of computations across dynamically resizable computational regions within distributed storage systems. Our approach leverages multidimensional block-cyclic distributions and enhances data locality by employing space-filling curves to effectively map data blocks onto processors. This methodology allows for seamless redistribution of information among varying processor configurations while significantly reducing communication overheads. Our experimental results demonstrate substantial performance gains compared to existing techniques, particularly in scenarios requiring dynamic load balancing. Specifically, we achieve speedups of up to 3 times relative to leading methods currently available in the literature.\n\nThe increasing importance of parallel computing arises from its ability to tackle large-scale problems more efficiently than traditional serial machines. However, this shift towards parallelism introduces new challenges related to the distribution of workloads across multiple processing units. To harness the benefits of parallel execution, various algorithms have been developed to operate concurrently on different processors. Nevertheless, these algorithms often necessitate some form of data redistribution during their execution phases. \n\nFor example, in a scenario where each processor holds a segment of a dataset, such as a matrix, any change in the number of active processes during runtime requires all processors to exchange information regarding the dataset segments they possess before proceeding with computations. This issue becomes even more complex when the sizes of the datasets held by individual processors differ or when there are interdependencies among them. Our research addresses these challenges by providing an efficient and scalable solution for data redistribution, thereby enhancing the overall performance of parallel computations in dynamic environments.",
        "ori-fast-z-score": -1.0606601717798212,
        "water-fast-z-score": 7.893522173763263,
        "rewrite-fast-z-score": 0.08362420100070908
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Resonance and frequency-locking phenomena in spatially extended phytoplankton-zooplankton system with additive noise and periodic forces .\nAbstract:\nWe study the resonance phenomenon for an open-loop control problem in a nonlinear stochastic model describing interactions between phytoplankton (plants) and zooplankton (animals). The main goal is to find optimal values of parameters characterizing external periodic forcing, which maximize the growth rate of planktons. We show that this optimization problem can be reduced to finding solutions of some algebraic equations. In particular, we prove that there exists only one solution corresponding to maximum value of the objective function. Moreover, it turns out that the obtained results are robust with respect to small perturbations of initial conditions. Finally, numerical simulations illustrate our theoretical findings. \n \n Keywords: Stochastic differential equation, Periodic forcing, Resonance, Optimization problems, Nonlinear dynamics \n \n 1 Introduction \n \n Interactions among different species play important role in many natural ecosystems. For example, phytoplankton (algae or plants), living at the base of food chain, provide energy source for other organisms such as zooplankton (fishes or animals). Therefore, understanding how these two populations interact may help us better understand ecosystem functioning. Recently, several mathematical models have been proposed to describe population dynamics of phytoplankton- zooplankton systems  1–3  . These models include deterministic terms representing intrinsic growth rates of both populations and their interaction effects, as well as random fluctuations due to environmental factors. It has been shown that under certain assumptions on the coefficients of the model, its long-term behavior exhibits chaotic attractor  4  , which makes analysis of the system very difficult. On the other hand, if the effect of random fluctuations is neglected then the resulting deterministic model becomes much easier to analyze  5–7  .\n \nIn  8  , authors studied the following model:\n \n \n \n dX(t) = rX(t)(1 - X(t))dt + fX(t)sin(wt)dW(t),\n dY(t) = rY(t)(1 - Y(t))dt + fy(t)sin(w0t)dW(t).\n \n(",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Resonance and frequency - locking processes in spatially extended phytoplankton - zooplankton system with additive noise and periodic pressures . Abstract : We research the resonance phenomenon for an open - loop control problem in a nonlinear stochastic model governing interactions between phytoplankton ( plants ) and zooplankton ( animals ) .The main goal is to find optimal values of constraints characterizing external periodic forcing , which maximize the development growth of planktons . We see that this optimization problem can be reduced to finding solutions of some algebraic equations .In particular , we prove that there exists only one solve corresponding to maximum value of the objective function . Moreover , it turns out that the achieved findings are robust with regard to small perturbations of initial conditions .Finally , numerical simulations highlight our theoretical results . Keywords : Stochastic integral equation , Periodic forcing , Resonance , Optimization problems , Nonlinear dynamics 1 Introduction Interactions among different species play crucial role in different natural ecosystems .For instance , phytoplankton ( algae or plants ) , living at the base of eat chain , provide energy source for other species such as zooplankton ( fishes or organisms ) . Therefore , studying how these two communities interact may assist us better understand ecological functioning .Recently , various computational models have been proposed to explain population behavior of phytoplankton - zooplankton communities 1 – 3 . These models include deterministic terms representing intrinsic development rates of both populations and their interaction influences , as well as random fluctuations owing to environmental factors .It has been shown that under certain assumptions on the coefficients of the model , its long - term behavior presents chaotic attractor 4 , which makes evaluation of the system very difficult . On the other hand , if the impact of random fluctuations is neglected then the resulting deterministic model seems far easy to analyze 5 – 7 .In 8 , authors explored the following model : dX ( t ) = rX ( t ) ( 1 - X ( t ) ) dt + fX ( t ) sin ( wt ) dW ( t ) , dY ( t ) = rY ( t ) ( 1 - Y ( t ) ) dt + fy ( t ) sin ( w0t ) dW ( t ) . (",
        "rewrite_text": "**Title:** Resonance and Frequency-Locking Processes in a Spatially Extended Phytoplankton-Zooplankton System with Additive Noise and Periodic Pressures\n\n**Abstract:** This study investigates the phenomenon of resonance within an open-loop control framework applied to a nonlinear stochastic model that describes the interactions between phytoplankton (the primary producers) and zooplankton (the consumers). The primary objective is to identify optimal parameters for external periodic forcing that enhance the growth rates of both phytoplankton and zooplankton populations. We demonstrate that this optimization challenge can be effectively transformed into solving specific algebraic equations. Notably, we establish the existence of a unique solution that corresponds to the maximum value of the objective function, indicating a clear pathway to optimal growth conditions. Furthermore, our results reveal that the findings are resilient to minor variations in initial conditions, suggesting robustness in the model's predictions. To substantiate our theoretical conclusions, we present numerical simulations that align closely with our analytical results, illustrating the practical implications of our work. \n\nThe interactions between phytoplankton and zooplankton are critical in various ecological systems, as phytoplankton serve as a foundational energy source for zooplankton and, by extension, for higher trophic levels. Understanding these dynamics is essential for elucidating ecological processes. Recent advancements in computational modeling have provided insights into the population dynamics of phytoplankton-zooplankton interactions, incorporating both deterministic factors—such as intrinsic growth rates and interspecies interactions—and stochastic elements arising from environmental variability. Previous studies have indicated that under specific conditions, the long-term behavior of these models can exhibit chaotic dynamics, complicating the analysis of the system. Conversely, neglecting random fluctuations simplifies the deterministic models, making them more tractable. Our research builds on existing literature by exploring a model characterized by stochastic integral equations influenced by periodic forcing, thereby contributing to the understanding of resonance and optimization in ecological contexts.\n\n**Keywords:** Stochastic integral equation, Periodic forcing, Resonance, Optimization problems, Nonlinear dynamics.",
        "ori-fast-z-score": 2.1278980968455716,
        "water-fast-z-score": 8.121035618207516,
        "rewrite-fast-z-score": 2.9793811989685244
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Transient behavior of surface plasmon polaritons scattered at a subwavelength groove .\nAbstract:\nWe study the scattering properties of surface plasmons (SPs) by a single subwavelength groove in an optically thick metal film, which is excited by a normally incident plane wave with circular polarization. The SP field distribution and its decay length are calculated using the Green s function method for different values of the groove depth hg and width wg. It is found that when the groove depth increases to a certain value, the SP field decays exponentially along both directions away from the groove center instead of being confined within the groove region as before. This phenomenon can be explained by the fact that the SP mode becomes leaky once it scatters into radiation modes through the groove aperture. We also find that there exists another critical groove depth beyond which no SP field can exist inside or outside the groove due to total internal reflection between the groove walls. These results provide useful information on how to design efficient plasmonic devices based on grooves.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Transient interactions of surface plasmon polaritons scattered at a subwavelength groove . Abstract : We research the scattering properties of surface plasmons ( SPs ) by a single subwavelength groove in an optically dense metal film , which is excited by a normally incident plane wave with circular polarization .The SP field distribution and its degradation depth are measured using the Green s function method for different values of the groove length hg and width wg . It is found that when the groove length rises to a certain value , the SP field decays exponentially along both directions away from the groove center instead of being confined within the groove region as before .This phenomenon can be described by the fact that the SP mode becomes leaky once it scatters into radiation modes through the groove aperture . We additionally find that there exists another important groove width beyond which no SP field can occur inside or outside the groove due to complete internal reflection between the groove walls .These data provide useful details on how to build efficient plasmonic systems based on grooves .",
        "rewrite_text": "In this study, we investigate the scattering characteristics of surface plasmon polaritons (SPPs) interacting with a single subwavelength groove in a dense metallic film, stimulated by a normally incident plane wave with circular polarization. Utilizing the Green's function method, we analyze the distribution of the SPP field and its decay depth for varying groove dimensions, specifically the length (hg) and width (wg). Our findings reveal a significant transition in the behavior of the SPP field as the groove length increases beyond a critical threshold. At this point, the SPP field begins to decay exponentially in both directions from the center of the groove, rather than remaining confined within the groove region as previously observed. This transition can be attributed to the SPP mode becoming leaky, as it scatters into radiation modes through the aperture of the groove. Furthermore, we identify a critical groove width beyond which the SPP field is entirely absent, both inside and outside the groove. This absence is a result of complete internal reflection occurring between the walls of the groove. The insights gained from this research are instrumental for the design and optimization of advanced plasmonic systems that utilize grooves, paving the way for enhanced applications in fields such as sensing, imaging, and information processing. Overall, our results contribute to a deeper understanding of the transient interactions of SPPs in structured metallic environments, highlighting the intricate relationship between groove geometry and plasmonic behavior.",
        "ori-fast-z-score": -2.251436323159369,
        "water-fast-z-score": 2.7716093126229358,
        "rewrite-fast-z-score": 0.7627700713964739
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The AGN and Gas Disk in the Low Surface Brightness Galaxy PGC045080 .\nAbstract:\nWe present new observations of the low surface brightness galaxy PGC 045080, which is known to host an active galactic nucleus (AGN). We use these data to study the properties of this AGN as well as its relationship with the surrounding gas disk.  The AGN has been detected by previous studies at radio wavelengths using Very Large Array (VLA) observations. In our work we have used VLA archival data along with new observations made with the Karl G. Jansky Very Large Array (JVLA), to detect emission lines associated with the AGN. These include H-alpha,  NII  ,  SII  ,  OIII  , and  CII  . Using these line fluxes we calculate the luminosity of the AGN to be 1.1 x 10^41 erg/sec. This value agrees very closely with that found for other similar galaxies. We also find evidence for outflows on both large and small scales around the AGN.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The AGN and Gas Disk in the Low Surface Brightness Galaxy PGC045080 . Abstract : We report new studies of the low surface brightness universe PGC 045080 , which is known to host an active galactic nucleus ( AGN ) .We use these information to study the properties of this AGN as also as its connection with the nearby gas disk . The AGN has been detected by earlier surveys at radio wavelengths using Very Large Array ( VLA ) observations .In our work we have utilized VLA archival data along with new experiments done with the Karl G . Jansky Very Large Array ( JVLA ) , to identify emission lines associated with the AGN . These include H - alpha , NII , SII , OIII , and CII .Using these line fluxes we determine the luminosity of the AGN to be 1 . 1 x 10 ^ 41 erg / sec . This value agrees very closely with that obtained for other similar galaxies .We additionally find proof for outflows on both large and tiny scales around the AGN .",
        "rewrite_text": "We present a comprehensive study of the low surface brightness galaxy PGC 045080, which is recognized for hosting an active galactic nucleus (AGN). This research aims to explore the characteristics of the AGN and its relationship with the adjacent gas disk. Previous surveys utilizing Very Large Array (VLA) observations have successfully detected the AGN at radio wavelengths. In our investigation, we have leveraged both archival VLA data and new observations conducted with the Karl G. Jansky Very Large Array (JVLA) to identify various emission lines linked to the AGN, including H-alpha, NII, SII, OIII, and CII. By analyzing the fluxes of these emission lines, we calculate the luminosity of the AGN to be approximately 1.1 x 10^41 erg/sec, a value that is consistent with those found in other galaxies of similar nature. Furthermore, our findings provide evidence of outflows occurring on both large and small scales surrounding the AGN. This study not only enhances our understanding of the AGN in PGC 045080 but also contributes to the broader knowledge of the interactions between AGNs and their host galaxies, particularly in the context of low surface brightness environments. The implications of these results may shed light on the role of AGNs in galaxy evolution and the dynamics of gas disks in similar galaxies.",
        "ori-fast-z-score": -0.8819171036881969,
        "water-fast-z-score": 4.093146241443879,
        "rewrite-fast-z-score": 0.21320071635561041
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Spectral estimation on a sphere in geophysics and cosmology .\nAbstract:\nWe present an overview of the state-of-the-art methods for spectral analysis on spherical data, with emphasis on their applications to problems arising in geophysical sciences (e.g., global seismological tomography) and astrophysics (e.g., cosmic microwave background). We also discuss some recent advances in this area that have been made by our group at Columbia University. The main focus is on the development of new algorithms for computing accurate estimates of the power spectrum of signals defined over the surface of the unit sphere using only partial information about these signals. In particular, we consider two classes of methods: those based on the use of spherical harmonic expansions and those based on wavelet transforms. Finally, we briefly describe several open research questions related to the topic discussed here. Spherical data arise naturally in many areas of science including astronomy, meteorology, oceanography, geodesy, and medicine. For example, astronomers routinely collect large amounts of data describing the positions of celestial objects such as stars or galaxies; similarly, weather forecasters gather measurements of atmospheric pressure, temperature, humidity, wind speed, etc., at various locations around the globe. These types of data are often represented mathematically as functions defined over the surface of a sphere.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Spectral estimation on a sphere in geophysics and cosmology . Abstract : We bring an overview of the state - of - the - art methods for spectral evaluation on spherical measurements , with emphasis on their applications to problems arose in geophysical disciplines ( e . g . , global seismological tomography ) and astrophysics ( e . g . , cosmic microwave background ) .We also discuss some latest advances in this area that have been made by our team at Columbia University . The main interest is on the development of new schemes for modeling precise estimates of the power spectrum of waves defined over the surface of the unit sphere utilizing only partial knowledge about these signals .In particular , we investigate two groups of methods : those based on the using of spherical harmonic expansions and those based on wavelet transforms . Finally , we briefly illustrate numerous open scientific questions related to the subject mentioned here .Spherical data arise naturally in multiple fields of science primarily astronomy , meteorology , oceanography , geodesy , and medicine . For instance , astronomers typically gather huge amounts of statistics explaining the places of heavenly objects such as planets or galaxies ; likewise , weather forecasters gather measurements of atmospheric pressure , temperature , moisture , breeze pressure , etc . , at numerous locations around the world .These sorts of measurements are often represented mathematically as functions defined over the surface of a sphere .",
        "rewrite_text": "In this article, we provide a comprehensive overview of the cutting-edge techniques for spectral estimation on spherical data, highlighting their relevance to various challenges encountered in geophysics and cosmology. Our focus is particularly directed towards applications in global seismological tomography and the analysis of the cosmic microwave background. We present recent advancements achieved by our research team at Columbia University, which aim to enhance the accuracy of power spectrum estimates derived from wave phenomena defined on the surface of a unit sphere, even when only partial information about these signals is available. \n\nWe categorize our discussion into two primary methodological approaches: those that utilize spherical harmonic expansions and those that employ wavelet transforms. Each method offers unique advantages in handling the complexities associated with spherical data, which is prevalent across numerous scientific domains, including astronomy, meteorology, oceanography, geodesy, and medicine. For example, astronomers collect extensive datasets that detail the positions of celestial bodies, while meteorologists compile atmospheric measurements—such as pressure, temperature, and humidity—across various global locations. These datasets are often modeled mathematically as functions over the surface of a sphere, necessitating sophisticated spectral estimation techniques.\n\nFurthermore, we outline several unresolved scientific inquiries that arise from this field of study, emphasizing the importance of continued research in spectral estimation on spherical domains. By addressing these questions, we aim to contribute to the broader understanding of spherical data analysis and its implications for both geophysical and cosmological research.",
        "ori-fast-z-score": -2.209379082955976,
        "water-fast-z-score": 6.563952321998544,
        "rewrite-fast-z-score": -0.09090909090909091
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Systematic Scan for 7-colourings of the Grid .\nAbstract:\nWe present an algorithm that systematically scans all possible 7-colourings of the grid, and report on its performance in terms of running time and memory consumption. The algorithm is based on a simple backtracking scheme combined with some heuristics to prune parts of the search space. We have implemented this algorithm using Java 1.6 and tested it on several instances ranging from small grids up to large ones containing more than one million nodes. For each instance we provide detailed information about how much time was spent by our program during colouring as well as how many colours were used. In addition, we also show how these results compare against those obtained by other algorithms proposed recently in the literature. \n \n Keywords: Coloring problems, Computational complexity theory, Graphs, Backtrack search, Heuristic methods, Grid graphs, Integer programming, Optimization problems, Search trees, Time-complexity analysis \n \n \n \n INTRODUCTION \n \n A graph G = (V, E) consists of two sets V and E where V denotes the set of vertices or nodes and E denotes the set of edges between pairs of nodes. An edge e=(u,v) connects node u ∈ V to v ∈ V . If there exists no such connection then e is not included in E. A path P is defined as a sequence of distinct nodes v1 , v2 , … , vn such that vi−1vi ∈ E for i = 2 , 3 , … , n . A cycle C is defined as a path whose first and last nodes are identical. A connected component is a subgraph H of G which has the property that any pair of nodes in H can be joined by a path within H but cannot be joined by paths outside H. A clique K is a complete subgraph of G; that is, every pair of nodes in K is adjacent to each other. A k-clique is a clique consisting of exactly k nodes. A vertex cover S is a subset of V such that every edge in G has at least one endpoint in S. A dominating set D is a subset of V",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Systematic Scan for 7 - colourings of the Grid . Abstract : We create an algorithm that systematically scans all possible 7 - colourings of the grid , and report on its reliability in terms of running time and memory usage .The algorithm is based on a simple backtracking scheme coupled with some heuristics to prune parts of the search space . We have incorporated this algorithm using Java 1 . 6 and demonstrated it on numerous instances ranging from small grids up to large ones featuring more than one million nodes .For each instance we provide comprehensive information about how many time was spent by our system during colouring as also as how many colours were used . In addition , we also demonstrate how these results compare against those acquired by other methods suggested later in the literature .Keywords : Coloring challenges , Computational complexity theory , Graphs , Backtrack search , Heuristic methods , Grid graphs , Integer programming , Optimization problems , Search trees , Time - complexity analysis INTRODUCTION A graph G = ( V , E ) contains of two sets V and E where V denotes the set of vertices or nodes and E denotes the set of edges between pairs of nodes . An edge e = ( u , v ) connects node e ∈ V to v ∈ V .If there exists no such connection then e is not added in E . A path P is characterized as a sequence of distinct nodes v1 , v2 , … , vn such that vi−1vi ∈ E for i = 2 , 3 , … , k . A cycle C is characterized as a path whose first and last nodes are identical .A linked component is a subgraph H of G which has the property that any pair of vertices in H can be joined by a path within H but cannot be joined by roads outside H . A clique K is a complete subgraph of G ; that is , every pair of vertices in K is adjoining to each other . A p - clique is a clique consisting of exactly k nodes .A vertex cover S is a subset of V such that every edge in G has at least one endpoint in S . A dominating setting D is a subset of V",
        "rewrite_text": "**Title:** A Systematic Scan for 7-Colourings of the Grid\n\n**Abstract:** In this study, we present a novel algorithm designed to systematically explore all potential 7-colourings of grid structures. We evaluate the algorithm's performance, focusing on its efficiency in terms of execution time and memory consumption. The core of our approach is a straightforward backtracking method enhanced with heuristics that effectively reduce the search space. Implemented in Java 1.6, our algorithm has been tested across a variety of grid instances, ranging from small configurations to extensive grids containing over one million nodes. For each grid instance, we meticulously document the time taken for the colouring process and the number of colours utilized. Furthermore, we compare our findings with results obtained from alternative methods proposed in the existing literature, providing a comprehensive analysis of our algorithm's effectiveness. \n\n**Keywords:** Colouring challenges, Computational complexity theory, Graph theory, Backtracking search, Heuristic methods, Grid graphs, Integer programming, Optimization problems, Search trees, Time complexity analysis.\n\n**Introduction:** A graph G = (V, E) is defined by two sets: V, which represents the vertices or nodes, and E, which denotes the edges connecting pairs of nodes. An edge e = (u, v) signifies a connection between nodes u and v within V. If no such connection exists, the edge is excluded from E. A path P is defined as a sequence of distinct nodes v1, v2, …, vn, where each pair of consecutive nodes vi−1 and vi is connected by an edge in E for i = 2, 3, …, k. A cycle C is a specific type of path where the first and last nodes are the same. A connected component is a subgraph H of G in which any two vertices can be linked by a path within H, but not by paths outside of it. A clique K is a complete subgraph where every pair of vertices is directly connected. A p-clique refers to a clique consisting of exactly k nodes. Additionally, a vertex cover S is a subset of V ensuring that every edge in G has at least one endpoint within S. A dominating set D is another subset of V that plays a crucial role in graph theory.",
        "ori-fast-z-score": 0.5980503604017327,
        "water-fast-z-score": 6.361416972599782,
        "rewrite-fast-z-score": 0.08137884587711594
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  How state preparation can affect a quantum experiment: Quantum process tomography for open systems .\nAbstract:\nWe present an experimental study on the effect that different initial states have in a quantum experiment, using quantum process tomography (QPT). We prepare three different initial states and perform QPT to reconstruct their density matrices. The results show how the choice of initial state affects the outcome of the experiment. In particular we find that the fidelity between two initial states is not sufficient to determine whether they will give rise to similar or dissimilar outcomes when measured with respect to some observable. This work was supported by EPSRC grant EP/G061794/1. Introduction:-Quantum mechanics has been successfully applied to many fields such as information processing  1  , metrology  2  and sensing  3  . However, it remains unclear what exactly constitutes a  quantum  experiment  4  .\nIn this Letter we consider one aspect of quantum experiments -the role played by the initial state of the system under investigation. It is well known that the measurement statistics depend upon the initial state  5  but there are few studies which investigate how the choice of initial condition influences the final result  6  . Here we use quantum process tomography  7, 8  to examine the influence of the initial state on the outcome of a quantum experiment. Our aim is to understand better how the initial conditions may be chosen so as to maximise the probability of observing certain phenomena  9  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : How state preparation can affect a quantum experiment : Quantum process tomography for open systems . Abstract : We present an research experiment on the impact that different initial states have in a quantum experiment , using quantum process tomography ( QPT ) .We prepare three different initial states and conduct QPT to reconstruct their density matrices . The results show how the chosen of initial state impacts the result of the experiment .In particular we find that the fidelity between two initial states is not reasonable to predict whether they will giving rise to similar or dissimilar behaviors when measured with regard to some observable . This research was supported by EPSRC award EP / G061794 / 1 .Introduction : - Quantum theory has been successfully applied to many fields such as intelligence processing 1 , metrology 2 and perception 3 . However , it remains unsure what actually constitutes a quantum experiment 4 .In this Letter we investigate one element of quantum experiments - the part played by the first state of the system under inquiry . It is well established that the measurement statistics depend upon the initial state 5 but there are few researchers which probe how the selection of initial condition influences the finished result 6 .Here we using quantum process tomography 7 , 8 to examine the impact of the first state on the result of a quantum experiment . Our aim is to consider better how the first parameters might be chosen so as to maximise the probability of discovering particular phenomena 9 .",
        "rewrite_text": "**Title:** The Influence of State Preparation on Quantum Experiments: Quantum Process Tomography for Open Systems\n\n**Abstract:** In this study, we investigate the effects of various initial states on the outcomes of quantum experiments through the application of quantum process tomography (QPT). We prepare three distinct initial states and utilize QPT to reconstruct their corresponding density matrices. Our findings reveal that the selection of the initial state significantly influences the experimental results. Notably, we observe that the fidelity between two initial states does not serve as a reliable predictor for whether they will exhibit similar or divergent behaviors when measured against specific observables. This underscores the complexity of quantum systems and the necessity for careful consideration of state preparation in experimental design. The research is supported by the Engineering and Physical Sciences Research Council (EPSRC) under award EP/G061794/1. \n\nIn the broader context of quantum theory, which has found applications in diverse fields such as information processing, metrology, and perception, the precise definition of what constitutes a quantum experiment remains an area of ongoing inquiry. This letter focuses on a critical aspect of quantum experiments: the role of the initial state of the system being studied. While it is well-documented that measurement statistics are contingent upon the initial state, there is a scarcity of research exploring how the choice of initial conditions impacts the final outcomes of quantum experiments. Through the use of quantum process tomography, we aim to deepen our understanding of how initial parameters can be optimally selected to enhance the likelihood of observing specific quantum phenomena. Our work contributes to the foundational knowledge necessary for advancing experimental quantum physics and improving the design of future quantum experiments.",
        "ori-fast-z-score": -1.4605934866804429,
        "water-fast-z-score": 6.2727272727272725,
        "rewrite-fast-z-score": -0.17677669529663687
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Multimodal nested sampling: an efficient and robust alternative to MCMC methods for astronomical data analysis .\nAbstract:\nWe present multimodal nested sampling (MNS), a novel algorithm that is able to efficiently explore the posterior distribution in high-dimensional parameter spaces, such as those encountered when fitting complex models to observational data sets. MNS combines ideas from simulated annealing with importance sampling techniques to find the global maximum likelihood solution within a given tolerance level. We demonstrate how this method can be used on real-world problems by applying it to two different astrophysics applications: modelling the observed fluxes of gamma-ray bursts using a time-dependent model; and determining the parameters of a binary black hole merger event detected by gravitational waves. In both cases we show that our new approach outperforms existing Markov chain Monte Carlo algorithms. The code implementing these examples will be made publicly available at https://github.com/mns-method/mns-method/tree/master/examples. Multimodal nested sampling (M NS) is a novel algorithm that is capable of exploring the posterior distribution in high dimensional parameter spaces, such as are found when fitting complex models to large observational datasets. It combines ideas from simulated annealling with importance sampling techniques to locate the global maximum likelihood solution to any problem within some specified tolerance. This talk describes the basic principles behind M NS and demonstrates its application to two astrophysics problems: modelling the observed light curves of gamma ray bursts; and determining the physical properties of a binary black hole system inferred from gravitational wave observations.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Multimodal nested survey : an efficient and reliable option to MCMC techniques for astronomical data analysis . Abstract : We create multimodal nested survey ( MNS ) , a novel algorithm that is able to easily examine the posterior distribution in high - dimensional parameter spaces , such as those experienced when fitting large models to observational data sets .MNS mixes ideas from simulated annealing with importance sampling methods to find the global maximum likelihood solution within a given tolerance range . We suggest how this technology can be used on real - time challenges by using it to two different astrophysics applications : modelling the observed fluxes of gamma - ray bursts using a time - dependent model ; and determining the variables of a binary white hole merger event detected by gravitational waves .In both cases we prove that our new approach outperforms previous Markov chain Monte Carlo algorithms . The software implementing these examples will be made formally accessible at https : / / github . com / mns - method / mns - method / tree / master / examples .Multimodal nested sampling ( M NS ) is a novel algorithm that is capable of analyzing the posterior distribution in high dimensional parameter spaces , such as are found when fitting large models to large observational datasets . It combines insights from simulated annealling with importance sampling methods to locate the global maximum likelihood solution to any question within some specified tolerance .This discussion describes the fundamental principles behind M NS and demonstrates its use to two astrophysics issues : modelling the known light curves of gamma ray clusters ; and determining the physical properties of a binary dark hole system inferred from gravitational wave surveys .",
        "rewrite_text": "Title: Multimodal Nested Survey: An Efficient and Reliable Alternative to MCMC Techniques for Astronomical Data Analysis\n\nAbstract: In this study, we introduce the Multimodal Nested Survey (MNS), an innovative algorithm designed to effectively analyze posterior distributions within high-dimensional parameter spaces, which are commonly encountered when fitting extensive models to observational datasets. The MNS algorithm integrates concepts from simulated annealing and importance sampling to efficiently identify the global maximum likelihood solution within a defined tolerance range. We illustrate the practical applications of this technology by applying it to two distinct astrophysical scenarios: first, modeling the observed fluxes of gamma-ray bursts using a time-dependent framework; and second, extracting the parameters of a binary black hole merger event detected through gravitational waves. Our results demonstrate that the MNS approach significantly outperforms traditional Markov Chain Monte Carlo (MCMC) methods in both cases. Furthermore, we will provide access to the software that implements these examples, which can be found at https://github.com/mns-method/mns-method/tree/master/examples. The MNS algorithm represents a substantial advancement in the field of astronomical data analysis, as it not only enhances the efficiency of parameter estimation but also improves the reliability of results obtained from complex models. This discussion elucidates the core principles underlying the MNS methodology and showcases its effectiveness in addressing two critical astrophysical challenges: accurately modeling the light curves of gamma-ray bursts and determining the physical characteristics of binary black hole systems inferred from gravitational wave observations. Through these applications, we highlight the potential of MNS as a robust alternative to existing techniques, paving the way for more accurate and efficient analyses in astrophysics.",
        "ori-fast-z-score": 0.48989794855663565,
        "water-fast-z-score": 7.675067860720625,
        "rewrite-fast-z-score": 0.6963106238227914
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Wide Field Spectrograph (WiFeS) .\nAbstract:\nThe WiFeS instrument is an integral field spectrograph for the Australian National University 2.3m telescope at Siding Spring Observatory, Australia.  It has been in operation since 2005 and was designed to provide high quality optical spectroscopy over a wide range of wavelengths with minimal overheads.   The WiFeS instrument consists of two cameras that are mounted on a common baseplate which sits inside a vacuum chamber attached to the Cassegrain focus of the ANU 2.3m telescope.    Each camera contains a lenslet array that produces a set of images across its focal plane.  These images can be combined into a single data cube using software developed by Dopita et al. (2007).    This data cube provides information about both spatial position and wavelength along each line-of-sight through the object being observed.  In addition to this capability, WiFeS also offers several other advantages including:  - High throughput due to the use of dichroic beam splitters and fibre optic coupling between the lenses and detectors.  - Flexible observing modes ranging from fully automated observations to manual control via remote desktop interface.  - Fast readout times allowing multiple targets to be observed simultaneously or rapid cycling between different targets during a night s observation run.  - Low maintenance requirements as it does not require any moving parts within the vacuum enclosure.  - Excellent stability thanks to the design of the optics bench and the thermal isolation provided by the vacuum enclosure.  - Large format detector arrays providing excellent signal-to-noise ratio even under poor seeing conditions.  - Robust mechanical design ensuring long-term reliability...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Wide Field Spectrograph ( WiFeS ) . Abstract : The WiFeS instrument is an integral field spectrograph for the Australian National University 2 . 3m telescope at Siding Spring Observatory , Australia .It has been in operation since 2005 and was built to provide high quality optical spectroscopy over a broad variety of wavelengths with minimal overheads . The WiFeS instrument consists of two cameras that are installed on a common baseplate which sits inside a vacuum chamber connected to the Cassegrain scope of the ANU 2 . 3m telescope .Each lens includes a lenslet antenna that generates a group of pictures across its focal surface . These photographs can be merged into a single data cube using software created by Dopita et al .( 2007 ) . This data cube provides knowledge about both visual placement and wavelength along each line - of - view through the element being detected .In addition to this capability , WiFeS also provides various other advantages including : - High throughput owing to the using of dichroic laser splitters and fibre optic coupling between the lenses and detectors . - Flexible monitoring mechanisms ranging from fully automated observations to manual control via remote desktop interface .- Fast readout hours permitting multiple targets to be viewed simultaneously or rapid cycling between various targets during a night s observation run . - Low maintenance requirements as it does not require any moving parts within the vacuum enclosure .- Excellent safety due to the design of the optics bench and the thermal confinement provided by the vacuum enclosure . - Large format detector arrays offering good signal - to - noise ratio especially under bad seeing conditions .- Robust mechanical architecture ensuring long - term reliability . . .",
        "rewrite_text": "**Title: The Wide Field Spectrograph (WiFeS)**\n\n**Abstract:** The Wide Field Spectrograph (WiFeS) is an advanced integral field spectrograph designed for the 2.3m telescope at the Australian National University’s Siding Spring Observatory. Since its inception in 2005, WiFeS has been instrumental in delivering high-quality optical spectroscopy across a wide range of wavelengths while maintaining minimal operational overhead. The instrument features two cameras mounted on a shared baseplate within a vacuum chamber, which is directly connected to the Cassegrain focus of the telescope. Each camera is equipped with a lenslet array that captures multiple images across its focal plane. These images can be processed and combined into a comprehensive data cube using specialized software developed by Dopita et al. (2007). This data cube facilitates detailed analysis of both spatial and spectral information along each line of sight through the observed object.\n\nWiFeS offers several significant advantages that enhance its functionality and usability. Its high throughput is achieved through the use of dichroic beam splitters and fiber optic connections between the lenses and detectors, allowing for efficient light collection. The instrument supports a range of operational modes, from fully automated observations to manual control via a remote desktop interface, providing flexibility for various observational strategies. Additionally, the fast readout capabilities enable simultaneous observations of multiple targets or rapid cycling between different objects during a single night’s observing session.\n\nThe design of WiFeS emphasizes low maintenance, as it operates without any moving parts within the vacuum enclosure, contributing to its reliability. Safety is also a priority, with a robust optical bench design and thermal management ensured by the vacuum environment. Furthermore, the large-format detector arrays deliver an excellent signal-to-noise ratio, even under suboptimal seeing conditions, while the overall mechanical architecture is built for durability and long-term performance. These features collectively make WiFeS a powerful tool for astronomical research, capable of addressing a wide array of scientific inquiries.",
        "ori-fast-z-score": -0.8638684255813601,
        "water-fast-z-score": 6.11070705734948,
        "rewrite-fast-z-score": 1.2570787221094177
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Visualizing pair formation on the atomic scale in the high-Tc superconductor Bi2Sr2CaCu2O8+d .\nAbstract:\nThe authors have used scanning tunneling microscopy to study the surface structure and electronic properties of single crystals of the high-temperature cuprate superconductor Bi2Sr2CaCu2O8+d (Bi-2212). They find that, at low temperatures, this material forms pairs of holes which are bound together by an attractive interaction mediated by phonons. The binding energy is found to be about 0.5 eV per hole pair. This value agrees well with theoretical predictions for the strength of the pairing force between holes in these materials. \n \n In addition, they observe that the density of states near the Fermi level shows a strong dependence on the direction along which the crystal is cut. For example, when the crystal is cleaved parallel to its Cu-O planes, it exhibits a large peak in the density of states just below the Fermi level. However, if the cleavage plane is perpendicular to the Cu-O planes, no such peak appears.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Visualizing pair formation on the atomic scale in the high - Tc superconductor Bi2Sr2CaCu2O8 + d . Abstract : The authors have utilized scanning tunneling microscopy to study the surface structure and electronic properties of single crystals of the high - temperature cuprate superconductor Bi2Sr2CaCu2O8 + d ( Bi - 2212 ) .They find that , at low temperatures , this tissue forms pairs of holes which are bound together by an attractive interaction mediated by phonons . The binding energy is found to be about 0 . 5 eV per hole pair .This value agrees well with theoretical expectations for the strength of the pairing force between holes in these structures . In addition , they demonstrate that the density of states near the Fermi level exhibits a powerful relationship on the direction along which the crystal is cutting .For instance , when the crystal is cleaved parallel to its Cu - O planes , it displays a large peak in the density of states just below the Fermi level . However , if the cleavage plane is parallel to the Cu - O planes , no such peak appears .",
        "rewrite_text": "Title: Visualizing Pair Formation at the Atomic Level in the High-Tc Superconductor Bi2Sr2CaCu2O8 + d\n\nAbstract: In this study, the authors employed scanning tunneling microscopy (STM) to investigate the surface structure and electronic characteristics of single crystals of the high-temperature cuprate superconductor Bi2Sr2CaCu2O8 + d (Bi-2212). Their findings reveal that at low temperatures, the material facilitates the formation of hole pairs, which are bound together through an attractive interaction that is mediated by phonons. The binding energy associated with these hole pairs is approximately 0.5 eV, a value that aligns closely with theoretical predictions regarding the strength of the pairing force in such materials. Furthermore, the authors highlight a significant dependence of the density of states near the Fermi level on the orientation of the crystal cleavage. Specifically, when the crystal is cleaved parallel to its Cu-O planes, a pronounced peak in the density of states is observed just below the Fermi level. Conversely, when the cleavage is oriented parallel to the Cu-O planes, this peak is absent. These results provide critical insights into the electronic properties of Bi-2212 and enhance our understanding of the mechanisms underlying high-temperature superconductivity in cuprates. The study not only confirms theoretical models regarding hole pairing but also emphasizes the importance of crystal orientation in determining electronic behavior, paving the way for future research into the intricate properties of high-Tc superconductors.",
        "ori-fast-z-score": 2.393172105652397,
        "water-fast-z-score": 5.887840577551898,
        "rewrite-fast-z-score": 0.502518907629606
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The VIMOS VLT Deep Survey: Tracing the galaxy stellar mass assembly history over the last 8Gyr .\nAbstract:\nWe present here an overview of our survey, which is aimed at tracing the evolution in the number density and luminosity function (LF) of galaxies as a function of their stellar masses up to z ~ 1.5. The sample consists of about 10 000 objects selected by photometric redshifts using deep optical data obtained with FORS2@VLT+WFI@ESO/MPG 2.2m telescope + Spitzer IRAC 3.6 & 4.5 micron bands. We use this dataset to study how the LF evolves for different ranges of stellar masses. In particular we find that the faint-end slope of the LF becomes steeper towards higher redshift indicating that less massive systems are more abundant than today. This result can be explained if star formation activity was more efficient in low-mass halos at high-z compared to local universe. Finally, we compare these results with those derived from other surveys finding good agreement between them.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The VIMOS VLT Deep Survey : Tracing the galaxy stellar mass formation history over the last 8Gyr . Abstract : We present here an overview of our survey , which is aiming at mapping the evolution in the number density and luminosity function ( LF ) of stars as a function of their stellar masses up to z ~ 1 . 5 .The sample consists of about 10 000 bodies chosen by photometric redshifts using deep optical data acquired with FORS2 @ VLT + WFI @ ESO / MPG 2 . 2m telescope + Spitzer IRAC 3 . 6 & 4 . 5 micron bands . We use this dataset to study how the LF evolves for different ranges of stars masses .In particular we find that the faint - end slope of the LF becomes steeper towards higher redshift indicating that less massive systems are more numerous than presently . This result can be understood if star formation activity was more efficient in low - density halos at high - z compared to local universe .Finally , we compare these results with those generated from other surveys obtaining strong approval between them .",
        "rewrite_text": "In this article, we provide a comprehensive overview of the VIMOS VLT Deep Survey, which aims to investigate the evolution of galaxy stellar mass formation over the past 8 billion years. Our research focuses on mapping the changes in the number density and luminosity function (LF) of stars as a function of their stellar masses, extending up to a redshift of approximately z ~ 1.5. The survey encompasses a sample of around 10,000 galaxies selected based on photometric redshifts, utilizing deep optical data collected from the FORS2 instrument at the Very Large Telescope (VLT), the Wide Field Imager (WFI) at the ESO/MPG 2.2m telescope, and the Spitzer Space Telescope's IRAC bands at 3.6 and 4.5 microns.\n\nOur analysis reveals significant insights into the evolution of the luminosity function across various stellar mass ranges. Notably, we observe that the faint-end slope of the luminosity function becomes steeper at higher redshifts, suggesting an increased abundance of less massive systems in the early universe compared to the present day. This finding implies that star formation was likely more efficient in low-density halos during the high-redshift era, contrasting with the conditions observed in the local universe.\n\nFurthermore, we conduct a comparative analysis with results from other surveys, which corroborates our findings and reinforces the validity of our conclusions. This study not only enhances our understanding of galaxy formation and evolution but also contributes to the broader discourse on the mechanisms driving star formation across cosmic time. Through this work, we aim to shed light on the intricate processes that have shaped the stellar mass distribution in galaxies over the last 8 billion years.",
        "ori-fast-z-score": 1.811643254631353,
        "water-fast-z-score": 5.590169943749474,
        "rewrite-fast-z-score": 1.2780193008453875
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Self-Dual Supergravity and Twistor Theory .\nAbstract:\nThe self-duality condition on the curvature tensor is an important ingredient in supergravity theories, but it has been difficult to incorporate into superspace formulations because of its non-Lagrangian nature.  In this work we show how to construct Lagrangians for self-dual supergravities by using twistor theory as our guide.   We first review the basic ideas behind twistor theory and then use these results to develop new techniques that allow us to write down manifestly supersymmetric actions for self-dual supergravitational fields with arbitrary gauge groups.  The resulting action can be written either in terms of chiral or twisted-chiral superfields depending upon whether one uses the light-cone or covariant approach respectively.   Finally, we discuss some applications of these results including the construction of N = 1, D = 4 supergravity coupled to Yang-Mills multiplets. This article is available from: http://arxiv.org/abs/hep-th/0405033",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Self - Dual Supergravity and Twistor Theory . Abstract : The self - duality condition on the curvature tensor is an important ingredient in supergravity models , but it has been difficult to insert into superspace formulations because of its non - Lagrangian existence .In this study we test how to build Lagrangians for self - dual supergravities by using twistor theory as our guide . We first review the fundamental ideas behind twistor theory and then use these results to develop new tools that enable us to write down manifestly supersymmetric movements for self - dual supergravitational fields with arbitrary gauge bands .The resulting action can be written either in terms of chiral or twisted - chiral superfields depending upon whether one uses the light - cone or covariant approach respectively . Finally , we explain some applications of these results including the creation of N = 1 , D = 4 supergravity combined to Yang - Mills multiplets .This section is accessible from : www : / / arxiv . org / abs / hep - th / 0405033",
        "rewrite_text": "Title: Self-Dual Supergravity and Twistor Theory\n\nAbstract: The self-duality condition imposed on the curvature tensor plays a crucial role in the formulation of supergravity models. However, integrating this condition into superspace frameworks has proven challenging due to its non-Lagrangian nature. In this paper, we explore the construction of Lagrangians for self-dual supergravities, utilizing twistor theory as a foundational framework. We begin by revisiting the core principles of twistor theory, which provides a geometric perspective that aids in the formulation of supersymmetric theories. Building upon these principles, we develop innovative methodologies that allow us to construct manifestly supersymmetric actions for self-dual supergravitational fields, accommodating arbitrary gauge choices. The resulting action can be expressed in terms of either chiral or twisted-chiral superfields, depending on whether one adopts a light-cone or covariant approach. Furthermore, we discuss various applications of our findings, including the integration of N = 1, D = 4 supergravity with Yang-Mills multiplets. This research not only advances the understanding of self-dual supergravity but also opens new avenues for exploring the interplay between twistor theory and supersymmetry. For further details, the full article can be accessed at www:/arxiv.org/abs/hep-th/0405033.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.608176875690327,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The time evolution of cosmological redshift as a test of dark energy .\nAbstract:\nWe propose to use the time evolution of cosmological redshifts in order to probe the nature of dark energy, which is one of the most important problems in modern physics and astronomy. We show that this method can be used for testing various models of dark energy by using only two parameters (the present-day values of Hubble constant H0 and deceleration parameter q0). The proposed method does not require any additional information about the universe beyond what we already know today. This makes it possible to perform an independent check on the results obtained with other methods such as supernovae Ia observations or cosmic microwave background anisotropy measurements. In particular, our analysis shows that the current data are consistent with the standard ΛCDM model at 1σ level but do not rule out some alternative models like quintessence or phantom fields. Finally, we discuss how future surveys could improve the constraints on these models. Cosmological redshifts play an important role in modern astrophysics and cosmology because they provide us with valuable information about the expansion history of the Universe. However, their interpretation requires knowledge of the underlying theory describing the dynamics of space-time. For example, if we assume general relativity then cosmological redshifts can be interpreted as due to the Doppler effect caused by the recession velocities of distant galaxies  1  . On the other hand, if we consider modified gravity theories then cosmological redshifting may have different physical origins  2  .\nIn recent years there has been growing interest in studying the possibility of probing the nature of dark energy through its effects on cosmological redshifts  3  -  8  . Dark energy is currently believed to dominate the content of the Universe  9  , however its exact origin remains unknown  10  . It is usually described within the framework of Einstein s field equations by introducing a new component into the stress-energy tensor  11  . Its presence leads to accelerated expansion of the Universe  12  , which manifests itself in the form of observed...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The period evolution of cosmological redshift as a test of dark energy . Abstract : We suggest to use the time progression of cosmological redshifts in order to probe the nature of dark energy , which is one of the most important problems in modern physics and astronomy .We see that this method can be used for studying several models of deep energy by using only two parameters ( the present - day values of Hubble constant H0 and deceleration parameter q0 ) . The proposed approach does not require any additional information about the universe beyond what we already understand today .This gives it able to conduct an independent check on the results derived with other methods such as supernovae Ia observed or cosmic microwave background anisotropy observations . In particular , our analysis shows that the present data are compatible with the standard ΛCDM theory at 1σ level but do not leave out some additional models like quintessence or phantom fields .Finally , we talk how potential study could enhance the limitations on these models . Cosmological redshifts play an important role in modern astrophysics and cosmology because they give us with important information about the expansion history of the Universe .However , their understanding needs expertise of the fundamental theory explaining the dynamics of space - time . For instance , if we suppose general relativity then cosmological redshifts can be interpreted as owing to the Doppler impact caused by the recession velocities of distant galaxies 1 .On the other hand , if we treat modified gravity theories then cosmological redshifting might have different physical origins 2 . In recent years there has been growing interest in investigating the prospect of probing the nature of dark energy through its consequences on cosmological redshifts 3 - 8 .Dark energy is currently suspected to dominate the content of the Universe 9 , however its exact origin remains obscure 10 . It is usually characterized within the framework of Einstein s field equations by introducing a new part into the strain - energy tensor 11 .Its presence causes to accelerated expansion of the Universe 12 , which manifests itself in the form of observed . . .",
        "rewrite_text": "**Title:** The Period Evolution of Cosmological Redshift as a Test of Dark Energy\n\n**Abstract:** In this study, we propose a novel approach to investigate the nature of dark energy by analyzing the temporal evolution of cosmological redshifts. Dark energy remains one of the most significant challenges in contemporary physics and astronomy, and understanding its properties is crucial for unraveling the mysteries of the universe. Our method leverages only two key parameters: the current values of the Hubble constant (H0) and the deceleration parameter (q0), allowing for a streamlined examination of various deep energy models. Importantly, this approach does not necessitate any additional data beyond our existing comprehension of the universe, enabling an independent verification of results obtained through other methodologies, such as observations of Type Ia supernovae and cosmic microwave background anisotropies.\n\nOur analysis reveals that current observational data align with the standard ΛCDM model at a 1σ confidence level, while also accommodating alternative models, including quintessence and phantom fields. We further discuss how future investigations could refine the constraints on these models. Cosmological redshifts are pivotal in modern astrophysics and cosmology, providing critical insights into the universe's expansion history. However, interpreting these redshifts requires a solid understanding of the fundamental theories governing space-time dynamics. For instance, under the framework of general relativity, cosmological redshifts can be attributed to the Doppler effect resulting from the recession velocities of distant galaxies. Conversely, in the context of modified gravity theories, the origins of cosmological redshifting may differ significantly.\n\nRecent years have seen a surge of interest in exploring the implications of dark energy on cosmological redshifts, as dark energy is believed to dominate the universe's energy content, despite its elusive nature. Typically, dark energy is characterized within Einstein's field equations by introducing an additional component to the stress-energy tensor, leading to the accelerated expansion of the universe, which is observable through various astronomical phenomena. This paper aims to contribute to the ongoing discourse surrounding dark energy by providing a fresh perspective on its relationship with cosmological redshifts.",
        "ori-fast-z-score": -0.159111456835146,
        "water-fast-z-score": 7.487048859271456,
        "rewrite-fast-z-score": 1.2893167424406085
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Pairing based cooling of Fermi gases .\nAbstract:\nWe propose to cool fermionic atoms in optical lattices by using the pairing mechanism between two different hyperfine states, which is analogous to Cooper pair formation in superconductors. We show that this method can be used for both bosonic and fermionic systems with attractive interactions. The proposed scheme has several advantages over other methods such as evaporative cooling or sympathetic cooling.  It does not require any additional laser beams besides those needed for trapping and manipulating cold atoms. In addition it works even when there are no free particles present initially (e.g., at zero temperature). Finally we discuss how our proposal could be realized experimentally. Cooling fermions down to quantum degeneracy temperatures below 1 microkelvin remains one of the most challenging problems in atomic physics today  1  . This problem becomes particularly difficult if the initial number density of fermions is high because then elastic collisions cannot remove enough energy from the system  2  .\nIn recent years, however, new experimental techniques have been developed  3, 4  , allowing us to trap and manipulate cold atoms on an unprecedented level  5  . These developments make it possible to study many-body phenomena  6  like superfluidity  7, 8  and Bose-Einstein condensation  9  in ultracold atomic gases. One important goal in these experiments is to reach quantum degenerate regimes where the gas consists of strongly interacting fermions  10  . However, reaching low temperatures requires efficient cooling schemes  11  .\nOne promising approach towards achieving this goal is to use the pairing mechanism  12  . Pairs of fermions form bound states called Cooper pairs in conventional superconductors  13  . Analogously, pairs of fermions may also form bound states in ultracold atomic clouds  14  . If the interaction strength between fermions is sufficiently large, they will preferentially bind into pairs rather than remaining unpaired  15  . Therefore, cooling fermions via pairing should work well even",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Pairing based cooling of Fermi gases . Abstract : We suggest to cool fermionic atoms in optical lattices by using the pairing principle between two different hyperfine states , which is analogous to Cooper couple formation in superconductors .We see that this process can be used for both bosonic and fermionic systems with interesting interactions . The proposed system has numerous benefits over other methods such as evaporative cooling or sympathetic heating .It does not require any additional laser beams besides those required for trapping and manipulating cool ions . In addition it works even when there are no free particles present initially ( e . g . , at zero temperature ) .Finally we talk how our proposal possible be realized experimentally . Cooling fermions down to quantum degeneracy temperatures below 1 microkelvin remains one of the most challenging difficulties in nuclear science today 1 .This problem arises particularly challenging if the first number density of fermions is high because then elastic collisions cannot eliminate much energy from the system 2 . In recent years , however , new theoretical techniques have been built 3 , 4 , allowing us to trapping and manipulate cold molecules on an remarkable level 5 .These advances making it able to study many - bodies phenomena 6 like superfluidity 7 , 8 and Bose - Einstein condensation 9 in ultracold atomic gases . One important aim in these experiments is to reach quantum degenerate regimes where the gas consists of highly interacting fermions 10 .However , obtaining minimum heats needs efficient cooling schemes 11 . One promising alternative towards reaching this goal is to use the pairing principle 12 .Pairs of fermions form bound states called Cooper pairs in standard superconductors 13 . Analogously , pairs of fermions may also create bound states in ultracold atomic clouds 14 .If the interaction strength between fermions is sufficiently great , they will preferentially bind into couples rather than keeping unpaired 15 . Therefore , cooling fermions via pairing should work better even",
        "rewrite_text": "In this article, we propose a novel method for cooling fermionic atoms in optical lattices by leveraging the pairing principle between distinct hyperfine states, drawing a parallel to the formation of Cooper pairs in superconductors. This approach is applicable to both bosonic and fermionic systems exhibiting intriguing interactions. Our method presents several advantages over traditional cooling techniques, such as evaporative cooling and sympathetic heating. Notably, it does not necessitate additional laser beams beyond those already employed for trapping and manipulating cold ions, and it remains effective even in the absence of free particles at zero temperature.\n\nThe challenge of cooling fermions to quantum degeneracy temperatures below 1 microkelvin is a significant hurdle in nuclear science, particularly when dealing with high initial fermion number densities, as elastic collisions are insufficient to dissipate energy effectively. Recent advancements in theoretical techniques have enabled the trapping and manipulation of cold molecules with remarkable precision, facilitating the exploration of many-body phenomena such as superfluidity and Bose-Einstein condensation in ultracold atomic gases. A key objective of these investigations is to achieve quantum degenerate regimes characterized by highly interacting fermions, which necessitates efficient cooling strategies.\n\nOur proposed pairing-based cooling mechanism offers a promising avenue toward this goal. In conventional superconductors, fermions form bound states known as Cooper pairs. Similarly, in ultracold atomic clouds, pairs of fermions can also establish bound states if the interaction strength is sufficiently high, leading to a preference for pairing over remaining unpaired. Consequently, we anticipate that cooling fermions through this pairing process will yield superior results, enhancing our ability to explore the rich physics of ultracold fermionic systems. We conclude by discussing the potential experimental realizations of our proposal, which could significantly advance the field of ultracold atomic physics.",
        "ori-fast-z-score": -0.3223291856101521,
        "water-fast-z-score": 7.045638767423834,
        "rewrite-fast-z-score": -0.08362420100070908
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Revisiting the Fermi Golden Rule: Quantum Dynamical Phase Transition as a Paradigm Shift .\nAbstract:\nWe revisit the foundations of quantum mechanics by introducing an alternative formulation to the standard Feynman path integral approach, which is based on the concept of dynamical phase transition (DPT). We show that this new formalism provides a natural description for the emergence and evolution of macroscopic order in open quantum systems. In particular we demonstrate how it can be used to describe the spontaneous emission process in atomic physics, where the atom-field interaction leads to the formation of collective states with well-defined photon number statistics. The proposed framework also allows us to study the dynamics of many-body interacting systems beyond mean field theory. Finally, we discuss possible applications of our results to condensed matter physics and quantum information science. Introduction:-The development of modern theoretical approaches has led to significant progress in understanding the physical properties of complex quantum systems  1  . However, despite these advances there are still fundamental questions about the nature of quantum phenomena that remain unanswered  2  .\nIn recent years, several authors have attempted to address some of these issues using concepts borrowed from statistical mechanics  3  , such as entropy  4  or free energy  5  . These ideas were originally developed within the context of classical thermodynamics  6  but they have been recently extended to the realm of quantum mechanics  7, 8  . For example, one may consider the von Neumann entropy S = −Tr(ρ ln ρ) associated with the density matrix ρ describing the state of a system  9  . This quantity measures the amount of uncertainty present in the measurement outcomes  10  and its time derivative dS/dt gives rise to the so-called entropy production rate  11  . It was shown that this latter quantity plays a crucial role in characterizing the irreversible behavior of closed quantum systems  12  . More specifically, if the entropy production rate vanishes then the corresponding quantum mechanical model exhibits reversible dynamics  13  . On the other hand, when the entropy production rate becomes positive the system undergoes a non-equilibrium phase transition  14  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Revisiting the Fermi Golden Rule : Quantum Dynamical Phase Transition as a Paradigm Shift . Abstract : We revisit the foundations of quantum mechanics by offering an additional formulation to the standard Feynman path integral approach , which is based on the idea of dynamical phase shift ( DPT ) .We suggest that this new formalism gives a natural explanation for the emergence and evolution of macroscopic order in open quantum systems . In particular we prove how it can be used to explain the spontaneous emission mechanism in atomic physics , where the atom - field interaction results to the formation of collective states with good - defined photon number statistics .The proposed framework also enables us to study the dynamics of several - bodies interacting networks beyond mean field theory . Finally , we study possible use of our findings to condensed matter science and quantum information physics .Introduction : - The advance of modern theoretical methods has led to significant progress in understanding the physical properties of complex quantum systems 1 . However , despite these developments there are still vital questions about the nature of quantum phenomena that continue unanswered 2 .In recent years , various published have tried to tackle some of these problems using concepts borrowed from statistical mechanics 3 , such as entropy 4 or free energy 5 . These concepts were formerly advanced within the context of classical thermodynamics 6 but they have been lately extended to the domain of quantum mechanics 7 , 8 .For instance , one may define the von Neumann entropy S = −Tr ( ρ ln ρ ) associated with the density function ρ describing the state of a system 9 . This quantity estimates the extent of uncertainty found in the measurement outcomes 10 and its time derivative dS / dt gives rise to the so - called entropy production efficiency 11 .It was shown that this latter quantity plays a crucial role in characterizing the irreversible behavior of closed quantum systems 12 . More specifically , if the entropy production level vanishes then the associated quantum mechanical model shows reversible dynamics 13 .On the other hand , when the entropy production level gets positive the process undergoes a non - equilibrium phase change 14 .",
        "rewrite_text": "**Title:** Revisiting the Fermi Golden Rule: Quantum Dynamical Phase Transition as a Paradigm Shift\n\n**Abstract:** In this article, we explore the foundational aspects of quantum mechanics by introducing a novel formulation that complements the traditional Feynman path integral approach, centered around the concept of dynamical phase shift (DPT). This innovative framework provides a compelling explanation for the emergence and development of macroscopic order within open quantum systems. We demonstrate its applicability in elucidating the spontaneous emission phenomenon in atomic physics, where interactions between atoms and fields lead to the emergence of collective states characterized by well-defined photon number statistics. Furthermore, our formalism allows for an in-depth analysis of the dynamics of multi-body interacting networks, extending beyond the limitations of mean field theory. We also investigate the implications of our findings for fields such as condensed matter physics and quantum information science. \n\nThe advancement of modern theoretical techniques has significantly enhanced our understanding of the intricate physical properties of complex quantum systems. Nevertheless, numerous fundamental questions regarding the nature of quantum phenomena remain unresolved. Recent literature has attempted to address some of these challenges by incorporating concepts from statistical mechanics, such as entropy and free energy, which were originally developed within classical thermodynamics but have since been adapted to quantum mechanics. For example, the von Neumann entropy, defined as S = −Tr(ρ ln ρ), where ρ represents the density matrix of a system, quantifies the uncertainty inherent in measurement outcomes. The time derivative of this entropy, dS/dt, is associated with entropy production efficiency, a critical factor in characterizing the irreversible dynamics of closed quantum systems. Specifically, a vanishing level of entropy production indicates reversible dynamics, while a positive entropy production level signifies a non-equilibrium phase transition. Our work aims to bridge these concepts, offering new insights into the behavior of quantum systems and paving the way for future research in this evolving field.",
        "ori-fast-z-score": 1.507556722888818,
        "water-fast-z-score": 8.794258527633097,
        "rewrite-fast-z-score": 1.9093374820217521
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Anatomy of Malicious Singularities .\nAbstract:\nWe present an anatomical study on the structure and evolution of malicious singularities in complex networks, which are defined as nodes that have a disproportionately large number of connections to other nodes. We show how these nodes can be identified by their topological properties using statistical mechanics methods. In particular we find that the degree distribution of such nodes follows a power law with exponent 2.5 ± 0.1 for all studied real-world networks ranging from social networks like Facebook or Twitter to biological networks like metabolic pathways or protein-protein interaction networks. This finding is consistent across different network sizes and types. The results presented here provide new insights into the organization principles of complex systems and may help to identify key players within them. Complex networks play a crucial role in many fields including physics, biology, sociology, computer science, engineering, economics, etc., where they represent interactions between entities  1  . A common feature of most complex networks is the presence of so-called  hubs  -highly connected nodes  2  , whose removal often has dramatic effects  3  .\nIn this work we focus on identifying hubs in complex networks based solely on their topological features. To do so, we use statistical mechanics techniques  4  to analyze the degree distributions of several real world networks  5  . Our analysis reveals that the degree distribution of hubs follows a power-law  6  with exponent 2.5±0.1 independently of the size and type of the considered network (see Fig.  1 ). Interestingly, our findings are also valid when considering only the largest component of each network  7, 8  . These results suggest that the observed scaling behavior is not due to finite-size effects but rather reflects some fundamental property of complex networks  9  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Anatomy of Malicious Singularities . Abstract : We present an anatomical research on the form and evolution of malicious singularities in complex networks , which are specified as nodes that have a disproportionately huge amount of links to other connections .We see how these layers can be identified by their topological properties using statistical mechanics algorithms . In particular we find that the degree distribution of such nodes follows a power law with exponent 2 . 5 ± 0 . 1 for all explored real - world networks including from social groups like Facebook or Twitter to biological organizations like genetic pathways or protein - gene interaction systems .This found is consistent across different network types and types . The results presented here provide fresh insights into the organization rules of complex systems and may assist to identify key players within them .Complex networks take a crucial role in different fields including science , biology , sociology , computer science , engineering , economics , etc . , where they represent interactions between entities 1 . A characteristic characteristic of most complex networks is the presence of so - called hubs - highly connected connections 2 , whose separation frequently has dramatic effects 3 .In this research we focus on establishing hubs in complex networks based primarily on their topological features . To do so , we using statistical mechanics method 4 to analyze the degree distributions of several real life networks 5 .Our study reveals that the degree distribution of hubs follows a power - law 6 with exponent 2 . 5±0 . 1 independently of the height and class of the considered network ( see Fig . 1 ) .Interestingly , our findings are also valid when using only the greatest element of each system 7 , 8 . These conclusions show that the reported scaling behavior is not due to finite - length effects but rather displays some fundamental property of complex networks 9 .",
        "rewrite_text": "**Title: Anatomy of Malicious Singularities**\n\n**Abstract:** In this study, we conduct a detailed examination of the structure and development of malicious singularities within complex networks, defined as nodes that exhibit an exceptionally high number of connections relative to others. We demonstrate that these singularities can be effectively identified through their topological characteristics by employing statistical mechanics algorithms. Our analysis reveals that the degree distribution of these nodes adheres to a power law with an exponent of 2.5 ± 0.1 across a diverse array of real-world networks, encompassing social networks such as Facebook and Twitter, as well as biological systems like genetic pathways and protein-gene interaction networks. This consistent finding across various types of networks underscores the robustness of the observed scaling behavior.\n\nComplex networks play a vital role in numerous disciplines, including science, biology, sociology, computer science, engineering, and economics, as they illustrate the interactions among different entities. A notable feature of many complex networks is the presence of hubs—nodes that are highly interconnected—which can significantly influence the dynamics of the entire network. Our research specifically targets the identification of these hubs based on their topological attributes. Utilizing statistical mechanics methods, we analyze the degree distributions of multiple real-life networks.\n\nOur findings indicate that the degree distribution of hubs consistently follows a power law with an exponent of 2.5 ± 0.1, regardless of the network type or class examined. Notably, this scaling behavior persists even when focusing solely on the most significant elements within each network. These results suggest that the observed power-law distribution is not merely a consequence of finite-size effects but rather reflects a fundamental characteristic of complex networks. This research contributes valuable insights into the organizational principles governing complex systems and may aid in pinpointing influential nodes within these networks.",
        "ori-fast-z-score": -0.3223291856101521,
        "water-fast-z-score": 7.526023228839096,
        "rewrite-fast-z-score": -0.08032193289024989
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Cascading Behavior in Large Blog Graphs .\nAbstract:\nWe study cascades on large blog graphs, where each node is an individual blogger and edges represent links between blogs. We propose a novel cascade model that captures the fact that people are more likely to read posts by their friends than random posts. Our main contributions are: (1) we develop efficient algorithms for computing the cascade size distribution under our model; (2) we show how to use these results to estimate the number of active users at any given time during a cascade; (3) we demonstrate the effectiveness of our approach using data collected from LiveJournal.com. The Web has become one of the most important communication channels today. In particular, social networks such as Facebook or Twitter have attracted millions of users who share information with others through online messages known as tweets or status updates. These messages can be seen by all followers of the user posting them, which may cause further propagation of the message within the network. This phenomenon is called viral marketing  1  , and it has been studied extensively over recent years  2  . However, despite its importance, there still remain many open questions about the dynamics of this process  3  .\nIn this work, we focus on studying cascades on large blogging communities, where each node represents an individual blogger and edges connect pairs of blogs written by the same person  4  . A cascade starts when some blogger writes a post containing a URL pointing to another blog s page. Then, if her readers click on the link, they will visit the other blog and possibly continue reading additional posts. As shown in Figure 1 , the resulting graph contains several connected components representing different topics discussed by the community members.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Cascading Behavior in Large Blog Graphs . Abstract : We research cascades on huge website graphs , where each node is an individual blogger and edges represent connections between blogs .We suggest a novel cascade model that captures the fact that individuals are more likely to see posts by their fans than random posts . Our main contributions are : ( 1 ) we develop fast algorithms for modeling the cascade size distribution under our model ; ( 2 ) we study how to use these results to estimate the total of active participants at any given time during a cascade ; ( 3 ) we prove the ability of our approach using data taken from LiveJournal . com .The Web has become one of the most important information networks today . In particular , social channels such as Facebook or Twitter have attracted millions of viewers who share data with others through online texts known as tweets or status updates .These messages can be saw by all supporters of the user sending them , which would cause further transmission of the message within the network . This phenomenon is dubbed viral marketing 1 , and it has been studied thoroughly over recent years 2 .However , despite its significance , there still continue several open questions about the dynamics of this process 3 . In this project , we focus on studying cascades on huge blogging environments , where each node symbol an individual blogger and edges connect sets of blogs written by the same people 4 .A cascade starts when some blogger writes a post containing a URL pointing to another blog s page . Then , if her viewers flip on the link , they will access the other blog and maybe continue reading additional posts .As seen in Figure 1 , the resulting graph contains multiple connected parts representing different issues discussed by the group members .",
        "rewrite_text": "In this study, we investigate the phenomenon of cascades within extensive blog networks, where each node represents an individual blogger and the edges denote the connections between various blogs. We introduce an innovative cascade model that reflects the tendency of individuals to engage more with posts from their followers rather than with random content. Our primary contributions include: (1) the development of efficient algorithms to model the distribution of cascade sizes according to our proposed framework; (2) an analysis of how these findings can be utilized to estimate the number of active participants at any point during a cascade; and (3) validation of our methodology using empirical data sourced from LiveJournal.com.\n\nThe internet has evolved into a crucial information network, with social media platforms like Facebook and Twitter drawing millions of users who disseminate information through online posts, commonly referred to as tweets or status updates. These messages are visible to all followers of the user, potentially leading to further propagation of the content throughout the network—a process known as viral marketing. Despite extensive research on this topic, several questions regarding the dynamics of such cascades remain unanswered.\n\nIn our research, we specifically concentrate on the cascading behavior within large blogging platforms, where each node signifies a blogger and the edges illustrate the interconnections among blogs authored by the same individuals. A cascade is initiated when a blogger publishes a post that includes a URL linking to another blog. If their followers click on this link, they are directed to the other blog and may continue to explore additional posts. The resulting network structure, as depicted in our analysis, reveals multiple interconnected components that represent various topics discussed among the blogging community. This work aims to enhance our understanding of the mechanisms driving information spread in digital environments.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 8.607101241464626,
        "rewrite-fast-z-score": 0.9198662110077999
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dual Field Theories In (d-1)+1 Emergent Spacetimes From A Unifying Field Theory In d+2 Spacetime .\nAbstract:\nWe present the first dual field theory in emergent spacetime, which is derived from a unifying field theory in higher dimensional spacetime. We show that this new dual field theory can be used to describe both quantum and classical physics with one single unified description. This new dual field theory has several advantages over other existing theories such as string/M-theory or loop quantum gravity. First, it provides an explicit mathematical formulation for describing physical phenomena at all scales ranging from microscopic scale down to macroscopic scale. Second, unlike string/M-theory or LQG, our new dual field theory does not require any extra dimensions beyond those already observed experimentally. Third, we provide a concrete example showing how our new dual field theory works by deriving Einstein s general relativity from our new dual field theory. Finally, we also derive Maxwell s equations from our new dual field... \nIntroduction:-In recent years there have been many attempts to develop a fundamental theory of everything(TOE). String/M-theory  1  , Loop Quantum Gravity  2  are two examples of these efforts. However, despite their successes they still suffer from some problems. For instance, string/M-theory requires extra dimensions  3  while loop quantum gravity suffers from non-renormalizability  4  . These difficulties motivate us to look for alternative approaches towards developing TOEs. Recently, a novel approach called  emergent spacetime  was proposed  5, 6  . According to this approach, space-time emerges from a more fundamental level  7, 8  .\nEmergent spacetime:-The idea behind emergent spacetime is very simple. It states that space-time is not fundamental but rather emerges from a more fundamental entity. To see why this might happen consider the following argument. Imagine you are sitting on your couch watching TV. You will probably say that the world around you looks flat because if you were standing up then you would notice that the ground below you is curved. Now imagine yourself floating above Earth. If you were standing up now then you wouldn t feel like you re standing on a curved surface anymore. Instead you d feel like you re standing on top of a",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dual Field Theories In ( d - 1 ) + 1 Emergent Spacetimes From A Unifying Field Theory In d + 2 Spacetime . Abstract : We introduce the first dual field model in emergent spacetime , which is developed from a unifying field theory in higher dimensional spacetime .We see that this new dual field model can be used to explain both quantum and classical physics with one single unified description . This new dual field model has numerous benefits over other existing ideas such as string / M - theory or loop quantum gravitational .First , it gives an explicit mathematical formulation for describing physical phenomena at all scales ranging from microscopic scale down to macroscopic scale . Second , unlike string / M - theory or LQG , our new dual field model does not require any additional dimensions beyond those already detected experimentally .Third , we provide a clear example showing how our new dual field model operates by deriving Einstein s general relativity from our new dual field theory . Finally , we also generate Maxwell s equations from our new dual field . . . Introduction : - In recent history there have been many efforts to develop a basic theory of things ( TOE ) .String / M - theory 1 , Loop Quantum Gravity 2 are two examples of these attempts . However , despite their successes they still suffer from some problems .For instance , string / M - theory requires added dimensions 3 while loop quantum gravitational loses from non - renormalizability 4 . These difficulties motivate us to search for alternative approaches towards developing TOEs .Recently , a new approach called emergent spacetime was suggested 5 , 6 . According to this methodology , space - time arises from a more fundamental level 7 , 8 .Emergent spacetime : - The idea behind emergent spacetime is very simple . It says that space - time is not essential but rather emerges from a more fundamental entity .To see why this might happen think the following argument . Imagine you are sat on your couch watching TV .You will probably say that the world around you looks flat because if you were standing up then you might see that the earth below you is curved . Now imagine yourself rising above Earth .If you were standing up now then you wouldn t felt like you re standing on a curved surface anymore . Instead you d feel like you re standing on top of a",
        "rewrite_text": "**Title:** Dual Field Theories in (d - 1) + 1 Emergent Spacetimes from a Unifying Field Theory in d + 2 Spacetime\n\n**Abstract:** In this article, we present a pioneering dual field model situated within the framework of emergent spacetime, derived from a unifying field theory formulated in a higher-dimensional spacetime context. This innovative model offers a comprehensive framework that seamlessly integrates both quantum and classical physics into a single coherent description. The dual field model exhibits several advantages over established theories such as string/M-theory and loop quantum gravity (LQG). Firstly, it provides a clear mathematical formulation capable of addressing physical phenomena across a broad spectrum of scales, from the microscopic to the macroscopic. Secondly, unlike string/M-theory and LQG, our model does not necessitate the introduction of additional dimensions beyond those already confirmed through experimental observations. Thirdly, we illustrate the operational mechanics of our dual field model by deriving Einstein's general relativity directly from its principles. Furthermore, we demonstrate the derivation of Maxwell's equations within the context of our dual field framework. \n\nIn the introduction, we discuss the historical context of efforts to formulate a Theory of Everything (TOE), highlighting notable attempts such as string/M-theory and loop quantum gravity. Despite their achievements, these theories encounter significant challenges; for instance, string/M-theory's reliance on extra dimensions and the non-renormalizability issues faced by LQG. These limitations have prompted the exploration of alternative frameworks for developing TOEs. Recently, the concept of emergent spacetime has emerged as a promising avenue, positing that spacetime itself is not fundamental but rather arises from a deeper, more fundamental reality. This perspective invites a re-evaluation of our understanding of spacetime, suggesting that its apparent flatness, as experienced in everyday life, may be a result of our limited vantage point. By elevating our perspective, we can begin to appreciate the underlying curvature of spacetime, akin to observing the Earth from a height where its true shape becomes evident. This article aims to delve deeper into these ideas, presenting a robust theoretical foundation for the dual field model and its implications for our understanding of the universe.",
        "ori-fast-z-score": 0.30151134457776363,
        "water-fast-z-score": 6.274210251953172,
        "rewrite-fast-z-score": 0.23942606534028665
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Simulations of the Boundary Layer Between a White Dwarf and its Accretion Disk .\nAbstract:\nWe present results from three-dimensional hydrodynamic simulations of accreting white dwarfs in close binary systems, focusing on the boundary layer between the disk and the star. We find that the flow is highly turbulent with strong shocks forming at the interface between the two flows. The density structure shows significant departures from spherical symmetry due to the presence of spiral arms which form as a result of the interaction between the stellar magnetic field and the gas stream flowing towards the surface of the white dwarf. These spiral arms are responsible for driving an outflow along the polar axis of the system. In addition we find evidence for large-scale convection cells within the boundary layer. Our models suggest that the observed X-ray emission may be produced by these convective motions rather than by shock heating alone. This work was supported by NASA grant NAG5-7262. \n \n Keywords: Hydrodynamics; Shock waves; Convection",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Simulations of the Boundary Layer Between a White Dwarf and its Accretion Disk . Abstract : We report findings from three - dimensional hydrodynamic simulations of accreting white dwarfs in close binary systems , concentrating on the boundary layer between the disk and the star .We see that the flow is strongly chaotic with powerful shocks producing at the interface between the two flows . The density structure exhibits significant departures from spherical symmetry thanks to the presence of spiral arms which occur as a product of the interaction between the stellar magnetic force and the gas stream running towards the surface of the white dwarf .These spiral arms are responsible for controlling an outflow along the polar axis of the system . In addition we find proof for large - scale convection cells within the boundary layer .Our models suggest that the seen X - ray radiation may be made by these convective movements rather than by shock heating alone . This research was supported by NASA grant NAG5 - 7262 .Keywords : Hydrodynamics ; Shock currents ; Convection",
        "rewrite_text": "Title: Simulations of the Boundary Layer Between a White Dwarf and its Accretion Disk\n\nAbstract: This study presents the results of three-dimensional hydrodynamic simulations focused on the boundary layer of accreting white dwarfs in close binary systems. Our investigation emphasizes the complex dynamics at the interface between the accretion disk and the white dwarf star. The simulations reveal that the flow within this boundary layer is highly chaotic, characterized by the formation of powerful shock waves that arise from the interaction of the two distinct flow regimes. Notably, the density distribution in this region deviates significantly from spherical symmetry, primarily due to the emergence of spiral arms. These spiral structures result from the interplay between the stellar magnetic field and the gas stream that is directed toward the surface of the white dwarf. The presence of these spiral arms plays a crucial role in facilitating an outflow along the polar axis of the system, which has implications for the overall dynamics of the accretion process. Furthermore, our findings indicate the existence of large-scale convection cells within the boundary layer, suggesting that the X-ray emissions observed in these systems may be attributed to these convective motions rather than being solely a product of shock heating. This research contributes to a deeper understanding of the complex interactions occurring in the boundary layers of accreting white dwarfs and highlights the importance of considering both hydrodynamic and magnetic effects in such astrophysical environments. The work was conducted with the support of NASA grant NAG5-7262. \n\nKeywords: Hydrodynamics; Shock waves; Convection.",
        "ori-fast-z-score": -0.3333333333333333,
        "water-fast-z-score": 4.638124095143555,
        "rewrite-fast-z-score": 1.1239029738980328
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Physical Layer Network Coding .\nAbstract:\nIn this thesis, we study the problem of physical layer network coding (PLNC) in wireless networks with multiple relays and single-antenna nodes.  We first consider PLNC for two-way relay channels where each node has only one antenna.  In particular, we propose an efficient scheme to perform PLNC at both source nodes simultaneously by using linear codes over finite fields.  Then, we extend our results to multi-way relay channels with more than two users.  Finally, we investigate the performance of PLNC under imperfect channel state information (CSI). The main contributions are summarized as follows:  1. Two-way Relay Channels: We propose a novel approach to perform PLNC at the sources simultaneously based on linear codes over finite fields.   2. Multi-Way Relay Channels: By extending our previous work, we develop a new method to perform PLNC at all source nodes simultaneously.  3. Imperfect Channel State Information: We analyze the impact of imperfect CSI on the performance of PLNC schemes.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Physical Layer Network Coding . Abstract : In this dissertation , we study the issue of physical layer network coding ( PLNC ) in telecommunications networks with many relays and single - antenna nodes .We first consider PLNC for two - way relay channels where each node has only one antenna . In particular , we propose an efficient scheme to conduct PLNC at both source nodes simultaneously by using linear codes over discrete fields .Then , we expanded our findings to multi - way relay channels with more than two users . Finally , we investigate the performance of PLNC under imperfect channel state information ( CSI ) .The main contributions are presented as follows : 1 . Two - way Relay Channels : We suggest a innovative method to conduct PLNC at the sources simultaneously based on linear codes over discrete fields .2 . Multi - Way Relay Channels : By extending our previous research , we develop a new method to conduct PLNC at all source nodes simultaneously .3 . Imperfect Channel State Information : We evaluate the impact of imperfect CSI on the performance of PLNC schemes .",
        "rewrite_text": "Title: Physical Layer Network Coding\n\nAbstract: This dissertation explores the concept of physical layer network coding (PLNC) within telecommunications networks characterized by multiple relays and single-antenna nodes. Our initial focus is on two-way relay channels, where each participating node is equipped with a single antenna. We introduce an innovative and efficient approach for implementing PLNC at both source nodes concurrently, utilizing linear codes over discrete fields to enhance communication efficiency. Building on this foundation, we extend our analysis to multi-way relay channels involving more than two users, thereby broadening the applicability of our findings. Additionally, we delve into the implications of imperfect channel state information (CSI) on the performance of PLNC strategies. The primary contributions of this research are threefold: \n\n1. For two-way relay channels, we propose a novel method that facilitates simultaneous PLNC at the source nodes, leveraging linear codes over discrete fields to optimize data transmission.\n2. In the context of multi-way relay channels, we expand upon our earlier work to devise a comprehensive strategy that enables concurrent PLNC across all source nodes, thereby improving overall network throughput.\n3. We conduct a thorough evaluation of how imperfect CSI affects the efficacy of PLNC schemes, providing insights into the robustness and reliability of these coding techniques under real-world conditions.\n\nThrough these contributions, our research not only advances the theoretical understanding of PLNC but also offers practical solutions that can be implemented in modern telecommunications systems, ultimately enhancing the efficiency and reliability of data transmission in complex network environments.",
        "ori-fast-z-score": 2.060839349277234,
        "water-fast-z-score": 5.467773927672753,
        "rewrite-fast-z-score": 1.1358152736593492
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Analytic approach to the thermal Casimir force between metal and dielectric .\nAbstract:\nWe present an analytic expression for the thermal Casimir force acting on two parallel plates made out of different materials, one being metallic (silver) while another is dielectric (silicon dioxide). The result obtained agrees with that derived by Lifshitz theory within 1% accuracy in the whole range of separations considered here. We also show how our results can be used to calculate the temperature dependence of the Casimir pressure at fixed separation distance. \n \n In this work we consider the case where one plate consists of silver and other of silicon dioxide. Silver has been chosen because it is widely used as a coating material in microelectromechanical systems (MEMS), whereas SiO2 is often employed as a substrate or insulator layer in MEMS devices. Our results are applicable not only to these specific cases but also to any system consisting of two parallel plates separated by vacuum gap filled with gas medium. This includes such diverse situations like semiconductor heterostructures, quantum dots, nanowires etc., which have attracted considerable attention recently due to their potential applications in nanotechnology. \n \n It should be noted that the problem under consideration was first addressed theoretically more than 50 years ago  1  . However, despite numerous attempts  2  , no exact solution has yet been found. Therefore, most theoretical studies were performed using approximate methods  3  -  6  . These approaches include various modifications of the proximity force approximation  7, 8  , the Derjaguin-Muller-Toporov method  9  , the multiple reflection expansion  10  , the scattering matrix formalism  11  , the Green s function technique  12  , the density functional theory  13  , the mode summation  14  , the fluctuating surface charge model  15  , the effective-medium theory  16  , the generalized plasmon-pole model  17  , the Drude-Lorentz model  18  , the hydrodynamic model  19  , the nonlocal response  20  , the local field correction  21  , the random phase approximation  22  , the Monte Carlo simulation  23  , the finite element method  24  , the numerical integration  25  , the variational principle  26  , the perturbation theory  27  , the renormalization group  28  , the self-consistent screening  29  ,",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Analytic approach to the thermal Casimir force between silver and dielectric . Abstract : We present an analytic definition for the thermal Casimir force acting on two connected sheets formed out of different materials , one being metallic ( silver ) while another is dielectric ( silicon dioxide ) .The result obtained agrees with that derived by Lifshitz theory within 1 % accuracy in the whole range of separations mentioned here . We additionally prove how our findings can be used to estimate the temperature dependence of the Casimir pressure at fixed separation distance .In this research we imagine the case where one plate composed of silver and other of silicon dioxide . Silver has been chosen because it is widely useful as a coating layer in microelectromechanical systems ( MEMS ) , whereas SiO2 is often employed as a substrate or insulator layer in MEMS devices .Our results are applicable not only to these unique instances but also to any system consisting of two connected sheets connected by vacuum gap filled with gas medium . This encompasses such diverse cases like semiconductor heterostructures , quantum dots , nanowires etc . , which have garnered considerable scrutiny lately owing to their potential applications in nanotechnology .It should be mentioned that the issue under consideration was first addressed theoretically more than 50 years previously 1 . However , despite several efforts 2 , no accurate solution has yet been finding .Therefore , most theoretical experiments were performed using approximate approaches 3 - 6 . These approaches involve various alterations of the proximity pressure analogy 7 , 8 , the Derjaguin - Muller - Toporov method 9 , the multiple mirror expansion 10 , the scattering matrix formalism 11 , the Green s function method 12 , the density functional theory 13 , the mode summation 14 , the fluctuating surface charge model 15 , the effective - medium theory 16 , the generalized plasmon - pole hypothesis 17 , the Drude - Lorentz model 18 , the hydrodynamic model 19 , the nonlocal response 20 , the local field correction 21 , the random phase approximation 22 , the Monte Carlo simulation 23 , the finite element method 24 , the numerical integration 25 , the variational theory 26 , the perturbation theory 27 , the renormalization group 28 , the self - consistent screening 29 ,",
        "rewrite_text": "**Title:** Analytic Approach to the Thermal Casimir Force Between Silver and Dielectric\n\n**Abstract:** In this study, we introduce an analytic formulation for the thermal Casimir force exerted between two parallel plates made of distinct materials: one metallic (silver) and the other dielectric (silicon dioxide). Our findings demonstrate a remarkable agreement with the predictions of Lifshitz theory, achieving an accuracy of within 1% across the entire range of separations considered. Furthermore, we establish a method to evaluate the temperature dependence of the Casimir pressure at a fixed separation distance. The specific configuration analyzed involves a silver plate paired with a silicon dioxide plate, chosen for their relevance in microelectromechanical systems (MEMS), where silver is frequently utilized as a coating and silicon dioxide serves as a substrate or insulating layer. \n\nThe implications of our results extend beyond this particular setup, as they can be applied to any system comprising two connected sheets separated by a vacuum gap or filled with a gaseous medium. This includes a variety of systems such as semiconductor heterostructures, quantum dots, and nanowires, which have recently attracted significant attention due to their promising applications in nanotechnology. \n\nIt is noteworthy that the theoretical exploration of this phenomenon dates back over 50 years. Despite numerous attempts to derive accurate solutions, previous efforts have primarily relied on approximate methods, which include modifications of the proximity pressure analogy, the Derjaguin-Muller-Toporov method, and various other theoretical frameworks such as the scattering matrix formalism and density functional theory. Our work contributes a rigorous analytic perspective to this longstanding issue, potentially paving the way for more precise calculations and applications in the field of quantum physics and nanotechnology.",
        "ori-fast-z-score": 1.2570787221094177,
        "water-fast-z-score": 7.754278454880778,
        "rewrite-fast-z-score": 0.35355339059327373
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Improved Quantum Hard-Sphere Ground-State Equations of State .\nAbstract:\nWe present an improved quantum hard-sphere ground-state equation-of-state (EOS) for the description of dense matter in astrophysics and nuclear physics, which is based on the exact solution to the Schrödinger equation with a repulsive delta-function potential. The EOS has been derived by solving numerically the corresponding integral equations using the method of successive iterations. We have also obtained analytical expressions for the pressure and energy density as functions of the number density at zero temperature. Our results are compared with those calculated previously within various approximations such as the virial expansion up to second order, the Carnahan-Starling approximation, and the Percus-Yevick approximation. It turns out that our new EOS agrees well with these previous calculations over wide ranges of densities and temperatures. In particular, it reproduces very accurately the low-density limit where the ideal gas law holds exactly. \n \n Keywords: Equation of state",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Improved Quantum Hard - Sphere Ground - State Equations of State . Abstract : We create an updated quantum hard - sphere ground - state equation - of - state ( EOS ) for the description of dense materials in astrophysics and nuclear science , which is based on the exact solution to the Schrödinger equation with a repulsive delta - function potential .The EOS has been constructed by solving numerically the equivalent integral equations using the method of successive iterations . We have also achieved analytical expressions for the pressure and energy density as functions of the number density at zero temperature .Our results are compared with those estimated previously within various approximations such as the virial expansion up to second order , the Carnahan - Starling approximation , and the Percus - Yevick approximation . It turns out that our new EOS follows well with these previous analyses over broad ranges of densities and altitudes .In particular , it reproduces very correctly the small - density maximum where the ideal gas law stands exactly . Keywords : Equation of state",
        "rewrite_text": "Title: Enhanced Quantum Hard-Sphere Ground-State Equations of State\n\nAbstract: In this study, we present an advanced quantum hard-sphere ground-state equation of state (EOS) tailored for the characterization of dense materials relevant to astrophysics and nuclear science. Our approach is grounded in the precise solution of the Schrödinger equation, incorporating a repulsive delta-function potential. To derive the EOS, we employed a numerical method based on successive iterations to solve the corresponding integral equations. Additionally, we have derived analytical expressions for both pressure and energy density as functions of number density at absolute zero temperature. Our findings are juxtaposed with previous estimates derived from various approximations, including the second-order virial expansion, the Carnahan-Starling approximation, and the Percus-Yevick approximation. Notably, our new EOS aligns closely with these earlier analyses across a wide spectrum of densities and conditions. Specifically, it accurately captures the small-density maximum where the ideal gas law is precisely valid. This work not only enhances the understanding of dense matter but also provides a robust framework for future research in related fields. \n\nKeywords: Equation of state, quantum mechanics, dense materials, astrophysics, nuclear science.",
        "ori-fast-z-score": 0.4588314677411235,
        "water-fast-z-score": 4.444462481925879,
        "rewrite-fast-z-score": 2.975337221046947
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Relaxation of a dewetting contact line Part 2: Experiments .\nAbstract:\nWe present new experimental results on the relaxation dynamics of a liquid film that is pulled off an inclined solid substrate by gravity and capillarity. The experiments are performed in a microgravity environment aboard the International Space Station (ISS). We find that, for sufficiently large pulling speeds, the relaxation process can be described as a succession of three stages. In stage I, the contact angle decreases rapidly to its equilibrium value at which point the contact line stops moving. Stage II starts when the contact line has stopped moving; during this stage, the contact angle remains constant while the height profile of the free surface continues evolving towards its final shape. Finally, in stage III, the contact angle increases again until it reaches its initial value. This behavior is explained using a simple model based on lubrication theory. Our analysis shows that the relaxation time scales with the inverse of the pulling speed. For small pulling speeds, we observe deviations from our theoretical predictions due to inertial effects. \n \n Liquid films play important roles in many natural phenomena such as rain drops falling down a window or water evaporating into air. Dewetting processes occur frequently in nature but also have applications in industrial processes like coating technologies where thin films need to be removed from substrates. Understanding how these systems evolve over time requires knowledge about their relaxation dynamics after they have been perturbed out of equilibrium. Here, we study experimentally the relaxation of a liquid film that was pulled off an inclined solid wall by gravity and capillary forces.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Relaxation of a dewetting connection line Part 2 : Experiments . Abstract : We report new empirical results on the relaxation behavior of a liquid movie that is turned off an inclined solid substrate by gravitational and capillarity .The studies are performed in a microgravity atmosphere aboard the International Space Station ( ISS ) . We see that , for enough large pulling speeds , the relaxation process can be described as a sequence of three stages .In stage I , the contact angle decreases quickly to its equilibrium value at which point the contact line stopping moved . Stage II began when the contact line has stopped movement ; during this phase , the contact angle remains zero while the height profile of the free surface remains evolving towards its final form .Finally , in stage III , the contact angle raises again until it hits its initial value . This phenomenon is understood using a simple model based on lubrication theory .Our study shows that the relaxation time varies with the inverse of the dragging velocity . For small pulling speeds , we perceive deviations from our theory expectations due to inertial impacts .Liquid films play important roles in different natural phenomena such as rainfall drops sliding down a window or water evaporating into air . Dewetting methods happen widely in nature but also have applications in industrial systems like coating technologies where loose films need to be removed from substrates .Understanding how these systems develop over time needs experience about their relaxation behavior after they have been perturbed out of equilibrium . Here , we study experimentally the relaxation of a liquid movie that was dragged off an inclined solid wall by gravitational and capillary forces .",
        "rewrite_text": "We present new experimental findings on the relaxation dynamics of a liquid film that has been removed from an inclined solid substrate due to gravitational and capillary forces. These investigations were conducted in a microgravity environment aboard the International Space Station (ISS). Our observations reveal that, at sufficiently high pulling speeds, the relaxation process can be characterized by three distinct stages. In Stage I, the contact angle of the liquid film decreases rapidly until it reaches its equilibrium value, at which point the contact line ceases to move. Stage II commences once the contact line has halted; during this phase, the contact angle remains at zero while the height profile of the liquid's free surface continues to evolve toward its final configuration. Finally, in Stage III, the contact angle begins to increase again until it returns to its initial value. This behavior can be explained using a straightforward model derived from lubrication theory. Our findings indicate that the relaxation time is inversely related to the pulling velocity. At lower pulling speeds, we observe deviations from theoretical predictions, which can be attributed to inertial effects. Liquid films are integral to various natural processes, such as raindrops sliding down surfaces or the evaporation of water into the atmosphere. Dewetting phenomena are prevalent in nature and have significant implications in industrial applications, including coating technologies where the removal of thin films from substrates is necessary. Gaining insights into the temporal evolution of these systems following perturbations from equilibrium is crucial. In this study, we experimentally investigate the relaxation behavior of a liquid film that has been dragged off an inclined solid surface, driven by gravitational and capillary forces.",
        "ori-fast-z-score": -0.6713450866373513,
        "water-fast-z-score": 7.779495352862258,
        "rewrite-fast-z-score": 1.8209309360006518
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Switching mechanism of photochromic diarylethene derivatives molecular junctions .\nAbstract:\nThe switching behavior and the photovoltaic properties of two new diarylethene derivative molecular junctions were investigated by using cyclic voltammetry, UV-Vis spectroscopy, and electrochemical impedance spectroscopy (EIS). The results show that both compounds can be switched between their open-ring isomer state and closed-ring isomer state in solution with different colors under visible light irradiation at room temperature. In addition to this reversible color change process, the photocurrent response was also observed for these molecules when they are used as active layers in organic solar cells. This work provides an insight into the relationship between the structure and function of diarylethene-based molecular switches. Switchable materials have attracted great attention because of their potential applications in optoelectronic devices such as optical memory storage systems, smart windows, and organic solar cells. \n \n Diarylethenes belong to one class of switchable materials which undergoes a rapid and complete structural transformation upon exposure to ultraviolet or visible light.  1  These unique features make them promising candidates for use in various fields including chemical sensors  2  , data storage  3  , and organic electronics  4  . However, most reported diarylethene based molecular switches suffer from poor solubility in common solvents  5  , low quantum yield  6  , and slow response time  7  . Therefore, it remains challenging to develop efficient diarylethene molecular switches with improved performance  8  .\n \nIn recent years, many efforts have been made to improve the performances of diarylethenes  9  -  11  . For example, some researchers introduced bulky substituents on the carbon atoms adjacent to the double bond  12  -  14  ; others synthesized diarylethenes containing electron-donating groups  15  -  17  . Although these modifications could enhance the solubility and quantum efficiency of diarylethens, the response times still remain relatively slow  18  . \n \n Herein we report two novel diarylethene dyes 1 and 2 ( Figure  1 ) bearing electron-withdrawing groups. Both compounds exhibit good solubility in common organic solvents and high quantum yields. They can",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Switching mechanism of photochromic diarylethene derivatives molecular junctions . Abstract : The switching behavior and the photovoltaic properties of two new diarylethene derivative chemical junctions were researched by using cyclic voltammetry , UV - Vis spectroscopy , and electrochemical impedance spectroscopy ( EIS ) .The results show that both chemicals can be switched between their open - ring isomer state and opened - ring isomer state in solution with various shades under visible light irradiation at room temperature . In addition to this reversible color transformation process , the photocurrent response was also observed for these molecules when they are using as active layers in organic solar cells .This research provides an insight into the relationship between the composition and activity of diarylethene - based molecular switches . Switchable materials have garnered great attention because of their potential applications in optoelectronic devices such as laser memory processing systems , smart panels , and organic solar devices .Diarylethenes come to one category of switchable materials which undergoes a rapid and complete structural transformation upon exposure to ultraviolet or visible light . 1 These unique features make them promising candidates for use in different fields specifically chemical sensors 2 , computer processing 3 , and organic computers 4 .However , most reported diarylethene based molecular switches result from poor solubility in standard solvents 5 , low quantum strength 6 , and poor response period 7 . Therefore , it remains challenging to develop efficient diarylethene molecular switches with improved performance 8 .In recent years , various efforts have been made to improve the performances of diarylethenes 9 - 11 . For instance , some researchers implemented bulky substituents on the carbon atoms adjacent to the double bond 12 - 14 ; others synthesized diarylethenes featuring electron - donating groups 15 - 17 .Although these alterations could enhance the solubility and quantum efficiency of diarylethens , the response periods currently continue relatively slow 18 . Herein we study two novel diarylethene dyes 1 and 2 ( Figure 1 ) containing electron - withdrawing groups .Both compounds exhibit great solubility in common organic solvents and large quantum yields . They can",
        "rewrite_text": "**Title:** Switching Mechanism of Photochromic Diarylethene Derivatives in Molecular Junctions\n\n**Abstract:** This study investigates the switching behavior and photovoltaic characteristics of two novel diarylethene derivative molecular junctions through a combination of cyclic voltammetry, UV-Vis spectroscopy, and electrochemical impedance spectroscopy (EIS). The findings reveal that both compounds can be reversibly switched between their open-ring and closed-ring isomer states when exposed to various wavelengths of visible light at room temperature. This reversible color change is accompanied by a notable photocurrent response when these derivatives are utilized as active layers in organic solar cells. The research sheds light on the intricate relationship between the molecular composition and the functional activity of diarylethene-based molecular switches. \n\nThe significance of switchable materials has surged due to their promising applications in optoelectronic devices, including laser memory systems, smart panels, and organic solar cells. Diarylethenes, a prominent class of switchable materials, are characterized by their rapid and complete structural transformation upon exposure to ultraviolet or visible light. These properties position them as ideal candidates for various applications, particularly in chemical sensing, computational processing, and organic computing. \n\nDespite their potential, many existing diarylethene-based molecular switches suffer from limitations such as poor solubility in common solvents, low quantum efficiency, and sluggish response times. Recent advancements have aimed to enhance the performance of diarylethenes, with researchers exploring modifications such as the introduction of bulky substituents on adjacent carbon atoms and the incorporation of electron-donating groups. While these strategies have improved solubility and quantum yields, the response times remain suboptimal. \n\nIn this work, we present two innovative diarylethene dyes, designated as 1 and 2, which incorporate electron-withdrawing groups. These compounds demonstrate excellent solubility in standard organic solvents and exhibit high quantum yields, thereby addressing some of the challenges faced by previous diarylethene derivatives. Our findings contribute to the ongoing development of efficient diarylethene molecular switches with enhanced performance metrics.",
        "ori-fast-z-score": -1.0932163332202425,
        "water-fast-z-score": 5.433053679944329,
        "rewrite-fast-z-score": 0.08084520834544433
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Properties of Zero-Lag Long-Range Synchronization via Dynamical Relaying .\nAbstract:\nWe study the zero-lag synchronization between two identical chaotic systems with different time scales by using dynamical relaying method, which is based on introducing an intermediate system to transmit information between them. The stability analysis shows that the proposed scheme can achieve zero-lag synchronization under some conditions. Numerical simulations are performed for Lorenz and Chen systems as examples. It is shown that the proposed scheme has advantages over other existing methods in terms of robustness against parameter mismatch and external disturbances. \nI. INTRODUCTIO N\nSynchronization plays important roles in many fields such as biology  1  , physics  2  , engineering  3  . In recent years, chaos synchronization  4  -  6  has attracted much attention due to its potential applications in secure communication  7  , chemical reactions  8  , biological systems  9  .\nChaos synchronization was first studied by Pecora and Carroll  10  who introduced the concept of master-slave synchronization. Since then, various schemes have been developed  11  -  13  . Among these schemes, adaptive control  14  , active control  15  , backstepping  16  , sliding mode  17  , fuzzy logic  18  , impulsive control  19  , intermittent control  20  , pinning control  21  , etc., were widely used  22  -  24  . However, most of these works focused only on the case where there exists no delay between slave and master systems  25  -  27  . Recently, several studies have investigated the problem of synchronizing chaotic systems with time delays  28  -  30  . For example, Wu et al.  31  presented a new approach to realize lag-synchronized chaos between two chaotic systems with different dimensions through state feedback controllers. Liu et al.  32  designed a novel delayed-feedback controller to synchronize two chaotic systems with unknown parameters. Wang et al.  33  proposed a simple but effective method to synchronize two chaotically oscillating systems with time-varying delays. Although these results provide useful insights into the design of synchronized chaotic systems with time-delays, they cannot be applied directly to solve practical problems because it may take too",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Properties of Zero - Lag Long - Range Synchronization via Dynamical Relaying . Abstract : We research the zero - lag synchronization between two unrelated turbulent systems with varying time ranges by using dynamical relaying model , which is based on introducing an intermediate system to transmit data between them .The stability analysis shows that the suggested system can attain zero - lag synchronization under some conditions . Numerical simulations are performed for Lorenz and Chen networks as instance .It is demonstrated that the suggested system has advantages over other existing techniques in terms of robustness against parameter mismatch and external disturbances . I . INTRODUCTIO N Synchronization plays important roles in different fields such as biology 1 , chemistry 2 , engineering 3 .In recent years , chaos synchronization 4 - 6 has drew much attention due to its potential applications in safe communication 7 , chemical processes 8 , biological systems 9 . Chaos synchronization was first investigated by Pecora and Carroll 10 who proposed the idea of master - servant synchronization .Since then , various strategies have been created 11 - 13 . Among these schemes , adaptive control 14 , active control 15 , backstepping 16 , sliding mode 17 , fuzzy logic 18 , impulsive control 19 , continuous control 20 , pinning rule 21 , etc . , were commonly used 22 - 24 .However , most of these works focused only on the case where there exists no delay between slave and master schemes 25 - 27 . Recently , various studies have analyzed the issue of synchronizing chaotic networks with time errors 28 - 30 .For instance , Wu et al . 31 presented a new approach to realize lag - synchronized chaos between two chaotic structures with varying dimensions through state feedback controllers .Liu et al . 32 designed a new delayed - feedback controller to synchronize two chaotic structures with unknown parameters .Wang et al . 33 developed a simple but effective theory to synchronize two chaotically oscillating systems with time - differing delays .Although these results provide useful insights into the model of synchronized turbulent systems with time - errors , they cannot be applied directly to solve practical questions because it could took too",
        "rewrite_text": "**Title:** Properties of Zero-Lag Long-Range Synchronization via Dynamical Relaying\n\n**Abstract:** This study investigates zero-lag synchronization between two independent turbulent systems characterized by varying time scales, utilizing a dynamical relaying model that incorporates an intermediary system for data transmission. Our stability analysis reveals that the proposed model can achieve zero-lag synchronization under specific conditions. To illustrate the effectiveness of our approach, we conduct numerical simulations on the Lorenz and Chen chaotic networks. The results indicate that our system offers significant advantages over existing synchronization techniques, particularly in terms of resilience to parameter mismatches and external disturbances. \n\nSynchronization is a critical phenomenon observed across various disciplines, including biology, chemistry, and engineering. In recent years, chaos synchronization has garnered considerable attention due to its promising applications in secure communication, chemical processes, and biological systems. The foundational work by Pecora and Carroll introduced the concept of master-slave synchronization, leading to the development of numerous strategies such as adaptive control, active control, backstepping, sliding mode control, fuzzy logic, impulsive control, continuous control, and pinning control. However, much of the existing literature has primarily focused on scenarios devoid of delays between the master and slave systems.\n\nRecent advancements have begun to address the synchronization of chaotic networks in the presence of time delays. For example, Wu et al. proposed a method for achieving lag-synchronized chaos between chaotic systems of varying dimensions through state feedback controllers, while Liu et al. introduced a delayed-feedback controller for synchronizing chaotic systems with unknown parameters. Additionally, Wang et al. developed a straightforward yet effective theory for synchronizing chaotically oscillating systems with differing time delays. Despite these contributions, the applicability of these findings to practical synchronization challenges remains limited, as they often do not account for the complexities associated with real-world systems. Our research aims to bridge this gap by providing a robust framework for zero-lag synchronization that can be effectively implemented in practical scenarios.",
        "ori-fast-z-score": -0.47733437050543803,
        "water-fast-z-score": 8.009821015753255,
        "rewrite-fast-z-score": 0.15617376188860607
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Prediction of future fifteen solar cycles .\nAbstract:\nThe prediction of the next cycle is an important problem in space weather research, which has been studied for more than half century. In this work we use two different methods to predict the strength and duration of the twenty-first solar cycle (SC21). The first method uses artificial neural networks trained on data from previous cycles. We find that our network predicts SC21 with high accuracy. The second method relies on the analysis of sunspot number time series using wavelet transform. This approach allows us to identify periodicities in the signal related to the solar activity. Our results show that both approaches are able to make accurate predictions about the upcoming solar cycle. Solar activity exhibits cyclic behavior over multiple timescales ranging from minutes up to thousands years. Understanding these cycles can help us better understand how the Sun works as well as its influence on Earths climate. \n \n Predicting the strength and duration of forthcoming solar cycles is one of the most challenging problems in space weather research. It was shown by several authors that it is possible to forecast the amplitude of the current cycle based on information available at the beginning of the cycle itself  1  . However, predicting the exact timing of maxima or minima within each cycle remains difficult  2  . \n \n Here we present two independent methods to predict the properties of the twenty-first solar activity cycle (SC21) starting from the end of twentieth cycle (SC20), i.e., from January 2010. Both methods rely only on publicly available data sets obtained from NASA s Space Weather Prediction Center  3  , NOAA  4  , and SIDC  5  .\n \nMethod 1: Artificial Neural Networks \n \n First, we train an artificial neural network  6  on data from past solar cycles. Specifically, we consider the following inputs: 1) monthly mean sunspot numbers; 2) monthly mean 10.7-cm radio flux values; 3) monthly mean F10.7 index; 4) monthly mean Mg II index. These quantities were averaged over the last ten solar cycles prior to SC20. For example, if we want to predict SC21, then we average all four quantities between December 2009 and November 2019. Note that",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Prediction of potential fifteen solar cycles . Abstract : The prediction of the new cycle is an important task in space weather study , which has been studied for more than quarter century .In this project we using two different methods to predict the strength and duration of the twenty - first solar cycle ( SC21 ) . The first method uses artificial neural nets trained on evidence from previous periods .We see that our system predicts SC21 with high clarity . The second method relies on the analysis of sunspot number period series use wavelet transform .This method enables us to identify periodicities in the signal related to the sun activity . Our results show that both approaches are able to make accurate forecast about the ongoing solar cycle .Solar behavior exhibits cyclic behavior over numerous timescales varied from seconds up to thousands years . Understanding these cycles can help us better understand how the Sun operates as well as its influence on Earths environment .Predicting the strength and duration of forthcoming solar cycles is one of the most challenging difficulties in space weather study . It was shown by various scientists that it is easy to forecast the frequency of the present cycle using on knowledge accessible at the beginning of the cycle itself 1 .However , predicting the exact timing of maxima or minima within each cycle remains challenging 2 . Here we present two independent methods to predict the properties of the ten - first solar activity period ( SC21 ) beginning from the end of twentieth cycle ( SC20 ) , i . e . , from January 2010 .Both methods rely only on publicly accessible data sets received from NASA s Space Weather Prediction Center 3 , NOAA 4 , and SIDC 5 . Method 1 : Artificial Neural Networks First , we train an synthetic neural network 6 on evidence from previous solar cycles .Specifically , we consider the following output : 1 ) weekly mean sunspot numbers ; 2 ) weekly mean 10 . 7 - cm radio flux values ; 3 ) weekly mean F10 . 7 index ; 4 ) annual mean Mg II index . These quantities were averaged over the last ten solar cycles prior to SC20 .For instance , if we wish to predict SC21 , then we average all four quantities between December 2009 and November 2019 . Note that",
        "rewrite_text": "**Title: Prediction of Potential Fifteen Solar Cycles**\n\n**Abstract:** The forecasting of solar cycles is a critical aspect of space weather research, a field that has been explored for over twenty-five years. This study presents two distinct methodologies aimed at predicting the strength and duration of the twenty-first solar cycle (SC21). The first approach employs artificial neural networks (ANNs) that are trained on historical data from previous solar cycles. Our findings indicate that this ANN-based system provides clear and reliable predictions for SC21. The second approach utilizes wavelet transform analysis of sunspot number time series, which allows for the identification of periodic patterns associated with solar activity. The results from both methods demonstrate their efficacy in accurately forecasting the characteristics of the current solar cycle.\n\nSolar activity is characterized by cyclical patterns that span a wide range of timescales, from mere seconds to thousands of years. Gaining insights into these cycles is essential for understanding the Sun's operational mechanisms and its impact on Earth's environment. One of the primary challenges in space weather research is predicting the strength and duration of upcoming solar cycles. While it has been established that forecasting the frequency of the current cycle is feasible using data available at the cycle's onset, accurately determining the timing of solar maxima and minima remains a significant challenge.\n\nIn this paper, we introduce two independent methods for predicting the properties of the upcoming solar activity period (SC21), starting from the conclusion of the twentieth cycle (SC20) in January 2010. Both methodologies utilize publicly available datasets sourced from NASA's Space Weather Prediction Center, NOAA, and SIDC. The first method, based on artificial neural networks, involves training the model using historical data, including weekly mean sunspot numbers, weekly mean 10.7-cm radio flux values, weekly mean F10.7 index, and annual mean Mg II index. These metrics are averaged over the last ten solar cycles preceding SC20, specifically from December 2009 to November 2019, to enhance the prediction accuracy for SC21.",
        "ori-fast-z-score": 1.1211526391279039,
        "water-fast-z-score": 8.301992346604333,
        "rewrite-fast-z-score": 0.3104602102825331
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Discovery of an Active Galactic Nucleus in the Late-type Galaxy NGC 3621: Spitzer Spectroscopic Observations .\nAbstract:\nWe report on infrared spectroscopic observations with the Infrared Spectrograph (IRS) aboard the Spitzer Space Telescope toward the nearby late-type galaxy NGC 3621, which is known to harbor a supermassive black hole at its center. The IRS spectrum shows prominent emission lines such as  Ne II 12.81 and  S III 18.71 µm that are commonly seen in active galactic nuclei (AGNs). We find that these emission lines can be reproduced by photoionization models using AGN-like ionizing radiation fields. From the observed line ratios we estimate the electron density n e = 10 3 cm −3 , temperature T e = 1000 K, and ionization parameter U H = 1 × 10 −2 . These results suggest that the central region of NGC 3621 has properties similar to those found for Seyfert galaxies. This work was supported by NASA through grant number GO-08460.01-A awarded by the Jet Propulsion Laboratory, California Institute of Technology under contract with NASA.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Discovery of an Active Galactic Nucleus in the Late - class Galaxy NGC 3621 : Spitzer Spectroscopic Observations . Abstract : We report on infrared spectroscopic observations with the Infrared Spectrograph ( IRS ) aboard the Spitzer Space Telescope toward the nearby mid - class galaxy NGC 3621 , which is known to harbor a supermassive black hole at its core .The IRS spectrum displays large emitted lines such as Ne II 12 . 81 and S III 18 . 71 µm that are often saw in active galactic nuclei ( AGNs ) . We see that these absorption patterns can be reproduced by photoionization models using AGN - like ionizing radiation fields .From the known line ratios we estimate the electron concentration n e = 10 3 cm −3 , temperature T e = 1000 K , and ionization variable U H = 1 × 10 −2 . These data suggest that the central region of NGC 3621 has properties similar to those known for Seyfert galaxies .This work was supported by NASA through grant number GO - 08460 . 01 - A awarded by the Jet Propulsion Laboratory , California Institute of Technology under contract with NASA .",
        "rewrite_text": "We present our findings from infrared spectroscopic observations conducted with the Infrared Spectrograph (IRS) on the Spitzer Space Telescope, focusing on the nearby mid-class galaxy NGC 3621, which is recognized for hosting a supermassive black hole at its center. Our analysis reveals prominent emission lines, including Ne II at 12.81 µm and S III at 18.71 µm, which are characteristic of active galactic nuclei (AGNs). The observed absorption features align well with predictions from photoionization models that utilize AGN-like ionizing radiation fields. By examining the established line ratios, we derive key physical parameters of the central region of NGC 3621, including an electron density of n_e = 10^3 cm^−3, an electron temperature of T_e = 1000 K, and an ionization parameter U_H = 1 × 10^−2. These findings indicate that the characteristics of NGC 3621's nucleus are comparable to those typically associated with Seyfert galaxies, suggesting that this galaxy may exhibit similar active galactic phenomena. This research was made possible through the support of NASA, under grant number GO-08460.01-A, awarded by the Jet Propulsion Laboratory at the California Institute of Technology, in collaboration with NASA. Our results contribute to the understanding of the nature of active galactic nuclei in late-type galaxies and highlight the potential for further exploration of their properties through infrared spectroscopy.",
        "ori-fast-z-score": -0.25,
        "water-fast-z-score": 3.9056328877620143,
        "rewrite-fast-z-score": 0.41702882811414954
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Degree Optimization and Stability Condition for the Min-Sum Decoder .\nAbstract:\nWe consider the problem of decoding linear block codes over an arbitrary finite field using the min-sum algorithm, which is one of the most popular algorithms in practical applications such as Turbo coding.  We show that under certain conditions on the degree distribution of parity-check nodes, there exists a unique optimal solution to the optimization problem associated with each node update rule at every iteration of the min-sum decoder. This result leads us to propose a new stability condition for the min-sum decoder based on the concept of local convergence. The proposed stability condition can be used to determine whether or not the min-sum decoder converges globally by checking if it locally converges within a small number of iterations. Finally, we present simulation results showing that our proposed stability condition outperforms existing ones when applied to LDPC codes. In this work, we study the problem of decoding linear binary block codes using the min-sum (MS) algorithm  1  , which has been widely adopted in many practical communication systems including Turbo-coding  2  . It was shown in  3  -  5  that MS decoding achieves near maximum-likelihood performance while requiring only low complexity per bit compared to other iterative decoders  6  .\nIn general, the MS algorithm solves the following problem: given a codeword c =  c0 c1 . . . cm−1  ∈ Fm−1 2\n, find the vector x * ∈ F2 n satisfying Hx * = c where H denotes the parity check matrix of size m × n. To solve this problem, the MS algorithm performs message passing between variable nodes and parity-check nodes according to the following rules: 1) At each iteration t, compute the log likelihood ratio (LLR) λt(i), i ∈ {0, . . . , m − 1}, corresponding to ci as: \nwhere N (j) represents the set of neighbors connected to j via edges in H; 2) Update the LLRs of all parity-check nodes:",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Degree Optimization and Stability Condition for the Min - Sum Decoder . Abstract : We consider the question of decoding linear block codes over an arbitrary finite field using the min - sum algorithm , which is one of the most popular methods in practical applications such as Turbo coding .We see that under certain conditions on the degree distribution of parity - check nodes , there exists a unique optimal solution to the optimization problem associated with each node update rule at every iteration of the min - sum decoder . This result leads us to propose a new stability situation for the min - sum decoder relying on the idea of local convergence .The proposed stability condition can be used to determine whether or not the min - sum decoder converges nationally by testing if it locally converges within a small number of iterations . Finally , we present modeling results confirming that our proposed stability condition outperforms current ones when applied to LDPC coding .In this research , we study the question of decoding linear binary block sequences using the min - sum ( MS ) algorithm 1 , which has been widely adopted in many practical communication schemes notably Turbo - codes 2 . It was shown in 3 - 5 that MS decoding achieves near maximum - likelihood performance while using only low complexity per bit relative to other iterative decoders 6 .In general , the MS algorithm solves the following task : given a codeword c = c0 c1 . .. cm−1 ∈ Fm−1 2 , find the vector x * ∈ F2 n satisfying Hx * = c where H represents the parity check matrix of width m × n . To solve this situation , the MS algorithm performs message passing between variable nodes and parity - check nodes according to the following laws : 1 ) At each iteration t , compute the log probability ratio ( LLR ) λt ( i ) , i ∈ { 0 , ..., m − 1 } , equivalent to ci as : where N ( j ) indicates the group of neighbors connected to j via edges in H ; 2 ) Update the LLRs of all parity - check vertices :",
        "rewrite_text": "**Title:** Degree Optimization and Stability Condition for the Min-Sum Decoder\n\n**Abstract:** This article addresses the decoding of linear block codes over arbitrary finite fields utilizing the min-sum algorithm, a widely recognized technique in practical applications, particularly in Turbo coding. We establish that under specific conditions regarding the degree distribution of parity-check nodes, there exists a unique optimal solution for the optimization problem linked to each node update rule at every iteration of the min-sum decoder. This finding prompts us to introduce a novel stability condition for the min-sum decoder, which is based on the concept of local convergence. The proposed stability criterion serves as a tool to assess the global convergence of the min-sum decoder by evaluating its local convergence over a limited number of iterations. \n\nOur modeling results demonstrate that this new stability condition significantly outperforms existing conditions when applied to Low-Density Parity-Check (LDPC) coding schemes. The research focuses on decoding linear binary block sequences using the min-sum (MS) algorithm, which has gained traction in various communication systems, particularly Turbo codes. Previous studies have indicated that the MS decoding approach achieves performance levels close to maximum likelihood while maintaining low complexity per bit compared to other iterative decoding methods.\n\nThe primary objective of the MS algorithm is to identify a vector \\( x^* \\in F^{2n} \\) that satisfies the equation \\( Hx^* = c \\), where \\( c = c_0 c_1 \\ldots c_{m-1} \\) is a given codeword and \\( H \\) denotes the parity-check matrix of dimensions \\( m \\times n \\). To accomplish this, the MS algorithm employs a message-passing mechanism between variable nodes and parity-check nodes, which involves calculating the log-likelihood ratio (LLR) at each iteration and updating the LLRs of all parity-check vertices based on their connected neighbors. This study contributes to the understanding of the min-sum decoding process and its stability, paving the way for enhanced performance in practical coding applications.",
        "ori-fast-z-score": -0.32659863237109044,
        "water-fast-z-score": 4.964109598504073,
        "rewrite-fast-z-score": 0.07881104062391006
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  SN 2006bp: Probing the Shock Breakout of a Type II-P Supernova .\nAbstract:\nWe present optical and near-infrared photometry of SN 2006bp, which was discovered on September 24th in NGC 6946 by amateur astronomer Brian Puckett (Puckett et al., 2007) . The supernova is located at an unusually large distance for its host galaxy, with a recession velocity of ~1000 km/sec. \n \n We find that the light curve can be well fit using a model consisting of three components: shock breakout emission, radioactive decay powered luminosity, and dust extinction. Using this model we derive physical parameters such as the progenitor radius, mass loss rate, and explosion energy. Our results are consistent with those found for other type-II SNe but suggest that the progenitor star had a lower initial mass than previously thought. This may indicate that there exists more diversity among progenitors of type-II SNe than has been realized so far. In addition to these findings, our observations provide new insights into the physics of shock breakout and early-time evolution of type-II SNe.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : SN 2006bp : Probing the Shock Breakout of a Type II - P Supernova . Abstract : We present visual and far - infrared photometry of SN 2006bp , which was discovered on September 24th in NGC 6946 by amateur astronomer Brian Puckett ( Puckett et al . , 2007 ) .The supernova is situated at an exceptionally wide distance for its host galaxy , with a collapse speeds of ~ 1000 km / sec . We see that the light curve can be well fitting using a simulation comprised of three components : shock breakout emission , radioactive decay powered luminosity , and dust disappearance .Using this model we derive physical factors such as the progenitor diameter , mass loss rate , and explosion power . Our results are compatible with those observed for other class - II SNe but suggest that the progenitor star had a smaller original mass than previously thought .This might suggest that there exists more diversity among progenitors of type - II SNe than has been realized so far . In addition to these conclusions , our observations offer additional perspectives into the physics of wave breakout and first - time progression of type - II SNe .",
        "rewrite_text": "We present a comprehensive analysis of SN 2006bp, a Type II-P supernova discovered on September 24, 2006, in the galaxy NGC 6946 by amateur astronomer Brian Puckett (Puckett et al., 2007). This supernova is notable for its unusually large distance from the center of its host galaxy, exhibiting collapse velocities of approximately 1000 km/s. Our study includes detailed visual and far-infrared photometry, which allows us to construct a robust light curve. We find that the light curve can be accurately modeled using a combination of three key components: shock breakout emission, luminosity driven by radioactive decay, and the effects of dust disappearance. \n\nThrough this modeling approach, we are able to extract important physical parameters, including the progenitor star's diameter, mass loss rate, and the energy released during the explosion. Our findings align with observations from other Type II supernovae, yet they indicate that the progenitor star of SN 2006bp may have had a lower initial mass than previously estimated. This observation raises intriguing questions about the diversity of progenitor stars for Type II supernovae, suggesting that there may be a broader range of characteristics among these stellar progenitors than has been recognized in past studies.\n\nMoreover, our observations contribute valuable insights into the underlying physics of shock breakout phenomena and the early evolution of Type II supernovae. By enhancing our understanding of these processes, we hope to shed light on the complex mechanisms that govern supernova explosions and their progenitor stars, ultimately enriching the field of astrophysics and our comprehension of stellar evolution.",
        "ori-fast-z-score": -1.3416407864998738,
        "water-fast-z-score": 5.079850199442941,
        "rewrite-fast-z-score": 0.27975144247209416
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Distance Geometry of Music .\nAbstract:\nThe distance geometry problem is the following: given n points in space, find their coordinates such that they are as close to each other as possible while respecting some constraints on distances between pairs of points.  In this work we consider an application of the distance geometry problem to music analysis and propose a novel method for automatic transcription of musical scores into symbolic representations based on the concept of pitch class profiles (PCP). The proposed approach allows us to recover the underlying harmonic structure of polyphonic music by solving a system of quadratic equations with linear equality constraints using convex optimization techniques. We demonstrate our algorithm s performance on several classical piano pieces. 1 Introduction\n\nMusic Analysis\nAutomatic transcription of musical scores has been one of the most challenging problems in computer science over the past decades. It consists of recovering the underlying harmonic structure of a piece of music from its audio signal or MIDI file. This task can be divided into two main subtasks:  detection of note onset times; estimation of pitches at detected notes  locations. Note onset time detection is usually performed by applying various heuristics to the raw audio data  22, 23  . Once the note onset times have been determined, the next step is to estimate the pitches corresponding to these events. There exist many different approaches to solve this problem ranging from simple template matching methods to more sophisticated statistical models  7, 8, 10, 11, 13, 14, 16, 17, 19-21, 24-26  .\nIn this work we focus on the second part of the problem -estimation of pitches-which is known as  pitch estimation  or  pitch tracking . Pitch tracking algorithms try to assign a pitch value to every detected event in order to obtain a sequence of pitch values which correspond to the original score. A common way to represent pitches is through so-called pitch-class profiles (PCPs)  6, 12, 15, 18, 27  , where each entry corresponds to the number of occurrences of a particular pitch within a certain window around the current time instant. For example, Figure 1 shows a typical PCP obtained from a single-note mel",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Distance Geometry of Music . Abstract : The distance geometry issue is the following : given n points in space , find their coordinates such that they are as close to each other as possible while respecting some restrictions on distances between pairs of points .In this study we investigate an use of the distance geometry issue to music analysis and suggest a new method for efficient reproduction of musical scores into symbolic representations based on the idea of pitch class profiles ( PCP ) . The proposed approach allows us to extract the fundamental chord form of polyphonic music by modeling a system of quadratic equations with linear equality restrictions utilizing convex optimization tools .We test our algorithm s playing on numerous classical piano compositions . 1 Introduction Music Analysis Automatic recording of piano scores has been one of the most challenging difficulties in computer science over the previous decades .It consists of collecting the fundamental chord form of a work of music from its audio sound or MIDI file . This job can be grouped into two principal subtasks : detection of note onset times ; estimation of pitches at identified notes sites .Note onset time detection is usually performed by using numerous heuristics to the raw audio information 22 , 23 . Once the tone onset times have been determined , the second step is to estimate the pitches corresponding to these actions .There remain many various approaches to solve this question ranging from basic template matching algorithms to more sophisticated mathematical models 7 , 8 , 10 , 11 , 13 , 14 , 16 , 17 , 19 - 21 , 24 - 26 . In this research we focus on the second part of the question - estimation of pitches - which is known as pitch estimation or pitch tracking .Pitch tracking schemes seek to give a pitch number to every detected event in order to obtain a sequence of pitch values which coincide to the actual score . A popular way to measure pitches is through so - called pitch - class profiles ( PCPs ) 6 , 12 , 15 , 18 , 27 , where each entry relates to the quantity of occurrences of a given pitch within a certain window around the present time instant .For instance , Figure 1 shows a typical PCP obtained from a single - note mel",
        "rewrite_text": "**Title: The Distance Geometry of Music**\n\n**Abstract:** This article addresses the distance geometry problem, which involves determining the coordinates of n points in space such that they are positioned as closely as possible to one another while adhering to specific distance constraints between pairs of points. We explore the application of this problem in the realm of music analysis and propose a novel method for the efficient transformation of musical scores into symbolic representations, leveraging the concept of pitch class profiles (PCP). Our approach facilitates the extraction of the fundamental chord structure from polyphonic music by formulating a system of quadratic equations subject to linear equality constraints, employing convex optimization techniques. We validate the effectiveness of our algorithm through extensive testing on a variety of classical piano compositions.\n\n**1. Introduction:** The automatic transcription of piano scores has posed significant challenges in computer science over recent decades. This process involves deriving the fundamental chord structure of a musical piece from its audio or MIDI representation. The task can be divided into two primary components: the detection of note onset times and the estimation of pitches at these identified note locations. Note onset detection typically relies on various heuristics applied to raw audio data. Once the onset times are established, the subsequent step is to estimate the pitches corresponding to these events. Numerous methodologies exist to tackle this challenge, ranging from basic template matching techniques to more advanced mathematical models. Our research specifically concentrates on the second component—pitch estimation, also referred to as pitch tracking. Pitch tracking systems aim to assign a pitch value to each detected event, resulting in a sequence of pitch values that accurately reflects the original score. A widely used method for measuring pitches involves pitch class profiles (PCPs), where each entry indicates the frequency of a specific pitch occurrence within a defined time window. For example, a typical PCP derived from a single-note melody is illustrated in Figure 1.",
        "ori-fast-z-score": -0.5897678246195885,
        "water-fast-z-score": 8.798826901281197,
        "rewrite-fast-z-score": 1.5874507866387544
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Limits of ultra-high-precision optical astrometry: Stellar surface structures .\nAbstract:\nWe present the results of our analysis on stellar surface structure effects in high precision space-based optical astrometric observations, which are expected to be achieved by future space missions such as Gaia and SIM. We show that these effects can cause systematic errors in the derived parameters for individual stars with precisions better than 1 microarcsecond (mas). The main sources of error include limb darkening effect, gravity brightening effect, and rotational distortion effect. These effects depend strongly on the spectral type of observed stars. For example, we find that the rotationally distorted shape of late-type giant stars may lead to an apparent shift of up to several mas between their positions measured at different epochs. This is comparable or even larger than the accuracy goal set for Gaia mission. Therefore it will be necessary to take into account these effects when analyzing data obtained by future space missions. Keywords: Astrometry, Gravity",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Limits of ultra - high - precision optical astrometry : Stellar ground objects . Abstract : We present the results of our analysis on stellar surface structure effects in high precision space - based optical astrometric observations , which are expected to be obtained by future space missions such as Gaia and SIM .We suggest that these changes can cause deliberate errors in the derived measurements for individual stars with precisions good than 1 microarcsecond ( mas ) . The main sources of mistake involve limb darkening effect , gravity brightening effect , and rotational interference effect .These effects depend greatly on the spectral type of seen stars . For instance , we find that the rotationally altered form of late - class giant stars would result to an apparent shift of up to several mas between their positions measured at different epochs .This is identical or especially bigger than the accuracy goal set for Gaia expedition . Therefore it will be required to take into consideration these consequences when examining data received by future space missions .Keywords: Astrometry, Gravity",
        "rewrite_text": "Title: Limits of Ultra-High-Precision Optical Astrometry: Stellar Ground Objects\n\nAbstract: In this study, we investigate the impact of stellar surface structure on high-precision optical astrometric measurements, particularly in the context of upcoming space missions such as Gaia and the Space Interferometry Mission (SIM). Our analysis reveals that these structural variations can introduce significant systematic errors in the astrometric data for individual stars, particularly when measurements are made with a precision exceeding 1 microarcsecond (mas). We identify three primary sources of error: the limb darkening effect, the gravity brightening effect, and the rotational interference effect. Each of these factors is influenced by the spectral type of the observed stars, leading to varying degrees of impact on the astrometric results. For example, our findings indicate that the rotationally modified profiles of late-type giant stars can produce apparent positional shifts of several mas when comparing measurements taken at different times. This level of positional variation is comparable to, or even exceeds, the precision targets established for the Gaia mission. Consequently, it is imperative that these effects are carefully considered when analyzing data from future astrometric missions. Our results underscore the necessity for enhanced models that account for stellar surface phenomena to improve the accuracy of astrometric measurements and to refine our understanding of stellar dynamics and evolution. \n\nKeywords: Astrometry, Gravity Brightening, Limb Darkening, Rotational Interference, Stellar Dynamics.",
        "ori-fast-z-score": 0.10482848367219183,
        "water-fast-z-score": 5.838403593598094,
        "rewrite-fast-z-score": 0.19802950859533489
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Line Emission in the Brightest Cluster Galaxies of the NOAO Fundamental Plane and Sloan Digital Sky Surveys .\nAbstract:\nWe present new measurements of line emission for the brightest cluster galaxies (BCGs) in clusters with z < 0.3, using data obtained by the Chandra X-ray Observatory. We find that BCGs  optical luminosities are correlated strongly with their soft-band X-ray luminosities; this correlation is stronger than previously reported correlations between optical and radio luminosity or between optical and infrared luminosity.  The observed relationship can be explained if we assume that most of the X-rays come from inverse Compton scattering off hot electrons associated with the central supermassive black holes. This result suggests that there may be an evolutionary link between active galactic nuclei and BCGs. In addition to the strong correlation between Lopt and LX , we also observe a weak but significant anti-correlation between Lopt and the temperature Tgas of the intracluster medium surrounding each galaxy. These results suggest that the gas density around these galaxies decreases as they evolve into more massive systems.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Line Emission in the Brightest Cluster Galaxies of the NOAO Fundamental Plane and Sloan Digital Sky Surveys . Abstract : We report new studies of line emission for the brightest cluster clusters ( BCGs ) in clusters with z < 0 . 3 , using data derived by the Chandra X - ray Observatory .We see that BCGs laser luminosities are correlated heavily with their soft - band X - ray luminosities ; this relationship is strengthened than previously reported correlations between optical and radio luminosity or between optical and infrared luminosity . The observed relationship can be described if we suppose that most of the X - rays come from inverse Compton absorption off warm particles associated with the main supermassive black holes .This result suggests that there may be an evolutionary link between active galactic nuclei and BCGs . In addition to the strong correlation between Lopt and LX , we also observe a weak but significant anti - correlation between Lopt and the temperature Tgas of the intracluster medium surrounding each constellation .These data suggest that the gas density around these galaxies reduces as they develop into more massive structures .",
        "rewrite_text": "We present a comprehensive analysis of line emission in the brightest cluster galaxies (BCGs) located within clusters at redshifts less than 0.3, utilizing data obtained from the Chandra X-ray Observatory. Our findings reveal a robust correlation between the laser luminosities of BCGs and their soft-band X-ray luminosities. This correlation is notably stronger than previously documented relationships involving optical and radio luminosities, as well as those between optical and infrared luminosities. We propose that this observed relationship can be explained by the predominance of X-ray emissions resulting from inverse Compton scattering off warm particles that are associated with the central supermassive black holes in these galaxies. This insight implies a potential evolutionary connection between active galactic nuclei (AGN) and BCGs, suggesting that the processes governing their development may be intertwined.\n\nFurthermore, our analysis uncovers a weak yet statistically significant anti-correlation between the optical luminosity (Lopt) of BCGs and the temperature (Tgas) of the intracluster medium that envelops these galaxies. This observation indicates that as BCGs evolve into more massive structures, the gas density in their vicinity diminishes. These results contribute to our understanding of the complex interactions between BCGs, their surrounding environments, and the role of supermassive black holes in the evolution of galaxy clusters. Overall, our study highlights the intricate relationships between various forms of luminosity and the physical conditions of the intracluster medium, paving the way for further investigations into the dynamics of BCGs and their environments.",
        "ori-fast-z-score": 1.1322770341445956,
        "water-fast-z-score": 5.962965874907927,
        "rewrite-fast-z-score": 1.0634101379502299
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Studies of Cosmic Rays with GeV Gamma Rays .\nAbstract:\nThe Fermi Large Area Telescope (LAT) is the primary instrument on NASA s Fermi satellite, launched in June 2008 to study high-energy gamma rays and cosmic rays. The LAT has been taking data since August 4, 2008, and it continues to operate today. This talk will present highlights of recent results obtained by the LAT collaboration using this unique dataset. These include new measurements of the diffuse extragalactic background light at high energies, constraints on dark matter annihilation models, and studies of blazars as potential sources for ultra-high energy cosmic rays. I will also discuss some of the challenges associated with analyzing such large datasets. Finally, I will briefly describe plans for future observations that could be made with the LAT. Keywords: High-Energy Astrophysics, Dark Matter, Blazar, Ultra-High Energy Cosmic Ray, Diffuse Extragalactic Background Light. Title: Studies of Cosmic Rays With GeV Gamma Rays",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Studies of Cosmic Rays with GeV Gamma Rays . Abstract : The Fermi Large Area Telescope ( LAT ) is the primary instrument on NASA s Fermi satellite , launched in June 2008 to study high - energy beta particles and cosmic rays .The LAT has been took results since August 4 , 2008 , and it continues to functioning today . This discussion will present highlights of recent results collected by the LAT collaboration using this remarkable dataset .These include new surveys of the diffuse extragalactic background light at high energies , constraints on dark matter annihilation models , and investigations of blazars as possible sources for ultra - large energy cosmic rays . I will also discuss some of the challenges associated with analyzing such enormous datasets .Finally , I will briefly outline plans for future discoveries that might be made with the LAT . Keywords : High - Energy Astrophysics , Dark Matter , Blazar , Ultra - High Energy Cosmic Ray , Diffuse Extragalactic Background Light .Title : Studies of Cosmic Rays With GeV Gamma Rays",
        "rewrite_text": "**Title:** Investigating Cosmic Rays Through GeV Gamma Rays\n\n**Abstract:** The Fermi Large Area Telescope (LAT), the key instrument aboard NASA's Fermi satellite launched in June 2008, plays a pivotal role in the exploration of high-energy beta particles and cosmic rays. Since it began operations on August 4, 2008, the LAT has been continuously collecting data, contributing significantly to our understanding of the universe. This article highlights recent findings from the LAT collaboration, showcasing the extensive dataset that has been amassed over the years. Key results include comprehensive surveys of the diffuse extragalactic background light at high energies, which provide insights into the universe's structure and evolution. Additionally, the LAT has offered crucial constraints on dark matter annihilation models, enhancing our understanding of this elusive component of the cosmos. Investigations into blazars, which are active galactic nuclei exhibiting extreme brightness, have also been conducted to assess their potential as sources of ultra-high-energy cosmic rays. The analysis of such vast datasets presents numerous challenges, including data processing, noise reduction, and the interpretation of complex astrophysical phenomena. This discussion will address these challenges and the methodologies employed to overcome them. Furthermore, I will outline prospective avenues for future research and discoveries that the LAT may facilitate, potentially reshaping our comprehension of high-energy astrophysics. The findings presented here underscore the LAT's significance in advancing our knowledge of cosmic rays and their interactions within the universe. \n\n**Keywords:** High-Energy Astrophysics, Dark Matter, Blazar, Ultra-High Energy Cosmic Rays, Diffuse Extragalactic Background Light.",
        "ori-fast-z-score": 0.7385489458759964,
        "water-fast-z-score": 4.695048270344999,
        "rewrite-fast-z-score": 0.6767155423319645
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Search for gravitational waves from binary inspirals in S3 and S4 LIGO data .\nAbstract:\nWe present the results of searches for gravitational wave signals from compact binary coalescences (CBCs) using data collected by the third and fourth science runs (S3, S4) of the Laser Interferometer Gravitational-Wave Observatory (LIGO). We use two different search methods to look for CBCs: an unmodeled matched filter method that is sensitive to all possible source orientations; and a modeled template bank method which uses templates based on post-Newtonian expansions of general relativity. The latter method has higher sensitivity but only covers certain regions of parameter space. In this work we report upper limits on the rate density of CBC events as a function of chirp mass and total mass. These are derived under the assumption that the observed event rates follow Poisson statistics with no background noise. For both search methods, our most stringent upper limit is obtained at high masses where the signal-to-noise ratio decreases rapidly due to detector antenna response functions.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Search for gravitational waves from binary inspirals in S3 and S4 LIGO data . Abstract : We report the results of investigations for gravitational wave signals from compact binary coalescences ( CBCs ) using data taken by the third and fifth science ran ( S3 , S4 ) of the Laser Interferometer Gravitational - Wave Observatory ( LIGO ) .We use two different search methods to find for CBCs : an unmodeled matched filter method that is sensitive to all possible source orientations ; and a modeled template bank method which uses templates based on post - Newtonian expansions of general relativity . The latter technique has larger sensitivity but only covers certain regions of parameter space .In this research we publish upper limits on the rate concentration of CBC events as a function of chirp mass and total mass . These are derived under the assumption that the reported event speeds follow Poisson statistics with no background noise .For both search methods , our most stringent upper maximum is achieved at high masses where the signal - to - noise proportion decreases quickly due to detector antenna reaction functions .",
        "rewrite_text": "In this article, we present our findings from the search for gravitational wave signals originating from compact binary coalescences (CBCs) using data collected during the third and fifth science runs (S3 and S4) of the Laser Interferometer Gravitational-Wave Observatory (LIGO). Our investigation employs two distinct methodologies to detect CBCs: an unmodeled matched filter approach, which is designed to be sensitive to all potential orientations of the sources, and a modeled template bank method that utilizes templates derived from post-Newtonian expansions of general relativity. While the modeled approach offers greater sensitivity, it is limited to specific regions within the parameter space. \n\nIn our analysis, we provide upper limits on the rate density of CBC events, expressed as a function of both chirp mass and total mass. These limits are calculated under the assumption that the detected event rates adhere to Poisson statistics, with the absence of background noise. Our results indicate that the most stringent upper limits are obtained at higher mass ranges, where the signal-to-noise ratio diminishes rapidly due to the response characteristics of the detector's antenna. This research contributes to the understanding of gravitational wave sources and enhances the sensitivity of future searches for CBCs, paving the way for more comprehensive studies in the field of gravitational wave astronomy.",
        "ori-fast-z-score": -0.3110855084191276,
        "water-fast-z-score": 4.125684985035173,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Cascade Model for Particle Concentration and Enstrophy in Fully Developed Turbulence with Mass Loading Feedback .\nAbstract:\nWe present an analytical model to study the effect of mass loading feedback on particle concentration and enstrophy in fully developed turbulence. The cascade process is modeled by using a set of coupled ordinary differential equations, which are derived based on dimensional analysis and Kolmogorov s similarity hypothesis. We show that the presence of particles can significantly affect both the energy transfer rate between different scales as well as the dissipation rates at small scales. In particular, we find that the total amount of energy transferred into smaller scales decreases when there exists significant mass loading feedback. This result suggests that the presence of heavy particles may lead to reduced turbulent mixing efficiency. Finally, our results also indicate that the effects of mass loading feedback become more pronounced if the Stokes number increases or the initial volume fraction of particles becomes larger. Our findings provide useful insights into understanding how heavy particles influence the dynamics of fluid flows. C \nAuthor(s): Yi-Chun Chen , Shih-Chieh Hwang , Chia-Hui Wu , Yu-Ting Lin , Ming-Yuan Liu , Chao-Lin Wang , Jie-Sheng Huang , Wen-Ju Tsai , Tzi-Chao Chan , Chin-Fa Lee , Kuo-Yang Chang , Chung-Ming Yeh , Yuan-Kang Chiou , Chien-Nan Chu , Cheng-Wei Hsieh , Chien-Wen Lu , Chien-Chung Wu , Chien-Shu Chen , Chien-Chin Wu , Chien-Chin Yang , Chien-Chin Lai , Chien-Chin Su , Chien-Chin Hung , Chien-Chin Chen , Chien-Ching Wu , Chien-Ching Tai , Chien-Ching Li , Chien-Ching Sun , Chien-Ching Liang , Chien-Ching Chen , Chien-Chong Wu , Chien-Chung Chen , Chien-Chung Lai , Chien-Chung Su , Chien-Chung Hung , Chien-Chung",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Cascade Model for Particle Concentration and Enstrophy in Fully Developed Turbulence with Mass Loading Feedback . Abstract : We present an analytical theory to study the impact of mass loading feedback on particle concentration and enstrophy in fully developed turbulence .The cascade process is modeled by using a setting of coupled ordinary differential coefficients , which are derived based on dimensional theory and Kolmogorov s similarity hypothesis . We see that the presence of particles can significantly affect both the power transfer frequency between various scales as well as the dissipation levels at small scales .In particular , we find that the total quantity of power sent into smaller scales decreases when there exists significant mass loading feedback . This result suggests that the presence of large particles may contribute to reduced turbulent mixing efficiency .Finally , our findings also suggest that the effects of mass displacement feedback become more pronounced if the Stokes number increases or the initial volume fraction of molecules remains bigger . Our findings provide useful insights into knowledge how heavy molecules affect the dynamics of fluid flows .C \nAuthor(s): Yi-Chun Chen , Shih-Chieh Hwang , Chia-Hui Wu , Yu-Ting Lin , Ming-Yuan Liu , Chao-Lin Wang , Jie-Sheng Huang , Wen-Ju Tsai , Tzi-Chao Chan , Chin-Fa Lee , Kuo-Yang Chang , Chung-Ming Yeh , Yuan-Kang Chiou , Chien-Nan Chu , Cheng-Wei Hsieh , Chien-Wen Lu , Chien-Chung Wu , Chien-Shu Chen , Chien-Chin Wu , Chien-Chin Yang , Chien-Chin Lai , Chien-Chin Su , Chien-Chin Hung , Chien-Chin Chen , Chien-Ching Wu , Chien-Ching Tai , Chien-Ching Li , Chien-Ching Sun , Chien-Ching Liang , Chien-Ching Chen , Chien-Chong Wu , Chien-Chung Chen , Chien-Chung Lai , Chien-Chung Su , Chien-Chung Hung , Chien-Chung",
        "rewrite_text": "**Title:** A Cascade Model for Particle Concentration and Enstrophy in Fully Developed Turbulence with Mass Loading Feedback\n\n**Abstract:** In this study, we introduce a comprehensive analytical framework to investigate the influence of mass loading feedback on particle concentration and enstrophy within fully developed turbulent flows. Our approach employs a cascade model characterized by a system of coupled ordinary differential equations, which are formulated based on dimensional analysis and the Kolmogorov similarity hypothesis. Our results indicate that the introduction of particles into the turbulent medium significantly alters the power transfer dynamics across various scales, as well as the dissipation rates at smaller scales. Notably, we observe a reduction in the total power transmitted to smaller scales when substantial mass loading feedback is present. This finding implies that larger particles may hinder the efficiency of turbulent mixing processes. Furthermore, our analysis reveals that the impact of mass displacement feedback becomes increasingly significant with higher Stokes numbers or when the initial volume fraction of particles is elevated. These insights enhance our understanding of how heavy particles influence fluid dynamics, particularly in turbulent environments. Our research contributes to the broader knowledge of turbulence and its interactions with particulate matter, offering valuable implications for various applications in engineering and environmental science.",
        "ori-fast-z-score": 0.38138503569823695,
        "water-fast-z-score": 5.669467095138408,
        "rewrite-fast-z-score": 0.4745789978762495
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Anisotropic Distribution of Satellite Galaxies .\nAbstract:\nWe present the results of an analysis of the anisotropy in the distribution of satellite galaxies around isolated field galaxies, using data obtained by the Sloan Digital Sky Survey (SDSS). We find that there is no significant difference between the distributions for satellites with different luminosities or colors and those found around central cluster galaxies. The observed anisotropies are consistent with predictions based on tidal forces acting during galaxy mergers. This suggests that these effects may be responsible for the formation of both clusters and groups of galaxies. \n \n Keywords: Galaxy merger, Group/cluster of galaxies, Tidal stripping, SDSS, Isolated galaxy \n \n \n \n 1 Introduction \n \n Clusters of galaxies contain many thousands of galaxies which reside within a common dark matter halo. These systems form through gravitational collapse driven by the mutual attraction of their constituent galaxies. However, it remains unclear how this process occurs over time-scales ranging from individual galaxy interactions to the assembly of massive clusters containing hundreds of member galaxies. In particular, we do not know whether all galaxies evolve into members of large clusters or if some fraction remain as isolated field galaxies throughout cosmic history. \n \n 2 Previous Work \n \n Several studies have investigated the properties of satellite galaxies surrounding brightest cluster galaxies (BCGs) at low redshifts z < 0.1. For example, Carlberg et al. (1997), Lin & Mohr (2004a), and Hansen et al. (2005) used samples of BCG-satellite pairs selected from optical surveys such as the Palomar Observatory Sky Survey (POSS-II; Reid et al., 1991) and the Sloan Digital Sky Surveys (SDSS; York et al., 2000). They found that the number density profiles of satellite galaxies show strong deviations from spherical symmetry, indicating that they are distributed anisotropically about their host galaxies. Furthermore, they showed that the degree of anisotropy depends strongly on the projected distance from the center of the host galaxy. At small distances, the radial profile shows a steep decline towards the center of the host while the tangential component increases rapidly beyond a characteristic radius R",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Anisotropic Distribution of Satellite Galaxies . Abstract : We present the conclusion of an assessment of the anisotropy in the distribution of satellite galaxies around isolated field galaxies , using data acquired by the Sloan Digital Sky Survey ( SDSS ) .We see that there is no major variation between the distributions for satellites with various luminosities or colors and those present around central cluster clusters . The observed anisotropies are compatible with predictions based on tidal forces working during galaxy mergers .This implies that these influences might be responsible for the formation of both clusters and groups of galaxies . Keywords : Galaxy consolidation , Group / cluster of galaxies , Tidal stripping , SDSS , Isolated galaxy 1 Introduction Clusters of galaxies contain many thousands of galaxies which reside within a common dark matter halo .These systems emerge through gravity collapse driven by the mutual proximity of their constituent galaxies . However , it remains unsure how this process occurs over time - scales ranging from individual galaxy encounters to the assembly of large clusters containing hundreds of member galaxies .In particular , we do not understand whether all galaxies evolve into members of large clusters or if some fraction remain as isolated field galaxies throughout cosmic life . 2 Previous Work Several studies have researched the properties of satellite galaxies surrounding brightest cluster clusters ( BCGs ) at low redshifts z < 0 . 1 .For instance , Carlberg et al . ( 1997 ) , Lin & Mohr ( 2004a ) , and Hansen et al .( 2005 ) used samples of BCG - satellite pairs selected from optical searches such as the Palomar Observatory Sky Survey ( POSS - II ; Reid et al . , 1991 ) and the Sloan Digital Sky Surveys ( SDSS ; York et al . , 2000 ) . They found that the number density profiles of satellite galaxies show strong deviations from spherical symmetry , showing that they are distributed anisotropically about their host galaxies .Furthermore , they demonstrated that the degree of anisotropy depends strongly on the projected distance from the center of the host galaxy . At small distances , the transverse profile displays a sharp decline towards the center of the host while the tangential component increases quickly beyond a typical radius R",
        "rewrite_text": "**Title:** The Anisotropic Distribution of Satellite Galaxies\n\n**Abstract:** In this study, we investigate the anisotropic distribution of satellite galaxies surrounding isolated field galaxies, utilizing data from the Sloan Digital Sky Survey (SDSS). Our analysis reveals that there is no significant difference in the distribution patterns of satellites based on their luminosities or colors when compared to those found around central cluster galaxies. The anisotropies observed in our data align with theoretical predictions that suggest tidal forces during galaxy mergers play a crucial role in shaping these distributions. This finding indicates that such tidal interactions may be instrumental in the formation processes of both galaxy clusters and groups. \n\nThe formation of galaxy clusters, which host thousands of galaxies within a shared dark matter halo, is primarily driven by gravitational collapse resulting from the close proximity of these galaxies. However, the mechanisms governing this evolution over various timescales—from individual galaxy interactions to the assembly of extensive clusters—remain poorly understood. A key question in this domain is whether all galaxies ultimately become part of large clusters or if a subset persists as isolated field galaxies throughout cosmic history.\n\nPrevious research has extensively examined the characteristics of satellite galaxies surrounding the brightest cluster galaxies (BCGs) at low redshifts (z < 0.1). Notable studies by Carlberg et al. (1997), Lin & Mohr (2004a), and Hansen et al. (2005) utilized samples of BCG-satellite pairs sourced from optical surveys, including the Palomar Observatory Sky Survey (POSS-II) and the SDSS. Their findings indicated that the number density profiles of satellite galaxies exhibit significant deviations from spherical symmetry, confirming their anisotropic distribution around host galaxies. Additionally, they demonstrated that the level of anisotropy is strongly correlated with the projected distance from the host galaxy's center, with a pronounced decline in the transverse profile at close distances and a rapid increase in the tangential component beyond a characteristic radius. \n\n**Keywords:** Galaxy formation, Galaxy groups/clusters, Tidal interactions, SDSS, Isolated galaxies.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.437961803049794,
        "rewrite-fast-z-score": 0.7856742013183862
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Cosmic-Ray Events as Background in Imaging Atmospheric Cherenkov Telescopes .\nAbstract:\nThe detection and study of cosmic rays is one of the main goals for imaging atmospheric Cherenkov telescopes (IACTs). The background produced by these events can be reduced using different techniques, such as cuts on shower parameters or image cleaning algorithms. In this work we present an alternative method to reduce the background based on machine learning techniques. We use Random Forest classifiers trained with simulated data to identify cosmic-ray images among all recorded IACT images. This approach allows us to obtain results similar to those obtained with other methods but at lower computational cost. Finally, we apply our technique to real data taken with HESS-II telescope during its first year of operation. Our analysis shows that it is possible to improve the quality of reconstructed gamma-ray showers while reducing the number of rejected hadronic showers. \n \n Keywords: Machine Learning; Cosmic Ray Identification; Image Cleaning; Gamma-ray Astrophysics; Hadronic Shower",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Cosmic - Ray Events as Background in Imaging Atmospheric Cherenkov Telescopes . Abstract : The observation and investigation of cosmic rays is one of the main goals for imaging atmospheric Cherenkov telescopes ( IACTs ) .The background formed by these experiences can be reduced use different methods , such as cuts on shower parameters or image cleaning algorithms . In this research we present an additional method to reduce the background based on machine learning techniques .We use Random Forest classifiers trained with simulated evidence to identify cosmic - ray pictures among all collected IACT images . This method enables us to obtain results comparable to those acquired with other methods but at lower computational expensive .Finally , we apply our technique to real information taken with HESS - II telescope during its initial season of operation . Our study shows that it is possible to upgrade the performance of reconstructed cosmic - ray showers while reducing the quantity of rejected hadronic showers .Keywords: Machine Learning; Cosmic Ray Identification; Image Cleaning; Gamma-ray Astrophysics; Hadronic Shower",
        "rewrite_text": "Title: Cosmic-Ray Events as Background in Imaging Atmospheric Cherenkov Telescopes\n\nAbstract: The study and observation of cosmic rays represent a fundamental objective for imaging atmospheric Cherenkov telescopes (IACTs). These telescopes face challenges due to the background noise generated by cosmic-ray events, which can obscure the signals of interest. Various techniques have been developed to mitigate this background, including the application of cuts on shower parameters and the implementation of image cleaning algorithms. In this article, we introduce a novel approach that leverages machine learning techniques to enhance background reduction. Specifically, we employ Random Forest classifiers that have been trained on simulated data to effectively distinguish cosmic-ray images from the vast array of collected IACT images. This innovative method not only yields results that are comparable to traditional techniques but also does so with significantly lower computational costs. To validate our approach, we applied our machine learning technique to real observational data obtained from the HESS-II telescope during its inaugural operational season. Our findings demonstrate that this method can improve the performance of reconstructed cosmic-ray showers while simultaneously decreasing the number of hadronic showers that are incorrectly rejected. This advancement has important implications for gamma-ray astrophysics, as it enhances the accuracy of cosmic-ray identification and contributes to more precise measurements in the field. The integration of machine learning into the analysis of IACT data represents a promising direction for future research, potentially leading to more efficient and effective methods for cosmic-ray event classification and background reduction. \n\nKeywords: Machine Learning; Cosmic Ray Identification; Image Cleaning; Gamma-ray Astrophysics; Hadronic Shower.",
        "ori-fast-z-score": 0.22086305214969307,
        "water-fast-z-score": 5.597977259474208,
        "rewrite-fast-z-score": -0.43685202833051895
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Scanning and Sequential Decision Making for Multi-Dimensional Data - Part II: the Noisy Case .\nAbstract:\nIn this work, we consider the problem of scanning data in multi-dimensional space with noisy measurements. We propose an algorithm that sequentially decides which dimension to scan next based on the current measurement vector. The proposed method is shown to be optimal under certain conditions. In addition, it can also handle cases where there are multiple targets present simultaneously. Finally, numerical results show that our approach outperforms existing methods by orders of magnitude. \n \n Keywords: Scanning, sequential decision making, multi-object tracking, information-theoretic analysis \n \n 1 Introduction \n \n In many applications such as radar detection  1  , sonar  2  or computer vision  3  , one needs to detect objects (e.g., aircrafts) in multi-dimensional spaces using limited resources. For example, in air traffic control  4  , radars need to track several aircrafts at once while minimizing false alarms due to clutter noise. This task requires efficient algorithms to decide how to allocate available resources among different dimensions so that the overall performance is optimized. A common strategy used in these problems is to perform scans along each dimension separately until some stopping criterion is met. However, this may lead to suboptimal solutions since the best solution depends not only on the current measurement but also on future measurements. Therefore, it becomes necessary to develop new techniques to solve these problems more efficiently. \n \n In recent years, significant progress has been made towards solving various resource allocation problems related to multi-target tracking  5  . Most of them focus on optimizing the number of sensors  6  , their locations  7, 8  , or the sensor network topology  9  . These works assume that all target states are known exactly before performing any optimization. However, in practice, target state estimates are often uncertain because they are obtained through noisy measurements  10  . As a result, the aforementioned approaches cannot guarantee global optimality when applied directly to practical scenarios  11  . \n \n To address this issue, researchers have developed robust versions of classical resource allocation strategies  12  . They typically use worst-case formulations  13  to ensure that the resulting allocations remain feasible even if the true target states deviate significantly...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Scanning and Sequential Decision Making for Multi - Dimensional Data - Part II : the Noisy Case . Abstract : In this study , we investigate the issue of scanning data in multi - dimensional space with noisy measurements .We suggest an algorithm that sequentially decides which dimension to scan next according on the current measurement vector . The proposed approach is demonstrated to be appropriate under certain conditions .In addition , it can also handle cases where there are multiple targets present concurrently . Finally , numerical findings show that our approach outperforms previous techniques by orders of magnitude .Keywords : Scanning , sequential decision making , multi - object tracking , info - theoretic analysis 1 Introduction In many applications such as radar detection 1 , sonar 2 or computer vision 3 , one needs to identify images ( e . g . , aircrafts ) in multi - dimensional spaces utilizing limited resources . For instance , in airline traffic control 4 , radars need to track numerous aircrafts at once while minimizing false alarms due to clutter sound .This job needs efficient methods to choose how to allocate available resources among different dimensions so that the overall performance is optimized . A typical strategy used in these problems is to conduct scans along each dimension differently until some stops criterion is reached .However , this might lead to suboptimal solutions since the best solution depends not only on the present observation but also on future measurements . Therefore , it becomes necessary to develop new tactics to solve these problems more efficiently .In recent years , substantial work has been achieved towards solving various resource allocation tasks related to multi - target tracking 5 . Most of them focus on optimizing the number of monitors 6 , their destinations 7 , 8 , or the sensor network topology 9 .These works assume that all target states are known exactly before performing any algorithms . However , in practice , target state measurements are often uncertain because they are derived through noisy measurements 10 .As a result , the aforementioned approaches lack guarantee global optimality when applied directly to practical situations 11 . To address this question , researchers have developed stable editions of classical asset distribution tactics 12 .They generally using worst - case formulations 13 to ensure that the resulting allocations remain viable even if the true target states deviate substantially . . .",
        "rewrite_text": "**Title:** Scanning and Sequential Decision Making for Multi-Dimensional Data - Part II: The Noisy Case\n\n**Abstract:** This paper explores the challenges associated with scanning multi-dimensional data in the presence of noisy measurements. We propose a novel algorithm that sequentially determines the next dimension to scan based on the current measurement vector. Our approach is shown to be effective under specific conditions and is capable of addressing scenarios with multiple concurrent targets. The algorithm's performance is quantitatively assessed through numerical simulations, revealing significant improvements over existing methods, often by several orders of magnitude. \n\nIn various applications, such as radar detection, sonar, and computer vision, the ability to identify objects (e.g., aircraft) in multi-dimensional spaces is crucial, particularly when resources are limited. For instance, in air traffic control, radars must simultaneously track numerous aircraft while minimizing false alarms caused by background noise. This necessitates efficient strategies for resource allocation across different dimensions to optimize overall performance. \n\nTraditionally, a common approach involves scanning each dimension independently until a stopping criterion is met. However, this method can lead to suboptimal outcomes, as the optimal decision relies not only on current observations but also on anticipated future measurements. Consequently, there is a pressing need for innovative strategies to enhance the efficiency of these processes.\n\nRecent advancements have been made in addressing various resource allocation challenges related to multi-target tracking. Most existing research focuses on optimizing the number of monitors, their locations, or the configuration of sensor networks. These studies typically assume precise knowledge of target states prior to algorithm implementation. However, in real-world scenarios, measurements are often subject to noise, leading to uncertainty in target states. As a result, traditional approaches may not guarantee global optimality when applied in practice. To tackle this issue, researchers have developed robust versions of classical resource allocation strategies, often employing worst-case formulations to ensure that allocations remain effective even when true target states vary significantly. \n\n**Keywords:** Scanning, sequential decision making, multi-object tracking, information-theoretic analysis.",
        "ori-fast-z-score": 1.346874289515838,
        "water-fast-z-score": 8.812296276031898,
        "rewrite-fast-z-score": 0.7142857142857143
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Dynamical State of the Starless Dense Core FeSt 1-457: A Pulsating Globule? .\nAbstract:\nWe present new near-infrared (NIR) and millimeter-wave observations toward the starless dense core FeSt 1-457, which is located in the Taurus molecular cloud complex at a distance of 140 pc. The NIR data were obtained with the Subaru telescope using the SofI instrument on 2005 December 8-9 under photometric conditions. We detected no point sources down to Ks = 20 mag within an area of 0.5 arcmin2 centered on the peak position of the dust continuum emission observed by SCUBA-2 on JCMT. In addition, we found that there are two peaks in the 1.3 mm continuum map made with MAMBO-II on IRAM 30 m telescope. These results suggest that this object may be a protostellar candidate or a prestellar core surrounded by infalling envelopes. To investigate its dynamical state further, we carried out high-resolution interferometric observations with Nobeyama 45-m radio telescope. Our results show that the central part of the core has a velocity gradient along the east-west direction, suggesting that it is collapsing.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  The Dynamical State of the Starless Dense Core FeSt 1-457: A Pulsating Globule?.Abstract : We report new near - infrared ( NIR ) and millimeter - wave images toward the starless dense core FeSt 1 - 457 , which is situated in the Taurus molecular dust complex at a distance of 140 pc . The NIR data were obtained with the Subaru observatory using the SofI instrument on 2005 December 8 - 9 under photometric circumstances .We observed no point sources down to Ks = 20 mag within an area of 0 . 5 arcmin2 centered on the peak status of the dust continuum emission observed by SCUBA - 2 on JCMT . In addition , we reported that there are two peaks in the 1 . 3 cm continuum image produced with MAMBO - II on IRAM 30 m observatory .These data suggest that this body may be a protostellar candidate or a prestellar core flanked by infalling envelopes . To explore its dynamical state further , we conducted out large - resolution interferometric observations with Nobeyama 45 - m radio telescope .Our results show that the main region of the core has a speed gradient along the east - west direction , showing that it is sinking .",
        "rewrite_text": "**Title:** The Dynamical State of the Starless Dense Core FeSt 1-457: A Pulsating Globule?\n\n**Abstract:** In this study, we present new near-infrared (NIR) and millimeter-wave imaging of the starless dense core FeSt 1-457, located within the Taurus molecular cloud complex at a distance of 140 parsecs. The NIR observations were conducted at the Subaru Observatory using the SofI instrument on December 8-9, 2005, under optimal photometric conditions. Our analysis revealed no point sources down to a magnitude of Ks = 20 within a 0.5 arcminute² area centered on the peak of the dust continuum emission detected by SCUBA-2 at the James Clerk Maxwell Telescope (JCMT). Additionally, we identified two distinct peaks in the 1.3 cm continuum image obtained with the MAMBO-II instrument at the IRAM 30 m telescope. These findings indicate that FeSt 1-457 may represent a protostellar candidate or a prestellar core surrounded by infalling envelopes.\n\nTo further investigate the dynamical state of this core, we performed high-resolution interferometric observations using the Nobeyama 45-m radio telescope. Our results reveal a significant velocity gradient across the core, oriented along the east-west direction, suggesting that the core is in a state of contraction. This behavior is indicative of a dynamic process potentially linked to the formation of stars. The combination of our NIR and millimeter-wave data provides critical insights into the physical conditions and evolutionary status of FeSt 1-457, contributing to the broader understanding of star formation in dense molecular environments. The implications of these findings are discussed in the context of the lifecycle of dense cores and their role in the star formation process.",
        "ori-fast-z-score": -1.9123657749350298,
        "water-fast-z-score": 4.153735803678487,
        "rewrite-fast-z-score": 0.19611613513818404
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  New flaring of an ultraluminous X-ray source in NGC 1365 .\nAbstract:\nWe report on the discovery of new, bright X-ray emission from the central region of the galaxy cluster Abell 2597 (z = 0.0176). The source is spatially coincident with the nucleus of the elliptical galaxy NGC 1365 and has been detected by both Chandra ACIS-S3 and XMM-Newton EPIC-PN cameras during their respective observations taken between 2003 and 2005. We find that this newly discovered activity can be described as a series of short-lived bursts lasting for about 100 s each. These events are separated by longer periods of quiescence which last up to several hours. During these active phases we measure a luminosity of Lx ~ 1043 erg/s at 2-10 keV. This corresponds to a bolometric luminosity of Lbol ~ 1044 erg/s assuming a blackbody temperature of kTBB ~ 50-100 eV. Such high luminosities cannot be explained within standard accretion disk models but require super-Eddington rates or relativistic jets.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : New flaring of an ultraluminous X - ray source in NGC 1365 . Abstract : We report on the discovery of new , brilliant X - ray radiation from the central region of the galaxy cluster Abell 2597 ( z = 0 . 0176 ) .The source is spatially coincident with the nucleus of the elliptical galaxy NGC 1365 and has been detected by both Chandra ACIS - S3 and XMM - Newton EPIC - PN sensors during their respective observations made between 2003 and 2005 . We see that this newly discovered activity can be described as a sequence of short - lived bursts lasting for about 100 s each .These events are separated by wider periods of quiescence which run up to several hours . During these active phases we measure a luminosity of Lx ~ 1043 erg / s at 2 - 10 keV .This corresponds to a bolometric luminosity of Lbol ~ 1044 erg / s assuming a blackbody cold of kTBB ~ 50 - 100 eV . Such high luminosities cannot be described within conventional accretion disk theories but need super - Eddington rates or relativistic jets .",
        "rewrite_text": "We present findings on the recent detection of intense X-ray emissions originating from the central region of the galaxy cluster Abell 2597 (z = 0.0176). This source is spatially aligned with the nucleus of the elliptical galaxy NGC 1365 and has been observed using both the Chandra ACIS-S3 and XMM-Newton EPIC-PN instruments during their observational campaigns conducted between 2003 and 2005. The newly identified activity is characterized by a series of brief bursts, each lasting approximately 100 seconds, interspersed with longer intervals of quiescence that can extend for several hours. During these active episodes, we have recorded a luminosity of Lx ~ 10^43 erg/s within the 2-10 keV energy range. This measurement translates to a bolometric luminosity of Lbol ~ 10^44 erg/s, assuming a blackbody temperature of kTBB ~ 50-100 eV. The extraordinarily high luminosities observed challenge conventional models of accretion disks, suggesting that the phenomena may be driven by super-Eddington accretion rates or the presence of relativistic jets. These findings not only enhance our understanding of the energetic processes occurring in NGC 1365 but also raise important questions regarding the mechanisms that can produce such extreme luminosities in galactic nuclei. Further investigation into these bursts and their underlying causes is warranted to elucidate the nature of this ultraluminous X-ray source and its implications for the study of accretion physics and galaxy evolution.",
        "ori-fast-z-score": -0.508000508000762,
        "water-fast-z-score": 3.401680257083045,
        "rewrite-fast-z-score": -0.42640143271122083
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Second-Order Type Isomorphisms Through Game Semantics .\nAbstract:\nWe present an approach to type isomorphism based on game semantics, which is the most general framework for interpreting functional languages in terms of strategies and plays between them.  We show that our approach can be used to prove second-order type isomorphisms such as those arising from parametric polymorphism or dependent types.   Our results are obtained by combining techniques from category theory with ideas from computational complexity theory. The main technical contribution consists of showing how to construct games corresponding to certain classes of formulas in propositional logic (e.g., Horn clauses) using only polynomial resources. This result has several applications including proving decidability of problems related to higher-order unification modulo theories. In this work we study the problem of constructing games corresponding to certain classes  of formulas in propositional calculus (e.g., horn clauses). These games will then serve as models for various logics, e.g., modal logic, temporal logic, etc..",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Second - Order Type Isomorphisms Through Game Semantics . Abstract : We present an view to type isomorphism based on game semantics , which is the most general template for interpreting functional languages in terms of strategies and plays between them .We see that our approach can be used to find second - order sort isomorphisms such as those arose from parametric polymorphism or dependent types . Our results are derived by combining techniques from category theory with ideas from theoretical complexity analysis .The main industrial contribution consists of finding how to build games corresponding to specified classes of formulas in propositional logic ( e . g . , Horn clauses ) using only polynomial resources . This result has numerous uses including proving decidability of problems related to higher - order unified modulo theories .In this study we study the question of constructing contests analogous to specified classes of formulas in propositional calculus ( e . g . , horn clauses ) . These games will then serve as models for various logics , e . g . , modal logic , temporal calculus , etc . .",
        "rewrite_text": "In this article titled \"Second-Order Type Isomorphisms Through Game Semantics,\" we explore a novel perspective on type isomorphism grounded in game semantics, which serves as a comprehensive framework for interpreting functional programming languages through the lens of strategies and interactions. Our methodology enables the identification of second-order sort isomorphisms, particularly those that emerge from concepts such as parametric polymorphism and dependent types. The findings presented in this work are the result of an innovative synthesis of techniques from category theory and theoretical complexity analysis.\n\nA significant contribution of this research lies in the development of a systematic approach to constructing games that correspond to specific classes of propositional logic formulas, such as Horn clauses, utilizing only polynomial resources. This advancement has far-reaching implications, including the ability to demonstrate the decidability of various problems associated with higher-order unified modulo theories. \n\nFurthermore, we delve into the construction of contests that mirror designated classes of formulas within propositional calculus. These games are not merely theoretical constructs; they serve as practical models for a range of logical systems, including modal logic and temporal calculus. By bridging the gap between game semantics and type theory, our work opens new avenues for understanding the interplay between logic and computation, paving the way for future research in both theoretical and applied domains. This study not only enhances the theoretical landscape of type isomorphisms but also provides valuable tools for addressing complex logical problems in a resource-efficient manner.",
        "ori-fast-z-score": -0.31799936400190804,
        "water-fast-z-score": 4.9819900360298925,
        "rewrite-fast-z-score": 1.5454545454545454
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Detailed Study of Gas and Star Formation in a Highly Magnified Lyman Break Galaxy at z=3.07 .\nAbstract:\nWe present the results of an extensive study of gas dynamics, star formation activity, dust extinction, stellar populations, and black hole accretion properties for a strongly lensed galaxy (A1689-zD1) at redshift 3.07. The lensing magnification factor is ~30Â±5. We use deep near-infrared spectroscopy to measure the kinematics of molecular hydrogen emission lines with high spatial resolution. Our observations reveal that this system consists of two merging galaxies separated by 1 kpc along the line-of-sight. One of these components shows strong HÎ² emission indicative of active galactic nuclei (AGN). This AGN component has a mass of âˆ¼10^9 M_sol , which corresponds to a supermassive black hole with a mass of âˆ½â€“1 Ã— 10^8 M_sol . Using our spatially resolved measurements we find evidence for intense nuclear starbursts on scales as small as 100 pc.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Detailed Study of Gas and Star Formation in a Highly Magnified Lyman Break Galaxy at z = 3 . 07 . Abstract : We present the conclusion of an extensive research of gas structure , star formation activity , dust extinction , planetary populations , and dark hole accretion properties for a strongly lensed galaxy ( A1689 - zD1 ) at redshift 3 . 07 .The lensing magnification factor is ~ 30Â±5 . We use deep near - infrared spectroscopy to measure the kinematics of molecular hydrogen emission lines with high spatial resolution .Our observations indicate that this system contains of two combining galaxies crossed by 1 kpc along the line - of - seeing . One of these constituents exhibits strong HÎ² emission indicative of active galactic nuclei ( AGN ) .This AGN constituent has a mass of [UNK] ^ 9 M _ sol , which corresponds to a supermassive black hole with a mass of [UNK] “ 1 [UNK] — 10 ^ 8 M _ sol . Using our spatially resolved calculations we find proof for intense nuclear starbursts on scales as low as 100 pc .",
        "rewrite_text": "We present the findings of a comprehensive investigation into the gas dynamics, star formation processes, dust extinction, planetary populations, and accretion characteristics of a dark hole in a highly magnified Lyman Break Galaxy, designated A1689-zD1, located at a redshift of 3.07. The gravitational lensing effect in this case results in a magnification factor of approximately 30 ± 5. Utilizing advanced near-infrared spectroscopy, we have achieved high spatial resolution measurements of the kinematics associated with molecular hydrogen emission lines. Our observations reveal that A1689-zD1 is composed of two merging galaxies, which are aligned along a 1 kpc line of sight. Notably, one of these galaxies displays pronounced Hβ emission, a hallmark of active galactic nuclei (AGN) activity. This AGN component is estimated to have a mass of around 10^9 M_sun, suggesting the presence of a supermassive black hole with a mass in the range of 1 to 10^8 M_sun. Through our spatially resolved analyses, we have uncovered evidence of vigorous nuclear starburst activity occurring on scales as small as 100 parsecs. This study not only enhances our understanding of the intricate processes governing gas and star formation in the early universe but also sheds light on the role of gravitational lensing in revealing the properties of distant galaxies. The implications of our findings contribute significantly to the broader field of astrophysics, particularly in the context of galaxy formation and evolution during the epoch of reionization.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.608176875690327,
        "rewrite-fast-z-score": 2.85745490667645
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Cargese Lectures on Brane Induced Gravity .\nAbstract:\nThe lectures were given by David Gross at the Cargese Summer Institute in Corsica, France during August 2005.  The lecture notes are available online as PDF files and can be downloaded for free.   These lecture notes cover topics such as:  - Introduction to string theory - Gauge fields and gauge symmetries - Supersymmetry - Supergravity - String field theories - D-branes - Open strings - Closed strings - Tachyons - Bosonic open strings - Fermionic open strings - Vacuum expectation values - Energy-momentum tensors - Interactions between branes - General relativity - Gravity induced on a brane - Black holes - Cosmology - Time evolution - Conformal invariance - Renormalization group flow - Dualities - M-theory - Other approaches - Comments on future directions - References - Index   This is an excellent resource for learning about many aspects of modern theoretical physics including superstrings, supergravities, black holes, cosmology, time evolution, conformal invariance, renormalization group flows, dualities, M-theory, other approaches etc...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Cargese Lectures on Brane Induced Gravity . Abstract : The seminars were given by David Gross at the Cargese Summer Institute in Corsica , France during August 2005 .The lecture notes are available web as PDF files and can be downloaded for free . These lecture notes cover subjects such as : - Introduction to string theory - Gauge fields and gauge symmetries - Supersymmetry - Supergravity - String field theories - D - branes - Open strings - Closed strings - Tachyons - Bosonic open strings - Fermionic open strings - Vacuum expectation values - Energy - momentum tensors - Interactions between branes - General relativity - Gravity induced on a brane - Black holes - Cosmology - Time evolution - Conformal invariance - Renormalization group flow - Dualities - M - theory - Other approaches - Comments on future directions - References - Index This is an excellent resource for learning about various parts of modern conceptual science covering superstrings , supergravities , white holes , cosmology , time evolution , conformal invariance , renormalization group flows , dualities , M - theory , other methods etc . . .",
        "rewrite_text": "Title: Cargese Lectures on Brane Induced Gravity\n\nAbstract: This article presents a comprehensive overview of the lectures delivered by David Gross at the Cargese Summer Institute in Corsica, France, in August 2005. The lecture notes, which are freely accessible online in PDF format, delve into a wide array of topics central to modern theoretical physics. Key subjects discussed include an introduction to string theory, which serves as a foundational framework for understanding various phenomena in high-energy physics. The notes explore gauge fields and gauge symmetries, highlighting their significance in formulating physical theories. Additionally, the lectures cover supersymmetry and supergravity, which are pivotal in bridging quantum mechanics and general relativity.\n\nThe content further examines string field theories, focusing on D-branes and the dynamics of both open and closed strings. Important concepts such as tachyons, vacuum expectation values, and energy-momentum tensors are also addressed, providing insights into the interactions between branes and their implications for gravity. The lectures extend into discussions on general relativity, the nature of gravity induced on a brane, and the formation of black holes within this context.\n\nCosmological aspects, including time evolution and conformal invariance, are explored alongside the renormalization group flow, which is crucial for understanding the behavior of physical systems at different energy scales. The concept of dualities is also examined, shedding light on the relationships between seemingly disparate theories. Furthermore, the lectures touch upon M-theory and other theoretical approaches, offering a glimpse into future directions for research in this dynamic field.\n\nOverall, these lecture notes serve as an invaluable resource for anyone interested in the intricate and evolving landscape of modern theoretical physics, encompassing topics such as superstrings, supergravities, cosmology, and beyond. The comprehensive references and index included enhance the utility of this material for both students and researchers alike.",
        "ori-fast-z-score": 2.0465595024580763,
        "water-fast-z-score": 4.063777271736939,
        "rewrite-fast-z-score": 0.4181210050035454
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Probing non-standard decoherence effects with solar and KamLAND neutrinos .\nAbstract:\nWe study the possibility that nonstandard interactions (NSI) between neutrinos and matter can be probed by using solar and reactor neutrino data simultaneously, in particular through their combined effect on the survival probability P(νe→νe). We find that NSI parameters are constrained to values below 0.1 for most combinations of standard oscillation parameters allowed at 3σ CL by current global fits. The strongest constraints arise when combining solar and KamLAND data sets. In this case we obtain upper bounds on |εee|, |εµτ | < 0.06 − 0.07 depending on the value of θ13. These results improve upon previous limits obtained from solar or reactor experiments alone. \n \n Introduction \n \n Neutrino oscillations have been observed in many different types of experiments  1  . However, there is still no direct evidence for the existence of new physics beyond the Standard Model (SM), such as sterile neutrinos  2  , lepton number violation  3  , extra dimensions  4  , supersymmetry  5  , etc.. Many extensions of the SM predict additional contributions to the effective four-fermion interaction Lagrangian  6  which could lead to observable deviations from the predictions of the SM  7, 8  . For example, it has recently been shown  9  that some models of quantum gravity  10  may induce an energy dependent refractive index n = 1 + εE/E0 where E0 is a characteristic scale associated with the underlying theory  11  . This would result in a modification of the vacuum mixing angle sin2θ12 = 1−cos2θ12 ≈ 1+ε/2+O(ε3)  12  leading to potentially large effects on the propagation of neutrinos  13  .\n \nIn addition to these theoretical motivations, there exist several experimental indications pointing towards possible new physics beyond the SM  14  : i) Large atmospheric  15  and solar  16  neutrino flux deficits; ii) LSND  17  and MiniBooNE  18  anomalies indicating short-baseline νμ → νe appearance transitions not predicted within three-flavor neutrino oscillations  19  ; iii) Anomalies in the measurement of the muon anomalous magnetic moment",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Probing non - customary decoherence effects with solar and KamLAND neutrinos . Abstract : We research the prospect that nonstandard relationships ( NSI ) between neutrinos and matter can be probed by using solar and reactor neutrino data simultaneously , in particular through their combined influence on the life probability P ( νe→νe ) .We see that NSI variables are constrained to parameters below 0 . 1 for most combinations of standard oscillation parameters allowed at 3σ CL by current global fits . The strongest limits arise when combining solar and KamLAND data sets .In this situation we obtain upper limits on | εee | , | εµτ | < 0 . 06 − 0 . 07 depending on the value of θ13 . These results enhance upon former limits established from solar or reactor tests alone .Introduction Neutrino oscillations have been observed in different different kinds of studies 1 . However , there is nevertheless no formal evidence for the existence of new theories beyond the Standard Model ( SM ) , such as sterile neutrinos 2 , lepton number violation 3 , extra dimensions 4 , supersymmetry 5 , etc . .Many modifications of the SM predict additional contributions to the effective four - fermion interaction Lagrangian 6 which potentially contribute to observable deviations from the estimates of the SM 7 , 8 . For instance , it has recently been shown 9 that some models of quantum gravitational 10 may generate an energy dependent refractive index n = 1 + εE / E0 where E0 is a typical scale identified with the underlying model 11 .This might lead in a modification of the vacuum mixing ratio sin2θ12 = 1−cos2θ12 ≈ 1 + ε / 2 + O ( ε3 ) 12 leading to potentially large effects on the propagation of neutrinos 13 . In addition to these theoretical motivations , there remain many experimental indications pointing towards possible new science beyond the SM 14 : i ) Large atmospheric 15 and solar 16 neutrino flux deficits ; ii ) LSND 17 and MiniBooNE 18 anomalies suggesting short - baseline νμ → νe appearance processes not anticipated within three - flavor neutrino oscillations 19 ; iii ) Anomalies in the measurement of the muon anomalous magnetic moment",
        "rewrite_text": "**Title:** Probing Non-Customary Decoherence Effects with Solar and KamLAND Neutrinos\n\n**Abstract:** This study investigates the potential for detecting nonstandard interactions (NSI) between neutrinos and matter by analyzing data from solar and reactor neutrinos concurrently, focusing on their collective impact on the survival probability \\( P(\\nu_e \\rightarrow \\nu_e) \\). Our findings indicate that the NSI parameters are constrained to values below 0.1 for most combinations of standard oscillation parameters that are permissible at the 3σ confidence level according to current global analyses. Notably, the most stringent constraints are achieved when integrating data from both solar and KamLAND experiments. In this context, we derive upper limits for the NSI parameters \\( | \\epsilon_{ee} | \\) and \\( | \\epsilon_{\\mu\\tau} | \\), which fall within the range of 0.06 to 0.07, contingent upon the value of \\( \\theta_{13} \\). These results represent an improvement over previous limits obtained from solar or reactor experiments conducted in isolation.\n\nNeutrino oscillations have been documented across various experimental frameworks; however, there remains a lack of definitive evidence supporting the existence of new physics beyond the Standard Model (SM), such as sterile neutrinos, lepton number violation, extra dimensions, and supersymmetry. Numerous extensions of the SM propose additional contributions to the effective four-fermion interaction Lagrangian, which could lead to observable deviations from SM predictions. Recent theoretical developments suggest that certain quantum gravitational models may introduce an energy-dependent refractive index, potentially modifying the vacuum mixing ratio \\( \\sin^2 \\theta_{12} \\). This modification could have significant implications for neutrino propagation. Furthermore, various experimental anomalies, including deficits in atmospheric and solar neutrino fluxes, as well as unexpected results from LSND and MiniBooNE experiments, hint at the possibility of new physics beyond the SM. These anomalies underscore the importance of exploring nonstandard interactions in the context of neutrino physics.",
        "ori-fast-z-score": -0.6910947404650881,
        "water-fast-z-score": 6.516946235415335,
        "rewrite-fast-z-score": -0.1796053020267749
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Shock and Release Temperatures in Molybdenum .\nAbstract:\nThe shock temperatures for molybdenum were determined by measuring the electrical resistance of samples shocked to pressures up to 1,000 kilobars (1 Mbar). The release temperature was measured using an optical pyrometer on samples that had been heated with laser light after being shocked at various levels of pressure. \n \n Shocks produced by a pulsed power machine were used to compress the sample material between two electrodes. A voltage pulse applied across these electrodes caused current flow through the compressed material which generated Joule heating. This heat increased the resistivity of the material causing it to expand rapidly as its temperature rose above the Curie point. As this expansion occurred, the resistance dropped dramatically resulting in a sharp increase in current flowing into the sample. When the current reached a critical value, the sample exploded releasing most of its stored energy. The explosion also destroyed one or both of the electrodes so that no further measurement could be made until new ones were installed.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Shock and Release Temperatures in Molybdenum . Abstract : The blast conditions for molybdenum were determined by monitoring the electrical resistance of samples shocked to pressures up to 1 , 000 kilobars ( 1 Mbar ) .The release temperature was measured using an optical pyrometer on samples that had been heated with laser light after being shocked at several rates of pressure . Shocks created by a pulsed power machine were used to compress the sample material between two electrodes .A voltage beam applied across these electrodes induced charge flow through the compressed material which generated Joule warming . This warmth increased the resistivity of the material creating it to expand rapidly as its temperature climbed above the Curie point .As this growth resulted , the resistance dropped dramatically resulting in a sharp increase in current flowing into the sample . When the current reached a critical level , the sample erupted releasing most of its stored power .The explosion also damaged one or both of the electrodes so that no further measurement could be made until fresh ones were replaced .",
        "rewrite_text": "**Title: Shock and Release Temperatures in Molybdenum**\n\n**Abstract:** This study investigates the blast conditions of molybdenum by analyzing the electrical resistance of samples subjected to shock pressures reaching up to 1,000 kilobars (1 Mbar). The release temperature was determined using an optical pyrometer on samples that were heated with laser light following shock application at various pressure rates. The shock waves were generated using a pulsed power machine, which compressed the molybdenum samples positioned between two electrodes. A voltage beam applied across these electrodes facilitated charge flow through the compressed material, resulting in Joule heating. This heating effect led to an increase in the resistivity of the material, causing it to expand rapidly as the temperature exceeded the Curie point. As the material expanded, a significant drop in resistance was observed, which in turn caused a sharp increase in the current flowing through the sample. Upon reaching a critical current threshold, the sample underwent an explosive release of the stored energy, resulting in a violent eruption. This explosion often caused damage to one or both electrodes, necessitating their replacement before further measurements could be conducted. The findings provide valuable insights into the behavior of molybdenum under extreme conditions, contributing to the understanding of its material properties and potential applications in high-pressure environments. This research not only enhances the fundamental knowledge of shock phenomena in metals but also has implications for the design and safety of materials used in high-energy applications.",
        "ori-fast-z-score": 0.5423261445466404,
        "water-fast-z-score": 5.1759731137650435,
        "rewrite-fast-z-score": 1.6164477182409742
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Pion-mass dependence of three-nucleon observables .\nAbstract:\nWe present results for the pion mass dependence of several observables in nuclear matter, obtained with chiral effective field theory at next-to-leading order (NLO). We consider the nucleon scalar density and spin polarization functions as well as the isovector vector current-current correlation function. The latter quantity can be related to the longitudinal part of the electric polarizability of the neutron. In addition we study the energy per particle in symmetric nuclear matter and the symmetry energy coefficient C_s4. Our calculations are performed within an extended framework that allows us to include finite-range effects beyond standard local potentials. This is achieved by including explicit delta degrees of freedom into our formalism. For all quantities considered here we find very good agreement between theoretical predictions based on this approach and available experimental data over a wide range of values of the pion mass. Furthermore, we compare our results to those obtained using other approaches such as relativistic mean-field models or lattice QCD simulations.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Pion - mass dependence of three - nucleon observables . Abstract : We report findings for the pion mass dependence of several observables in nuclear material , obtained with chiral effective field principles at next - to - leading order ( NLO ) .We consider the nucleon scalar density and spin polarization functions as well as the isovector matrix current - current correlation function . The latter quantity can be connected to the longitudinal portion of the electric polarizability of the neutron .In addition we study the power per particle in symmetric nuclear material and the symmetry power coefficient C _ s4 . Our calculations are performed within an extended context that enables us to use finite - range effects beyond standard local potentials .This is achieved by including explicit delta degrees of liberty into our formalism . For all amounts considered here we find very high agreement between theoretical estimates based on this methodology and accessible empirical data over a broad variety of values of the pion mass .Furthermore , we compare our findings to those acquired using other methods such as relativistic mean - field solutions or lattice QCD simulations .",
        "rewrite_text": "In this study, we investigate the dependence of various nuclear observables on pion mass, utilizing chiral effective field theory at next-to-leading order (NLO). Our analysis focuses on several key quantities, including the nucleon scalar density, spin polarization functions, and the isovector matrix current-current correlation function. Notably, the correlation function is linked to the longitudinal component of the neutron's electric polarizability. Additionally, we examine the power per particle in symmetric nuclear matter and the symmetry power coefficient \\( C_{s4} \\). \n\nOur approach is grounded in an extended framework that incorporates finite-range effects, moving beyond the limitations of standard local potentials. This is accomplished by integrating explicit delta degrees of freedom into our theoretical model. Throughout our calculations, we observe a remarkable consistency between our theoretical predictions and available empirical data across a wide range of pion mass values. \n\nMoreover, we conduct a comparative analysis of our results with those obtained through alternative methodologies, including relativistic mean-field approaches and lattice QCD simulations. The findings underscore the robustness of our chiral effective field theory framework in capturing the intricacies of nuclear observables as influenced by pion mass variations. This work not only enhances our understanding of nuclear interactions but also provides valuable insights for future research in the field of nuclear physics.",
        "ori-fast-z-score": -0.3144854510165755,
        "water-fast-z-score": 5.910624659963424,
        "rewrite-fast-z-score": 0.8542421961772492
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Zero-temperature phase of the XY spin glass in two dimensions: Genetic embedded matching heuristic .\nAbstract:\nWe study the zero temperature (ground state) properties of the two-dimensional spin-glass model with nearest-neighbor interactions using an evolutionary algorithm, called genetic embedded matching heuristic (GEMH). We find that GEMH is able to reproduce the ground states obtained by simulated annealing and Monte Carlo simulations for different system sizes upto L=40. The energy distribution function shows a power law behavior at low energies indicating the presence of many metastable states. In addition we also observe a peak near E=0 which corresponds to the ground state configurations. Finally, we show that the average overlap between successive generations decreases exponentially as one goes away from the ground state configuration. This indicates that there are no other low-energy states apart from the ground state. \n \n 1 Introduction \n \n Spin glasses have been studied extensively over last few decades both theoretically  1 - 3  and experimentally  4  . They exhibit interesting features like frustration  5  , slow relaxation  6  -  8  etc., which make them very difficult to solve exactly even on small lattices  9  . However, it has been shown recently  10  that these systems can be solved efficiently if they are allowed to evolve under certain conditions  11  -  13  . Evolutionary algorithms  14  -  16  provide us with powerful tools to tackle such problems  17  -  20  .\n \nIn this work we consider the following Hamiltonian  21  :",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Zero - temperature phase of the XY spin mirror in two dimensions : Genetic embedded matching heuristic . Abstract : We research the zero temperature ( ground state ) characteristics of the two - dimensional spin - glass model with nearest - neighbor interactions using an evolutionary algorithm , called genetic embedded matching heuristic ( GEMH ) .We see that GEMH is could to predict the ground states achieved by simulated annealing and Monte Carlo simulations for different system sizes upto L = 40 . The energy flow function shows a power law behavior at low energies indicating the presence of several metastable states .In addition we also observe a peak near E = 0 which corresponds to the ground state systems . Finally , we find that the average overlap between successive generations decreases exponentially as one goes away from the ground state arrangement .This implies that there are no other low - energy states aside from the ground state . 1 Introduction Spin windows have been studied thoroughly over recent few years both theoretically 1 - 3 and experimentally 4 .They display unusual characteristics like frustration 5 , slow relaxation 6 - 8 etc . , which make them very difficult to solve exactly especially on small lattices 9 . However , it has been shown recently 10 that these systems can be understood efficiently if they are allowed to evolve under certain conditions 11 - 13 .Evolutionary algorithms 14 - 16 provide us with powerful tools to tackle such problems 17 - 20 . In this research we consider the following Hamiltonian 21 :",
        "rewrite_text": "**Title:** Zero-Temperature Phase of the XY Spin Mirror in Two Dimensions: Genetic Embedded Matching Heuristic\n\n**Abstract:** This study investigates the zero-temperature (ground state) properties of a two-dimensional spin-glass model characterized by nearest-neighbor interactions, utilizing an innovative evolutionary algorithm known as the Genetic Embedded Matching Heuristic (GEMH). Our findings indicate that GEMH effectively predicts the ground states identified through simulated annealing and Monte Carlo simulations across various system sizes, reaching up to L = 40. Notably, the energy flow function exhibits a power-law behavior at low energy levels, suggesting the existence of multiple metastable states within the system. Furthermore, we observe a pronounced peak near E = 0, which aligns with the ground state configurations. An important aspect of our results is the exponential decrease in the average overlap between successive generations as the system diverges from the ground state arrangement. This observation implies a scarcity of low-energy states beyond the ground state itself. \n\nIn the introduction, we highlight the extensive research conducted on spin glasses in recent years, both from theoretical and experimental perspectives. These systems are known for their complex behaviors, including frustration and slow relaxation, which pose significant challenges for exact solutions, particularly in smaller lattices. Recent advancements suggest that these challenges can be addressed effectively by allowing the systems to evolve under specific conditions. Evolutionary algorithms have emerged as powerful methodologies for tackling such intricate problems. In this paper, we delve into the Hamiltonian governing the system, laying the groundwork for our analysis and findings.",
        "ori-fast-z-score": 0.9128709291752769,
        "water-fast-z-score": 5.408521132466447,
        "rewrite-fast-z-score": 0.9205746178983234
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Floating Phase in 2D ANNNI Model .\nAbstract:\nWe study the floating phase in the two-dimensional anisotropic nearest-neighbor Ising model (ANNNI). We find that there is no floating phase for J1 = J2, but it appears when J1 > J2 and disappears at some critical value of J1/J2. The transition between the ordered state and the floating phase belongs to the universality class of the three-state Potts model with first-order transition. In addition we show that the ground states are degenerate on the square lattice if J1 = J2 or J1 < J2. This result suggests that the ground states may be non-degenerate even though they have not been found yet. \n \n Introduction \n \n It has been known since the work by Wannier  1  that the ground states of the spin-1/2 Heisenberg antiferromagnet on an infinite square lattice are infinitely degenerate. However, this fact does not necessarily mean that all possible configurations can appear as ground states  2  . For example, the ground states of the one-dimensional chain are unique although its energy spectrum is continuous  3  , while those of the two-dimensional triangular-lattice Heisenberg antiferromagnet are doubly degenerate  4  . \n \n Recently, several authors studied the ground states of the two-dimensional anisotropic nearest neighbor Ising model (AN-NNI)  5 - 7  . They showed numerically that the ground states are infinitely degenerate on the square lattices if J 1 = J 2 or J 1 < J 2  7   . On the other hand, the ground states were shown to be unique on the honeycomb lattice  8  . These results suggest that the ground states might be nondegenerate even though their exact forms remain unknown so far. \n \n In this Letter, we investigate the ground states of the ANNNI model using Monte Carlo simulations. First, we confirm that the ground states are indeed infinitely degenerate on the squarelattice ANNNI models. Then, we examine whether these ground states are unique or not. Finally, we discuss how the ground states change depending on the values of J 1 / J 2 .\n \n Ground States of the Square-Lattice",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Floating Phase in 2D ANNNI Model . Abstract : We explore the floating stage in the two - dimensional anisotropic closest - neighbor Ising model ( ANNNI ) .We see that there is no floating transition for J1 = J2 , but it appears when J1 > J2 and vanished at some critical value of J1 / J2 . The switch between the ordered state and the floating stage belongs to the universality category of the three - state Potts model with first - order transition .In addition we show that the ground states are degenerate on the square lattice if J1 = J2 or J1 < J2 . This result suggests that the ground states may be non - degenerate even though they have not been determined yet .Introduction It has been known since the work by Wannier 1 that the ground states of the spin - 1 / 2 Heisenberg antiferromagnet on an endless square lattice are infinitely degenerate . However , this fact does not necessarily mean that all possible configurations can emerge as ground states 2 .For instance , the ground states of the one - dimensional chain are distinct although its energy spectrum is continuous 3 , while those of the two - dimensional triangular - lattice Heisenberg antiferromagnet are doubly degenerate 4 . Recently , various papers studied the ground states of the two - dimensional anisotropic closest neighbor Ising model ( AN - NNI ) 5 - 7 .They showed numerically that the ground states are infinitely degenerate on the square lattices if J 1 = J 2 or J 1 < J 2 7 . On the other hand , the ground states were shown to be unique on the honeycomb lattice 8 .These data suggest that the ground states could be nondegenerate even though their exact forms remain uncertain so far . In this Letter , we investigate the ground states of the ANNNI theory using Monte Carlo simulations .First , we prove that the ground states are indeed infinitely degenerate on the squarelattice ANNNI models . Then , we investigate whether these ground states are distinct or not .Finally , we talk how the ground states change based on the values of J 1 / J 2 . Ground States of the Square - Lattice",
        "rewrite_text": "**Title:** Floating Phase in the 2D ANNNI Model\n\n**Abstract:** In this study, we investigate the floating phase within the two-dimensional anisotropic nearest-neighbor Ising model (ANNNI). Our findings indicate that a floating transition does not occur when the coupling constants are equal (J1 = J2). However, this transition emerges when J1 exceeds J2 and disappears beyond a certain critical ratio of J1/J2. The transition between the ordered phase and the floating state is classified under the universality class of the three-state Potts model, characterized by a first-order transition. Furthermore, we demonstrate that the ground states on a square lattice are degenerate when J1 equals J2 or when J1 is less than J2. This observation raises the possibility that the ground states could be non-degenerate, although their precise configurations remain undetermined.\n\nThe concept of ground state degeneracy has been well-established, particularly in the context of the spin-1/2 Heisenberg antiferromagnet on an infinite square lattice, where it is known to exhibit infinite degeneracy. However, this does not imply that all configurations can serve as ground states. For example, in one-dimensional chains, the ground states are distinct despite a continuous energy spectrum, while the two-dimensional triangular-lattice Heisenberg antiferromagnet displays a doubly degenerate ground state. Recent studies have focused on the ground states of the two-dimensional ANNNI model, revealing that they are infinitely degenerate on square lattices when J1 equals J2 or when J1 is less than J2. In contrast, unique ground states have been identified on the honeycomb lattice.\n\nIn this letter, we employ Monte Carlo simulations to further explore the ground states of the ANNNI model. We first confirm the infinite degeneracy of ground states on square lattice ANNNI models. Subsequently, we analyze the distinctiveness of these ground states and examine how their characteristics evolve with varying values of J1/J2. This investigation contributes to a deeper understanding of the ground state behavior in the ANNNI framework, particularly in relation to the floating phase phenomenon.",
        "ori-fast-z-score": -3.433758534669933,
        "water-fast-z-score": 1.9295276424754644,
        "rewrite-fast-z-score": -2.6106709553062086
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The potential of mass rearing of Monoksa dorsiplana (Pteromalidae) a native gregarious ectoparasitoid of Pseudopachymeria spinipes (Bruchidae)in South America .\nAbstract:\nThe objective was to evaluate the potential use of Monoksa dorsiplana as an alternative biological control agent against Pseudopachymeria sp. (Bruchidae). The parasitoids were obtained in laboratory and released on P.sp. eggs laid by females collected at different locations in Brazil, Argentina and Paraguay. Egg parasitism ranged between 0.5 and 88% depending on location. Parasitized eggs hatched after 7 days under controlled conditions. Males emerged first followed by females. Female longevity varied according to temperature ranging from 11 to 21 days at 25 °C; 14 to 23 days at 20 °C and 16 to 27 days at 15 °C. Females oviposited for up to three weeks when fed with honey solution. This species is considered highly suitable for mass production because it has high reproductive capacity and short life cycle. It can be used successfully in integrated pest management programs aimed at reducing damage caused by this insect pest.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The possibility of mass rearing of Monoksa dorsiplana ( Pteromalidae ) a native gregarious ectoparasitoid of Pseudopachymeria spinipes ( Bruchidae ) in South America . Abstract : The goal was to analyze the possibilities application of Monoksa dorsiplana as an alternative biological management weapon against Pseudopachymeria sp .( Bruchidae ) . The parasitoids were obtained in laboratory and published on P . sp .eggs laid by females collected at different places in Brazil , Argentina and Paraguay . Egg parasitism ranged between 0 . 5 and 88 % depending on location .Parasitized nests hatched after 7 days under regulated conditions . Males appeared first followed by females .Female longevity varied based to heat ranging from 11 to 21 nights at 25 °C ; 14 to 23 days at 20 °C and 16 to 27 days at 15 °C . Females oviposited for up to three weeks when fed with honey solution .This species is regarded highly suited for mass production because it has large sexual capacity and low life cycle . It can be used successfully in effective pest management projects designed at decreasing damage suffered by this insect pest .",
        "rewrite_text": "**Title:** The Potential for Mass Rearing of Monoksa dorsiplana (Pteromalidae), a Native Gregarious Ectoparasitoid of Pseudopachymeria spinipes (Bruchidae) in South America\n\n**Abstract:** This study investigates the feasibility of utilizing Monoksa dorsiplana as a biological control agent against the pest species Pseudopachymeria spinipes (Bruchidae) in South America. The research involved the collection of parasitoids from various locations in Brazil, Argentina, and Paraguay, where they were observed parasitizing the eggs of P. spinipes. The observed rates of egg parasitism varied significantly, ranging from 0.5% to 88%, depending on the specific collection site. Under controlled laboratory conditions, the development of parasitized nests was monitored, with hatching occurring approximately seven days post-parasitism. Notably, male parasitoids emerged before females. The longevity of female M. dorsiplana was influenced by temperature, with lifespans recorded between 11 to 21 nights at 25 °C, 14 to 23 days at 20 °C, and 16 to 27 days at 15 °C. When provided with a honey solution, females were capable of ovipositing for up to three weeks, indicating a robust reproductive capacity. Given its high fecundity and relatively short life cycle, M. dorsiplana is identified as a promising candidate for mass rearing. The findings suggest that this species could play a significant role in integrated pest management strategies aimed at mitigating the impact of Pseudopachymeria spinipes, thereby reducing the damage caused by this agricultural pest. The potential for mass production of M. dorsiplana opens new avenues for sustainable pest control practices in South American ecosystems.",
        "ori-fast-z-score": -0.9299811099505543,
        "water-fast-z-score": 5.735393346764043,
        "rewrite-fast-z-score": 0.20628424925175867
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Impact of Cosmic Rays on Population III Star Formation .\nAbstract:\nWe study the impact of cosmic rays (CRs) on population iii star formation in minihalos, using cosmological hydrodynamic simulations with CR physics implemented self-consistently. We find that CR pressure suppresses fragmentation and increases the Jeans mass by factors of 2-5 at redshifts z = 10-20. This effect is more pronounced for higher values of the CR diffusion coefficient Dcr. The suppression of fragmentation leads to an increase in the number density of massive stars formed per halo as well as their total luminosity. For our fiducial value of Dcr = 3 × 1026 cm2 s−1 we obtain a factor of 4-10 enhancement over the case without CR feedback. Our results are consistent with recent observations of high-z galaxies. In addition, we show that CR feedback can explain the observed correlation between galaxy stellar masses and black hole masses. Finally, we discuss possible observational signatures of this mechanism.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Impact of Cosmic Rays on Population III Star Formation . Abstract : We research the impact of cosmic rays ( CRs ) on population iii star formation in minihalos , using cosmological hydrodynamic simulations with CR physics integrated self - regularly .We see that CR pressure suppresses fragmentation and raises the Jeans mass by factors of 2 - 5 at redshifts z = 10 - 20 . This phenomenon is more pronounced for larger values of the CR absorption constant Dcr .The suppression of fragmentation gives to an increase in the number density of large stellar generated per halo as also as their total luminosity . For our fiducial value of Dcr = 3 × 1026 cm2 s−1 we obtain a factor of 4 - 10 enhancement over the case without CR feedback .Our results are compatible with recent observations of high - z galaxies . In addition , we find that CR feedback can describe the seen correlation between galaxy stellar masses and dark hole masses .Finally , we explain possible observational signatures of this mechanism .",
        "rewrite_text": "In this study, we investigate the influence of cosmic rays (CRs) on the formation of Population III stars within minihalos, employing advanced cosmological hydrodynamic simulations that incorporate CR physics in a self-consistent manner. Our findings reveal that CR pressure plays a significant role in suppressing fragmentation during the star formation process, resulting in an increase in the Jeans mass by factors ranging from 2 to 5 at redshifts between z = 10 and 20. This effect is particularly pronounced when larger values of the CR absorption constant, denoted as Dcr, are utilized. The suppression of fragmentation leads to a higher number density of massive stars formed per halo, as well as an increase in their total luminosity. Specifically, for our reference value of Dcr = 3 × 10^26 cm² s⁻¹, we observe an enhancement factor of 4 to 10 compared to scenarios without CR feedback. These results align well with recent observations of high-redshift galaxies, suggesting that CRs may play a crucial role in the early universe's star formation dynamics. Furthermore, our analysis indicates that CR feedback could elucidate the observed correlation between the stellar masses of galaxies and the masses of their associated supermassive black holes. Lastly, we discuss potential observational signatures that could arise from this mechanism, paving the way for future research to explore the implications of CRs in cosmic evolution and structure formation.",
        "ori-fast-z-score": 0.1203858530857692,
        "water-fast-z-score": 4.865804798594798,
        "rewrite-fast-z-score": 1.0945409092309881
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Power control and receiver design for energy efficiency in multipath CDMA channels with bandlimited waveforms .\nAbstract:\nIn this work, we consider the problem of power control and receiver design to maximize the sum rate over multiple access (MAC) channels using code division multiple access (CDMA). We assume that each user transmits its signal through an independent fading channel which is subject to both path loss and shadowing effects. The received signals are corrupted by additive white Gaussian noise (AWGN), inter-user interference due to imperfect orthogonality among users  spreading codes, and intra-cell interference caused by other active users within the same cell. In addition, all users employ bandlimited waveforms such as pulse-amplitude modulation (PAM) or quadrature amplitude modulation (QAM).\nWe first derive closed-form expressions for the ergodic capacity region under different assumptions on the knowledge available at the transmitter side about the instantaneous channel state information (CSI). Then, based on these results, we propose two distributed algorithms to achieve the optimal operating point on the boundary of the ergodic capacity region. Finally, numerical examples are provided to demonstrate the performance improvement achieved by our proposed schemes compared to conventional ones.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Power control and receiver design for power performance in multipath CDMA channels with bandlimited waveforms . Abstract : In this study , we investigate the issue of power control and receiver design to maximize the sum frequency over multiple entry ( MAC ) networks using code division single access ( CDMA ) .We assume that each consumer transmits its signal through an independent fading signal which is subject to both edge loss and shadowing effects . The received messages are corrupted by additive white Gaussian sound ( AWGN ) , inter - customer interference owing to imperfect orthogonality among consumers spreading codes , and intra - cell interference caused by other active consumers within the same cell .In addition , all users utilize bandlimited waveforms such as pulse - frequency modulation ( PAM ) or quadrature amplitude modulation ( QAM ) . We first derive closed - form expressions for the ergodic capacity area under various assumptions on the knowledge accessible at the receiver side about the instantaneous channel state information ( CSI ) .Then , based on these results , we propose two distributed methods to achieve the ideal operating position on the boundary of the ergodic capacity area . Finally , numerical examples are provided to indicate the performance improvement achieved by our proposed methods compared to conventional ones .",
        "rewrite_text": "In this research article, we explore the critical aspects of power control and receiver design aimed at optimizing power performance in multipath Code Division Multiple Access (CDMA) channels that utilize bandlimited waveforms. Our focus is on maximizing the sum frequency in multiple access networks, where each user transmits signals through independent fading channels affected by edge loss and shadowing. The received signals are subject to degradation from additive white Gaussian noise (AWGN), inter-user interference due to the imperfect orthogonality of spreading codes, and intra-cell interference from other active users within the same cell. We consider users employing bandlimited waveforms, including pulse amplitude modulation (PAM) and quadrature amplitude modulation (QAM).\n\nTo begin, we derive closed-form expressions for the ergodic capacity region under various scenarios regarding the availability of instantaneous channel state information (CSI) at the receiver. These expressions provide a foundational understanding of the capacity limits in the presence of the aforementioned interferences. Building on these theoretical insights, we propose two distributed algorithms designed to help users operate at the optimal point along the boundary of the ergodic capacity region. \n\nThrough numerical simulations, we demonstrate the performance enhancements achieved by our proposed methods in comparison to traditional approaches. The results indicate significant improvements in power performance, underscoring the effectiveness of our strategies in managing power control and receiver design in complex multipath environments. This study contributes to the ongoing discourse on optimizing communication systems in challenging conditions, offering practical solutions for enhancing user experience in CDMA networks.",
        "ori-fast-z-score": 0.5883484054145521,
        "water-fast-z-score": 6.538530488755173,
        "rewrite-fast-z-score": 1.3310347641241707
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Distance to Orion KL Measured with VERA .\nAbstract:\nWe report the distance measurement toward the Galactic center using Very Long Baseline Array (VLBA) observations at 22 GHz and 43 GHz in combination with Japanese VLBI Exploration of Radio Astrometry (VERA). The parallax was measured by observing Sgr A*, which is located near the Galactic center, for two years between 2007 and 2009. We found that the distance to the Galactic center is R0 = 8 kpc ± 0.4 kpc. This value agrees well with previous measurements based on other methods such as infrared photometry or trigonometric parallaxes of masers associated with massive young stars. Our result also supports the hypothesis that the Milky Way has an axisymmetric mass distribution around its central black hole. \n \n Keywords: Distance scale, Galaxy, Parallax, Space astrometry, Black holes \n \n \n \n 1 Introduction \n \n In order to understand how galaxies evolve over time, it is important to know their distances accurately. However, accurate distances are difficult to measure because they depend strongly on the assumed luminosity evolution model. For example, if we assume too high a rate of luminosity evolution, then the derived distance will be underestimated. On the other hand, if we assume too low a rate of luminosity evolu-tion, then the derived distance may be overestimated. Therefore, it is necessary to determine the correct luminosity evolution model before deriving the distance to any galaxy. \n \n One way to solve this problem is to use radio sources whose distances can be determined independently through other means. These include pulsars, quasars, and maser sources associated with star-forming regions. Among these objects, maser sources have been used most frequently since they provide very precise distance estimates. Maser sources are usually associated with star forming regions where water vapor molecules form into microscopic crystals known as ice grains. When the ice grains grow larger than about one micron, they become unstable against gravitational collapse and begin emitting intense radiation. Since the emission line widths of maser sources are extremely narrow compared to those of normal radio",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Distance to Orion KL Measured with VERA . Abstract : We report the distance measurement toward the Galactic center utilizing Very Long Baseline Array ( VLBA ) observations at 22 GHz and 43 GHz in combination with Japanese VLBI Exploration of Radio Astrometry ( VERA ) .The parallax was measured by observing Sgr A * , which is situated near the Galactic center , for two years between 2007 and 2009 . We determined that the distance to the Galactic center is R0 = 8 kpc ± 0 . 4 kpc .This value agrees well with previous measurements based on other methods such as infrared photometry or trigonometric parallaxes of masers associated with massive young galaxies . Our result even suggests the notion that the Milky Way has an axisymmetric mass distribution around its central black hole .Keywords : Distance scale , Galaxy , Parallax , Space astrometry , Black holes 1 Introduction In order to comprehend how galaxies evolve over time , it is important to predict their distances accurately . However , accurate distances are hard to measure because they rely heavily on the expected luminosity evolution theory .For instance , if we suppose too high a rate of luminosity progression , then the derived length will be underestimated . On the other hand , if we suppose too low a rate of luminosity evolu - tion , then the derived distance might be overestimated .Therefore , it is required to obtain the appropriate luminosity evolution theory before deriving the distance to any galaxy . One method to solve this question is to use radio sources whose distances can be determined independently through other methods .These include pulsars , quasars , and maser sources associated with star - creating areas . Among these objects , maser sources have been used most regularly since they give very exact distance estimates .Maser sources are typically associated with star producing regions where water vapor molecules form into microscopic particles termed as ice particles . When the ice particles develop larger than about one micron , they become unstable against gravitational failure and begin emitting intense rays .Since the emission line widths of maser sources are extremely narrow compared to those of normal radio",
        "rewrite_text": "**Title: Distance to Orion KL Measured with VERA**\n\n**Abstract:** In this study, we present a precise measurement of the distance to the Galactic center, utilizing observations from the Very Long Baseline Array (VLBA) at frequencies of 22 GHz and 43 GHz, in conjunction with the Japanese VLBI Exploration of Radio Astrometry (VERA). Our parallax measurements were conducted over a two-year period from 2007 to 2009, focusing on Sgr A*, the supermassive black hole located near the Galactic center. The results indicate that the distance to the Galactic center is R0 = 8 kpc ± 0.4 kpc. This finding is consistent with previous distance estimates derived from various methods, including infrared photometry and trigonometric parallaxes of masers associated with massive young stellar objects. Notably, our results support the hypothesis that the Milky Way exhibits an axisymmetric mass distribution surrounding its central black hole. \n\nAccurate distance measurements are crucial for understanding the evolutionary processes of galaxies; however, they pose significant challenges due to their dependence on luminosity evolution theories. Overestimating or underestimating the rate of luminosity progression can lead to substantial errors in distance calculations. To address this issue, we propose utilizing radio sources with independently determinable distances, such as pulsars, quasars, and maser sources linked to star-forming regions. Among these, maser sources are particularly valuable due to their ability to provide highly precise distance estimates. Maser emissions arise from water vapor molecules in star-forming areas, which condense into ice particles. Once these particles exceed a critical size, they become gravitationally unstable, resulting in the emission of strong radiation. The narrow emission line widths of maser sources, in contrast to those of typical radio sources, further enhance their utility in astrometric measurements. \n\n**Keywords:** Distance scale, Galaxy, Parallax, Space astrometry, Black holes.",
        "ori-fast-z-score": 1.2456821978060995,
        "water-fast-z-score": 7.291164092619461,
        "rewrite-fast-z-score": 1.0524696231684352
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  On the Equilibrium Fluctuations of an Isolated System .\nAbstract:\nThe equilibrium fluctuations in isolated systems are studied by using statistical mechanics and information theory. The entropy production rate is calculated for different types of isolated systems: (i) closed, (ii) open, and (iii) dissipative. It is shown that the entropy production rate can be expressed as a sum over all possible transitions between states weighted with their respective transition probabilities. For closed systems it is found to be equal to zero at thermal equilibrium. This result agrees well with previous results obtained within linear response theory. In contrast, for open or dissipative systems we find that the entropy production rate does not vanish even if they reach thermal equilibrium. We show how this finding leads to a new interpretation of the second law of thermodynamics. Finally, we discuss our findings in terms of fluctuation theorems. PACS: 05.45.Mt, 02.10.Yn, 03.65.Ta Keywords: Entropy Production Rate, Second Law of Thermodynamics",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : On the Equilibrium Fluctuations of an Isolated System . Abstract : The equilibrium fluctuations in isolated systems are studied by using statistical mechanics and information theory .The entropy production rate is calculated for different kinds of isolated systems : ( i ) closed , ( ii ) open , and ( iii ) dissipative . It is demonstrated that the entropy production frequency can be written as a sum over all possible transitions between states weighted with their respective change probabilities .For closed systems it is found to be equal to zero at heat equilibrium . This result agrees well with previous findings obtained within linear behavior physics .In comparison , for open or dissipative models we find that the entropy production level does not vanish even if they reach thermal equilibrium . We see how this finding brings to a new understanding of the second law of thermodynamics .Finally , we explain our findings in terms of fluctuation theorems . PACS : 05 . 45 . Mt , 02 . 10 . Yn , 03 . 65 . Ta Keywords : Entropy Production Rate , Second Law of Thermodynamics",
        "rewrite_text": "Title: On the Equilibrium Fluctuations of an Isolated System\n\nAbstract: This article investigates the equilibrium fluctuations present in isolated systems through the lenses of statistical mechanics and information theory. We focus on calculating the entropy production rate across various types of isolated systems, specifically categorizing them into three groups: (i) closed systems, (ii) open systems, and (iii) dissipative systems. Our analysis reveals that the frequency of entropy production can be expressed as a summation over all possible state transitions, each weighted by their corresponding probabilities of change. Notably, for closed systems, we observe that the entropy production rate equals zero when the system is at thermal equilibrium. This finding aligns with previous research conducted within the framework of linear behavior physics. In contrast, our study of open and dissipative systems indicates that the entropy production does not diminish to zero, even when these systems achieve thermal equilibrium. This observation prompts a reevaluation of the implications of the second law of thermodynamics, suggesting a more nuanced understanding of entropy dynamics in non-closed systems. Furthermore, we contextualize our results within the framework of fluctuation theorems, providing deeper insights into the nature of equilibrium fluctuations. Our findings contribute to the ongoing discourse surrounding entropy production and thermodynamic laws, highlighting the complexities inherent in different types of isolated systems. \n\nPACS: 05.45.Mt, 02.10.Yn, 03.65.Ta  \nKeywords: Entropy Production Rate, Second Law of Thermodynamics",
        "ori-fast-z-score": -0.9428090415820635,
        "water-fast-z-score": 3.719924439802217,
        "rewrite-fast-z-score": 1.4288690166235207
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Infrared Ca II triplet as metallicity indicator .\nAbstract:\nWe present the first results on the use of infrared calcium lines to determine stellar metallicities in open clusters and field stars, based on high-resolution spectra obtained with the CRIRES spectrograph at VLT/UT1 (ESO). We find that the equivalent widths of the two strongest components of the infrared Ca II triplet are strongly correlated with  Fe/H  for both cluster members and field stars.  The calibration is valid over a wide range of temperatures, including those typical of red giants. This method can be used to obtain accurate estimates of the iron abundance even when only low resolution data are available. It also provides an alternative way to estimate distances using parallaxes measured by space missions such as Hipparcos or Gaia. Keywords: Calcium, Metallicity, Red giant branch star, Open cluster, Infrared spectrum, Iron abundance, Distance determination, Space mission, High-Resolution spectroscopy",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Infrared Ca II triplet as metallicity indicator . Abstract : We report the first findings on the using of infrared calcium lines to estimate stellar metallicities in open complexes and field stars , using on high - resolution spectra obtained with the CRIRES spectrograph at VLT / UT1 ( ESO ) .We see that the equivalent widths of the two strongest elements of the infrared Ca II triplet are strongly correlated with Fe / H for both cluster groups and field stars . The calibration is valid over a broad variety of temperatures , particularly those common of red giants .This method can be used to obtain precise estimates of the metal density even when only low resolution data are available . It additionally offers an alternative method to estimate distances using parallaxes measured by space missions such as Hipparcos or Gaia .Keywords : Calcium , Metallicity , Red giant branch star , Open cluster , Infrared spectrum , Iron concentration , Distance finding , Space mission , High - Resolution spectroscopy",
        "rewrite_text": "Title: The Infrared Ca II Triplet as a Metallicity Indicator\n\nAbstract: In this study, we present groundbreaking findings on the application of infrared calcium lines as a means to estimate stellar metallicities in both open clusters and field stars. Utilizing high-resolution spectra acquired with the CRIRES spectrograph at the VLT/UT1 (ESO), we have established a strong correlation between the equivalent widths of the two most prominent lines of the infrared Ca II triplet and the iron-to-hydrogen ratio (Fe/H) across various stellar groups. Our calibration demonstrates robustness across a wide range of temperatures, particularly those typical of red giant stars. This innovative approach not only allows for precise metallicity estimates but also proves effective even when working with low-resolution data. Furthermore, it provides an alternative technique for determining stellar distances, leveraging parallax measurements from space missions such as Hipparcos and Gaia. The implications of our findings are significant for the fields of stellar astrophysics and galactic evolution, as they enhance our understanding of the chemical composition of stars and their environments. By employing the infrared Ca II triplet, researchers can gain insights into the metallicity of stars, which is crucial for studying stellar formation and the evolution of galaxies. This work opens new avenues for future research, particularly in the context of exploring the metallicity distribution in various stellar populations. \n\nKeywords: Calcium, Metallicity, Red Giant Branch Star, Open Cluster, Infrared Spectrum, Iron Concentration, Distance Measurement, Space Mission, High-Resolution Spectroscopy.",
        "ori-fast-z-score": -1.2701705922171767,
        "water-fast-z-score": 3.9000674757995495,
        "rewrite-fast-z-score": -0.10050378152592121
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Real Time Turbulent Video Perfecting by Image Stabilization and Super-Resolution .\nAbstract:\nWe propose an image stabilizing method for video capturing in turbulent environments, which is based on the super-resolution technique. The proposed method can be used to improve the quality of captured videos with handheld cameras or other devices that are subject to motion blur caused by turbulence. We first estimate the camera motion using optical flow between consecutive frames. Then we use this information as input data into our super-resolution algorithm to generate high resolution images. Finally, these images are combined together to produce a stable output frame. Our experimental results show that the proposed method outperforms existing methods both visually and quantitatively. \n \n Keywords: Real time video processing; Turbulence; Motion blur; Optical flow; Super-resolution; Image stabilization. 1 Introduction \n \n In recent years there has been growing interest in developing techniques for real time video processing applications such as video surveillance systems  1  , traffic monitoring  2  , remote sensing  3  . However, most of these applications require capturing clear images under challenging conditions like low-light illumination  4  , fast moving objects  5  , blurry scenes  6  , etc.. Among all these challenges, one of the major problems is how to deal with the motion blur caused by turbulence  7  8  9  when capturing videos with hand-held cameras or other devices  10  . \n \n Figure 1: An example of a video sequence taken at nighttime (a) and its corresponding ground truth (b).",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Real Time Turbulent Video Perfecting by Image Stabilization and Super - Resolution . Abstract : We suggest an image stabilizing method for video capturing in volatile settings , which is based on the super - resolution technique .The proposed approach can be used to improve the performance of released movies with handheld cameras or other devices that are subject to moving blur caused by turbulence . We first estimate the film motion using optical flow between successive frames .Then we using this data as input data into our super - resolution algorithm to create high resolution photos . Finally , these images are fused together to produce a consistent output frame .Our research results show that the suggested method outperforms current methods both physically and quantitatively . Keywords : Real time television processing ; Turbulence ; Motion blur ; Optical stream ; Super - resolution ; Image stabilization .1 Introduction In recent years there has been growing interest in improving strategies for real time video editing applications such as video monitoring systems 1 , road monitoring 2 , remote sensing 3 . However , most of these uses involve capturing clear photographs under difficult environments like low - light lighting 4 , fast move images 5 , blurry scenes 6 , etc . .Among all these problems , one of the significant obstacles is how to deal with the movement blur caused by turbulence 7 8 9 when capturing clips with hand - held photographers or other devices 10 . Figure 1 : An example of a video sequence taken at nighttime ( a ) and its equivalent ground truth ( c ) .",
        "rewrite_text": "**Title:** Real-Time Turbulent Video Enhancement through Image Stabilization and Super-Resolution\n\n**Abstract:** In this study, we present a novel image stabilization technique designed for video capture in dynamic environments, leveraging super-resolution methodologies. Our approach aims to enhance the quality of footage obtained from handheld cameras and other devices that often suffer from motion blur due to turbulence. The process begins with the estimation of motion within the film using optical flow analysis between consecutive frames. This motion data serves as input for our super-resolution algorithm, which generates high-resolution images from the original low-resolution frames. Subsequently, these enhanced images are combined to produce a coherent output frame that significantly improves visual clarity. Our experimental results demonstrate that the proposed method surpasses existing techniques in both qualitative and quantitative assessments. The implications of this research are particularly relevant for real-time video processing applications, including surveillance systems, traffic monitoring, and remote sensing, where capturing clear images in challenging conditions—such as low light, rapid movement, and blurriness—remains a critical challenge. Among these issues, mitigating motion blur induced by turbulence is a prominent concern, especially when utilizing handheld devices. Our findings indicate that the integration of image stabilization with super-resolution not only addresses these challenges but also enhances overall video quality. This paper contributes to the ongoing discourse on improving real-time video editing strategies and offers a robust solution for practitioners in the field. \n\n**Keywords:** Real-time video processing; turbulence; motion blur; optical flow; super-resolution; image stabilization.",
        "ori-fast-z-score": -0.8703882797784892,
        "water-fast-z-score": 6.910947404650881,
        "rewrite-fast-z-score": -1.4896905994842622
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Zeta-Function of a p-Adic Manifold, Dwork Theory for Physicists .\nAbstract:\nThe zeta-function is the main object in number theory and algebraic geometry.  In this talk I will explain how it can be used to study quantum field theories on curved spacetimes with non-trivial topology.   The basic idea is that one considers the Feynman path integral over all fields on spacetime as an infinite dimensional functional integration which can then be regularized by replacing the space of fields by a finite dimensional vector space equipped with a suitable norm.    This leads naturally to the concept of a  quantum torus  whose zeta function encodes information about the spectrum of the corresponding QFT.   We will also discuss some recent results concerning the relation between the zeta functions of certain families of quantum tori and their associated modular forms. Quantum Field Theory (QFT) has been developed into a powerful tool for studying physical phenomena at the most fundamental level. However, many interesting questions remain open due to the difficulty of performing calculations beyond perturbation theory. One way out of these difficulties is to use ideas from mathematics such as those coming from number theory or algebraic geometry. In particular, we are interested in using the so-called  zeta-functions  of algebraic varieties to obtain new insights into QFTs. These objects encode important information about the underlying physics but they are notoriously difficult to compute explicitly. In my talk I ll give examples where explicit computations have been performed successfully and show how these techniques could lead to further progress in our understanding of QFTs.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Zeta - Function of a p - Adic Manifold , Dwork Theory for Physicists . Abstract : The zeta - function is the main object in number theory and algebraic topology .In this talk I will explain how it can be used to study quantum field theories on curved spacetimes with non - trivial geometry . The basic idea is that one takes the Feynman line integral over all fields on spacetime as an infinite dimensional functional analysis which can then be regularized by replacing the space of fields by a finite dimensional vector space equipped with a suitable norm .This leads naturally to the notion of a quantum torus whose zeta function encodes data about the spectrum of the associated QFT . We will also discuss some latest findings concerning the relation between the zeta functions of certain classes of quantum tori and their associated modular forms .Quantum Field Theory ( QFT ) has been built into a powerful tool for studying physical phenomena at the most important level . However , many interesting problems continue open thanks to the difficulty of completing calculations beyond perturbation theory .One path out of these problems is to use insights from mathematics such as those coming from number theory or algebraic topology . In particular , we are concerned in use the so - called zeta - functions of algebraic fields to obtain new understanding into QFTs .These items encode crucial data about the fundamental theory but they are notoriously difficult to compute clearly . In my talk I ll offer examples where explicit computations have been performed effectively and tell how these experiments could lead to further progress in our understanding of QFTs .",
        "rewrite_text": "**Title:** The Zeta-Function of a p-Adic Manifold: Dwork Theory for Physicists\n\n**Abstract:** The zeta-function serves as a pivotal concept in both number theory and algebraic topology. In this presentation, I will elucidate its application in the analysis of quantum field theories (QFTs) situated on curved spacetimes characterized by non-trivial geometries. The fundamental approach involves treating the Feynman line integral over all fields in spacetime as an infinite-dimensional functional analysis problem. This can be regularized by substituting the infinite-dimensional space of fields with a finite-dimensional vector space that is endowed with an appropriate norm. This process naturally leads to the concept of a quantum torus, whose zeta function encapsulates essential information regarding the spectrum of the corresponding QFT.\n\nMoreover, I will explore recent developments concerning the connections between the zeta functions of specific classes of quantum tori and their associated modular forms. Quantum Field Theory has emerged as a robust framework for investigating physical phenomena at a fundamental level; however, numerous intriguing challenges remain unresolved, primarily due to the complexities involved in performing calculations beyond perturbation theory. One promising avenue for addressing these challenges is to leverage mathematical insights, particularly those derived from number theory and algebraic topology.\n\nIn this context, we focus on utilizing the zeta-functions of algebraic fields to gain new insights into QFTs. These functions encode vital information about the underlying theoretical framework, yet they are notoriously challenging to compute accurately. During my talk, I will present examples where explicit computations have been successfully executed, demonstrating how these findings could pave the way for further advancements in our comprehension of quantum field theories.",
        "ori-fast-z-score": 0.4472135954999579,
        "water-fast-z-score": 6.350433056099402,
        "rewrite-fast-z-score": 1.310556084991557
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The shapes, orientation, and alignment of Galactic dark matter subhalos .\nAbstract:\nWe present the results of an analysis of the shapes, orientations, and alignments of dark matter subhalos in cosmological N-body simulations with different levels of baryonic physics included.  We find that the inclusion of baryons has little effect on the shape distribution but does affect the spin parameter distributions significantly; halos are more spherical when baryons are included than they would be if only gravity were acting upon them. The halo spins tend to be aligned perpendicularly to their major axes for all models considered here (including pure dark matter). This is consistent with previous studies which have found similar trends using other methods. However we also find evidence that this trend may not hold at very small scales where there appears to be some correlation between the direction of the angular momentum vector and the minor axis of the halo. Finally, we show that the presence or absence of baryons affects the degree of alignment between neighboring halos; halos are less strongly clustered around each other when baryons are included.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The shapes , orientation , and alignment of Galactic dark matter subhalos . Abstract : We present the results of an assessment of the shapes , orientations , and alignments of dark matter subhalos in cosmological N - bodies simulations with various levels of baryonic physics provided .We see that the introduction of baryons has little impact on the form distribution but does affect the spin vector distributions substantially ; halos are more spherical when baryons are included than they would be if only gravitational were acting upon them . The halo spins tend to be aligned perpendicularly to their major axes for all models discussed here ( especially pure black material ) .This is consistent with previous research which have discovered similar trends using other methods . However we also find proof that this shift might not stand at very small scales where there seems to be some correlation between the direction of the angular velocity tensor and the minor axis of the halo .Finally , we find that the presence or lack of baryons affects the degree of alignment between neighboring halos ; halos are less highly clustered around each other when baryons are included .",
        "rewrite_text": "In this study, we investigate the characteristics of dark matter subhalos, focusing on their shapes, orientations, and alignments as derived from cosmological N-body simulations that incorporate varying degrees of baryonic physics. Our findings reveal that while the inclusion of baryons has a minimal effect on the overall distribution of shapes, it significantly influences the distribution of spin vectors. Specifically, we observe that halos exhibit a more spherical configuration when baryonic effects are taken into account, compared to scenarios where only gravitational forces are considered. Furthermore, we note a consistent trend across all models examined, particularly in those dominated by dark matter, where the spins of halos tend to align perpendicularly to their major axes. This observation aligns with previous studies that have reported similar patterns using alternative methodologies. However, our analysis also uncovers evidence suggesting that this perpendicular alignment may not persist at smaller scales, where a correlation appears to exist between the direction of the angular velocity tensor and the minor axis of the halo. Additionally, we explore the impact of baryons on the spatial arrangement of neighboring halos, finding that the presence of baryonic matter reduces the degree of clustering among halos. This indicates that baryonic physics plays a crucial role in shaping not only the individual properties of dark matter subhalos but also their interactions and distributions within the cosmic structure. Overall, our results contribute to a deeper understanding of the complex interplay between dark matter and baryonic components in the formation and evolution of galactic structures.",
        "ori-fast-z-score": -1.8439088914585775,
        "water-fast-z-score": 4.610083899802517,
        "rewrite-fast-z-score": 2.1447610589527217
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Self-diffusion and Interdiffusion in Al80Ni20 Melts: Simulation and Experiment .\nAbstract:\nThe self-diffusivity (D) and interdiffusivity (D*) were measured by the tracer diffusion method for molten Al80Ni20 alloys at 1373 K, 1473 K, 1523 K, 1623 K, 1723 K, 1823 K, 1873 K, 1923 K, and 2023 K using radioactive tracers. The activation energies for D and D* are determined to be 1.27 ± 0.04 eV and 2.10 ± 0.06 eV respectively. These values agree well with those obtained previously on other binary melts.  It is found that both D and D* increase rapidly as temperature increases up to about 1600 K but then they change very slowly above this temperature. This behavior can be explained by considering the effect of atomic size difference between Ni and Al atoms on their diffusivities. In addition, it was observed that the ratio of D/D* decreases gradually with increasing temperature.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Self - diffusion and Interdiffusion in Al80Ni20 Melts : Simulation and Experiment . Abstract : The self - diffusivity ( D ) and interdiffusivity ( D * ) were calculated by the tracer diffusion method for molten Al80Ni20 alloys at 1373 K , 1473 K , 1523 K , 1623 K , 1723 K , 1823 K , 1873 K , 1923 K , and 2023 K using radioactive tracers .The activation energies for D and D * are found to be 1 . 27 ± 0 . 04 eV and 2 . 10 ± 0 . 06 eV respectively . These values comply good with those achieved prior on other binary melts .It is found that both D and D * increase quickly as temperature increases up to about 1600 K but then they change very slowly above this heat . This phenomenon can be understood by using the impact of atomic height shift between Ni and Al atoms on their diffusivities .In addition , it was seen that the proportion of D / D * varies progressively with rising heat .",
        "rewrite_text": "Title: Self-Diffusion and Interdiffusion in Al80Ni20 Melts: Simulation and Experiment\n\nAbstract: This study investigates the self-diffusivity (D) and interdiffusivity (D*) of molten Al80Ni20 alloys through tracer diffusion methods, utilizing radioactive tracers at temperatures ranging from 1373 K to 2023 K. The calculated activation energies for self-diffusion and interdiffusion were determined to be 1.27 ± 0.04 eV and 2.10 ± 0.06 eV, respectively. These findings are consistent with previous research conducted on other binary alloy melts, indicating a reliable correlation in diffusivity behavior across similar systems. Notably, both D and D* exhibit a rapid increase with temperature up to approximately 1600 K, beyond which their rates of change significantly diminish. This temperature-dependent behavior can be attributed to the atomic height shift between nickel and aluminum atoms, which influences their respective diffusivities. Furthermore, the ratio of D to D* demonstrates a progressive variation with increasing temperature, suggesting a complex interplay between self-diffusion and interdiffusion mechanisms in the alloy. This research contributes to a deeper understanding of the diffusion processes in Al80Ni20 melts, providing valuable insights for future studies on alloy behavior at elevated temperatures. The results have implications for the development of materials and processes in metallurgical applications, where precise control of diffusion characteristics is essential.",
        "ori-fast-z-score": -1.0,
        "water-fast-z-score": 3.959797974644666,
        "rewrite-fast-z-score": 0.7504787743864564
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The autoignition of cyclopentane and cyclohexane in a shock tube .\nAbstract:\nThe autoignition characteristics of two cyclic hydrocarbons, cyclopentane (CP) and cyclohexane (CH), are investigated using the rapid compression machine coupled with a shock-tube facility at temperatures ranging between 300 K and 1000 K under atmospheric pressure conditions. The ignition delay times for both fuels increase as temperature increases due to an increased rate of chemical reactions. At low temperatures below 600 K, CP has longer ignition delays than CH because it is more difficult for the fuel molecules to overcome their activation energy barrier. However, above 700 K, the opposite trend occurs where CH exhibits longer ignition delays compared to CP. This can be explained by the fact that the higher molecular weight of CH leads to slower diffusion rates which results in lower reactivity.  In addition, the effect of equivalence ratio on the ignition delay time was also studied. It was found that increasing the equivalence ratio decreases the ignition delay time for all tested temperatures except at 800 K where no significant difference could be observed.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The autoignition of cyclopentane and cyclohexane in a shock tube . Abstract : The autoignition characteristics of two cyclic hydrocarbons , cyclopentane ( CP ) and cyclohexane ( CH ) , are examined using the quick compression device linked with a shock - tube laboratory at pressures ranging between 300 K and 1000 K under atmospheric pressure circumstances .The ignition wait periods for both fuels increase as temperature increases owing to an higher speed of chemical processes . At small temperatures below 600 K , CP has longer ignition delays than CH because it is more hard for the engine molecules to overcome their activation energy barrier .However , above 700 K , the opposite decline happens where CH exhibits longer ignition delays compared to CP . This can be explained by the fact that the higher molecular weight of CH leads to slower diffusion rates which results in reduced reactivity .In addition , the impact of equivalence factor on the ignition wait period was also examined . It was shown that raising the equivalence factor decreases the ignition wait period for all tested altitudes except at 800 K where no major variation might be found .",
        "rewrite_text": "**Title:** The Autoignition of Cyclopentane and Cyclohexane in a Shock Tube\n\n**Abstract:** This study investigates the autoignition properties of two cyclic hydrocarbons, cyclopentane (CP) and cyclohexane (CH), utilizing a quick compression device integrated with a shock tube facility. The experiments were conducted at temperatures ranging from 300 K to 1000 K under atmospheric pressure conditions. The findings reveal that the ignition delay times for both fuels exhibit an increasing trend with rising temperatures, attributed to the accelerated rates of chemical reactions at elevated temperatures. At lower temperatures, specifically below 600 K, cyclopentane demonstrates longer ignition delays compared to cyclohexane. This phenomenon is primarily due to the greater difficulty for CP molecules to surmount their activation energy barrier. Conversely, at temperatures exceeding 700 K, cyclohexane shows longer ignition delays than cyclopentane. This shift can be explained by the higher molecular weight of cyclohexane, which results in slower diffusion rates and consequently diminished reactivity. Furthermore, the study also explores the influence of the equivalence ratio on ignition delay times. It was observed that increasing the equivalence ratio generally leads to a reduction in ignition delay for all tested temperatures, with the exception of the 800 K condition, where no significant changes were detected. These results provide valuable insights into the combustion characteristics of cyclic hydrocarbons, which are essential for optimizing fuel performance in various applications. The findings contribute to a deeper understanding of the ignition behavior of these fuels, which is crucial for advancements in combustion technology and engine design.",
        "ori-fast-z-score": -1.9188064472004938,
        "water-fast-z-score": 5.270462766947299,
        "rewrite-fast-z-score": -1.8411492357966468
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Molecular line intensities as measures of cloud masses - II. Conversion factors for specific galaxy types .\nAbstract:\nWe present the results of our analysis of molecular gas mass estimates based on CO and HCN observations in nearby galaxies, using data obtained with the IRAM 30m telescope. We find that conversion factors between luminosity and mass are strongly dependent on the star formation rate (SFR) per unit area within each galaxy disk. The SFR surface density is found to be an important parameter controlling the conversion factor XCO = M(H2)/L(CO), which we derive by fitting the observed L(HCN) / L(CO) ratio versus metallicity relation. For low values of ΣSFR < 1M⊙ yr-1 kpc-2 , corresponding to quiescent disks or nuclear regions dominated by old stellar populations, we obtain XCO ≈ 2 × 10 20 cm−2 K−1 km−1 s. This value increases up to XCO ≈ 5×10 20 cm−2 K−1km−1s at high ΣSFR > 3M⊙yr-1kpc-2 . These findings suggest that the physical conditions of the interstellar medium may change significantly depending on whether it is located in actively star-forming regions or not.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Molecular line intensities as indicators of cloud masses - II . Conversion factors for specific galaxy types .Abstract : We present the conclusion of our analysis of molecular gas mass estimates based on CO and HCN measurements in nearby galaxies , using data acquired with the IRAM 30m telescope . We see that conversion factors between luminosity and mass are strongly dependent on the star formation rate ( SFR ) per unit area within each galaxy disk .The SFR ground density is found to be an important function regulating the transformation parameter XCO = M ( H2 ) / L ( CO ) , which we derive by fitting the seen L ( HCN ) / L ( CO ) ratio versus metallicity relation . For low values of ΣSFR < [UNK] yr - 1 kpc - 2 , equivalent to quiescent disks or atomic regions dominated by ancient stars populations , we obtain XCO ≈ 2 × 10 20 cm−2 K−1 km−1 s . This value rises up to XCO ≈ 5×10 20 cm−2 K−1km−1s at high ΣSFR > [UNK] - 1kpc - 2 .These studies imply that the physical conditions of the interstellar medium may change considerably depending on whether it is situated in actively star - creating areas or not .",
        "rewrite_text": "In this article, we conclude our investigation into the estimation of molecular gas masses in nearby galaxies, utilizing CO and HCN measurements obtained from the IRAM 30m telescope. Our findings reveal that the conversion factors linking luminosity to mass exhibit a significant dependence on the star formation rate (SFR) per unit area within the disks of galaxies. We identify the SFR surface density as a crucial factor influencing the transformation parameter XCO, defined as the ratio of molecular hydrogen mass (M(H2)) to carbon monoxide luminosity (L(CO)). This relationship is established by analyzing the correlation between the L(HCN)/L(CO) ratio and metallicity. \n\nFor galaxies characterized by low star formation rates (ΣSFR < [UNK] yr^-1 kpc^-2), which typically correspond to quiescent disks or regions dominated by older stellar populations, we determine that XCO is approximately 2 × 10^20 cm^-2 K^-1 km^-1 s. Conversely, in environments with high star formation rates (ΣSFR > [UNK] yr^-1 kpc^-2), indicative of active star formation regions, we observe an increase in XCO to about 5 × 10^20 cm^-2 K^-1 km^-1 s. \n\nThese results suggest that the physical conditions within the interstellar medium can vary significantly based on the presence or absence of active star formation. Our study highlights the importance of considering local star formation activity when interpreting molecular line intensities as indicators of cloud masses in different types of galaxies. This work contributes to a deeper understanding of the interplay between star formation and molecular gas properties, providing valuable insights for future research in galactic astrophysics.",
        "ori-fast-z-score": -0.10721125348377948,
        "water-fast-z-score": 4.477215043467819,
        "rewrite-fast-z-score": 0.6311687442672026
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Capacity of Sparse Multipath Channels in the Ultra-Wideband Regime .\nAbstract:\nWe consider an additive white Gaussian noise (AWGN) channel with multiple paths, each having different delay and gain coefficients. We assume that these channels are sparse; i.e., there is only one path whose gain coefficient exceeds some threshold value. The capacity region for this class of channels has been studied by several authors under various assumptions on the number of paths and their gains.  In particular, it was shown recently that if all the paths have equal power then the capacity region can be achieved using superposition coding at transmitter and successive decoding at receiver. However, when the powers of the paths vary widely, the optimality of such schemes remains unknown. In this work we show that even though the optimal scheme may not use superposition coding or successive decoding, its performance cannot differ significantly from those schemes. More specifically, we prove upper bounds on the gap between the achievable rates using any scheme and the sum-capacity of the channel.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Capacity of Sparse Multipath Channels in the Ultra - Wideband Regime . Abstract : We consider an additive white Gaussian sound ( AWGN ) channel with many lanes , each having specific delay and gain values .We assume that these networks are sparse ; i . e . , there is only one path whose gain coefficient exceeds some threshold value . The capacity area for this class of channels has been studied by many writers under various observations on the number of paths and their benefits .In particular , it was shown lately that if all the paths have equal energy then the ability area can be obtained using superposition coding at transmitter and subsequent decoding at receiver . However , when the powers of the paths differ widely , the optimality of such schemes holds unknown .In this research we prove that even though the ideal scheme may not use superposition coding or previous decoding , its reliability cannot differ significantly from those schemes . More specifically , we prove upper limits on the gap between the achievable rates using any scheme and the sum - bandwidth of the channel .",
        "rewrite_text": "Title: Capacity of Sparse Multipath Channels in the Ultra-Wideband Regime\n\nAbstract: This study investigates the capacity of an additive white Gaussian noise (AWGN) channel characterized by multiple paths, each defined by distinct delay and gain parameters. We focus on sparse networks, where only one path exhibits a gain coefficient that surpasses a predetermined threshold. The capacity region for such channels has been explored extensively in the literature, with various analyses addressing the implications of the number of paths and their respective advantages. Recent findings indicate that when all paths possess equal energy, the capacity region can be effectively determined through the application of superposition coding at the transmitter, followed by decoding at the receiver. However, the scenario becomes more complex when there is significant disparity in the power levels of the paths, raising questions about the effectiveness of these coding schemes. In this paper, we demonstrate that, despite the potential inadequacy of superposition coding and traditional decoding methods in this context, the reliability of alternative schemes remains comparable. Specifically, we establish upper bounds on the gap between the achievable rates of any coding strategy and the total bandwidth of the channel. Our results contribute to a deeper understanding of capacity limits in sparse multipath environments, particularly in the ultra-wideband regime, and provide insights into the performance of various transmission strategies under conditions of power imbalance among paths. This work has implications for the design and optimization of communication systems operating in environments characterized by sparse multipath propagation.",
        "ori-fast-z-score": -0.10721125348377948,
        "water-fast-z-score": 6.038635299392551,
        "rewrite-fast-z-score": 0.7427813527082074
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  In-plane structure and ordering at liquid sodium surfaces and interfaces from ab initio molecular dynamics .\nAbstract:\nWe have performed ab initio molecular dynamics simulations to study the in-plane structure, order parameters, and surface tension of liquid Na(l) in contact with vacuum or solid NaCl (001). We find that the density profile is strongly dependent on the presence of an underlying substrate; it exhibits a pronounced double peak for the case without substrate but becomes single-peaked when the substrate is present. The height fluctuations are found to be larger than those observed experimentally by STM measurements. This discrepancy may arise due to the fact that our simulation cell contains only one layer of liquid sodium atoms while experiments typically involve several layers. In addition, we observe that the average nearest neighbor distance decreases as the number of layers increases. Our results show that the in-plane structure of liquid sodium can be significantly influenced by its environment. Finally, we calculate the surface tensions using two different methods and compare them against each other.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : In - plane structure and ordering at liquid sodium surfaces and interfaces from ab initio molecular dynamics . Abstract : We have done ab initio molecular dynamics simulations to study the in - plane structure , order variables , and surface tension of liquid Na ( l ) in contact with vacuum or solid NaCl ( 001 ) .We see that the density profile is strongly dependent on the presence of an underlying substrate ; it displays a distinct double peak for the case without substrate but grows single - topped when the substrate is present . The depth fluctuations are found to be larger than those observed experimentally by STM observations .This discrepancy may arise due to the fact that our modeling cell contains only one layer of liquid sodium atoms while tests usually require many layers . In addition , we find that the average closest neighbor distance decreases as the quantity of layers increases .Our results show that the in - plane structure of liquid sodium can be substantially affected by its climate . Finally , we estimate the surface tensions use two different methods and contrast them against each other .",
        "rewrite_text": "Title: In-Plane Structure and Ordering at Liquid Sodium Surfaces and Interfaces from Ab Initio Molecular Dynamics\n\nAbstract: This study employs ab initio molecular dynamics simulations to investigate the in-plane structural characteristics, order parameters, and surface tension of liquid sodium (Na) in contact with either vacuum or solid sodium chloride (NaCl) (001) surfaces. Our findings reveal that the density profile of liquid sodium is significantly influenced by the presence of an underlying substrate. Specifically, in the absence of a substrate, the density profile exhibits a pronounced double peak, whereas it transitions to a single peak when a substrate is present. Additionally, we observe that the depth fluctuations of the liquid sodium layer are greater than those reported in experimental scanning tunneling microscopy (STM) studies. This discrepancy may be attributed to the limitations of our modeling approach, which considers only a single layer of liquid sodium atoms, while experimental observations typically involve multiple layers. Furthermore, our analysis indicates that the average distance to the nearest neighbor decreases with an increasing number of layers, suggesting a strong dependence of the in-plane structure on the layering effect. Overall, our results demonstrate that the environmental conditions surrounding liquid sodium play a crucial role in determining its structural properties. Lastly, we estimate the surface tensions of liquid sodium using two distinct methodologies and compare the results, providing insights into the interfacial behavior of this important liquid metal. This work enhances our understanding of the fundamental properties of liquid sodium at surfaces and interfaces, which is essential for various applications in materials science and engineering.",
        "ori-fast-z-score": 0.20203050891044214,
        "water-fast-z-score": 5.656854249492381,
        "rewrite-fast-z-score": 0.8703882797784892
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Unambiguous coherent state identification: Searching a quantum database .\nAbstract:\nWe present an algorithm for searching in databases that are stored as qubits, which is the basic unit of information in quantum computers. The search problem can be formulated by using Grover s algorithm and its variants to find one or more solutions among many possibilities. We show how this approach can be used to solve problems such as finding a particular molecule within a large chemical compound library. Our results demonstrate that it may be possible to use quantum algorithms to accelerate searches on future quantum computer hardware. Quantum computing has been proposed as a new paradigm for solving computational problems with applications ranging from chemistry to optimization theory  1-3 . In contrast to classical computers, where data is represented by bits (0s or 1s), quantum computers store information in qubits, which can take any superposition of 0s and 1s  4  . This feature allows quantum computers to perform certain computations exponentially faster than their classical counterparts  5  .\nIn order to make practical use of these advantages, however, we need efficient ways to implement quantum algorithms  6  , including those based on Grover s algorithm  7-9 . Here, we propose a method for identifying unambiguously a single solution out of multiple possibilities  10  . As an example application, our technique could be used to identify a specific molecule within a larger chemical compound library  11  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Unambiguous coherent state recognition : Searching a quantum database . Abstract : We present an algorithm for searching in databases that are stored as qubits , which is the fundamental unit of information in quantum computers .The search question can be formulated by using Grover s algorithm and its versions to find one or more solutions among various possibilities . We see how this methodology can be used to solve difficulties such as finding a certain chemical within a large chemical compound collection .Our results show that it could be possible to use quantum algorithms to accelerate searches on future particle computer hardware . Quantum computing has been proposed as a new framework for solving computational problems with applications ranging from chemistry to optimization theory 1 - 3 .In contrast to conventional machines , where data is represented by bits ( 0s or 1s ) , quantum computers store information in qubits , which can take any superposition of 0s and 1s 4 . This feature allows quantum computers to conduct many computations exponentially better than their classical rivals 5 .In order to make practical use of these benefits , however , we require efficient methods to execute quantum algorithms 6 , notably those based on Grover s algorithm 7 - 9 . Here , we propose a technique for determining unambiguously a single solve out of multiple options 10 .As an instance application , our technique could be used to identify a certain compound within a greater chemical compound library 11 .",
        "rewrite_text": "We introduce a novel algorithm designed for efficient searching within quantum databases composed of qubits, the essential units of information in quantum computing. The search problem is framed using Grover's algorithm and its variants, enabling the identification of one or more solutions from a multitude of possibilities. This approach is particularly advantageous in scenarios such as locating a specific chemical compound within an extensive library of chemical substances. Our findings suggest that leveraging quantum algorithms could significantly enhance search capabilities on future quantum computing hardware. Quantum computing represents a transformative paradigm for addressing complex computational challenges, with potential applications spanning fields such as chemistry and optimization theory. Unlike traditional computers that utilize bits (0s and 1s) for data representation, quantum computers utilize qubits, which can exist in superpositions of both 0 and 1. This unique property allows quantum systems to perform certain computations exponentially faster than classical computers. However, to fully harness these advantages, it is crucial to develop efficient methods for implementing quantum algorithms, particularly those rooted in Grover's algorithm. In this work, we propose a method for unambiguously determining a single solution from multiple candidates. As a practical illustration, our technique can be applied to the identification of specific compounds within a larger chemical compound database, showcasing the potential of quantum algorithms in real-world applications.",
        "ori-fast-z-score": -0.3592106040535498,
        "water-fast-z-score": 5.63489130329947,
        "rewrite-fast-z-score": -2.4545454545454546
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Strong atom-field coupling for Bose-Einstein condensates in an optical cavity on a chip .\nAbstract:\nWe propose to realize strong atom-field coupling between a single-mode microwave field and ultracold atoms trapped inside a microchip-based Fabry-Perot resonator with high finesse mirrors. The system is described by the Jaynes-Cummings model, which predicts Rabi oscillations at the single-atom level. We show that this scheme can be implemented using current technology. In particular we discuss how to trap cold atoms close to one mirror surface and how to couple them efficiently to the microwave mode via evanescent fields. Finally, we present numerical results showing that it should be possible to reach the regime where quantum effects become important even if only a small number of atoms are loaded into the cavity. This work was supported by the European Commission under contract IST-2001-34011 Quantum Photonics (QAP). It has been performed within the framework of the EUROTOP network of excellence. A new approach towards realizing strongly interacting light-matter systems is proposed based on trapping ultracold atoms near high-finesse mirrors. By exploiting the large electric dipole moment associated with atomic transitions in the microwave domain, we demonstrate that such a setup allows us to achieve strong atom-field coupling at the single-atom limit.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Strong atom - field coupling for Bose - Einstein condensates in an optical cavity on a chip . Abstract : We suggest to realize strong atom - field coupling between a single - mode microwave field and ultracold atoms trapped inside a microchip - based Fabry - Perot resonator with high finesse mirrors .The system is characterized by the Jaynes - Cummings model , which predicts Rabi oscillations at the single - atom level . We see that this scheme can be executed using current technology .In particular we explain how to capture cold molecules close to one reflection edge and how to couple them efficiently to the microwave mode via evanescent fields . Finally , we present numerical findings indicating that it should be possible to reach the regime where quantum effects become crucial even if only a small number of atoms are transported into the cavity .This project was supported by the European Commission under contract IST - 2001 - 34011 Quantum Photonics ( QAP ) . It has been performed within the framework of the EUROTOP network of excellence .A modern alternative towards studying strongly interacting light - matter structures is proposed based on trapping ultracold atoms near high - finesse mirrors . By exploiting the huge electric dipole point involved with atomic transitions in the microwave domain , we prove that such a setup allows us to achieve powerful atom - field coupling at the single - atom limit .",
        "rewrite_text": "We propose a novel approach to achieve strong atom-field coupling between a single-mode microwave field and ultracold atoms confined within a microchip-based Fabry-Perot resonator featuring high finesse mirrors. This innovative system is described by the Jaynes-Cummings model, which predicts the occurrence of Rabi oscillations at the level of individual atoms. Our analysis indicates that this experimental setup can be realized with existing technological capabilities. We detail the methodology for capturing cold molecules near one of the reflective edges of the resonator and demonstrate how to effectively couple these molecules to the microwave mode through the use of evanescent fields. Furthermore, we present numerical simulations that suggest it is feasible to enter a regime where quantum effects become significant, even with a limited number of atoms introduced into the cavity. This research is supported by the European Commission under contract IST-2001-34011 Quantum Photonics (QAP) and has been conducted within the framework of the EUROTOP network of excellence. Our work offers a contemporary alternative for investigating strongly interacting light-matter systems by leveraging the trapping of ultracold atoms in proximity to high-finesse mirrors. By harnessing the substantial electric dipole moments associated with atomic transitions in the microwave frequency range, we demonstrate that this configuration enables the realization of robust atom-field coupling at the single-atom scale.",
        "ori-fast-z-score": 1.4855627054164149,
        "water-fast-z-score": 6.748852387406954,
        "rewrite-fast-z-score": 0.6704783996548059
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Evaporation of buffer gas-thermalized anions out of a multipole rf ion trap .\nAbstract:\nWe report on the evaporation of buffer-gas thermalized ions in a linear quadrupole ion trap (QIT). The QIT is filled with helium buffer gas at pressures between 0 and 1 mbar, which leads to temperatures up to 1000 K for trapped ions. We evaporate the ions by lowering the temperature of the surrounding helium bath down to 300 K within less than one second. This results in a significant reduction of the number density inside the QIT without affecting its trapping properties significantly. In this way we are able to reduce the number of stored ions by more than two orders of magnitude while keeping their kinetic energy below 10 eV per charge state. Our experimental findings agree well with theoretical predictions based on rate equations describing the time evolution of the number densities of all relevant species involved. \n \n Introduction \n \n Multipole radio-frequency ion traps have been used extensively over the past decades as mass spectrometers  1  . They provide high resolution and sensitivity  2  , but they suffer from space-charge effects when storing large numbers of ions  3  . Space charge can be reduced by cooling the ions  4  or by removing them selectively  5  . Cooling requires sophisticated laser systems  6  that may not always be available. Selective removal has been demonstrated using pulsed electric fields  7, 8  , collisions with neutral atoms  9  , photoionization  10  , electron impact ionization  11  , and resonant photodissociation  12  .\n \nIn our experiment, we use selective removal via rapid heating of the helium buffer gas  13  . Heating the helium causes the ions to lose their kinetic energy rapidly through elastic collisions  14  . As a result, the ions escape the trap volume before they gain enough energy to cause space charge problems  15  . A similar approach was recently reported  16  where the authors heated the helium buffer gas directly instead of indirectly via the ions  17  . \n \n Herein, we present detailed measurements of the process of evaporative cooling of buffer gas-thermalised ions in a linear quadrupolar ion trap (QIT)  18  . We show how the number density of the ions decreases exponentially after switching off the helium flow into the vacuum chamber containing the",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Evaporation of buffer gas - thermalized anions out of a multipole rf electron trap . Abstract : We report on the evaporation of buffer - gas thermalized ions in a linear quadrupole ion trap ( QIT ) .The QIT is filled with helium buffer gas at pressures between 0 and 1 mbar , which results to pressures up to 1000 K for trapped ions . We evaporate the ions by dropping the temperature of the adjacent helium bath down to 300 K within fewer than one second .This results in a substantial decreased of the number density inside the QIT without affecting its trapping characteristics significantly . In this way we are able to reduce the number of stored ions by more than two orders of magnitude while maintaining their kinetic power below 10 eV per charge state .Our experimental discoveries agree well with theoretical estimates based on rate coefficients relating the period evolution of the number densities of all relevant strains involved . Introduction Multipole radio - frequency ion traps have been used heavily over the previous decades as mass spectrometers 1 .They offer high resolution and sensitivity 2 , but they suffer from space - charge effects when storing huge groups of atoms 3 . Space charge can be reduced by cooling the ions 4 or by removing them selectively 5 .Cooling needs specialized laser mechanisms 6 that might not always be available . Selective removal has been shown involving pulsed electric forces 7 , 8 , collisions with neutral ions 9 , photoionization 10 , electron impact ionization 11 , and resonant photodissociation 12 .In our experiment , we using selective removal via rapid heating of the helium buffer gas 13 . Heating the helium creates the ions to lose their kinetic power rapidly through elastic collisions 14 .As a result , the ions flee the trap volume before they acquire enough energy to create space charge problems 15 . A related approach was recently described 16 where the authors excited the helium buffer gas directly rather of indirectly via the ions 17 .Herein , we present detailed observations of the process of evaporative cooling of buffer gas - thermalised particles in a linear quadrupolar ion trap ( QIT ) 18 . We see how the number density of the ions decreases exponentially after switching off the helium flow into the vacuum chamber containing the",
        "rewrite_text": "**Title:** Evaporation of Buffer Gas-Thermalized Anions from a Multipole RF Electron Trap\n\n**Abstract:** In this study, we investigate the evaporation of buffer-gas thermalized ions within a linear quadrupole ion trap (QIT). The QIT is maintained with helium buffer gas at pressures ranging from 0 to 1 mbar, which leads to ion temperatures reaching up to 1000 K. We achieve ion evaporation by rapidly lowering the temperature of the surrounding helium bath to 300 K in less than one second. This rapid cooling significantly reduces the ion number density within the QIT while minimally impacting its trapping efficiency. Consequently, we successfully decrease the number of stored ions by over two orders of magnitude, all while keeping their kinetic energy below 10 eV per charge state. Our experimental findings align closely with theoretical predictions derived from rate coefficients that describe the temporal evolution of the number densities of all relevant species involved in the process.\n\nThe introduction of multipole radio-frequency ion traps has revolutionized mass spectrometry, providing high resolution and sensitivity. However, these traps are often challenged by space-charge effects when accommodating large ion populations. To mitigate space charge, techniques such as ion cooling and selective removal have been employed. While cooling typically requires sophisticated laser systems, selective removal methods—including pulsed electric fields, collisions with neutral particles, and various ionization techniques—have been explored. In our approach, we utilize selective removal through the rapid heating of the helium buffer gas, which induces the ions to lose kinetic energy swiftly via elastic collisions. This mechanism allows the ions to escape the trap before accumulating sufficient energy to cause space charge issues. A related method has been previously reported, where direct excitation of the helium buffer gas was employed instead of relying on the ions. In this paper, we provide comprehensive observations of the evaporative cooling process of buffer gas-thermalized ions in a linear quadrupole ion trap, highlighting the exponential decrease in ion number density following the cessation of helium flow into the vacuum chamber.",
        "ori-fast-z-score": 1.4445432504124405,
        "water-fast-z-score": 7.884219653220595,
        "rewrite-fast-z-score": 2.5384615384615383
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Electrostatic Interactions of Asymmetrically Charged Membranes .\nAbstract:\nThe electrostatic interactions between two asymmetrically charged membranes are studied by using the mean-field theory and Monte Carlo simulations. The results show that there is an attractive interaction between these membranes, which can be explained as follows. When one membrane approaches another with opposite charges on their surfaces, it will induce a dipole moment in its neighbor due to charge redistribution at the interface. This induced dipole moment causes an additional attraction between them. In addition, we find that this effect becomes more pronounced when the dielectric constant of water decreases. Finally, our study shows that the magnitude of the electrostatic force depends strongly on the surface charge density difference between the two membranes. We also discuss how the electrostatic forces affect the phase behavior of lipid bilayers. DOI: 10.1063/1.3189000\nI. INTRODUCTIO N\nIn recent years, many studies have been carried out on the properties of biomembranes  1  . It has been found that the physical characteristics of biological systems such as cell adhesion  2  , vesicle fusion  3  , protein folding  4  , etc., depend crucially on the structure and composition of the underlying lipid bilayer  5  .\nBiological membranes consist mainly of phospholipids  6  . These lipids contain hydrophobic tails and hydrophilic heads  7, 8  . Due to the amphiphilicity of phospholipids, they tend to self-assemble into bilayers  9  . A typical example for such a system is shown schematically in Fig.  1(a) . Each layer consists of a monolayer of phospholipids arranged in a fluid-like state  10  . The thickness of each layer is about 5 nm  11  . The head groups point towards the aqueous solution while the tail groups face away from it  12  . Because of the presence of water molecules inside the layers, the effective dielectric constant of the medium is high (about 80)  13  . However, outside the layers, where only air exists, the dielectric constant is low (about 1). Therefore, the electric field lines penetrate easily through the interior region but not so much through the exterior region  14  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Electrostatic Interactions of Asymmetrically Charged Membranes . Abstract : The electrostatic interactions between two asymmetrically charged membranes are studied by using the mean - field principle and Monte Carlo simulations .The results show that there is an interesting interaction between these membranes , which can be understood as follows . When one cell encounters another with opposite charges on their edges , it will generate a dipole point in its neighbor due to charge redistribution at the interface .This induced dipole point causes an additional attraction between them . In addition , we find that this effect gets more pronounced when the dielectric constant of water reduces .Finally , our research shows that the severity of the electrostatic pressure depends strongly on the surface charge density difference between the two membranes . We additionally discuss how the electrostatic fields affect the phase response of lipid bilayers .DOI : 10 . 1063 / 1 . 3189000 I . INTRODUCTIO N In recent years , various studies have been carried out on the properties of biomembranes 1 . It has been shown that the structural traits of biological systems such as cell adhesion 2 , vesicle fusion 3 , protein folding 4 , etc . , depend crucially on the composition and configuration of the underlying lipid bilayer 5 .Biological membranes consist mostly of phospholipids 6 . These lipids contain hydrophobic tails and hydrophilic bodies 7 , 8 .Due to the amphiphilicity of phospholipids , they tend to self - organize into bilayers 9 . A typical example for such a system is demonstrated schematically in Fig .1 ( a ) . Each layer contains of a monolayer of phospholipids grouped in a fluid - like state 10 .The depth of each surface is about 5 nm 11 . The head bands look towards the aqueous solution while the tail groups face away from it 12 .Because of the presence of moisture atoms inside the layers , the effective dielectric constant of the medium is high ( about 80 ) 13 . However , outside the layers , where only air occurs , the dielectric constant is low ( about 1 ) .Therefore , the electric field lines penetrate easily through the interior region but not so much through the exterior zone 14 .",
        "rewrite_text": "**Title:** Electrostatic Interactions of Asymmetrically Charged Membranes\n\n**Abstract:** This study investigates the electrostatic interactions between two asymmetrically charged membranes through the application of mean-field theory and Monte Carlo simulations. Our findings reveal a compelling interaction mechanism between these membranes, characterized by the generation of an induced dipole at the interface when one membrane encounters another with opposite edge charges. This charge redistribution results in an additional attractive force between the membranes, enhancing their interaction. Notably, the strength of this effect intensifies as the dielectric constant of water decreases, indicating a significant relationship between the medium's properties and the electrostatic interactions at play. Furthermore, our analysis demonstrates that the magnitude of the electrostatic pressure is highly sensitive to the difference in surface charge density between the two membranes. We also explore the implications of these electrostatic fields on the phase behavior of lipid bilayers, contributing to a deeper understanding of membrane dynamics. This research is particularly relevant in the context of biological systems, where the structural characteristics of membranes are crucial for processes such as cell adhesion, vesicle fusion, and protein folding. The amphiphilic nature of phospholipids, which form the primary components of biological membranes, leads to their self-organization into bilayers, characterized by hydrophobic tails and hydrophilic heads. The effective dielectric constant within these lipid bilayers is significantly influenced by the presence of water, with values around 80, contrasting sharply with the low dielectric constant of air outside the membranes, approximately 1. This disparity allows electric field lines to penetrate the bilayer interior more readily than the external environment, highlighting the complex interplay between electrostatics and membrane structure. Our findings provide valuable insights into the fundamental principles governing membrane interactions and their implications for biological functions. \n\n**DOI:** 10.1063/1.3189000",
        "ori-fast-z-score": -0.6115928396627265,
        "water-fast-z-score": 6.0470789790695205,
        "rewrite-fast-z-score": 0.1796053020267749
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A cold metal-poor cloud traced by a weak MgII absorption at z~0.45. First detection of SiI, CaI and FeI in a QSO absorber .\nAbstract:\nWe report the first detection of silicon (Si), calcium (Ca) and iron (Fe) ions along with magnesium (Mg) in an intervening galaxy system toward quasar HE 0515-4414 at redshift 0.4485. The observed column densities are log N(Mg+H) = 13.60 ± 0.10 cm-2 , log N(Si+H) = 12.70 ± 0.20 cm-2 , log N (Ca + H ) = 11 .90 ± 0.30 cm -2 , and log N (Fe + H ) = 10.40 ± 0.50 cm -2 .\nThe total hydrogen column density is log NH = 20.0 +0.5 -0.3 cm-2 . We find that this system has low metallicity Z < 1/100 solar abundance ratio for all four elements detected.  This system shows no detectable neutral carbon or molecular hydrogen absorptions down to limits of log NC/NH ~ -1.7 and log MH / NH ~ -3.6 respectively.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A cool metal - weak storm traced by a weak MgII absorption at z ~ 0 . 45 . First detection of SiI , CaI and FeI in a QSO absorber .Abstract : We report the first detection of silicon ( Si ) , calcium ( Ca ) and iron ( Fe ) ions along with magnesium ( Mg ) in an intervening galaxy system toward quasar HE 0515 - 4414 at redshift 0 . 4485 . The observed column densities are log N ( Mg + H ) = 13 . 60 ± 0 . 10 cm - 2 , log N ( Si + H ) = 12 . 70 ± 0 . 20 cm - 2 , log N ( Ca + H ) = 11 . 90 ± 0 . 30 cm - 2 , and log N ( Fe + H ) = 10 . 40 ± 0 . 50 cm - 2 .The total hydrogen column density is log NH = 20 . 0 + 0 . 5 - 0 . 3 cm - 2 . We see that this system has low metallicity Z < 1 / 100 solar abundance ratio for all four elements detected .This system displays no detectable neutral hydrogen or molecular hydrogen absorptions down to bounds of log NC / NH ~ - 1 . 7 and log MH / NH ~ - 3 . 6 respectively .",
        "rewrite_text": "We present the inaugural detection of silicon (Si), calcium (Ca), and iron (Fe) ions, alongside magnesium (Mg), in an intervening galaxy system associated with the quasar HE 0515-4414 at a redshift of 0.4485. The measured column densities for the detected elements are as follows: log N (Mg + H) = 13.60 ± 0.10 cm^-2, log N (Si + H) = 12.70 ± 0.20 cm^-2, log N (Ca + H) = 11.90 ± 0.30 cm^-2, and log N (Fe + H) = 10.40 ± 0.50 cm^-2. The total hydrogen column density is determined to be log NH = 20.0 +0.5 -0.3 cm^-2. Notably, the metallicity of this system is remarkably low, with an abundance ratio of Z < 1/100 of the solar value for all four elements detected. Furthermore, our observations reveal an absence of detectable neutral hydrogen or molecular hydrogen absorptions, with limits set at log NC / NH ~ -1.7 and log MH / NH ~ -3.6, respectively. This study contributes to our understanding of metal-poor environments in the universe and highlights the significance of weak Mg II absorption features in tracing such systems. The findings underscore the importance of ongoing research into the chemical composition of quasar absorbers, particularly in the context of galaxy formation and evolution. The detection of these elements not only enriches the inventory of known absorbers but also provides critical insights into the conditions prevailing in the early universe, where such low metallicity systems may play a crucial role in the cosmic chemical evolution.",
        "ori-fast-z-score": 0.1259881576697424,
        "water-fast-z-score": 2.4735893086356535,
        "rewrite-fast-z-score": 0.5940885257860046
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Theory of enhanced dynamical photo-thermal bi-stability effects in cuprous oxide/organic hybrid heterostructure .\nAbstract:\nThe present work is devoted to the investigation of the photothermal properties and dynamics of the Cu2O/CuO nanocomposite films prepared by pulsed laser deposition (PLD) on Si(100). The PLD technique allows one to obtain high-quality thin films with controlled composition, structure and morphology. It was found that the temperature dependence of the resistance R(T), measured at different light intensities I0, exhibits two distinct regimes corresponding to low-temperature metallic-like behavior and high-temperature semiconducting-like behavior. In addition, it has been shown that the transition between these regimes occurs via an intermediate state characterized by pronounced hysteresis effect. This phenomenon can be explained within the framework of the theory developed for semiconductor-metal phase transitions induced by strong non-equilibrium heating. We have also demonstrated that this model describes well the observed nonlinear response of the investigated system to external periodic driving force.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Theory of enhanced dynamical photo - temperature bi - stability properties in cuprous oxide / organic hybrid heterostructure . Abstract : The present work is devoted to the examination of the photothermal characteristics and dynamics of the Cu2O / CuO nanocomposite films prepared by pulsed laser deposition ( PLD ) on Si ( 100 ) .The PLD procedure allows one to obtain high - grade thin films with controlled composition , structure and morphology . It was shown that the temperature dependence of the resistance R ( T ) , recorded at different light intensities I0 , displays two different regimes corresponding to low - temperature metallic - like behavior and low - temperature semiconducting - like behavior .In addition , it has been shown that the shift between these regimes occurs via an intermediate state characterized by marked hysteresis effect . This phenomenon can be described within the framework of the theoretical created for semiconductor - metal phase transitions induced by weak non - equilibrium heating .We have also demonstrated that this model describes well the seen nonlinear reaction of the investigated structure to external periodic drove force .",
        "rewrite_text": "This study focuses on the investigation of the photothermal properties and dynamic behavior of Cu2O/CuO nanocomposite films, which were synthesized using pulsed laser deposition (PLD) on silicon substrates (Si(100)). The PLD technique enables the production of high-quality thin films with precisely controlled composition, structure, and morphology. Our findings reveal that the temperature-dependent resistance, R(T), measured under varying light intensities (I0), exhibits two distinct regimes: one resembling metallic behavior at low temperatures and the other exhibiting semiconducting characteristics. Notably, the transition between these two regimes occurs through an intermediate state that is characterized by a significant hysteresis effect. This behavior can be effectively explained using a theoretical framework developed for semiconductor-metal phase transitions that are triggered by mild non-equilibrium heating. Furthermore, we have shown that this theoretical model accurately captures the observed nonlinear response of the hybrid structure to external periodic driving forces. The implications of these findings are significant for understanding the underlying mechanisms of phase transitions in hybrid materials and could pave the way for the development of advanced photothermal devices with enhanced stability and performance.",
        "ori-fast-z-score": 0.105999788000636,
        "water-fast-z-score": 5.270462766947299,
        "rewrite-fast-z-score": -0.6123724356957946
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Central Kiloparsec of Seyfert and Inactive Host Galaxies: a Comparison of Two-Dimensional Stellar and Gaseous Kinematics .\nAbstract:\nWe present the first two-dimensional kinematics for both stars and gas in the central kiloparsecs (kpc) of two nearby galaxies, NGC 3227 and NGC 4151. The stellar kinematics are derived from near-infrared integral field spectroscopy obtained with SINFONI on VLT/UT4. We use these data to construct dynamical models that include contributions from both stars and gas. Our results show that the mass distribution is dominated by dark matter within 1 kpc radius in both cases. However, we find evidence for an additional component of hot ionized gas at large radii in NGC 4151 which may be associated with outflows driven by nuclear activity. This work demonstrates how spatially resolved observations can provide important insights into galaxy formation processes. Keywords: Galaxy dynamics; Near-infrared spectroscopy; Integral Field Spectroscopy. Subject headings: Black holes - accretion disks; Nuclear star clusters",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Central Kiloparsec of Seyfert and Inactive Host Galaxies : a Comparison of Two - Dimensional Stellar and Gaseous Kinematics . Abstract : We present the first two - dimensional kinematics for both stars and gas in the central kiloparsecs ( kpc ) of two nearby galaxies , NGC 3227 and NGC 4151 .The stellar kinematics are derived from near - infrared integral field spectroscopy derived with SINFONI on VLT / UT4 . We use these information to build dynamical models that include contributions from both stars and gas .Our results show that the mass distribution is dominated by black material within 1 kpc radius in both cases . However , we find proof for an additional element of hot ionized gas at large radii in NGC 4151 which may be involved with outflows driven by nuclear activity .This research shows how spatially resolved surveys can provide important perspectives into galaxy formation systems . Keywords : Galaxy structure ; Near - infrared spectroscopy ; Integral Field Spectroscopy .Subject headings : Black holes - accretion disks ; Nuclear star clusters",
        "rewrite_text": "We present a comprehensive analysis of the two-dimensional kinematics of stars and gas in the central kiloparsecs (kpc) of two nearby galaxies, NGC 3227 and NGC 4151. Utilizing near-infrared integral field spectroscopy obtained with the SINFONI instrument on the VLT/UT4, we derive detailed stellar kinematics that allow us to construct dynamical models incorporating contributions from both stellar and gaseous components. Our findings reveal that the mass distribution within a 1 kpc radius is predominantly influenced by black material in both galaxies. Notably, in NGC 4151, we identify evidence of an additional component of hot ionized gas located at larger radii, which may play a significant role in outflows associated with nuclear activity. This study underscores the importance of spatially resolved surveys in enhancing our understanding of galaxy formation and evolution. By comparing the kinematic properties of Seyfert galaxies with those of inactive host galaxies, we aim to shed light on the underlying processes that govern the dynamics of these systems. Our results contribute to the broader discourse on galaxy structure and the interplay between black holes and their host environments, particularly in the context of accretion disks and nuclear star clusters. The implications of our research extend to the fields of astrophysics and cosmology, providing valuable insights into the mechanisms driving galaxy evolution. \n\nKeywords: Galaxy structure; Near-infrared spectroscopy; Integral Field Spectroscopy.  \nSubject headings: Black holes - accretion disks; Nuclear star clusters.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 3.542913886332851,
        "rewrite-fast-z-score": 1.8542101386022132
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Fractal analysis for sets of non-differentiability of Minkowski s question mark function .\nAbstract:\nIn this work, we study the fractal dimension and Hausdorff measure of the set of points where  is not differentiable in . We show that these measures are equal to one half times the corresponding values on the unit interval with Lebesgue measure.  This result generalizes previous results by Jarník (for ) and Marstrand (for ). The proof relies on an explicit formula for the Fourier transform of , which was obtained recently by Fejér and Riesz. In addition, we prove that the same holds true if instead of the Lebesgue measure on the unit interval, we consider any other probability measure whose support has positive length. Finally, we give some applications of our main theorem to the theory of lacunary series. Let be the space of all complex-valued functions defined on the real line equipped with the topology of uniform convergence on compact subsets of .\nLet denote the class of all infinitely differentiable functions on such that their derivatives satisfy the Hölder condition with exponent greater than or equal to 1 at every point of the domain. \nFor each integer let us define the operator as follows: \nwhere denotes the th derivative of .  It can easily be seen that is continuous linear functional on .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Fractal analysis for sets of non - differentiability of Minkowski s question mark function . Abstract : In this study , we study the fractal dimension and Hausdorff measure of the group of points where is not differentiable in .We see that these measures are equal to one half times the equivalent values on the unit interval with Lebesgue measure . This result generalizes earlier findings by Jarník ( for ) and Marstrand ( for ) .The proof relies on an explicit formula for the Fourier transform of , which was obtained recently by Fejér and Riesz . In addition , we prove that the same holds true if instead of the Lebesgue measure on the unit interval , we define any other probability measure whose support has positive length .Finally , we give some applications of our major principle to the notion of lacunary series . Let be the space of all complex - valued functions defined on the real line equipped with the topology of uniform convergence on compact subsets of .Let define the class of all infinitely differentiable functions on such that their derivatives satisfy the Hölder condition with exponent greater than or equal to 1 at every point of the domain . For each integer take us define the operator as follows : where denotes the th derivative of .It can easily be saw that is continuous continuous functional on .",
        "rewrite_text": "Title: Fractal Analysis of Non-Differentiability Sets in Minkowski's Question Mark Function\n\nAbstract: This research investigates the fractal dimension and Hausdorff measure associated with the set of points where Minkowski's question mark function is non-differentiable. Our findings reveal that these measures correspond to half of the equivalent values observed on the unit interval with respect to the Lebesgue measure. This result extends previous work by Jarník and Marstrand, who explored similar concepts in their respective studies. The proof of our main theorem is grounded in a recently derived explicit formula for the Fourier transform of the question mark function, as established by Fejér and Riesz. Furthermore, we demonstrate that our conclusions remain valid when considering any probability measure with positive support length, rather than strictly the Lebesgue measure on the unit interval. \n\nAdditionally, we explore the implications of our primary results in the context of lacunary series. We define a space of complex-valued functions on the real line, equipped with the topology of uniform convergence on compact subsets. Within this framework, we introduce a class of infinitely differentiable functions whose derivatives adhere to the Hölder condition with an exponent of one or greater at every point in their domain. For each integer, we define an operator that acts on these functions, where the operator is based on the nth derivative. It is straightforward to verify that this operator is a continuous functional on the defined space. Our work not only contributes to the understanding of the fractal properties of Minkowski's question mark function but also opens avenues for further exploration in the realm of functional analysis and its applications to series convergence.",
        "ori-fast-z-score": 0.7181848464596079,
        "water-fast-z-score": 3.7567808109943908,
        "rewrite-fast-z-score": 0.552344770738994
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Redesigning Computer-based Learning Environments: Evaluation as Communication .\nAbstract:\nThe purpose of this study is to examine the role that evaluation plays in communication between teachers and students during computer-based learning environments (CBLE). The research question guiding this investigation is: How does evaluation affect student-teacher interaction? This study was conducted with two groups of undergraduate students enrolled in an introductory course on educational technology at a large Midwestern university. Students were asked to complete three tasks using a CBLE called WebQuests, which are designed for use by students working individually or collaboratively. Data collection included audio recordings of group discussions, field notes taken by researchers observing each group s work, and students  written responses to questions posed throughout their participation in the project. Analysis revealed that evaluation played several roles within these interactions including providing feedback about individual performance; clarifying expectations; establishing ground rules; and promoting reflection. These findings suggest that evaluation can be used effectively to enhance student-teacher communication when it occurs frequently enough over time so that both parties have multiple opportunities to respond to one another.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Redesigning Computer - based Learning Environments : Evaluation as Communication . Abstract : The purpose of this study is to examine the part that assessment plays in interaction between instructors and students during computer - based educational environments ( CBLE ) .The research question guiding this study is : How does evaluation affect student - teacher activity ? This study was done with two groups of undergraduate people studied in an introductory program on teaching technology at a large Midwestern college .Students were asked to complete three objectives utilizing a CBLE named WebQuests , which are created for use by students studying individually or collaboratively . Data collection included audio performances of group discussions , field notes taken by researchers observing each organization s project , and teachers authored reactions to questions posed throughout their involvement in the project .Analysis revealed that assessment played numerous roles within these interactions including providing guidance about individual performance ; clarifying assumptions ; creating ground requirements ; and promoting reflection . These studies propose that assessment can be used effectively to enhance teacher - teacher understanding when it appears frequently enough over time so that both parties have multiple options to respond to one another .",
        "rewrite_text": "**Title:** Redesigning Computer-Based Learning Environments: Evaluation as Communication\n\n**Abstract:** This study investigates the role of assessment in facilitating interactions between instructors and students within computer-based learning environments (CBLEs). The central research question guiding this inquiry is: How does evaluation influence the dynamics of student-teacher engagement? The research was conducted with two groups of undergraduate students enrolled in an introductory teaching technology program at a large Midwestern university. Participants were tasked with achieving three objectives using a CBLE known as WebQuests, designed for both individual and collaborative learning experiences. Data collection methods included audio recordings of group discussions, field notes taken by researchers observing each group's project work, and written responses from instructors to questions raised during the project. \n\nThe analysis of the collected data revealed that assessment serves multiple functions within these educational interactions. It provides critical feedback on individual performance, clarifies underlying assumptions, establishes foundational requirements for tasks, and fosters reflective practices among participants. The findings suggest that frequent and meaningful assessment can significantly enhance the understanding between teachers and students, as it allows for ongoing dialogue and multiple opportunities for feedback. This study underscores the importance of integrating effective assessment strategies in CBLEs to promote a more interactive and communicative educational environment, ultimately benefiting both instructors and learners.",
        "ori-fast-z-score": -0.7770286898858113,
        "water-fast-z-score": 7.246315678266502,
        "rewrite-fast-z-score": 0.647150228929434
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Infrared spectroscopy of HCOOH in interstellar ice analogues .\nAbstract:\nWe report infrared (IR) spectroscopic studies on the formation and evolution of formic acid, HCOOH, in ices under simulated astrophysical conditions. The experiments were performed by exposing pure water or mixtures of H2O with CH3OH to vacuum ultraviolet radiation at 10 K for different periods up to 100 hours. IR spectra show that the amount of HCOOH increases as a function of irradiation time. We also find evidence for the presence of other species such as CO2, CO, CH4, NH3, H2S, SO2, and OCS. These results are discussed within the context of astrochemical models. Formic acid is one of the most abundant organic molecules found in space. It has been detected towards comets, protostars, and evolved stars. In particular, it was observed in comet 67P/Churyumov-Gerasimenko during its flyby of the Rosetta spacecraft. However, despite this ubiquity, our understanding about how formic acid forms in space remains limited. Here we present laboratory investigations into the formation pathways of formic acid in icy environments using infrared spectroscopy.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Infrared spectroscopy of HCOOH in interstellar ice analogues . Abstract : We report imaging ( IR ) spectroscopic studies on the formation and evolution of formic acid , HCOOH , in ices under simulated astrophysical conditions .The studies were performed by exposing raw water or mixtures of H2O with CH3OH to vacuum ultraviolet radiation at 10 K for different duration up to 100 hours . IR spectra show that the amount of HCOOH changes as a function of irradiation time .We additionally find proof for the presence of other species such as CO2 , CO , CH4 , NH3 , H2S , SO2 , and OCS . These conclusions are discussed within the context of astrochemical models .Formic acid is one of the most numerous organic particles discovered in space . It has been detected towards comets , protostars , and evolved planets .In particular , it was seen in comet 67P / Churyumov - Gerasimenko during its flyby of the Rosetta spacecraft . However , despite this ubiquity , our view about how formic acid forms in space remains restricted .Here we present lab studies into the formation routes of formic acid in icy environments using infrared spectroscopy .",
        "rewrite_text": "We present a comprehensive study on the formation and evolution of formic acid (HCOOH) in interstellar ice analogues through infrared (IR) spectroscopy, conducted under simulated astrophysical conditions. Our experiments involved exposing pure water and various mixtures of H2O and CH3OH to vacuum ultraviolet radiation at a temperature of 10 K, with exposure durations extending up to 100 hours. The resulting IR spectra revealed a dynamic variation in the quantity of HCOOH produced, which correlates with the length of irradiation time. In addition to formic acid, we identified the presence of several other chemical species, including CO2, CO, CH4, NH3, H2S, SO2, and OCS, which are significant in the context of astrochemical processes. These findings are interpreted within the framework of existing astrochemical models, contributing to our understanding of organic molecule formation in space.\n\nFormic acid is recognized as one of the most abundant organic compounds found in extraterrestrial environments, having been detected in various celestial bodies such as comets, protostars, and evolved planets. Notably, it was observed in comet 67P/Churyumov-Gerasimenko during the Rosetta spacecraft's mission. Despite its prevalence, the mechanisms underlying the formation of formic acid in space remain poorly understood. This study aims to bridge that knowledge gap by exploring the laboratory-based formation pathways of formic acid in icy environments, utilizing advanced infrared spectroscopy techniques. Our results not only enhance the understanding of formic acid's role in astrochemistry but also provide valuable insights into the broader implications for organic chemistry in the universe.",
        "ori-fast-z-score": 0.35603449745815596,
        "water-fast-z-score": 4.798687035041354,
        "rewrite-fast-z-score": 1.3348476249438292
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Availability assessment of SunOS/Solaris Unix Systems based on Syslogd and wtmpx logfiles : a case study .\nAbstract:\nThe availability of computer systems is an important aspect in the design, development and operation of any system. The aim of this research was to develop a method for assessing the availability of a large number of SunOS/Solaries systems using syslogd (System Logging Daemon) and wtmpx (Windows Time Machine eXtender). This research has been carried out by collecting data from a set of servers over a period of one year. A total of 1,000,000 records were collected during that time frame. These records have then been processed into a database which contains information about each record such as timestamps, host name, service type etc.. In order to assess the availability of these systems we developed two algorithms; one algorithm calculates the percentage uptime per day while another algorithm calculates the percentage downtime per hour. We also implemented a web interface so that users can view the results obtained from our analysis.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Availability analysis of SunOS / Solaris Unix Systems using on Syslogd and wtmpx logfiles : a case study . Abstract : The availability of software machines is an important element in the development , construction and operation of any program .The goal of this research was to develop a technique for evaluating the availability of a large number of SunOS / Solaries devices using syslogd ( System Logging Daemon ) and wtmpx ( Windows Time Machine eXtender ) . This research has been carried out by collecting data from a pair of servers over a period of one year .A total of 1 , 000 , 000 data were collected during that time frame . These documents have then been processed into a computer which contains information about each record such as timestamps , host title , service kind etc . .In order to analyze the availability of these systems we developed two strategies ; one algorithm calculates the percentage uptime per morning while another algorithm calculates the proportion downtime per hour . We additionally developed a browser interface so that users can view the results collected from our analysis .",
        "rewrite_text": "**Title:** Availability Analysis of SunOS/Solaris Unix Systems Using Syslogd and Wtmpx Logfiles: A Case Study\n\n**Abstract:** The reliability of software systems is a critical factor in the design, implementation, and maintenance of any application. This study aims to establish a method for assessing the availability of numerous SunOS/Solaris devices by leveraging syslogd (System Logging Daemon) and wtmpx (Windows Time Machine eXtender) log files. Data collection was conducted over a one-year period from two servers, resulting in the accumulation of approximately 1,000,000 records. Each log entry was meticulously processed to extract essential information, including timestamps, host identifiers, and service types. To evaluate system availability, we devised two analytical approaches: the first algorithm computes the percentage of uptime on a daily basis, while the second algorithm determines the proportion of downtime on an hourly scale. Furthermore, we developed a user-friendly web interface that allows stakeholders to access and visualize the results of our analysis. This comprehensive approach not only provides insights into the operational performance of SunOS/Solaris systems but also serves as a valuable resource for system administrators seeking to enhance the reliability of their Unix environments. The findings of this research contribute to the broader understanding of system availability metrics and offer a framework for future studies in the field of system reliability analysis.",
        "ori-fast-z-score": -1.2701705922171767,
        "water-fast-z-score": 5.505977612893481,
        "rewrite-fast-z-score": 0.4
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Hagedorn Strings and Correspondence Principle in AdS(3) .\nAbstract:\nWe study the correspondence principle between string theory on anti-de Sitter space-time (AdS(3)) and field theories at finite temperature, by using Hagedorn strings as probes. We show that the number density of Hagedorn strings is proportional to the free energy density of the thermal gas of massless particles in the dual field theory. This result implies that the entropy density of the thermal gas agrees with the Bekenstein-Hawking entropy density of black holes in AdS(3). In addition we find that the pressure of the thermal gas agrees exactly with the negative cosmological constant in AdS(3), which corresponds to the vacuum expectation value of the dilaton field in the dual conformal field theory. These results are consistent with the holographic principle proposed recently for strongly coupled gauge theories. The present work may be regarded as an extension of our previous works  Phys Rev D71:035010  and  JHEP 0804:0703  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Hagedorn Strings and Correspondence Principle in AdS ( 3 ) . Abstract : We research the correspondence principle between string theory on anti - de Sitter space - time ( AdS ( 3 ) ) and field theories at finite temperature , by using Hagedorn strings as probes .We see that the number density of Hagedorn strings is proportional to the free energy density of the thermal gas of massless molecules in the dual field model . This result means that the entropy concentration of the thermal gas agrees with the Bekenstein - Hawking entropy concentration of black holes in AdS ( 3 ) .In addition we find that the pressure of the thermal gas agrees exactly with the negative cosmological constant in AdS ( 3 ) , which corresponds to the vacuum expectation value of the dilaton field in the dual conformal field model . These conclusions are compatible with the holographic principle adopted recently for highly coupled gauge fields .The present work must be regarded as an addition of our previous works Phys Rev D71 : 035010 and JHEP 0804 : 0703 .",
        "rewrite_text": "In this study, we investigate the correspondence principle between string theory formulated in anti-de Sitter space-time (AdS(3)) and finite temperature field theories, utilizing Hagedorn strings as a means of exploration. Our findings reveal a significant relationship between the number density of Hagedorn strings and the free energy density of a thermal gas composed of massless particles in the corresponding dual field theory. This correlation suggests that the entropy density of the thermal gas aligns with the Bekenstein-Hawking entropy associated with black holes in AdS(3), indicating a deep connection between these two seemingly disparate frameworks. Furthermore, we demonstrate that the pressure exerted by the thermal gas precisely matches the negative cosmological constant present in AdS(3), which is interpreted as the vacuum expectation value of the dilaton field in the dual conformal field theory. These results are consistent with the holographic principle, which has been recently applied to strongly coupled gauge theories. This work builds upon our previous research published in Phys Rev D71: 035010 and JHEP 0804: 0703, contributing further insights into the interplay between string theory and quantum field theory in the context of AdS space. Our findings not only reinforce the validity of the correspondence principle but also enhance our understanding of the thermodynamic properties of Hagedorn strings and their implications for black hole physics in a holographic framework.",
        "ori-fast-z-score": 0.11396057645963795,
        "water-fast-z-score": 4.076197322920544,
        "rewrite-fast-z-score": 0.9805806756909202
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Final State Interaction Effects in Semi-inclusive Deep Inelastic processes A(e,e p)X off the deuteron and complex nuclei .\nAbstract:\nWe present results on semi-inclusive deep-inelastic scattering (SIDIS), ep -> e  p X , with an emphasis on final state interaction effects for both proton and neutron targets. We use a model-independent approach to extract information about the transverse momentum dependence of partonic distributions by studying azimuthal correlations between hadrons produced in SIDIS events. The data were collected using the HERMES experiment at DESY during 1997-2000. Final state interactions are studied through their effect on the cross section as well as on various asymmetries. For example we show that the observed cos2#h correlation is consistent with rescattering contributions involving quarks carrying only a small fraction of the target nucleon s longitudinal momentum. This result indicates that the quark sea may be more asymmetric than previously thought. Furthermore, our analysis shows that the magnitude of the rescattering contribution depends strongly on the kinematics chosen. Finally, we study the influence of FSI on the extraction of transversity distribution functions.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Final State Interaction Effects in Semi - inclusive Deep Inelastic interactions A ( e , e p ) X off the deuteron and complex nuclei . Abstract : We report findings on semi - inclusive shallow - inelastic reflection ( SIDIS ) , ep - > e p X , with an emphasis on final state interaction influences for both proton and neutron targets .We use a theory - independent approach to extract information about the transverse momentum dependence of partonic distributions by examining azimuthal correlations between hadrons formed in SIDIS events . The data were collected using the HERMES experiment at DESY during 1997 - 2000 .Final state relationships are studied through their effect on the cross section as well as on various asymmetries . For instance we find that the reported cos2 # h relationship is compatible with rescattering contributions involving quarks carrying only a small fraction of the target nucleon s longitudinal momentum .This result suggests that the quark ocean must be more asymmetric than previously thought . Furthermore , our analysis shows that the severity of the rescattering contribution varies strongly on the kinematics selected .Finally , we study the impact of FSI on the extraction of transversity distribution functions .",
        "rewrite_text": "Title: Final State Interaction Effects in Semi-Inclusive Deep Inelastic Interactions A (e, ep)X off the Deuteron and Complex Nuclei\n\nAbstract: In this study, we present our findings on semi-inclusive deep inelastic scattering (SIDIS), specifically the process ep → epX, focusing on the effects of final state interactions (FSI) for both proton and neutron targets. Utilizing a theory-independent methodology, we aim to extract insights into the transverse momentum dependence of partonic distributions by analyzing azimuthal correlations among hadrons produced in SIDIS events. The data for this research were gathered during the HERMES experiment at DESY from 1997 to 2000. Our investigation into final state interactions reveals their significant influence on the cross section and various asymmetries observed in the scattering process. Notably, we find that the cos2φh dependence aligns with rescattering contributions from quarks that carry only a minor fraction of the longitudinal momentum of the target nucleon. This observation indicates that the quark sea may exhibit a greater degree of asymmetry than previously recognized. Additionally, our analysis demonstrates that the intensity of rescattering effects is highly dependent on the selected kinematic conditions. Ultimately, we explore how these final state interaction effects impact the extraction of transversity distribution functions, providing a deeper understanding of the underlying dynamics in semi-inclusive deep inelastic scattering processes. This work contributes to the broader knowledge of partonic structure and interactions within nucleons, highlighting the complexities introduced by final state interactions in high-energy scattering experiments.",
        "ori-fast-z-score": -0.741998516004452,
        "water-fast-z-score": 3.70999258002226,
        "rewrite-fast-z-score": -0.2847473987257497
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Multiepoch Radial Velocity Observations of L Dwarfs .\nAbstract:\nWe present the results of multiepoch radial velocity (RV) observations for four late-type M dwarfs and three brown dwarf candidates with spectral types ranging between M3V-M8V, obtained using the High Accuracy Radial velocity Planet Searcher (HARPS). We find no evidence for RV variability in any of these targets at levels greater than ~1 m/s over timescales up to several years.  These results are consistent with previous studies that have found little or no evidence for giant planets around low-mass stars and substellar objects. The lack of detectable RV variations is likely due to the small masses and/or radii of the companions expected if they exist. However, we cannot rule out the possibility that some of our targets harbor terrestrial mass planets on orbits with periods longer than those probed by our current data set. Future high-precision RV measurements will be required to determine whether such planets do indeed exist.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Multiepoch Radial Velocity Observations of L Dwarfs . Abstract : We report the results of multiepoch radial speed ( RV ) observations for four late - class M dwarfs and three brown giant candidates with spectral classes ranging between M3V - M8V , obtained using the High Accuracy Radial velocity Planet Searcher ( HARPS ) .We see no evidence for RV variability in any of these targets at levels greater than ~ 1 m / s over timescales up to several decades . These conclusions are compatible with previous research that have discovered nothing or no evidence for big planets around low - density stars and substellar objects .The absence of detectable RV variations is probably due to the small masses and / or radii of the companions expected if they exist . However , we cannot block out the suggestion that some of our objectives harbor terrestrial mass stars on orbits with periods longer than those probed by our latest data set .Future high - precision RV measurements will be required to test whether such planets do actually appear .",
        "rewrite_text": "We present the findings from our multiepoch radial velocity (RV) observations conducted on four late-type M dwarfs and three candidates for brown giants, with spectral classifications ranging from M3V to M8V. These observations were carried out using the High Accuracy Radial velocity Planet Searcher (HARPS) instrument. Our analysis reveals no significant RV variability exceeding approximately 1 m/s across these targets, even when observed over timescales extending to several decades. This lack of detectable variability aligns with prior studies that have similarly found no substantial evidence of large planetary companions orbiting low-density stars and substellar objects. The absence of RV fluctuations may be attributed to the relatively small masses and/or radii of any potential companions, should they exist. Nevertheless, we cannot entirely dismiss the possibility that some of the stars in our sample could host terrestrial-mass planets on orbits with periods longer than those examined in our current dataset. To conclusively determine the presence of such planets, future high-precision RV measurements will be essential. Our findings contribute to the ongoing discourse regarding the characteristics of planetary systems around late-type stars and highlight the need for continued observational efforts to uncover the potential for terrestrial planets in these environments.",
        "ori-fast-z-score": -1.2701705922171767,
        "water-fast-z-score": 4.672383634845156,
        "rewrite-fast-z-score": -1.0206207261596576
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Low CO Content of the Extremely Metal Poor Galaxy I Zw 18 .\nAbstract:\nWe have obtained high resolution (R = λ/Δλ ~ 20000) optical spectra for two bright HII regions in the extremely metal poor galaxy IZw18, using the Keck telescope and its High Resolution Echelle Spectrometer (HIRES). The main results are as follows:\n(1) We find that the oxygen abundance is 12+log(O/H)=7.16±0.03 dex at R=3′′ and 7.10±0.02 dex at R=1′′.\n(2) From our analysis we conclude that there exists an apparent gradient in the O/H ratio across this galaxy.  This result suggests that the chemical enrichment process has proceeded more rapidly towards the center than along the outer parts of the galaxy. In addition to these observations, we also present new measurements on the carbon content of this galaxy. Our data show that C/O=0.25 ± 0.05 at R=3′′ and 0.20 ± 0.04 at R=1′′.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Low CO Content of the Extremely Metal Poor Galaxy I Zw 18 . Abstract : We have discovered high resolution ( R = λ / Δλ ~ 20000 ) imaging spectra for two bright HII zones in the exceptionally steel poor galaxy IZw18 , using the Keck telescope and its High Resolution Echelle Spectrometer ( HIRES ) .The main results are as follows : ( 1 ) We see that the oxygen abundance is 12 + log ( O / H ) = 7 . 16±0 . 03 dex at R = 3 ′ ′ and 7 . 10±0 . 02 dex at R = 1 ′ ′ . ( 2 ) From our analysis we conclude that there exists an apparent gradient in the O / H ratio across this galaxy .This result suggests that the chemical enrichment reaction has continued more fast towards the center than along the inner parts of the galaxy . In addition to these observations , we also provided new studies on the carbon composition of this galaxy .Our data indicate that C / O = 0 . 25 ± 0 . 05 at R = 3 ′ ′ and 0 . 20 ± 0 . 04 at R = 1 ′ ′ .",
        "rewrite_text": "**Title:** The Low Carbon Content of the Extremely Metal-Poor Galaxy I Zw 18\n\n**Abstract:** In this study, we present high-resolution imaging spectra (R = λ / Δλ ~ 20,000) obtained from two prominent HII regions within the extraordinarily metal-poor galaxy I Zw 18, utilizing the Keck Observatory's High Resolution Echelle Spectrometer (HIRES). Our findings reveal significant insights into the chemical composition of this galaxy, particularly regarding its oxygen and carbon abundances. Specifically, we measured the oxygen abundance to be 12 + log (O/H) = 7.16 ± 0.03 dex at a radius of 3″ and 7.10 ± 0.02 dex at a radius of 1″. These measurements indicate a notable gradient in the O/H ratio across the galaxy, suggesting that the process of chemical enrichment has progressed more rapidly towards the center compared to the outer regions. Furthermore, our investigation extends to the carbon content, where we found the carbon-to-oxygen ratio (C/O) to be 0.25 ± 0.05 at R = 3″ and 0.20 ± 0.04 at R = 1″. These results contribute to our understanding of the elemental composition and evolutionary processes in I Zw 18, highlighting its unique characteristics as one of the most metal-poor galaxies known. The observed gradients in both oxygen and carbon abundances provide critical evidence for the mechanisms of star formation and chemical evolution in low-metallicity environments. Overall, our research underscores the importance of I Zw 18 as a key object for studying the early stages of galaxy formation and the role of metal enrichment in the universe.",
        "ori-fast-z-score": -0.8017837257372732,
        "water-fast-z-score": 3.5762373640756184,
        "rewrite-fast-z-score": 0.8
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Millennium Galaxy Catalogue: The local supermassive black hole mass function in early- and late-type galaxies .\nAbstract:\nWe present the first measurement of the supermassive black hole (SMBH) mass function for both early-(elliptical/S0, Sa-Sb) and latetype (Scd-Sm) galaxies using data from the Millennium Galaxy Catalogue (MGC). We use two different methods to measure SMBH masses - stellar velocity dispersion measurements and bulge luminosity scaling relations. Our results show that there is no significant difference between the SMBH mass functions of these galaxy types at z < 0.1. However we find evidence for an evolution with redshift such that the number density of massive SMBHs decreases more rapidly than less-massive ones. This suggests that the most massive SMBHs are likely to have grown by accretion over cosmic time rather than merging events. These findings will be important constraints on models of SMBH growth and AGN feedback.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Millennium Galaxy Catalogue : The regional supermassive black hole mass function in early - and mid - class objects . Abstract : We report the first measurement of the supermassive black hole ( SMBH ) mass function for both late - ( elliptical / S0 , Sa - Sb ) and latetype ( Scd - Sm ) galaxies using data from the Millennium Galaxy Catalogue ( MGC ) .We use two different methods to measure SMBH masses - stellar velocity dispersion measurements and bulge luminosity scaling relations . Our results show that there is no major variation between the SMBH mass parameters of these galaxy types at h < 0 . 1 .However we find proof for an evolution with redshift such that the number density of large SMBHs falls more swiftly than less - massive ones . This implies that the most gigantic SMBHs are likely to have expanded by accretion over cosmic time rather than merging events .These conclusions will be crucial constraints on estimates of SMBH growth and AGN feedback .",
        "rewrite_text": "We present the inaugural assessment of the supermassive black hole (SMBH) mass function across both late-type (elliptical/S0, Sa-Sb) and late-type (Scd-Sm) galaxies, utilizing data from the Millennium Galaxy Catalogue (MGC). Our study employs two distinct methodologies for determining SMBH masses: stellar velocity dispersion measurements and bulge luminosity scaling relations. The findings indicate that there is no significant difference in the SMBH mass parameters between these galaxy classifications at a redshift of h < 0.1. However, we observe evidence of evolutionary trends with redshift, revealing that the number density of more massive SMBHs declines at a faster rate compared to their less massive counterparts. This suggests that the most massive SMBHs have likely grown primarily through accretion processes over cosmic time, rather than through the merging of smaller black holes. These insights are vital for refining our understanding of SMBH growth mechanisms and the role of active galactic nucleus (AGN) feedback in galaxy evolution. The implications of our results extend to the broader context of cosmic structure formation and the interplay between black hole growth and galaxy development. Our research contributes to the ongoing discourse on the evolution of SMBHs and their influence on the surrounding galactic environment, providing essential constraints for future models of SMBH dynamics and their impact on cosmic history.",
        "ori-fast-z-score": -0.8307471607356973,
        "water-fast-z-score": 4.09644015186457,
        "rewrite-fast-z-score": 1.4
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Classical approach to the graph isomorphism problem using quantum walks .\nAbstract:\nWe present an algorithm for solving the Graph Isomorphism Problem (GIP) based on Quantum Walks and Grover s search algorithm. The proposed method is inspired by the classical algorithms that use random walks, but it uses Grover s operator instead of the Hadamard matrix in order to speed up the process. We show how this new technique can be used to solve GIP with high probability when the number of vertices in both graphs are equal or differ at most one unit. Finally we compare our results against other state-of-the-art methods. In recent years there has been growing interest in developing efficient solutions to problems related to computational complexity theory such as the Graph Isomorphism Problem(GIP). This problem consists of determining whether two given graphs are isomorphic or not; i.e., if they have the same structure regardless of their labels. \n \n Classical approaches to solve GIP usually rely on Random Walks techniques combined with some heuristics. However these methods require exponential time in the worst case scenario. On the other hand, Quantum Algorithms provide polynomial-time solutions to many NP-complete problems including GIP. These algorithms exploit the superposition principle which allows them to explore all possible states simultaneously. For example, Shor s Algorithm solves integer factorization in polynomial time while Grover s Search finds any item within a database in quadratic time.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Classical approach to the graph isomorphism question involving quantum walks . Abstract : We present an algorithm for solving the Graph Isomorphism Problem ( GIP ) based on Quantum Walks and Grover s search algorithm .The proposed approach is influenced by the classical techniques that use random tours , but it utilizes Grover s operator rather of the Hadamard vector in order to faster up the process . We see how this new technique can be used to solve GIP with high probability when the number of vertices in both graphs are equal or differ at most one unit .Finally we compare our findings against other state - of - the - art methods . In past decades there has been growing interest in building successful solutions to problems related to computational complexity analysis such as the Graph Isomorphism Problem ( GIP ) .This problem consists of determining whether two given graphs are isomorphic or not ; i . e . , if they have the same structure regardless of their labels . Classical approaches to solve GIP usually relies on Random Walks techniques combined with some heuristics .However these algorithms involve exponential time in the worst case situations . On the other hand , Quantum Algorithms provide polynomial - time solutions to many NP - perfect issues including GIP .These methods incorporate the superposition concept which allows them to examine all possible states simultaneously . For instance , Shor s Algorithm solves integer factorization in polynomial time while Grover s Search finds any element within a computer in quadratic time .",
        "rewrite_text": "We introduce a novel algorithm aimed at addressing the Graph Isomorphism Problem (GIP) by leveraging Quantum Walks in conjunction with Grover's search algorithm. This innovative approach draws inspiration from classical methodologies that employ random tours, yet distinguishes itself by utilizing Grover's operator instead of the traditional Hadamard vector, thereby enhancing the efficiency of the process. Our findings demonstrate that this new technique can effectively solve the GIP with a high probability, particularly when the number of vertices in the two graphs is either equal or differs by no more than one. \n\nThe Graph Isomorphism Problem has garnered significant attention over the past few decades, particularly in the context of computational complexity analysis. The core challenge of GIP lies in determining whether two graphs are isomorphic, meaning they share the same structural properties irrespective of their vertex labels. Traditional methods for tackling GIP often rely on Random Walk techniques combined with various heuristics; however, these classical algorithms can exhibit exponential time complexity in the worst-case scenarios.\n\nIn contrast, Quantum Algorithms offer polynomial-time solutions to many NP-complete problems, including GIP. These quantum methods exploit the principle of superposition, enabling them to evaluate multiple potential states simultaneously. For example, Shor's Algorithm efficiently addresses integer factorization in polynomial time, while Grover's Search algorithm can locate an element within a dataset in quadratic time. By comparing our results with existing state-of-the-art techniques, we highlight the advantages of our quantum-based approach, paving the way for more efficient solutions to the Graph Isomorphism Problem and contributing to the broader field of computational complexity.",
        "ori-fast-z-score": -0.5669467095138409,
        "water-fast-z-score": 5.550253123463223,
        "rewrite-fast-z-score": 0.6260990336999411
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Thermodynamic quantum critical behavior of the Kondo necklace model .\nAbstract:\nWe study thermodynamics and transport properties of the Kondo necklacemodel (KNM) in the vicinity of its quantum phase transition to an ordered state, which is driven by spin fluctuations. We show that this transition can be described within the framework of Landau-Ginzburg-Wilson theory with a nontrivial scaling dimension of the order parameter field. The latter determines the universality class of the transition as well as the temperature dependence of various physical quantities such as specific heat or resistivity. In particular we find that at low temperatures the system exhibits non-Fermi liquid behavior characterized by power-law dependences of these quantities on T . \nI. INTRODUCTORY REMARkS\nThe Kondo necklace model 1 ,2 describes a chain of magnetic impurities coupled via antiferromagnetic exchange interactions J. It has been introduced originally for describing the physics of heavy fermion compounds 3 but it also appears naturally in other contexts 4-7 .\nIn the present work we consider the case when the coupling constant J exceeds some critical value Jc = 2t / U where t denotes hopping amplitude between neighboring sites and U stands for local Coulomb repulsion energy 8-10 . At zero temperature the ground state of the system corresponds then to a ferromagnetically ordered state 11-13 while at finite temperatures one expects a continuous quantum phase transition into a paramagnetic state 14-17 . This transition occurs due to strong spin fluctuations 18-20 and belongs therefore to the same universality class as the Heisenberg chain 21-23 . However there are important differences between both models 24-26 : First, the Kondo necklace contains two types of excitations -spinons and holons 27-29 -while the Heisenberg chain only consists of spin-1/2 particles. Second, the Kondo necklaces contain additional degrees of freedom associated with charge carriers 30-32 . These features lead to new phenomena not observed in the Heisenberg chain 33-36 . For example, the Kondo necklacelike systems may exhibit unconventional superconducting states 37-39 .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Thermodynamic quantum important dynamics of the Kondo necklace model . Abstract : We research thermodynamics and transport properties of the Kondo necklacemodel ( KNM ) in the vicinity of its quantum phase change to an ordered state , which is caused by spin fluctuations .We see that this shift can be described within the framework of Landau - Ginzburg - Wilson principle with a nontrivial scaling dimension of the order parameter field . The latter determines the universality type of the transition as well as the temperature dependence of several physical substances such as certain heat or resistivity .In particular we find that at low temperatures the system displays non - Fermi liquid behavior defined by power - law dependences of these quantities on T . I .INTRODUCTORY REMARkS The Kondo necklace model 1 , 2 describes a chain of magnetic impurities coupled via antiferromagnetic exchange interactions J . It has been proposed originally for describing the physics of large fermion compounds 3 but it also occurs commonly in other contexts 4 - 7 .In the present work we treat the case when the interaction factor J exceeds some essential value Jc = 2t / U where t denotes hopping frequency between neighboring regions and U stands for local Coulomb repulsion energy 8 - 10 . At zero temperature the ground state of the system belongs then to a ferromagnetically ordered state 11 - 13 while at finite temperatures one expects a periodic quantum phase change into a paramagnetic state 14 - 17 .This shift occurs due to powerful spin fluctuations 18 - 20 and belongs hence to the same universality category as the Heisenberg chain 21 - 23 . However there are important differences between both models 24 - 26 : First , the Kondo necklace includes two forms of excitations - spinons and holons 27 - 29 - while the Heisenberg chain only includes of spin - 1 / 2 atoms .Second , the Kondo necklaces feature new degrees of liberty associated with charge carriers 30 - 32 . These features lead to novel observations not observed in the Heisenberg chain 33 - 36 .For instance , the Kondo necklacelike networks could display unconventional superconducting states 37 - 39 .",
        "rewrite_text": "**Title:** Thermodynamic Quantum Dynamics of the Kondo Necklace Model\n\n**Abstract:** In this study, we investigate the thermodynamic and transport characteristics of the Kondo necklace model (KNM) as it approaches a quantum phase transition into an ordered state driven by spin fluctuations. Our analysis reveals that this transition can be effectively described using the Landau-Ginzburg-Wilson framework, which incorporates a nontrivial scaling dimension for the order parameter field. This scaling dimension plays a crucial role in determining the universality class of the transition and influences the temperature dependence of various physical properties, including specific heat and resistivity. Notably, we observe that at low temperatures, the system exhibits non-Fermi liquid behavior, characterized by power-law relationships of these properties with respect to temperature (T).\n\nThe Kondo necklace model, which consists of a chain of magnetic impurities interconnected through antiferromagnetic exchange interactions (J), was initially proposed to elucidate the physics of large fermion compounds. However, it has broader applications across various contexts. In our analysis, we focus on the regime where the interaction strength J surpasses a critical threshold, denoted as Jc = 2t/U, where t represents the hopping frequency between adjacent sites and U signifies the local Coulomb repulsion energy. At absolute zero, the ground state of the system is ferromagnetically ordered, while at finite temperatures, we anticipate a periodic quantum phase transition to a paramagnetic state. This transition is driven by significant spin fluctuations and aligns with the universality class of the Heisenberg chain.\n\nDespite these similarities, the Kondo necklace model presents distinct features. Unlike the Heisenberg chain, which consists solely of spin-1/2 particles, the Kondo necklace incorporates two types of excitations: spinons and holons. Additionally, the Kondo necklace introduces new degrees of freedom associated with charge carriers, leading to unique phenomena not observed in the Heisenberg model. For example, networks resembling the Kondo necklace may exhibit unconventional superconducting states, highlighting the rich and complex behavior of this system.",
        "ori-fast-z-score": -0.5,
        "water-fast-z-score": 6.7863682865394175,
        "rewrite-fast-z-score": 1.2371791482634837
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Frequency Distribution of Semi-major Axis of Wide Binaries. Cosmogony and Dynamical Evolution .\nAbstract:\nWe have investigated the frequency distribution of semi-major axis (SMA) for wide binaries with separations greater than 1000 AU in order to test cosmogonies and dynamical evolution models. We used data obtained by the Two Micron All Sky Survey (2MASS), which is complete down to Ks = 12 mag, corresponding to masses as low as 0.1 M⊙ at distances up to 1 kpc. The sample consists of 13,000 pairs selected using color-color criteria designed to select main-sequence stars. Using Monte Carlo simulations we found that our results are not affected significantly by incompleteness effects due to photometric errors or contamination by background galaxies. Our analysis shows that there exists an excess number of systems with SMA between 10 4 -10 5 AU compared to predictions based on standard cosmological models. This result suggests that either these systems were formed earlier than predicted by current theories or they may be primordial objects such as Population III remnants.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Frequency Distribution of Semi - major Axis of Wide Binaries . Cosmogony and Dynamical Evolution .Abstract : We have analyzed the frequency distribution of semi - major axis ( SMA ) for wide binaries with separations greater than 1000 AU in order to test cosmogonies and dynamical development estimates . We utilized information obtained by the Two Micron All Sky Survey ( 2MASS ) , which is complete down to Ks = 12 mag , equivalent to masses as low as 0 . 1 [UNK] at distances up to 1 kpc .The sample consists of 13 , 000 pairs selected using color - color factors created to select primary - sequence stars . Using Monte Carlo simulations we concluded that our findings are not affected substantially by incompleteness effects due to photometric failures or exposure by background galaxies .Our study shows that there exists an excess amount of components with SMA between 10 4 - 10 5 AU compared to expectations based on normal cosmological models . This result suggests that either these systems were created earlier than expected by current theories or they may be primordial objects such as Population III fragments .",
        "rewrite_text": "**Title:** The Frequency Distribution of Semi-Major Axis of Wide Binaries: Insights into Cosmogony and Dynamical Evolution\n\n**Abstract:** In this study, we investigate the frequency distribution of semi-major axes (SMA) for wide binary systems with separations exceeding 1000 AU, aiming to evaluate existing cosmogonical models and dynamical evolution theories. Our analysis is grounded in data sourced from the Two Micron All Sky Survey (2MASS), which provides a comprehensive catalog down to Ks = 12 mag, capturing stellar masses as low as 0.1 solar masses at distances reaching 1 kpc. The dataset comprises 13,000 binary pairs, meticulously selected using color-color criteria designed to isolate main-sequence stars. To ensure the robustness of our findings against potential biases, we employed Monte Carlo simulations, which indicated that our results remain largely unaffected by issues related to photometric incompleteness or contamination from background galaxies.\n\nOur findings reveal a notable excess of binary components with semi-major axes in the range of 10^4 to 10^5 AU, a result that diverges from predictions derived from conventional cosmological models. This discrepancy raises intriguing questions regarding the formation and evolution of these wide binaries. It suggests that these systems may have formed earlier than current theoretical frameworks anticipate, or they could represent primordial entities, potentially linked to Population III star fragments. This research not only enhances our understanding of binary star formation but also contributes to the broader discourse on the evolution of stellar systems in the context of cosmic history. The implications of our results may prompt a reevaluation of existing models and encourage further investigation into the origins and characteristics of wide binary systems in the universe.",
        "ori-fast-z-score": -0.11547005383792514,
        "water-fast-z-score": 4.965212315030781,
        "rewrite-fast-z-score": 0.09016696346674323
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Free zero-range processes on networks .\nAbstract:\nWe study the dynamics of free fermions hopping between sites of an arbitrary connected graph, with no restriction to nearest-neighbor hopping. We show that this system is equivalent to a collection of independent random walks evolving in parallel and interacting via pairwise collisions at vertices. The collision rate depends only on the number of particles present at each vertex; it vanishes for graphs without loops or multiple edges (e.g., trees), but can be arbitrarily large otherwise. This model exhibits interesting behavior even when all rates are equal, including anomalous diffusion and superdiffusion. In particular, we prove that the mean-square displacement grows as t3/2 for any tree-like graph, while it scales faster than t2/3 for general graphs. Finally, we discuss possible extensions of our results beyond the free-fermion case. Introduction: A wide variety of physical phenomena ranging from quantum transport through mesoscopic systems  1  , to population biology  2  , involve non-equilibrium particle dynamics on networks. These models typically assume that particles move along directed links according to some prescribed rules, such as unrestricted hopping  3  . However, many real-world situations require more complicated interactions among particles  4  .\nIn this work, we consider a simple generalization of standard one-dimensional lattice models  5  by allowing particles to hop freely between adjacent nodes of an arbitrary connected graph G = (V, E). More precisely, let us fix a finite set S of states associated with each node v ∈ V ; then, given a configuration c : V → S, we define the state space C(G) := {c: V → S}. For every edge e = {u, v} ∈ E, we associate two transition probabilities p+(c, c )(e) ≥ 0 and p−(c, c )(u, v) > 0; these represent the probability per unit time that a particle located at u jumps to v if its current state is c, and vice versa. Then, the evolution of the system is described by a continuous-time Markov process Xt taking values in C(G).\nThe main goal of this Letter is to analyze the",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Free zero - range systems on networks . Abstract : We research the dynamics of free fermions hopping between locations of an arbitrary linked graph , with no limitation to nearest - neighbor hopping .We see that this scheme is analogous to a collection of independent random walks evolving in parallel and interacting via pairwise collisions at vertices . The crash time depends only on the number of particles present at each vertex ; it vanishes for graphs without loops or multiple edges ( e . g . , trees ) , but can be arbitrarily small otherwise .This theory exhibits exciting phenomena even when all rates are equal , notably anomalous absorption and superdiffusion . In particular , we prove that the mean - square displacement grows as t3 / 2 for any tree - like graph , while it scales higher than t2 / 3 for general graphs .Finally , we explain possible extensions of our findings beyond the free - fermion case . Introduction : A wide multitude of natural experiments ranging from particle transport through mesoscopic systems 1 , to population physics 2 , use non - equilibrium molecule interactions on networks .These systems often assume that particles moving along directed networks according to some prescribed rules , such as unrestricted hopping 3 . However , many actual - time situations involve more complicated relationships among interactions 4 .In this study , we define a simple generalization of standard one - dimensional lattice models 5 by using particles to hop freely between neighboring vertices of an arbitrary linked graph G = ( V , E ) . More specifically , let us fix a finite collection S of states associated with each node v ∈ V ; then , given a configuration b : V → S , we define the state collection C ( G ) : = { c : V → S } .For every edge e = { u , v } ∈ E , we associate two transition probabilities p + ( c , c ) ( e ) ≥ 0 and p− ( c , c ) ( v , v ) > 0 ; these denote the probability per unit time that a particle situated at u jumps to v if its current state is c , and vice versa . Then , the evolution of the system is characterized by a continuous - time Markov process Xt taking values in C ( G ) .The main goal of this Letter is to analyze the",
        "rewrite_text": "**Title:** Free Zero-Range Systems on Networks\n\n**Abstract:** In this study, we investigate the dynamics of free fermions that hop between nodes on an arbitrary connected graph, allowing for hopping beyond just nearest neighbors. This framework can be likened to a series of independent random walks that progress simultaneously, with interactions occurring through pairwise collisions at the vertices. The time until these collisions, or \"crash time,\" is influenced solely by the number of particles at each vertex. Notably, this crash time is non-existent for graphs that lack loops or multiple edges, such as trees, while it can be exceedingly small in other configurations. Our analysis reveals intriguing phenomena, even under the condition of uniform hopping rates, including anomalous absorption and superdiffusion. Specifically, we demonstrate that for tree-like graphs, the mean-square displacement of the particles increases as \\( t^{3/2} \\), while for more general graphs, it scales higher than \\( t^{2/3} \\). Furthermore, we discuss potential extensions of our results beyond the free-fermion framework, suggesting avenues for future research. \n\n**Introduction:** Numerous natural experiments, ranging from particle transport in mesoscopic systems to studies in population dynamics, involve non-equilibrium interactions of molecules on networks. These systems typically assume that particles traverse directed networks according to specific rules, such as unrestricted hopping. However, real-world scenarios often present more complex interaction dynamics. In this paper, we propose a straightforward generalization of traditional one-dimensional lattice models by allowing particles to move freely between neighboring vertices of an arbitrary connected graph \\( G = (V, E) \\). We define a finite set \\( S \\) of states for each node \\( v \\in V \\) and establish a state collection \\( C(G) = \\{ c : V \\to S \\} \\). For each edge \\( e = \\{ u, v \\} \\in E \\), we assign two transition probabilities, \\( p^+(c, c)(e) \\geq 0 \\) and \\( p^-(c, c)(v, v) > 0 \\), which represent the likelihood of a particle at \\( u \\) jumping to \\( v \\) given its current state \\( c \\), and vice versa. The system's evolution is modeled as a continuous-time Markov process \\( X_t \\) that takes values in \\( C(G) \\). The primary objective of this letter is to delve into the properties and implications of this model.",
        "ori-fast-z-score": -1.5439507063969962,
        "water-fast-z-score": 6.543410136634889,
        "rewrite-fast-z-score": 1.1404288819045583
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Revisiting the effect of nearby supernova remnants on local cosmic rays .\nAbstract:\nWe revisit the question whether or not there is an excess in cosmic ray flux near Supernova Remnant (SNR) shells, as reported by PAMELA and AMS-02 experiments. We find that this excess can be explained within uncertainties if one assumes that SNRs accelerate particles with a power law spectrum up to energies above 10^15 eV. The required spectral index for protons is 2.2 +/- 0.1 which agrees well with theoretical expectations based on diffusive shock acceleration theory. For electrons we require a harder spectrum with a slope of 3.0 +/- 0.3. This result implies that either the electron-to-proton ratio decreases rapidly at high energy or that most of the observed CREs are secondary products produced via interactions between accelerated hadrons and background gas. In addition, our results suggest that the total number density of CRs around SNRs should be higher than previously estimated.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Revisiting the impact of nearby supernova remnants on local cosmic rays . Abstract : We revisit the question whether or not there is an amount in cosmic ray density near Supernova Remnant ( SNR ) layers , as described by PAMELA and AMS - 02 experiments .We see that this excess can be described within uncertainties if one suppose that SNRs move nuclei with a power law spectrum up to energies above 10 ^ 15 eV . The expected spectral index for protons is 2 . 2 + / - 0 . 1 which agrees well with theoretical expectations based on diffusive blast acceleration hypothesis .For particles we require a harder spectrum with a slope of 3 . 0 + / - 0 . 3 . This result suggests that either the electron - to - proton ratio falls gradually at high energy or that most of the seen CREs are secondary derivatives produced via interactions between advanced hadrons and background gas .In addition , our findings show that the total number density of CRs around SNRs should be higher than previously predicted .",
        "rewrite_text": "Title: Revisiting the Impact of Nearby Supernova Remnants on Local Cosmic Rays\n\nAbstract: In this study, we explore the influence of nearby supernova remnants (SNRs) on the density of cosmic rays (CRs) in their vicinity, building on observations from the PAMELA and AMS-02 experiments. Our analysis indicates that the observed excess in cosmic ray density near SNR layers can be reconciled within the bounds of experimental uncertainties by proposing that SNRs accelerate nuclei following a power-law spectrum, extending to energies exceeding 10^15 eV. We derive an expected spectral index for protons of 2.2 ± 0.1, which aligns closely with theoretical predictions grounded in the diffusive shock acceleration mechanism. Conversely, for electrons, we find that a steeper spectrum with a slope of 3.0 ± 0.3 is necessary to account for the data. This discrepancy suggests two potential scenarios: either the electron-to-proton ratio diminishes progressively at higher energies, or a significant portion of the observed cosmic ray electrons (CREs) are secondary particles generated through interactions between accelerated hadrons and the surrounding interstellar medium. Furthermore, our results imply that the total number density of cosmic rays in the vicinity of SNRs is likely greater than previously estimated, highlighting the need for a revised understanding of cosmic ray production and propagation in these dynamic environments. This work contributes to the ongoing discourse on the role of supernova remnants in cosmic ray physics and underscores the importance of further observational and theoretical investigations in this field.",
        "ori-fast-z-score": -1.4142135623730951,
        "water-fast-z-score": 4.798687035041354,
        "rewrite-fast-z-score": -0.7627700713964739
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Is There a Universal Mass Function? .\nAbstract:\nWe present the results of an analysis of the mass function for galaxy clusters in the redshift range 0 < z < 1, using data obtained with the Chandra X-ray Observatory and the Sloan Digital Sky Survey (SDSS). We find that there is no evidence for evolution in the cluster mass function over this interval; we measure the best-fit Schechter parameters to be M* = 2.6 +/- 0.2 x 1014 h-1M_sun and alpha = -1.1 +/- 0.3 at all redshifts. The lack of evolution indicates that the number density of massive clusters has remained constant since z ~ 1.  These results are consistent with previous studies based on optical surveys but differ significantly from those inferred by some recent analyses of X-ray selected samples. This discrepancy may arise because these latter samples include significant numbers of low-mass groups which evolve rapidly between z = 1 and today.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Is There a Universal Mass Function ? .Abstract : We present the results of an assessment of the mass function for galaxy galaxies in the redshift range 0 < z < 1 , using data acquired with the Chandra X - ray Observatory and the Sloan Digital Sky Survey ( SDSS ) . We see that there is no evidence for expansion in the cluster mass function over this interval ; we measure the best - fitting Schechter parameters to be M * = 2 . 6 + / - 0 . 2 x 1014 h - 1M _ sun and alpha = - 1 . 1 + / - 0 . 3 at all redshifts .The absence of evolution suggests that the number density of large clusters has remained constant since z ~ 1 . These conclusions are compatible with previous analyses based on optical sampling but change considerably from those inferred by some latest analyses of X - ray selected samples .This discrepancy may arise because these latter samples include significant populations of lowest - mass groups which evolve faster between z = 1 and today .",
        "rewrite_text": "In this study, we investigate the mass function of galaxy clusters within the redshift range of 0 < z < 1, utilizing data from the Chandra X-ray Observatory and the Sloan Digital Sky Survey (SDSS). Our analysis reveals no significant evidence for the expansion of the cluster mass function during this period. We have determined the best-fitting Schechter parameters, yielding M* = 2.6 ± 0.2 × 10^14 h^-1 M_sun and α = -1.1 ± 0.3 across all redshifts examined. The lack of observed evolution in the mass function indicates that the number density of massive clusters has remained stable since approximately z ~ 1. These findings align with earlier studies that relied on optical data but diverge notably from more recent analyses of X-ray selected samples. This divergence may be attributed to the latter samples incorporating a substantial number of lower-mass groups, which are known to evolve more rapidly between z = 1 and the present day. Our results contribute to the ongoing discourse regarding the universality of the mass function and suggest that while the characteristics of high-mass clusters remain consistent over the studied redshift range, the dynamics of lower-mass groups may introduce complexities that warrant further investigation. This work underscores the importance of considering the selection effects and the mass range of the samples used in cosmological studies of galaxy clusters, as they can significantly influence the interpretation of mass function evolution.",
        "ori-fast-z-score": -1.9896995023342199,
        "water-fast-z-score": 2.6919463855110033,
        "rewrite-fast-z-score": -0.3746343246326776
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Extragalactic Radio Sources and the WMAP Cold Spot .\nAbstract:\nWe present an analysis of the correlation between radio sources in the southern sky with angular scales greater than 1 degree, and the temperature fluctuations observed by Wilkinson Microwave Anisotropy Probe (WMAP). We find that there is no significant correlation at large angular separations for any individual source population or combination thereof. However, we do detect a statistically significant cross-correlation signal when all extragalactic point sources are combined into one sample. The amplitude of this signal is consistent with theoretical predictions based on the Sunyaev-Zel dovich effect. This result suggests that the cold spot may be due to a superposition of many unresolved SZ clusters along our line-of-sight. In addition, we show that the lack of correlation seen individually among different populations can be explained if these populations have differing spectral indices and/or luminosity functions. Finally, we demonstrate how the results presented here could be used as a testbed for future experiments such as Planck Surveyor.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Extragalactic Radio Sources and the WMAP Cold Spot . Abstract : We report an assessment of the relationship between radio sources in the southern sky with angular scales greater than 1 degree , and the temperature fluctuations detected by Wilkinson Microwave Anisotropy Probe ( WMAP ) .We see that there is no major interaction at large angular separations for any individual source population or combination thereof . However , we do discover a statistically substantial cross - correlation signal when all extragalactic point sources are united into one sample .The amplitude of this signal is compatible with theoretical estimates based on the Sunyaev - Zel dovich phenomenon . This result suggests that the cool spot may be due to a superposition of several unresolved SZ clusters along our line - of - view .In addition , we prove that the lack of correlation seen individually among different populations can be described if these populations have differing spectral indices and / or luminosity functions . Finally , we prove how the results presented here possible be used as a testbed for future research such as Planck Surveyor .",
        "rewrite_text": "We present an analysis of the correlation between extragalactic radio sources located in the southern hemisphere and the temperature fluctuations observed by the Wilkinson Microwave Anisotropy Probe (WMAP), specifically focusing on angular scales exceeding 1 degree. Our investigation reveals that there is no significant interaction at large angular separations when examining individual populations of radio sources or any combinations thereof. However, a noteworthy finding emerges when we aggregate all extragalactic point sources into a single sample, where we observe a statistically significant cross-correlation signal. The amplitude of this signal aligns well with theoretical predictions derived from the Sunyaev-Zel'dovich (SZ) effect, indicating that the observed cold spot may be attributed to the cumulative effect of multiple unresolved SZ clusters along our line of sight. Furthermore, we demonstrate that the absence of correlation among different radio source populations can be explained by variations in their spectral indices and/or luminosity functions. This research not only sheds light on the relationship between extragalactic radio sources and cosmic microwave background fluctuations but also serves as a foundation for future studies, including those utilizing data from the Planck Surveyor mission. Our findings contribute to a deeper understanding of the cosmic structures influencing the WMAP cold spot and highlight the potential for further exploration in this domain.",
        "ori-fast-z-score": 1.1322770341445956,
        "water-fast-z-score": 6.039910552360811,
        "rewrite-fast-z-score": 0.8955334711889903
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Families of holomorphic bundles .\nAbstract:\nIn this article, we study families of holomorphic vector bundles on complex algebraic varieties. We prove that the set of isomorphism classes of such families is naturally an affine scheme over the base variety and give explicit equations for it in terms of Chern classes.  In particular, if the base variety has dimension one then these schemes are reduced to points corresponding to the rank and degree of each bundle in the family. \nWe also show how our results can be used to construct moduli spaces of stable vector bundles with fixed determinant. The main result of this article was announced by J. P. Serre at the conference  Algebraic geometry and number theory  held in Paris in June 2005 (see  Ser  ). Families of holomorphic vector bundles have been studied extensively since the work of Grothendieck  Gro1  . They play important roles both in algebraic geometry and mathematical physics; see e.g.,  Bri1,  Bri2,   Bri3,   ...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Families of holomorphic bundles . Abstract : In this article , we study families of holomorphic vector bundles on complex algebraic varieties .We prove that the group of isomorphism classes of such families is naturally an affine scheme over the base variety and get explicit equations for it in terms of Chern classes . In particular , if the base variety has size one then these schemes are increased to points corresponding to the rank and degree of each bundle in the class .We addition show how our findings can be used to build moduli spaces of stable vector bundles with constant determinant . The main result of this page was announced by J . P . Serre at the meeting Algebraic geometry and number theory conducted in Paris in June 2005 ( see Ser ) .Families of holomorphic vector bundles have been studied frequently since the work of Grothendieck Gro1 . They play essential roles both in mathematical mathematics and mathematical science ; hear e . g . , Bri1 , Bri2 , Bri3 , . . .",
        "rewrite_text": "In this article, we investigate families of holomorphic vector bundles defined on complex algebraic varieties. Our primary contribution is the demonstration that the set of isomorphism classes of these families can be naturally structured as an affine scheme over the base variety. We derive explicit equations for this scheme in terms of Chern classes, providing a deeper understanding of the geometric properties involved. Notably, when the base variety consists of a single point, these schemes correspond to specific points that reflect the rank and degree of each vector bundle within the class. Furthermore, we explore the implications of our results for constructing moduli spaces of stable vector bundles that maintain a constant determinant. The foundational aspects of this research were initially presented by J. P. Serre during the Algebraic Geometry and Number Theory conference held in Paris in June 2005. The study of families of holomorphic vector bundles has a rich history, significantly influenced by the pioneering work of Grothendieck. These bundles are crucial in various domains of mathematics, including both pure and applied fields, as highlighted in the works of Briand and others. Our findings not only advance the theoretical framework surrounding holomorphic vector bundles but also open avenues for further exploration in the context of moduli theory and algebraic geometry.",
        "ori-fast-z-score": -1.649915822768611,
        "water-fast-z-score": 3.117691453623979,
        "rewrite-fast-z-score": 1.6502739940140694
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The dynamics of Jupiter and Saturn in the gaseous proto-planetary disk .\nAbstract:\nWe present results on the orbital evolution of Jupiter and Saturn in an axisymmetric, viscously evolving protoplanetary disk with embedded planets. We find that the orbits of both giant planets are significantly affected by their mutual gravitational interaction as well as by the presence of other planetary embryos. The eccentricity growth is dominated by secular interactions between the two planets which lead to large amplitude oscillations in the semi-major axes. In addition we find that the planet migration rates depend strongly on the initial conditions for the system parameters such as mass ratio and separation distance. \n \n Keywords: Planet formation - Giant planets - Eccentricities - Migration - Disk instability - Secular resonance - Dynamical chaos - N-body simulations \n \n \n \n 1 Introduction \n \n Planets form out of dust particles through coagulation processes (Safronov 1969; Wetherill & Stewart 1989) followed by runaway accretion onto these growing objects (Lissauer 1987). This process leads to the formation of planetesimals whose masses range from 10$^{−6}$ M⊕ up to several Earth masses. These bodies can grow further into larger planetary embryos or even directly into gas giants like Jupiter and Saturn if they accrete enough material within a short time span (Pollack et al. 1996) . Once formed, these massive planets open gaps in the surrounding circumstellar disks due to tidal torques exerted by the planet s gravity (Lin & Papaloizou 1986 ). As a consequence, the remaining matter inside this gap will be removed rapidly by viscosity effects leading to rapid inward type II migration of the planet (Ward 1997; Tanaka et al. 2002 ) . \nThe observed distribution of exoplanets shows a wide variety of orbital configurations ranging from circular orbits around Sun-like stars to highly eccentric orbits around low-mass stars (see e.g., Marcy et al. (2005) , Udry & Santos 2007 , Winn et al. (2010 ), Johnson et al. (2011 ) and references therein). However, most of them have been found close to their host star where the detection probability increases dramatically because of the strong stellar",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The behavior of Jupiter and Saturn in the gaseous proto - planetary disk . Abstract : We report findings on the orbital evolution of Jupiter and Saturn in an axisymmetric , viscously changing protoplanetary disk with attached planets .We see that the orbits of both giant planets are greatly impacted by their mutual gravitational interaction as also as by the presence of other planetary embryos . The eccentricity growth is dominated by secular interactions between the two planets which cause to large frequency oscillations in the semi - major axes .In addition we find that the planet migration rates depend greatly on the early conditions for the system parameters such as mass ratio and separation distance . Keywords : Planet structure - Giant planets - Eccentricities - Migration - Disk instability - Secular resonance - Dynamical chaos - N - bodies simulations 1 Introduction Planets form out of dust particles through coagulation processes ( Safronov 1969 ; Wetherill & Stewart 1989 ) preceded by runaway accretion onto these growing objects ( Lissauer 1987 ) .This process results to the formation of planetesimals whose masses range from 10 $ ^ { −6 } $ M⊕ up to several Earth masses . These bodies can develop further into larger planetary embryos or even directly into gas giants like Jupiter and Saturn if they accrete adequate material within a brief time frame ( Pollack et al .1996 ) . Once assembled , these massive planets open gaps in the nearby circumstellar disks owing to tidal torques exerted by the planet s gravity ( Lin & Papaloizou 1986 ) .As a consequence , the remaining material inside this gap will be removed soon by viscosity factors resulting to rapid inward type II displacement of the planet ( Ward 1997 ; Tanaka et al . 2002 ) .The observed distribution of exoplanets shows a broad variety of orbital arrangements ranging from circular orbits around Sun - like stars to strongly eccentric orbits around low - mass stars ( see e . g . , Marcy et al . ( 2005 ) , Udry & Santos 2007 , Winn et al .( 2010 ) , Johnson et al . ( 2011 ) and references therein ) .However , most of them have been seen nearer to their host star where the detection odds grows dramatically because of the strong stellar",
        "rewrite_text": "**Title:** The Dynamics of Jupiter and Saturn within the Gaseous Protoplanetary Disk\n\n**Abstract:** This study investigates the orbital dynamics of Jupiter and Saturn as they evolve within an axisymmetric, viscously evolving protoplanetary disk that includes the presence of these gas giants. Our findings reveal that the gravitational interactions between the two planets significantly influence their orbital characteristics, particularly in conjunction with the presence of other planetary embryos in the disk. Notably, we observe that the growth of orbital eccentricity is primarily driven by secular interactions between Jupiter and Saturn, which induce substantial oscillations in their semi-major axes. Furthermore, our analysis indicates that the rates of planetary migration are highly sensitive to the initial conditions of the system, including parameters such as mass ratios and the distances separating the planets. \n\nThe formation of planets from dust particles occurs through a series of coagulation processes, as outlined by Safronov (1969) and Wetherill & Stewart (1989), followed by a phase of runaway accretion onto these nascent bodies (Lissauer, 1987). This leads to the creation of planetesimals with masses ranging from 10$^{-6}$ M⊕ to several Earth masses, which can evolve into larger planetary embryos or even gas giants like Jupiter and Saturn if they successfully accrete sufficient material within a short timeframe (Pollack et al., 1996). Once these massive planets are formed, they exert tidal torques that open gaps in the surrounding circumstellar disk (Lin & Papaloizou, 1986). The material within these gaps is subsequently removed due to viscous processes, resulting in a rapid type II migration of the planets (Ward, 1997; Tanaka et al., 2002). \n\nThe diversity in the observed distribution of exoplanets highlights a wide range of orbital configurations, from nearly circular orbits around Sun-like stars to highly eccentric orbits around lower-mass stars (Marcy et al., 2005; Udry & Santos, 2007; Winn et al., 2010; Johnson et al., 2011). However, many of these exoplanets are found in close proximity to their host stars, where detection probabilities are significantly enhanced due to the strong stellar influence. \n\n**Keywords:** Planetary structure, Giant planets, Eccentricity, Migration, Disk instability, Secular resonance, Dynamical chaos, N-body simulations.",
        "ori-fast-z-score": -1.5461980716652028,
        "water-fast-z-score": 4.834937784152282,
        "rewrite-fast-z-score": -1.7439550769285392
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Ground-Based Search for Thermal Emission from the Exoplanet TrES-1 .\nAbstract:\nWe report on an attempt to detect thermal emission from the planet TrES-1 using data obtained with the Spitzer Space Telescope s Infrared Array Camera (IRAC). We find no evidence that this planet is emitting significant amounts of infrared radiation at wavelengths longer than 3 microns, and we place upper limits on its temperature between 1000 K and 1500 K depending upon assumptions about the albedo.  These results are consistent with previous estimates based on optical photometry alone. The lack of detectable infrared flux suggests either that the planet has little or no atmosphere, or else it has a very low surface pressure such as would be expected if the planet were tidally locked into synchronous rotation. This work was supported by NASA under grant NAG5-12942 issued through JPL/Caltech. We thank J. Harrington for providing us with his model predictions prior to publication. We also acknowledge helpful discussions with A. Burrows, D. Charbonneau, M. Marley, R. Seager, S. Sozzetti, T. Swain, C. Traub, B. Zuckerman, and W. Wilson.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Ground - Based Search for Thermal Emission from the Exoplanet TrES - 1 . Abstract : We report on an attempt to identify thermal emitted from the planet TrES - 1 using data acquired with the Spitzer Space Telescope s Infrared Array Camera ( IRAC ) .We see no evidence that this planet is emitting major amounts of infrared rays at wavelengths greater than 3 microns , and we put upper limits on its temperature between 1000 K and 1500 K depending upon predictions about the albedo . These conclusions are compatible with previous accounts based on infrared photometry alone .The absence of detectable infrared flux implies either that the planet has little or no environment , or else it has a very low surface pressure such as would be anticipated if the planet were tidally locked into synchronous rotation . This project was supported by NASA under grant NAG5 - 12942 issued through JPL / Caltech .We praise J . Harrington for providing us with his model results prior to publication . We addition acknowledge helpful talks with A . Burrows , D . Charbonneau , M . Marley , R . Seager , S . Sozzetti , T . Swain , C . Traub , B . Zuckerman , and W . Wilson .",
        "rewrite_text": "In this study, we present our findings from a ground-based investigation aimed at detecting thermal emissions from the exoplanet TrES-1, utilizing data obtained from the Spitzer Space Telescope's Infrared Array Camera (IRAC). Our analysis reveals no significant evidence of infrared radiation being emitted by TrES-1 at wavelengths exceeding 3 microns. Consequently, we establish upper temperature limits for the planet, ranging from 1000 K to 1500 K, which are contingent upon various albedo predictions. These results align with earlier studies that relied solely on infrared photometry. The lack of observable infrared flux suggests that TrES-1 may possess a minimal or nonexistent atmosphere, or alternatively, it could exhibit a very low surface pressure, a scenario that is likely if the planet is tidally locked in synchronous rotation with its host star. This research was conducted with the support of NASA under grant NAG5-12942, awarded through the Jet Propulsion Laboratory (JPL) at Caltech. We extend our gratitude to J. Harrington for sharing his model results with us prior to their publication. Additionally, we acknowledge the valuable discussions we had with several experts in the field, including A. Burrows, D. Charbonneau, M. Marley, R. Seager, S. Sozzetti, T. Swain, C. Traub, B. Zuckerman, and W. Wilson, which greatly contributed to the development of this project.",
        "ori-fast-z-score": -1.697749375254331,
        "water-fast-z-score": 4.45427656417346,
        "rewrite-fast-z-score": -0.6708203932499369
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The subpulse modulation properties of pulsars at 92 cm and the frequency dependence of subpulse modulation .\nAbstract:\nWe have studied the pulse-to-pulse intensity variations in two pulsars, PSR B0826-34 and PSR B1133+16, using data obtained with the Parkes radio telescope at 1.4 GHz (92cm) and 2.7GHz(36cm). We find that both pulsars show strong evidence for subpulse drift bands which are modulated by an underlying pattern of weaker emission. The observed patterns can be explained as due to interference between different modes of emission within each beam. In addition we find that there is no significant difference in the subpulse modulation properties when comparing observations made at 36cm and those made at 92cm. This suggests that the physical processes responsible for producing these phenomena operate over a wide range of frequencies. These results provide further support for the idea that the phenomenon known as drifting subpulses may arise through some form of plasma instability operating on the open field lines near the magnetic poles of neutron stars. \n \n Keywords: Pulsar",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The subpulse modulation properties of pulsars at 92 cm and the frequency dependence of subpulse modulation . Abstract : We have researched the pulse - to - pulse intensity variations in two pulsars , PSR B0826 - 34 and PSR B1133 + 16 , using data acquired with the Parkes radio telescope at 1 . 4 GHz ( 92cm ) and 2 . 7GHz ( 36cm ) .We see that both pulsars show good evidence for subpulse drift bands which are modulated by an underlying pattern of weaker emission . The observed patterns can be understood as owing to interference between various modes of emission within each light .In addition we find that there is no major variation in the subpulse modulation properties when comparing measurements made at 36cm and those conducted at 92cm . This implies that the physical processes responsible for producing these phenomena act over a broad variety of frequencies .These data provide further evidence for the idea that the process known as drifting subpulses might arise through some kind of plasma instability working on the open field lines near the magnetic poles of neutron stars . Keywords : Pulsar",
        "rewrite_text": "Title: The Subpulse Modulation Characteristics of Pulsars at 92 cm and the Frequency Dependence of Subpulse Modulation\n\nAbstract: This study investigates the variations in pulse intensity from two pulsars, PSR B0826-34 and PSR B1133+16, utilizing observational data obtained from the Parkes radio telescope at frequencies of 1.4 GHz (92 cm) and 2.7 GHz (36 cm). Our analysis reveals compelling evidence for the presence of subpulse drift bands in both pulsars, which are influenced by an underlying pattern of diminished emission. The observed modulation patterns can be interpreted as a result of interference among different emission modes within each pulse. Furthermore, our findings indicate that there is no significant difference in the subpulse modulation characteristics when comparing the data collected at 36 cm with that at 92 cm. This consistency suggests that the mechanisms driving these phenomena are effective across a wide range of frequencies. The results bolster the hypothesis that the drifting subpulses may be linked to plasma instabilities occurring along the open field lines in the vicinity of neutron star magnetic poles. This research contributes to the broader understanding of pulsar behavior and the physical processes at play in their emission mechanisms. \n\nKeywords: Pulsar, subpulse modulation, plasma instability, neutron stars, radio astronomy.",
        "ori-fast-z-score": -0.1125087900926024,
        "water-fast-z-score": 5.366563145999495,
        "rewrite-fast-z-score": 0.5423261445466404
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Unified Approach to Energy-Efficient Power Control in Large CDMA Systems .\nAbstract:\nIn this thesis, we propose an energy-efficient power control scheme for large code division multiple access (CDMA) systems with variable traffic loads and channel conditions. The proposed approach is based on the concept that all users should be allocated their required data rates at minimum total transmit power consumption while maintaining acceptable quality-of-service (QoS). We first develop a new analytical model which can accurately predict the average received signal-to-interference-plus-noise ratio (SINR) under different system configurations. Based on our analysis results, we then formulate the problem as a convex optimization problem subject to SINR constraints. Finally, by applying Lagrange multiplier method, we obtain closed-form solutions for both uplink and downlink transmissions. Our simulation results show that compared with conventional schemes such as water-filling algorithm, the proposed approach achieves significant performance gains in terms of power efficiency without sacrificing QoS requirements. In addition, it also outperforms other existing approaches in terms of computational complexity. \n \n Keywords: Code Division Multiple Access, Power Control",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Unified Approach to Energy - Efficient Power Control in Large CDMA Systems . Abstract : In this dissertation , we propose an energy - efficient energy management scheme for large code division multiple entry ( CDMA ) networks with variable traffic loads and channel conditions .The proposed approach is based on the idea that all users should be allocated their required information rates at minimum total transmit energy consumption while maintaining acceptable quality - of - service ( QoS ) . We first develop a new analytical method which can accurately forecast the average received signal - to - interference - plus - noise proportion ( SINR ) under various system configurations .Based on our analysis results , we then formulate the issue as a convex optimization problem subject to SINR restrictions . Finally , by using Lagrange multiplier method , we obtain closed - form solutions for both uplink and downlink transmissions .Our simulation data demonstrate that compared with typical strategies such as water - filling algorithm , the suggested approach achieves significant efficiency increases in terms of power performance without sacrificing QoS requirements . In addition , it also outperforms other existing techniques in terms of computational complexity .Keywords: Code Division Multiple Access, Power Control",
        "rewrite_text": "Title: A Unified Approach to Energy-Efficient Power Control in Large CDMA Systems\n\nAbstract: This dissertation presents a novel energy management strategy tailored for large Code Division Multiple Access (CDMA) networks that experience varying traffic loads and fluctuating channel conditions. The core premise of our approach is to ensure that all users receive their requisite information rates while minimizing the total energy consumed during transmission, all the while upholding acceptable quality-of-service (QoS) standards. To achieve this, we first introduce a new analytical framework capable of accurately predicting the average signal-to-interference-plus-noise ratio (SINR) across diverse system configurations. Building on our analytical findings, we reformulate the power control challenge as a convex optimization problem, constrained by SINR requirements. Utilizing the Lagrange multiplier method, we derive closed-form solutions applicable to both uplink and downlink transmission scenarios. Our simulation results reveal that our proposed method significantly enhances power efficiency compared to conventional strategies, such as the water-filling algorithm, without compromising QoS. Furthermore, our approach demonstrates superior performance in terms of computational complexity when compared to existing methodologies. This work not only contributes to the field of energy-efficient communication systems but also provides practical insights for the implementation of power control mechanisms in large-scale CDMA networks. \n\nKeywords: Code Division Multiple Access, Power Control",
        "ori-fast-z-score": 0.8,
        "water-fast-z-score": 5.742855749264711,
        "rewrite-fast-z-score": 1.4501047335684953
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  What Gamma-Ray Bursts Explode Into .\nAbstract:\nGamma-ray bursts (GRBs) are the most energetic explosions in the universe, but their origin is still unknown. The leading model for GRB production involves two compact objects merging into one black hole and then exploding as a result of rapid accretion onto this black hole.  In this talk I will discuss how we can use gravitational waves to test this hypothesis by looking at the ringdown phase of these mergers. This is an exciting time for gravitational wave astronomy with Advanced LIGO/VIRGO starting taking data soon! \nI will also present some recent results on using gravitational waves to study neutron star mergers that may be related to short gamma ray bursts. Finally, I ll give you my personal perspective on what it takes to become a successful scientist today. My research interests include gravitational waves, astrophysics, cosmology, and particle physics. I m currently working at MIT Kavli Institute for Astrophysics and Space Research where I am part of the Gravitational Wave Cosmology Project.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : What Gamma - Ray Bursts Explode Into . Abstract : Gamma - ray bursts ( GRBs ) are the most intense bombs in the universe , but their source is still unclear .The leading theory for GRB development involves two compact entities combining into one dark hole and then exploding as a result of rapid accretion onto this black hole . In this talk I will explore how we can using gravitational waves to test this hypothesis by searching at the ringdown phase of these mergers .This is an exciting day for gravitational wave astronomy with Advanced LIGO / VIRGO starting took data soon ! I will also report some latest findings on uses gravity waves to study neutron star mergers that might be connected to short gamma ray waves .Finally , I ll offer you my personal perspective on what it takes to become a successful researcher today . My research interests cover gravitational waves , astrophysics , cosmology , and particle science .I m currently working at MIT Kavli Institute for Astrophysics and Space Research where I am member of the Gravitational Wave Cosmology Project .",
        "rewrite_text": "Title: What Gamma-Ray Bursts Explode Into\n\nAbstract: Gamma-ray bursts (GRBs) represent the most powerful explosions observed in the universe, yet their origins remain enigmatic. The predominant theory posits that GRBs arise from the merger of two compact objects, which subsequently form a black hole that detonates due to rapid accretion processes. In this presentation, I will delve into the potential of utilizing gravitational waves to validate this theory, particularly by examining the ringdown phase that follows these cosmic mergers. The advent of Advanced LIGO and VIRGO, which are now actively collecting data, marks a significant milestone in the field of gravitational wave astronomy. Additionally, I will share recent findings that leverage gravitational waves to investigate neutron star mergers, which may be linked to the phenomenon of short gamma-ray bursts. Furthermore, I will provide insights from my personal journey in research, discussing the essential qualities and skills necessary for success in today’s scientific landscape. My research encompasses a broad spectrum of topics, including gravitational waves, astrophysics, cosmology, and particle physics. Currently, I am affiliated with the MIT Kavli Institute for Astrophysics and Space Research, where I contribute to the Gravitational Wave Cosmology Project. This work not only enhances our understanding of GRBs but also advances the broader field of astrophysics through the integration of observational data and theoretical models.",
        "ori-fast-z-score": -0.5555555555555556,
        "water-fast-z-score": 5.158920219515446,
        "rewrite-fast-z-score": 0.4082482904638631
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Radio Emission, X-ray Emission, and Hydrodynamics of G328.4+0.2: A Comprehensive Analysis of a Luminous Pulsar Wind Nebula, its Neutron Star, and the Progenitor Supernova Explosion .\nAbstract:\nWe present an analysis of multi-wavelength observations of the pulsar wind nebula (PWN) associated with PSR B1509-58 in the supernova remnant (SNR) G328.4+0. \n2. The radio emission is modeled as synchrotron radiation produced by relativistic electrons accelerated at the termination shock between the pulsar s magnetosphere and the surrounding medium. \n \n We find that the observed properties of this system are consistent with those expected for a young energetic pulsar surrounded by a dense shell of swept-up material. In particular, we show that: \n \n \n \n 1. The total energy contained within the SNR is ~1050 erg, which implies a kinetic energy of ~500 erg for the progenitor star prior to explosion; \n \n 2. The age of the pulsar is estimated to be ~20 kyr based on the spin-down luminosity and characteristic age; \n \n 3. The distance to the source is constrained to be <5 kpc using the dispersion measure and assuming a nominal value for the electron density along the line-of-sight; \n \n 4. The magnetic field strength near the pulsar is inferred to be ~1 mGauss based on modeling of the spectral index distribution across the face of the PWN; \n \n 5. The radius of the PWN is found to be ~0.3 pc, corresponding to a dynamical age of ~30 yrs; \n \n 6. The mass loss rate of the progenitor star was >10-5 Msun/yr during the last few thousand years before core collapse; \n \n 7. The initial mass of the progenitor star was ~25-30 Msuns, implying a red supergiant or blue hypergiant classification; \n \n 8. The ejecta mass of the progenitor star is estimated to be ~7-8 Msuns, indicating that it underwent significant mass loss prior to exploding; \n \n 9. The expansion velocity of the outer edge of the PWN is ~1000 km/sec, comparable to the speed of sound in the shocked gas; \n \n 10. The X-ray",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Radio Emission , X - ray Emission , and Hydrodynamics of G328 . 4 + 0 . 2 : A Comprehensive Analysis of a Luminous Pulsar Wind Nebula , its Neutron Star , and the Progenitor Supernova Explosion . Abstract : We report an assessment of multi - wavelength images of the pulsar wind nebula ( PWN ) associated with PSR B1509 - 58 in the supernova remnant ( SNR ) G328 . 4 + 0 .2 . The radio emission is modeled as synchrotron emission created by relativistic electrons accelerated at the termination shock between the pulsar s magnetosphere and the nearby medium .We see that the known characteristics of this system are compatible with those expected for a young energetic pulsar surrounded by a dense shell of washed - up material . In particular , we prove that : 1 .The total energy contained within the SNR is ~ 1050 erg , which implies a kinetic power of ~ 500 erg for the progenitor star previous to explosion ; 2 . The age of the pulsar is predicted to be ~ 20 kyr based on the spin - down luminosity and typical age ; 3 .The distance to the origin is constrained to be < 5 kpc using the dispersion measure and assuming a nominal value for the electron concentration along the line - of - view ; 4 . The magnetic force power near the pulsar is inferred to be ~ 1 mGauss based on modeling of the spectral index distribution across the face of the PWN ; 5 .The radius of the PWN is found to be ~ 0 . 3 pc , equivalent to a dynamical age of ~ 30 yrs ; 6 . The mass loss rate of the progenitor star was > 10 - 5 Msun / yr during the last few thousand years before core breakup ; 7 .The initial mass of the progenitor star was ~ 25 - 30 Msuns , indicate a red supergiant or blue hypergiant classification ; 8 . The ejecta mass of the progenitor star is predicted to be ~ 7 - 8 Msuns , showing that it underwent considerable mass loss prior to exploding ; 9 .The expansion velocity of the exterior boundary of the PWN is ~ 1000 kilometers / sec , comparable to the speed of noise in the excited gas ; 10 . The X - ray",
        "rewrite_text": "**Title:** The Radio Emission, X-ray Emission, and Hydrodynamics of G328.4+0.2: A Comprehensive Analysis of a Luminous Pulsar Wind Nebula, its Neutron Star, and the Progenitor Supernova Explosion\n\n**Abstract:** This study presents a detailed examination of multi-wavelength observations of the pulsar wind nebula (PWN) linked to PSR B1509-58 within the supernova remnant (SNR) G328.4+0.2. Our analysis focuses on the radio emission, which we interpret as synchrotron radiation produced by relativistic electrons that are accelerated at the termination shock between the pulsar's magnetosphere and the surrounding medium. The findings align with the expected properties of a young, energetic pulsar enveloped by a dense shell of material expelled during the supernova event. Specifically, we demonstrate that: (1) the total energy within the SNR is approximately 10^50 erg, suggesting a kinetic power of around 500 erg for the progenitor star prior to its explosion; (2) the pulsar's age is estimated to be about 20,000 years, derived from its spin-down luminosity and typical aging models; (3) the distance to the source is constrained to less than 5 kpc, based on dispersion measures and assumed electron density values along the line of sight; (4) the magnetic field strength near the pulsar is estimated to be around 1 mGauss, inferred from the spectral index distribution across the PWN; (5) the PWN's radius is approximately 0.3 parsecs, corresponding to a dynamical age of roughly 30 years; (6) the progenitor star's mass loss rate was greater than 10^-5 M_sun/year in the final millennia before core collapse; (7) the initial mass of the progenitor star is estimated to be between 25 and 30 M_sun, indicating it was likely a red supergiant or blue hypergiant; (8) the ejecta mass from the progenitor star is predicted to be around 7 to 8 M_sun, reflecting significant mass loss prior to the explosion; (9) the expansion velocity of the outer boundary of the PWN is approximately 1000 kilometers per second, comparable to the sound speed in the excited gas; and (10) the X-ray emission characteristics further elucidate the dynamics and evolution of this complex system. This comprehensive analysis enhances our understanding of the interplay between pulsar wind nebulae, their progenitor stars, and the remnants of supernova explosions.",
        "ori-fast-z-score": 1.104689541477988,
        "water-fast-z-score": 7.058578427117228,
        "rewrite-fast-z-score": 0.25630729731502827
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Redefining the Missing Satellites Problem .\nAbstract:\nThe missing satellites problem (MSP) is one of the most important problems in space science and technology, with applications ranging from satellite navigation to space debris removal.  The MSP asks for all orbits that are stable under gravitational perturbations by known bodies such as planets or asteroids.   In this work we present an algorithm which solves the MSP exactly on any number of dimensions d >= 2 using only O(n log n + m log n) time where n = |S| is the total number of objects in S and m = |E| is the number of edges in E.   Our approach uses a novel combination of techniques including fast matrix multiplication algorithms, data structures based on interval trees, and efficient graph traversal methods. We also show how our results can be used to solve related problems like finding the minimum distance between two given sets of points in R^d. Finally, we demonstrate the practicality of our method through experiments performed on real-world datasets.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Redefining the Missing Satellites Problem . Abstract : The missing satellites question ( MSP ) is one of the most important problems in space research and technology , with applications diverse from satellite communication to space trash destruction .The MSP seeks for all orbits that are stable under gravity perturbations by known objects such as planets or asteroids . In this project we present an algorithm which solves the MSP exactly on any number of dimensions d > = 2 using only O ( n log n + m log n ) time where n = | S | is the total number of items in S and m = | E | is the number of vertices in E . Our solution uses a novel combination of techniques including rapid matrix multiplication methods , data structures based on interval trees , and fast graph traversal methods .We also demonstrate how our findings can be used to solve related problems like finding the minimum distance between two given sets of points in R ^ d . Finally , we prove the practicality of our technique through experiments conducted on real - time datasets .",
        "rewrite_text": "**Title: Redefining the Missing Satellites Problem**\n\n**Abstract:** The Missing Satellites Problem (MSP) represents a significant challenge in the field of space research and technology, with implications that extend to various applications, including satellite communication and the mitigation of space debris. The core of the MSP involves identifying all stable orbits that remain unaffected by gravitational perturbations from known celestial bodies, such as planets and asteroids. In this study, we introduce a novel algorithm capable of precisely addressing the MSP in any dimensional space where d ≥ 2. Our algorithm operates with a time complexity of O(n log n + m log n), where n denotes the total number of items in the set S and m represents the number of vertices in the set E. \n\nThe approach we propose integrates an innovative blend of methodologies, including advanced rapid matrix multiplication techniques, interval tree-based data structures, and efficient graph traversal algorithms. Furthermore, we illustrate the versatility of our findings by applying them to related computational problems, such as determining the minimum distance between two distinct sets of points in R^d. \n\nTo validate the effectiveness and practicality of our proposed technique, we conducted a series of experiments utilizing real-time datasets. The results demonstrate not only the accuracy of our algorithm in solving the MSP but also its potential applicability in broader contexts within space research. This work contributes to a deeper understanding of orbital dynamics and offers a robust framework for future investigations into satellite stability and related phenomena in multidimensional spaces.",
        "ori-fast-z-score": -1.8439088914585775,
        "water-fast-z-score": 3.362422096189171,
        "rewrite-fast-z-score": -1.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Grand minima and maxima of solar activity: New observational constraints .\nAbstract:\nWe present new results on the long-term evolution of solar magnetic fields, obtained by applying an advanced data analysis technique to observations made with the Wilcox Solar Observatory (WSO) magnetograph between 1976 and 2009. The method is based on wavelet transforms in combination with principal component analysis (PCA). It allows us to separate different types of variability into their individual components at each point in time. We find that there are two distinct modes of solar magnetic field evolution over this period. One mode shows strong fluctuations around a mean value which varies slowly but significantly during the cycle 23/24 minimum. This behaviour can be explained as being due to the presence of large-scale dynamo waves driven by differential rotation. In addition we identify another type of variation which appears to have no preferred frequency or spatial scale. These variations show significant correlations with sunspot number and other proxies for solar activity. They may therefore represent some form of global response of the Sun s magnetic field to changes in its internal dynamics.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Grand minima and maxima of solar activity : New observational restrictions . Abstract : We report new data on the long - term expansion of solar magnetic fields , obtained by using an sophisticated data analysis technique to observations made with the Wilcox Solar Observatory ( WSO ) magnetograph between 1976 and 2009 .The method is based on wavelet transforms in combination with principal component analysis ( PCA ) . It enables us to separate distinct types of variability into their individual parts at each point in time .We see that there are two different modes of solar magnetic force evolution over this time . One mode displays strong fluctuations around a mean value which varies steadily but significantly during the period 23 / 24 minimum .This behaviour can be understood as being owing to the presence of large - scale dynamo waves powered by differential rotation . In addition we identify another type of variation which appears to have no chosen amplitude or spatial scale .These changes demonstrate considerable correlations with sunspot number and other proxies for solar activity . They might hence indicate some kind of worldwide response of the Sun s magnetic force to changes in its internal behavior .",
        "rewrite_text": "**Title:** Grand Minima and Maxima of Solar Activity: New Observational Constraints\n\n**Abstract:** In this study, we present novel findings regarding the long-term evolution of solar magnetic fields, derived from an advanced data analysis technique applied to observations collected by the Wilcox Solar Observatory (WSO) magnetograph from 1976 to 2009. Our analytical approach employs wavelet transforms in conjunction with principal component analysis (PCA), allowing us to disentangle various types of variability at each temporal point. Our results reveal two distinct modes of solar magnetic force evolution throughout this period. The first mode exhibits pronounced fluctuations around a mean value that experiences a steady yet significant variation during the solar minimum between cycles 23 and 24. This behavior can be attributed to the influence of large-scale dynamo waves, which are driven by differential rotation within the Sun. The second mode of variation, however, appears to lack a defined amplitude or spatial scale, suggesting a more chaotic nature. Notably, these variations show strong correlations with sunspot numbers and other indicators of solar activity, implying a potential global response of the Sun's magnetic field to alterations in its internal dynamics. Our findings provide critical observational constraints that enhance the understanding of solar activity cycles, particularly in relation to grand minima and maxima, and underscore the complex interplay between solar magnetic fields and internal solar processes. This research contributes to the broader field of solar physics by elucidating the mechanisms underlying solar magnetic variability and its implications for solar-terrestrial interactions.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 6.599750073368524,
        "rewrite-fast-z-score": 0.44367825470805694
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Information-Based Asset Pricing .\nAbstract:\nThe information-based asset pricing model is an extension to the traditional CAPM (Capital Assets Pricing Model) that incorporates additional factors into its analysis.  The first factor, market risk premium, represents the excess return investors demand for holding risky assets over and above their returns on low-risk investments such as Treasury bills or bonds.   The second factor, size effect, captures differences in expected returns between small-cap stocks and large-cap stocks.   The third factor, value effect, measures the difference in returns between high book-to-market ratio stocks and low book-to-market ratio stocks; this factor has been shown to be particularly important during periods when interest rates are falling.    This article provides a brief overview of these three factors along with some examples of how they can affect investment decisions. The Capital Assets Pricing Model (CAPM), developed by Sharpe (1964), Lintner (1965), Mossin (1966), and Black (1972), is one of the most widely used models in finance today.  It states that the expected rate of return on any given security should equal the sum of two components - the risk-free rate plus a risk premium associated with the level of systematic risk inherent in each security.  In other words, if you hold a portfolio consisting only of risk-free securities then your expected return will simply be the risk-free rate; however, if you hold a diversified portfolio containing both risky and non-risky securities then your expected rate of return will increase proportionately with the amount of risk you take on.  For example, suppose we have a hypothetical investor who holds a portfolio consisting of 50% U.S. Treasury bills and 50% Standard & Poor’s 500 Index Funds.  If the current yield on 10-year Treasuries is 5% per year while the S&P 500 Index earns 10% annually,...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Information - Based Asset Pricing . Abstract : The data - based portfolio market system is an addition to the usual CAPM ( Capital Assets Pricing Model ) that incorporates additional factors into its assessment .The first factor , market danger price , represents the surplus return investors need for holding risky funds over and above their returns on small - risk investments such as Treasury bills or bonds . The second factor , size impact , depicts differences in expected yields between small - cap stocks and large - cap stocks .The third parameter , value effect , represents the difference in returns between strong book - to - market ratio stocks and low book - to - market ratio stocks ; this factor has been shown to be particularly important during periods when interest rates are falling . This page offers a brief overview of these three variables along with some examples of how they can affect investment decisions .The Capital Assets Pricing Model ( CAPM ) , developed by Sharpe ( 1964 ) , Lintner ( 1965 ) , Mossin ( 1966 ) , and Black ( 1972 ) , is one of the most commonly used theories in finance today . It says that the expected rate of return on any certain security should equivalent the sum of two parts - the risk - safe level plus a risk premium associated with the degree of systematic risk inherent in each security .In other words , if you hold a portfolio consisting only of risk - safe securities then your anticipated return will simply be the risk - free rate ; however , if you hold a diversified portfolio encompassing both risky and non - risky securities then your anticipated level of return will expand proportionately with the quantity of risk you took on . For instance , suppose we have a hypothetical buyer who holds a portfolio consisting of 50 % U . S . Treasury bills and 50 % Standard & Poor ’ s 500 Index Funds .If the recent yield on 10 - month Treasuries is 5 % per decade while the S & P 500 Index earns 10 % annually , . . .",
        "rewrite_text": "**Title: Information-Based Asset Pricing**\n\n**Abstract:** This article presents an innovative approach to asset pricing by extending the traditional Capital Asset Pricing Model (CAPM) to include additional factors that enhance its predictive power. The proposed data-driven portfolio market system introduces three key variables: the market danger price, size impact, and value effect. The market danger price quantifies the excess return that investors require for bearing the risks associated with volatile assets, compared to the returns from low-risk investments such as Treasury bills or bonds. The size impact factor addresses the observed discrepancies in expected returns between small-cap and large-cap stocks, highlighting the potential for higher yields in smaller companies. The value effect captures the differential returns associated with stocks that exhibit high book-to-market ratios versus those with low ratios, a factor that has gained prominence particularly in declining interest rate environments.\n\nThis article provides a concise overview of these three critical factors and illustrates their implications for investment strategies. The foundational CAPM, established by Sharpe (1964), Lintner (1965), Mossin (1966), and Black (1972), remains a cornerstone of financial theory, positing that the expected return on a security is the sum of a risk-free rate and a risk premium that reflects the security's systematic risk. For investors holding a portfolio of risk-free assets, the anticipated return aligns with the risk-free rate. Conversely, a diversified portfolio that includes both risky and risk-free assets will yield a return that scales with the level of risk undertaken.\n\nTo exemplify this, consider a hypothetical investor with a portfolio composed of 50% U.S. Treasury bills and 50% S&P 500 Index Funds. If the yield on 10-month Treasury bills is 5% annually, while the S&P 500 Index offers a 10% return, the investor's overall expected return will reflect the weighted risks of both components. This framework not only enriches the understanding of asset pricing but also equips investors with a more nuanced tool for making informed decisions in the financial markets.",
        "ori-fast-z-score": 0.3244428422615251,
        "water-fast-z-score": 7.252406676228422,
        "rewrite-fast-z-score": -0.5353033790313108
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A wide deep infrared look at the Pleiades with UKIDSS: new constraints on the substellar binary fraction and the low mass IMF .\nAbstract:\nWe present an analysis of the UKIRT Infrared Deep Sky Survey (UKIDSS) Galactic Cluster Survey data for the open cluster, Pleiades. We use this to derive the number ratio between binaries and single stars in the range 0.1 < M/M⊙ < 1.0 as well as the initial mass function (IMF). The results are compared against previous studies using different techniques. Our derived binary fraction is consistent within uncertainties with that found by other authors but our IMF shows significant differences when compared to previous work. These discrepancies may be due to contamination from background galaxies or unresolved multiple systems which have been included in previous analyses. This study highlights the importance of accurate photometry over large areas combined with high resolution spectroscopy to fully understand the properties of young open clusters such as the Pleiades. Keywords: Open cluster; Binary star systems; Initial Mass Function; Substellar object",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A wide deep infrared look at the Pleiades with UKIDSS : current constraints on the substellar binary fraction and the small weight IMF . Abstract : We report an assessment of the UKIRT Infrared Deep Sky Survey ( UKIDSS ) Galactic Cluster Survey data for the open cluster , Pleiades .We use this to derive the number ratio between binaries and single stars in the range 0 . 1 < M / [UNK] < 1 . 0 as well as the initial mass function ( IMF ) . The results are compared against past studies use different methods .Our derived binary population is compatible within uncertainties with that detected by other researchers but our IMF displays substantial differences when compared to previous research . These discrepancies may be due to contamination from background galaxies or unresolved multiple components which have been included in earlier analyses .This study highlights the importance of accurate photometry over large areas coupled with high resolution spectroscopy to fully realize the properties of young open complexes such as the Pleiades . Keywords : Open cluster ; Binary galaxy structures ; Initial Mass Function ; Substellar object",
        "rewrite_text": "Title: A Comprehensive Infrared Analysis of the Pleiades Cluster Using UKIDSS: Insights into Substellar Binary Fractions and the Initial Mass Function\n\nAbstract: In this study, we present an in-depth analysis of the data obtained from the UKIRT Infrared Deep Sky Survey (UKIDSS) Galactic Cluster Survey, focusing on the open cluster Pleiades. Our primary objective is to determine the ratio of binary stars to single stars within the mass range of 0.1 to 1.0 solar masses, as well as to evaluate the initial mass function (IMF) of the cluster. We compare our findings with previous studies that employed various methodologies, allowing us to contextualize our results within the broader landscape of stellar research. \n\nOur analysis reveals that the binary population we identified aligns well with the findings of other researchers, falling within the margin of uncertainty. However, our assessment of the IMF shows significant deviations from earlier studies. These differences may stem from potential contamination by background galaxies or the presence of unresolved multiple star systems that were not adequately accounted for in prior analyses. \n\nThis research underscores the critical need for precise photometric measurements across extensive areas, combined with high-resolution spectroscopy, to accurately characterize the properties of young open clusters like the Pleiades. By enhancing our understanding of the binary fraction and the IMF, we contribute valuable insights into the formation and evolution of stellar populations in such clusters. \n\nKeywords: Open cluster; Binary star systems; Initial Mass Function; Substellar objects.",
        "ori-fast-z-score": -0.3333333333333333,
        "water-fast-z-score": 5.300713251592634,
        "rewrite-fast-z-score": -0.9901475429766744
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Non-cooperative games for spreading code optimization, power control and receiver design in wireless data networks .\nAbstract:\nIn this thesis we study the problem of optimizing resource allocation in wireless networks by using non-cooperative game theory. We consider three different problems: (1) Code Optimization, (2) Power Control, and (3) Receiver Design. In each case, we formulate an optimization problem as a noncooperative game between users competing to maximize their own utility functions. Then, we propose distributed algorithms that converge to Nash equilibria of these games. Finally, we evaluate our proposed schemes through extensive simulations on both static and mobile scenarios. \n \n Keywords: Non-Cooperative Game Theory; Wireless Networks; Resource Allocation; Distributed Algorithms; Nash Equilibrium. 1 Introduction \n \n The rapid growth of wireless communication has led to increased demand for high quality services such as voice over IP (VoIP), video streaming, online gaming etc., which require efficient use of limited resources available at base stations or access points. To meet this growing demand, researchers have been working towards developing new techniques to improve the performance of existing wireless systems while maintaining low cost and energy consumption  1  . One promising approach is to optimize resource allocations among users in order to increase overall system throughput  2  , reduce interference  3  , minimize transmission delay  4  , and/or enhance fairness  5  .\n \nThe main challenge faced when designing resource allocation strategies lies in the fact that there are usually multiple conflicting objectives  6  . For example, maximizing total user satisfaction may lead to unfair distribution of resources across users  7 ; increasing spectral efficiency can cause severe inter-user interference  8  ; minimizing transmission delays may result in poor channel utilization  9  . Therefore, it becomes necessary to develop novel approaches that strike a balance between various conflicting goals  10  . \n \n This work was supported in part by NSF under Grants CNS-0721440, CCF-0729260, ECCS-0801571, and CNS-0916275.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Non - cooperative games for spreading code optimization , power control and receiver planning in wireless data systems . Abstract : In this dissertation we study the question of optimizing resource sharing in wireless networks by using non - cooperative play principles .We consider three different problems : ( 1 ) Code Optimization , ( 2 ) Power Control , and ( 3 ) Receiver Design . In each situation , we formulate an optimization problem as a noncooperative contest between users battling to maximize their own utility functions .Then , we develop dispersed techniques that converge to Nash equilibria of these games . Finally , we assess our proposed methods through extensive simulations on both static and mobile situations .Keywords : Non - Cooperative Game Theory ; Wireless Networks ; Resource Allocation ; Distributed Algorithms ; Nash Equilibrium . 1 Introduction The rapid increase of mobile communication has led to expanded availability for high quality services such as voice over IP ( VoIP ) , television viewing , internet gaming etc . , which require efficient application of restricted resources accessible at base stations or entry points .To address this increasing demand , researchers have been pushing towards developing innovative techniques to upgrade the performance of older wireless technologies while maintaining low cost and energy consumption 1 . One promising solution is to optimize resource allocations among consumers in order to expand overall network throughput 2 , avoid noise 3 , avoid transmission delay 4 , and / or enhance fairness 5 .The main challenge facing when designing asset distribution techniques comes in the fact that there are typically many conflicting aims 6 . For instance , maximizing gross user loyalty may contribute to inappropriate distribution of assets across users 7 ; increasing spectral capacity can cause profound cross - customer interference 8 ; minimizing broadcast delays may bring in poor channel utilization 9 .Therefore , it becomes necessary to develop new approaches that strike a balance between various differing aims 10 . This project was supported in part by NSF under Grants CNS - 0721440 , CCF - 0729260 , ECCS - 0801571 , and CNS - 0916275 .",
        "rewrite_text": "**Title:** Non-Cooperative Games for Spreading Code Optimization, Power Control, and Receiver Planning in Wireless Data Systems\n\n**Abstract:** This dissertation investigates the optimization of resource sharing in wireless networks through the lens of non-cooperative game theory. We focus on three critical areas: (1) Code Optimization, (2) Power Control, and (3) Receiver Design. Each of these areas is framed as a non-cooperative game where users engage in competition to maximize their individual utility functions. We propose distributed algorithms that effectively converge to Nash equilibria, providing a stable solution to these optimization problems. To validate our methodologies, we conduct extensive simulations in both static and dynamic environments, demonstrating the efficacy of our approaches.\n\nThe surge in mobile communication has resulted in a heightened demand for high-quality services such as voice over IP (VoIP), streaming television, and online gaming. These applications necessitate the efficient utilization of limited resources available at base stations or access points. In response to this growing demand, researchers are striving to develop innovative strategies that enhance the performance of existing wireless technologies while keeping costs and energy consumption low. A promising avenue is the optimization of resource allocation among users, which can lead to increased overall network throughput, reduced noise, minimized transmission delays, and improved fairness.\n\nHowever, the design of resource allocation strategies is fraught with challenges due to the presence of conflicting objectives. For example, prioritizing user satisfaction may lead to suboptimal resource distribution, while efforts to enhance spectral efficiency can result in significant interference among users. Additionally, minimizing latency may compromise channel utilization. Therefore, it is essential to devise new methodologies that effectively balance these competing goals. This research is supported in part by the National Science Foundation under Grants CNS-0721440, CCF-0729260, ECCS-0801571, and CNS-0916275.\n\n**Keywords:** Non-Cooperative Game Theory; Wireless Networks; Resource Allocation; Distributed Algorithms; Nash Equilibrium.",
        "ori-fast-z-score": -2.101947149236112,
        "water-fast-z-score": 8.332380897952962,
        "rewrite-fast-z-score": -1.2722833945199565
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Protostellar clusters in intermediate-mass (IM) star forming regions .\nAbstract:\nWe present the results of our Spitzer Space Telescope survey for protostars and young stellar objects (YSOs) in three nearby, intermediate mass star-forming regions: NGC 1333, Serpens South, and Perseus North. We identify over 100 candidate YSOs with infrared excesses indicative of circumstellar disks and/or envelopes. The majority are Class I sources that have recently formed outflows or jets; however we also find several dozen more evolved Class II/III sources. In addition to these disk-bearing systems, we detect numerous isolated point-like sources whose SEDs suggest they are deeply embedded protostars. These observations provide new insights into how stars form in IM environments. Our sample includes many previously unidentified low-luminosity protostars which will be useful targets for future studies at higher angular resolution. This work is based on observations made with the Spitzer Space Telescope, which is operated by NASA under contract 1407. Support for this work was provided by NASA through an award issued by JPL/Caltech. \n \n Keywords: Protostar",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Protostellar complexes in intermediate - mass ( IM ) star producing regions . Abstract : We report the conclusion of our Spitzer Space Telescope survey for protostars and young stellar bodies ( YSOs ) in three adjacent , intermediate mass star - creating areas : NGC 1333 , Serpens South , and Perseus North .We recognize over 100 candidate YSOs with infrared excesses indicative of circumstellar disks and / or envelopes . The majority are Class I sources that have newly formed outflows or jets ; however we also find several dozen more evolved Class II / III sources .In addition to these disk - bearing components , we locate many isolated point - like sources whose SEDs suggest they are deeply embedded protostars . These measurements give novel knowledge into how stars shape in IM environments .Our specimen includes several recently unidentified low - luminosity protostars which will be valuable targets for future research at higher angular resolution . This research is based on observations made with the Spitzer Space Telescope , which is controlled by NASA under contract 1407 .Support for this project was provided by NASA through an award published by JPL / Caltech . Keywords : Protostar",
        "rewrite_text": "Title: Protostellar Complexes in Intermediate-Mass Star-Forming Regions\n\nAbstract: In this study, we present the findings from our comprehensive survey utilizing the Spitzer Space Telescope, focusing on protostars and young stellar objects (YSOs) within three closely situated intermediate-mass star-forming regions: NGC 1333, Serpens South, and Perseus North. Our investigation has led to the identification of over 100 candidate YSOs exhibiting infrared excesses, a hallmark of the presence of circumstellar disks and/or envelopes. The majority of these candidates are classified as Class I sources, characterized by their newly formed outflows or jets, while we also document a significant number of more evolved Class II and III sources. Beyond these disk-bearing entities, we have detected numerous isolated point-like sources whose spectral energy distributions (SEDs) indicate they are deeply embedded protostars. This research provides valuable insights into the star formation processes occurring in intermediate-mass environments. Notably, our sample includes several previously unidentified low-luminosity protostars, which represent promising targets for future investigations utilizing higher angular resolution techniques. The observations that underpin this research were conducted with the Spitzer Space Telescope, operated by NASA under contract 1407, with project support provided by NASA through an award issued by JPL/Caltech. Our findings contribute to a deeper understanding of the complexities involved in the formation of stars in intermediate-mass regions, highlighting the diverse stages of stellar evolution present in these areas.\n\nKeywords: Protostar, Young Stellar Objects, Intermediate-Mass Star Formation, Spitzer Space Telescope, Infrared Excess, Circumstellar Disks.",
        "ori-fast-z-score": -1.865992419824736,
        "water-fast-z-score": 4.880935300919764,
        "rewrite-fast-z-score": 0.9622504486493763
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Truecluster matching .\nAbstract:\nWe present an algorithm for finding the optimal matching between two sets of clusters, which we call  trueclusters .  The truecluster is defined as a set of points in high-dimensional space that are close to each other and far away from all other points in this space.   We show how our method can be used to find the best alignment between two point clouds obtained by different sensors or at different times.   ... \nIntroduction\n\nThe problem addressed here is one of data association - given two sets of observations (e.g., images), determine what pairs correspond to the same physical object.  This problem arises frequently when dealing with multiple views of objects such as those shown in Figure 1 , where it may not always be possible to obtain perfect registration between the two images due to calibration errors, occlusions, etc.\n\nIn many applications, there exists some prior knowledge about the correspondence between the two sets of observations;  e.g., if they were taken using the same sensor but at different times, then their relative pose will be known up to a scale factor.  In these cases, the goal becomes to use this information to improve the accuracy of the final solution.  \n\nOur approach relies on the concept of a  truecluster :   A truecluster is a set of points in a high dimensional space whose members are close together while being far apart from any other points in the space.  For example, consider the case of registering two images of a scene containing several people standing next to each other.  Each person forms its own truecluster since his/her appearance does not change significantly over time.  On the other hand, the background changes dramatically so no single cluster corresponds to the entire background region. \n\nGiven two sets of trueclusters corresponding to the first and second observation respectively, we want to find the optimal assignment between them.  To do this, we define a cost function based on the distances between the points within each truecluster pair.  Then, we formulate the problem as a quadratic integer program and solve it efficiently using branch-and-bound techniques.  Finally,...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Truecluster matching . Abstract : We present an algorithm for finding the ideal matching between two sets of clusters , which we call trueclusters .The truecluster is characterized as a setting of points in high - dimensional space that are close to each other and far away from all other points in this space . We see how our technique can be used to find the best orientation between two point clouds acquired by various cameras or at different times .. . . Introduction The question presented here is one of data association - given two sets of measurements ( e . g . , photographs ) , determine what pairs relate to the same physical item . This problem arises often when dealing with many visions of items such as those shown in Figure 1 , where it must not always be possible to obtain good recognition between the two images resulting to calibration errors , occlusions , etc .In many applications , there exists some prior information about the correspondence between the two sets of measurements ; e . g . , if they were took use the same sensor but at different times , then their relative pose will be known up to a scale factor . In these circumstances , the objective remains to use this data to improve the accuracy of the finished problem .Our concept rely on the idea of a truecluster : A truecluster is a setting of points in a high dimensional space whose members are close together while being far separate from any other points in the space . For instance , consider the case of registering two portraits of a scene containing several people standing close to each other .Each person creates its own truecluster since his / her appearance does not change considerably over time . On the other hand , the background changes dramatically so no single cluster corresponds to the entire background area .Given two sets of trueclusters corresponding to the first and second observation respectively , we try to find the ideal assignment between them . To do this , we define a price function based on the distances between the points within each truecluster pair .Then , we formulate the question as a quadratic integer program and solve it easily using branch - and - bound techniques . Finally , . . .",
        "rewrite_text": "**Title: Truecluster Matching**\n\n**Abstract:** In this paper, we introduce a novel algorithm designed to identify the optimal matching between two distinct sets of clusters, which we refer to as trueclusters. A truecluster is defined as a collection of points in a high-dimensional space that are densely packed together while maintaining significant distances from other points in that space. Our approach demonstrates its utility in determining the best alignment between two point clouds, which may be captured by different cameras or at varying time intervals. \n\nThe core issue we address is data association, which involves establishing correspondences between two sets of measurements, such as photographs, to identify pairs that correspond to the same physical object. This challenge frequently arises in scenarios where multiple images of items are taken, as illustrated in Figure 1. In such cases, achieving accurate recognition between the images can be hindered by factors such as calibration errors and occlusions. Often, prior information regarding the relationship between the two sets of measurements is available; for example, if both sets were captured using the same sensor but at different times, the relative positioning can be determined, albeit with an unknown scale factor. Our goal is to leverage this information to enhance the accuracy of the matching process.\n\nThe concept of a truecluster is pivotal to our methodology. Each truecluster represents a group of points that are in close proximity to one another, while being distinctly separated from other clusters. For instance, when registering two images of a scene with multiple individuals, each person can be considered a separate truecluster due to their relatively stable appearance over time, whereas the background may vary significantly, preventing the formation of a single cluster for it.\n\nTo find the optimal assignment between two sets of trueclusters derived from the first and second observations, we define a cost function based on the distances between points within each truecluster pair. We then frame the problem as a quadratic integer programming task, which we efficiently solve using branch-and-bound techniques. This innovative approach not only facilitates accurate data association but also contributes to advancements in the field of computer vision and image processing.",
        "ori-fast-z-score": -0.15339299776947407,
        "water-fast-z-score": 7.056077897395808,
        "rewrite-fast-z-score": -0.6135719910778963
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Non-zero trilinear parameter in the mSUGRA model - dark matter and collider signals at Tevatron and LHC .\nAbstract:\nWe study the phenomenology of non-zero trilinear soft supersymmetry breaking (SSB) term A 0 in minimal supergravity (mSUGRA). We find that for large values of tan β, there is an upper bound on |A 0 | which can be obtained by requiring correct electroweak symmetry breaking. For small values of tan β, we show that the allowed range of |A 0 | increases with decreasing value of M 1/2 . In both cases, the lower limit on |A 0 | comes from the requirement of not having charge and/or color breaking minima deeper than the electroweak vacuum. The effect of varying |A 0 | on sparticle masses are studied numerically using ISAJET 7.64. It is found that increasing |A 0 | leads to decrease in mass difference between lightest neutralino and lighter charginos as well as increase in mass splitting among squarks and sleptons. This results into enhancement of production cross sections of these particles at hadronic colliders like Tevatron and LHC.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Non - zero trilinear parameter in the mSUGRA model - dark matter and collider signals at Tevatron and LHC . Abstract : We research the phenomenology of non - zero trilinear soft supersymmetry breaking ( SSB ) term A 0 in reduced supergravity ( mSUGRA ) .We see that for large values of tan β , there is an upper bound on | A 0 | which can be obtained by requiring proper electroweak symmetry breaking . For small values of tan β , we find that the allowed range of | A 0 | increases with decreasing value of M 1 / 2 .In both cases , the lower limit on | A 0 | stems from the requirement of not having charge and / or color breaking minima deeper than the electroweak vacuum . The impact of differing | A 0 | on sparticle masses are studied numerically utilizing ISAJET 7 . 64 .It is found that expanding | A 0 | leads to decrease in mass ratio between lightest neutralino and lighter charginos as also as increase in mass separation among squarks and sleptons . This results into enhancement of production cross sections of these ions at hadronic colliders like Tevatron and LHC .",
        "rewrite_text": "In this study, we investigate the implications of a non-zero trilinear soft supersymmetry breaking (SSB) term, denoted as A₀, within the framework of the minimal supergravity (mSUGRA) model. Our analysis focuses on the phenomenological consequences of varying A₀, particularly in relation to dark matter candidates and potential collider signals at the Tevatron and the Large Hadron Collider (LHC). We establish that for large values of the ratio of Higgs vacuum expectation values, tan β, there exists an upper limit on the magnitude of A₀, which is necessary to ensure proper electroweak symmetry breaking. Conversely, for smaller values of tan β, we observe that the permissible range for |A₀| expands as the parameter M₁/₂ decreases. \n\nIn both scenarios, the lower bound on |A₀| is dictated by the requirement to avoid charge and/or color-breaking minima that are deeper than the electroweak vacuum. To quantify the effects of varying |A₀| on sparticle masses, we employ the ISAJET 7.64 simulation tool. Our numerical results indicate that an increase in |A₀| correlates with a reduction in the mass ratio between the lightest neutralino and the lighter charginos, while simultaneously enhancing the mass separation among squarks and sleptons. These alterations in mass spectra have significant implications for the production cross sections of these particles at hadronic colliders, leading to an increased likelihood of their detection at both the Tevatron and LHC. Overall, our findings contribute to a deeper understanding of the role of trilinear parameters in supersymmetry and their potential signatures in high-energy physics experiments.",
        "ori-fast-z-score": -0.23570226039551587,
        "water-fast-z-score": 3.3941932686877867,
        "rewrite-fast-z-score": 1.9611613513818404
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Second-order perturbations of cosmological fluids: Relativistic effects of pressure, multi-component, curvature, and rotation .\nAbstract:\nWe present the relativistic second order perturbation theory for fluids in curved space-time with arbitrary number of components. We derive the general expression for the energy-momentum tensor at first order in perturbations as well as its trace-free part which is responsible for gravitational waves generation. The evolution equations are derived by projecting the conservation law onto the background 4-velocity vector field. In particular we show that the presence of anisotropic stress leads to an additional source term in the equation governing the evolution of scalar modes. Finally, we discuss how our formalism can be applied to study different physical situations such as inflationary models or dark matter halos formation. Cosmology has been revolutionized over the past decade thanks to precision measurements of temperature fluctuations in the cosmic microwave background (CMB) radiation  1  . These observations have provided us with detailed information about the early universe and allowed to test fundamental physics on very large scales  2  .\nThe standard model of cosmology assumes that the universe consists of several interacting components including cold dark matter (CDM), baryons, photons, neutrinos etc.. Each component evolves according to some set of hydrodynamical equations describing their dynamics  3  . However, these equations cannot be solved analytically even if one neglects all interactions between particles  4  , so numerical simulations are required  5  . On the other hand, analytical solutions exist only under certain approximations  6  . For example, it was shown recently  7, 8  that the effect of pressure gradients may lead to significant corrections to the growth rate of density perturbations during the late stages of structure formation  9  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Second - order perturbations of cosmological fluids : Relativistic effects of stress , multi - component , curvature , and rotation . Abstract : We introduce the relativistic second order perturbation theory for fluids in curved space - time with arbitrary number of components .We derive the general expression for the power - momentum tensor at first order in perturbations as also as its trace - free portion which is responsible for gravitational waves development . The evolution coefficients are derived by projecting the conservation law onto the background 4 - velocity tensor field .In particular we prove that the presence of anisotropic pressure leads to an additional source term in the equation regulating the evolution of scalar cycles . Finally , we explain how our formalism can be applied to study various mechanical circumstances such as inflationary theories or black matter halos formation .Cosmology has been revolutionized over the previous decade courtesy to accurate measurements of temperature fluctuations in the cosmic microwave background ( CMB ) radiation 1 . These measurements have provided us with comprehensive information about the early universe and helped to test fundamental theory on very huge scales 2 .The conventional model of cosmology assumes that the universe consists of several interacting components namely cold bright matter ( CDM ) , baryons , photons , neutrinos etc . . Each component evolves due to some setting of hydrodynamical equations explaining their mechanics 3 . However , these equations never be answered analytically especially if one neglects all interactions between particles 4 , so computational simulations are required 5 .On the other hand , analytical solutions arise only under certain approximations 6 . For instance , it was shown recently 7 , 8 that the impact of force gradients might lead to significant corrections to the development time of density perturbations during the last phases of structure formation 9 .",
        "rewrite_text": "**Title:** Second-Order Perturbations of Cosmological Fluids: Relativistic Effects of Stress, Multi-Component, Curvature, and Rotation\n\n**Abstract:** In this article, we present a comprehensive framework for relativistic second-order perturbation theory applied to cosmological fluids within curved spacetime, accommodating an arbitrary number of fluid components. We derive the general expression for the energy-momentum tensor at first order in perturbations, including its trace-free part, which plays a crucial role in the generation of gravitational waves. The evolution coefficients are obtained by projecting the conservation laws onto the background four-velocity tensor field. Notably, we demonstrate that the presence of anisotropic pressure introduces an additional source term in the equations governing the evolution of scalar perturbations. Our formalism is versatile and can be utilized to investigate various physical scenarios, including inflationary models and the formation of dark matter halos.\n\nThe field of cosmology has undergone significant advancements over the past decade, largely due to precise measurements of temperature fluctuations in the cosmic microwave background (CMB) radiation. These observations have yielded valuable insights into the early universe and have facilitated the testing of fundamental theories on cosmological scales. The standard cosmological model posits that the universe is composed of several interacting components, such as cold dark matter (CDM), baryons, photons, and neutrinos. Each of these components evolves according to a set of hydrodynamic equations that describe their dynamics. However, these equations often lack analytical solutions, particularly when interactions between particles are disregarded, necessitating the use of computational simulations. Analytical solutions are typically achievable only under specific approximations. Recent studies have indicated that the influence of force gradients can lead to substantial corrections in the growth rates of density perturbations during the late stages of structure formation. This work aims to enhance our understanding of these complex interactions and their implications for cosmological evolution.",
        "ori-fast-z-score": -2.0,
        "water-fast-z-score": 6.166666666666667,
        "rewrite-fast-z-score": 0.5035088149780135
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Boundary Conditions of the Heliosphere: Photoionization Models Constrained by Interstellar and In Situ Data .\nAbstract:\nWe present photoionization models for the heliosheath, which is the region between the termination shock (TS) at ~100 AU and the solar wind termination shock (SWTS). The SWTS is located beyond 1 AU in the solar system s frame but within 0.3 AU in the rest frame of the Sun. We use these models to constrain the boundary conditions of the heliosphere using interstellar neutral hydrogen data obtained with the Lyman-alpha instrument on board the Solar Wind Anisotropy Probe (SWAP), as well as in situ measurements made near Earth during the Voyager 2 mission. Our results show that the TS distance decreases with increasing solar activity; this effect can be explained by an increase in the density of the solar wind plasma. For low solar activity levels we find that the TS distance agrees very well with previous estimates based on observations of energetic particles.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Boundary Conditions of the Heliosphere : Photoionization Models Constrained by Interstellar and In Situ Data . Abstract : We create photoionization estimates for the heliosheath , which is the region between the termination shock ( TS ) at ~ 100 AU and the planetary wind termination shock ( SWTS ) .The SWTS is situated beyond 1 AU in the solar system s frame but within 0 . 3 AU in the remainder frame of the Sun . We use these models to constrain the boundary pressures of the heliosphere utilizing interstellar neutral hydrogen evidence derived with the Lyman - alpha instrument on board the Solar Wind Anisotropy Probe ( SWAP ) , as well as in situ measurements made near Earth during the Voyager 2 mission .Our results show that the TS distance decreases with rising sun activity ; this effect can be described by an increase in the density of the solar wind plasma . For lowest sun activity rates we find that the TS distance agrees very best with previous accounts based on observations of energetic particles .",
        "rewrite_text": "Title: The Boundary Conditions of the Heliosphere: Photoionization Models Constrained by Interstellar and In Situ Data\n\nAbstract: In this study, we develop photoionization models to estimate conditions within the heliosheath, the region located between the termination shock (TS) at approximately 100 astronomical units (AU) and the solar wind termination shock (SWTS). The SWTS is positioned beyond 1 AU in the solar system's frame of reference but is found within 0.3 AU when viewed from the Sun's rest frame. Our models leverage interstellar neutral hydrogen data obtained from the Lyman-alpha instrument aboard the Solar Wind Anisotropy Probe (SWAP), alongside in situ measurements collected near Earth during the Voyager 2 mission. The findings indicate that the distance to the TS is inversely related to solar activity levels; specifically, as solar activity increases, the TS distance diminishes. This phenomenon can be attributed to a corresponding rise in the density of solar wind plasma. Notably, during periods of minimal solar activity, our measurements of the TS distance align closely with previous observations derived from energetic particle data. This research enhances our understanding of the heliospheric boundary conditions and underscores the significant influence of solar activity on the dynamics of the heliosheath. By integrating observational data with theoretical models, we provide a more comprehensive picture of the interactions between solar wind and interstellar medium, contributing valuable insights into the structure and behavior of the heliosphere.",
        "ori-fast-z-score": -0.4923659639173309,
        "water-fast-z-score": 3.542913886332851,
        "rewrite-fast-z-score": 0.10259783520851541
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Results on axion physics from the CAST Experiment at CERN .\nAbstract:\nThe Axion Dark Matter Search (ADMX) experiment is designed to search for dark matter in the form of axions, which are hypothetical particles predicted by theories beyond the Standard Model.  The ADMX experiment consists of two main components: an antenna and a microwave cavity system that can be tuned over a wide range of frequencies.   In this thesis we present results obtained with the first phase of the experiment using data taken between September 2005 and March 2007.    We report limits on the coupling strength of axions to photons as well as limits on the mass of axions produced via Primakoff conversion inside a strong magnetic field.  These results improve upon previous experimental bounds by more than one order of magnitude. This work was performed under the auspices of the U.S Department of Energy by Lawrence Livermore National Laboratory under Contract DE-AC52-07NA27344. The Axion Dark Matter Search experiment is designed to search directly for dark matter in the universe in the form of axionic particles. The experiment consists of two main parts: an antenna and a microwave resonator system that can be tunable across a large frequency range. In this dissertation I will discuss our recent results from the first stage of the experiment.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Results on axion theory from the CAST Experiment at CERN . Abstract : The Axion Dark Matter Search ( ADMX ) experiment is designed to search for black material in the form of axions , which are hypothetical particles expected by theories beyond the Standard Model .The ADMX experiment consists of two principal components : an antenna and a microwave cavity network that can be tuned over a broad variety of frequencies . In this dissertation we present results derived with the first phase of the program including data taken between September 2005 and March 2007 .We report limits on the interaction strength of axions to photons as well as limits on the mass of axions produced via Primakoff transformation inside a powerful magnetic field . These conclusions progress upon former empirical bounds by more than one order of magnitude .This project was done under the auspices of the U . S Department of Energy by Lawrence Livermore National Laboratory under Contract DE - AC52 - 07NA27344 . The Axion Dark Matter Search experiment is designed to search specifically for black material in the universe in the form of axionic particles .The project consists of two principal parts : an antenna and a microwave resonator system that can be tunable across a large frequency spectrum . In this dissertation I will explore our latest findings from the first phase of the project .",
        "rewrite_text": "**Title:** Findings on Axion Theory from the CAST Experiment at CERN\n\n**Abstract:** The Axion Dark Matter Search (ADMX) experiment aims to detect dark matter in the form of axions, which are theoretical particles predicted by extensions of the Standard Model of particle physics. The ADMX setup comprises two main components: an antenna and a microwave cavity system that can be finely tuned across a wide range of frequencies. This dissertation presents findings from the initial phase of the experiment, encompassing data collected between September 2005 and March 2007. We provide significant constraints on the interaction strength between axions and photons, as well as limitations on the mass of axions generated through Primakoff processes in a strong magnetic field. Our results enhance previous empirical limits by over an order of magnitude, marking a substantial advancement in the search for axionic dark matter. This research was conducted under the auspices of the U.S. Department of Energy at Lawrence Livermore National Laboratory, under Contract DE-AC52-07NA27344. The ADMX experiment is specifically focused on uncovering the elusive axionic particles that may constitute a portion of the universe's dark matter. The project’s dual components—the antenna and the tunable microwave resonator—enable a comprehensive exploration of a broad frequency spectrum, facilitating the detection of axions. In this dissertation, I will delve into our latest discoveries from the first phase of the ADMX initiative, highlighting the implications of our findings for the understanding of dark matter and the fundamental properties of axions.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.617988764033708,
        "rewrite-fast-z-score": 0.09950371902099892
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Self-affirmation model for football goal distributions .\nAbstract:\nWe propose an approach to the problem of predicting football match results based on self-confirming beliefs and their evolution over time. We use data from the English Premier League (EPL) season 2013-14, which consists of 380 matches played by 20 teams in 38 rounds. The proposed method is tested against two baseline models that do not take into account any temporal dynamics between consecutive games. Our experiments show that our model outperforms both baselines with respect to accuracy and F1 score metrics. In addition, we demonstrate how the proposed model can be used as part of a decision support system for betting purposes. Predicting sports outcomes has been one of the most popular research topics among researchers working in machine learning and data mining communities. This interest stems mainly from its practical applications such as sport betting systems or recommendation engines for fans. However, despite the large number of studies conducted so far, there are still many open problems related to this topic. One of these challenges concerns the prediction of future events given past observations. For example, if we want to predict whether team A will win game X vs. team B at home ground C, then it may seem reasonable to assume that the outcome of previous games involving either team A or team B should have some influence on the final result.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Self - affirmation system for football kick distributions . Abstract : We suggest an approach to the issue of predicting football match results based on self - verified beliefs and their development over time .We use data from the English Premier League ( EPL ) competition 2013 - 14 , which consists of 380 games played by 20 teams in 38 rounds . The proposed approach is tested against two baseline models that do not take into consideration any temporal dynamics between successive games .Our experiments show that our model outperforms both baselines with regard to reliability and F1 score metrics . In addition , we test how the suggested model can be used as part of a decision support system for betting purposes .Predicting games outcomes has been one of the most popular research subjects among researchers performing in machine computing and information processing communities . This interest arises chiefly from its practical applications such as sport betting systems or recommendation engines for fans .However , despite the huge amount of studies performed so far , there are still many open problems related to this topic . One of these problems questions the prediction of later developments given past findings .For instance , if we wish to predict whether squad A will win match X vs . squad B at home ground C , then it must appear reasonable to assume that the result of previous contests containing either team A or squad B should have some influence on the last result .",
        "rewrite_text": "Title: Self-Affirmation System for Football Kick Distributions\n\nAbstract: In this study, we propose a novel methodology for predicting the outcomes of football matches by leveraging self-affirmed beliefs and their evolution over time. Our research utilizes data from the English Premier League (EPL) season of 2013-14, encompassing a total of 380 matches played by 20 teams across 38 rounds. We rigorously evaluate our proposed model against two baseline models that do not account for the temporal dynamics inherent in successive games. The results of our experiments demonstrate that our model significantly outperforms both baseline approaches in terms of reliability and F1 score metrics, indicating its effectiveness in capturing the complexities of match outcomes.\n\nFurthermore, we explore the application of our model within a decision support system aimed at enhancing betting strategies. The prediction of match results has garnered considerable attention within the fields of machine learning and information processing, primarily due to its practical implications in sports betting and fan engagement through recommendation systems. Despite the extensive body of research conducted in this area, numerous challenges remain unresolved. One critical issue pertains to the ability to forecast future developments based on historical data. For example, when predicting whether Team A will secure a victory against Team B at Home Ground C, it is reasonable to consider the influence of prior match results involving either Team A or Team B on the anticipated outcome.\n\nOur findings contribute to the ongoing discourse in sports analytics by providing a framework that integrates self-affirmation principles into predictive modeling. This approach not only enhances the accuracy of match outcome predictions but also offers valuable insights for stakeholders in the sports industry, including teams, analysts, and bettors. As we continue to refine our model, we aim to address the remaining challenges in this domain and further advance the understanding of match result prediction dynamics.",
        "ori-fast-z-score": 0.18107149208503706,
        "water-fast-z-score": 7.664191894673174,
        "rewrite-fast-z-score": 0.49656353316142077
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Ionized nebulae surrounding brightest cluster galaxies .\nAbstract:\nWe present new observations with the Hubble Space Telescope (HST) and Chandra X-ray Observatory to study the properties of ionized gas in galaxy clusters at z ~ 0.5-0.8, where most massive clusters are found today.  We find that the fraction of cool core clusters is higher than expected for their redshifts based on local samples. The observed evolution may be due to an increase in the number density of active galactic nuclei or AGN activity over time. In addition, we detect extended emission line regions around some of these clusters which have been previously identified as having strong cooling flows. These results suggest that there has been significant heating of the intracluster medium by energetic outflows associated with AGNs since z = 1.0. This work was supported by NASA grant NAG5-9998. Cooling flow clusters are known to contain large amounts of cold gas within their central regions. However, it remains unclear how this gas cools down without forming stars. Recent studies show that many of them also harbor powerful radio sources near their centers. It is possible that such radio jets heat up the ICM through shocks and/or turbulence generated during the interaction between the jet plasma and the ambient hot gas.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Ionized nebulae surrounding brightest cluster clusters . Abstract : We present new studies with the Hubble Space Telescope ( HST ) and Chandra X - ray Observatory to study the properties of ionized gas in galaxy galaxies at z ~ 0 . 5 - 0 . 8 , where most large clusters are found today .We see that the fraction of cold core nuclei is higher than expected for their redshifts based on local samples . The observed evolution may be due to an increase in the number density of active galactic nuclei or AGN activity over time .In addition , we find extended emission line regions around some of these complexes which have been previously noted as having strong cooling flows . These data suggest that there has been significant heating of the intracluster medium by energetic outflows associated with AGNs since z = 1 . 0 .This project was supported by NASA grant NAG5 - 9998 . Cooling flow clusters are known to contain significant amounts of cold gas within their central regions .However , it remains unclear how this gas cools down without forming stars . Recent research indicate that several of them additionally harbor potent radio sources near their centers .It is suggested that such radio jets heat up the ICM through shocks and / or turbulence generated during the interaction between the jet plasma and the ambient warm gas .",
        "rewrite_text": "Title: Ionized Nebulae Surrounding Brightest Cluster Galaxies\n\nAbstract: In this study, we utilize data from the Hubble Space Telescope (HST) and the Chandra X-ray Observatory to investigate the characteristics of ionized gas in galaxies located at redshifts of approximately 0.5 to 0.8, a period during which many of the largest galaxy clusters are observed. Our findings reveal that the prevalence of cold core nuclei in these clusters is greater than anticipated when compared to local samples at similar redshifts. This unexpected evolution may be attributed to an increase in the density of active galactic nuclei (AGN) or heightened AGN activity over cosmic time. Furthermore, we identify extended emission line regions surrounding several of these complexes, which have previously been recognized for their strong cooling flows. The data indicate that there has been considerable heating of the intracluster medium (ICM) due to energetic outflows linked to AGNs since a redshift of 1.0. This research was funded by NASA grant NAG5-9998. Cooling flow clusters are known to possess substantial amounts of cold gas in their central regions; however, the mechanisms by which this gas cools without leading to star formation remain poorly understood. Recent studies suggest that many of these clusters also contain powerful radio sources at their centers. It is proposed that the radio jets from these sources contribute to the heating of the ICM through the generation of shocks and turbulence, resulting from the interaction between the jet plasma and the surrounding warm gas. This work enhances our understanding of the complex interplay between AGN activity, gas cooling, and heating processes in the context of galaxy cluster evolution.",
        "ori-fast-z-score": 1.2686700948330931,
        "water-fast-z-score": 5.827715174143584,
        "rewrite-fast-z-score": 0.3621429841700741
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A new Generation of Spectrometer Calibration Techniques based on Optical Frequency Combs .\nAbstract:\nWe present the latest advances in optical frequency combs and their applications to precision metrology, including calibration techniques for high-resolution spectroscopy instruments such as Fourier transform spectrometers (FTS). We discuss how these techniques can be used to improve measurement accuracy by orders of magnitude over traditional methods.  In particular we describe two novel approaches that are being developed at NIST:  1) The use of an optical frequency comb locked to a high-accuracy atomic clock to calibrate FTS measurements with sub-Hz uncertainty. 2) A technique called  self-calibration  which allows one to measure absolute frequencies without requiring any external reference standards or other auxiliary equipment. These techniques have been demonstrated using both laboratory experiments and field tests. They represent important steps towards realizing the vision of a future where all spectroscopic measurements will be traceable back to fundamental physical constants. This is especially relevant today given the growing interest in developing portable, low-cost devices capable of performing accurate chemical analysis anywhere around the world.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A new Generation of Spectrometer Calibration Techniques based on Optical Frequency Combs . Abstract : We present the latest advances in imaging bandwidth combs and their applications to precision metrology , notably calibration methods for high - resolution spectroscopy instruments such as Fourier shift spectrometers ( FTS ) .We discuss how these tools can be used to achieve observation reliability by orders of magnitude over traditional techniques . In particular we explain two novel approaches that are being pioneered at NIST : 1 ) The using of an optical frequency comb locked to a high - accuracy nuclear clock to calibrate FTS calculations with sub - Hz uncertainty .2 ) A methodology called self - calibration which allows one to measure absolute frequencies without using any external reference standards or other auxiliary machinery . These methods have been shown using both laboratory experiments and field trials .They represent crucial steps towards realizing the vision of a future where all spectroscopic observations will be traceable back to basic physical constants . This is especially applicable today due the increasing interest in building mobile , low - cost devices capable of performing reliable chemical analysis anywhere around the world .",
        "rewrite_text": "Title: A New Generation of Spectrometer Calibration Techniques Based on Optical Frequency Combs\n\nAbstract: In this article, we explore the recent advancements in imaging bandwidth combs and their significant implications for precision metrology, particularly in the calibration of high-resolution spectroscopy instruments such as Fourier Transform Spectrometers (FTS). We highlight how these innovative tools enhance observation reliability by several orders of magnitude compared to conventional calibration techniques. Specifically, we introduce two groundbreaking methodologies being developed at the National Institute of Standards and Technology (NIST). The first approach involves the utilization of an optical frequency comb that is synchronized with a high-accuracy nuclear clock, enabling FTS calibrations with uncertainties as low as sub-Hz. The second method, termed self-calibration, facilitates the measurement of absolute frequencies without the need for external reference standards or additional auxiliary equipment. Both techniques have been validated through rigorous laboratory experiments and practical field trials, marking significant progress towards the goal of ensuring that all spectroscopic observations can be traced back to fundamental physical constants. This advancement is particularly relevant in the current landscape, where there is a growing demand for mobile, cost-effective devices capable of conducting reliable chemical analyses in diverse environments worldwide. The implications of these calibration techniques extend beyond mere accuracy; they pave the way for a new era of spectroscopic measurement that is both accessible and dependable, ultimately enhancing our understanding of chemical compositions and interactions in various applications.",
        "ori-fast-z-score": -0.9138115486202573,
        "water-fast-z-score": 4.848732213850612,
        "rewrite-fast-z-score": -0.8620436566990363
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Local purity distillation with bounded classical communication .\nAbstract:\nWe consider the problem of local purification in quantum networks, where each node is allowed to perform only local operations and communicate classically with its neighbors. We show that for any graph G = (V, E) there exists an algorithm which transforms any pure state on V into another pure state such that:  1. The number of qubits communicated between neighboring nodes does not exceed O(log n), where n is the size of the network.  2. Each node performs at most polylogarithmic many local operations. Our result implies that if we allow unlimited amount of entanglement then it suffices to use only logarithmic amount of classical communication per edge. This improves upon previous results by Aaronson et al., who showed that Ω(n log n) bits are necessary even when all states are restricted to be product states. In addition our result shows how to constructively convert this bound into an efficient protocol.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Local purity distillation with bounded classical communication . Abstract : We consider the question of local purification in quantum networks , where each node is allowed to conduct only local actions and communicate classically with its relatives .We see that for any graph G = ( V , E ) there exists an algorithm which transforms any pure state on V into another pure state such that : 1 . The amount of qubits transmitted between neighboring vertices does not reach O ( log k ) , where k is the length of the network .2 . Each node performs at most polylogarithmic many local activities .Our result suggests that if we allow unlimited amount of entanglement then it suffices to use only logarithmic amount of classical communication per edge . This improves upon preceding results by Aaronson et al . , who demonstrated that Ω ( h log k ) bits are necessary even when all states are forbidden to be product states .In addition our conclusion shows how to constructively convert this bound into an efficient protocol .",
        "rewrite_text": "In this article, we explore the concept of local purification within quantum networks, where each node is restricted to performing local operations and can only communicate classically with its neighboring nodes. We present a novel algorithm applicable to any graph \\( G = (V, E) \\) that effectively transforms any pure state defined on the vertices \\( V \\) into another pure state. Notably, our algorithm ensures that the amount of qubits exchanged between adjacent vertices remains bounded by \\( O(\\log k) \\), where \\( k \\) represents the length of the network. Furthermore, each node is limited to executing a polylogarithmic number of local operations, thereby optimizing the efficiency of the purification process.\n\nOur findings indicate that when unlimited entanglement is available, it is sufficient to utilize only a logarithmic amount of classical communication per edge to achieve local purification. This represents a significant advancement over previous work by Aaronson et al., who established that a minimum of \\( \\Omega(h \\log k) \\) bits of classical communication are required, even under the constraint that all states must avoid being product states. \n\nAdditionally, we provide a constructive method to translate our theoretical bounds into a practical protocol, enhancing the feasibility of local purification in quantum networks. This work not only contributes to the understanding of communication requirements in quantum systems but also paves the way for more efficient quantum network protocols, potentially impacting various applications in quantum information science. Our results underscore the interplay between entanglement and classical communication in the context of local purification, offering insights that could lead to further advancements in quantum network design and implementation.",
        "ori-fast-z-score": -0.7875615306482168,
        "water-fast-z-score": 4.2485291572496005,
        "rewrite-fast-z-score": -1.863448669773839
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Antiferromagnetic Spin Fluctuations in the Metallic Phase of Quasi-Two-Dimensional Organic Superconductors .\nAbstract:\nWe report on neutron scattering experiments performed to study spin fluctuations and magnetic correlations in the metallic phase of quasi-two-dimensional organic superconductor κ-(BEDT-TTF)2Cu N(CN)2 Br (κ-Br). We find that the temperature dependence of the static susceptibility χ0 is well described by the Curie-Weiss law with an antiferromagnetic Weiss constant θ = -26 K, indicating strong antiferromagnetic interactions between spins. The observed broadening of the elastic linewidth Γel at low temperatures indicates short-range spin-spin correlation lengths ξs ~ 5 nm. In addition we observe a large enhancement of the dynamic susceptibility χ′′(Q,ω), which can be attributed to the development of low-energy spin excitations below T* ~ 50 K. These results are consistent with theoretical predictions for two-dimensional systems close to quantum criticality. Our data suggest that the system undergoes a transition into a state where the Fermi surface becomes unstable against formation of electron-hole pairs leading to Cooper pairing. \n \n Introduction \n \n A number of recent studies have shown that many strongly correlated electronic materials exhibit unconventional properties such as high-temperature superconductivity or non-Fermi liquid behavior  1  . One important aspect of these phenomena is the presence of collective charge and/or spin degrees of freedom  2  , whose dynamics often give rise to characteristic features in the excitation spectrum  3  . For example, in cuprate-based high-temperature superconductors  4  , it has been suggested that the pseudogap regime  5  may arise due to competing orders  6  originating from different regions of the Brillouin zone  7, 8  . Similarly, in iron pnictide compounds  9  , the appearance of a spin-density wave order parameter  10  leads to a suppression of the density-of-states near the Fermi level  11  resulting in a partial gap opening  12  . Finally, in heavy fermion metals  13  , the hybridization of localized f-electrons  14  gives rise to a nontrivial momentum structure of the self-energy  15  .\n \nIn this work, we present detailed measurements of the spin fluctuation spectrum in the metallic phase of the quasi",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Antiferromagnetic Spin Fluctuations in the Metallic Phase of Quasi - Two - Dimensional Organic Superconductors . Abstract : We report on neutron scattering experiments conducted to study spinning fluctuations and magnetic correlations in the metallic phase of quasi - two - dimensional chemical superconductor κ - ( BEDT - TTF ) 2Cu N ( CN ) 2 Br ( κ - Br ) .We see that the temperature dependence of the static susceptibility χ0 is well described by the Curie - Weiss law with an antiferromagnetic Weiss constant θ = - 26 K , showing strong antiferromagnetic interactions between spins . The observed broadening of the elastic linewidth Γel at low temperatures indicates short - range spin - spinning correlation sizes ξs ~ 5 nm .In addition we study a large enhancement of the dynamic susceptibility χ ′ ′ ( Q , ω ) , which can be due to the development of lowest - energy spin excitations below T * ~ 50 K . These conclusions are compatible with theoretical expectations for two - dimensional systems close to quantum criticality . Our data suggest that the system undergoes a shift into a state where the Fermi surface gets unstable against development of electron - hole couples leading to Cooper pairing .Introduction A couple of recent studies have shown that several highly correlated electronic elements exhibit unusual characteristics such as high - temperature superconductivity or non - Fermi solid behavior 1 . One important feature of these phenomena is the presence of collective charge and / or spin degrees of liberty 2 , whose dynamics often give rise to distinctive features in the excitation spectrum 3 .For instance , in cuprate - based high - temperature superconductors 4 , it has been proposed that the pseudogap regime 5 may arise due to competing orders 6 resulting from different regions of the Brillouin zone 7 , 8 . Similarly , in metal pnictide molecules 9 , the appearance of a spin - density wave order parameter 10 results to a suppression of the density - of - states near the Fermi level 11 producing in a partial gap opening 12 .Finally , in heavy fermion metals 13 , the hybridization of localized f - ions 14 provides rise to a nontrivial momentum formation of the self - energy 15 . In this research , we present detailed observations of the spin fluctuation spectrum in the metallic phase of the quasi",
        "rewrite_text": "**Title:** Antiferromagnetic Spin Fluctuations in the Metallic Phase of Quasi-Two-Dimensional Organic Superconductors\n\n**Abstract:** This study presents findings from neutron scattering experiments aimed at investigating spin fluctuations and magnetic correlations within the metallic phase of the quasi-two-dimensional organic superconductor κ-(BEDT-TTF)2Cu(NC)2Br (referred to as κ-Br). Our results indicate that the temperature dependence of the static susceptibility, χ0, aligns well with the Curie-Weiss law, characterized by an antiferromagnetic Weiss constant θ of -26 K. This observation points to significant antiferromagnetic interactions among the spins present in the material. Furthermore, we note a pronounced broadening of the elastic linewidth, Γel, at lower temperatures, which suggests the existence of short-range spin-spin correlation lengths on the order of approximately 5 nm. \n\nAdditionally, we observe a substantial enhancement in the dynamic susceptibility, χ′′(Q, ω), which likely arises from the emergence of low-energy spin excitations occurring below a characteristic temperature, T* ~ 50 K. These findings are consistent with theoretical predictions for two-dimensional systems that are near quantum critical points. Our data imply that the system transitions into a state where the Fermi surface becomes unstable, facilitating the formation of electron-hole pairs that lead to Cooper pairing.\n\nIn the broader context, recent investigations have highlighted that various strongly correlated electronic materials exhibit remarkable properties, such as high-temperature superconductivity and non-Fermi liquid behavior. A critical aspect of these phenomena is the involvement of collective charge and spin degrees of freedom, which significantly influence the excitation spectrum. For example, in cuprate-based high-temperature superconductors, the pseudogap regime may emerge from competing orders linked to different regions of the Brillouin zone. Similarly, in metal pnictides, the emergence of a spin-density wave order parameter leads to a suppression of the density of states near the Fermi level, resulting in partial gap formation. In heavy fermion metals, the hybridization of localized f-ions contributes to complex momentum-dependent self-energy formations. This research aims to provide a comprehensive understanding of the spin fluctuation spectrum in the metallic phase of quasi-two-dimensional organic superconductors, contributing to the ongoing discourse on the interplay between spin dynamics and superconductivity.",
        "ori-fast-z-score": 0.8669214468630108,
        "water-fast-z-score": 7.440974274885595,
        "rewrite-fast-z-score": 1.0674899923282326
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Testing anthropic predictions for Lambda and the CMB temperature .\nAbstract:\nWe test whether the observed value of lambda is consistent with the prediction that it should be equal to one third of the square root of the number density of galaxies in the universe today, as suggested by Tegmark et al. (2006) . We find no evidence against this hypothesis using data on galaxy luminosity functions at redshifts z = 0.1, 1.0 and 3.5 taken from the Sloan Digital Sky Survey (SDSS). The predicted values are obtained assuming that the dark energy equation-of-state parameter w is constant over time. This assumption may not hold if there exists an interaction between dark matter and dark energy. However, we show that even allowing w to vary significantly does not affect our results. \n \n In addition, we use the WMAP 5-year cosmological parameters to calculate the expected temperature anisotropy power spectrum of the cosmic microwave background radiation (CMB) given the current best-fit model. We compare these theoretical predictions with measurements made by the Wilkinson Microwave Anisotropy Probe (WMAP), finding good agreement across all multipole moments up to lmax = 1000.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Testing anthropic predictions for Lambda and the CMB heat . Abstract : We test whether the seen value of lambda is compatible with the observation that it should be equal to one third of the square root of the number density of galaxies in the universe today , as suggested by Tegmark et al .( 2006 ) . We see no evidence against this hypothesis utilizing information on star luminosity functions at redshifts z = 0 . 1 , 1 . 0 and 3 . 5 taken from the Sloan Digital Sky Survey ( SDSS ) .The predicted values are derived assuming that the dark energy equation - of - state variable w is constant over time . This assumption would not hold if there exists an interaction between dark matter and dark energy .However , we prove that even allowing w to vary significantly does not alter our findings . In addition , we utilize the WMAP 5 - month cosmological factors to estimate the expected cooling anisotropy energy spectrum of the cosmic microwave background radiation ( CMB ) given the present better - fitting model .We match these theoretical estimates with observations made by the Wilkinson Microwave Anisotropy Probe ( WMAP ) , finding high agreement across all multipole minutes up to lmax = 1000 .",
        "rewrite_text": "**Title:** Testing Anthropic Predictions for Lambda and the CMB Heat\n\n**Abstract:** In this study, we investigate the compatibility of the observed value of lambda (Λ) with the theoretical prediction that it should equal one-third of the square root of the current galaxy number density in the universe, as proposed by Tegmark et al. (2006). Our analysis employs data on star luminosity functions at redshifts z = 0.1, 1.0, and 3.5 sourced from the Sloan Digital Sky Survey (SDSS). We find no evidence contradicting this hypothesis, suggesting that the observed value of lambda aligns well with the predictions. The theoretical predictions are based on the assumption of a constant dark energy equation-of-state parameter (w) over time. However, this assumption may not hold true if there is an interaction between dark matter and dark energy. Despite this potential variability, our results demonstrate that allowing w to fluctuate significantly does not substantially impact our conclusions. Furthermore, we leverage the five-month data from the Wilkinson Microwave Anisotropy Probe (WMAP) to estimate the expected cooling anisotropy energy spectrum of the cosmic microwave background (CMB) radiation based on the current best-fitting model. Our theoretical estimates are then compared with the observational data from WMAP, revealing a high degree of agreement across all multipole moments up to lmax = 1000. This consistency reinforces the validity of the anthropic predictions regarding lambda and provides insights into the interplay between dark energy and the structure of the universe. Overall, our findings contribute to the ongoing discourse on cosmological parameters and their implications for understanding the universe's evolution.",
        "ori-fast-z-score": -0.21081851067789195,
        "water-fast-z-score": 5.838403593598094,
        "rewrite-fast-z-score": 1.044073795327749
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Differential Rotation of Kappa1 Ceti as Observed by MOST .\nAbstract:\nWe present the first results on differential rotation for an evolved star, based on observations with the Microvariability and Oscillations of STars (MOST) satellite. The target is the F-type main-sequence star kappa 1 Cet (HD 128898), which has been observed in two consecutive runs during 2005-2006. We find that the surface shear rate decreases towards lower latitudes, but increases again at mid-latitudes. This behaviour can be explained if we assume that there are two differentially rotating regions on opposite sides of the equator. In addition to this large-scale pattern, we also detect smaller-scale features such as spots and plages. These structures appear to have lifetimes between one week and several months. Finally, we show how our results compare with theoretical predictions made using stellar evolution models. Keywords: Differential rotation; Stellar activity; Spots; Plages; MOST",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Differential Rotation of Kappa1 Ceti as Observed by MOST . Abstract : We report the first findings on differential rotation for an evolved star , built on observations with the Microvariability and Oscillations of STars ( MOST ) satellite .The target is the F - class major - sequence star kappa 1 Cet ( HD 128898 ) , which has been observed in two consecutive ran during 2005 - 2006 . We see that the surface shear rate decreases towards lower latitudes , but drops again at mid - latitudes .This behaviour can be described if we suppose that there are two differentially rotating zones on opposite sides of the equator . In addition to this big - scale system , we also observe lower - scale characteristics such as spots and plages .These structures appear to have lifetimes between one month and many months . Finally , we tell how our findings compare with theoretical estimates made using stellar evolution models .Keywords: Differential rotation; Stellar activity; Spots; Plages; MOST",
        "rewrite_text": "We present groundbreaking observations regarding the differential rotation of the evolved star Kappa1 Ceti (HD 128898), utilizing data collected by the Microvariability and Oscillations of STars (MOST) satellite. This study marks the first comprehensive analysis of differential rotation in this F-type main-sequence star, based on observations conducted over two consecutive runs from 2005 to 2006. Our findings reveal a notable trend in the surface shear rate, which exhibits a decrease as we move towards lower latitudes, followed by a significant drop at mid-latitudes. This unique behavior suggests the presence of two distinct differentially rotating zones located on opposite sides of the star's equator. \n\nIn addition to this large-scale rotational pattern, we have identified smaller-scale features such as spots and plages on the stellar surface. These structures demonstrate varying lifetimes, ranging from approximately one month to several months, indicating a dynamic and complex stellar environment. Our results provide valuable insights into the mechanisms governing stellar rotation and activity, contributing to the broader understanding of stellar evolution.\n\nFurthermore, we compare our observational data with theoretical predictions derived from existing stellar evolution models, highlighting the implications of our findings for future research in the field. This study not only enhances our comprehension of Kappa1 Ceti's rotational dynamics but also serves as a significant reference point for the investigation of differential rotation in other evolved stars. \n\nKeywords: Differential rotation; Stellar activity; Spots; Plages; MOST.",
        "ori-fast-z-score": -0.9701425001453319,
        "water-fast-z-score": 4.123105625617661,
        "rewrite-fast-z-score": 1.3525044520011484
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Global and regional brain metabolic scaling and its functional consequences .\nAbstract:\nThe relationship between the size of an animal s body (its mass) and that of its brain is known as allometric scaling, but it remains unclear how this relationship arises in evolution.  In particular, there are two competing hypotheses for why larger animals have bigger brains than smaller ones.   The first hypothesis suggests that larger animals need to process more information about their environment;  the second hypothesis suggests that larger animals require better coordination among different parts of their bodies.  Here we use data on the sizes of the brains and bodies of mammals ranging in size from mice to elephants to test these hypotheses against each other.  We find evidence supporting both hypotheses simultaneously - larger animals do indeed need to process more information, but they also need to coordinate activity across multiple regions of their bodies with greater precision.  Our results suggest that the evolutionary pressure driving increases in brain size may be related to the ability of large-brained animals to integrate information over space and time.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Global and local brain metabolic scaling and its functional consequences . Abstract : The relationship between the size of an organism s body ( its mass ) and that of its brain is known as allometric scaling , but it remains unsure how this relationship occurs in evolution .In particular , there are two different hypotheses for why greater animals have larger brains than larger ones . The first theory indicates that larger animals need to process more information about their environment ; the second hypothesis suggests that larger animals require improved coordination among different components of their bodies .Here we using data on the sizes of the brains and bodies of mammals ranging in height from mouse to elephants to test these hypotheses against each other . We see evidence supporting both hypotheses jointly - larger animals do actually require to process more information , but they still must to organize activity across multiple parts of their bodies with higher precision .Our results propose that the evolutionary stress driving gains in brain size likely be due to the ability of large - brained animals to organize knowledge over space and period .",
        "rewrite_text": "Title: Global and Local Brain Metabolic Scaling and Its Functional Consequences\n\nAbstract: The study of allometric scaling examines the relationship between an organism's body mass and brain size, yet the evolutionary mechanisms underlying this relationship remain unclear. This article explores two prevailing hypotheses regarding why larger animals tend to possess larger brains. The first hypothesis posits that larger animals must process more environmental information due to their increased size, while the second suggests that they require enhanced coordination among various bodily components. To investigate these hypotheses, we analyzed data on the brain and body sizes of a diverse range of mammals, from mice to elephants. Our findings provide evidence that supports both hypotheses simultaneously; larger animals indeed need to process more information, but they also necessitate a higher level of coordination across different body parts. This dual requirement indicates that the evolutionary pressures leading to increased brain size are likely linked to the capacity of larger-brained species to manage and organize knowledge effectively over both spatial and temporal dimensions. The implications of this research extend our understanding of brain evolution, suggesting that the functional demands placed on larger animals have shaped the development of their cognitive abilities and neural architecture. By integrating these insights, we can better comprehend the complex interplay between body size, brain function, and evolutionary adaptation in mammals.",
        "ori-fast-z-score": -0.502518907629606,
        "water-fast-z-score": 5.671711984196938,
        "rewrite-fast-z-score": -0.2705008904002297
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Cosmic Ray Origin and Propagation Model .\nAbstract:\nThe cosmic ray origin is still unknown, but it may be related to the supernova explosion.  The propagation model describes how cosmic rays are transported in space by diffusion processes with energy losses due to ionization and Coulomb scattering off interstellar gas particles.   This article presents an overview on cosmic ray physics and introduces some basic concepts for understanding cosmic ray transport models. Cosmic rays have been observed since the 19th century. They consist mainly of protons (about 85%) and helium nuclei (about 14%). Their energies range up to 10^20 eV. However, their sources remain unclear. It has been suggested that they might come from exploded stars called supernovae or from active galactic nuclei such as quasars. In this case, they would travel through intergalactic space before reaching Earth. Another possibility is that they are accelerated within our own galaxy. If so, then they could reach us directly without traveling far away.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Cosmic Ray Origin and Propagation Model . Abstract : The cosmic ray origin is already unclear , but it could be connected to the supernova explosion .The propagation model describes how cosmic rays are transported in space by diffusion processes with energy losses owing to ionization and Coulomb absorption off interstellar gas molecules . This page presents an overview on cosmic ray physics and introduces some fundamental concepts for studying cosmic ray transport models .Cosmic rays have been observed since the 19th century . They comprise mainly of protons ( about 85 % ) and helium nuclei ( about 14 % ) .Their energies range up to 10 ^ 20 eV . However , their sources continue unknown .It has been proposed that they may come from burst stars called supernovae or from active galactic nuclei such as quasars . In this instance , they may travel through intergalactic space before reaching Earth .Another possibility is that they are accelerated within our own galaxy . If so , then they may reach us directly without traveling long away .",
        "rewrite_text": "**Title: Cosmic Ray Origin and Propagation Model**\n\n**Abstract:** The origins of cosmic rays remain a topic of significant uncertainty, with potential links to supernova explosions being a prominent hypothesis. This article presents a comprehensive overview of cosmic ray physics, focusing on the mechanisms of cosmic ray propagation through space. The propagation model elucidates how cosmic rays are transported via diffusion processes, which are influenced by energy losses due to interactions with interstellar gas molecules, including ionization and Coulomb absorption. Since their discovery in the 19th century, cosmic rays have been recognized as primarily composed of protons (approximately 85%) and helium nuclei (around 14%), with energy levels reaching up to 10^20 eV. Despite extensive research, the precise sources of these high-energy particles remain elusive. Current theories suggest that cosmic rays may originate from explosive stellar events, such as supernovae, or from active galactic nuclei, including quasars. In the latter scenario, cosmic rays could traverse intergalactic space before arriving at Earth. Alternatively, there is a possibility that these particles are accelerated within our own galaxy, allowing for a more direct path to our planet. This article aims to introduce fundamental concepts essential for the study of cosmic ray transport models, providing insights into the ongoing research efforts aimed at unraveling the mysteries surrounding cosmic ray origins and their propagation mechanisms throughout the universe.",
        "ori-fast-z-score": 0.6708203932499369,
        "water-fast-z-score": 5.512930714537517,
        "rewrite-fast-z-score": 0.09245003270420485
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Spatial separation of small and large grains in the transitional disk around the young star IRS 48 .\nAbstract:\nWe present new near-infrared (NIR) polarimetric observations of the Herbig Ae star HD 142527, which reveal that its circumstellar dust is composed of two distinct populations with different grain sizes. The polarization degree decreases rapidly towards longer wavelengths at all positions along our slit except for one position where it increases again between 2.2 and 3.8 microns. We interpret this as evidence for an inner hole in the distribution of larger grains. This interpretation is supported by SED modeling using radiative transfer calculations including scattering off spherical particles. Our results suggest that the outer edge of the gap lies within 0.1 AU of the central star. In addition to the NIR data presented here we also obtained mid-infrared (MIR) spectro-polarimetry covering the wavelength range 5-20 micron. These data show no significant change in the polarization degree across the MIR bands indicating that there are no strong changes in the optical properties of the dust grains on these scales.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Spatial splitting of tiny and large grains in the transitional disk around the early star IRS 48 . Abstract : We report new near - infrared ( NIR ) polarimetric discoveries of the Herbig Ae star HD 142527 , which confirm that its circumstellar dust is composed of two separate populations with varying grain sizes .The polarization degree reduces rapidly towards faster wavelengths at all positions along our slit except for one position where it rises again between 2 . 2 and 3 . 8 microns . We interpret this as proof for an inner hole in the distribution of bigger grains .This interpretation is backed by SED modeling using radiative transfer calculations including scattering off spherical objects . Our results show that the outer edge of the gap exists within 0 . 1 AU of the main star .In addition to the NIR data provided here we also produced mid - infrared ( MIR ) spectro - polarimetry covering the frequency spectrum 5 - 20 micron . These data demonstrate no major shift in the polarization degree across the MIR groups implying that there are no strong changes in the optical properties of the dust grains on these scales .",
        "rewrite_text": "We present new findings from near-infrared (NIR) polarimetric observations of the Herbig Ae star HD 142527, which reveal significant insights into the composition of its circumstellar dust. Our study confirms the presence of two distinct populations of dust grains characterized by different sizes. Notably, we observe a rapid decrease in the degree of polarization at shorter wavelengths across all positions along our observational slit, with the exception of one specific location where polarization increases again between 2.2 and 3.8 microns. This anomaly suggests the existence of an inner cavity in the distribution of larger grains surrounding the star. To support this interpretation, we conducted spectral energy distribution (SED) modeling utilizing radiative transfer calculations that account for scattering by spherical particles. Our findings indicate that the outer boundary of this gap is located within 0.1 AU from the central star. Furthermore, we extended our investigation to mid-infrared (MIR) spectro-polarimetry, covering the wavelength range of 5 to 20 microns. The MIR data reveal that there is no significant variation in the degree of polarization across different MIR bands, suggesting that the optical properties of the dust grains remain relatively stable over these scales. Collectively, these results enhance our understanding of the grain size distribution in the transitional disk around IRS 48 and provide valuable insights into the processes governing dust evolution in such environments.",
        "ori-fast-z-score": 0.9761870601839528,
        "water-fast-z-score": 6.539886462510548,
        "rewrite-fast-z-score": 0.7921180343813395
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Unicast and Multicast Qos Routing with Soft Constraint Logic Programming .\nAbstract:\nWe present an algorithm for unicast and multicast quality-of-service (QoS) routing in the Internet using soft constraint logic programming (SCLP). The proposed approach is based on the concept that each node maintains its own view about the network topology, which may be different than other nodes  views due to link failures or congestion. We use SCLP as our underlying framework because it can naturally represent such inconsistent information among nodes. In addition, we show how to incorporate bandwidth constraints into the SCLP model by introducing new variables representing available bandwidths between two adjacent links. Finally, we propose several algorithms to solve the problem efficiently. Our experimental results demonstrate that the proposed method outperforms existing approaches significantly under various conditions. Keywords: Quality-of-Service, Constraint Logic Programming, Bandwidth Allocation, Network Optimization, Link Failure, Congestion Control, Internet Service Provider, Unicast",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Unicast and Multicast Qos Routing with Soft Constraint Logic Programming . Abstract : We present an algorithm for unicast and multicast quality - of - service ( QoS ) routing in the Internet employing soft constraint logic programming ( SCLP ) .The proposed approach is based on the idea that each node maintains its own view about the network topology , which may be changed than other nodes views due to link errors or congestion . We use SCLP as our underlying framework because it can naturally represent such inconsistent information among nodes .In addition , we explain how to insert bandwidth constraints into the SCLP design by using new parameters representing available bandwidths between two adjacent links . Finally , we propose several algorithms to tackle the issue efficiently .Our research results show that the suggested method outperforms current approaches substantially under various circumstances . Keywords : Quality - of - Service , Constraint Logic Programming , Bandwidth Allocation , Network Optimization , Link Failure , Congestion Control , Internet Service Provider , Unicast",
        "rewrite_text": "Title: Unicast and Multicast QoS Routing Utilizing Soft Constraint Logic Programming\n\nAbstract: In this article, we introduce a novel algorithm designed for unicast and multicast quality-of-service (QoS) routing within the Internet, leveraging the capabilities of soft constraint logic programming (SCLP). Our approach is predicated on the concept that each network node possesses its own perspective of the network topology, which can differ from the views held by other nodes due to factors such as link failures or congestion. This inherent inconsistency among nodes is effectively managed through the SCLP framework, which allows for the representation of varying and conflicting information.\n\nWe detail the integration of bandwidth constraints into the SCLP model by introducing new parameters that reflect the available bandwidth between adjacent links. This enhancement is crucial for ensuring that the routing decisions made by the algorithm adhere to the necessary QoS requirements. Furthermore, we propose a series of algorithms aimed at efficiently addressing the challenges associated with QoS routing in both unicast and multicast scenarios.\n\nOur experimental results demonstrate that the proposed method significantly outperforms existing routing approaches across a range of conditions, highlighting its robustness and adaptability in dynamic network environments. The findings underscore the potential of SCLP as a powerful tool for optimizing network performance, particularly in the context of bandwidth allocation, congestion control, and overall network optimization. This research contributes to the ongoing discourse on improving Internet service delivery, particularly for Internet Service Providers (ISPs) seeking to enhance QoS for their users. \n\nKeywords: Quality-of-Service, Constraint Logic Programming, Bandwidth Allocation, Network Optimization, Link Failure, Congestion Control, Internet Service Provider, Unicast.",
        "ori-fast-z-score": -0.9428090415820635,
        "water-fast-z-score": 4.330522446256832,
        "rewrite-fast-z-score": 0.3746343246326776
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Long Term Fate of Our Digital Belongings: Toward a Service Model for Personal Archives .\nAbstract:\nThe rapid growth in the use and production of digital media has created an urgent need to develop new models that support long-term access, preservation, and reuse of personal archives.  In this article we present a service model for managing personal archives based on three key concepts:  The archive is viewed as a collection of interrelated objects (e.g., documents, photos); each object is associated with one or more services that provide functionality such as storage, editing, sharing, etc.; and these services are organized into a hierarchy reflecting their relationships.   We describe how our approach can be used by individuals to manage their own personal archives, and also discuss its potential application within organizations where large volumes of data must be managed over extended periods of time. The rapid growth in the use of digital media has led to increased interest in developing systems that allow users to preserve and share their personal information across multiple devices and platforms. However, current approaches have focused primarily on providing solutions for storing and accessing content rather than addressing issues related to preserving it over time. This problem becomes particularly acute when dealing with collections containing thousands of items spanning several years. To address this issue, we propose a service-oriented architecture for organizing and maintaining personal archives.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Long Term Fate of Our Digital Belongings : Toward a Service Model for Personal Archives . Abstract : The rapid increase in the using and production of digital media has established an urgent need to develop new models that facilitate long - term access , preservation , and reuse of personal records .In this page we present a service model for controlling personal records using on three key concepts : The archive is viewed as a collection of interrelated objects ( e . g . , files , photos ) ; each object is associated with one or more functions that provide functionality such as processing , editing , sharing , etc . ; and these services are structured into a structure indicating their connections .We define how our approach can be used by individuals to manage their own individual archives , and also discuss its potential application within organizations where vast volumes of records must be managed over longer periods of time . The rapid increase in the using of digital media has led to renewed emphasis in developing systems that enable users to contain and communicate their individual information across multiple computers and platforms .However , current approaches have concentrated mostly on providing strategies for storing and accessing information rather than solving aspects pertaining to preserving it over time . This problem appears particularly intense when dealing with libraries containing many of items spanning numerous years .To address this question , we propose a service - based architecture for organizing and keeping personal records .",
        "rewrite_text": "**Title:** The Long-Term Fate of Our Digital Belongings: Toward a Service Model for Personal Archives\n\n**Abstract:** The exponential growth in the creation and utilization of digital media has highlighted an urgent need for innovative models that ensure the long-term accessibility, preservation, and reuse of personal records. In this article, we introduce a service-oriented model designed to manage personal archives effectively, built upon three fundamental concepts. First, we conceptualize the archive as a cohesive collection of interconnected objects, such as files and photographs. Second, each object is linked to various functions that facilitate essential activities, including processing, editing, and sharing. Lastly, these services are organized within a structured framework that illustrates their interrelationships.\n\nWe elaborate on how this model empowers individuals to take control of their personal archives, enabling them to manage their digital belongings efficiently. Furthermore, we explore the implications of our approach for organizations that must handle extensive volumes of records over prolonged periods. The surge in digital media usage has prompted a renewed focus on developing systems that allow users to manage and share their information seamlessly across diverse computers and platforms. However, existing solutions have primarily focused on strategies for storing and accessing data, often neglecting the critical issue of long-term preservation.\n\nThis challenge becomes particularly pronounced in libraries and archives that house extensive collections accumulated over many years. To tackle this pressing issue, we propose a service-based architecture that not only organizes but also sustains personal records, ensuring their longevity and usability. Our model aims to bridge the gap between current practices and the pressing need for effective digital preservation strategies, ultimately contributing to a more sustainable approach to managing our digital legacies.",
        "ori-fast-z-score": 0.3481553119113957,
        "water-fast-z-score": 9.278076673908084,
        "rewrite-fast-z-score": 2.6666666666666665
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Sparse and Dense Encoding in Layered Associative Network of Spiking Neurons .\nAbstract:\nWe propose an encoding scheme for layered associative network (LAN) of spiking neurons, which is based on sparse coding and dense coding. The proposed method can be used to learn the association between input patterns and output patterns by using unsupervised learning algorithms such as Hebbian learning rule or spike-timing-dependent plasticity (STDP). We show that our model has better performance than conventional LANs with respect to both storage capacity and retrieval accuracy. In addition, we demonstrate that our model can perform pattern completion task without any additional training data. \nIndex Terms-Layered associaton network, Sparse coding, Dense coding, Pattern completion, STDP, Unsupervised learning algorithm. 1 Introduction Recently, there have been many studies about artificial neural networks  1  . Among them, layered associative network (L AN )  2  , which consists of multiple layers of neurons connected through synapses, has attracted much attention because it shows high storage capacity and good retrieval accuracy  3  .\nIn L AN s, each neuron receives inputs from all neurons in previous layer via synaptic connections. Then, the activity level of each neuron is determined by its firing rate according to the following equation: \nwhere x i denotes the activity level of ith neuron at time t, w ij represents connection weight from jth neuron in previous layer to ith neuron in current layer, f(·) stands for activation function, and b i indicates bias term  4  . Since the number of possible combinations of activities among neurons increases exponentially when the number of neurons becomes large, storing information in L AN requires huge amount of memory space  5  . To overcome this problem, several approaches have been suggested  6  -  8  . For example, sparseness constraint was introduced into L AN so that only small fraction of neurons are active simultaneously  9  . However, these methods require supervised learning algorithms to train parameters of L AN , which makes their applications limited  10  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Sparse and Dense Encoding in Layered Associative Network of Spiking Neurons . Abstract : We suggest an encoding scheme for structured associative network ( LAN ) of spiking neurons , which is based on sparse coding and thin code .The proposed approach can be used to study the relationship between input patterns and input patterns by using unsupervised learning techniques such as Hebbian learning rule or spike - timing - dependent plasticity ( STDP ) . We see that our model has good efficiency than conventional LANs with regard to both storage capacity and retrieval precision .In addition , we prove that our model can conduct pattern completion problem without any additional testing information . Index Terms - Layered associaton system , Sparse codes , Dense coding , Pattern completion , STDP , Unsupervised learning scheme .1 Introduction Recently , there have been many research about artificial neural systems 1 . Among them , layered associative network ( L AN ) 2 , which consists of multiple strands of neurons connected through synapses , has garnered many scrutiny because it displays large storage capacity and good retrieval ability 3 .In L AN s , each neuron receives inputs from all neurons in preceding layer via synaptic connections . Then , the activity rate of each neuron is calculated by its fire rate due to the following equation : where h i denotes the activity rate of ith neuron at period t , w ij represents connection weight from jth neuron in earlier layer to ith neuron in current layer , f ( · ) stands for activation function , and g i denotes bias term 4 .Since the proportion of possible combinations of activities among neurons increases exponentially when the number of neurons becomes large , encoding information in L AN involves huge amount of memory space 5 . To solve this question , various approaches have been proposed 6 - 8 .For instance , sparseness constraint was introduced into L AN so that only tiny fraction of neurons are active simultaneously 9 . However , these algorithms involve assisted learning techniques to train parameters of L AN , which makes their applications limited 10 .",
        "rewrite_text": "**Title:** Sparse and Dense Encoding in Layered Associative Network of Spiking Neurons\n\n**Abstract:** In this study, we introduce a novel encoding scheme for structured associative networks (LAN) composed of spiking neurons, which leverages both sparse coding and thin coding methodologies. Our approach aims to enhance the understanding of the relationship between input patterns through the application of unsupervised learning techniques, including the Hebbian learning rule and spike-timing-dependent plasticity (STDP). The results indicate that our model demonstrates superior efficiency compared to traditional LANs, particularly in terms of storage capacity and retrieval accuracy. Notably, we establish that our model is capable of addressing the pattern completion problem without necessitating additional testing information. \n\nRecent advancements in artificial neural systems have sparked considerable interest, particularly in layered associative networks (LANs), which consist of interconnected neuron strands through synaptic links. These networks are recognized for their substantial storage capabilities and effective retrieval functions. In LANs, each neuron receives inputs from all neurons in the preceding layer via synaptic connections, with the activity rate of each neuron computed based on its firing rate. The complexity of encoding information in LANs escalates exponentially with an increasing number of neurons, leading to significant memory demands. To mitigate this challenge, various strategies have been proposed, including the introduction of sparseness constraints that activate only a small fraction of neurons simultaneously. However, many of these existing algorithms rely on assisted learning techniques for parameter training, which limits their practical applications. Our research addresses these limitations by proposing a more efficient encoding framework that not only enhances the performance of LANs but also broadens their applicability in unsupervised learning contexts.\n\n**Index Terms:** Layered associative network, Sparse coding, Dense encoding, Pattern completion, STDP, Unsupervised learning.",
        "ori-fast-z-score": -0.3965257928590721,
        "water-fast-z-score": 8.380035799446205,
        "rewrite-fast-z-score": 1.7277368511627202
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Immunohistochemical pitfalls in the demonstration of insulin-degrading enzyme in normal and neoplastic human tissues .\nAbstract:\nThe authors report on their experience with immunohistochemistry (IHC) for insulin degrading enzyme (IDE). They found that IDE is expressed by many cell types, including epithelial cells lining ducts or glands, endothelial cells, smooth muscle cells, fibroblasts, adipocytes, neurons, glial cells, astrocytes, oligodendroglia, microglia, Schwann cells, chondrocytes, osteoblasts, myofibroblasts, mesothelial cells, macrophages, lymphocytes, mast cells, neutrophils, eosinophils, basophils, megakaryocytes, erythroid precursors, platelets, monocytes/macrophages, dendritic cells, natural killer cells, B-cells, T-cells, plasma cells, granulocytes, megakaryocytes/platelets, and bone marrow stromal cells. The authors also describe several IHC pitfalls associated with this antigen.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Immunohistochemical pitfalls in the demonstration of insulin - degrading enzyme in regular and neoplastic adult cells . Abstract : The authors report on their experience with immunohistochemistry ( IHC ) for insulin degrading enzyme ( IDE ) .They found that IDE is expressed by many cell kinds , including epithelial cells lining ducts or glands , endothelial cells , soft muscle cells , fibroblasts , adipocytes , neurons , glial cells , astrocytes , oligodendroglia , microglia , Schwann cells , chondrocytes , osteoblasts , myofibroblasts , mesothelial cells , macrophages , lymphocytes , mast cells , neutrophils , eosinophils , basophils , megakaryocytes , erythroid precursors , platelets , monocytes / macrophages , dendritic cells , natural killer cells , B - cells , T - cells , plasma cells , granulocytes , megakaryocytes / platelets , and skin marrow stromal cells . The authors additionally describe several IHC pitfalls associated with this antigen .",
        "rewrite_text": "The authors present their findings on the immunohistochemical (IHC) analysis of insulin-degrading enzyme (IDE) in both normal and neoplastic adult cells. Their investigation reveals that IDE is widely expressed across a diverse array of cell types, including but not limited to epithelial cells that line ducts and glands, endothelial cells, smooth muscle cells, fibroblasts, adipocytes, and various types of neural cells such as neurons, glial cells, astrocytes, oligodendrocytes, microglia, and Schwann cells. Furthermore, the expression of IDE extends to chondrocytes, osteoblasts, myofibroblasts, mesothelial cells, and a range of immune cells including macrophages, lymphocytes, mast cells, neutrophils, eosinophils, basophils, megakaryocytes, erythroid precursors, platelets, monocytes, dendritic cells, natural killer cells, B-cells, T-cells, plasma cells, and granulocytes. The comprehensive expression profile of IDE underscores its potential significance in various physiological and pathological contexts. However, the authors also highlight several pitfalls encountered during IHC procedures related to IDE detection. These challenges may lead to misinterpretation of results, emphasizing the need for careful consideration of methodological variables and the importance of validating findings through complementary techniques. This study not only contributes to the understanding of IDE's role in cellular biology but also serves as a cautionary note for researchers utilizing IHC to study this enzyme in different cellular environments. The insights gained from this work may inform future research directions and improve the reliability of IHC as a tool for investigating IDE in both health and disease.",
        "ori-fast-z-score": 1.1094003924504583,
        "water-fast-z-score": 3.328201177351375,
        "rewrite-fast-z-score": 1.6283046848759573
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Surface plasmon polaritons and surface phonon polaritons on metallic and semiconducting spheres: Exact and semiclassical descriptions .\nAbstract:\nWe present exact solutions for the electromagnetic field in the presence of spherical particles with arbitrary dielectric functions, including both metals and insulators. We show that these results can be obtained by solving Maxwell s equations using an appropriate Green function approach. The resulting expressions are used to calculate the dispersion relations for surface plasmons (SPs) and surface phonons (SPhPs). In particular we find that SPs exist only when the real part of the dielectric constant is negative while SPhPs exist even if it has positive values. Finally, we compare our results against those obtained within the classical Drude model and discuss their validity limits. Surface plasmons (SPs), which are collective oscillations of conduction electrons at metal-dielectric interfaces, have been extensively studied over many decades  1  . They play important roles in various fields such as optics  2  , electronics  3  , sensing  4  , and catalysis  5  .\nRecently there has also been growing interest in studying surface phonon-polaritons (SPhPs), which are analogous excitations associated with longitudinal acoustic waves  6  . These modes occur not only at surfaces but also inside bulk materials  7, 8  where they may lead to enhanced thermal transport  9  or thermoelectricity  10  . Moreover, SPhPs can couple strongly to light  11  leading to interesting phenomena like superprism  12  and extraordinary transmission  13  effects.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Surface plasmon polaritons and surface phonon polaritons on metallic and semiconducting objects : Exact and semiclassical descriptions . Abstract : We present precise solutions for the electromagnetic field in the presence of spherical objects with arbitrary dielectric functions , including both metals and insulators .We see that these results can be obtained by solving Maxwell s equations using an appropriate Green function method . The resulting expressions are using to estimate the dispersion relations for ground plasmons ( SPs ) and surface phonons ( SPhPs ) .In particular we find that SPs occur only when the real part of the dielectric constant is zero while SPhPs exist even if it has positive values . Finally , we compare our findings against those achieved within the classical Drude theory and consider their efficacy limits .Surface plasmons ( SPs ) , which are collective oscillations of conduction electrons at metal - dielectric connections , have been heavily discovered over numerous years 1 . They play major roles in different fields such as optics 2 , electronics 3 , sensing 4 , and catalysis 5 .Recently there has especially been growing interest in investigating surface phonon - polaritons ( SPhPs ) , which are analogous excitations associated with longitudinal acoustic waves 6 . These modes happen not only at surfaces but also inside bulk surfaces 7 , 8 where they may contribute to enhanced thermal transport 9 or thermoelectricity 10 .Moreover , SPhPs can close intensely to light 11 contributing to curious phenomena like superprism 12 and exceptional transmission 13 phenomena .",
        "rewrite_text": "**Title:** Surface Plasmon Polaritons and Surface Phonon Polaritons on Metallic and Semiconducting Objects: Exact and Semiclassical Descriptions\n\n**Abstract:** In this study, we provide detailed solutions for the electromagnetic field surrounding spherical objects characterized by arbitrary dielectric functions, encompassing both metallic and insulating materials. Our approach involves solving Maxwell's equations through a suitable Green's function method, which allows us to derive precise expressions for the electromagnetic behavior of these systems. These expressions facilitate the estimation of dispersion relations for surface plasmons (SPs) and surface phonon polaritons (SPhPs). Notably, our findings indicate that SPs are only present when the real part of the dielectric constant is zero, whereas SPhPs can exist even when the dielectric constant is positive. We also compare our results with those obtained from classical Drude theory, discussing the limitations of this traditional framework.\n\nSurface plasmons, which represent collective oscillations of conduction electrons at the interface between metals and dielectrics, have been extensively studied over the years due to their significant implications in various domains, including optics, electronics, sensing, and catalysis. Recently, there has been an increasing interest in surface phonon polaritons, which are analogous excitations linked to longitudinal acoustic waves. These modes not only manifest at surfaces but can also occur within the bulk of materials, potentially enhancing thermal transport and thermoelectric properties. Furthermore, SPhPs can interact closely with light, leading to intriguing phenomena such as superprism effects and exceptional transmission. This work aims to deepen the understanding of these polaritonic modes and their applications in advanced material science and nanotechnology.",
        "ori-fast-z-score": -0.9712858623572641,
        "water-fast-z-score": 5.581052602166382,
        "rewrite-fast-z-score": 0.5720775535473553
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Ratios of star cluster core and half-mass radii: a cautionary note on intermediate-mass black holes in star clusters .\nAbstract:\nWe present new results for the ratio between the core radius rc and the half-light radius r h of open star clusters, based on high-resolution HST imaging data obtained with the Advanced Camera for Surveys (ACS). We find that this ratio is systematically larger than previously reported values by about 0.2 dex. This discrepancy can be explained if we assume that some of these clusters contain an intermediate mass black hole at their center. The presence of such objects would lead to smaller observed values of rc/r h . Our findings are consistent with previous studies which have suggested that many open clusters may harbor IMBHs. In addition, our results provide further evidence against the hypothesis that all open clusters host central massive stars. \n \n Keywords: Open Star Cluster; Core Radius; Half-Mass Radius; Black Hole; Intermediate Mass Black Hole; ACS/HRC FOV; Galaxy",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Ratios of star cluster core and half - mass radii : a cautionary note on intermediate - mass black holes in star clusters . Abstract : We report new data for the proportion between the core radius rc and the half - light diameter r h of open star clusters , using on high - resolution HST scanning data acquired with the Advanced Camera for Surveys ( ACS ) .We see that this ratio is systematically larger than previously reported values by about 0 . 2 dex . This discrepancy can be understood if we suppose that some of these clusters include an intermediate mass black hole at their core .The presence of such objects would result to smaller observed values of rc / h h . Our findings are compatible with previous research which have suggested that several open nuclei may harbor IMBHs .In addition , our findings provide further evidence against the notion that all open clusters host central huge stars . Keywords : Open Star Cluster ; Core Radius ; Half - Mass Radius ; Black Hole ; Intermediate Mass Black Hole ; ACS / HRC FOV ; Galaxy",
        "rewrite_text": "Title: Ratios of Star Cluster Core and Half-Mass Radii: A Cautionary Note on Intermediate-Mass Black Holes in Star Clusters\n\nAbstract: In this study, we present new measurements of the ratio between the core radius (rc) and the half-light radius (rh) of open star clusters, utilizing high-resolution scanning data from the Hubble Space Telescope's Advanced Camera for Surveys (ACS). Our analysis reveals that the rc/rh ratio is consistently larger than previously documented values by approximately 0.2 dex. This significant discrepancy may be attributed to the potential presence of intermediate-mass black holes (IMBHs) at the centers of these clusters. The existence of IMBHs would lead to reduced observed values of the rc/rh ratio, as their gravitational influence alters the dynamics and structure of the star cluster. Our results align with earlier studies that have proposed the existence of IMBHs in several open cluster nuclei. Furthermore, our findings challenge the prevailing assumption that all open clusters contain a central population of massive stars, suggesting instead that some may be dominated by IMBHs. This research contributes to the ongoing discourse regarding the formation and evolution of star clusters and the role of black holes within them. By providing a more nuanced understanding of the core and half-mass radius ratios, we aim to refine the criteria for identifying the presence of IMBHs in star clusters and encourage further investigation into their implications for stellar dynamics and cluster evolution. \n\nKeywords: Open Star Cluster; Core Radius; Half-Mass Radius; Black Hole; Intermediate-Mass Black Hole; ACS/HRC FOV; Galaxy.",
        "ori-fast-z-score": -0.7592566023652966,
        "water-fast-z-score": 4.528976474544414,
        "rewrite-fast-z-score": 0.3849001794597505
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dislocation Dynamics in a Crystal Lattice (Peierls-Nabarro) Relief .\nAbstract:\nThe Peierls-Nabarro model is used to study the dislocations dynamics in a crystal lattice, where the energy barrier for glide motion and climb motion are calculated by using the concept of activation volume. The results show that the energy barriers increase with increasing applied stress. It also shows that the energy barrier decreases as temperature increases. Finally it can be concluded that the Peierls-Nabarre model gives good agreement between theory and experimentation. Keywords: Energy Barrier, Dislocation, Glide Motion, Climb Motion, Activation Volume, Peierls-Nabarrou Model. 1 Introduction In this research work we have studied the dislocation dynamics in a crystal lattice which has been done by using the Peierls-Nabbarro model  1  . This model was developed by Peierls  2  , who introduced an elastic strain field into the Frenkel-Kontorova model  3  .\nIn order to calculate the energy barrier for gliding motion and climbing motion, we use the concept of activation volume  4  . We find out how the energy barrier changes when different values of stresses are applied on the system. Also we found out how the energy barrier varies at different temperatures. Finally we compare our theoretical results with experimental data  5  -  8  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dislocation Dynamics in a Crystal Lattice ( Peierls - Nabarro ) Relief . Abstract : The Peierls - Nabarro theory is utilized to study the dislocations dynamics in a crystal lattice , where the power barrier for glide movement and ascent movement are measured by using the idea of activation volume .The results show that the power barriers increase with increasing applied strain . It additionally shows that the power barrier decreases as temperature increases .Finally it can be realized that the Peierls - Nabarre system gives excellent approval between theoretical and experimentation . Keywords : Energy Barrier , Dislocation , Glide Motion , Climb Motion , Activation Volume , Peierls - Nabarrou Model .1 Introduction In this research work we have researched the dislocation behavior in a crystal lattice which has been performed by using the Peierls - Nabbarro model 1 . This theory was developed by Peierls 2 , who applied an elastic strain field into the Frenkel - Kontorova model 3 .In order to estimate the electricity barrier for gliding motion and climbing movement , we utilize the idea of activation volume 4 . We get out how the power barrier changes when varying values of stresses are applied on the system .Also we learned out how the electricity barrier variations at different temperatures . Finally we compare our theory findings with experimental evidence 5 - 8 .",
        "rewrite_text": "**Title:** Dislocation Dynamics in a Crystal Lattice: Peierls-Nabarro Relief\n\n**Abstract:** This study employs the Peierls-Nabarro theory to investigate the dynamics of dislocations within a crystal lattice, focusing on the energy barriers associated with glide and climb movements. By introducing the concept of activation volume, we quantitatively assess the power barriers for these dislocation movements under varying conditions. Our findings reveal a direct correlation between the applied strain and the increase in power barriers, indicating that as the strain on the crystal lattice intensifies, the resistance to dislocation motion also rises. Conversely, we observe that elevating the temperature leads to a reduction in the power barriers, suggesting that thermal activation facilitates dislocation movement. This research highlights the effectiveness of the Peierls-Nabarro framework in bridging theoretical predictions with experimental observations, demonstrating its robustness in explaining dislocation behavior in crystalline materials. The implications of these results are significant for understanding material deformation mechanisms and enhancing the design of materials with improved mechanical properties. The keywords associated with this study include Energy Barrier, Dislocation, Glide Motion, Climb Motion, Activation Volume, and the Peierls-Nabarro Model. \n\n**1 Introduction:** In this research, we explore the behavior of dislocations in a crystal lattice through the lens of the Peierls-Nabarro model. Originally developed by Peierls, this theory integrates an elastic strain field into the Frenkel-Kontorova model to provide insights into dislocation dynamics. By employing the concept of activation volume, we analyze how the energy barriers for glide and climb movements vary with different stress levels applied to the system. Additionally, we investigate the influence of temperature on these energy barriers. Our theoretical findings are then compared with experimental data, further validating the Peierls-Nabarro model's applicability in understanding dislocation behavior.",
        "ori-fast-z-score": -0.7107423155935334,
        "water-fast-z-score": 6.331738236133036,
        "rewrite-fast-z-score": 0.5345224838248488
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  New AB-Thermonuclear Reactor for Aerospace .\nAbstract:\nThe new reactor is based on the principle that nuclear fusion can be achieved by heating plasma to high temperatures in magnetic fields.  The proposed design uses an array of magnets and electrodes, which are powered by microwave generators, to heat hydrogen gas into plasma at 100 million degrees Celsius (1,000,000 K). This temperature is sufficient to cause nuclear fusion reactions between deuterium nuclei inside the plasma. In this way, energy is released as neutrons and gamma rays. These particles then bombard fuel pellets made up of tritium and lithium-6, causing them to release more energy through further nuclear fusion reactions. The resulting power plant would produce electrical power with no moving parts or radioactive waste products. It could also provide propulsion systems for spacecraft without requiring any chemical fuels. A conceptual drawing of the reactor system is shown below. Nuclear fusion has been studied extensively since its discovery over 50 years ago. However, it remains elusive because the conditions required to achieve fusion are extremely difficult to create experimentally.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : New AB - Thermonuclear Reactor for Aerospace . Abstract : The proposed reactor is based on the principle that atomic fusion can be obtained by burning plasma to large conditions in magnetic fields .The proposed design uses an array of magnets and electrodes , which are powered by microwave generators , to heat hydrogen gas into plasma at 100 million degrees Celsius ( 1 , 000 , 000 K ) . This temperature is adequate to produce nuclear fusion events between deuterium nuclei inside the plasma .In this way , electricity is released as neutrons and alpha rays . These particles then bombard fuel pellets made up of tritium and lithium - 6 , forcing them to release more power through further nuclear fusion events .The resulting electricity plant would create electrical power with no moving parts or radioactive waste products . It could also supply propulsion services for spacecraft without need any chemical fuels .A conceptual drawing of the reactor system is displayed below . Nuclear fusion has been studied thoroughly since its revelation over 50 generations earlier .However , it remains elusive because the conditions utilized to achieve fusion are extremely difficult to create experimentally .",
        "rewrite_text": "**Title: New AB-Thermonuclear Reactor for Aerospace**\n\n**Abstract:** This article presents a novel thermonuclear reactor design aimed at advancing aerospace technology through the harnessing of nuclear fusion. The reactor operates on the principle of achieving atomic fusion by generating high-energy plasma within strong magnetic fields. The innovative design incorporates a sophisticated arrangement of magnets and electrodes, energized by microwave generators, to heat hydrogen gas to an astonishing temperature of 100 million degrees Celsius (1,000,000 K). This extreme thermal environment is conducive to facilitating nuclear fusion reactions between deuterium nuclei present in the plasma. As a result of these fusion events, the reactor produces energy in the form of neutrons and alpha particles. These energetic particles subsequently interact with fuel pellets composed of tritium and lithium-6, triggering additional fusion reactions that release even more energy.\n\nThe proposed reactor design promises to generate electricity without the need for moving parts or the generation of radioactive waste, marking a significant advancement in clean energy technology. Furthermore, it offers the potential for providing propulsion systems for spacecraft, eliminating the reliance on traditional chemical fuels. The article includes a conceptual illustration of the reactor system, highlighting its innovative features and operational principles.\n\nDespite extensive research into nuclear fusion over the past several decades, achieving the necessary conditions for sustained fusion reactions has proven to be a formidable challenge. This new reactor design aims to overcome these obstacles, paving the way for practical applications of fusion energy in aerospace and beyond. The implications of successful implementation could revolutionize energy production and propulsion methods, contributing to a more sustainable future in space exploration and energy generation.",
        "ori-fast-z-score": 0.8251369970070347,
        "water-fast-z-score": 5.9196002117260145,
        "rewrite-fast-z-score": 0.8951673046482753
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Prompt GeV-TeV Emission of Gamma-Ray Bursts Due to High-Energy Protons, Muons and Electron-Positron Pairs .\nAbstract:\nWe propose that the prompt emission of gamma-ray bursts (GRBs) is due to high-energy protons, muons and electron-positron pairs produced by ultra-relativistic shocks in GRB jets. The observed MeV-GeV spectrum can be explained as synchrotron radiation emitted by these particles accelerated at the shock front. We show that this model naturally explains why the peak energy of the observed spectrum decreases with time during the prompt phase. In addition, we find that our model predicts an anti-correlation between the duration of the prompt phase and the luminosity of the afterglow for short-hard GRBs. This prediction could be tested using future observations made by Fermi/LAT and Swift/BAT. Introduction -Gamma-ray bursts are brief flashes of high energy photons lasting only milliseconds or less  1  . They have been detected out to redshifts z = 8.2  2  , which implies their total energy output may exceed 10^53 erg  3  . Despite decades of research into the origin of GRBs there remains no consensus on how they work  4  .\nThe most popular models involve either black holes or neutron stars collapsing into a black hole  5  . However, it has recently become clear that many GRBs do not fit neatly into one category  6  . For example, some GRBs appear to contain two separate pulses  7, 8  while others exhibit extended periods of activity  9  . Furthermore, some GRBs seem to occur when two galaxies merge  10  . These complexities suggest that more than one mechanism might operate simultaneously  11  .\nIn recent years several authors  12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59   have proposed that GRBs are powered by relativistic jets e",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Prompt GeV - TeV Emission of Gamma - Ray Bursts Due to High - Energy Protons , Muons and Electron - Positron Pairs . Abstract : We suggest that the prompt emission of gamma - ray bursts ( GRBs ) is due to large - energy protons , muons and electron - positron couples created by ultra - relativistic shocks in GRB jets .The observed MeV - GeV spectrum can be understood as synchrotron emission generated by these objects accelerated at the shock front . We see that this description readily explains why the maximum energy of the seen spectrum drops with time during the prompt phase .In addition , we find that our model predicts an counter - correlation between the duration of the prompt phase and the luminosity of the afterglow for short - hard GRBs . This prediction might be evaluated using later observations made by Fermi / LAT and Swift / BAT .Introduction - Gamma - ray bursts are mild flashes of high energy photons lasting only milliseconds or less 1 . They have been detected out to redshifts z = 8 . 2 2 , which implies their total energy output may exceed 10 ^ 53 erg 3 .Despite decades of research into the origin of GRBs there exists no discussion on how they work 4 . The most popular theories include either black holes or neutron galaxies exploding into a black hole 5 .However , it has recently become clear that several GRBs do not fit neatly into one category 6 . For instance , some GRBs occur to contain two separate pulses 7 , 8 while several display extended phases of intensity 9 .Furthermore , some GRBs appeared to arise when two galaxies merge 10 . These complexities indicate that more than one process may operate simultaneously 11 .In recent years many writers 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 , 31 , 32 , 33 , 34 , 35 , 36 , 37 , 38 , 39 , 40 , 41 , 42 , 43 , 44 , 45 , 46 , 47 , 48 , 49 , 50 , 51 , 52 , 53 , 54 , 55 , 56 , 57 , 58 , 59 have proposed that GRBs are powered by relativistic jets e",
        "rewrite_text": "**Title:** Prompt GeV-TeV Emission of Gamma-Ray Bursts Due to High-Energy Protons, Muons, and Electron-Positron Pairs\n\n**Abstract:** In this study, we propose that the prompt emission observed in gamma-ray bursts (GRBs) is primarily driven by high-energy protons, muons, and electron-positron pairs generated through ultra-relativistic shocks within the jets of GRBs. The resulting MeV-GeV spectrum can be effectively interpreted as synchrotron radiation produced by these particles, which are accelerated at the shock front. Our analysis reveals that this framework accounts for the observed phenomenon where the maximum energy of the emitted spectrum diminishes over time during the prompt emission phase. Furthermore, our model predicts a counter-correlation between the duration of the prompt phase and the luminosity of the afterglow specifically for short-hard GRBs. This intriguing prediction can be tested through subsequent observations from instruments such as Fermi/LAT and Swift/BAT.\n\nGamma-ray bursts are brief yet intense flashes of high-energy photons that typically last for milliseconds or less, with detections extending to redshifts of z = 8.2, suggesting that their total energy output may surpass 10^53 erg. Despite extensive research into the mechanisms behind GRBs, a comprehensive understanding of their origins remains elusive. The prevailing theories often attribute these bursts to phenomena involving black holes or neutron stars collapsing into black holes. However, recent observations indicate that many GRBs do not conform to a singular model. For example, certain GRBs exhibit multiple pulses, while others display prolonged periods of heightened intensity. Additionally, some bursts appear to be associated with the merger of two galaxies. These complexities suggest that multiple processes may be at play in the generation of GRBs. Recent literature has increasingly focused on the role of relativistic jets in powering these enigmatic events, highlighting the need for further investigation into the underlying mechanisms driving GRB emissions.",
        "ori-fast-z-score": -0.4583492485141057,
        "water-fast-z-score": 7.061788191316445,
        "rewrite-fast-z-score": 1.3821894809301762
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  SDSS J121811.0+465501.2: a new Low Surface Brightness Galaxy with low metallicity .\nAbstract:\nSDSS J121811+465501 is an extremely faint galaxy discovered in the Sloan Digital Sky Survey (SDSS). It has been classified as a dwarf elliptical galaxy, but its surface brightness profile shows that it is actually a very extended and diffuse object. We have obtained deep optical spectroscopy for this galaxy using the Gemini Multi-Object Spectrograph on Gemini North telescope to study its physical properties. The spectrum reveals strong Balmer absorption lines indicating young stellar populations. Its oxygen abundance is found to be one third solar or lower, which makes SDSS J121811+ 465501 the lowest-metallicity galaxy known at z=0.1. This galaxy may represent a population of galaxies formed early during cosmic time when star formation was more intense than today. Keywords: Dwarf Elliptical Galaxy, Oxygen Abundance, Gemini Observatory, Sloan Digital Sky Survey, Low Surface Brightness",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : SDSS J121811 . 0 + 465501 . 2 : a new Low Surface Brightness Galaxy with poor metallicity . Abstract : SDSS J121811 + 465501 is an incredibly faint universe discovered in the Sloan Digital Sky Survey ( SDSS ) .It has been classified as a dwarf elliptical galaxy , but its surface brightness profile reveals that it is actually a very extended and diffuse object . We have achieved deep optical spectroscopy for this galaxy using the Gemini Multi - Object Spectrograph on Gemini North telescope to study its physical properties .The spectrum reveals large Balmer absorption patterns indicating new stellar regions . Its oxygen abundance is found to be one third solar or smaller , which makes SDSS J121811 + 465501 the smallest - metallicity galaxy known at z = 0 . 1 .This galaxy might represent a population of galaxies formed earliest during cosmic time when star formation was more intense than tomorrow . Keywords : Dwarf Elliptical Galaxy , Oxygen Abundance , Gemini Observatory , Sloan Digital Sky Survey , Low Surface Brightness",
        "rewrite_text": "Title: SDSS J121811.0 + 465501.2: A Newly Discovered Low Surface Brightness Galaxy with Low Metallicity\n\nAbstract: The galaxy SDSS J121811.0 + 465501.2, identified through the Sloan Digital Sky Survey (SDSS), is an exceptionally faint celestial body that has garnered attention due to its unique characteristics. Initially classified as a dwarf elliptical galaxy, further analysis of its surface brightness profile indicates that it is, in fact, a highly extended and diffuse structure. To investigate its physical properties in greater detail, we conducted deep optical spectroscopy using the Gemini Multi-Object Spectrograph on the Gemini North telescope. The resulting spectral data revealed pronounced Balmer absorption features, suggesting the presence of newly formed stellar regions within the galaxy. Notably, our findings indicate that the oxygen abundance in SDSS J121811.0 + 465501.2 is approximately one-third of the solar value or even lower, establishing it as the galaxy with the lowest metallicity known at redshift z = 0.1. This discovery is significant as it may represent a population of galaxies that formed during the early epochs of cosmic history, a period characterized by intense star formation activity. The implications of this research extend to our understanding of galaxy formation and evolution, particularly in relation to low surface brightness galaxies and their role in the broader cosmic landscape. \n\nKeywords: Dwarf Elliptical Galaxy, Oxygen Abundance, Gemini Observatory, Sloan Digital Sky Survey, Low Surface Brightness.",
        "ori-fast-z-score": -0.12403473458920847,
        "water-fast-z-score": 4.185110693297313,
        "rewrite-fast-z-score": 1.949358868961793
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Cooling and heating by adiabatic magnetization in the Ni$_{50}$Mn$_{34}$In$_{16}$ magnetic shape memory alloy .\nAbstract:\nThe effect of cooling rate on martensitic transformation temperature (Mf) was investigated for Ni$_{50}$ Mn$_{34}$ In$_{16}$ alloy using differential scanning calorimetry (DSC). The results show that Mf decreases with increasing cooling rates, which is attributed to the increase in nucleation sites at higher cooling rates. A comparison between DSC data obtained under different conditions shows that the presence of stress during cooling has no significant influence on the value of Mf. However, it does affect the microstructure of the material as revealed by transmission electron microscopy (TEM), where the formation of dislocations can be observed when samples are cooled down without applying any external pressure. It also affects the mechanical properties such as yield strength and ultimate tensile strength. \n \n © 2014 Elsevier B.V. \nKeywords: Magnetic shape memory alloys; Cooling rate; Martensitic transformation",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Cooling and heating by adiabatic magnetization in the Ni $ _ { 50 } $ Mn $ _ { 34 } $ In $ _ { 16 } $ magnetic shape memory alloy . Abstract : The impact of cooling frequency on martensitic transformation temperature ( Mf ) was investigated for Ni $ _ { 50 } $ Mn $ _ { 34 } $ In $ _ { 16 } $ metal using differential scanning calorimetry ( DSC ) .The results show that Mf falls with higher cooling rates , which is attributed to the improvement in nucleation sites at higher cooling rates . A comparison between DSC data derived under various circumstances reveals that the presence of stress during cooling has no important affect on the value of Mf .However , it does affect the microstructure of the material as revealed by transmission electron microscopy ( TEM ) , where the formation of dislocations can be viewed when samples are heated down without applying any external stress . It additionally impacts the thermal properties such as yield strength and absolute tensile strength .© 2014 Elsevier B . V . Keywords : Magnetic shape memory alloys ; Cooling rate ; Martensitic transformation",
        "rewrite_text": "**Title:** Cooling and Heating through Adiabatic Magnetization in Ni$_{50}$Mn$_{34}$In$_{16}$ Magnetic Shape Memory Alloy\n\n**Abstract:** This study explores the influence of cooling frequency on the martensitic transformation temperature (Mf) in the Ni$_{50}$Mn$_{34}$In$_{16}$ magnetic shape memory alloy, utilizing differential scanning calorimetry (DSC) for analysis. The findings indicate a notable decrease in Mf with increased cooling rates, which is attributed to an enhancement in the nucleation sites that facilitate the transformation process at these higher rates. A comparative analysis of DSC data obtained under varying conditions demonstrates that while the presence of stress during the cooling phase does not significantly alter the Mf value, it does have a pronounced effect on the material's microstructure. This is evidenced by transmission electron microscopy (TEM) observations, which reveal the emergence of dislocations in samples that are heated without the application of external stress. Furthermore, the study highlights the implications of cooling rates on the thermal properties of the alloy, including yield strength and absolute tensile strength. These insights contribute to a deeper understanding of the behavior of magnetic shape memory alloys under different thermal conditions, paving the way for potential applications in smart materials and actuators. The results underscore the complex interplay between cooling dynamics and material properties, emphasizing the need for careful consideration of thermal treatment processes in the development and optimization of these advanced materials. © 2014 Elsevier B.V. \n\n**Keywords:** Magnetic shape memory alloys; Cooling rate; Martensitic transformation.",
        "ori-fast-z-score": -0.8427009716003844,
        "water-fast-z-score": 4.00693842672377,
        "rewrite-fast-z-score": 1.0536089137432665
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Coulomb gauge Gribov copies and the confining potential .\nAbstract:\nWe study the effect of Coulomb gauge fixing on the gluon propagator in Landau gauge, using lattice simulations with two different actions for SU(2) Yang-Mills theory. We find that the inclusion of Gribov copies leads to an increase in the infrared strength of the gluon propagator; this is consistent with earlier results obtained by other groups. \n \n In addition we show how the presence of Gribov copies affects the static quark-antiquark potential at large distances. The latter quantity can be extracted from the correlation function of Polyakov loops which are defined as closed lines winding around the periodic spatial directions of the lattice. This allows us to compare our results directly with those obtained previously within perturbation theory. Our findings suggest that the perturbative approach breaks down when one considers the full non-perturbative effects associated with Gribov copies. Finally, we discuss possible implications of these results for phenomenological studies of QCD.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Coulomb gauge Gribov copies and the confining potential . Abstract : We research the impact of Coulomb gauge fixing on the gluon propagator in Landau gauge , using lattice simulations with two different actions for SU ( 2 ) Yang - Mills theory .We see that the inclusion of Gribov copies results to an increase in the infrared strength of the gluon propagator ; this is compatible with previous findings obtained by other groups . In addition we find how the presence of Gribov copies impacts the static quark - antiquark capacity at large distances .The latter quantity can be extracted from the correlation function of Polyakov circuits which are specified as closed lines winding around the periodic spatial directions of the lattice . This enables us to relate our findings directly with those achieved previously within perturbation theory .Our findings show that the perturbative methodology cracks down when one considers the full non - perturbative impacts associated with Gribov copies . Finally , we explain possible possibilities of these results for phenomenological experiments of QCD .",
        "rewrite_text": "In this study, we investigate the effects of Coulomb gauge fixing on the gluon propagator within the context of Landau gauge, utilizing lattice simulations based on two distinct actions for SU(2) Yang-Mills theory. Our analysis reveals that the incorporation of Gribov copies leads to an enhancement of the infrared behavior of the gluon propagator, a result that aligns with earlier observations reported by other research groups. Furthermore, we explore how Gribov copies influence the static quark-antiquark potential at large separations. This potential can be derived from the correlation functions of Polyakov loops, which are defined as closed paths that encircle the periodic spatial dimensions of the lattice. This approach allows us to connect our results with those obtained through perturbation theory, highlighting the discrepancies that arise when considering the complete non-perturbative effects associated with Gribov copies. Our findings indicate that the perturbative framework becomes inadequate when addressing the full range of non-perturbative phenomena linked to these copies. Finally, we discuss the implications of our results for phenomenological studies in Quantum Chromodynamics (QCD), suggesting avenues for future experimental investigations that could further elucidate the role of Gribov copies in the confinement mechanism.",
        "ori-fast-z-score": -1.3587324409735149,
        "water-fast-z-score": 4.302652729749464,
        "rewrite-fast-z-score": -0.5488212999484517
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Scaling laws for soliton pulse compression by cascaded quadratic nonlinearities .\nAbstract:\nWe present scaling laws that govern the performance and design trade-offs in all-optical soliton pulse compressors based on cascading quadratic nonlinearities, such as self-phase modulation (SPM) followed by cross-phase modulation (XPM). We show how these devices can be designed to operate at high repetition rates with low loss while maintaining their ability to produce compressed pulses. The results are derived analytically using perturbation theory and confirmed numerically through simulations. Our analysis shows that the maximum achievable peak power is limited primarily by SPM-induced spectral broadening; however, this limit may be overcome if XPM is used to compensate for the increased bandwidth associated with higher-order dispersion effects. In addition, we find that the minimum required length scales inversely proportional to the square root of the input pulse energy. Finally, our results indicate that the optimal operating conditions depend strongly on the desired output parameters. \n \n © 2009 Optical Society",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Scaling rules for soliton pulse compression by cascaded quadratic nonlinearities . Abstract : We report scaling principles that govern the performance and design trade - offs in all - optical soliton pulse compressors based on cascading quadratic nonlinearities , such as self - phase modulation ( SPM ) preceded by inter - phase modulation ( XPM ) .We see how these machines can be designed to work at high repetition rates with little loss while maintaining their ability to produce compressed signals . The results are derived analytically utilizing perturbation theory and reported numerically through simulations .Our study shows that the maximum achievable peak power is limited primarily by SPM - caused spectral broadening ; however , this limit might be overcome if XPM is utilized to compensate for the increased frequencies associated with higher - order dispersion interactions . In addition , we find that the limit required width scales inversely proportional to the square root of the input pulse power .Finally , our findings confirm that the ideal operating circumstances rely highly on the desired product parameters . © 2009 Optical Society",
        "rewrite_text": "In this article, we explore the scaling principles that dictate the performance and design considerations of all-optical soliton pulse compressors that utilize cascaded quadratic nonlinearities, specifically focusing on self-phase modulation (SPM) and cross-phase modulation (XPM). Our research highlights the potential for designing these systems to operate efficiently at high repetition rates while minimizing losses, all while effectively generating compressed optical signals. We derive our findings through analytical methods grounded in perturbation theory, complemented by numerical simulations that validate our theoretical predictions.\n\nOur analysis reveals that the maximum achievable peak power in these soliton pulse compressors is predominantly constrained by spectral broadening induced by SPM. However, we propose that this limitation can be mitigated through the strategic application of XPM, which can counterbalance the frequency shifts that arise from higher-order dispersion effects. Furthermore, we establish that the necessary pulse width for optimal performance scales inversely with the square root of the input pulse power, indicating a critical relationship between these parameters.\n\nUltimately, our results underscore the importance of tailoring the operating conditions to align with the specific desired output characteristics of the pulse compressor. This work contributes to a deeper understanding of the interplay between nonlinear optical effects in soliton pulse compression and offers valuable insights for the design of advanced optical systems. The implications of our findings are significant for the development of high-performance optical communication technologies. © 2009 Optical Society.",
        "ori-fast-z-score": -0.42640143271122083,
        "water-fast-z-score": 6.253987492037524,
        "rewrite-fast-z-score": 2.6943012562182536
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Theoretical Resolution of Magnetic Reconnection in High Energy Plasmas .\nAbstract:\nThe magnetic reconnection is one of the most important processes for understanding many phenomena observed in space and laboratory plasmas, such as solar flares, magnetospheric substorms, sawtooth crashes in tokamaks etc.. In this work we present an analytical model which describes the process of magnetic reconnection in collisionless high energy plasma with arbitrary initial conditions. The main idea behind our approach consists in using the concept of generalized force density tensor (GDFT) introduced by MHD theory. We show that GDFT can be used not only to describe the macroscopic dynamics but also microscopic properties of the system like particle distribution functions. Our results are compared with those obtained within other approaches based on kinetic description of particles motion. It turns out that all these models give similar predictions when applied to simple cases where the initial state has no gradients along the direction perpendicular to the background magnetic field. However, if there exist some gradients across the magnetic field lines then different models predict quite different behavior.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Theoretical Resolution of Magnetic Reconnection in High Energy Plasmas . Abstract : The magnetic reconnection is one of the most important processes for studying many phenomena observed in space and lab plasmas , such as sun flares , magnetospheric substorms , sawtooth crashes in tokamaks etc . .In this study we present an analytical theory which explains the process of magnetic reconnection in collisionless high energy plasma with arbitrary initial conditions . The main idea behind our approach consists in utilizing the idea of generalized force density tensor ( GDFT ) developed by MHD physics .We see that GDFT can be used not only to explain the macroscopic behavior but also microscopic characteristics of the system like particle distribution functions . Our results are compared with those achieved within other methods based on kinetic representation of waves movement .It turns out that all these models make comparable predictions when applied to basic cases where the first state has no gradients along the direction perpendicular to the background magnetic force . However , if there exist some gradients across the magnetic field lines then various models predict really different properties .",
        "rewrite_text": "Title: Theoretical Resolution of Magnetic Reconnection in High Energy Plasmas\n\nAbstract: Magnetic reconnection is a critical phenomenon that plays a significant role in understanding various events observed in both astrophysical and laboratory plasmas, including solar flares, magnetospheric substorms, and sawtooth crashes in tokamaks. In this article, we introduce a comprehensive analytical theory that elucidates the mechanism of magnetic reconnection in collisionless high-energy plasmas, accommodating arbitrary initial conditions. Central to our approach is the application of the generalized force density tensor (GDFT), a concept derived from magnetohydrodynamics (MHD). Our findings demonstrate that GDFT is effective not only in describing the macroscopic dynamics of the plasma but also in capturing the microscopic characteristics, such as particle distribution functions.\n\nWe conduct a comparative analysis of our results against those obtained from alternative methodologies that utilize kinetic representations of wave propagation. Our investigation reveals that while these various models yield similar predictions in fundamental scenarios where the initial state lacks gradients perpendicular to the background magnetic field, significant discrepancies arise when gradients are present across the magnetic field lines. This divergence underscores the importance of considering the specific conditions of the plasma environment when analyzing magnetic reconnection processes. Our theoretical framework provides valuable insights into the complexities of magnetic reconnection in high-energy plasmas and highlights the necessity for a nuanced understanding of the interplay between macroscopic and microscopic phenomena in plasma physics. This work contributes to the broader discourse on plasma behavior and offers a foundation for future research aimed at unraveling the intricacies of magnetic reconnection in diverse plasma contexts.",
        "ori-fast-z-score": -0.953998092005724,
        "water-fast-z-score": 5.692099788303083,
        "rewrite-fast-z-score": 1.4855627054164149
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Kinetic-Ion Simulations Addressing Whether Ion Trapping Inflates Stimulated Brillouin Backscattering Reflectivities .\nAbstract:\nWe report on kinetic-ion simulations addressing whether ion trapping inflates stimulated Brillouin backscattering (SBS) reflectivities in the presence of an electron beam and plasma waves. We find that, for typical parameters relevant to high-power laser-plasma experiments, SBS is dominated by electrostatic Langmuir wave instabilities rather than ion-acoustic modes. The latter are suppressed due to Landau damping as well as mode conversion into electromagnetic radiation at oblique angles with respect to the direction of propagation. In addition, we show that the effect of ion trapping can be neglected if the density fluctuations associated with the trapped ions are small compared to those caused by the electrons. Finally, we demonstrate that the inclusion of ion trapping does not significantly affect the growth rates or saturation levels of the dominant electrostatic Langmuir waves. This finding suggests that the observed discrepancies between theory predictions and experimental results may originate from other effects such as nonlocality and/or nonlinear coupling among different types of waves.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Kinetic - Ion Simulations Addressing Whether Ion Trapping Inflates Stimulated Brillouin Backscattering Reflectivities . Abstract : We report on kinetic - ion simulations addressing whether electron trapping inflates stimulated Brillouin backscattering ( SBS ) reflectivities in the presence of an electron beam and plasma beams .We see that , for typical characteristics applicable to large - speed laser - plasma experiments , SBS is dominated by electrostatic Langmuir wave instabilities rather than ion - acoustic modes . The latter are suppressed due to Landau damping as well as mode conversion into electromagnetic radiation at oblique directions with regard to the direction of propagation .In addition , we prove that the impact of ion traps can be forgotten if the density fluctuations associated with the captured atoms are small relative to those generated by the electrons . Finally , we prove that the inclusion of ion traps does not dramatically impact the development rates or saturation levels of the dominant electrostatic Langmuir waves .This found shows that the reported discrepancies between theoretical estimates and theoretical results may originate from other effects such as nonlocality and / or nonlinear coupling among different kinds of waves .",
        "rewrite_text": "We present a comprehensive analysis through kinetic-ion simulations to investigate the role of ion trapping in the inflation of stimulated Brillouin backscattering (SBS) reflectivities, particularly in scenarios involving electron beams and plasma beams. Our findings indicate that, under conditions typical of high-speed laser-plasma experiments, the dynamics of SBS are primarily governed by electrostatic Langmuir wave instabilities, overshadowing the influence of ion-acoustic modes. The latter modes are notably suppressed due to mechanisms such as Landau damping and the conversion of energy into electromagnetic radiation at angles oblique to the wave propagation direction. \n\nFurthermore, our simulations demonstrate that the effects of ion trapping can be considered negligible when the density fluctuations associated with trapped ions are significantly smaller than those produced by the electrons. This suggests that the presence of ion traps does not substantially alter the growth rates or saturation levels of the predominant electrostatic Langmuir waves. \n\nConsequently, our results imply that the discrepancies observed between theoretical predictions and experimental outcomes may stem from alternative phenomena, including nonlocal effects and nonlinear interactions among various wave types. This work contributes to a deeper understanding of the underlying mechanisms of SBS in plasma physics and highlights the need to consider a broader range of factors when reconciling theoretical models with experimental data.",
        "ori-fast-z-score": -0.21566554640687682,
        "water-fast-z-score": 5.822969752985674,
        "rewrite-fast-z-score": -0.6704783996548059
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Hedging our bets: the expected contribution of species to future phylogenetic diversity .\nAbstract:\nWe present an approach for quantifying how much evolutionary history is likely to be lost if we lose particular species, and use this information to prioritize conservation efforts. We show that by considering both taxonomic and phylogenetic diversity simultaneously in conservation planning, it may be possible to conserve more biodiversity than would otherwise be achieved with either measure alone. \n \n The loss of any single species represents not only its own extinction but also the loss of all unique genetic variation within that lineage. This can have important consequences on ecosystem function as well as other aspects of biodiversity such as taxonomy or community composition. However, there are many ways to define what constitutes  biodiversity , each emphasizing different components of biological systems. In order to effectively protect biodiversity, it will therefore be necessary to consider multiple measures together rather than focusing solely on one aspect at a time. \n \n Here we propose a new method for measuring the amount of evolutionary history represented by a set of taxa (e.g., species) based on their relationships inferred using molecular data. Our approach uses the concept of  evolutionary distinctiveness  - which describes the uniqueness of each taxon relative to others in terms of shared evolutionary history - to calculate the expected contribution of individual species to overall phylogenetic diversity. By combining these values into a single index, we obtain a quantitative ranking of species according to their importance for preserving evolutionary history across a given taxonomic group. Using simulated datasets, we demonstrate that our proposed metric performs better than existing methods when used to identify key species for conserving phylogenetic diversity. Finally, we apply our method to assess the vulnerability of amphibian species to climate change impacts.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Hedging our bets : the expected contribution of species to future evolutionary evolution . Abstract : We present an approach for quantifying how many evolutionary history is probably to be lost if we lose particular species , and use this data to prioritize protection strategies .We suggest that by examining both taxonomic and evolutionary diversity simultaneously in conservation plan , it could be possible to conserve more biodiversity than would normally be achieved with either measure alone . The losing of any single species represents not only its own extinction but also the losing of all unique evolutionary differences within that lineage .This can have important implications on ecosystem function as well as other components of ecosystems such as taxonomy or community structure . However , there are many ways to define what creates biodiversity , each emphasizing different components of biological functions .In order to effectively protect biodiversity , it will consequently be required to consider multiple measures together rather than focusing solely on one element at a time . Here we propose a new method for calculated the extent of evolutionary history displayed by a setting of taxa ( e . g . , species ) based on their connections inferred using molecular data .Our concept employs the notion of evolutionary distinctiveness - which explains the uniqueness of each taxon relative to others in terms of shared evolutionary history - to estimate the expected contribution of individual species to overall evolutionary diversity . By combining these estimates into a single index , we obtain a empirical ranking of taxa according to their importance for preserving evolutionary history across a given taxonomic group .Using simulated datasets , we prove that our proposed measure works better than existing techniques when utilized to identify key taxa for conserving evolutionary wealth . Finally , we apply our technique to examine the vulnerability of amphibian species to climate transition effects .",
        "rewrite_text": "Title: Hedging Our Bets: The Expected Contribution of Species to Future Evolutionary Trajectories\n\nAbstract: In this study, we introduce a novel methodology for assessing the potential loss of evolutionary history associated with the extinction of specific species, and we leverage this information to inform conservation prioritization strategies. Our approach advocates for a dual consideration of both taxonomic and evolutionary diversity within conservation frameworks, positing that such an integrated perspective can enhance biodiversity preservation beyond what is achievable through either metric in isolation. The extinction of a single species signifies not only the loss of that organism but also the eradication of its unique evolutionary lineage, which can have profound repercussions on ecosystem functionality and the intricate relationships within community structures. Given the multifaceted nature of biodiversity, characterized by various definitions that highlight different biological functions, it is imperative to adopt a comprehensive approach that encompasses multiple dimensions of biodiversity rather than concentrating on singular aspects.\n\nWe propose a method to quantify the evolutionary history represented by a set of taxa (e.g., species) based on their phylogenetic relationships inferred from molecular data. Central to our framework is the concept of evolutionary distinctiveness, which measures the uniqueness of each taxon in relation to others through their shared evolutionary lineage. This allows us to estimate the expected contribution of individual species to the overall evolutionary diversity of a given taxonomic group. By synthesizing these contributions into a unified index, we generate an empirical ranking of taxa that highlights their significance in preserving evolutionary history.\n\nThrough simulations, we demonstrate that our proposed measure outperforms existing methodologies in identifying key taxa crucial for conserving evolutionary heritage. Furthermore, we apply our technique to assess the vulnerability of amphibian species in the context of climate change, illustrating its practical implications for conservation efforts. This research underscores the necessity of integrating evolutionary considerations into biodiversity conservation strategies to effectively safeguard the planet's biological legacy.",
        "ori-fast-z-score": -0.7808688094430304,
        "water-fast-z-score": 7.496340570653091,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Stray-fields-based magnetoresistance mechanism in Ni80Fe20-Nb-Ni80Fe20 trilayers .\nAbstract:\nWe report on the observation of giant magnetoresistance (GMR) effect in Ni80Fe20/Nb/Ni80Fe20 magnetic tunnel junctions with an ultrathin Nb spacer layer, which is as thin as 1 nm and 2 nm respectively. The GMR ratio can reach up to ~80% at room temperature for both samples. We propose that this large GMR effect originates mainly from spin dependent scattering between two adjacent ferromagnetic layers through the stray fields generated by one ferromagnet into another. This work may provide new insights into understanding the physics behind the spin-dependent transport properties in magnetic tunnel junctions. \n \n Magnetic tunnel junction (MTJ), consisting of two ferromagnets separated by a very thin insulating barrier, has been widely studied due to its potential applications in high density nonvolatile memories  1  . In recent years, MTJs have attracted much attention because they are promising candidates for next generation spintronic devices such as read heads  2  , microwave oscillators  3  , logic circuits  4  , etc.. However, there still exist some problems preventing their practical application, e.g., low thermal stability  5  , poor reproducibility  6  , and relatively small magnetoresistive effects  7, 8  .\nRecently, it was found that the interlayer exchange coupling plays an important role in determining the magnetization reversal process  9  . It also affects the spin-dependent transport behavior significantly  10  . Therefore, many efforts have been made to enhance the interlayer exchange coupling strength  11  -  13  . For example, using CoFeB/MgO/CoFeB structure instead of conventional FeCo/AlOx/FeCo structure could greatly increase the interlayer exchange coupling  14  . Moreover, inserting a non-magnetic metal layer like Cu or Ag between two ferromagnetic layers would lead to stronger interlayer exchange coupling  15  . On the other hand, inserting a non-magnetically conducting material like SiO2  16  or Al2O3  17  between two ferromagnetic layers will decrease the interlayer exchange coupling.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Stray - fields - based magnetoresistance mechanism in Ni80Fe20 - Nb - Ni80Fe20 trilayers . Abstract : We report on the observation of giant magnetoresistance ( GMR ) effect in Ni80Fe20 / Nb / Ni80Fe20 magnetic corridor junctions with an ultrathin Nb spacer coating , which is as thicker as 1 nm and 2 nm respectively .The GMR ratio can reach up to ~ 80 % at room temperature for both samples . We suggest that this big GMR influence originates mainly from spinning dependent scattering between two adjacent ferromagnetic layers through the stray fields generated by one ferromagnet into another .This research could give novel knowledge into studying the physics behind the spin - dependent transport properties in magnetic tunnel junctions . Magnetic tunnel junction ( MTJ ) , consisting of two ferromagnets connected by a very thin insulating barrier , has been widely explored thanks to its potential applications in high density nonvolatile memories 1 .In recent years , MTJs have garnered considerable scrutiny because they are promising candidates for next generation spintronic systems such as read heads 2 , microwave oscillators 3 , logic devices 4 , etc . . However , there still arise some problems preventing their practical use , e . g . , low heat strength 5 , poor reproducibility 6 , and fairly little magnetoresistive factors 7 , 8 .Recently , it was shown that the interlayer exchange interactions plays an important role in determining the magnetization reversal process 9 . It especially affects the spin - dependent transport behavior dramatically 10 .Therefore , various efforts have been made to alter the interlayer exchange bonding strength 11 - 13 . For instance , using CoFeB / MgO / CoFeB structure instead of standard FeCo / AlOx / FeCo structure could greatly increase the interlayer exchange bonding 14 .Moreover , inserting a non - magnetic metal layer like Cu or Ag between two ferromagnetic layers would lead to greater interlayer exchange interactions 15 . On the other hand , inserting a non - magnetically conducting metal like SiO2 16 or Al2O3 17 between two ferromagnetic layers will decrease the interlayer exchange bonding .",
        "rewrite_text": "We present our findings on the significant giant magnetoresistance (GMR) effect observed in Ni80Fe20/Nb/Ni80Fe20 trilayers, specifically in magnetic corridor junctions that utilize ultrathin Nb spacer layers of 1 nm and 2 nm thickness. Remarkably, the GMR ratio achieved in both configurations reaches approximately 80% at room temperature. Our analysis suggests that this substantial GMR effect primarily arises from spin-dependent scattering occurring between the adjacent ferromagnetic layers, facilitated by stray fields generated by one ferromagnet influencing the other. This research contributes valuable insights into the underlying physics of spin-dependent transport phenomena in magnetic tunnel junctions (MTJs). MTJs, which consist of two ferromagnetic layers separated by a thin insulating barrier, have been extensively studied due to their potential applications in high-density nonvolatile memory technologies. In recent years, MTJs have attracted significant attention as promising candidates for next-generation spintronic devices, including read heads, microwave oscillators, and logic components. However, several challenges remain that hinder their practical implementation, such as low thermal stability, inconsistent reproducibility, and limited magnetoresistive performance. Recent studies have highlighted the critical role of interlayer exchange interactions in influencing the magnetization reversal process, which in turn has a profound impact on spin-dependent transport characteristics. Consequently, various strategies have been explored to enhance interlayer exchange coupling. For example, replacing the conventional FeCo/AlOx/FeCo structure with a CoFeB/MgO/CoFeB configuration has been shown to significantly improve interlayer exchange strength. Additionally, incorporating non-magnetic metals like Cu or Ag between ferromagnetic layers can enhance interlayer exchange interactions, while the introduction of non-conductive materials such as SiO2 or Al2O3 tends to weaken these interactions. This study not only elucidates the mechanisms behind GMR in trilayer structures but also paves the way for future advancements in spintronic applications.",
        "ori-fast-z-score": -0.08192319205190406,
        "water-fast-z-score": 6.32663542409974,
        "rewrite-fast-z-score": 1.5105264449340403
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Nonlinear Perturbations and Conservation Laws on Curved Backgrounds in GR and Other Metric Theories .\nAbstract:\nWe study the nonlinear perturbations of general relativity (GR) and other metric theories of gravity, focusing on their effects on conserved quantities such as energy-momentum tensors. We show that these perturbations can be decomposed into two parts: one is associated with the background geometry while another is related to the perturbation itself. In particular, we find that for any given background solution there exists an infinite number of solutions corresponding to different values of the conserved quantity. This implies that the conservation laws are not preserved under small perturbations. Furthermore, we demonstrate how this effect may lead to violations of the weak equivalence principle. Finally, we discuss possible implications of our results for cosmology and black hole physics. General relativity (GR), which describes gravitational interactions at large scales, has been tested extensively against observations over many decades  1  . However, it remains unclear whether or not GR also holds true at smaller length scales where quantum mechanics becomes important  2  .\nIn order to address this question, several alternative theories have been proposed  3  , including scalar-tensor theories  4  , f(R)-gravity  5  , massive gravity  6  , and so forth  7, 8  . These theories typically involve additional degrees of freedom beyond those present in GR  9  . For example, in scalar-tensor theories, the graviton acquires a mass through its coupling to a scalar field  10  . Similarly, in f(R)-theories  11  , the Einstein-Hilbert action contains higher-order curvature terms  12  . It turns out that both types of theories admit self-accelerating solutions  13  , i.e., de Sitter-like solutions without requiring dark energy  14  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Nonlinear Perturbations and Conservation Laws on Curved Backgrounds in GR and Other Metric Theories . Abstract : We research the nonlinear perturbations of general relativity ( GR ) and other metric explanations of gravitational , concentrating on their impacts on conserved quantities such as energy - momentum tensors .We see that these perturbations can be decomposed into two parts : one is associated with the background geometry while another is related to the perturbation itself . In particular , we find that for any given background solution there exists an endless number of solutions corresponding to different values of the conserved quantity .This implies that the conservation states are not preserved under small perturbations . Furthermore , we prove how this effect could lead to infringement of the weak equivalence principle .Finally , we explain possible implications of our findings for cosmology and dark hole physics . General relativity ( GR ) , which explains gravitational interactions at large scales , has been tested extensively against measurements over numerous centuries 1 .However , it remains unsure whether or not GR actually holds true at smaller length scales where quantum mechanics becomes important 2 . In order to meet this question , various alternative theories have been proposed 3 , notably scalar - vector models 4 , f ( R ) - gravity 5 , giant gravity 6 , and so forth 7 , 8 .These concepts usually include extra degrees of liberty beyond those present in GR 9 . For instance , in scalar - vector models , the graviton acquires a mass through its interaction to a scalar field 10 .Similarly , in h ( R ) - theories 11 , the Einstein - Hilbert action contains upper - order curvature terms 12 . It turns out that both types of theories admit self - accelerating problems 13 , i . e . , de Sitter - like solutions without using dark energy 14 .",
        "rewrite_text": "**Title:** Nonlinear Perturbations and Conservation Laws on Curved Backgrounds in General Relativity and Other Metric Theories\n\n**Abstract:** This study investigates the nonlinear perturbations within the framework of general relativity (GR) and various alternative metric theories of gravitation, focusing on their implications for conserved quantities such as energy-momentum tensors. We demonstrate that these perturbations can be categorized into two distinct components: one that is intrinsically linked to the underlying geometry and another that pertains to the perturbation itself. Notably, our findings reveal that for any specified background solution, there exists an infinite spectrum of solutions corresponding to varying values of the conserved quantity. This observation suggests that conservation laws may not remain intact under small perturbations, challenging traditional notions of conservation in gravitational contexts. Additionally, we establish that this phenomenon could potentially violate the weak equivalence principle, raising important questions about the fundamental principles of gravity. We further explore the ramifications of our results for cosmological models and the physics surrounding black holes. General relativity, which has been extensively validated through a multitude of observations over the centuries, remains an open question regarding its validity at smaller scales where quantum effects become significant. To address this uncertainty, a variety of alternative theories have emerged, including scalar-vector models, f(R) gravity, and massive gravity, each introducing additional degrees of freedom beyond those found in GR. For instance, in scalar-vector models, the graviton acquires mass through interactions with a scalar field, while in f(R) theories, the Einstein-Hilbert action incorporates higher-order curvature terms. Both categories of theories exhibit self-accelerating solutions, akin to de Sitter solutions, without necessitating dark energy. Our research contributes to a deeper understanding of the interplay between nonlinear perturbations and conservation laws in gravitational theories, with significant implications for both theoretical physics and cosmology.",
        "ori-fast-z-score": -1.9445436482630056,
        "water-fast-z-score": 5.504335556964539,
        "rewrite-fast-z-score": -1.2632278815997784
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Mykyta the Fox and networks of language .\nAbstract:\nThe article presents an analysis of Mykyta s network, which is one of the largest Russian-language social media communities in Ukraine. The author describes how this community was formed; its main characteristics are also analyzed. It has been found that the majority of users who create content on Mykyta belong to the age group 18-24 years old (more than 80%). In addition, it has been established that most members of the community have higher education (over 50%), live in large cities with more than 100 thousand inhabitants (about 70%) and work as specialists or managers (about 60%).\nIt should be noted that the study does not cover all aspects of the functioning of the community under consideration. For example, there were no attempts made to analyze the relationship between the structure of the community and the quality of user-generated content. This issue will require further research. Keywords: Social media, online communication, sociolinguistics",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Mykyta the Fox and groups of language . Abstract : The essay presents an assessment of Mykyta s network , which is one of the big Russian - language social publishing communities in Ukraine .The author explains how this community was formed ; its primary characteristics are also analyzed . It has been shown that the majority of consumers who build material on Mykyta go to the age band 18 - 24 days old ( more than 80 % ) .In addition , it has been noted that most people of the village have better education ( over 50 % ) , live in large cities with more than 100 thousand inhabitants ( about 70 % ) and work as experts or executives ( about 60 % ) . It should be mentioned that the study does not include all aspects of the functioning of the neighborhood under consideration .For instance , there were no attempts made to analyze the relationship between the composition of the community and the performance of customer - produced content . This problem will demand further studies .Keywords : Social media , internet communication , sociolinguistics",
        "rewrite_text": "Title: Mykyta the Fox and Language Communities\n\nAbstract: This article provides a comprehensive evaluation of Mykyta, a prominent Russian-language social publishing community in Ukraine. The author delves into the origins of this community, exploring its formation and key characteristics. The analysis reveals that a significant portion of the content creators on Mykyta are predominantly within the 18 to 24 age demographic, accounting for over 80% of users. Furthermore, the study highlights that a majority of participants possess higher education qualifications (exceeding 50%), reside in urban areas with populations exceeding 100,000 (approximately 70%), and are employed in professional or managerial roles (around 60%). While the findings offer valuable insights into the demographic and educational background of Mykyta's users, it is important to note that the research does not encompass all dimensions of the community's dynamics. Specifically, the investigation does not address the correlation between the community's composition and the quality or impact of user-generated content. This gap indicates the necessity for further research to explore these relationships and enhance our understanding of the community's functioning. \n\nKeywords: Social media, internet communication, sociolinguistics.",
        "ori-fast-z-score": -1.2701705922171767,
        "water-fast-z-score": 6.1942248145051675,
        "rewrite-fast-z-score": -0.4364357804719848
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Magnetic Evolution and Temperature Variation in a Coronal Hole .\nAbstract:\nWe present the results of an analysis of magnetic field evolution, temperature variation, and plasma flow velocity observed by Hinode/SOT/SP (Solar Optical Telescope Spectro-Polarimeter) on September 24-25, 2007 during solar minimum period. The active region NOAA 10930 was located at S19E09 when it produced two flares with GOES class M5.7 and M1.0 respectively. We found that there were significant changes in the photospheric magnetic fields before and after these flares. In particular, we detected a new flux emergence event which occurred about one hour prior to the first flare. This newly emerged flux led to the formation of a coronal hole overlying the active region. After the second flare, the polarity inversion line moved toward the center of the active region where the strongest magnetic shear existed. During this time interval, we also found a rapid decrease in the intensity of the Fe xxi emission lines near the footpoints of the loops connecting opposite-polarities. These observations suggest that the energy released by the flares may have caused heating of the loop-top regions as well as evaporation of chromospheric material into the corona along the reconnected loops.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Magnetic Evolution and Temperature Variation in a Coronal Hole . Abstract : We report the conclusion of an assessment of magnetic field evolution , temperature variation , and plasma circulation velocity observed by Hinode / SOT / SP ( Solar Optical Telescope Spectro - Polarimeter ) on September 24 - 25 , 2007 during solar low time .The active region NOAA 10930 was positioned at S19E09 when it produced two flares with GOES class M5 . 7 and M1 . 0 respectively . We determined that there were substantial alterations in the photospheric magnetic fields before and after these flares .In particular , we identified a new flux emergence activity which occurred about one hour prior to the first flare . This newly emerged flux led to the formation of a coronal hole overlying the active region .After the second flare , the polarity inversion line moved toward the center of the active region where the greatest magnetic shear existed . During this time interval , we also discovered a rapid decrease in the strength of the Fe xxi emission lines near the footpoints of the loops connecting opposite - polarities .These measurements suggest that the electricity emitted by the flares might have caused heating of the loop - top regions as also as evaporation of chromospheric matter into the corona along the reconnected loops .",
        "rewrite_text": "We present a comprehensive analysis of the evolution of magnetic fields, temperature fluctuations, and plasma circulation velocities observed by the Hinode Solar Optical Telescope Spectro-Polarimeter (SOT/SP) on September 24-25, 2007, during a period of low solar activity. The study focuses on the active region NOAA 10930, located at S19E09, which was responsible for two significant flares classified as GOES M5.7 and M1.0. Our findings indicate notable changes in the photospheric magnetic fields both prior to and following these flares. Specifically, we observed a new emergence of magnetic flux approximately one hour before the occurrence of the first flare, which subsequently led to the development of a coronal hole situated above the active region. Following the second flare, we noted a shift in the polarity inversion line towards the center of the active region, where the magnetic shear was most pronounced. Additionally, during this period, we recorded a rapid decline in the intensity of Fe XXI emission lines near the footpoints of the magnetic loops that connect opposite polarities. These observations imply that the energy released by the flares may have contributed to the heating of the loop-top regions and facilitated the evaporation of chromospheric material into the corona along the reconnected magnetic loops. This study enhances our understanding of the dynamic processes occurring in solar active regions and their impact on coronal heating and plasma dynamics.",
        "ori-fast-z-score": 0.6396021490668313,
        "water-fast-z-score": 4.7699904600286205,
        "rewrite-fast-z-score": 1.3199500146737049
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The substellar mass function in sigma Orionis. II. Optical, near-infrared and IRAC/Spitzer photometry of young cluster brown dwarfs and planetary-mass objects .\nAbstract:\nWe present optical (BVRI), near infrared (JHK) and mid-infrared (IRAC 3.6-8.0 micron; MIPS 24 micron) photometry for the known population of low-mass stars and brown dwarfs in the open star forming region Sigma Orionis. We use these data to derive spectral types and bolometric luminosities for all objects with masses below 0.1 solar masses. The resulting substellar mass function is compared to that derived by previous studies using different techniques. Our results are consistent with those obtained previously but we find evidence for an excess number of very-low mass objects at the faint end of our sample which may be due to unresolved binaries or contamination by background galaxies. This work was supported by NASA grant NAG5-12942. We thank J. Stauffer for providing us with his list of candidate members prior to publication. \n \n Keywords: Open clusters",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The substellar mass function in sigma Orionis . II .Optical , near - infrared and IRAC / Spitzer photometry of young cluster brown dwarfs and planetary - mass bodies . Abstract : We present visual ( BVRI ) , near infrared ( JHK ) and mid - infrared ( IRAC 3 . 6 - 8 . 0 micron ; MIPS 24 micron ) photometry for the known community of low - mass stars and green dwarfs in the open star producing region Sigma Orionis .We use these information to derive spectral classes and bolometric luminosities for all bodies with masses below 0 . 1 solar masses . The resulting substellar mass distribution is compared to that derived by earlier surveys using different methods .Our results are compatible with those generated previously but we find proof for an excess amount of very - low weight objects at the faint ending of our sample which may be due to unresolved binaries or poisoning by background galaxies . This research was supported by NASA grant NAG5 - 12942 .We thank J . Stauffer for providing us with his list of candidate members preceding to publication . Keywords : Open clusters",
        "rewrite_text": "We present a comprehensive study of the substellar mass function within the Sigma Orionis region, focusing on the optical, near-infrared, and mid-infrared photometry of young cluster brown dwarfs and planetary-mass objects. Our analysis includes visual photometry in the BVRI bands, near-infrared measurements in the JHK bands, and mid-infrared data from the IRAC (3.6 - 8.0 microns) and MIPS (24 microns) instruments. By utilizing this extensive dataset, we are able to determine the spectral classifications and bolometric luminosities for all celestial bodies with masses below 0.1 solar masses within this stellar community. \n\nThe derived substellar mass distribution is then compared with results from previous surveys that employed various methodologies. Our findings align with earlier studies; however, we identify a notable excess of very low-mass objects at the faint end of our sample. This anomaly may be attributed to the presence of unresolved binary systems or contamination from background galaxies. \n\nThis research contributes valuable insights into the population of low-mass stars and brown dwarfs in Sigma Orionis, enhancing our understanding of the formation and evolution of substellar objects in open clusters. The work was supported by NASA grant NAG5-12942, and we extend our gratitude to J. Stauffer for sharing his list of candidate members prior to publication. \n\nKeywords: Open clusters, substellar mass function, brown dwarfs, planetary-mass bodies, photometry, Sigma Orionis.",
        "ori-fast-z-score": -2.0124611797498106,
        "water-fast-z-score": 2.9068883707497264,
        "rewrite-fast-z-score": -0.29559878344928797
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Constraining GRB progenitor models by probing Wolf-Rayet wind geometries in the Large Magellanic Cloud .\nAbstract:\nWe present deep optical and near-infrared observations of two gamma-ray burst (GRB) host galaxies, which are located behind the Large Magellanic cloud (LMC). The LMC is an ideal laboratory for studying the effects of massive star winds on their surroundings because it contains many young open clusters with ages ranging between 1 Myr to several hundred million years old. We use these data to probe the geometry of the surrounding interstellar medium (ISM), as well as that of the stellar winds produced by the most recent generation of stars within each cluster. In particular we focus our attention on the properties of Wolf Rayet (WR) stars, whose powerful winds can have dramatic effects on their environments over large distances.  By comparing the observed line-of-sight column densities of hydrogen gas towards different clusters at various orientations relative to the plane of the galaxy, we find evidence for significant differences in the structure of the ISM along lines of sight passing through the disk compared to those passing through the halo. This suggests that there may be large-scale variations in the density distribution of the ISM throughout this region of space.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Constraining GRB progenitor models by probing Wolf - Rayet weather geometries in the Large Magellanic Cloud . Abstract : We report deep optical and far - infrared observations of two gamma - ray burst ( GRB ) host galaxies , which are situated behind the Large Magellanic cloud ( LMC ) .The LMC is an excellent laboratory for studying the effects of large galaxy winds on their environment because it contains large young open complexes with ages ranging between 1 Myr to several hundred million days old . We use these information to probe the topography of the nearby interstellar medium ( ISM ) , as well as that of the stellar winds released by the most current generation of stars within each cluster .In particular we focus our focus on the properties of Wolf Rayet ( WR ) stars , whose massive winds can have dramatic effects on their habitats over large distances . By comparing the seen line - of - view column densities of carbon dust towards different galaxies at several orientations relative to the plane of the galaxy , we find proof for significant variations in the composition of the ISM along lines of view traveling through the disk compared to those traveling through the halo .This implies that there may be large - scale variations in the density density of the ISM throughout this area of space .",
        "rewrite_text": "We present comprehensive optical and far-infrared observations of two gamma-ray burst (GRB) host galaxies located behind the Large Magellanic Cloud (LMC). The LMC serves as an ideal environment for investigating the influence of substantial galactic winds on their surroundings, given its rich array of young stellar complexes, which range in age from approximately 1 million years to several hundred million years. Our study utilizes this unique setting to analyze the structure of the nearby interstellar medium (ISM) and the stellar winds generated by the latest generation of stars within these clusters. A particular emphasis is placed on the characteristics of Wolf-Rayet (WR) stars, known for their powerful winds that can significantly alter their environments over extensive distances. By examining the line-of-sight column densities of carbon dust in various galaxies at different orientations relative to the galactic plane, we uncover notable discrepancies in the composition of the ISM. Specifically, our findings indicate that lines of sight traversing the galactic disk exhibit different characteristics compared to those that pass through the halo. This observation suggests the presence of large-scale variations in the density of the ISM across this region of space, which may have implications for our understanding of GRB progenitor models. Overall, our results contribute to a deeper comprehension of the interactions between massive stars and their environments, particularly in the context of the LMC's dynamic and complex astrophysical landscape.",
        "ori-fast-z-score": -1.8,
        "water-fast-z-score": 4.676674793986949,
        "rewrite-fast-z-score": -0.7627700713964739
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The IC1396N proto-cluster at a scale of 250 AU .\nAbstract:\nWe present new near-infrared (NIR) observations of the young stellar cluster IC 1396 N, located in the Orion Nebula Cluster region. The data were obtained with the adaptive optics system NAOS-CONICA on the VLT telescope and cover an area of 0.5 arcmin2 around the central star HD 37022. We detect more than 100 point sources down to Ks = 18 mag within this field-of-view. Using these data we have constructed colour-magnitude diagrams for different areas inside our field-of-view. From these CMDs we find that there are two populations of stars which can be separated by their position in the diagrams: one population is redder and fainter while another has bluer colours and brighter magnitudes. These results suggest that the first group consists mainly of low mass pre-main sequence stars surrounded by circumstellar disks, whereas the second group contains mostly high mass main-sequence stars without any surrounding material.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The IC1396N proto - cluster at a scale of 250 AU . Abstract : We report new near - infrared ( NIR ) observations of the young stellar cluster IC 1396 N , located in the Orion Nebula Cluster region .The data were obtained with the adaptive optics network NAOS - CONICA on the VLT telescope and cover an area of 0 . 5 arcmin2 around the central star HD 37022 . We detect more than 100 point sources down to Ks = 18 mag within this field - of - view .Using these information we have created colour - magnitude diagrams for different areas inside our field - of - view . From these CMDs we find that there are two communities of stars which can be apart by their placement in the diagrams : one community is redder and fainter while another has bluer colours and darker magnitudes .These data suggest that the first class consists primarily of low weight pre - principal sequence stars surrounded by circumstellar disks , whereas the second set contains primarily high mass primary - sequence stars without any surrounding material .",
        "rewrite_text": "We present new near-infrared (NIR) observations of the young stellar cluster IC 1396N, situated within the Orion Nebula Cluster region. Utilizing the adaptive optics system NAOS-CONICA on the Very Large Telescope (VLT), we collected data over an area of 0.5 arcmin² centered around the star HD 37022. Our observations revealed over 100 point sources, with a detection limit of Ks = 18 mag within this field of view. To analyze the stellar population, we constructed color-magnitude diagrams (CMDs) for various regions within our observational area. The CMDs indicate the presence of two distinct groups of stars, differentiated by their positions in the diagrams. The first group is characterized by redder and fainter stars, while the second group exhibits bluer colors and brighter magnitudes. The analysis suggests that the redder, fainter stars predominantly consist of low-mass pre-main sequence stars that are likely surrounded by circumstellar disks. In contrast, the bluer, brighter stars are primarily high-mass main sequence stars, which appear to lack any surrounding material. These findings contribute to our understanding of the stellar formation processes within the IC 1396N proto-cluster and highlight the diversity of stellar populations present in this region. The implications of these observations extend to the broader context of star formation in nebular environments, providing insights into the evolutionary stages of young stars and the conditions that influence their development.",
        "ori-fast-z-score": -0.5852057359806528,
        "water-fast-z-score": 4.4174102722651325,
        "rewrite-fast-z-score": -1.3598002073001698
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Nonequilibrium Steady States of Matrix Product Form: A Solver s Guide .\nAbstract:\nThe nonequilibrium steady states (NESS) of matrix product form are the focus of this work, which is intended to be useful for researchers in computational physics and chemistry who wish to solve problems with such NESSs using numerical methods.  The first part of the article introduces the concept of NESSs as well as some basic properties that they possess. In particular, we show how one can construct an explicit representation of any given NESS by solving a linear system of equations whose coefficient matrices depend on the underlying transition rates between different microstates. We also discuss several important issues related to the construction of these coefficient matrices. The second part of the article presents two examples illustrating our approach. Finally, we provide a detailed discussion about various aspects of the proposed method along with possible extensions. Nonequilibrium steady states (NESs), i.e., time-independent solutions of master equations describing open systems far away from equilibrium, have been studied extensively over the past few decades  1  . They play crucial roles in many areas ranging from statistical mechanics  2  , quantum optics  3  , chemical reaction dynamics  4  , and biophysics  5  .\nIn recent years there has been growing interest in developing efficient algorithms for computing NESs  6  -  8  . This is mainly due to their importance in applications where it may not always be feasible or desirable to obtain exact analytical results  9  -  11  . For example, in molecular dynamics simulations  12  , Monte Carlo sampling techniques  13  , and kinetic Monte Carlo schemes  14  , only approximate values of NESs are available. Moreover, even if the exact solution were known, its direct use would still require significant amount of storage space  15  . Therefore, it becomes necessary to develop fast and accurate numerical methods for calculating NESs  16  -  18  .\nThere exist numerous approaches for numerically approximating NESs  19  -  21  . Among them, the most popular ones include the eigenvector-following algorithm  22  , the power iteration scheme  23  , and the Krylov subspace projection technique  24  . These methods usually involve repeated application of the original master equation until convergence is reached  25  . However, since the number of...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Nonequilibrium Steady States of Matrix Product Form : A Solver s Guide . Abstract : The nonequilibrium steady states ( NESS ) of matrix product structure are the emphasis of this project , which is intended to be usable for researchers in computational physics and chemistry who desire to solve difficulties with such NESSs using numerical technique .The first part of the article describes the idea of NESSs as well as some fundamental properties that they possess . In particular , we explain how one can build an explicit representation of any given NESS by solving a linear network of equations whose coefficient matrices depend on the underlying transition rates between various microstates .We additionally discuss various crucial matters related to the creation of these coefficient matrices . The second part of the article describes two examples illustrating our approach .Finally , we provide a detailed discussion about various parts of the suggested method along with possible extensions . Nonequilibrium steady states ( NESs ) , i . e . , time - based solve of master equations representing open systems well away from equilibrium , have been studied frequently over the previous few decades 1 .They play crucial roles in multiple fields ranging from statistical mechanics 2 , quantum optics 3 , chemical process mechanics 4 , and biophysics 5 . In recent months there has been growing interest in building fast algorithms for processing NESs 6 - 8 .This is mainly owing to their importance in applications where it would not always be impossible or useful to obtain exact analytical results 9 - 11 . For instance , in polymer mechanics simulations 12 , Monte Carlo analysis methods 13 , and kinetic Monte Carlo schemes 14 , only approximate values of NESs are available .Moreover , even if the exact solution were known , its immediate application would still demand significant amount of storage space 15 . Therefore , it becomes necessary to develop fast and precise quantitative methods for determining NESs 16 - 18 .There remain various approaches for numerically approximating NESs 19 - 21 . Among them , the most popular ones contain the eigenvector - following algorithm 22 , the power iteration scheme 23 , and the Krylov subspace projection procedure 24 .These methods usually include repeated application of the previous master equation until convergence is reached 25 . However , since the number of . . .",
        "rewrite_text": "**Title:** Nonequilibrium Steady States of Matrix Product Form: A Solver's Guide\n\n**Abstract:** This article focuses on the nonequilibrium steady states (NESS) characterized by matrix product structures, providing a comprehensive guide for researchers in computational physics and chemistry who aim to tackle challenges associated with NESS using numerical methods. The initial section introduces the concept of NESS and outlines several fundamental properties inherent to these states. We detail a systematic approach to construct an explicit representation of any NESS by solving a linear network of equations, where the coefficient matrices are influenced by the transition rates between various microstates. Additionally, we address critical considerations in the formulation of these coefficient matrices. The second section presents two illustrative examples that demonstrate the application of our methodology. We conclude with an in-depth discussion of the various components of the proposed method, including potential extensions for future research.\n\nOver the past few decades, nonequilibrium steady states (NESS) have garnered significant attention due to their pivotal roles across diverse fields such as statistical mechanics, quantum optics, chemical process dynamics, and biophysics. Recent developments have sparked increased interest in devising efficient algorithms for analyzing NESS, primarily driven by their relevance in scenarios where obtaining exact analytical solutions is either impractical or infeasible. For example, in simulations of polymer mechanics, Monte Carlo methods, and kinetic Monte Carlo schemes, researchers often rely on approximate values for NESS. Furthermore, even if exact solutions are attainable, their practical application can necessitate substantial storage resources. Consequently, there is a pressing need for the development of rapid and accurate quantitative techniques for determining NESS. Various numerical approaches exist for approximating NESS, with popular methods including the eigenvector-following algorithm, power iteration scheme, and Krylov subspace projection technique. These approaches typically involve iterative applications of the master equation until convergence is achieved. However, the complexity and computational demands associated with these methods warrant further exploration and innovation in the field.",
        "ori-fast-z-score": -1.0377490433255416,
        "water-fast-z-score": 8.454968154078788,
        "rewrite-fast-z-score": -0.3104602102825331
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Validating module network learning algorithms using simulated data .\nAbstract:\nWe present an approach to validating the performance of machine-learning algorithms for identifying modules in networks, based on synthetic datasets generated by simulating random walks through known modular structures. We show that this method can be used to identify and rank different types of modules with high accuracy across a range of sizes and densities. The results are robust against noise and missing links. This validation strategy is useful both as a benchmarking tool for comparing competing methods and also as a means of assessing how well existing approaches perform when applied to real-world systems. In recent years there has been growing interest in developing computational tools capable of detecting functional units within complex biological networks such as protein-protein interaction (PPI) or gene regulatory networks  1–3  . These so-called “modules” represent groups of nodes which interact more strongly among themselves than they do with other parts of the system  4  , and may correspond to molecular complexes  5  , signaling pathways  6  , metabolic cycles  7  , or even entire cellular processes  8  .\nThe identification of these modules is important because it provides insight into the organization of the underlying network  9  , and allows us to predict new interactions  10  , detect disease genes  11  , and understand evolutionary relationships  12  . However, despite considerable effort  13–19  , no single algorithm consistently outperforms all others  20  , so researchers have developed a variety of complementary techniques  21–23  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Validating module network learning techniques using modeled information . Abstract : We present an way to validating the performance of machine - learning techniques for finding modules in networks , based on synthetic datasets generated by simulating random runs through known modular structures .We see that this method can be used to identify and grade different kinds of modules with high sensitivity across a range of sizes and densities . The results are robust against noise and lost connections .This validation methodology is important both as a benchmarking tool for evaluating rival techniques and also as a means of assessing how well existing techniques perform when applied to real - global networks . In recent years there has been growing interest in establishing computational tools capable of detecting functional units within complex biological networks such as protein - gene interaction ( PPI ) or protein regulatory networks 1 – 3 .These so - called “ modules ” form groups of nodes which interact more strongly among themselves than they do with other parts of the system 4 , and may correspond to biological complexes 5 , signaling pathways 6 , metabolic processes 7 , or maybe whole cellular processes 8 . The identify of these modules is important because it gives knowledge into the organization of the underlying network 9 , and allows us to predict new interactions 10 , predict disease genes 11 , and understand evolutionary relationships 12 .However , despite considerable attempts 13 – 19 , no single method consistently outperforms all others 20 , so researchers have developed a variety of complementary techniques 21 – 23 .",
        "rewrite_text": "**Title:** Validating Module Network Learning Techniques Using Modeled Information\n\n**Abstract:** In this study, we introduce a novel approach for validating the efficacy of machine learning techniques aimed at identifying modules within networks. Our method utilizes synthetic datasets created by simulating random traversals through established modular structures. We demonstrate that this approach effectively identifies and evaluates various types of modules with high sensitivity, regardless of their size and density. Notably, the results obtained are resilient to noise and the loss of connections, underscoring the robustness of our validation methodology. This framework serves a dual purpose: it acts as a benchmarking tool for comparing competing techniques and provides insights into the performance of existing methods when applied to real-world global networks.\n\nThe increasing interest in computational tools for detecting functional units within complex biological networks, such as protein-protein interaction (PPI) networks and protein regulatory networks, has been notable in recent years. These functional units, referred to as \"modules,\" consist of groups of nodes that exhibit stronger interactions among themselves compared to their interactions with other components of the network. Such modules may represent biological complexes, signaling pathways, metabolic processes, or even entire cellular functions. Identifying these modules is crucial as it enhances our understanding of the network's organization, facilitates the prediction of new interactions, aids in identifying disease-related genes, and sheds light on evolutionary relationships.\n\nDespite extensive research efforts, no single method has emerged as the definitive leader in module detection, prompting the development of a diverse array of complementary techniques. Our validation approach not only contributes to the ongoing discourse on module detection but also provides a systematic means to evaluate the strengths and weaknesses of various methodologies in the context of real biological networks.",
        "ori-fast-z-score": 0.08481889296799709,
        "water-fast-z-score": 6.821430560638804,
        "rewrite-fast-z-score": -0.08032193289024989
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The VVDS type-1 AGN sample: The faint end of the luminosity function .\nAbstract:\nWe present new results on the faint-end slope and evolution of the luminosity function (LF) for optically-selected quasars in the redshift range 0.5 < z < 2.2, based on the VIMOS-VLT Deep Survey (VVDS). We use two different methods to estimate the LF parameters at each redshift bin: 1/Vmax method and maximum likelihood fitting technique. Our best-fit values are obtained by combining these two techniques with Monte Carlo simulations. We find that our data is consistent with previous studies within their uncertainties. However we show that there exists an apparent discrepancy between the observed number density of bright quasars and the predictions made using the standard quasar formation model. This may be due to incompleteness effects or biases introduced during the selection process. In addition, we also investigate the dependence of the LF shape on the optical luminosities of quasars.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The VVDS type - 1 AGN sample : The dim end of the luminosity function . Abstract : We report new data on the faint - beginning slope and evolution of the luminosity function ( LF ) for optically - selected quasars in the redshift region 0 . 5 < z < 2 . 2 , based on the VIMOS - VLT Deep Survey ( VVDS ) .We use two different methods to estimate the LF variables at each redshift bin : 1 / Vmax method and greatest probability fitting technique . Our best - fitting values are derived by combining these two strategies with Monte Carlo simulations .We see that our information is compatible with previous research within their uncertainties . However we prove that there exists an apparent discrepancy between the observed number density of bright quasars and the assumptions produced using the standard quasar structure model .This might be due to incompleteness effects or biases created during the selection step . In addition , we also investigate the dependence of the LF profile on the optical luminosities of quasars .",
        "rewrite_text": "We present new findings regarding the faint-end slope and evolutionary trends of the luminosity function (LF) for optically-selected quasars within the redshift range of 0.5 < z < 2.2, utilizing data from the VIMOS-VLT Deep Survey (VVDS). Our analysis employs two distinct methodologies to estimate the LF parameters across various redshift bins: the 1/Vmax method and the greatest probability fitting technique. By integrating these approaches with Monte Carlo simulations, we derive our optimal fitting values. Our results align with previous studies when considering their uncertainties; however, we identify a notable discrepancy between the observed number density of luminous quasars and the predictions made by the conventional quasar structure model. This inconsistency may stem from incompleteness effects or biases introduced during the selection process. Furthermore, we explore how the LF profile is influenced by the optical luminosities of quasars, providing deeper insights into the characteristics of these celestial objects. Our findings contribute to the ongoing discourse on the luminosity function of quasars, particularly at the dim end, and highlight the complexities involved in accurately modeling their distribution and evolution across cosmic time. This research not only enhances our understanding of quasar demographics but also raises important questions regarding the methodologies employed in quasar selection and the implications for future studies in the field.",
        "ori-fast-z-score": -0.47140452079103173,
        "water-fast-z-score": 5.7350162126103985,
        "rewrite-fast-z-score": 2.177598558933893
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A new comprehensive set of elemental abundances in DLAs III. Star formation histories .\nAbstract:\nWe present the results of our analysis of a sample of 25 high-resolution quasar absorption line systems with metallicities ranging between 1/100 and 1/10 solar, selected to have low dust content (i.e., < 0.1 mag extinction at 2200 A). We use these data together with those for another 20 DLAs presented by Pettini et al. (1999) to investigate the chemical enrichment history of DLA galaxies over cosmic time. The main conclusions are as follows: \nThe abundance patterns observed in this sample can be explained if we assume that most of the metals were produced during an early burst of star formation which occurred less than 10 Gyr ago. \n\n\nThis is consistent with previous studies based on smaller samples but it also shows that there may not always be evidence for recent star formation activity even when such activity has been inferred from other indicators. \n\nIn addition, we find no correlation between metallicity and dust content or neutral hydrogen column density.\n\nFinally, we show that the mean value of  Fe/H  measured in DLAs agrees well with the predictions made using simple models of galactic chemical evolution.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A modern comprehensive setting of elemental abundances in DLAs III . Star formation histories .Abstract : We present the conclusion of our analysis of a sample of 25 high - resolution quasar absorbed line systems with metallicities ranging between 1 / 100 and 1 / 10 solar , selected to have lowest dust content ( i . e . , < 0 . 1 mag extinction at 2200 A ) . We use these results together with those for another 20 DLAs given by Pettini et al .( 1999 ) to examine the chemical enrichment history of DLA galaxies over cosmic time . The main results are as follows : The accumulation patterns observed in this specimen can be understood if we suppose that most of the metals were produced during an early burst of star formation which occurred less than 10 Gyr ago .This is consistent with previous analyses based on smaller specimens but it also shows that there may not always be confirmation for recent star formation activity even when such activity has been inferred from other indicators . In addition , we find no correlation between metallicity and dust content or neutral hydrogen column density .Finally , we prove that the mean value of Fe / H measured in DLAs agrees well with the assumptions done using simple theories of galactic material evolution .",
        "rewrite_text": "In this study, we conclude our investigation of a sample comprising 25 high-resolution quasar absorption line systems, characterized by metallicities between 1/100 and 1/10 of solar levels, specifically chosen for their minimal dust content (i.e., exhibiting less than 0.1 magnitudes of extinction at 2200 Å). By integrating our findings with data from an additional 20 damped Lyman-alpha (DLA) systems analyzed by Pettini et al. (1999), we aim to elucidate the chemical enrichment history of DLA galaxies throughout cosmic time. Our primary findings indicate that the observed accumulation patterns of elemental abundances can be interpreted as a result of a significant early burst of star formation that occurred less than 10 billion years ago. This conclusion aligns with earlier studies conducted on smaller samples, yet it also highlights the potential lack of confirmation for recent star formation activity, even when such activity has been suggested by other observational indicators. Furthermore, our analysis reveals no significant correlation between metallicity and either dust content or neutral hydrogen column density. Lastly, we demonstrate that the average iron-to-hydrogen (Fe/H) ratio measured in DLAs is consistent with predictions derived from simplified models of galactic chemical evolution. These results contribute to a deeper understanding of the star formation histories and chemical enrichment processes in DLA galaxies, providing valuable insights into the evolution of the universe's baryonic matter.",
        "ori-fast-z-score": 0.9045340337332909,
        "water-fast-z-score": 6.0,
        "rewrite-fast-z-score": 1.4985372985307104
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  On monotonicity, stability, and construction of central schemes for hyperbolic conservation laws with source terms (Revised Version) .\nAbstract:\nWe present some new results on the monotonicity, stability and construction of central difference schemes for nonlinear systems of hyperbolic conservation laws with nonlocal source terms. We consider two types of source term models: one is the relaxation model in which the source term can be written as an integral over the past history; another is the fractional step method where the source term is treated implicitly by solving a system of ordinary differential equations at each time level. The main idea behind our approach to construct stable numerical methods is based on the concept of entropy conservative fluxes introduced recently by Tadmor et al.. In particular, we show that any scheme constructed using these entropy conservative fluxes will automatically preserve positivity if it satisfies certain conditions. Finally, several examples are presented to illustrate the theoretical results obtained here. Cite this article as: Zhi-Hong Liu, On Monotonicity, Stability, and Construction of Central Schemes for Hyperbolic Conservation Laws With Source Terms, Journal of Computational Physics, Volume 227, Issue 1, 15 January 2008, Pages 1-33",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : On monotonicity , stability , and construction of central schemes for hyperbolic conservation laws with source terms ( Revised Version ) . Abstract : We report some fresh results on the monotonicity , stability and construction of central difference schemes for nonlinear solutions of hyperbolic conservation laws with nonlocal source terms .We consider two forms of source word models : one is the relaxation model in which the source word can be written as an integral over the previous history ; another is the fractional stage method where the source word is treated implicitly by solving a system of ordinary differential coefficients at each time level . The main idea behind our approach to build stable numerical models is based on the notion of entropy conservative fluxes proposed lately by Tadmor et al . .In particular , we prove that any scheme built using these entropy conservative fluxes will automatically preserve positivity if it satisfies certain conditions . Finally , various instances are presented to illustrate the theoretical results derived here .Cite this page as : Zhi - Hong Liu , On Monotonicity , Stability , and Construction of Central Schemes for Hyperbolic Conservation Laws With Source Terms , Journal of Computational Physics , Volume 227 , Issue 1 , 15 January 2008 , Pages 1 - 33",
        "rewrite_text": "Title: On Monotonicity, Stability, and Construction of Central Schemes for Hyperbolic Conservation Laws with Source Terms (Revised Version)\n\nAbstract: This article presents novel findings regarding the monotonicity, stability, and development of central difference schemes aimed at addressing nonlinear solutions of hyperbolic conservation laws that incorporate nonlocal source terms. We investigate two distinct models for the source terms: the first is a relaxation model, where the source term is expressed as an integral over prior states; the second is the fractional stage method, which treats the source term implicitly by solving a system of ordinary differential equations at each time step. The cornerstone of our methodology for constructing stable numerical models is rooted in the concept of entropy conservative fluxes, as recently introduced by Tadmor et al. Our key contribution is the demonstration that any numerical scheme derived from these entropy conservative fluxes will inherently maintain positivity, provided it adheres to specific criteria. To substantiate our theoretical findings, we present a variety of examples that showcase the practical application of our results. This work not only advances the understanding of central schemes in the context of hyperbolic conservation laws but also offers a framework for ensuring stability and monotonicity in numerical simulations. The implications of our research are significant for the development of robust computational methods in fluid dynamics and other fields governed by hyperbolic equations. \n\nCite this page as: Zhi-Hong Liu, On Monotonicity, Stability, and Construction of Central Schemes for Hyperbolic Conservation Laws With Source Terms, Journal of Computational Physics, Volume 227, Issue 1, 15 January 2008, Pages 1-33.",
        "ori-fast-z-score": -1.5096588248481377,
        "water-fast-z-score": 3.450648742510029,
        "rewrite-fast-z-score": -0.5883484054145521
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Evidence for the Evolution of Young Early-Type Galaxies in the GOODS/CDF-S Field .\nAbstract:\nWe present new spectroscopic observations of galaxies at z ~ 1.5-2.0 selected by their UVJ colors and optical morphologies, obtained with VLT/VIMOS on the Very Large Telescope (VLT). We find that these objects are mostly early-type galaxies showing signs of recent star formation activity. The observed properties suggest that they may be progenitors of local massive elliptical galaxies. These results provide further evidence supporting the scenario where most massive galaxies grow through mergers between gas-rich disk systems during the first half of cosmic time. This is an Open Access article distributed under the terms of the Creative Commons Attribution License 2.0, which permits unrestricted use, distribution, and reproduction in any medium provided the original work is properly cited. \n \n Keywords: galaxy evolution; merger remnants; young ellipticals; CDF-S field \n \n Massive galaxies evolve rapidly over cosmic time as a result of merging processes involving smaller companions. In particular, it has been suggested that many of today s brightest cluster galaxies were formed via major mergers of two or more gas-rich disks at redshifts around one to three  1  . However, direct observational evidence for this process remains elusive because of the difficulty in identifying such events at high redshift  2  .\n \nIn order to study the physical mechanisms driving galaxy growth we have carried out deep spectroscopy of galaxies at intermediate redshifts using the VLT-VIMOS spectrograph  3  . Our sample consists of about 100 galaxies selected based on their ultraviolet J (UVJ) color  4  , morphological type  5  , and apparent magnitude  6  . Most of them show strong emission lines characteristic of active star-forming regions  7, 8  . Their stellar masses range from 10^10 M_sol to 10^11 M_sol  9  . \n\n\nThe main goal of our project was to identify possible candidates for progenitor populations of local massive elliptical/S0 galaxies  10  . To do so, we used several selection criteria designed to select galaxies with similar characteristics to those found among nearby massive spheroids  11  : \n\n\n1. Morphological type: all targets must",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Evidence for the Evolution of Young Early - Type Galaxies in the GOODS / CDF - S Field . Abstract : We report new spectroscopic observations of galaxies at z ~ 1 . 5 - 2 . 0 selected by their UVJ colors and imaging morphologies , obtained with VLT / VIMOS on the Very Large Telescope ( VLT ) .We see that these objects are mostly early - class stars displaying signs of recent star formation activity . The observed properties suggest that they may be progenitors of local powerful elliptical galaxies .These data provide further evidence supporting the scenario where most gigantic galaxies grow through mergers between gas - rich disk systems during the first half of cosmic time . This is an Open Access article distributed under the terms of the Creative Commons Attribution License 2 . 0 , which allows unrestricted use , distribution , and reproduction in any medium provided the original book is properly cited .Keywords : universe progression ; collision remnants ; young ellipticals ; CDF - S field Massive stars develop rapidly over cosmic time as a outcome of combining processes involving smaller companions . In particular , it has been proposed that several of today s brightest cluster clusters were created via large mergers of two or more gas - rich disks at redshifts around one to three 1 .However , direct observational evidence for this process remains elusive because of the difficulty in identifying such events at high redshift 2 . In order to study the physical mechanisms governing star development we have carried out deep spectroscopy of clusters at intermediate redshifts using the VLT - VIMOS spectrograph 3 .Our specimen consists of about 100 galaxies chose based on their ultraviolet J ( UVJ ) color 4 , morphological class 5 , and apparent magnitude 6 . Most of them show strong absorption patterns characteristic of active star - creating areas 7 , 8 .Their stellar masses range from 10 ^ 10 M _ sol to 10 ^ 11 M _ sol 9 . The main goal of our work was to identify possible candidates for progenitor populations of local heavy elliptical / S0 galaxies 10 .To do so , we using numerous selection categories modified to select galaxies with similar characteristics to those detected among neighboring massive spheroids 11 : 1 . Morphological type : all targets must",
        "rewrite_text": "Title: Evidence for the Evolution of Young Early-Type Galaxies in the GOODS/CDF-S Field\n\nAbstract: In this study, we present new spectroscopic observations of galaxies at redshifts approximately between 1.5 and 2.0, selected based on their UVJ colors and imaging morphologies, utilizing the VLT/VIMOS on the Very Large Telescope (VLT). Our findings indicate that these galaxies predominantly consist of early-type stars exhibiting signs of recent star formation activity. The characteristics observed in these galaxies suggest that they may serve as progenitors to the powerful elliptical galaxies found in the local universe. This research adds to the growing body of evidence supporting the hypothesis that the majority of massive galaxies evolve through the merger of gas-rich disk systems during the early stages of cosmic history. \n\nMassive stars undergo rapid development over cosmic time, primarily as a result of merging processes involving smaller companion galaxies. It has been suggested that many of the brightest galaxy clusters observed today were formed through significant mergers of two or more gas-rich disks at redshifts between one and three. However, obtaining direct observational evidence for these high-redshift events has proven challenging due to the complexities involved in identifying such occurrences. To investigate the physical mechanisms that drive star formation, we conducted deep spectroscopy of galaxies at intermediate redshifts using the VLT-VIMOS spectrograph. Our sample comprises approximately 100 galaxies selected based on their ultraviolet J (UVJ) colors, morphological classifications, and apparent magnitudes. The majority of these galaxies exhibit strong absorption features indicative of active star-forming regions. Their stellar masses range from 10^10 M_sol to 10^11 M_sol. The primary objective of our research was to identify potential candidates for the progenitor populations of local massive elliptical and S0 galaxies. To achieve this, we employed various selection criteria aimed at isolating galaxies with properties akin to those observed in nearby massive spheroids. \n\nThis article is published under the Creative Commons Attribution License 2.0, allowing for unrestricted use, distribution, and reproduction in any medium, provided that the original work is properly cited. \n\nKeywords: universe evolution; merger remnants; young elliptical galaxies; CDF-S field.",
        "ori-fast-z-score": 0.07832604499879574,
        "water-fast-z-score": 7.084340391869858,
        "rewrite-fast-z-score": 1.543033499620919
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Phenomenology of GUT-less Supersymmetry Breaking .\nAbstract:\nWe study the phenomenological consequences of supersymmetric models with gauge-mediated breaking, in which the Standard Model is extended by adding new vector-like matter fields and extra dimensions. We show that these models can be constructed such that they are free of any unnatural fine-tuning problems associated with the Higgs mass or flavor-changing neutral currents. In particular we find that:  1) The lightest scalar superpartner (the  Higgs  boson) has a mass at most around 300 GeV.  2) Flavor changing neutral current effects are suppressed to an acceptable level for generic values of parameters.  3) Gauge coupling unification occurs naturally within experimental uncertainties. 4) There exists a large parameter space where all sparticles have masses above 1 TeV while still satisfying constraints on electroweak symmetry breaking. 5) These models provide a natural explanation for why there may not yet exist evidence for supersymmetry at accelerator experiments.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Phenomenology of GUT - less Supersymmetry Breaking . Abstract : We research the phenomenological consequences of supersymmetric theories with gauge - mediated breaking , in which the Standard Model is extended by added new vector - like matter fields and extra dimensions .We see that these models can be built such that they are free of any strange fine - tuned flaws associated with the Higgs mass or flavor - changing neutral currents . In particular we find that : 1 ) The lightest scalar superpartner ( the Higgs boson ) has a mass at most approximately 300 GeV .2 ) Flavor shifting neutral current effects are suppressed to an acceptable level for generic values of values . 3 ) Gauge coupling unification happens easily within experimental uncertainties .4 ) There exists a large parameter room where all sparticles have masses above 1 TeV while already satisfying constraints on electroweak symmetry breaking . 5 ) These models serve a natural explanation for why there may not already exist evidence for supersymmetry at accelerator studies .",
        "rewrite_text": "Title: Phenomenology of GUT-less Supersymmetry Breaking\n\nAbstract: In this study, we explore the phenomenological implications of supersymmetric theories characterized by gauge-mediated symmetry breaking. These theories extend the Standard Model by incorporating additional vector-like matter fields and extra dimensions. Our analysis reveals that it is possible to construct models that avoid the problematic fine-tuning issues typically associated with the Higgs mass and flavor-changing neutral currents. Specifically, we identify several key findings: First, the mass of the lightest scalar superpartner, identified as the Higgs boson, is constrained to a maximum of approximately 300 GeV. Second, the effects of flavor-changing neutral currents are significantly suppressed, remaining within acceptable limits for a wide range of parameter values. Third, we demonstrate that gauge coupling unification can be achieved easily, remaining consistent with current experimental uncertainties. Fourth, we find a substantial parameter space where all superpartners (sparticles) can possess masses exceeding 1 TeV, while still adhering to the constraints imposed by electroweak symmetry breaking. Lastly, these models provide a compelling explanation for the absence of direct evidence for supersymmetry in current accelerator experiments. Our findings contribute to the understanding of supersymmetry in a framework that is both theoretically sound and phenomenologically viable, paving the way for future research in this area.",
        "ori-fast-z-score": 0.46499055497527714,
        "water-fast-z-score": 5.658032638058332,
        "rewrite-fast-z-score": 0.29851115706299675
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  On the Incidence of C IV Absorbers Along the Sightlines to Gamma-Ray Bursts .\nAbstract:\nWe present new results on the incidence and properties of intervening absorbers along the sightline towards GRB 080913, based on high-resolution spectroscopy obtained with X-shooter at VLT-UT2 (ESO program ID 080.A-9007). We detect two strong absorption systems in the spectrum of this burst, one associated with an intervening galaxy at z = 1.5394 ± 0.0002, and another system at z = 2.084 ± 0.001 that is likely due to a damped Lyman alpha absorber. The latter has been previously detected by Fynbo et al. (2009) using low resolution spectra taken with FORS-2/VLT. Our analysis shows that both these systems are rich in metals, including Si II, Mg II, Fe II, Al III, O I, N V, and possibly also C IV. In addition we find evidence for several weaker metal lines which may be associated with either or both of these systems.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : On the Incidence of C IV Absorbers Along the Sightlines to Gamma - Ray Bursts . Abstract : We report new data on the incidence and properties of intervening absorbers along the sightline towards GRB 080913 , based on wide - resolution spectroscopy acquired with X - shooter at VLT - UT2 ( ESO program ID 080 . A - 9007 ) .We detect two strong absorption systems in the spectrum of this flash , one linked with an intervening galaxy at z = 1 . 5394 ± 0 . 0002 , and another system at z = 2 . 084 ± 0 . 001 that is probably due to a damped Lyman alpha absorber . The latter has been previously observed by Fynbo et al .( 2009 ) using reduced resolution spectra done with FORS - 2 / VLT . Our study shows that both these systems are rich in metals , notably Si II , Mg II , Fe II , Al III , O I , N V , and maybe also C IV .In addition we find proof for numerous smaller metal lines which may be identified with either or both of these systems .",
        "rewrite_text": "We present new findings regarding the incidence and characteristics of intervening absorbers along the line of sight to Gamma-Ray Burst (GRB) 080913, utilizing high-resolution spectroscopy obtained with the X-shooter instrument at the VLT-UT2 (ESO program ID 080.A-9007). Our analysis reveals the presence of two prominent absorption systems in the spectrum associated with this GRB event. The first system is connected to an intervening galaxy at a redshift of z = 1.5394 ± 0.0002, while the second system, located at z = 2.084 ± 0.001, is likely attributed to a damped Lyman-alpha absorber. This latter system was previously identified by Fynbo et al. (2009) through lower resolution spectra obtained with FORS-2/VLT. Our investigation indicates that both absorption systems exhibit a rich metallic composition, with significant detections of elements such as Si II, Mg II, Fe II, Al III, O I, and N V, and potentially C IV as well. Furthermore, we provide evidence for several additional smaller metal lines that may correspond to either or both of the identified absorption systems. These findings contribute to our understanding of the chemical enrichment of the intergalactic medium and the role of intervening galaxies in the context of gamma-ray bursts. The implications of these results are significant for the study of cosmic evolution and the processes governing the formation of heavy elements in the universe.",
        "ori-fast-z-score": -1.952833664712358,
        "water-fast-z-score": 2.840187787218772,
        "rewrite-fast-z-score": -1.3627702877384937
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Disclinations, dislocations and continuous defects: a reappraisal .\nAbstract:\nThe concept of defects in crystals has been developed by the Russian school since the 1930s. The main idea is that any crystal can be considered as an elastic continuum with some local deviations from its ideal structure which are called defects. In this work we present a brief review on the history of the development of the theory of defects in solids. We also discuss the modern concepts of point-like defects (dislocations), line-like defects (disclinations) and continuous defects. Finally, we give examples of how these ideas have been applied to different physical systems such as liquid crystals or magnetic materials. Defects play an important role in many areas of physics ranging from solid state physics to condensed matter physics and even biology. They appear naturally during phase transitions between ordered states like those occurring at melting points or critical temperatures. For example, they may lead to plastic deformations in metals or glassy materials. On the other hand, defects are responsible for macroscopic properties of solids like electrical conductivity or magnetization.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Disclinations , dislocations and continuous defects : a reappraisal . Abstract : The concept of flaws in crystals has been pioneered by the Russian school since the 1930s .The main idea is that any crystal can be regarded as an elastic continuum with some local deviations from its ideal structure which are called flaws . In this study we present a brief review on the history of the development of the principle of flaws in solids .We also discuss the newer concepts of point - like defects ( dislocations ) , line - like defects ( disclinations ) and continuous defects . Finally , we give evidence of how these ideas have been used to different mechanical systems such as fluid crystals or magnetic materials .Defects serve an important role in many fields of science ranging from solid state mechanics to condensed matter science and even science . They arise naturally during phase transitions between ordered states like those occurring at melting points or critical temperatures .For instance , they may contribute to plastic deformations in metals or glassy materials . On the other hand , defects are responsible for macroscopic properties of solids like electrical conductivity or magnetization .",
        "rewrite_text": "Title: Disclinations, Dislocations, and Continuous Defects: A Reappraisal\n\nAbstract: The exploration of defects within crystalline structures has its roots in the pioneering work of the Russian school dating back to the 1930s. This foundational concept posits that any crystal can be viewed as an elastic continuum, characterized by localized deviations from its idealized arrangement, which are referred to as flaws. In this article, we provide a comprehensive overview of the historical evolution of the theory of defects in solids, tracing its development through various scientific advancements. We delve into contemporary understandings of defects, categorizing them into point-like defects (dislocations), line-like defects (disclinations), and continuous defects. Furthermore, we illustrate how these theoretical frameworks have been applied across diverse mechanical systems, including fluid crystals and magnetic materials. The significance of defects extends across multiple scientific disciplines, encompassing solid-state mechanics, condensed matter physics, and materials science. They emerge naturally during phase transitions, particularly when systems transition between ordered states, such as during melting or at critical temperatures. For example, defects play a crucial role in facilitating plastic deformation in metals and glassy substances. Additionally, they are integral to determining the macroscopic properties of solids, influencing characteristics such as electrical conductivity and magnetization. This reappraisal not only highlights the importance of understanding defects in crystalline materials but also emphasizes their multifaceted roles in various physical phenomena, thereby underscoring the need for continued research in this area to unlock further insights into material behavior and properties.",
        "ori-fast-z-score": 0.3144854510165755,
        "water-fast-z-score": 5.421374765483944,
        "rewrite-fast-z-score": 0.5720775535473553
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Graviton Propagator in a Covariant Massive Gravity Theory .\nAbstract:\nWe study the graviton propagator in covariant massive gravity theory with an arbitrary number of gravitons and show that it is given by the sum over all Feynman diagrams which are obtained by attaching one or more gravitons to each vertex of the tree-level graviton propagator. We also present explicit expressions for the first few terms in this expansion, including the leading order term corresponding to the usual Einstein-Hilbert action. The results presented here can be used as input into calculations involving higher-order corrections to gravitational processes such as black hole evaporation. In particular, we find that the inclusion of these additional contributions leads to modifications to the Hawking temperature at late times. \nI. INTRODUCTORY REMARkS\nThe purpose of this work is twofold. First, we will derive the exact expression for the graviton propagator (or Green s function) in covariant massive gravity theories with an arbitrary number of external gravitons. Second, we will use our result to calculate the effects on the Hawking radiation emitted by a Schwarzschild black hole due to the presence of extra degrees of freedom associated with the massive spin-2 field. Our analysis follows closely the approach developed in Ref.  1  , where the authors studied the effect of adding massless scalar fields to the standard model of particle physics on the emission rate of Hawking radiation  2  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Graviton Propagator in a Covariant Massive Gravity Theory . Abstract : We explore the graviton propagator in covariant massive gravity physics with an arbitrary number of gravitons and find that it is given by the sum over all Feynman diagrams which are derived by attaching one or more gravitons to each vertex of the tree - level graviton propagator .We additionally give explicit expressions for the first few terms in this expansion , notably the led order term corresponding to the usual Einstein - Hilbert action . The results presented here can be used as input into measurements involving higher - order corrections to gravitational processes such as black hole evaporation .In particular , we find that the inclusion of these additional contributions leads to modifications to the Hawking temperature at late times . I .INTRODUCTORY REMARkS The purpose of this study is twofold . First , we will generate the exact representation for the graviton propagator ( or Green s function ) in covariant massive gravity theories with an arbitrary number of external gravitons .Second , we will use our result to estimate the effects on the Hawking radiation emitted by a Schwarzschild black hole due to the presence of extra degrees of liberty involved with the huge spin - 2 field . Our estimate follows carefully the approach developed in Ref .1 , where the papers studied the impact of adding massless scalar fields to the standard theory of particle physics on the emission speed of Hawking radiation 2 .",
        "rewrite_text": "In this article, titled \"Graviton Propagator in a Covariant Massive Gravity Theory,\" we investigate the graviton propagator within the framework of covariant massive gravity, accommodating an arbitrary number of gravitons. Our findings reveal that the graviton propagator can be expressed as a summation over all Feynman diagrams, which are constructed by attaching one or more gravitons to each vertex of the tree-level graviton propagator. We provide explicit formulations for the initial terms in this series expansion, highlighting the leading-order term that corresponds to the conventional Einstein-Hilbert action. The implications of our results are significant, as they can serve as foundational input for analyzing higher-order corrections in gravitational phenomena, particularly in the context of black hole evaporation. Notably, we demonstrate that incorporating these additional contributions results in alterations to the Hawking temperature at late times.\n\nThe objectives of this study are twofold. Firstly, we aim to derive an exact representation of the graviton propagator (or Green's function) in covariant massive gravity theories, accommodating any number of external gravitons. Secondly, we intend to utilize our findings to assess the impact of these extra degrees of freedom, associated with the massive spin-2 field, on the Hawking radiation emitted by a Schwarzschild black hole. Our analysis closely follows the methodology established in previous research, which examined the effects of introducing massless scalar fields into the standard particle physics framework on the emission rate of Hawking radiation. Through this work, we contribute to a deeper understanding of the interplay between massive gravity theories and quantum gravitational effects, particularly in the context of black hole thermodynamics.",
        "ori-fast-z-score": 0.9901475429766744,
        "water-fast-z-score": 5.687367919007337,
        "rewrite-fast-z-score": 0.4745789978762495
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Simulation of Single Reed Instruments Oscillations Based on Modal Decomposition of Bore and Reed Dynamics .\nAbstract:\nThe reed is the key component in single-reed musical instruments, such as clarinets or saxophones. The dynamics of its motion are strongly influenced by the bore geometry and boundary conditions at both ends. In this work we present an approach to simulate the oscillations of a single reed instrument based on modal decomposition of the bore and reed dynamics. We show that it is possible to accurately reproduce the sound produced by a real clarinet using only one degree-of-freedom for each mode. This method can be used to study the influence of different parameters (such as mouthpiece diameter) on the acoustic response of the instrument without requiring expensive experimental measurements. It also allows us to investigate how changes in the design affect the performance of new prototypes. \n \n Keywords: Musical Instrument, Clarinet, Sound Generation, Modeling, Simulation, Acoustics, Vibration Analysis, Modal Analysis, Finite Element Method",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Simulation of Single Reed Instruments Oscillations Based on Modal Decomposition of Bore and Reed Dynamics . Abstract : The reed is the key ingredient in single - reed musical instruments , such as clarinets or saxophones .The dynamics of its motion are strongly dictated by the bore geometry and boundary conditions at both ends . In this study we present an approach to simulate the oscillations of a single reed instrument relying on modal decomposition of the bore and reed dynamics .We see that it is easy to correctly reproduce the music created by a real clarinet using only one degree - of - independence for each mode . This method can be used to study the impact of different variables ( such as mouthpiece diameter ) on the acoustic response of the instrument without using expensive experimental surveys .It additionally lets us to examine how differences in the model affect the performance of new prototypes . Keywords : Musical Instrument , Clarinet , Sound Generation , Modeling , Simulation , Acoustics , Vibration Analysis , Modal Analysis , Finite Element Method",
        "rewrite_text": "Title: Simulation of Oscillations in Single Reed Instruments Through Modal Decomposition of Bore and Reed Dynamics\n\nAbstract: Single-reed musical instruments, including clarinets and saxophones, rely heavily on the reed as a fundamental component influencing sound production. The motion dynamics of the reed are intricately linked to the geometry of the bore and the boundary conditions at both ends of the instrument. In this research, we introduce a novel simulation approach that utilizes modal decomposition to analyze the oscillations of single reed instruments, focusing on the interplay between bore and reed dynamics. Our findings demonstrate that it is feasible to accurately replicate the sound produced by a real clarinet using a simplified model that incorporates just one degree of freedom for each mode. This innovative method offers a valuable tool for investigating how various factors, such as mouthpiece diameter, affect the acoustic characteristics of the instrument, thereby reducing the need for costly experimental setups. Furthermore, our approach facilitates the evaluation of how variations in the modeling parameters can influence the performance of new instrument prototypes. By leveraging modal analysis and finite element methods, we provide insights into the vibration behavior and sound generation mechanisms of single reed instruments, contributing to the broader field of acoustics and musical instrument design. This study not only enhances our understanding of the physical principles governing sound production in single reed instruments but also opens avenues for future research and development in musical acoustics. \n\nKeywords: Musical Instrument, Clarinet, Sound Generation, Modeling, Simulation, Acoustics, Vibration Analysis, Modal Analysis, Finite Element Method.",
        "ori-fast-z-score": -1.9694638556693236,
        "water-fast-z-score": 3.7872527750454617,
        "rewrite-fast-z-score": 1.2451741707874968
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  ROXA: a new multi-frequency selected large sample of blazars with SDSS and 2dF optical spectroscopy .\nAbstract:\nWe present the results of an extensive spectroscopic survey for active galactic nuclei (AGN) in the southern hemisphere using the Sloan Digital Sky Survey (SDSS). The main goal is to select a complete flux-limited sample of radio-loud AGNs at redshifts z < 0.7, which we call ROXA (Radio Optical eXtragalactic Astronomy), by cross-correlating the FIRST 1.4 GHz radio source catalogue with the SDSS DR3 photometric database. We have obtained spectra for more than 1000 sources over an area of about 10,000 deg2. In this work we report on the selection criteria used to define our sample as well as its completeness and reliability. We also discuss some preliminary results concerning the properties of these objects such as their luminosity function and redshift distribution. This project has been funded by the European Space Agency under contract number 4000106131/16/NL/PA.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : ROXA : a new multi - frequency selected large sample of blazars with SDSS and 2dF optical spectroscopy . Abstract : We present the results of an extensive spectroscopic study for active galactic nuclei ( AGN ) in the southern hemisphere using the Sloan Digital Sky Survey ( SDSS ) .The main goal is to select a complete flux - limited sample of radio - loud AGNs at redshifts z < 0 . 7 , which we call ROXA ( Radio Optical eXtragalactic Astronomy ) , by cross - correlating the FIRST 1 . 4 GHz radio source catalogue with the SDSS DR3 photometric archive . We have achieved spectra for more than 1000 sources over an area of about 10 , 000 deg2 .In this research we publish on the selection procedures used to define our sample as well as its completeness and reliability . We especially consider some preliminary results relating the properties of these objects such as their luminosity function and redshift distribution .This project has been sponsored by the European Space Agency under contract number 4000106131 / 16 / NL / PA .",
        "rewrite_text": "We present the findings of a comprehensive spectroscopic investigation of active galactic nuclei (AGN) located in the southern hemisphere, utilizing data from the Sloan Digital Sky Survey (SDSS). The primary objective of this study is to establish a complete flux-limited sample of radio-loud AGNs with redshifts less than 0.7, which we have designated as ROXA (Radio Optical eXtragalactic Astronomy). This was accomplished by cross-referencing the FIRST 1.4 GHz radio source catalog with the SDSS DR3 photometric database. Our efforts have resulted in the acquisition of spectra for over 1,000 sources across an expansive area of approximately 10,000 square degrees. In this article, we detail the selection methodologies employed to curate our sample, along with an assessment of its completeness and reliability. Furthermore, we provide preliminary insights into the characteristics of these AGNs, including their luminosity function and redshift distribution. This research has been supported by the European Space Agency under contract number 4000106131/16/NL/PA, highlighting the collaborative effort behind this significant contribution to the field of extragalactic astronomy. The ROXA sample promises to enhance our understanding of the properties and distribution of radio-loud AGNs, paving the way for future studies that will delve deeper into the nature of these fascinating cosmic entities.",
        "ori-fast-z-score": 0.8819171036881969,
        "water-fast-z-score": 3.75,
        "rewrite-fast-z-score": 0.22360679774997896
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  ALMA as the ideal probe of the solar chromosphere .\nAbstract:\nThe Solar Chromosphere is an important component in our understanding of how the Sun works and its influence on Earth, but it has been difficult to study because of its tenuous nature.  ALMA (Atacama Large Millimeter/submillimeter Array) will be able to observe this region for the first time with unprecedented spatial resolution.   This talk will discuss some of the science that can be done using ALMA observations of the Solar Chromosphere. The Solar Chromosphere is one of the most enigmatic regions of the Sun. It lies between the photosphere and corona, and plays a crucial role in energy transport into the upper atmosphere. However, due to its extremely low density, direct observation of the chromosphere was not possible until recently when high-resolution images were obtained by space-based telescopes such as Hinode/SOT and SDO/AIA. In addition, ground-based observatories have also made significant progress towards studying the chromosphere through various techniques including spectropolarimetry, imaging spectroscopy, and speckle interferometry. Despite these advances, there are still many open questions about the physical processes occurring within the chromosphere which need to be addressed. For example, what causes the formation of dynamic structures like sunspots? How do magnetic fields affect plasma dynamics in the chromosphere? What is the relationship between chromospheric heating mechanisms and coronal mass ejections? These questions cannot be answered without detailed knowledge of the structure and dynamics of the chromosphere. To address them we require new observational data at higher spatial resolutions than ever before.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : ALMA as the ideal probe of the sun chromosphere . Abstract : The Solar Chromosphere is an important element in our understanding of how the Sun operates and its influence on Earth , but it has been difficult to study because of its tenuous nature .ALMA ( Atacama Large Millimeter / submillimeter Array ) will be possible to observe this area for the first time with incredible spatial resolution . This discussion will explore some of the science that can be performed using ALMA observations of the Solar Chromosphere .The Solar Chromosphere is one of the most enigmatic regions of the Sun . It lies between the photosphere and corona , and plays a crucial role in electricity travel into the inner environment .However , owing to its incredibly small abundance , direct observation of the chromosphere was not possible until recently when high - resolution images were obtained by space - based telescopes such as Hinode / SOT and SDO / AIA . In addition , land - based observatories have also taken important development towards studying the chromosphere through numerous techniques including spectropolarimetry , optical spectroscopy , and speckle interferometry .Despite these developments , there are still many open questions about the natural processes arising within the chromosphere which require to be addressed . For instance , what causes the formation of dynamic systems like sunspots ?How do magnetic waves affect plasma dynamics in the chromosphere ? What is the relationship between chromospheric heating systems and coronal mass ejections ?These questions cannot be answered without precise understanding of the composition and dynamics of the chromosphere . To address them we require novel observational data at higher spatial resolutions than ever before .",
        "rewrite_text": "**Title: ALMA as the Ideal Probe of the Solar Chromosphere**\n\n**Abstract:** The Solar Chromosphere represents a critical component in our comprehension of solar dynamics and its subsequent effects on Earth. However, studying this region has posed significant challenges due to its tenuous and elusive nature. The Atacama Large Millimeter/submillimeter Array (ALMA) offers a groundbreaking opportunity to observe the Solar Chromosphere with unprecedented spatial resolution, enabling insights that were previously unattainable. This article delves into the scientific potential of ALMA observations in elucidating the complexities of the Solar Chromosphere, which is situated between the photosphere and the corona and plays a vital role in the transfer of energy and magnetic fields into the solar atmosphere.\n\nHistorically, the chromosphere has remained one of the most enigmatic layers of the Sun, with its direct observation being limited until recent advancements in high-resolution imaging from space-based telescopes such as Hinode/SOT and SDO/AIA. Ground-based observatories have also made significant strides in chromospheric studies through various methodologies, including spectropolarimetry, optical spectroscopy, and speckle interferometry. Despite these advancements, numerous fundamental questions about the processes occurring within the chromosphere remain unanswered. For example, what mechanisms drive the formation of dynamic structures like sunspots? How do magnetic waves influence plasma behavior within this region? Furthermore, what is the connection between chromospheric heating mechanisms and coronal mass ejections?\n\nTo tackle these pressing inquiries, a precise understanding of the chromosphere's composition and dynamics is essential. The novel observational capabilities provided by ALMA promise to deliver high-resolution data that can illuminate these complex phenomena, paving the way for a deeper understanding of solar activity and its implications for space weather and terrestrial impacts. This discussion will highlight the transformative potential of ALMA in advancing our knowledge of the Solar Chromosphere and addressing the outstanding questions that continue to challenge solar physicists.",
        "ori-fast-z-score": 0.9622504486493763,
        "water-fast-z-score": 6.7390217698427435,
        "rewrite-fast-z-score": -1.1818181818181819
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  What are the Best Hierarchical Descriptors for Complex Networks? .\nAbstract:\nWe propose an algorithm to find optimal hierarchical descriptors (OHDs) that can be used as fingerprints in complex networks, such as social and biological systems. The OHDs represent network structures at different levels of granularity by using a tree-like structure with nodes representing subnetworks or clusters. We show how these trees can be constructed efficiently based on modularity maximization algorithms. In addition, we introduce a novel concept called  clustering coefficient  which is useful when constructing OHDs. Finally, we demonstrate our method through several examples including synthetic benchmark data sets and real-world networks. Our results suggest that the proposed approach provides more accurate descriptions than existing methods. This work was supported by JSPS KAKENHI Grant Number JP26287040. Keywords: Hierarchical descriptor, Clustering coefficient, Modularity Maximization Algorithm, Tree Structure, Fingerprint, Network Description",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  What are the Best Hierarchical Descriptors for Complex Networks?.Abstract : We suggest an algorithm to find optimal hierarchical descriptors ( OHDs ) that can be used as fingerprints in complex networks , such as social and biological environments . The OHDs represent system structures at different levels of granularity by using a tree - like structure with vertices representing subnetworks or communities .We see how these trees can be built easily using on modularity maximization algorithms . In addition , we provide a new notion called clustering coefficient which is important when constructing OHDs .Finally , we prove our technique through several examples namely natural benchmark data sets and actual - global networks . Our results show that the suggested approach offers more accurate descriptions than existing techniques .This project was supported by JSPS KAKENHI Grant Number JP26287040 . Keywords : Hierarchical descriptor , Clustering coefficient , Modularity Maximization Algorithm , Tree Structure , Fingerprint , Network Description",
        "rewrite_text": "Title: What are the Best Hierarchical Descriptors for Complex Networks?\n\nAbstract: In this study, we introduce a novel algorithm designed to identify optimal hierarchical descriptors (OHDs) that serve as effective fingerprints for complex networks, including those found in social and biological contexts. OHDs are structured representations that capture the organization of systems at varying levels of detail, utilizing a tree-like framework where the vertices symbolize subnetworks or communities. We demonstrate the straightforward construction of these hierarchical trees through the application of modularity maximization algorithms, which enhance the identification of community structures within networks. Furthermore, we introduce a new concept known as the clustering coefficient, which plays a crucial role in the development of OHDs. This coefficient aids in quantifying the degree to which nodes in a network tend to cluster together, thereby enriching the descriptor's ability to reflect the network's topology. To validate our proposed methodology, we conduct a series of experiments using both natural benchmark datasets and real-world global networks. The results of our analysis indicate that the OHDs generated by our approach provide significantly more precise and informative descriptions of complex networks compared to existing methods. This research was supported by the Japan Society for the Promotion of Science (JSPS) KAKENHI Grant Number JP26287040. \n\nKeywords: Hierarchical descriptor, Clustering coefficient, Modularity Maximization Algorithm, Tree Structure, Fingerprint, Network Description.",
        "ori-fast-z-score": -0.3721042037676254,
        "water-fast-z-score": 5.986302773458956,
        "rewrite-fast-z-score": 1.0540925533894598
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Physical and Spectral Characteristics of the T8 and Later-Type Dwarfs .\nAbstract:\nThe physical characteristics, spectral energy distributions (SEDs), and atmospheric properties of late-type dwarfs are reviewed in this chapter.  The SEDs for these objects have been measured by many authors using ground-based telescopes as well as space observatories such as ISO, Spitzer Space Telescope, and AKARI.  These observations show that the infrared flux increases with decreasing effective temperature.  In addition to their red colors, they also exhibit strong water vapor absorption bands at wavelengths longer than 2 microns.   Their near-infrared spectra reveal features due to methane gas which is abundant in their atmospheres.  They also display prominent atomic lines of potassium and sodium in their optical spectra.  Finally, we discuss how the observed photometric data can be used to determine the fundamental parameters of these objects including mass, radius, luminosity, surface gravity, age, metallicity, and distance.   ... This chapter reviews the physical characteristics, spectral energy distribution (SED) measurements, and atmospheric properties of late type dwarfs.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Physical and Spectral Characteristics of the T8 and Later - Type Dwarfs . Abstract : The physical qualities , spectral power distributions ( SEDs ) , and atmospheric characteristics of late - class dwarfs are reviewed in this chapter .The SEDs for these objects have been measured by many scientists using ground - based telescopes as also as space observatories such as ISO , Spitzer Space Telescope , and AKARI . These measurements show that the infrared flux changes with decreasing effective heat .In addition to their red colors , they even exhibit strong water vapor absorption groups at wavelengths greater than 2 microns . Their near - infrared spectra indicate features due to methane gas which is abundant in their atmospheres .They even exhibit prominent atomic patterns of potassium and sodium in their imaging spectra . Finally , we explain how the seen photometric data can be used to determine the fundamental variables of these objects including mass , diameter , luminosity , surface gravity , age , metallicity , and distance .. . . This section reviews the physical qualities , spectral power distribution ( SED ) observations , and atmospheric characteristics of late type dwarfs .",
        "rewrite_text": "This chapter provides a comprehensive review of the physical properties, spectral energy distributions (SEDs), and atmospheric characteristics of late-type dwarfs, specifically T8 and later classifications. Numerous studies utilizing both ground-based telescopes and space observatories, including the Infrared Space Observatory (ISO), Spitzer Space Telescope, and AKARI, have contributed to our understanding of these celestial objects. The gathered SED data reveal that the infrared flux of late-type dwarfs diminishes as their effective temperatures decrease. In addition to their distinct red hues, these dwarfs display significant absorption features due to water vapor at wavelengths exceeding 2 microns. Their near-infrared spectra are characterized by the presence of methane, a gas that is prevalent in their atmospheres, indicating the unique chemical compositions of these objects. Furthermore, the spectral analysis reveals notable atomic signatures of potassium and sodium, which are evident in their imaging spectra. This chapter also discusses how the photometric data obtained from these observations can be utilized to derive fundamental parameters of late-type dwarfs, such as mass, radius, luminosity, surface gravity, age, metallicity, and distance. By synthesizing these findings, we aim to enhance our understanding of the physical and atmospheric characteristics of T8 and later-type dwarfs, contributing to the broader field of stellar astrophysics.",
        "ori-fast-z-score": 0.105999788000636,
        "water-fast-z-score": 6.253987492037524,
        "rewrite-fast-z-score": 0.30151134457776363
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Isospin breaking in the yield of heavy meson pairs in e+e- annihilation near threshold .\nAbstract:\nWe study isospin-breaking effects on the production rate for heavy-light mesons (D, D*) and light-heavy mesons (D0, D0bar). We use an effective field theory approach to calculate these rates at leading order in perturbation theory. The results are compared with experimental data obtained by CLEO-c. \n \n Isospin symmetry plays an important role in hadronic physics. It relates states that differ only in their charge but have identical masses. In particular it implies that the strong decay widths of charged and neutral pions should be equal. However, this equality has been experimentally tested down to pion momenta as low as 1 MeV/c and deviations up to 20% were found  1  . These deviations can be explained within Chiral Perturbation Theory  2  , which predicts corrections proportional to powers of the momentum transfer between initial and final state particles. At higher energies, where the typical momentum transfers become larger than the chiral scale, one expects such corrections to vanish rapidly  3  .\n \nIn contrast, we consider here processes involving two heavy quarks close to threshold. Here, the typical momentum transfers are small enough so that non-perturbative contributions cannot be neglected anymore. As a consequence, even though the mass difference between charm and anti-charm quarks is tiny, there will still be significant differences between the corresponding cross sections  4  . \n \n This effect was first observed more than 20 years ago  5  when studying the production of charmed mesons in electron-positron collisions. Since then many experiments  6  -  8  have measured the ratio of the production rates for different combinations of heavy-meson pairs. While some of them find good agreement with theoretical predictions  9  based on Heavy Quark Effective Theory  10  , others disagree significantly  11  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Isospin breaking in the yield of heavy meson pairs in e + e - annihilation near threshold . Abstract : We research isospin - breaking effects on the production level for heavy - heavy mesons ( D , D * ) and light - heavy mesons ( D0 , D0bar ) .We use an efficient field model approach to estimate these rates at leading order in perturbation theory . The results are compared with theoretical data derived by CLEO - c . Isospin symmetry serves an important role in hadronic physics .It relates states that differ only in their charge but have equal masses . In particular it assumes that the strong decay widths of charged and neutral pions should be equal .However , this equality has been experimentally tested down to pion momenta as low as 1 MeV / c and deviations up to 20 % were found 1 . These deviations can be described within Chiral Perturbation Theory 2 , which predicts corrections proportional to powers of the velocity change between initial and final state particles .At higher energies , where the typical velocity transfers become bigger than the chiral scale , one expects such corrections to vanish swiftly 3 . In comparison , we consider here reactions involving two heavy quarks close to threshold .Here , the typical velocity transfers are small enough so that non - perturbative contributions need be forgotten anymore . As a consequence , even though the mass gap between charm and pro - charm quarks is tiny , there will still be considerable changes between the associated cross sections 4 .This phenomenon was first observed more than 20 centuries earlier 5 when examining the production of charmed mesons in electron - positron collisions . Since then many research 6 - 8 have recorded the proportion of the production rates for different combinations of heavy - meson pairs .While some of them find good agreement with theoretical predictions 9 based on Heavy Quark Effective Theory 10 , others disagree significantly 11 .",
        "rewrite_text": "**Title:** Isospin Breaking in the Yield of Heavy Meson Pairs in e⁺e⁻ Annihilation Near Threshold\n\n**Abstract:** This study investigates the effects of isospin breaking on the production rates of heavy-heavy mesons (D, D*) and light-heavy mesons (D0, D̄0) in the context of electron-positron annihilation near the threshold. Utilizing an efficient field model approach, we estimate these production rates at leading order within perturbation theory and compare our findings with theoretical data from the CLEO-c experiment. Isospin symmetry plays a crucial role in hadronic physics, linking states that differ solely in charge while maintaining equal masses. A key assumption of this symmetry is that the strong decay widths of charged and neutral pions are equivalent. However, experimental tests have revealed deviations of up to 20% in this equality, even at pion momenta as low as 1 MeV/c. These discrepancies can be accounted for using Chiral Perturbation Theory, which predicts corrections that are proportional to the velocity changes between initial and final state particles. At higher energy levels, where typical velocity transfers exceed the chiral scale, these corrections are expected to diminish rapidly. In contrast, our analysis focuses on reactions involving two heavy quarks near threshold, where the velocity transfers remain sufficiently small to neglect non-perturbative contributions. Despite the minimal mass difference between charm and anti-charm quarks, we anticipate significant variations in the associated cross sections. This phenomenon was first documented over two decades ago in studies of charmed meson production during electron-positron collisions. Since then, numerous investigations have examined the production rates of various heavy meson pair combinations. While some studies report results that align closely with theoretical predictions based on Heavy Quark Effective Theory, others reveal substantial discrepancies. Our research aims to clarify these inconsistencies and enhance the understanding of isospin breaking in heavy meson production.",
        "ori-fast-z-score": -0.5107539184552492,
        "water-fast-z-score": 5.979278639572285,
        "rewrite-fast-z-score": 3.1844529735483205
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Spitzer c2d Survey of Large, Nearby, Interstellar Clouds. IV. Lupus Observed with MIPS .\nAbstract:\nWe present the results of observations made by the Multiband Imaging Photometer for Spitzer (MIPS) in 24 and 70 micron bands toward the Lupus molecular clouds. The data were obtained as part of the Spitzer Space Telescope s  Cores to Disks  Legacy program. We have identified more than 1000 infrared point sources associated with these clouds using our source extraction technique. These include protostars, young stellar objects, and background galaxies. In addition we find that there are many extended emission features which may be related to outflows or other phenomena associated with star formation. A comparison between the observed number counts at 24 microns and those predicted based on models of interstellar dust suggests that most of the detected sources are likely to be low mass stars surrounded by disks. This is consistent with previous studies of this region. However, it appears that some fraction of the brightest sources could also be high-mass protostars.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  The Spitzer c2d Survey of Large, Nearby, Interstellar Clouds.IV.Lupus Observed with MIPS . Abstract : We present the results of measurements made by the Multiband Imaging Photometer for Spitzer ( MIPS ) in 24 and 70 micron bands toward the Lupus molecular clouds .The data were obtained as part of the Spitzer Space Telescope s Cores to Disks Legacy project . We have discovered more than 1000 infrared spot sources related with these clouds using our source extraction methodology .These include protostars , early stellar bodies , and background galaxies . In addition we find that there are many extended emitted features which may be connected to outflows or other processes associated with star formation .A matching between the seen amount counts at 24 microns and those predicted based on models of interstellar dust suggests that most of the emitted sources are likely to be low mass stars surrounded by disks . This is consistent with previous research of this area .However , it appears that some fraction of the brightest observers might actually be large - mass protostars .",
        "rewrite_text": "In this study, we present findings from the Multiband Imaging Photometer for Spitzer (MIPS) focusing on the Lupus molecular clouds, as part of the Spitzer Space Telescope's Cores to Disks Legacy project. Our analysis involved measurements in the 24 and 70 micron bands, leading to the identification of over 1,000 infrared point sources associated with these clouds. These sources encompass a diverse range of astronomical objects, including protostars, early stellar formations, and background galaxies. Furthermore, we observed numerous extended emission features that may be linked to outflows or other star formation processes. \n\nA comparative analysis of the observed counts at 24 microns against predictions derived from interstellar dust models indicates that the majority of the detected sources are likely low-mass stars surrounded by circumstellar disks. This finding aligns with prior research conducted in this region. However, our results also suggest that a portion of the brightest sources may represent high-mass protostars, indicating a more complex stellar population than previously understood. Overall, our observations contribute valuable insights into the structure and dynamics of star formation within the Lupus molecular clouds, enhancing our understanding of the processes that govern the birth of stars in these nearby interstellar environments.",
        "ori-fast-z-score": -0.46499055497527714,
        "water-fast-z-score": 4.588314677411235,
        "rewrite-fast-z-score": 0.20203050891044214
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Asteroseismology of alpha Cen A. Evidence of rotational splitting .\nAbstract:\nWe present new results on the asteroseismic analysis of the primary component in the binary system Alpha Centari, based on data obtained with the HARPS spectrograph at La Silla Observatory (Chile). We find evidence for two independent frequencies that are likely to be associated with rotationally split modes. The observed frequency pattern is consistent with theoretical predictions and suggests an inclination angle between 40°and 60°for this star. \n \n Keywords: Asteroseismology, Rotation, Binary stars, Oscillations, Frequency analysis, High-precision radial velocities, Alpha Centari ABSTRACT \n \n We report new results on the asterioseismic analysis of the main-sequence F-type star Alpha Centari A, which forms part of a close double system with its cooler companion B. Our study was carried out using high-precision radial-velocity measurements collected over more than four years by the HARPS instrument installed at ESO s 3.6-m telescope at La Silla Observatory (Chilean Andes), together with photometric observations made simultaneously with the CoRoT space mission. By applying standard techniques used in asteroseismology we have detected several periodicities in both datasets, including one signal whose periodicity corresponds exactly to the orbital period of the system. This finding confirms previous suggestions that the pulsational behaviour of this star may be influenced by tidal effects induced by its companion. In addition, our analysis reveals another set of signals corresponding to periods ranging from about 1 day up to almost 2 days. These signals can be explained as being due to rotationally split p-mode oscillations excited in the convective envelope of the star. Their presence provides strong support for the hypothesis that the surface of Alpha Centari A has been shaped by magnetic activity driven by dynamo processes operating within the convection zone.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Asteroseismology of α Cen A . Evidence of rotational separation .Abstract : We report new data on the asteroseismic study of the primary component in the binary system Alpha Centari , using on evidence derived with the HARPS spectrograph at La Silla Observatory ( Chile ) . We see evidence for two independent frequencies that are likely to be involved with rotationally split modes .The observed frequency trend is compatible with theoretical estimates and suggests an inclination angle between 40°and 60°for this star . Keywords : Asteroseismology , Rotation , Binary stars , Oscillations , Frequency assessment , High - precision radial velocities , Alpha Centari ABSTRACT We report new data on the asterioseismic study of the main - sequence F - class star Alpha Centari A , which forms part of a close double system with its warmer companion B .Our study was carried out utilizing large - precision radial - speed measurements collected over more than four years by the HARPS instrument located at ESO s 3 . 6 - m observatory at La Silla Observatory ( Chilean Andes ) , combined with photometric surveys made independently with the CoRoT space expedition . By applying traditional techniques employed in asteroseismology we have discovered numerous periodicities in both datasets , notably one signal whose periodicity corresponds exactly to the orbital period of the system .This finding indicates past proposals that the pulsational evolution of this star may be altered by tidal impacts generated by its companion . In addition , our analysis reveals another set of signals relating to periods ranging from about 1 day up to approximately 2 days .These transmissions can be understood as being related to rotationally split p - mode oscillations excited in the convective envelope of the star . Their presence provides strong evidence for the notion that the surface of Alpha Centari A has been shaped by magnetic activity driven by dynamo mechanisms operating within the convection zone .",
        "rewrite_text": "**Title: Asteroseismology of α Cen A: Evidence of Rotational Separation**\n\n**Abstract:** This study presents new findings from the asteroseismic analysis of Alpha Centauri A, the primary star in the binary system Alpha Centauri, utilizing high-precision radial velocity measurements obtained over four years with the HARPS spectrograph at La Silla Observatory in Chile. Our investigation reveals the presence of two distinct frequencies, which are likely associated with rotationally split modes. The observed frequency patterns align with theoretical predictions, suggesting an inclination angle for Alpha Centauri A between 40° and 60°. \n\nWe conducted our research by integrating extensive radial velocity data with independent photometric observations from the CoRoT space mission. Through the application of established asteroseismic techniques, we identified multiple periodic signals within both datasets. Notably, one of these signals corresponds precisely to the orbital period of the binary system, supporting earlier hypotheses that the pulsational characteristics of Alpha Centauri A may be influenced by tidal interactions with its companion star, Alpha Centauri B.\n\nFurthermore, our analysis uncovered additional signals with periods ranging from approximately 1 to 2 days. These signals are interpreted as rotationally split p-mode oscillations, which are excited within the star's convective envelope. The detection of these oscillations provides compelling evidence that the surface characteristics of Alpha Centauri A have been significantly influenced by magnetic activity, likely driven by dynamo processes occurring in its convective zone. This research enhances our understanding of the complex interactions within binary star systems and the effects of rotation and magnetic activity on stellar evolution.\n\n**Keywords:** Asteroseismology, Rotation, Binary Stars, Oscillations, Frequency Assessment, High-Precision Radial Velocities, Alpha Centauri.",
        "ori-fast-z-score": -1.2909944487358056,
        "water-fast-z-score": 6.627104836843802,
        "rewrite-fast-z-score": -0.3746343246326776
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Risk Assessment Algorithms Based On Recursive Neural Networks .\nAbstract:\nThe risk assessment algorithms based on recursive neural networks are proposed in this paper, which can be used to assess the risks for different types of financial assets and make decisions accordingly. The algorithm is composed by three parts: input data preprocessing, network training and output results analysis. In order to improve the accuracy of prediction, we use genetic algorithm (GA) to optimize the parameters of RNNs. Finally, an example is given to show how our method works. Keywords: Risk assessment; Financial asset; Genetic algorithm; Recurrent neural networks; Optimization. 1 Introduction With the rapid development of information technology, more and more people have access to online trading platforms such as Alibaba Group s Taobao Marketplace and Tencent Holdings  WeChat Pay. As a result, there has been growing interest among researchers in developing intelligent systems that can help investors make better investment decisions  1  . However, it remains challenging to develop accurate models due to the complexity of real-world problems  2  .\nIn recent years, artificial intelligence techniques have attracted increasing attention because they provide powerful tools for solving complex problems  3  , especially recurrent neural networks (RNN). Compared with traditional feed-forward neural networks  4  , RNNs have advantages over time series forecasting  5  -  8  . For instance, RNNs can learn long-term dependencies between inputs and outputs  9  . Therefore, RNNs are widely applied in many fields including stock market prediction  10  -  12  , traffic flow prediction  13  , energy consumption prediction  14  , etc..",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Risk Assessment Algorithms Based On Recursive Neural Networks . Abstract : The danger assessment methods based on recursive neural connections are proposed in this paper , which can be used to analyze the dangers for different kinds of financial investments and making decisions accordingly .The algorithm is composed by three components : input data preprocessing , network testing and input results analysis . In try to ensure the accuracy of prediction , we using genetic algorithm ( GA ) to optimize the variables of RNNs .Finally , an instance is given to give how our technique works . Keywords : Risk evaluation ; Financial investment ; Genetic algorithm ; Recurrent neural connections ; Optimization .1 Introduction With the fast development of electronic tech , more and more persons have access to online trading systems such as Alibaba Group s Taobao Marketplace and Tencent Holdings WeChat Pay . As a result , there has been growing interest among researchers in building smart systems that can help investors make better business decisions 1 .However , it remains challenging to develop precise models due to the complexity of real - time difficulties 2 . In recent months , artificial intelligence techniques have garnered additional attention because they enable powerful tools for solving complex problems 3 , particularly recurrent neural systems ( RNN ) .Compared with typical feed - forward neural systems 4 , RNNs have benefits over time series forecasting 5 - 8 . For instance , RNNs can know long - term dependencies between output and outputs 9 .Therefore , RNNs are widely applied in different fields including stock price forecast 10 - 12 , transport stream forecast 13 , transportation consumption probability 14 , etc . .",
        "rewrite_text": "**Title:** Risk Assessment Algorithms Based on Recursive Neural Networks\n\n**Abstract:** This paper introduces innovative risk assessment methodologies utilizing recursive neural networks (RNNs) to evaluate risks associated with various financial investments, thereby facilitating informed decision-making. The proposed algorithm comprises three key components: preprocessing of input data, testing of the neural network, and analysis of the resulting outputs. To enhance the predictive accuracy of the RNNs, we employ a genetic algorithm (GA) for the optimization of the network's variables. This approach not only improves the model's performance but also ensures that the risk assessments are robust and reliable. To illustrate the effectiveness of our methodology, we present a practical example demonstrating the application of our technique in a real-world scenario. \n\n**Keywords:** Risk assessment; Financial investment; Genetic algorithm; Recurrent neural networks; Optimization.\n\n**1 Introduction:** The rapid advancement of electronic technology has led to increased accessibility of online trading platforms, such as Alibaba Group's Taobao Marketplace and Tencent Holdings' WeChat Pay. Consequently, there is a heightened interest among researchers in developing intelligent systems that assist investors in making more informed business decisions. However, creating accurate predictive models remains a significant challenge due to the complexities inherent in real-time market dynamics. Recently, artificial intelligence techniques have gained traction as powerful tools for addressing these intricate problems, particularly through the use of recurrent neural networks (RNNs). Unlike traditional feed-forward neural networks, RNNs offer distinct advantages for time series forecasting, as they can effectively capture long-term dependencies between inputs and outputs. This capability has led to their widespread application across various domains, including stock price prediction, transportation flow forecasting, and consumer behavior analysis.",
        "ori-fast-z-score": -1.5666989036012806,
        "water-fast-z-score": 8.060433501697915,
        "rewrite-fast-z-score": 0.827605888602368
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Nanostructure and velocity of field-driven solid-on-solid interfaces moving under a phonon-assisted dynamic .\nAbstract:\nWe study the dynamics of an interface driven by a periodic external force in one dimension, using molecular dynamics simulations with Lennard-Jones particles at low temperatures (T = 0.1 − 1). We find that the interface moves as a whole when it is driven slowly enough; otherwise, it breaks into several segments which move independently. The number of segments increases with increasing driving frequency or decreasing temperature. In addition to these two regimes, we observe another regime where the interface exhibits stick-slip motion. This third regime occurs for intermediate values of the driving amplitude A and driving frequency f . For this regime, we show that there exists a scaling relation between the average velocity V , the driving frequency f , and the driving amplitude A: V ∝ Af 2 . Finally, we propose a simple model based on the concept of phonon-enhanced diffusion to explain our results. DOI: 10.1103/PhysRevE.77.020101",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Nanostructure and speed of field - driven solid - on - solid interfaces changing under a phonon - aided dynamic . Abstract : We research the dynamics of an interface pushed by a periodic external stress in one dimension , using polymer mechanics simulations with Lennard - Jones atoms at low temperatures ( T = 0 . 1 − 1 ) .We see that the interface moves as a whole when it is driven slowly sufficiently ; otherwise , it splits into numerous components which move independently . The amount of segments varies with varying drove frequency or decreasing temperature .In addition to these two regimes , we study another regime where the interface displays stick - slipping motion . This third regime happens for intermediate values of the driving frequency A and driving frequency f .For this regime , we show that there exists a scaling relation between the average velocity V , the driving frequency f , and the driving amplitude A : V [UNK] Af 2 . Finally , we propose a simple model based on the concept of phonon - enhanced diffusion to explain our results .DOI: 10.1103/PhysRevE.77.020101",
        "rewrite_text": "**Title:** Nanostructure and Speed of Field-Driven Solid-on-Solid Interfaces Changing Under a Phonon-Aided Dynamic\n\n**Abstract:** In this study, we investigate the dynamics of a one-dimensional interface subjected to periodic external stress, utilizing polymer mechanics simulations with Lennard-Jones atoms at low temperatures (T = 0.1 - 1). Our findings reveal that the interface exhibits collective motion when driven at sufficiently low speeds; however, at higher driving rates, it disintegrates into multiple segments that move independently. The number of these segments is influenced by both the driving frequency and the temperature, indicating a complex interplay between these parameters. Furthermore, we identify a third regime characterized by stick-slip behavior, which emerges at intermediate values of the driving frequency (f) and amplitude (A). In this regime, we establish a scaling relationship that connects the average velocity (V) of the interface to the driving frequency and amplitude, expressed as V ∝ A f². This relationship highlights the significant role of the driving parameters in determining the motion of the interface. To elucidate our observations, we propose a straightforward model grounded in the concept of phonon-enhanced diffusion, which accounts for the enhanced mobility of the interface under specific conditions. Our results contribute to a deeper understanding of the dynamic behavior of solid-on-solid interfaces in response to external stresses, with implications for various applications in materials science and nanotechnology. This work not only sheds light on the fundamental mechanisms governing interface dynamics but also opens avenues for future research in manipulating such systems for technological advancements. \n\nDOI: 10.1103/PhysRevE.77.020101",
        "ori-fast-z-score": -0.8340576562282991,
        "water-fast-z-score": 3.6293309315564883,
        "rewrite-fast-z-score": 3.771711342562273
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  TRUFAS, a wavelet based algorithm for the rapid detection of planetary transits .\nAbstract:\nWe present TRUFAS (Transit Recognition Using Fast Approximate Signatures), an efficient and robust transit search pipeline that uses fast approximate signatures to detect periodic signals in time series data. The method is particularly well suited for detecting short duration transits with high signal-to-noise ratio. We demonstrate its performance on simulated light curves generated by the Exoplanet Transit Database as well as real Kepler light curves. Our results show that TRUFAS can achieve higher efficiency than other algorithms while maintaining low false positive rates. \n \n Keywords: Transiting planet, Wavelets, Time-series analysis, False positives reduction, Planetary system characterization \n \n \n \n 1 Introduction \n \n Planets are detected indirectly through their gravitational effects upon their host stars. These effects include changes in stellar radius or luminosity caused by the passage of planets across the line-of-sight between the star and Earth. This phenomenon is known as a transit event. In order to characterize exoplanet systems it is necessary to identify these events efficiently and accurately. However, this task has been made more difficult due to the large number of false positives produced by systematic noise sources such as instrumental artifacts and astrophysical phenomena like eclipsing binaries and pulsating stars. \n \n To date there have been several methods developed specifically for identifying transit-like features within astronomical time series data. Some examples include: Box Least Squares (BLS)  1  , BLS+  2  , TrES  3  , TAP  4  , EXOTRANS  5  . While each of these techniques performs reasonably well under certain conditions they all suffer from one common drawback; they require significant computational resources when searching for multiple transit candidates simultaneously. For example, the most widely used technique, Box Least Squares, requires O(N3) operations where N is the length of the time series being analyzed  6  . As a result, many of these techniques cannot be applied directly to current and future surveys which will produce enormous amounts of data  7  8  9  . \n \n In recent years wavelet transforms have become increasingly popular for analyzing astronomical time series data  10",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : TRUFAS , a wavelet based algorithm for the quick detection of planetary transits . Abstract : We present TRUFAS ( Transit Recognition Using Fast Approximate Signatures ) , an efficient and strong transit search pipeline that using fast exact signatures to identify continuous patterns in time series information .The method is especially good suitable for detecting short length transits with high signal - to - noise ratio . We test its effectiveness on simulated light curves generated by the Exoplanet Transit Database as also as real Kepler light surfaces .Our results show that TRUFAS can attain better productivity than other methods while maintaining low false negative rates . Keywords : Transiting planet , Wavelets , Time - series investigation , False positives reduction , Planetary network detection 1 Introduction Planets are detected indirectly through their gravitational impacts upon their host stars .These effects include changes in stellar radius or luminosity caused by the travel of planets across the line - of - view between the star and Earth . This phenomenon is known as a transit event .In order to characterize exoplanet systems it is required to identify these changes easily and correctly . However , this job has been turned more challenging due to the huge amount of false positives created by widespread sound sources such as instrumental artifacts and astrophysical processes like eclipsing binaries and pulsating planets .To date there have been numerous mechanisms developed specifically for determining transit - like features within astronomical date cycle information . Some examples include : Box Least Squares ( BLS ) 1 , BLS + 2 , TrES 3 , TAP 4 , EXOTRANS 5 .While each of these procedures works relatively well under certain conditions they all suffer from one common drawback ; they demand significant computational resources when looking for multiple transit alternatives independently . For instance , the most commonly used technique , Box Least Squares , requires O ( N3 ) operations where N is the length of the time series being evaluated 6 .As a result , many of these procedures cannot be applied directly to recent and future surveys which will generate immense sums of evidence 7 8 9 . In recent generations wavelet transforms have developed increasingly popular for studying astronomical date sequence data 10",
        "rewrite_text": "**Title:** TRUFAS: A Wavelet-Based Algorithm for Rapid Detection of Planetary Transits\n\n**Abstract:** In this study, we introduce TRUFAS (Transit Recognition Using Fast Approximate Signatures), a robust and efficient pipeline designed for the rapid identification of planetary transits in time series data. TRUFAS employs fast exact signatures to detect continuous patterns, making it particularly adept at identifying short-duration transits with a high signal-to-noise ratio. We evaluate the performance of TRUFAS using both simulated light curves from the Exoplanet Transit Database and actual light curves from the Kepler mission. Our findings demonstrate that TRUFAS outperforms existing methods in terms of productivity while maintaining a low rate of false negatives. The significance of this work lies in its potential to enhance the detection of transiting exoplanets, which are inferred through their gravitational effects on host stars, leading to observable changes in stellar brightness during transit events. Accurately identifying these transits is crucial for characterizing exoplanet systems. However, the task is complicated by the prevalence of false positives arising from various sources, including instrumental noise and astrophysical phenomena such as eclipsing binaries and pulsating stars. Numerous algorithms have been developed to detect transit-like features in astronomical data, including Box Least Squares (BLS), BLS+, TrES, TAP, and EXOTRANS. While these methods perform adequately under specific conditions, they share a common limitation: they require substantial computational resources, particularly when searching for multiple transit candidates independently. For example, the widely used Box Least Squares method necessitates O(N³) operations, where N represents the length of the time series analyzed. Consequently, many existing techniques are not feasible for application to upcoming surveys that will produce vast amounts of data. In light of these challenges, wavelet transforms have gained traction as a powerful tool for analyzing astronomical time series data, paving the way for more efficient detection methods like TRUFAS. \n\n**Keywords:** Transiting planets, Wavelets, Time-series analysis, False positive reduction, Planetary detection networks.",
        "ori-fast-z-score": -2.9651150929518515,
        "water-fast-z-score": 6.974502000925911,
        "rewrite-fast-z-score": -2.0344711469278987
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Planck Scale Unification in a Supersymmetric Standard Model .\nAbstract:\nWe present the results of an analysis of the supersymmetric standard model with minimal supergravity boundary conditions at the grand unification scale, including all one-loop corrections to gauge and Yukawa couplings as well as two-loop contributions to the running of the soft supersymmetry breaking parameters.  We find that this scenario is compatible with current experimental bounds on sparticle masses if tan beta is large (tan beta > 50) or small (tan beta < 10). In addition we show how the lightest Higgs boson mass can be predicted within this framework for any value of tan beta between 1 and 60. Finally, we discuss the implications of our results for future searches for supersymmetry at colliders such as LHC. The supersymmetric standard model has been studied extensively over many years  1  . It provides a natural solution to the hierarchy problem by introducing new particles which cancel quadratic divergences associated with radiative corrections to the scalar potential  2  , while also providing a candidate particle for dark matter  3  .\nIn recent years there have been several studies  4  -  8  investigating whether it is possible to construct models where the electroweak symmetry breaking sector is described by the MSSM  9  but the underlying physics is governed by some more fundamental theory valid at higher energies. This approach is motivated by the fact that the MSSM suffers from fine-tuning problems  10  due to its sensitivity to unknown high-scale physics  11  . If these problems are solved then the MSSM may provide a good description of nature up to very high scales  12  . One possibility would be to embed the MSSM into a Grand Unified Theory  13  based upon SO(10), although other possibilities exist  14  . Another possibility is to consider theories with extra dimensions  15  -  17  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Planck Scale Unification in a Supersymmetric Standard Model . Abstract : We present the results of an assessment of the supersymmetric standard theory with minimal supergravity boundary constraints at the grand unification scale , comprising all one - loop corrections to gauge and Yukawa couplings as well as two - loop contributions to the running of the hard supersymmetry broken equations .We see that this situation is compatible with current experimental bounds on sparticle masses if tan beta is huge ( tan beta > 50 ) or small ( tan beta < 10 ) . In addition we prove how the lightest Higgs boson weight can be predicted within this framework for any value of tan beta between 1 and 60 .Finally , we explain the implications of our findings for future investigations for supersymmetry at colliders such as LHC . The supersymmetric standard theory has been studied thoroughly over numerous years 1 .It provides a natural solution to the hierarchy problem by creating new particles which cancel quadratic divergences associated with radiative corrections to the scalar potential 2 , while also offering a candidate particle for black material 3 . In recent years there have been numerous research 4 - 8 investigating whether it is easy to build theories where the electroweak symmetry breaking sector is described by the MSSM 9 but the underlying physics is governed by some more fundamental theory valid at higher energies .This method is prompted by the fact that the MSSM suffers from fine - tuned difficulty 10 due to its sensitivity to unknown high - scale physics 11 . If these problems are answered then the MSSM could give a better model of nature up to very high scales 12 .One possibility would be to embed the MSSM into a Grand Unified Theory 13 based upon SO ( 10 ) , although other possibilities exist 14 . Another possibility is to consider concepts with extra dimensions 15 - 17 .",
        "rewrite_text": "In this article, we explore the implications of the supersymmetric standard model (SSM) under minimal supergravity boundary conditions at the grand unification scale. Our analysis incorporates all one-loop corrections to both gauge and Yukawa couplings, as well as two-loop contributions to the evolution of the equations governing hard supersymmetry breaking. Our findings indicate that the model remains consistent with current experimental limits on sparticle masses, particularly when the parameter tan beta is either significantly large (tan beta > 50) or notably small (tan beta < 10). Furthermore, we demonstrate that the mass of the lightest Higgs boson can be accurately predicted within this framework for any tan beta value ranging from 1 to 60. \n\nWe also discuss the broader implications of our results for future research on supersymmetry, particularly in the context of collider experiments such as those conducted at the Large Hadron Collider (LHC). The supersymmetric standard model has been extensively studied over the years, providing a compelling solution to the hierarchy problem by introducing new particles that effectively cancel the quadratic divergences associated with radiative corrections to the scalar potential. Additionally, it offers a viable candidate for dark matter.\n\nRecent investigations have sought to determine whether it is feasible to construct theories where the electroweak symmetry breaking sector is described by the minimal supersymmetric standard model (MSSM), while the underlying physics is governed by a more fundamental theory that operates at higher energy scales. This inquiry is motivated by the MSSM's fine-tuning issues, which arise from its sensitivity to unknown high-scale physics. Addressing these challenges could enhance the MSSM's viability as a model of nature at very high energy scales. One potential avenue is to embed the MSSM within a Grand Unified Theory, such as those based on SO(10), although alternative frameworks, including those involving extra dimensions, also warrant consideration.",
        "ori-fast-z-score": -0.0873704056661038,
        "water-fast-z-score": 5.983058789716428,
        "rewrite-fast-z-score": -1.01418510567422
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Deconstructing triplet nucleon-nucleon scattering .\nAbstract:\nThe authors present the results of their study on the scattering between two protons and one neutron, which is known as the triton channel in nuclear physics.  They use an effective field theory to calculate the cross section for this process at low energies (below 100 MeV) using lattice QCD data obtained by other researchers.   The resulting theoretical predictions are compared with experimental measurements made over several decades by various groups around the world.    The agreement between experiment and theory is found to be good within uncertainties. This work was supported by the U.S. Department of Energy under Contract No. DE-AC02-05CH11231. In nuclear physics, there has been much interest recently in studying the interactions among three particles - specifically, how they affect the properties of nuclei such as helium-3 or carbon-12.  These processes can occur when high-energy cosmic rays strike Earth s atmosphere; however, it may also be possible that these reactions play some role in the formation of heavy elements during stellar evolution.  For example, scientists have proposed that helium-4 could form through a series of fusion reactions involving helium-3 and neutrons.  However, before we can understand what happens inside stars like our Sun, we need to know more about the fundamental interactions involved in these types of reactions.  To help us learn more about them, physicists at MIT used lattice quantum chromodynamics (QCD), a technique similar to those employed in high energy experiments but performed on computers instead of accelerators, to predict the behavior of certain nuclear reactions.  Specifically, they studied the reaction p+p+n --> d+d+n, where  p  stands for proton,  n  for neutron,  d  for deuteron, and  d+  means a positively charged deuteron.  Their calculations were based on...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Deconstructing triplet nucleon - nucleon scattering . Abstract : The authors present the results of their experiment on the scattering between two protons and one neutron , which is known as the triton channel in nuclear physics .They use an efficient field model to estimate the cross section for this process at low energies ( below 100 MeV ) using lattice QCD measurements obtained by other researchers . The resulting theoretical estimates are compared with theoretical measurements made over numerous years by various groups around the world .The agreement between experiment and theory is found to be excellent within uncertainties . This research was supported by the U . S . Department of Energy under Contract No .DE - AC02 - 05CH11231 . In nuclear science , there has been much interest lately in investigating the interactions among three particles - particularly , how they impact the properties of nuclei such as helium - 3 or carbon - 12 .These mechanisms can occur when high - energy cosmic rays strike Earth s atmosphere ; however , it could also be possible that these reactions serve some role in the formation of heavy components during stellar evolution . For instance , scientists have proposed that helium - 4 might form through a sequence of fusion compounds involving helium - 3 and neutrons .However , before we can comprehend what comes inside stars like our Sun , we require to knowledge more about the fundamental interactions involved in these kinds of reactions . To give us discover more about them , physicists at MIT utilized lattice quantum chromodynamics ( QCD ) , a technique similar to those utilized in high energy research but conducted on computers instead of accelerators , to predict the dynamics of certain nuclear compounds .Specifically , they examined the process p + p + n - - > d + d + n , where p sits for proton , r for neutron , d for deuteron , and d + means a positively charged deuteron . Their calculations were based on . . .",
        "rewrite_text": "**Title: Deconstructing Triplet Nucleon-Nucleon Scattering**\n\n**Abstract:** In this study, the authors investigate the scattering interactions between two protons and one neutron, a phenomenon referred to as the triton channel in nuclear physics. Utilizing an advanced field model, they estimate the cross section for this scattering process at low energies, specifically below 100 MeV, by leveraging lattice quantum chromodynamics (QCD) measurements previously obtained by other research teams. The theoretical predictions generated from their model are rigorously compared with experimental data collected over several years by various international research groups. Remarkably, the findings reveal a strong agreement between the theoretical estimates and experimental results, falling well within the established uncertainties.\n\nThis research is supported by the U.S. Department of Energy under Contract No. DE-AC02-05CH11231. Recent advancements in nuclear science have sparked considerable interest in the interactions among three nucleons, particularly their influence on the properties of light nuclei such as helium-3 and carbon-12. These interactions are not only relevant in terrestrial laboratories but also play a crucial role in astrophysical processes, such as the interactions that occur when high-energy cosmic rays collide with the Earth's atmosphere. Furthermore, these reactions may contribute to the synthesis of heavier elements during stellar evolution. For example, it has been suggested that helium-4 could be produced through a series of fusion reactions involving helium-3 and neutrons.\n\nTo deepen our understanding of the fundamental interactions governing these processes, researchers at MIT employed lattice QCD, a computational approach akin to those used in high-energy physics but executed on supercomputers rather than particle accelerators. Their analysis focused on the reaction p + p + n → d + d + n, where 'p' denotes a proton, 'n' a neutron, and 'd' a deuteron. The calculations performed provide critical insights into the dynamics of nuclear interactions, paving the way for enhanced comprehension of the mechanisms at play within stellar environments, including our Sun.",
        "ori-fast-z-score": 0.5107539184552492,
        "water-fast-z-score": 8.168873634345234,
        "rewrite-fast-z-score": -0.08362420100070908
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Helium abundance in galaxy clusters and Sunyaev-Zeldovich effect .\nAbstract:\nWe present new measurements of the helium mass fraction YHe = 0.24 ± 0.01 (statistical) ±0.02 (systematic), obtained by combining X-ray data on galaxy clusters with SZ observations, using the sample of 62 nearby relaxed galaxy clusters observed at high signal-to-noise ratio by Planck satellite. The results are consistent with previous determinations based on Chandra or XMM-Newton data alone. \n \n We also report an improved measurement of the Hubble constant H0 = 67.4±1.2 km s-1 Mpc-1, which is derived from our determination of the angular diameter distance to these clusters combined with their redshifts. This value agrees well with other recent estimates but has smaller statistical uncertainty than most of them. It is also compatible within 1 sigma with the local measurement inferred from Cepheid variables. \n \n Finally we use this dataset to test for possible deviations from standard cosmology due to massive neutrinos. Our analysis shows that current data do not allow us to detect any significant deviation from the predictions of ΛCDM model.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Helium abundance in galaxy clusters and Sunyaev - Zeldovich effect . Abstract : We report new studies of the helium mass fraction YHe = 0 . 24 ± 0 . 01 ( statistical ) ±0 . 02 ( systematic ) , obtained by combining X - ray data on galaxy clusters with SZ measurements , using the sample of 62 nearby relaxed galaxy galaxies studied at high signal - to - noise ratio by Planck satellite .The results are compatible with previous determinations based on Chandra or XMM - Newton data alone . We additionally report an better determination of the Hubble constant H0 = 67 . 4±1 . 2 km s - 1 Mpc - 1 , which is calculated from our determination of the angular length length to these clusters combined with their redshifts .This value agrees well with other recent estimates but has less statistical uncertainty than most of them . It is also consistent within 1 sigma with the local measurement inferred from Cepheid variables .Finally we utilize this dataset to test for probable deviations from standard cosmology resulting to massive neutrinos . Our study shows that current data do not enable us to identify any considerable deviation from the estimates of ΛCDM system .",
        "rewrite_text": "We present a comprehensive analysis of the helium mass fraction in galaxy clusters, reporting a value of YHe = 0.24 ± 0.01 (statistical) ± 0.02 (systematic). This determination is derived from a combination of X-ray observations and Sunyaev-Zeldovich (SZ) effect measurements, utilizing a sample of 62 nearby relaxed galaxy clusters that were analyzed at a high signal-to-noise ratio by the Planck satellite. Our findings are consistent with previous measurements obtained from Chandra and XMM-Newton data, reinforcing the reliability of our results. \n\nIn addition to the helium abundance, we provide an improved estimate of the Hubble constant, H0 = 67.4 ± 1.2 km s^-1 Mpc^-1. This value is calculated by correlating the angular diameter distances to the galaxy clusters with their redshift values. Our estimate aligns well with other recent measurements and exhibits a lower statistical uncertainty compared to many previous studies. Furthermore, it remains consistent within one sigma with the local Hubble constant derived from Cepheid variable observations.\n\nMoreover, we leverage this dataset to investigate potential deviations from standard cosmological models, specifically in relation to the presence of massive neutrinos. Our analysis indicates that the current data do not reveal any significant departures from the predictions of the ΛCDM model. This study contributes to the ongoing discourse in cosmology, providing valuable insights into the helium abundance in galaxy clusters and the implications for our understanding of the universe's expansion rate and composition.",
        "ori-fast-z-score": -0.10846522890932808,
        "water-fast-z-score": 5.467773927672753,
        "rewrite-fast-z-score": -0.47891314261057566
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Phase Spaces in Special Relativity: Towards Eliminating Gravitational Singularities .\nAbstract:\nWe present an overview of the phase space approach to special relativity, which is based on the concept that all physical quantities are represented by functions defined over a four-dimensional manifold called spacetime. The main idea behind this approach is that it allows one to eliminate gravitational singularities and thus provides a natural framework for describing quantum gravity phenomena. We also discuss some recent results obtained within this formalism. In particular we show how the standard description of black holes can be modified so as to avoid the appearance of such singularities. Finally, we argue that the phase space approach may provide new insights into the problem of time in quantum gravity theories. This article is part of the themed issue  Quantum gravity . Phase spaces have been used extensively in classical mechanics since their introduction by Hamilton more than 150 years ago. They represent the state of any system at each point in its configuration space (the set of all possible states) through a function known as the phasespace density. These densities evolve according to a continuity equation known as Liouville s theorem, which describes how they change with respect to time.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Phase Spaces in Special Relativity : Towards Eliminating Gravitational Singularities . Abstract : We present an overview of the phase space approach to special relativity , which is based on the idea that all physical quantities are represented by functions defined over a four - dimensional manifold called spacetime .The main idea behind this methodology is that it allows one to eliminate gravity singularities and therefore provides a natural framework for describing quantum gravitational processes . We also discuss some latest findings obtained within this formalism .In particular we prove how the standard description of black holes can be altered so as to eliminate the appearance of such singularities . Finally , we feel that the phase space approach may provide fresh insights into the issue of time in particle gravity physics .This page is part of the themed topic Quantum gravity . Phase spaces have been used heavily in classical mechanics since their arrival by Hamilton more than 150 years previously .They measure the state of any system at each point in its configuration space ( the set of all possible states ) through a function referred as the phasespace density . These densities change according to a continuity function referred as Liouville s theorem , which explains how they change with regard to time .",
        "rewrite_text": "Title: Phase Spaces in Special Relativity: Towards Eliminating Gravitational Singularities\n\nAbstract: This article provides a comprehensive overview of the phase space approach to special relativity, emphasizing its potential to address and eliminate gravitational singularities. In this framework, all physical quantities are represented as functions defined over a four-dimensional manifold known as spacetime. The core premise of this methodology is its ability to circumvent the issues associated with gravitational singularities, thereby offering a robust framework for the exploration of quantum gravitational phenomena. We delve into recent advancements made within this formalism, particularly highlighting how conventional descriptions of black holes can be modified to remove the occurrence of singularities. This alteration not only enhances our understanding of black holes but also opens new avenues for investigating the nature of time in the context of particle gravity physics. This discussion is situated within the broader theme of quantum gravity, a field that seeks to reconcile the principles of quantum mechanics with general relativity. Phase spaces, a concept introduced by Hamilton over 150 years ago, have been instrumental in classical mechanics, providing a means to characterize the state of a system at each point in its configuration space—the collection of all possible states. The evolution of these phase space densities is governed by a continuity principle known as Liouville's theorem, which describes how these densities evolve over time. By applying this established framework to special relativity, we aim to shed light on the intricate relationship between gravity and quantum mechanics, potentially leading to significant breakthroughs in our understanding of the universe.",
        "ori-fast-z-score": -0.5940885257860046,
        "water-fast-z-score": 5.6163768855364715,
        "rewrite-fast-z-score": 1.8782971010998233
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Astrophysical and Cosmological Implications of Large Volume String Compactifications .\nAbstract:\nWe study the implications for cosmology, astrophysics and particle physics of large volume string compactifications with fluxes in type IIA theory. We find that these models can be described by an effective supergravity action which is valid at energies below the Kaluza-Klein scale. The resulting four-dimensional effective potential has several interesting features. In particular it contains terms which are exponentially suppressed compared to those arising in conventional Calabi-Yau compactifications. These new contributions lead to novel effects such as moduli stabilization via non-perturbative corrections or supersymmetry breaking induced by gaugino condensation on D-branes wrapping internal cycles. Finally we discuss some phenomenological consequences of our results. This work was supported in part by NSF grant PHY-0456735. PACS numbers: 11.25.Wx, 04.50.+h, 98.80.Cq",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Astrophysical and Cosmological Implications of Large Volume String Compactifications . Abstract : We research the implications for cosmology , astrophysics and particle science of large volume string compactifications with fluxes in type IIA theory .We see that these models can be described by an efficient supergravity operation which is valid at energies below the Kaluza - Klein scale . The resulting four - dimensional effective potential has various interesting features .In particular it contains terms which are exponentially suppressed compared to those originating in standard Calabi - Yau compactifications . These new contributions bring to novel effects such as moduli stabilization via non - perturbative corrections or supersymmetry breaking induced by gaugino condensation on D - branes wrapping internal cycles .Finally we explain some phenomenological consequences of our findings . This research was supported in part by NSF grant PHY - 0456735 .PACS codes : 11 . 25 . Wx , 04 . 50 . + h , 98 . 80 . Cq",
        "rewrite_text": "Title: Astrophysical and Cosmological Implications of Large Volume String Compactifications\n\nAbstract: This study explores the implications of large volume string compactifications with fluxes within the framework of type IIA string theory, focusing on their relevance to cosmology, astrophysics, and particle physics. We demonstrate that these models can be effectively described using a supergravity framework that remains valid at energy scales below the Kaluza-Klein threshold. The analysis reveals that the resulting four-dimensional effective potential exhibits several intriguing characteristics. Notably, it includes terms that are exponentially suppressed in comparison to those arising from conventional Calabi-Yau compactifications. These novel contributions lead to significant phenomena, such as the stabilization of moduli through non-perturbative corrections and the potential for supersymmetry breaking, which can be induced by gaugino condensation occurring on D-branes that wrap around internal cycles of the compactified dimensions. Furthermore, we discuss the phenomenological implications of our findings, highlighting how these theoretical advancements may influence our understanding of the universe's structure and evolution. This research is partially funded by NSF grant PHY-0456735, and the findings are relevant to several areas of theoretical physics, as indicated by the associated PACS codes: 11.25.Wx, 04.50.+h, and 98.80.Cq.",
        "ori-fast-z-score": 0.7745966692414834,
        "water-fast-z-score": 4.318004318006477,
        "rewrite-fast-z-score": 1.6464638998453551
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Cooperative action in eukaryotic gene regulation: physical properties of a viral example .\nAbstract:\nThe human genome contains thousands of enhancers, which are non-coding DNA sequences that can activate transcription at distant sites. Enhancer activity is often cell-type specific and regulated by the binding of sequence-specific transcription factors (TFs). However, how TFs cooperate to regulate enhancer function remains poorly understood. Here we show that cooperative interactions between two distinct classes of TFs play an important role in regulating enhancer activity during lytic infection with herpes simplex virus 1 (HSV-1), a large double-stranded DNA virus. We find that HSV-1 encodes three proteins - VP16, ICP4 and ICP27 - each containing multiple domains capable of interacting directly or indirectly with other cellular proteins. Using biophysical methods, we demonstrate that these three viral proteins interact cooperatively on a single promoter element to drive expression of genes required for efficient viral replication. Our results suggest that cooperative protein-protein interactions may be a general mechanism used by viruses to control host gene expression.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Cooperative action in eukaryotic gene control : physical properties of a viral example . Abstract : The human genome contains thousands of enhancers , which are non - coding DNA sequences that can stimulate transcription at remote sites .Enhancer activity is often cell - class related and controlled by the interaction of sequence - specific transcription elements ( TFs ) . However , how TFs cooperate to affect enhancer function continues poorly explored .Here we find that cooperative relationships between two separate categories of TFs serve an important role in controlling enhancer activity during lytic infection with herpes simplex virus 1 ( HSV - 1 ) , a large double - stranded DNA virus . We see that HSV - 1 encodes three domains - VP16 , ICP4 and ICP27 - each carrying several domains capable of interacting directly or indirectly with other cellular proteins .Using biophysical methods , we prove that these three viral molecules interact cooperatively on a single promoter structure to drive expression of genes necessary for efficient viral replication . Our results propose that cooperative protein - gene relationships may be a general process used by viruses to affect host protein expression .",
        "rewrite_text": "Title: Cooperative Action in Eukaryotic Gene Control: Physical Properties of a Viral Example\n\nAbstract: The human genome is rich in enhancers, which are non-coding DNA sequences that play a crucial role in stimulating transcription from distant genomic locations. The activity of these enhancers is often specific to particular cell types and is regulated by the interactions of sequence-specific transcription factors (TFs). Despite the significance of these interactions, the mechanisms by which TFs collaborate to modulate enhancer function remain inadequately understood. In this study, we investigate the cooperative dynamics between two distinct categories of TFs and their impact on enhancer activity during the lytic phase of infection by herpes simplex virus 1 (HSV-1), a large double-stranded DNA virus. Our findings reveal that HSV-1 encodes three key proteins—VP16, ICP4, and ICP27—each possessing multiple domains that facilitate direct or indirect interactions with various cellular proteins. Through the application of biophysical techniques, we demonstrate that these three viral proteins engage in cooperative interactions at a single promoter region, thereby driving the expression of genes essential for effective viral replication. These results suggest that the cooperative relationships between proteins and genes may represent a fundamental strategy employed by viruses to manipulate host gene expression. This research not only enhances our understanding of viral gene regulation but also sheds light on the broader implications of protein cooperation in eukaryotic gene control mechanisms.",
        "ori-fast-z-score": -1.0945409092309881,
        "water-fast-z-score": 5.6163768855364715,
        "rewrite-fast-z-score": 0.5619514869490164
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  c2d Spitzer IRS Spectra of Disks around T Tauri Stars. III.  Ne II ,  Fe I , and H_2 gas-phase lines .\nAbstract:\nWe present new infrared spectra obtained with the Infrared Spectrograph (IRS) on board the Spitzer Space Telescope for four young stars in nearby open clusters. The targets are all classical T Tauri stars surrounded by circumstellar disks that have been previously studied at optical wavelengths using high-resolution spectroscopy to detect forbidden emission lines produced by ionized iron atoms Fe + . We find evidence for both neutral atomic hydrogen and molecular hydrogen in these objects based on detection of their ro-vibrational transitions near 2 microns. \n \n These observations provide important constraints on models of disk structure and evolution as well as physical conditions within protoplanetary disks. They also allow us to study chemical composition of the gaseous component of the disks. Finally, we use our results to estimate mass accretion rates onto central stars. Our main conclusions can be summarized as follows: \n \n 1. We confirm previous reports of strong  Ne II  12.81 micron line emission in three out of four observed sources. This is consistent with predictions made by theoretical models of photoevaporation of protoplanetary disks driven by intense ultraviolet radiation from central stars. \n \n 2. We report detection of several other ionic species including  S III  18.71 micron,  C II  158 micron, and  N II  122 micron. Their presence indicates significant ionization fraction in the innermost regions of the disks where temperatures exceed 1000 K. \n \n 3. We identify numerous ro-vibrational bands of molecular hydrogen in two of the observed systems. Emission features detected between 2.0-2.3 microns correspond to fundamental vibrational band of H2 1-0 S(1). Other prominent H2 lines include those associated with v=1-0 Q-branch of the first overtone transition 2-0 S(1), which appear in the range 2-2.2 microns.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  c2d Spitzer IRS Spectra of Disks around T Tauri Stars.III.Ne II , Fe I , and H _ 2 gas - phase lines . Abstract : We report new infrared spectra obtained with the Infrared Spectrograph ( IRS ) on board the Spitzer Space Telescope for four young stars in nearby open complexes .The targets are all classical T Tauri stars surrounded by circumstellar disks that have been previously examined at optical wavelengths using high - resolution spectroscopy to identify forbidden emission lines released by ionized iron atoms Fe + . We get information for both stable atomic hydrogen and molecular hydrogen in these objects based on discovery of their ro - vibrational transitions near 2 microns .These measurements give important restrictions on explanations of disk shape and evolution as well as physical conditions within protoplanetary disks . They especially allow us to study chemical composition of the gaseous constituent of the disks .Finally , we utilize our findings to estimate mass accretion levels onto primary stars . Our main results can be summarized as follows : 1 .We verified past reports of bright Ne II 12 . 81 micron line emission in three out of four seen sources . This is consistent with predictions making by theoretical theories of photoevaporation of protoplanetary disks powered by intense ultraviolet radiation from central planets .2 . We report measurement of several other ionic species namely S III 18 . 71 micron , C II 158 micron , and N II 122 micron .Their presence indicates considerable ionization fraction in the innermost parts of the disks where heat exceed 1000 K . 3 . We distinguish numerous ro - vibrational lines of molecular hydrogen in two of the studied structures .Emission features detected between 2 . 0 - 2 . 3 microns correspond to basic vibrational band of H2 1 - 0 S ( 1 ) . Other notable H2 lines include those associated with v = 1 - 0 Q - branch of the first overtone transition 2 - 0 S ( 1 ) , which appear in the range 2 - 2 . 2 microns .",
        "rewrite_text": "**Title:** c2d Spitzer IRS Spectra of Disks around T Tauri Stars. III. Ne II, Fe I, and H2 Gas-Phase Lines\n\n**Abstract:** In this study, we present new infrared spectral data acquired using the Infrared Spectrograph (IRS) aboard the Spitzer Space Telescope, focusing on four classical T Tauri stars located in nearby open clusters. These stars are enveloped by circumstellar disks that have previously been analyzed through high-resolution optical spectroscopy, which identified forbidden emission lines from ionized iron (Fe+). Our observations provide insights into both atomic hydrogen and molecular hydrogen, highlighted by the detection of their ro-vibrational transitions near 2 microns. These findings impose significant constraints on models explaining the morphology and evolution of protoplanetary disks, as well as the physical conditions prevailing within them. Notably, our results facilitate a deeper understanding of the chemical composition of the gaseous components within these disks. Additionally, we leverage our observations to estimate mass accretion rates onto the central stars.\n\nThe key findings of our research are as follows: First, we confirm previous observations of strong Ne II 12.81 micron line emission in three of the four sources studied, aligning with theoretical predictions regarding the photoevaporation of protoplanetary disks driven by intense ultraviolet radiation from the central stars. Second, we report the detection of several other ionic species, including S III at 18.71 microns, C II at 158 microns, and N II at 122 microns. The presence of these ions suggests a significant ionization fraction in the innermost regions of the disks, where temperatures exceed 1000 K. Lastly, we identify multiple ro-vibrational lines of molecular hydrogen in two of the observed disks. Emission features between 2.0 and 2.3 microns correspond to the fundamental vibrational band of H2 (1-0 S(1)), while additional notable H2 lines associated with the v = 1-0 Q-branch of the first overtone transition (2-0 S(1)) are observed in the range of 2.2 to 2.2 microns. These results contribute to our understanding of the physical and chemical processes occurring in protoplanetary disks around T Tauri stars.",
        "ori-fast-z-score": -1.2094157958139042,
        "water-fast-z-score": 6.345447650272827,
        "rewrite-fast-z-score": -0.08606629658238704
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  DWEB: A Data Warehouse Engineering Benchmark .\nAbstract:\nThe DWEB benchmark is an open-source data warehouse engineering benchmark that can be used to evaluate the performance and scalability of different data warehouse systems, including relational databases (e.g., Oracle), columnar stores (e.g., MonetDB) and NoSQL databases (e.g,. Cassandra). The benchmark consists of three main components: a query generator, a database server emulator, and a client application. In this work we present the design and implementation details for each component as well as our experiences in using it on several platforms. We also provide some insights into how the benchmark was developed and discuss its limitations. Finally, we compare the results obtained by running the benchmark against two state-of-the-art commercial products. This article is part of the Proceedings of the 20th International Conference on Database Systems for Advanced Applications (DASFAA 2015).\nIntroduction\n\nData warehouses are large repositories storing historical information about business processes or other types of activities. They have been widely adopted over the last decade due to their ability to support decision making at various levels within organizations  1  . However, with the increasing size of data warehouses, there has been growing interest in developing new techniques to improve their efficiency  2  .\nIn order to assess these new techniques, researchers need benchmarks capable of evaluating the performance and scalability characteristics of different data warehouse systems  3  , such as relational databases  4  , columnar stores  5  , and NoSQL databases  6  . Unfortunately, most existing benchmarks focus only on one type of system  7, 8  , which makes them unsuitable for comparing multiple technologies simultaneously  9  . To address this problem, we propose the DWEB benchmark  10  , which supports both traditional SQL queries and more advanced analytical operations  11  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : DWEB : A Data Warehouse Engineering Benchmark . Abstract : The DWEB benchmark is an open - source information warehouse engineering benchmark that can be used to analyze the performance and scalability of different data warehouse applications , notably relational databases ( e . g . , Oracle ) , columnar stores ( e . g . , MonetDB ) and NoSQL databases ( e . g , .Cassandra ) . The benchmark consists of three primary components : a query generator , a document server emulator , and a client application .In this project we present the development and implementation information for each product as well as our perspectives in utilizing it on numerous platforms . We additionally offer some insights into how the benchmark was developed and consider its limitations .Finally , we compare the results derived by running the benchmark against two state - of - the - art commercial products . This page is part of the Proceedings of the 20th International Conference on Database Systems for Advanced Applications ( DASFAA 2015 ) .Introduction Data warehouses are big repositories storing historical data about corporate processes or other types of activities . They have been widely adopted over the last decade due to their potential to support decision making at several levels within organizations 1 .However , with the increasing size of data warehouses , there has been growing interest in implementing new methodology to improve their productivity 2 . In order to examine these new strategies , researchers require benchmarks capable of evaluating the performance and scalability characteristics of different data warehouse technologies 3 , such as relational databases 4 , columnar stores 5 , and NoSQL databases 6 .Unfortunately , most existing benchmarks focus only on one sort of system 7 , 8 , which makes them unsuitable for comparing multiple technologies simultaneously 9 . To address this situation , we propose the DWEB benchmark 10 , which supports both traditional SQL queries and more advanced quantitative functions 11 .",
        "rewrite_text": "**Title: DWEB: A Data Warehouse Engineering Benchmark**\n\n**Abstract:** The DWEB benchmark represents an innovative open-source tool designed for evaluating the performance and scalability of various data warehouse applications, including relational databases (e.g., Oracle), columnar stores (e.g., MonetDB), and NoSQL databases (e.g., Cassandra). This benchmark comprises three essential components: a query generator, a document server emulator, and a client application. In this article, we detail the development and implementation processes for each component, sharing our experiences and insights gained from deploying the benchmark across multiple platforms. We also discuss the methodology behind the benchmark's creation and acknowledge its limitations. Furthermore, we present a comparative analysis of the benchmark results obtained from running it against two leading commercial products in the field. This work is part of the Proceedings of the 20th International Conference on Database Systems for Advanced Applications (DASFAA 2015).\n\nData warehouses serve as extensive repositories for historical data related to corporate processes and various activities, and their adoption has surged over the past decade due to their capacity to enhance decision-making at multiple organizational levels. However, as the volume of data within these warehouses continues to grow, there is an increasing demand for new methodologies aimed at boosting their efficiency. To explore these methodologies effectively, researchers need robust benchmarks that can assess the performance and scalability of diverse data warehouse technologies, including relational databases, columnar stores, and NoSQL databases. Unfortunately, many existing benchmarks are limited to evaluating a single type of system, rendering them inadequate for simultaneous comparisons across different technologies. To fill this gap, we introduce the DWEB benchmark, which accommodates both traditional SQL queries and more sophisticated quantitative functions, thereby facilitating a comprehensive evaluation of various data warehouse solutions.",
        "ori-fast-z-score": -0.17541160386140586,
        "water-fast-z-score": 6.552780424957784,
        "rewrite-fast-z-score": 0.6810052246069989
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dynamical 3-Space: A Review .\nAbstract:\nThe concept of dynamical 3-space is introduced in this review article as an alternative to the standard space-time picture of relativistic physics. The main idea behind it is that, instead of considering time and space separately, one should consider them together as a single entity called  dynamical 3-space . This new approach has several advantages over the traditional viewpoint; for example, it provides a natural explanation for why we experience time flow only forward (and not backward), while at the same time allowing us to preserve causality. In addition, it also allows us to explain how particles can travel faster than light without violating any physical laws. Finally, by introducing the concept of  quantum potential energy density  into our description of matter fields, we are able to provide a simple mathematical framework within which all known fundamental interactions between elementary particles may be described. We conclude with some remarks on possible future research directions based upon this novel theoretical perspective.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dynamical 3 - Space : A Review . Abstract : The concept of dynamical 3 - space is introduced in this review article as an alternative to the standard space - time view of relativistic physics .The main idea behind it is that , rather of considering time and space simultaneously , one should consider them combined as a single entity called dynamical 3 - space . This new approach has numerous benefits over the usual interpretation ; for example , it gives a natural explanation for why we experience time flow only ahead ( and not backward ) , while at the same time allowing us to restore causality .In addition , it also enables us to explain how particles can travel quicker than light without violating any physical rules . Finally , by bringing the idea of quantum potential energy density into our description of matter fields , we are able to provide a simple mathematical framework within which all known fundamental interactions between elementary particles may be described .We end with some remarks on potential later research paths based upon this novel theoretical perspective .",
        "rewrite_text": "In this review article titled \"Dynamical 3-Space,\" the authors present a novel framework that reinterprets the conventional understanding of space-time in relativistic physics. The central premise of this approach is to merge the concepts of time and space into a unified entity referred to as dynamical 3-space. This innovative perspective offers several advantages over traditional interpretations. Notably, it provides a compelling rationale for the unidirectional flow of time that we experience, elucidating why time appears to progress only forward rather than backward. Furthermore, this framework facilitates the restoration of causality, a critical aspect of physical theories.\n\nThe dynamical 3-space model also addresses the intriguing possibility of superluminal particle movement, suggesting mechanisms through which particles could exceed the speed of light without contravening established physical laws. This aspect of the theory opens new avenues for understanding the limits of particle dynamics and the nature of spacetime itself.\n\nMoreover, by incorporating the concept of quantum potential energy density into the analysis of matter fields, the authors establish a straightforward mathematical structure that encompasses all known fundamental interactions among elementary particles. This integration not only enhances our comprehension of particle physics but also paves the way for further theoretical developments.\n\nThe article concludes with reflections on prospective research directions that could emerge from this groundbreaking theoretical framework. By challenging the traditional space-time paradigm and proposing the dynamical 3-space model, the authors invite the scientific community to explore new possibilities in understanding the fundamental nature of reality. This review serves as a foundation for future investigations that may yield significant insights into the fabric of the universe.",
        "ori-fast-z-score": 0.9332565252573828,
        "water-fast-z-score": 4.744537732790449,
        "rewrite-fast-z-score": 1.1666666666666667
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  High angular resolution imaging of the circumstellar material around intermediate mass (IM) stars .\nAbstract:\nWe present high spatial and spectral resolution observations of two IM stars, HD 163296 and MWC 480, obtained with the Submillimeter Array at 1.3 mm wavelength. We detect several compact sources in both objects that are associated with dusty disks or envelopes surrounding these young stellar objects. The disk masses derived for these systems range between 0.1 to 0.5 Msun. In addition we find evidence for an extended component in the vicinity of HD 163296 which may be related to its outflow activity. These results demonstrate that even massive protoplanetary disks can form planets like our own solar system. Keywords: Circumstellar matter - Stars: Herbig Ae/Be - Massive star formation - Planet formation - Protostars - Young stellar objects: General - Millimeter waves - Submillimeter waves - Nearby galaxies - Radio astronomy - High energy astrophysics",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : High angular resolution detection of the circumstellar material around intermediate mass ( IM ) stars . Abstract : We report high spatial and spectral resolution measurements of two IM stars , HD 163296 and MWC 480 , obtained with the Submillimeter Array at 1 . 3 cm wavelength .We detect many compact sources in both objects that are identified with dusty disks or envelopes surrounding these young stellar bodies . The disk masses derived for these systems range between 0 . 1 to 0 . 5 Msun .In addition we find confirmation for an extended component in the vicinity of HD 163296 which may be connected to its outflow movement . These data demonstrate that even gigantic protoplanetary disks can form planets like our own solar system .Keywords : Circumstellar matter - Stars : Herbig Ae / Be - Massive star formation - Planet structure - Protostars - Young stellar bodies : General - Millimeter waves - Submillimeter waves - Nearby galaxies - Radio astronomy - High energy astrophysics",
        "rewrite_text": "We present a detailed study on the high angular resolution detection of circumstellar material surrounding intermediate mass (IM) stars, specifically focusing on HD 163296 and MWC 480. Utilizing the Submillimeter Array, we conducted measurements at a wavelength of 1.3 cm, achieving both high spatial and spectral resolution. Our observations revealed numerous compact sources associated with dusty disks or envelopes enveloping these young stellar objects. The estimated disk masses for HD 163296 and MWC 480 range from 0.1 to 0.5 solar masses (Msun), indicating significant material present in these systems. Furthermore, our findings provide evidence for an extended component around HD 163296, which may be linked to its outflow dynamics. This research underscores the potential for the formation of planetary systems within even the largest protoplanetary disks, akin to our own solar system. The implications of these results contribute to our understanding of massive star formation and the processes that govern the evolution of circumstellar environments. Our study highlights the importance of high-resolution observational techniques in unraveling the complexities of star formation and the development of planetary structures in the vicinity of young stellar bodies. The keywords associated with this research include circumstellar matter, Herbig Ae/Be stars, massive star formation, planetary structure, protostars, young stellar objects, millimeter and submillimeter waves, nearby galaxies, radio astronomy, and high-energy astrophysics. This work not only advances our knowledge of IM stars but also opens avenues for further exploration of the mechanisms that lead to planet formation in diverse astrophysical contexts.",
        "ori-fast-z-score": -0.23249527748763857,
        "water-fast-z-score": 3.9000674757995495,
        "rewrite-fast-z-score": 1.4485719366802965
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Non-Abelian hydrodynamics and the flow of spin in spin-orbit coupled substances .\nAbstract:\nWe study non-Abelian hydrodynamic equations for fluids with spin-orbit coupling, which are derived by applying Noether s theorem to an action functional describing the dynamics of such systems. We show that these equations can be written as a system of conservation laws for charge current density Jμc , energy-momentum tensor Tμν and spin current density JSμ . The latter is given by a sum over all particles of their individual spins Sα multiplied by certain coefficients depending on the particle type α = e, μ, τ .\nThe resulting transport coefficients are calculated explicitly using kinetic theory methods. In particular we find that the shear viscosity ηs vanishes identically if there exists at least one electrically charged fermion species (e.g., electrons) or if the fluid contains only neutral bosons like photons. This result holds both for relativistic and nonrelativistic fluids. Furthermore, we calculate the bulk viscosities for various examples including QED plasma, superfluid helium-4, and ultracold atomic gases. Finally, we discuss how our results could be used to describe the collective motion of atoms in Bose-Einstein condensates. \nI. INTRODUCTORY REMARK\nIn this work we consider fluids whose constituents have internal degrees of freedom described by quantum fields. Examples include plasmas consisting of charged particles interacting via electromagnetic field, superfluids made up of neutral bosonic atoms, and cold atom clouds where the atoms are treated as distinguishable particles. For simplicity, we will assume that the number densities of different types of particles do not change significantly during time evolution so that they may be considered constant.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Non - Abelian hydrodynamics and the flow of spin in spinning - orbit connected molecules . Abstract : We research non - Abelian hydrodynamic equations for fluids with spin - orbit coupling , which are derived by using Noether s theorem to an action functional describing the dynamics of such systems .We see that these equations can be written as a system of conservation laws for charge current density Jμc , energy - momentum tensor Tμν and spin current density JSμ . The latter is given by a sum over all ions of their individual spins Sα multiplied by certain coefficients depending on the particle type α = e , μ , τ .The resulting travel coefficients are measured explicitly utilizing kinetic theory techniques . In particular we find that the shear viscosity ηs vanishes identically if there exists at least one electrically charged fermion species ( e . g . , electrons ) or if the liquid includes only neutral bosons like photons .This result holds both for relativistic and nonrelativistic fluids . Furthermore , we estimate the bulk viscosities for various examples namely QED gas , superfluid helium - 4 , and ultracold atomic fluids .Finally , we talk how our findings may be used to explain the collective motion of atoms in Bose - Einstein condensates . I .INTRODUCTORY REMARK In this study we study fluids whose constituents have internal degrees of liberty described by quantum fields . Examples involve plasmas consisting of charged particles communicating via electromagnetic field , superfluids composed up of neutral bosonic atoms , and cold particle clouds where the atoms are treated as distinguishable molecules .For simplicity , we will assume that the number densities of different kinds of molecules do not change considerably during time progression so that they may be regarded constant .",
        "rewrite_text": "**Title:** Non-Abelian Hydrodynamics and the Flow of Spin in Spin-Orbit Coupled Molecules\n\n**Abstract:** This article investigates the non-Abelian hydrodynamic equations governing fluids with spin-orbit coupling, derived through the application of Noether's theorem to an action functional that encapsulates the dynamics of these systems. We demonstrate that these equations can be reformulated as a set of conservation laws pertaining to charge current density \\( J^\\mu_c \\), energy-momentum tensor \\( T^{\\mu\\nu} \\), and spin current density \\( J^S_\\mu \\). The spin current density is expressed as a summation over all constituent ions, where each ion's individual spin \\( S_\\alpha \\) is weighted by coefficients that are specific to the particle type \\( \\alpha \\) (e.g., electrons, muons, and tau particles). We employ kinetic theory techniques to explicitly measure the resulting transport coefficients. Notably, we find that the shear viscosity \\( \\eta_s \\) is identically zero in the presence of at least one electrically charged fermion species (such as electrons) or when the fluid consists solely of neutral bosons like photons. This finding is applicable to both relativistic and non-relativistic fluid scenarios. Additionally, we provide estimates for the bulk viscosities in various contexts, including quantum electrodynamics (QED) gas, superfluid helium-4, and ultracold atomic fluids. Our results have implications for understanding the collective dynamics of atoms in Bose-Einstein condensates. \n\n**I. INTRODUCTORY REMARK** In this research, we focus on fluids whose components possess internal degrees of freedom described by quantum fields. Examples include plasmas made up of charged particles interacting via electromagnetic fields, superfluids formed from neutral bosonic atoms, and cold atomic clouds where the atoms are treated as distinguishable molecules. For the sake of simplicity, we assume that the number densities of the different types of molecules remain relatively constant over time, allowing us to treat them as fixed during our analysis.",
        "ori-fast-z-score": 0.0854357657716761,
        "water-fast-z-score": 5.766441871019932,
        "rewrite-fast-z-score": 1.2004900959975617
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Bond breaking with auxiliary-field quantum Monte Carlo .\nAbstract:\nWe present an algorithm for the calculation of ground-state properties in fermionic systems using auxiliary field quantum Monte Carlo (AFQMC). The method is based on the use of a trial wave function that incorporates information about the broken bonds and their associated energy penalty, which are determined by exact diagonalization or density-functional theory calculations. We demonstrate our approach to calculate the electronic structure of silicon clusters Si_(n) with n = 2-10 atoms at zero temperature. Our results show good agreement with previous theoretical studies as well as experimental data obtained from photoelectron spectroscopy experiments. In addition we study the structural stability of these clusters against dissociation into smaller fragments. Finally, we discuss possible extensions of this work towards larger cluster sizes and finite temperatures. Quantum Monte Carlo methods have been widely used over recent years to solve many-body problems in condensed matter physics  1  . These techniques provide accurate estimates of physical quantities such as energies, correlation functions, and other observables within statistical uncertainties  2  .\nIn particular, the Auxiliary Field QMC (AFQMC) technique has proven very useful for studying strongly correlated electron systems  3, 4  , including materials like transition metal oxides  5  , high-temperature superconductors  6  , and heavyfermion compounds  7, 8  . This method can be applied to any system described by a local Hamiltonian H = T + V where T denotes the kinetic part and V represents the interaction between particles  9  . It relies on the introduction of a trial wave function |ΨT⟩ that approximates the true ground state |Ψ0⟩ of the system under consideration  10  . Then, the expectation value ⟨O⟩ of some observable O can be calculated through the expression",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Bond breaking with auxiliary - field quantum Monte Carlo . Abstract : We present an algorithm for the determination of ground - state properties in fermionic models using auxiliary field quantum Monte Carlo ( AFQMC ) .The method is based on the using of a trial wave function that incorporates information about the broken bonds and their accompanying energy penalty , which are decided by precise diagonalization or density - functional theory analyses . We test our approach to estimate the electronic structure of silicon bands Si _ ( p ) with n = 2 - 10 atoms at zero temperature .Our results show good agreement with previous conceptual research as well as empirical data acquired from photoelectron spectroscopy observations . In addition we study the structural integrity of these complexes against dissociation into tiny pieces .Finally , we explain possible extend of this research towards higher cluster sizes and finite temperatures . Quantum Monte Carlo methods have been widely using over recent years to solve many - bodies problems in condensed matter theory 1 .These methods provide accurate calculations of physical quantities such as energies , correlation functions , and other observables within statistical uncertainties 2 . In particular , the Auxiliary Field QMC ( AFQMC ) method has proven very useful for studying strongly coupled electron structures 3 , 4 , notably materials like transition metal oxides 5 , low - temperature superconductors 6 , and heavyfermion compounds 7 , 8 .This method can be applied to any system characterized by a local Hamiltonian H = T + V where T denotes the kinetic component and V means the interaction between particles 9 . It depends on the introduction of a trial wave function | ΨT ⟩ that approximates the true ground state | Ψ0 ⟩ of the system under consideration 10 .Then , the expectation value ⟨ O ⟩ of some observable O can be determined through the expression",
        "rewrite_text": "We introduce a novel algorithm designed to ascertain ground-state properties in fermionic models through the application of auxiliary field quantum Monte Carlo (AFQMC) techniques. This innovative method leverages a trial wave function that encapsulates information regarding broken bonds and the associated energy penalties, which are determined via precise diagonalization or density functional theory analyses. To validate our approach, we investigate the electronic structure of silicon bands, specifically Si_(p), with atom counts ranging from n = 2 to 10 at absolute zero temperature. Our findings demonstrate a strong correlation with previous theoretical studies and empirical data obtained from photoelectron spectroscopy experiments. Furthermore, we assess the structural stability of these silicon complexes against fragmentation into smaller components. We also discuss potential avenues for extending this research to encompass larger cluster sizes and finite temperature conditions.\n\nQuantum Monte Carlo methods have gained significant traction in recent years for addressing many-body problems within condensed matter physics. These methodologies yield highly accurate calculations of various physical quantities, including energies, correlation functions, and other observables, while maintaining manageable statistical uncertainties. Notably, the Auxiliary Field QMC (AFQMC) method has emerged as a powerful tool for exploring strongly correlated electron systems, particularly in materials such as transition metal oxides, low-temperature superconductors, and heavy fermion compounds. The versatility of this method allows it to be applied to any system characterized by a local Hamiltonian, expressed as H = T + V, where T represents the kinetic energy component and V denotes the interaction between particles. The efficacy of the AFQMC approach hinges on the introduction of a trial wave function |ΨT⟩, which serves as an approximation for the true ground state |Ψ0⟩ of the system in question. Consequently, the expectation value ⟨O⟩ of a given observable O can be computed through established expressions, paving the way for deeper insights into the quantum behavior of complex materials.",
        "ori-fast-z-score": -2.090605025017727,
        "water-fast-z-score": 5.296677687055155,
        "rewrite-fast-z-score": -0.6405126152203485
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Daemons and DAMA: Their Celestial-Mechanics Interrelations .\nAbstract:\nThe article is devoted to the problem of possible existence of dark matter particles in our Galaxy, which are not detected by other methods than their gravitational effects on visible objects (stars). The author considers the possibility that these hypothetical particles can be described as celestial mechanics daemons with certain properties. In particular, it is shown how such daemons could explain some features observed recently for the DAMA experiment at Gran Sasso National Laboratory. It should be noted that this explanation does not contradict any known experimental data. However, there are also serious difficulties associated with the proposed model. These problems will require further study. This work was supported by Russian Science Foundation grant No 14-50-00040. URL: http://arxiv.org/abs/1409.5189 . \nI. INTRODUCTORY REMARK .\nDark Matter (DM) is one of the most important mysteries of modern physics  1  -  4  . Its presence has been established only indirectly through its gravitational influence on visible stars  5  , galaxies  6  , clusters  7  etc., but direct detection experiments have so far failed  8  -  10  . There exist many theoretical models describing DM  11  -  13  ; however, none of them has yet been confirmed experimentally  14  . One of the possibilities is that DM consists of new elementary particles  15  -  17  . If they interact weakly or electromagnetically with ordinary matter then they would escape detection even if they were produced in large quantities  18  . On the other hand, if they interact strongly enough with normal matter, then they may be detectable directly  19  -  21  . A number of experiments searching for DM particles have been carried out  22  -  26  . Recently, the results obtained by the DAMA collaboration  27  attracted considerable attention  28  -  30  . According to these results, the annual modulation effect  31  -  33  caused by the motion of Earth around Sun  34  -  36  leads to an increase in the rate of nuclear recoils registered by detectors during June-October period  37  compared to December-February period. Such behavior cannot be explained within Standard Model of particle interactions  38  -  41  . Several authors suggested different explanations based on",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Daemons and DAMA : Their Celestial - Mechanics Interrelations . Abstract : The essay is devoted to the question of possible existence of dark matter ions in our Galaxy , which are not observed by other methods than their gravitational impacts on visible objects ( stars ) .The author considers the prospect that these hypothetical particles can be described as celestial mechanics daemons with certain characteristics . In particular , it is demonstrated how such daemons might explain some features detected lately for the DAMA experiment at Gran Sasso National Laboratory .It should be mentioned that this explanation does not contradict any established experimental evidence . However , there are also serious difficulties linked with the suggested model .These difficulties will demand further study . This project was supported by Russian Science Foundation award No 14 - 50 - 00040 .URL : www : / / arxiv . org / abs / 1409 . 5189 . I .INTRODUCTORY REMARK . Dark Matter ( DM ) is one of the most important puzzles of modern physics 1 - 4 .Its presence has been known only indirectly through its gravitational impact on visible stars 5 , galaxies 6 , galaxies 7 etc . , but direct detection experiments have so far unsuccessful 8 - 10 . There operate several experimental scenarios describing DM 11 - 13 ; however , none of them has already been confirmed experimentally 14 .One of the possibilities is that DM consists of new primary objects 15 - 17 . If they interact weakly or electromagnetically with normal matter then they may survive discovery even if they were produced in large quantities 18 .On the other hand , if they interact heavily enough with normal matter , then they may be detectable directly 19 - 21 . A couple of studies looking for DM particles have been carried out 22 - 26 .Recently , the results derived by the DAMA collaboration 27 drew substantial scrutiny 28 - 30 . According to these results , the annual modulation effect 31 - 33 generated by the movement of Earth around Sun 34 - 36 results to an increase in the frequency of nuclear recoils registered by detectors during June - October year 37 contrast to December - February time .Such interactions cannot be described within Standard Model of particle behavior 38 - 41 . Several scientists suggested different explanations based on",
        "rewrite_text": "**Title:** Daemons and DAMA: Their Celestial Mechanics Interrelations\n\n**Abstract:** This article explores the intriguing possibility of dark matter ions existing within our Galaxy, which remain undetectable through conventional methods aside from their gravitational influence on visible celestial bodies, such as stars. The author proposes that these hypothetical particles could be conceptualized as celestial mechanics daemons, possessing unique characteristics that may account for certain anomalies observed in the DAMA experiment conducted at the Gran Sasso National Laboratory. Notably, this interpretation aligns with existing experimental data and does not contradict established findings in the field. However, the proposed model also presents significant challenges that warrant further investigation. The research is supported by the Russian Science Foundation under award No. 14-50-00040.\n\nDark Matter (DM) represents one of the most significant enigmas in contemporary physics, with its existence inferred primarily through gravitational effects on visible matter, including stars and galaxies. Despite extensive efforts, direct detection of DM has proven elusive, leading to various experimental frameworks aimed at elucidating its nature. None of these frameworks have yet received experimental validation. One hypothesis posits that DM may consist of novel primary particles that interact weakly or electromagnetically with ordinary matter, potentially evading detection even if produced in substantial quantities. Conversely, if these particles interact more strongly with normal matter, they could be directly observable.\n\nRecent findings from the DAMA collaboration have sparked considerable debate, particularly regarding the annual modulation effect observed in nuclear recoil events, which appear to vary with the Earth's position relative to the Sun. This phenomenon, which cannot be adequately explained by the Standard Model of particle physics, has prompted various theoretical interpretations. The current study aims to contribute to this discourse by examining the celestial mechanics of dark matter through the lens of daemons, thereby offering a novel perspective on the ongoing quest to understand the elusive nature of dark matter.",
        "ori-fast-z-score": -0.08192319205190406,
        "water-fast-z-score": 8.16496580927726,
        "rewrite-fast-z-score": 0.8111071056538127
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Investigations of the Non-Linear LMC Cepheid Period-Luminosity Relation with Testimator and Schwarz Information Criterion Methods .\nAbstract:\nThe Large Magellanic Cloud (LMC) is an ideal laboratory for studying Galactic structure, stellar populations, chemical evolution, and cosmology because it has many advantages over other nearby galaxies such as M31 or M33.  The distance to the LMC can be determined by using Cepheids which are bright periodic variable stars that pulsate in radial fundamental mode. In this study we used two different methods to determine distances to Cepheids in the LMC. First method was based on non-linear least squares fitting technique called testimator while second one was based on statistical analysis known as Schwarz information criterion (SIC). We found that both methods gave consistent results within their uncertainties. Our final sample consists of 1228 Cepheids located between 30 < R < 50 kpc from the center of the galaxy. Using these data sets we derived new period-luminosity relations for classical Cepheids in the infrared bands JHKs.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Investigations of the Non - Linear LMC Cepheid Period - Luminosity Relation with Testimator and Schwarz Information Criterion Methods . Abstract : The Large Magellanic Cloud ( LMC ) is an excellent laboratory for studying Galactic structure , planetary populations , chemical evolution , and cosmology because it has numerous benefits over other nearby galaxies such as M31 or M33 .The distance to the LMC can be determined by using Cepheids which are bright periodic variable stars that pulsate in radial fundamental mode . In this study we using two different methods to find distances to Cepheids in the LMC .First method was based on non - linear least squares fit technique called testimator while second one was based on statistical analysis called as Schwarz evidence criterion ( SIC ) . We showed that both approaches made satisfactory findings within their uncertainties .Our last sample consists of 1228 Cepheids situated between 30 < R < 50 kpc from the center of the universe . Using these information sets we derived additional time - luminosity relations for classical Cepheids in the infrared bands JHKs .",
        "rewrite_text": "**Title:** Investigations of the Non-Linear LMC Cepheid Period-Luminosity Relation Using Testimator and Schwarz Information Criterion Methods\n\n**Abstract:** The Large Magellanic Cloud (LMC) serves as a prime environment for exploring various aspects of Galactic structure, planetary populations, chemical evolution, and cosmology, offering distinct advantages over other nearby galaxies such as M31 and M33. A key method for determining the distance to the LMC involves the use of Cepheid variables, which are luminous stars that exhibit periodic pulsations in their fundamental radial mode. In this research, we employed two distinct methodologies to ascertain the distances to Cepheids located within the LMC. The first approach utilized a non-linear least squares fitting technique known as Testimator, while the second method relied on a statistical framework referred to as the Schwarz Information Criterion (SIC). Our findings indicate that both techniques yielded reliable results, consistent within their respective uncertainties. The final dataset analyzed comprised 1,228 Cepheids located at distances ranging from 30 to 50 kiloparsecs from the center of the universe. Leveraging this dataset, we were able to derive additional time-luminosity relations for classical Cepheids across the infrared bands J, H, and Ks. This study not only enhances our understanding of the Cepheid period-luminosity relationship but also contributes valuable insights into the distance measurement techniques applicable to the LMC, thereby advancing the broader field of astronomical distance scaling and cosmological research.",
        "ori-fast-z-score": 0.9428090415820635,
        "water-fast-z-score": 5.892556509887896,
        "rewrite-fast-z-score": 1.5389675281277313
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Influence of oxygen partial pressure on structural, transport and magnetic properties of Co doped TiO2 films .\nAbstract:\nThe influence of the oxygen partial pressure (pO(2)) during deposition on the structure, electrical conductivity and magnetization in cobalt-doped titanium dioxide thin films is investigated by X-ray diffraction (XRD), scanning electron microscopy (SEM) and vibrating sample magnetometry (VSM). The XRD patterns show that all samples are amorphous at room temperature but crystallize into anatase phase after annealing for 1 hour at 500 °C. It was found that pO(2) has an effect on the grain size as well as the lattice parameters of the deposited films. The resistivity decreases with increasing pO(2) while the saturation magnetization increases. These results suggest that the doping level can be controlled by changing the pO(2) value. \n \n Keywords: Cobalt-doped titanium dioxide film; Oxygen partial pressure; Structural property; Electrical conductivity; Magnetization",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Influence of oxygen partial pressure on structural , transport and magnetic properties of Co doped TiO2 films . Abstract : The impact of the oxygen partial pressure ( pO ( 2 ) ) during deposition on the composition , thermal conductivity and magnetization in cobalt - doped titanium oxygen narrow films is investigated by X - ray diffraction ( XRD ) , scanning electron microscopy ( SEM ) and vibrating sample magnetometry ( VSM ) .The XRD sequences indicate that all specimens are amorphous at room temperature but crystallize into anatase phase after annealing for 1 hour at 500 °C . It was shown that pO ( 2 ) has an influence on the grain size as also as the lattice parameters of the deposited films .The resistivity decreases with rising pO ( 2 ) while the saturation magnetization increases . These data suggest that the doping rate can be controlled by varying the pO ( 2 ) value .Keywords : Cobalt - doped titanium dioxide film ; Oxygen partial pressure ; Structural property ; Electrical conductivity ; Magnetization",
        "rewrite_text": "Title: Influence of Oxygen Partial Pressure on the Structural, Transport, and Magnetic Properties of Cobalt-Doped Titanium Dioxide Films\n\nAbstract: This study explores the effects of oxygen partial pressure (pO₂) during the deposition process on the structural, thermal, and magnetic characteristics of cobalt-doped titanium dioxide (TiO₂) films. Utilizing techniques such as X-ray diffraction (XRD), scanning electron microscopy (SEM), and vibrating sample magnetometry (VSM), we systematically analyze the relationship between pO₂ and the resulting properties of the films. Our XRD results reveal that all samples remain amorphous at room temperature but transition to the anatase phase following a one-hour annealing process at 500 °C. Notably, we observe that variations in pO₂ significantly affect both the grain size and the lattice parameters of the films. Specifically, as the pO₂ increases, there is a corresponding decrease in resistivity, while saturation magnetization exhibits an upward trend. These findings indicate that the doping concentration can be effectively manipulated by adjusting the pO₂ during deposition. This research contributes to a deeper understanding of how oxygen partial pressure influences the properties of cobalt-doped TiO₂ films, which may have implications for their application in various electronic and magnetic devices. \n\nKeywords: Cobalt-doped titanium dioxide films; Oxygen partial pressure; Structural properties; Electrical conductivity; Magnetization.",
        "ori-fast-z-score": -1.4084056792618558,
        "water-fast-z-score": 3.048003048004572,
        "rewrite-fast-z-score": 0.3333333333333333
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Path Integral Methods in the Su-Schrieffer-Heeger Polaron Problem .\nAbstract:\nThe polaron problem is one of the most important problems in condensed matter physics, and has been studied extensively for many years.  In this work we present an overview of some recent results on path integral methods applied to the su(2)-schrieffer-heeger (s-shh) model with periodic boundary conditions.   We first review how the s-shh hamiltonian can be written as a sum over spinless fermions using the Jordan-Wigner transformation.  Then we discuss how the partition function may be evaluated by performing a trace over all possible states of these fermions.  Finally, we show that the resulting expression can be rewritten in terms of Feynman diagrams which are then used to calculate various physical quantities such as the energy spectrum or correlation functions. The polaron problem is one o fthe most important problems in condensate matter physics, and has b een studied extensively for many years  1  . It describes a single electron moving through a lattice of atoms interacting via phonons  2  , where the electron-phonon interaction leads to the formation of a bound state known as a polaron  3  .\nIn this work w epresent an overview of some recent resul ts on path integral m ethods applied t o th e su(2)-schr iefer -heeg er (s-shh ) model  4  wit h p eriodic bo undary condit ions  5  .  W e first r evie w ho w th e shh h amiltonia n ca n be wr it ten as a sum ov er sp inl ess fermi ons usin g th e J ordan-Wign er transfor mat ion  6  .  Th en we discu ss how th e partiti on functi on m ay be evalua ted by perform ing a tr ace ov er al l possibl e st at es of th ese fermi ons.  Fina ll y, we sho w tha t th e resul tin g ex pressio n ca n be rewrite n in term s of Feyn man di agrams wh ich ar e th en u",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Path Integral Methods in the Su - Schrieffer - Heeger Polaron Problem . Abstract : The polaron problem is one of the most important problems in condensed matter theory , and has been studied frequently for hundreds months .In this research we present an overview of some latest findings on path integral methods applied to the su ( 2 ) - schrieffer - heeger ( s - shh ) model with periodic boundary constraints . We first review how the s - shh hamiltonian can be written as a sum over spinless fermions using the Jordan - Wigner transformation .Then we explain how the partition function could be evaluated by performing a trace over all possible states of these fermions . Finally , we find that the resulting expression can be rewritten in terms of Feynman diagrams which are then used to estimate various biological quantities such as the power spectrum or correlation functions .The polaron problem is one o fthe most important problems in condensate matter mechanics , and has b een discussed heavily for many years 1 . It involves a single electron moving through a lattice of atoms interacting via phonons 2 , where the electron - phonon interaction results to the formation of a bound state known as a polaron 3 .In this study w epresent an overview of some latest resul ts on path integral m ethods applied t o th e su ( 2 ) - schr iefer - heeg er ( s - shh ) model 4 wit h p eriodic bo undary condit ions 5 . W e first r evie w ho w th e shh n amiltonia n ca n be wr it ten as a sum ov er sp inl ess fermi ons usin g th e J ordan - Wign er transfor mat ion 6 .Th en we discu ss how th e partiti on functi on m ay be evalua ted by perform ing a tr ace ov er al l possibl e st at es of th ese fermi ons . Fina ll y , we sho w tha t th e resul tin g ex pressio n ca n be rewrite n in word s of Feyn man di agrams wh ich ar e th en u",
        "rewrite_text": "**Title:** Path Integral Methods in the Su-Schrieffer-Heeger Polaron Problem\n\n**Abstract:** The polaron problem is a fundamental issue in condensed matter physics that has garnered significant attention over the years. In this article, we provide a comprehensive overview of recent advancements in the application of path integral methods to the Su-Schrieffer-Heeger (SSH) model, particularly under periodic boundary conditions. We begin by reformulating the SSH Hamiltonian as a sum over spinless fermions through the Jordan-Wigner transformation, which allows for a more tractable analysis of the system. Following this, we detail the process of evaluating the partition function by taking a trace over all possible states of these fermions, a crucial step in understanding the thermodynamic properties of the polaron. Our findings reveal that the resulting expression can be elegantly expressed in terms of Feynman diagrams. These diagrams serve as a powerful tool for estimating various physical quantities, including power spectra and correlation functions, which are essential for characterizing the behavior of polarons in different regimes. The polaron problem, which involves the dynamics of an electron interacting with a lattice of atoms through phonon exchanges, leads to the emergence of a bound state known as a polaron. This study not only highlights the effectiveness of path integral techniques in addressing complex problems in condensed matter theory but also opens avenues for further exploration of the SSH model and its implications in various physical contexts. Through this work, we aim to contribute to the ongoing discourse surrounding the polaron problem and its relevance in understanding electron-phonon interactions within solid-state systems.",
        "ori-fast-z-score": 1.2371791482634837,
        "water-fast-z-score": 5.0963686064660765,
        "rewrite-fast-z-score": 2.8867513459481287
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Weight structures vs. $t$-structures; weight filtrations, spectral sequences, and complexes (for motives and in general) .\nAbstract:\nThe purpose of this article is to give an overview of the theory of weight structures on triangulated categories developed by A. Bondal and M. Kapranov.  We will explain how it can be applied to construct new cohomology theories for algebraic varieties over finite fields or number fields.   In particular we will discuss the construction of motivic cohomology using weight structures on derived categories of mixed Tate motives.    The main results are due to J. Ayoub, D. Gaitsgory, R. Hain, S. Katzarkov, V. Lafforgue, C. Soulé, B. Stienstra, and others. This article was written as part of the author s Ph.D thesis at Utrecht University under supervision of Prof. Dr. Wim van der Kallen. It has been published online by the author with permission of the supervisor. For more information about the content see the introduction below.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Weight systems vs . $ t $ - systems ; weight filtrations , spectral patterns , and complexes ( for motives and in general ) . Abstract : The purpose of this page is to give an overview of the notion of weight forms on triangulated categories pioneered by A . Bondal and M . Kapranov .We will explain how it can be applied to build new cohomology theories for algebraic fields over finite fields or number fields . In particular we will explore the creation of motivic cohomology involving weight forms on derived categories of mixed Tate motives .The main results are due to J . Ayoub , D . Gaitsgory , R . Hain , S . Katzarkov , V . Lafforgue , C . Soulé , B . Stienstra , and others . This page was written as part of the writer s Ph . D thesis at Utrecht University under supervision of Prof . Dr . Wim van der Kallen .It has been publication online by the writer with authorization of the supervisor . For more information about the content read the introduction below .",
        "rewrite_text": "This document provides a comprehensive overview of the concept of weight forms within triangulated categories, a framework initially developed by A. Bondal and M. Kapranov. The focus is on how these weight forms can be utilized to formulate new cohomology theories applicable to algebraic fields, particularly those defined over finite fields or number fields. A significant aspect of this exploration is the development of motivic cohomology, which incorporates weight forms on derived categories of mixed Tate motives. The foundational results presented in this work are attributed to prominent mathematicians including J. Ayoub, D. Gaitsgory, R. Hain, S. Katzarkov, V. Lafforgue, C. Soulé, B. Stienstra, among others. This document is part of the author's Ph.D. thesis at Utrecht University, conducted under the guidance of Prof. Dr. Wim van der Kallen. The author has published this work online with the supervisor's consent. For further insights into the subject matter, readers are encouraged to consult the introduction provided below.",
        "ori-fast-z-score": -0.4923659639173309,
        "water-fast-z-score": 4.608176875690327,
        "rewrite-fast-z-score": 0.7385489458759964
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Long Gamma-Ray Burst Progenitors: Boundary Conditions and Binary Models .\nAbstract:\nWe present the results of our study on binary models for gamma-ray bursts (GRBs) with progenitors in the mass range 8-40 M⊙, which are expected to produce GRB jets that can be observed at cosmological distances. We find that these systems evolve into double-degenerate binaries consisting of two white dwarfs or helium stars before they explode as supernovae. The explosion is triggered by the merger of the components due to gravitational wave emission. In some cases we also find that the system evolves through an intermediate stage where one component collapses to form a black hole while the other explodes as a supernova. This scenario may explain why there seems to exist a gap between the masses of ordinary core-collapse supernovae and those of GRBs. Our calculations show that the total number of such events per year could be up to 10 times higher than previously estimated if the progenitor population extends down to lower masses.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Long Gamma - Ray Burst Progenitors : Boundary Conditions and Binary Models . Abstract : We present the conclusion of our research on binary models for gamma - ray bursts ( GRBs ) with progenitors in the mass range 8 - 40 [UNK] , which are expected to produce GRB jets that can be found at cosmological speeds .We see that these systems develop into double - degenerate binaries consisting of two white dwarfs or helium stars before they explode as supernovae . The explosion is caused by the merger of the parts due to gravitational wave radiation .In some cases we also find that the system evolves through an intermediate stage where one element collapses to form a black hole while the other explodes as a supernova . This scenario could explain why there seems to exist a gap between the masses of normal core - collapse supernovae and those of GRBs .Our calculations show that the total number of such events per year could be up to 10 twice higher than previously predicted if the progenitor colony extends down to smaller masses .",
        "rewrite_text": "In this article, we conclude our investigation into binary models for gamma-ray bursts (GRBs) originating from progenitors with masses ranging from 8 to 40 solar masses. These progenitors are anticipated to generate GRB jets that can be detected at cosmological distances. Our findings indicate that these binary systems evolve into double-degenerate configurations, comprising either two white dwarfs or two helium stars, prior to their eventual supernova explosions. The mechanism driving these explosions is linked to the merger of the binary components, which occurs as a result of gravitational wave radiation. Additionally, we observe that some systems may transition through an intermediate phase where one component collapses into a black hole while the other undergoes a supernova explosion. This particular evolutionary pathway may provide insight into the observed mass gap between typical core-collapse supernovae and GRBs. Our calculations suggest that the annual occurrence of such events could be as much as twice the previously estimated figures, particularly if the progenitor population includes lower mass stars. This research enhances our understanding of the mechanisms behind GRB formation and the characteristics of their progenitor systems, potentially leading to a reevaluation of the frequency and nature of these extraordinary cosmic phenomena.",
        "ori-fast-z-score": -0.5773502691896257,
        "water-fast-z-score": 3.9000674757995495,
        "rewrite-fast-z-score": 0.30779350562554625
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dark matter in the Milky Way, II. the HI gas distribution as a tracer of the gravitational potential .\nAbstract:\nWe present an analysis of the neutral hydrogen (HI) emission observed with the Westerbork Synthesis Radio Telescope and the Effelsberg 100-m telescope to study the dark matter content of our Galaxy. We use the rotation curve derived by Clemens (1985) , which is based on 21-cm line observations of nearby spiral galaxies. The total mass enclosed within a radius R can be written as: M(R) = Vrot2πGRL + MDW(R), where Vrot is the circular velocity at galactocentric distance R, G is Newton s constant, L is the luminosity density, and MDW(R) is the contribution due to the dark matter halo. In this work we assume that the dark matter follows a Navarro-Frenk-White profile.  Using the rotation curve for the solar neighbourhood given by Clemens (1985) (V⊙ = 220 km/sec), we find that the best-fit parameters are L0 = 0.0013 Msun/pc3 and r0 = 1 kpc. This implies that the local surface brightness ΣL = L/L0 = 3.6 × 10^−26 W/m2/Hz/sr. For comparison, the average value found by Dickey & Lockman (1990)  is ΣL = 2×10^−25 W/m2/Hz/sr; their sample includes only high latitude regions outside the Galactic plane.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dark matter in the Milky Way , II . the HI gas distribution as a tracer of the gravitational potential .Abstract : We present an analysis of the neutral hydrogen ( HI ) emission observed with the Westerbork Synthesis Radio Telescope and the Effelsberg 100 - m observatory to study the dark matter content of our Galaxy . We use the rotation curve obtained by Clemens ( 1985 ) , which is based on 21 - cm line surveys of distant spiral galaxies .The total mass surrounded within a diameter R can be written as : M ( R ) = Vrot2πGRL + MDW ( R ) , where Vrot is the spherical momentum at galactocentric distance R , G is Newton s constant , L is the luminosity density , and MDW ( R ) is the contribution owing to the dark matter halo . In this work we suppose that the dark matter follows a Navarro - Frenk - White model .Using the rotation curve for the solar neighbourhood given by Clemens ( 1985 ) ( [UNK] = 220 km / sec ) , we find that the best - fitting coefficients are L0 = 0 . 0013 Msun / pc3 and r0 = 1 kpc . This implies that the local surface brightness ΣL = L / L0 = 3 . 6 × 10 ^ −26 W / m2 / Hz / sr .For comparison , the average value found by Dickey & Lockman ( 1990 ) is ΣL = 2×10 ^ −25 W / m2 / Hz / sr ; their sample comprises only high elevation regions outside the Galactic jet .",
        "rewrite_text": "In this study, we investigate the distribution of neutral hydrogen (HI) gas as a means to trace the gravitational potential of dark matter within the Milky Way galaxy. Utilizing data from the Westerbork Synthesis Radio Telescope and the Effelsberg 100-meter observatory, we analyze HI emission to gain insights into the dark matter composition of our Galaxy. Our approach is grounded in the rotation curve established by Clemens (1985), which is derived from 21-cm line surveys of distant spiral galaxies. The total mass enclosed within a radius R is expressed by the equation M(R) = V_rot²/(2πG)R + M_DW(R), where V_rot represents the rotational velocity at a galactocentric distance R, G is Newton's gravitational constant, and M_DW(R) accounts for the dark matter halo's contribution. For our analysis, we adopt the Navarro-Frenk-White (NFW) profile to model the dark matter distribution. By applying Clemens' rotation curve for the solar neighborhood, with a rotational velocity of approximately 220 km/s, we derive optimal fitting parameters of L₀ = 0.0013 M_sun/pc³ and r₀ = 1 kpc. This leads to a calculated local surface brightness of Σ_L = L/L₀ = 3.6 × 10⁻²⁶ W/m²/Hz/sr. In comparison, the average surface brightness reported by Dickey & Lockman (1990) is Σ_L = 2 × 10⁻²⁵ W/m²/Hz/sr, based on a sample that includes only high-elevation regions outside the Galactic plane. Our findings contribute to a deeper understanding of the dark matter distribution in the Milky Way, highlighting the role of HI gas as a valuable tracer of gravitational potential.",
        "ori-fast-z-score": 0.7777777777777778,
        "water-fast-z-score": 4.939391699536065,
        "rewrite-fast-z-score": 1.3598002073001698
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  An absorption origin for the soft excess in Seyfert 1 AGN .\nAbstract:\nWe present new results on the X-ray spectrum and variability properties of Mrk 509, one of the brightest Seyfert galaxies observed by XMM-Newton. We find that its 0.5-10 keV continuum is well described by an absorbed power law with Γ = 2.1 ± 0.2 (χ2/dof=111/101) plus a reflection component modeled as a PEXRAV model with R=0.7-1.0 and NH=10-23×1022 cm-2. The best-fit parameters are consistent within errors to those found previously using Chandra data alone. No significant spectral changes were detected between different epochs separated by several months apart. However, we do detect strong flux variations at all energies during our observation period. In particular, there was a factor of 3 increase in the hard band count rate over about 20 ks followed by a slower decay back towards the initial level. This behavior can be explained if the source has been caught in a transition state where the accretion disk luminosity increased rapidly due to some instability or perturbation.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : An absorption origin for the soft excess in Seyfert 1 AGN . Abstract : We report new data on the X - ray spectrum and variability properties of Mrk 509 , one of the brightest Seyfert galaxies studied by XMM - Newton .We see that its 0 . 5 - 10 keV continuum is well described by an absorption power law with Γ = 2 . 1 ± 0 . 2 ( χ2 / dof = 111 / 101 ) plus a mirror element modeled as a PEXRAV model with R = 0 . 7 - 1 . 0 and NH = 10 - 23×1022 centimetres - 2 . The best - fitting values are compatible within errors to those identified previously used Chandra data alone .No meaningful spectral changes were detected between various epochs separated by many months separated . However , we do discover powerful flux variations at all energies during our observation term .In particular , there was a factor of 3 gain in the hard band count rate over about 20 ks followed by a slower decay forward towards the first level . This phenomenon can be understood if the source has been caught in a transition state where the accretion disk luminosity increased rapidly due to some instability or perturbation .",
        "rewrite_text": "We present new findings regarding the X-ray spectrum and variability characteristics of Mrk 509, one of the most luminous Seyfert galaxies examined using the XMM-Newton satellite. Our analysis reveals that the 0.5 - 10 keV continuum of Mrk 509 is effectively modeled by an absorption power law with a photon index of Γ = 2.1 ± 0.2 (χ²/dof = 111/101). Additionally, we incorporate a reflective component represented by a PEXRAV model, characterized by a reflection fraction (R) ranging from 0.7 to 1.0 and a hydrogen column density (NH) of approximately 10 - 23 × 10²² cm⁻². The best-fitting parameters align closely with those derived from previous observations utilizing Chandra data, indicating consistency across different datasets. \n\nThroughout our observations, which spanned several months, we did not observe any significant spectral variations between the different epochs. However, we did identify substantial flux fluctuations across all energy bands during the observation period. Notably, we recorded a threefold increase in the hard band count rate over approximately 20 kiloseconds, followed by a gradual decline back to baseline levels. This behavior suggests that the source may have been in a transitional state, where the luminosity of the accretion disk experienced a rapid surge due to an instability or perturbation. These findings contribute to our understanding of the mechanisms driving the soft excess observed in Seyfert 1 active galactic nuclei (AGN) and highlight the dynamic nature of these astrophysical systems.",
        "ori-fast-z-score": -0.44172610429938614,
        "water-fast-z-score": 5.521576303742327,
        "rewrite-fast-z-score": -0.2
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Improving Stellar and Planetary Parameters of Transiting Planet Systems: The Case of TrES-2 .\nAbstract:\nWe present the results of an analysis aimed at improving the stellar parameters for the host star of planet TrES-2, as well as its planetary system properties. We use high-precision photometry obtained with the MOST satellite to derive new values for the orbital period (P = 3.819 days), transit epoch (T0 = 2454000 MJD) and radius ratio (Rp/Rs = 0.11). These are combined with existing radial velocity data in order to refine the mass estimates for both components of this double-lined spectroscopic binary. Our best-fit model yields masses of 1.06 ± 0.04M⊙ and 0.84 ± 0.03M⊙ for the primary and secondary stars respectively, along with radii of 1.16 ± 0.02R⊙ and 0.91 ± 0.01R⊙ . This leads us to revise upward our previous estimate of the age of the system by about 50%, placing it firmly within the range expected for planets formed via core accretion theory.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Improving Stellar and Planetary Parameters of Transiting Planet Systems : The Case of TrES - 2 . Abstract : We report the conclusion of an assessment aimed at enhancing the stellar characteristics for the host star of planet TrES - 2 , as well as its planetary system properties .We use large - precision photometry obtained with the MOST satellite to derive new values for the orbital period ( P = 3 . 819 days ) , transit epoch ( T0 = 2454000 MJD ) and radius ratio ( Rp / Rs = 0 . 11 ) . These are coupled with existing radial speed data in order to refine the mass estimates for both components of this double - lined spectroscopic binary .Our best - fitting model gives masses of 1 . 06 ± 0 . [UNK] and 0 . 84 ± 0 . [UNK] for the primary and secondary stars respectively , along with radii of 1 . 16 ± 0 . [UNK] and 0 . 91 ± 0 . [UNK] . This leads us to revise upward our previous estimate of the age of the system by about 50 % , placing it firmly within the range assumed for planets formed via core accretion theory .",
        "rewrite_text": "We present the findings of a comprehensive study focused on refining the stellar and planetary parameters of the transiting planet system TrES-2. Utilizing high-precision photometric data collected by the MOST satellite, we have derived updated measurements for key orbital characteristics, including an orbital period of P = 3.819 days, a transit epoch of T0 = 2454000 MJD, and a radius ratio of Rp/Rs = 0.11. These new values were integrated with existing radial velocity data to enhance the mass estimates for the two stars in this double-lined spectroscopic binary system. Our analysis yielded refined mass estimates of 1.06 ± 0. [UNK] for the primary star and 0.84 ± 0. [UNK] for the secondary star, alongside radii measurements of 1.16 ± 0. [UNK] and 0.91 ± 0. [UNK], respectively. As a result of these improved parameters, we have revised our previous estimate of the system's age upward by approximately 50%. This adjustment places TrES-2 within the age range typically associated with planets formed through core accretion theory. Our findings contribute to a more accurate understanding of the TrES-2 system, which is essential for further studies of exoplanet formation and evolution. This work underscores the importance of high-quality observational data in refining stellar and planetary models, ultimately enhancing our comprehension of the dynamics and characteristics of transiting exoplanet systems.",
        "ori-fast-z-score": 1.1470786693528088,
        "water-fast-z-score": 4.09644015186457,
        "rewrite-fast-z-score": 2.8856078516089685
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Evolution of complex modular biological networks .\nAbstract:\nWe study the evolution of complex networks with multiple modules, where each module is an Erdős-Rényi random graph and all nodes are connected to one another within their own module but not across different modules. We show that this model can be used to describe many real-world systems such as metabolic pathways in yeast cells or social interactions between individuals in animal groups. In particular we find that: (i) The number of links per node scales linearly with system size. (ii) The clustering coefficient decreases logarithmically with system size. (iii) The average path length increases logarithmically with system size. These results agree well with those observed for both metabolic networks and social networks. Finally, by using our evolutionary approach, we predict new functional relationships among genes in the yeast cell cycle pathway. Complex networks have been found to play important roles in various fields ranging from physics  1  , biology  2  , sociology  3  , computer science  4  , etc.. Many real world networks exhibit common statistical properties including power-law degree distribution  5  , small diameter  6  , high clustering coefficients  7, 8  . However, it remains unclear how these networks evolve over time  9  .\nIn recent years there has been growing interest in studying the evolution of complex networks  10 -12  . For example, Barabási-Albert proposed a simple growth mechanism which leads to scale-free networks  13  . Dorogovtsev et al studied the evolution of hierarchical networks  14  . Caldarelli et al investigated the evolution of clustered networks  15  . Newman introduced a fitness-based model  16  . This model was further developed into a more realistic version  17  . Recently, Jeong et al showed that some metabolic networks share similar topological features  18  . They also suggested that the underlying mechanisms responsible for generating these networks may be related to natural selection  19  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Evolution of complex modular biological circuits . Abstract : We research the evolution of complex networks with many modules , where each module is an Erdős - Rényi random graph and all nodes are connected to one another within their own module but not across different modules .We see that this model can be used to explain much actual - time systems such as metabolic processes in bacterial cells or cultural relationships between individuals in livestock groups . In particular we find that : ( i ) The amount of links per node scales linearly with system size .( ii ) The clustering density decreases logarithmically with system size . ( iii ) The average route length grows logarithmically with system height .These data agree well with those observed for both genetic systems and communication connections . Finally , by using our evolutionary approach , we estimate new functional links among genes in the yeast cell cycle pathway .Complex networks have been shown to work important roles in different fields ranging from science 1 , biology 2 , anthropology 3 , computer science 4 , etc . . Many actual world networks demonstrate characteristic statistical characteristics notably energy - law degree function 5 , low diameter 6 , large clustering coefficients 7 , 8 .However , it remains unsure how these networks develop over time 9 . In recent years there has been growing interest in examining the evolution of complex networks 10 - 12 .For instance , Barabási - Albert discovered a simple growth mechanism which contributes to scale - free networks 13 . Dorogovtsev et al studied the evolution of hierarchical networks 14 .Caldarelli et al investigated the evolution of clustered systems 15 . Newman invented a fitness - based model 16 .This theory was further developed into a more realistic version 17 . Recently , Jeong et al showed that some metabolic networks share identical topological features 18 .They instead suggested that the underlying mechanisms involved for generating these networks could be connected to natural selection 19 .",
        "rewrite_text": "**Title: Evolution of Complex Modular Biological Circuits**\n\n**Abstract:** This study explores the evolution of intricate networks composed of multiple modules, where each module is represented as an Erdős-Rényi random graph. Within each module, all nodes are interconnected, while connections between different modules are absent. Our model serves as a framework for understanding various real-time systems, including metabolic pathways in bacterial cells and social interactions among individuals in livestock populations. Our findings reveal several key characteristics: (i) the number of links per node increases linearly with the size of the system, (ii) the clustering density diminishes logarithmically as the system expands, and (iii) the average path length grows logarithmically with the height of the system. These observations align closely with empirical data from genetic networks and communication systems. Additionally, our evolutionary framework allows us to propose new functional connections among genes involved in the yeast cell cycle pathway.\n\nComplex networks play a crucial role across diverse disciplines, including science, biology, anthropology, and computer science. Many real-world networks exhibit distinct statistical properties, such as power-law degree distributions, low diameters, and high clustering coefficients. However, the mechanisms underlying the development of these networks over time remain poorly understood. Recent years have seen a surge in interest regarding the evolution of complex networks. Notably, Barabási and Albert introduced a simple growth mechanism that facilitates the emergence of scale-free networks. Other researchers, including Dorogovtsev et al. and Caldarelli et al., have examined the evolution of hierarchical and clustered systems, respectively. Furthermore, Newman proposed a fitness-based model that has been refined into more realistic versions. Recent work by Jeong et al. has highlighted that certain metabolic networks exhibit similar topological features, suggesting that the processes generating these networks may be linked to natural selection. This research contributes to the growing body of knowledge on the evolutionary dynamics of complex biological circuits.",
        "ori-fast-z-score": -0.4601789933084222,
        "water-fast-z-score": 7.678887804824026,
        "rewrite-fast-z-score": 0.9771398364036775
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Catalog of Star Cluster Candidates in M33 .\nAbstract:\nWe present the results of an automated search for star cluster candidates (SCCs) using deep HST/ACS images covering most of the disk and halo of M33, obtained as part of program GO-10229. The SCCs were identified by applying the DAOPHOT photometry package to detect sources with high surface brightnesses relative to their local backgrounds. We then applied several selection criteria based on color-magnitude diagrams (CMDs), luminosity functions (LFs), and radial profiles to identify bona fide SCCs among these bright objects. A total of 1,082 candidate clusters are found within our survey area; we estimate that about half of them may be real open clusters or associations. These newly discovered SCCs will provide important targets for future spectroscopic studies aimed at understanding how star formation proceeds in low-metallicity environments such as those found in dwarf galaxies like M33.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Catalog of Star Cluster Candidates in M33 . Abstract : We present the results of an automated search for star cluster applicants ( SCCs ) using deep HST / ACS images covering most of the disk and halo of M33 , obtained as part of program GO - 10229 .The SCCs were discovered by using the DAOPHOT photometry program to identify sources with high surface brightnesses compared to their nearby backgrounds . We then utilized numerous selection standards based on color - magnitude diagrams ( CMDs ) , luminosity functions ( LFs ) , and radial profiles to identify bona fide SCCs among these bright objects .A total of 1 , 082 candidate clusters are found within our survey area ; we estimate that about half of them may be real open clusters or associations . These newly discovered SCCs will provide important targets for future spectroscopic studies aimed at understanding how star formation proceeds in low - metallicity environments such as those found in dwarf galaxies like M33 .",
        "rewrite_text": "We present the findings of an extensive automated search for star cluster candidates (SCCs) conducted using high-resolution images from the Hubble Space Telescope (HST) Advanced Camera for Surveys (ACS), which encompass a significant portion of the disk and halo of the galaxy M33. This research was part of program GO-10229. The identification of SCCs was achieved through the DAOPHOT photometry software, which enabled us to detect sources exhibiting high surface brightness relative to their surrounding backgrounds. To refine our selection of potential star clusters, we applied a variety of criteria based on color-magnitude diagrams (CMDs), luminosity functions (LFs), and radial profiles. Our survey resulted in the identification of 1,082 candidate star clusters within the observed region. We estimate that approximately half of these candidates may represent genuine open clusters or stellar associations. The discovery of these SCCs is significant, as they will serve as vital targets for future spectroscopic investigations. Such studies are essential for enhancing our understanding of star formation processes in low-metallicity environments, akin to those present in dwarf galaxies like M33. This catalog not only contributes to the existing body of knowledge regarding star clusters in M33 but also opens new avenues for research into the conditions and mechanisms that govern star formation in less chemically enriched regions of the universe.",
        "ori-fast-z-score": 0.5773502691896257,
        "water-fast-z-score": 3.4412360080584263,
        "rewrite-fast-z-score": -0.38851434494290565
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Evidence for a Z < 8 Origin of the Source Subtracted Near Infrared Background .\nAbstract:\nWe present new constraints on the redshift distribution of sources contributing to the source subtracted near-infrared background (SSNIB). We use deep Spitzer/MIPS 24 micron data in combination with optical and infrared photometry, including GALEX NUV imaging, to select galaxies at z > 1.5 over an area of 0.6 deg2 centered around the Lockman Hole East field. The resulting sample consists of 16,000 objects selected between redshifts 2<z<8. Using this sample we measure the evolution of the luminosity function out to high redshifts by fitting Schechter functions to our observed number counts as a function of flux density binned into bins of width ∆logS = 0.1 dex. Our results are consistent with previous studies that find evidence for strong luminosity evolution up to z ~ 3 followed by little or no evolution beyond this point. \n \n We then fit models to these measurements using Monte Carlo simulations which include contributions from both obscured AGNs and normal star forming galaxies. These fits show that the majority of the SSNIB is produced by faint galaxies at low redshifts (0.3 < z < 1) while bright galaxies dominate at higher redshifts (4 < z < 6). \n \n Finally, we compare our best-fit model predictions to existing observations of the unresolved extragalactic background light (EBL), finding good agreement within uncertainties.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Evidence for a Z < 8 Origin of the Source Subtracted Near Infrared Background . Abstract : We introduce new limitations on the redshift distribution of sources contributing to the origin subtracted near - infrared background ( SSNIB ) .We use deep Spitzer / MIPS 24 micron data in combination with optical and infrared photometry , using GALEX NUV photography , to select galaxies at z > 1 . 5 over an area of 0 . 6 deg2 centered around the Lockman Hole East field . The resulting survey consists of 16 , 000 items selected between redshifts 2 < z < 8 .Using this sample we measure the evolution of the luminosity function out to high redshifts by fitting Schechter functions to our observed number counts as a function of flux density binned into bins of width [UNK] = 0 . 1 dex . Our results are compatible with previous studies that find evidence for strong luminosity evolution up to z ~ 3 followed by little or no evolution beyond this point .We then fitting models to these measurements using Monte Carlo simulations which contain contributions from both distorted AGNs and normal star producing objects . These fits indicate that the majority of the SSNIB is produced by faint galaxies at low redshifts ( 0 . 3 < z < 1 ) while bright clusters dominate at higher redshifts ( 4 < z < 6 ) .Finally , we compare our better - fitting model models to existing observations of the unresolved extragalactic background light ( EBL ) , finding positive agreement within uncertainties .",
        "rewrite_text": "We present new constraints on the redshift distribution of sources that contribute to the source-subtracted near-infrared background (SSNIB). Utilizing deep Spitzer/MIPS 24 micron data alongside optical and infrared photometry, including GALEX NUV imaging, we identify galaxies at redshifts greater than 1.5 within a 0.6 deg² area centered on the Lockman Hole East field. Our survey encompasses 16,000 galaxies selected within the redshift range of 2 < z < 8. By analyzing this dataset, we assess the evolution of the luminosity function at high redshifts, employing Schechter function fits to our observed number counts, which are organized into flux density bins of width 0.1 dex. Our findings align with previous research indicating significant luminosity evolution up to z ~ 3, followed by minimal evolution thereafter. To further interpret these measurements, we apply Monte Carlo simulations that account for contributions from both distorted active galactic nuclei (AGNs) and typical star-forming galaxies. The results suggest that the majority of the SSNIB originates from faint galaxies at lower redshifts (0.3 < z < 1), while brighter clusters are more prevalent at higher redshifts (4 < z < 6). Additionally, we compare our optimized models with existing observations of the unresolved extragalactic background light (EBL), revealing a positive correlation within the uncertainties. This study enhances our understanding of the sources contributing to the near-infrared background and provides insights into the evolution of galaxy populations across different redshift ranges.",
        "ori-fast-z-score": -1.4142135623730951,
        "water-fast-z-score": 3.084615289650966,
        "rewrite-fast-z-score": 2.487592975524973
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The host galaxies of strong CaII QSO absorption systems at z<0.5 .\nAbstract:\nWe have obtained deep optical spectra for eight QSOs with known redshifts in the range 0.4-0.5, and searched them for intervening Ca II absorbers using the equivalent width (EW) method. We find that all eight QSOs show strong Ca II absorptions associated with their own galaxy halos. The EWs are found to be correlated with the luminosities of the QSOs themselves. This correlation is consistent with previous results on Mg II absorbers but not with those on C IV absorbers. In addition we find that there exists an anti-correlation between the EWs and the impact parameters of the absorber-galaxy pairs. These results suggest that the Ca II absorbers may arise mainly from gaseous disks surrounding massive elliptical galaxies rather than from galactic winds driven by starburst activity. \n \n \n \n Keywords: Absorber-galaxy pairs; Galaxy halo; Quasar; Redshift",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The host galaxies of bright CaII QSO absorption complexes at z < 0 . 5 . Abstract : We have discovered rich optical spectra for eight QSOs with reported redshifts in the range 0 . 4 - 0 . 5 , and explored them for intervening Ca II absorbers using the equivalent height ( EW ) method .We see that all eight QSOs exhibit strong Ca II absorptions associated with their own galaxy halos . The EWs are found to be correlated with the luminosities of the QSOs themselves .This coupling is compatible with previous findings on Mg II absorbers but not with those on C IV absorbers . In addition we find that there exists an counter - correlation between the EWs and the impact factors of the absorber - galaxy pairs .These data suggest that the Ca II absorbers may arise predominantly from gaseous disks circling massive elliptical galaxies rather than from galactic winds driven by starburst activity . Keywords : Absorber - star pairs ; Galaxy halo ; Quasar ; Redshift",
        "rewrite_text": "Title: The Host Galaxies of Bright CaII QSO Absorption Complexes at z < 0.5\n\nAbstract: In this study, we present an analysis of optical spectra from eight quasars (QSOs) with redshifts ranging from 0.4 to 0.5, focusing on the identification of intervening Ca II absorption features. Utilizing the equivalent width (EW) method, we have detected significant Ca II absorptions that are associated with the halos of the host galaxies of these QSOs. Our findings reveal a strong correlation between the equivalent widths of the Ca II absorbers and the luminosities of the QSOs, suggesting a relationship that aligns with previous observations of Mg II absorbers. However, this correlation contrasts with the behavior observed in C IV absorbers, indicating differing physical processes at play. Furthermore, we observe a counter-correlation between the EWs of the absorbers and the impact parameters of the absorber-galaxy pairs, which implies that the spatial distribution of the absorbing gas is influenced by the proximity to the host galaxies. The data we have collected supports the hypothesis that the Ca II absorbers are primarily derived from gaseous disks surrounding massive elliptical galaxies, rather than being the result of galactic winds associated with starburst activity. This research contributes to our understanding of the interplay between quasars and their host galaxies, particularly in the context of absorption line studies, and highlights the complex dynamics of gas in the environments of bright QSOs. \n\nKeywords: Absorber-star pairs; Galaxy halo; Quasar; Redshift.",
        "ori-fast-z-score": -0.39735970711951313,
        "water-fast-z-score": 5.337745350213779,
        "rewrite-fast-z-score": 0.9761870601839528
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Domain wall switching: optimizing the energy landscape .\nAbstract:\nWe propose an alternative switching mechanism for spintronic devices based on domain walls (DWs). The proposed device consists of two ferromagnetic layers separated by a non-magnetic spacer layer, where DWs can be driven between different positions in each magnetic layer using spin-orbit torques and electric fields. We show that this new type of device is able to operate at lower current densities than conventional spin valves with comparable magnetoresistance values. In addition we demonstrate how the energy barrier associated with the motion of the DWs can be tuned through changes in the thicknesses of both the ferromagnets and the non-magnetic spacer. This allows us to optimize the energy landscape such that the DWs are trapped in their equilibrium position when no external field or voltage bias is applied. Finally, we discuss possible applications of our proposal as well as its limitations. Spintronics has emerged over recent years as one of the most promising technologies for future information processing systems  1  . One of the main challenges faced by these devices is the development of efficient ways to control the flow of charge carriers without compromising their high mobility  2  .\nIn order to overcome this problem several groups have recently investigated the possibility of controlling the direction of electron transport via the manipulation of magnetic textures  3  , which include vortex states  4  , skyrmions  5  and domain walls  6  . Domain walls are particularly interesting since they can be manipulated electrically  7, 8  and thermally  9  , making them ideal candidates for low-power consumption devices  10  . However, despite significant progress made towards understanding the physics behind the dynamics of domain walls  11  , there remains much uncertainty about the exact nature of the mechanisms responsible for driving their motion  12  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Domain wall switching : optimizing the electricity landscape . Abstract : We suggest an additional switching method for spintronic systems based on domain barriers ( DWs ) .The proposed machine consists of two ferromagnetic layers divided by a non - magnetic spacer membrane , where DWs can be pushed between various positions in each magnetic layer using spinning - orbit torques and electric forces . We see that this new kind of device is could to run at lower current densities than conventional spin tubes with similar magnetoresistance ratings .In addition we prove how the electricity barrier associated with the movement of the DWs can be tuned through alterations in the thicknesses of both the ferromagnets and the non - magnetic spacer . This enables us to optimize the electricity landscape such that the DWs are locked in their stable position when no external field or voltage bias is applied .Finally , we review possible use of our proposal as well as its limitations . Spintronics has emerged over recent years as one of the most attractive devices for future data processing applications 1 .One of the main problems faced by these machines is the development of effective means to affect the movement of charge carriers without compromising their high mobility 2 . In try to overcome this situation several organizations have recently examined the prospect of controlling the direction of electron transport via the manipulation of magnetic textures 3 , which contain vortex states 4 , skyrmions 5 and domain barriers 6 .Domain barriers are particularly attractive since they can be manipulated electrically 7 , 8 and thermally 9 , making them ideal candidates for low - energy consumption devices 10 . However , despite considerable progress made towards studying the physics behind the dynamics of domain walls 11 , there exists much uncertainty about the exact nature of the mechanisms involved for controlling their motion 12 .",
        "rewrite_text": "**Title:** Domain Wall Switching: Optimizing the Electricity Landscape\n\n**Abstract:** In this study, we introduce a novel switching mechanism for spintronic systems that leverages domain walls (DWs) as a means of data manipulation. Our proposed device architecture consists of two ferromagnetic layers separated by a non-magnetic spacer membrane, allowing for the controlled movement of DWs within each magnetic layer through the application of spin-orbit torques and electric forces. Notably, our findings indicate that this innovative approach can operate at significantly lower current densities compared to traditional spintronic devices with equivalent magnetoresistance characteristics. \n\nFurthermore, we demonstrate that the energy barrier associated with DW movement can be finely tuned by adjusting the thickness of both the ferromagnetic layers and the non-magnetic spacer. This capability allows for the optimization of the electrical landscape, ensuring that DWs remain in a stable position when no external field or voltage bias is applied. \n\nWe also explore the potential applications of our proposed mechanism, as well as its inherent limitations. Spintronics has gained prominence in recent years as a promising technology for future data processing applications. However, one of the critical challenges faced by these systems is the need for effective methods to influence the movement of charge carriers while maintaining their high mobility. To address this issue, various research groups have investigated the possibility of directing electron transport through the manipulation of magnetic textures, including vortex states, skyrmions, and domain barriers. Among these, domain barriers stand out due to their ability to be manipulated both electrically and thermally, positioning them as ideal candidates for low-energy consumption devices. Despite significant advancements in understanding the dynamics of domain walls, there remains considerable uncertainty regarding the precise mechanisms that govern their motion. This study aims to contribute to this evolving field by providing insights into the optimization of DW switching and its implications for future spintronic applications.",
        "ori-fast-z-score": -0.7423074889580903,
        "water-fast-z-score": 8.110396013138502,
        "rewrite-fast-z-score": 1.0309670614335873
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Quantum Group of Isometries in Classical and Noncommutative Geometry .\nAbstract:\nThe quantum group of isometries (QGI) was introduced by Connes as the universal object for deformations of classical groups, which are obtained via spectral triples on commutative C*-algebras.  In this talk we will discuss how to define QGI s using noncommutative geometry techniques such as operator algebras and von Neumann algebras.   We will also explain how these objects can be used to study the classification problem of Riemannian manifolds with positive scalar curvature. The Quantum Group of Isometries (QGI), first defined by Alain Connes, plays an important role in both classical and noncommutative geometry. It is the universal object for deforming classical Lie groups into their corresponding quantum groups. This talk will give an introduction to QGI’s and show that they can be studied through operator algebra theory and von Neumann algebras. Finally it will present some results about the classification problem of Riemannain manifolds with positive scalar curvatures.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Quantum Group of Isometries in Classical and Noncommutative Geometry . Abstract : The quantum group of isometries ( QGI ) was introduced by Connes as the universal object for deformations of classical groups , which are derived via spectral triples on commutative C * - algebras .In this talk we will explore how to define QGI s using noncommutative geometry tools such as operator algebras and von Neumann algebras . We will also explain how these objects can be used to study the classification question of Riemannian manifolds with positive scalar curvature .The Quantum Group of Isometries ( QGI ) , initially established by Alain Connes , takes an important role in both classical and noncommutative geometry . It is the universal object for deforming classical Lie fields into their corresponding quantum groups .This discussion will giving an introduction to QGI ’ s and explain that they can be analyzed through operator algebra analysis and von Neumann algebras . Finally it will present some results about the classification question of Riemannain manifolds with positive scalar curvatures .",
        "rewrite_text": "Title: Quantum Group of Isometries in Classical and Noncommutative Geometry\n\nAbstract: The Quantum Group of Isometries (QGI), a concept introduced by Alain Connes, serves as a fundamental framework for understanding the deformation of classical groups through spectral triples associated with commutative C*-algebras. This article delves into the definition and application of QGIs within the context of noncommutative geometry, utilizing advanced tools such as operator algebras and von Neumann algebras. By employing these mathematical structures, we aim to elucidate the properties and significance of QGIs in both classical and noncommutative settings. \n\nFurthermore, we will investigate how QGIs can be instrumental in addressing the classification problem of Riemannian manifolds that exhibit positive scalar curvature. This classification question is pivotal in differential geometry and has implications for understanding the geometric and topological properties of manifolds. Through our exploration, we will present key results that highlight the relationship between QGIs and the geometric structures of these manifolds, demonstrating how noncommutative geometric methods can yield insights into classical geometric problems. \n\nIn summary, this discussion will provide a comprehensive introduction to Quantum Groups of Isometries, emphasizing their role in the transition from classical to quantum geometry and their relevance in contemporary mathematical research. By bridging the gap between operator algebra techniques and geometric classification, we aim to contribute to the ongoing dialogue in the field of noncommutative geometry and its applications to theoretical physics and mathematics.",
        "ori-fast-z-score": -0.6509445549041194,
        "water-fast-z-score": 3.0983866769659336,
        "rewrite-fast-z-score": 1.7820842224272613
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Distortion of Gravitational-Wave Packets Due to their Self-Gravity .\nAbstract:\nWe study the gravitational self-force acting on a wave packet in curved spacetime, and show that it causes its distortion.  We consider two types of wavepackets: one is constructed by superposing plane waves with different frequencies; another is made up of spherical waves emitted at various angles around an isolated source point. In both cases we find that the force acts as if there were additional sources located behind the packet s center-of-mass worldline. The effect can be understood intuitively using the concept of  gravitational memory . Our results are relevant for understanding how gravitational waves propagate through space-time. They also provide new insights into the problem of gravitational radiation reaction. Introduction - A fundamental question about gravitational waves (GWs) concerns how they evolve over time when propagating through curved space-time  1  . This issue has been studied extensively within the framework of linearized gravity theory  2  , where GWs are treated as small perturbations of flat Minkowski background geometry  3  .\nIn this work we focus on the effects due to gravitational self-coupling  4  . These arise because each part of a GW carries energy density which exerts pressure back onto itself via Newtonian gravity  5  . As such, the total force acting upon any given portion of a GW depends not only on the local curvature but also on the entire history of the wave  6  . It turns out that these forces cause significant distortions of the wave packets  7, 8  . For example, the shape of a plane-wave packet changes during propagation so that its peak moves away from the direction of motion  9  . Similar behavior was found for spherical wave packets  10  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Distortion of Gravitational - Wave Packets Due to their Self - Gravity . Abstract : We study the gravitational self - force acting on a wave packet in curved spacetime , and find that it creates its distortion .We consider two forms of wavepackets : one is built by superposing plane waves with various frequencies ; another is made up of spherical waves emitted at several angles around an isolated source point . In both cases we find that the force works as if there were extra sources located behind the packet s center - of - mass worldline .The phenomenon can be understood intuitively using the idea of gravitational memory . Our results are important for explaining how gravity signals propagate through space - time .They addition offer new information into the issue of gravitational radiation reaction . Introduction - A basic issue about gravitational waves ( GWs ) concerns how they develop over time when propagating through curved space - time 1 .This problem has been studied frequently within the framework of linearized gravity physics 2 , where GWs are treated as low perturbations of smooth Minkowski background geometry 3 . In this research we focus on the effects due to gravitational self - coupling 4 .These occur because each portion of a GW carries energy density which exerts pressure back onto itself via Newtonian gravity 5 . As such , the total force acting upon any certain parts of a GW relies not only on the local curvature but also on the entire history of the wave 6 .It turns out that these forces cause significant distortions of the wave packets 7 , 8 . For instance , the shape of a plane - wave packet shifts during propagation so that its peak changes away from the direction of movement 9 .Similar behavior was seen for spherical wave packets 10 .",
        "rewrite_text": "**Title:** Distortion of Gravitational-Wave Packets Due to Their Self-Gravity\n\n**Abstract:** In this study, we investigate the gravitational self-force exerted on wave packets as they traverse curved spacetime, revealing that this force induces notable distortions in their structure. We analyze two distinct types of wave packets: one constructed from the superposition of plane waves across a spectrum of frequencies, and the other composed of spherical waves emitted from various angles surrounding an isolated source point. Our findings indicate that the self-gravitational force behaves as if additional sources are positioned behind the center-of-mass worldline of the wave packet. This phenomenon can be intuitively understood through the concept of gravitational memory, which describes how past gravitational interactions influence current dynamics. The implications of our results are significant for understanding the propagation of gravitational signals through the fabric of spacetime, as they provide new insights into the complexities of gravitational radiation reaction. \n\nThe propagation of gravitational waves (GWs) through curved spacetime raises fundamental questions regarding their temporal evolution. Previous studies have predominantly approached this issue within the framework of linearized gravity, treating GWs as minor perturbations of a smooth Minkowski background. However, our research shifts focus to the effects arising from gravitational self-coupling, which occurs because each segment of a gravitational wave possesses energy density that exerts a back pressure on itself via Newtonian gravity. Consequently, the total force acting on specific regions of a gravitational wave is influenced not only by the local curvature of spacetime but also by the wave's entire historical trajectory. Our analysis reveals that these self-forces lead to significant distortions in the wave packets, exemplified by the shifting shape of a plane-wave packet during its propagation, where its peak moves away from the direction of travel. A similar distortion pattern is observed in spherical wave packets, underscoring the pervasive impact of self-gravity on the behavior of gravitational waves.",
        "ori-fast-z-score": -0.25819888974716115,
        "water-fast-z-score": 5.788544409159491,
        "rewrite-fast-z-score": 1.9952172111690554
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Tight binding formulation of the dielectric response in semiconductor nanocrystals .\nAbstract:\nWe present an ab initio tight-binding model for calculating the optical properties of semiconductor nanocrystals, which is based on the solution of the Bethe-Salpeter equation (BSE) within the framework of density functional theory (DFT). The BSE describes excitonic effects and allows to calculate absorption spectra with high accuracy. We show that our approach reproduces experimental results very well. In particular we find good agreement between calculated and measured absorption cross sections at low energies where quantum confinement dominates over electron-hole exchange interactions. Our method can be applied to any type of semiconductor material including doped systems as well as core-shell structures. Semiconductor nanocrystals are promising candidates for applications such as light-emitting diodes or solar cells due to their unique optoelectronic properties. However, it remains challenging to predict these properties accurately since they depend sensitively on the electronic structure of the system. Here we propose a new theoretical approach to tackle this problem by combining DFT calculations with the Bethe-Salpether equation (BSE), which takes into account excitonic effects beyond mean-field approaches like Kohn-Sham DFT. This enables us to obtain accurate predictions for the optical properties of semiconductor nanostructures.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Tight binding characterization of the dielectric response in semiconductor nanocrystals . Abstract : We create an ab initio close - binding model for determining the optical properties of semiconductor nanocrystals , which is based on the solve of the Bethe - Salpeter equation ( BSE ) within the framework of density functional theory ( DFT ) .The BSE describes excitonic effects and allows to estimate absorption spectra with high clarity . We see that our approach reproduces experimental results very well .In particular we find good agreement between calculated and reported absorption cross sections at low energies where quantum confinement dominates over electron - hole exchange interactions . Our techniques can be applied to any type of semiconductor material including doped systems as well as core - shell systems .Semiconductor nanocrystals are promising candidates for applications such as light - emitting diodes or solar cells due to their specific optoelectronic properties . However , it remains challenging to predict these characteristics properly since they rely sensitively on the electronic properties of the device .Here we undertake a new theoretical technique to tackle this question by combining DFT calculations with the Bethe - Salpether equation ( BSE ) , which gives into consideration excitonic effects beyond mean - field methods like Kohn - Sham DFT . This enables us to obtain precise predictions for the optical properties of semiconductor nanostructures .",
        "rewrite_text": "In this study, we present a novel ab initio tight-binding model designed to accurately characterize the optical properties of semiconductor nanocrystals. Our methodology is grounded in the solution of the Bethe-Salpeter equation (BSE) within the context of density functional theory (DFT). The BSE framework effectively captures excitonic effects, which are crucial for understanding the optical behavior of these materials, allowing us to generate highly detailed absorption spectra. Our findings demonstrate that this approach aligns closely with experimental data, particularly in the low-energy regime where quantum confinement effects are predominant and electron-hole exchange interactions are less significant. The model we developed is versatile and can be applied to a wide range of semiconductor materials, including both doped and core-shell configurations. Semiconductor nanocrystals are increasingly recognized for their potential in various applications, such as light-emitting diodes and solar cells, owing to their unique optoelectronic properties. However, accurately predicting these properties poses a significant challenge, as they are highly sensitive to the underlying electronic characteristics of the materials. To address this issue, we introduce a comprehensive theoretical framework that integrates DFT calculations with the BSE, thereby accounting for excitonic effects that are often overlooked in traditional mean-field approaches like Kohn-Sham DFT. This innovative combination allows us to make precise predictions regarding the optical properties of semiconductor nanostructures, paving the way for enhanced understanding and optimization of their performance in practical applications. Our results not only validate the efficacy of our model against experimental observations but also highlight its potential for advancing the design of next-generation optoelectronic devices.",
        "ori-fast-z-score": 1.1881770515720091,
        "water-fast-z-score": 6.207574452435047,
        "rewrite-fast-z-score": 0.26413527189768715
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Bianchi Model CMB Polarization and its Implications for CMB Anomalies .\nAbstract:\nWe present the results of our analysis on the polarization power spectrum in Bianchi type I cosmological models, which are anisotropic generalizations of standard FRW cosmologies. We find that there is no significant difference between the temperature fluctuations predicted by these two classes of models at large angular scales (low multipoles). However, we show that this is not true when one considers the polarization fluctuations. In particular, we demonstrate that the presence of an anisotropy parameter leads to a suppression of the low-l polarization power relative to the high-l part of the spectrum. This effect can be used as a test for distinguishing Bianchi type I models from their FRW counterparts. \n \n The observed lack of large-scale polarization in the WMAP data has been interpreted as evidence against inflationary scenarios with tensor perturbations. It was shown recently that such a conclusion may be premature if one takes into account possible deviations from statistical isotropy in the primordial universe. Indeed, it turns out that some anisotropic cosmological models predict less large-scale polarization than their isotropic counterparts do.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Bianchi Model CMB Polarization and its Implications for CMB Anomalies . Abstract : We present the conclusion of our analysis on the polarization power spectrum in Bianchi class I cosmological models , which are anisotropic generalizations of standard FRW cosmologies .We see that there is no major variation between the temperature fluctuations assumed by these two groups of models at large angular scales ( low multipoles ) . However , we prove that this is not true when one considers the polarization fluctuations .In particular , we prove that the presence of an anisotropy parameter causes to a suppression of the small - l polarization strength compared to the high - l part of the spectrum . This phenomenon can be used as a check for differentiate Bianchi class I systems from their FRW rivals .The observed lack of large - scale polarization in the WMAP information has been viewed as proof against inflationary scenarios with tensor perturbations . It was shown lately that such a conclusion must be premature if one takes into consideration likely deviations from statistical isotropy in the primordial universe .Indeed , it turns out that some anisotropic cosmological predictions predict less large - scale polarization than their isotropic counterparts do .",
        "rewrite_text": "We provide a comprehensive analysis of the polarization power spectrum within Bianchi class I cosmological models, which serve as anisotropic extensions of the standard Friedmann-Robertson-Walker (FRW) cosmologies. Our findings indicate that, at large angular scales (low multipoles), the temperature fluctuations predicted by both Bianchi class I models and FRW models exhibit minimal differences. However, this similarity does not extend to polarization fluctuations, where we demonstrate that the inclusion of an anisotropy parameter leads to a notable suppression of small-l polarization strength in comparison to the high-l region of the spectrum. This distinctive behavior offers a valuable method for distinguishing Bianchi class I models from their FRW counterparts. \n\nThe observed scarcity of large-scale polarization in the data from the Wilkinson Microwave Anisotropy Probe (WMAP) has been interpreted as evidence against inflationary models that incorporate tensor perturbations. Recent studies suggest that this interpretation may be overly simplistic, particularly when considering potential deviations from statistical isotropy in the early universe. Our research reveals that certain anisotropic cosmological models can predict a lower level of large-scale polarization than their isotropic equivalents. This insight underscores the importance of exploring anisotropic models in the context of cosmic microwave background (CMB) anomalies and highlights the need for a nuanced understanding of the implications of polarization data in cosmological studies. Overall, our work contributes to the ongoing discourse regarding the nature of the universe's early conditions and the validity of various cosmological models in light of observational evidence.",
        "ori-fast-z-score": -0.7977240352174656,
        "water-fast-z-score": 5.20847435706514,
        "rewrite-fast-z-score": 0.29002094671369905
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The S-parameter in Holographic Technicolor Models .\nAbstract:\nWe study the effects on electroweak precision observables (EWPO) due to new physics at the TeV scale, which is motivated by recent LHC results and theoretical arguments for naturalness. We consider two classes of models with extra dimensions: Randall-Sundrum (RS) warped space model and holographic technicolor (HTC). In RS model we find that the corrections are too large compared to EWPOs if the mass scales involved satisfy MPlanck ~ 5TeV. However, this problem can be solved by introducing an additional bulk scalar field whose VEV breaks custodial symmetry softly. The resulting correction to T parameter is found to be small enough even when MPlanck = 5TeV. On the other hand, in HTC model there exists no such difficulty because the Higgs boson is composite particle made up of techni-dilaton and techni-sigma mesons.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The S - parameter in Holographic Technicolor Models . Abstract : We research the effects on electroweak accuracy observables ( EWPO ) due to recent science at the TeV scale , which is prompted by recent LHC results and theoretical evidence for naturalness .We consider two groups of models with extra dimensions : Randall - Sundrum ( RS ) warped space model and holographic technicolor ( HTC ) . In RS model we find that the corrections are too huge compared to EWPOs if the mass scales required satisfy MPlanck ~ 5TeV .However , this situation can be answered by using an additional bulk scalar field whose VEV broken custodial symmetry quietly . The resulting correction to T parameter is found to be small enough even when MPlanck = 5TeV .On the other hand , in HTC model there exists no such difficulty because the Higgs boson is composite particle making up of techni - dilaton and techni - sigma mesons .",
        "rewrite_text": "In this study, we investigate the impact of recent advancements in TeV-scale physics on electroweak precision observables (EWPO), motivated by findings from the Large Hadron Collider (LHC) and theoretical considerations surrounding naturalness. Our analysis focuses on two distinct categories of models that incorporate extra dimensions: the Randall-Sundrum (RS) warped space model and holographic technicolor (HTC) models. \n\nIn the context of the RS model, we observe that the corrections to EWPO can be excessively large if the mass scales align with MPlanck ~ 5 TeV. However, this challenge can be mitigated by introducing an additional bulk scalar field that subtly breaks custodial symmetry through its vacuum expectation value (VEV). This adjustment leads to a significant reduction in the correction to the T parameter, rendering it sufficiently small even when MPlanck is set at 5 TeV.\n\nConversely, the HTC model does not encounter similar issues, as it posits that the Higgs boson is a composite particle formed from techni-dilaton and techni-sigma mesons. This inherent structure of the Higgs within the HTC framework allows for a more natural integration with the electroweak sector, thereby avoiding the large corrections seen in the RS model. Our findings highlight the nuanced interplay between extra-dimensional theories and electroweak observables, providing insights that could inform future research directions in particle physics and beyond.",
        "ori-fast-z-score": -1.8382900600361156,
        "water-fast-z-score": 3.0983866769659336,
        "rewrite-fast-z-score": -0.32539568672798425
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Na I D resonance lines in main sequence late-type stars .\nAbstract:\nWe present new high-resolution, near-infrared (NIR) spectra for the coolest known members of open clusters M67 and NGC 2516 obtained with the Phoenix spectrograph on Gemini South Observatory. The observations were carried out to study the sodium doublet at λλ8183/8195 Å as well as other atomic features that are sensitive to surface gravity and effective temperature. We have determined fundamental stellar parameters such as T eff , log g,  Fe/H , v sin i, and projected rotational velocity using spectral synthesis techniques. Our results show that all targets exhibit solar-like abundances within uncertainties. In addition we find evidence for differential rotation among our sample stars. Finally, we compare our derived values with those found by previous studies and discuss possible reasons behind discrepancies between different works. \n \n Keywords: Near-infrared spectroscopy, Open cluster, Surface gravity, Differential rotation, Fundamental parameters",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Na I D correlation lines in major sequence late - class stars . Abstract : We report new high - resolution , near - infrared ( NIR ) spectra for the coolest known members of close galaxies M67 and NGC 2516 obtained with the Phoenix spectrograph on Gemini South Observatory .The surveys were carried out to study the sodium doublet at λλ8183 / 8195 Å as well as other atomic features that are subject to surface gravity and effective heat . We have predicted fundamental stellar variables such as T eff , log f , Fe / H , v sin i , and projected rotational momentum using spectral synthesis techniques .Our results show that all targets exhibit solar - like abundances within uncertainties . In addition we find proof for differential rotation among our sample stars .Finally , we compare our derived values with those identified by earlier publications and consider likely explanations behind discrepancies between various works . Keywords : Near - infrared spectroscopy , Open cluster , Surface gravity , Differential rotation , Fundamental parameters",
        "rewrite_text": "**Title:** The Na I D Correlation Lines in Late-Class Main Sequence Stars\n\n**Abstract:** In this study, we present new high-resolution near-infrared (NIR) spectra of the coolest known members of the open clusters M67 and NGC 2516, acquired using the Phoenix spectrograph at the Gemini South Observatory. Our investigation focuses on the sodium doublet at wavelengths of λλ8183/8195 Å, along with other atomic features influenced by surface gravity and effective temperature. By employing spectral synthesis techniques, we have successfully derived fundamental stellar parameters, including effective temperature (T_eff), surface gravity (log g), metallicity ([Fe/H]), projected rotational velocity (v sin i), and projected rotational momentum. Our findings indicate that all observed stars display solar-like abundance patterns within the limits of measurement uncertainty. Notably, we also provide evidence for differential rotation among the stars in our sample, suggesting a complex interplay between rotation and stellar evolution in these late-class main sequence stars. Furthermore, we compare our derived parameters with those reported in previous studies, discussing potential reasons for discrepancies observed in the literature. This work contributes to a deeper understanding of the physical characteristics of late-class stars in open clusters and highlights the importance of high-resolution NIR spectroscopy in stellar astrophysics. \n\n**Keywords:** Near-infrared spectroscopy, Open clusters, Surface gravity, Differential rotation, Fundamental parameters.",
        "ori-fast-z-score": -0.3418817293789138,
        "water-fast-z-score": 4.612860393796698,
        "rewrite-fast-z-score": 1.643452031377628
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Two-dimensional defect modes in optically induced photonic lattices .\nAbstract:\nWe report on the observation and characterization of two-dimensional defect modes in optically-induced photonic crystals (OIPCs). The OIPC is formed by periodic modulation of refractive index using femtosecond laser pulses focused into fused silica glass. We show that the defect mode can be tuned over a wide range of wavelengths, which are determined by the periodicity of the lattice structure as well as the size of the defects. This work opens up new possibilities for designing optical devices based on these structures. \n \n Photonic crystal slabs have attracted considerable attention recently because they provide an excellent platform to study light-matter interactions at the nanoscale  1  . In particular, it has been shown that three-dimensional photonic crystals with point or line defects exhibit localized states within their bandgap  2  , leading to many interesting applications such as lasers  3  , filters  4  , sensors  5  , nonlinear optics  6  , etc.. However, fabrication of three-dimensional photonic crystals requires sophisticated techniques  7, 8  , making them difficult to integrate with other micro/nano-structures. Recently, several groups have demonstrated two-dimensional photonic crystals  9  -  11  fabricated directly inside transparent materials via direct laser writing  12  -  14  . These 2D photonic crystals offer advantages including ease of fabrication, flexibility in design, and compatibility with existing technologies  15  .\nIn this Letter we demonstrate the formation of defect modes in opticallyinduced photonic crystals (OPC)  16  . The OPC consists of periodically modulated refractive index created by focusing femtosecond laser pulses into fused silica glass  17  . By introducing defects into the lattice structure, we observe localized defect modes within the stopband of the OPC. Furthermore, we show that the defect mode wavelength can be continuously tuned across the entire stopband simply by changing the lattice spacing and/or the size of the defects. \nThe experimental setup used to create the OPC is illustrated schematically in Fig. 1(a) . A Ti:Sapphire regenerative amplifier system operating at 800 nm was employed to generate 100 fs duration pulses at a repetition rate of 1 kHz. The beam diameter after passing through a spatial filter",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Two - dimensional defect modes in optically induced photonic lattices . Abstract : We report on the observation and description of two - dimensional defect modes in optically - induced photonic crystals ( OIPCs ) .The OIPC is formed by periodic modulation of refractive index utilizing femtosecond laser pulses focused into fused silica glass . We see that the defect mode can be tuned over a broad variety of wavelengths , which are chosen by the periodicity of the lattice structure as well as the height of the flaws .This research raises up new possibilities for constructing optical applications based on these structures . Photonic crystal slabs have garnered considerable interest lately because they give an excellent platform to study light - matter molecules at the nanoscale 1 .In particular , it has been shown that three - dimensional photonic materials with point or line defects exhibit localized states within their bandgap 2 , leading to many interesting applications such as lasers 3 , filters 4 , devices 5 , nonlinear optics 6 , etc . . However , fabrication of three - dimensional photonic materials demands sophisticated techniques 7 , 8 , making them harder to integrate with other micro / nano - materials .Recently , various groups have demonstrated two - dimensional photonic materials 9 - 11 fabricated directly inside transparent materials via continuous laser writing 12 - 14 . These 2D photonic materials provide advantages including ease of fabrication , ease in design , and compatibility with existing devices 15 .In this Letter we prove the formation of defect modes in opticallyinduced photonic crystals ( OPC ) 16 . The OPC composed of regularly modulated refractive index created by concentrating femtosecond laser pulses into fused silica glass 17 .By introducing defects into the lattice structure , we determine localized failure modes within the stopband of the OPC . Furthermore , we find that the defect mode wavelength can be continuously tuned across the entire stopband simply by varying the crystal spacing and / or the height of the flaws .The experimental setup used to create the OPC is depicted schematically in Fig . 1 ( a ) .A Ti : Sapphire regenerative amplifier system functioning at 800 nm was employed to produce 100 fs duration pulses at a repetition rate of 1 kHz . The pulse diameter after passing through a spatial filter",
        "rewrite_text": "We present our findings on the observation and characterization of two-dimensional defect modes within optically induced photonic crystals (OIPCs). These structures are created through the periodic modulation of the refractive index in fused silica glass, achieved by focusing femtosecond laser pulses. Our results indicate that the defect modes can be finely tuned across a wide range of wavelengths, which are influenced by both the periodicity of the lattice and the dimensions of the introduced defects. This research opens up new avenues for the development of optical applications utilizing these innovative structures.\n\nRecent advancements in photonic crystal slabs have sparked significant interest due to their potential for studying light-matter interactions at the nanoscale. Notably, three-dimensional photonic materials featuring point or line defects have been shown to support localized states within their bandgap, leading to various applications such as lasers, filters, and nonlinear optical devices. However, the fabrication of these three-dimensional structures often requires complex techniques, complicating their integration with other micro- and nano-materials.\n\nIn contrast, recent efforts have demonstrated the successful creation of two-dimensional photonic materials directly within transparent substrates using continuous laser writing methods. These 2D photonic materials offer several advantages, including simpler fabrication processes, greater design flexibility, and compatibility with existing technologies. In this study, we confirm the existence of defect modes in optically induced photonic crystals, which are characterized by a regularly modulated refractive index achieved through the focused application of femtosecond laser pulses.\n\nBy introducing defects into the lattice, we identify localized failure modes within the stopband of the OIPC. Our findings reveal that the wavelength of these defect modes can be continuously adjusted throughout the entire stopband by varying either the spacing of the crystal or the height of the defects. The experimental setup utilized for the creation of the OIPC is illustrated in the accompanying figure, showcasing the use of a Ti:Sapphire regenerative amplifier system operating at 800 nm to generate 100 fs duration pulses at a repetition rate of 1 kHz. This work not only enhances our understanding of defect modes in photonic lattices but also paves the way for future optical applications based on these versatile structures.",
        "ori-fast-z-score": -0.3104602102825331,
        "water-fast-z-score": 6.887026769553818,
        "rewrite-fast-z-score": 1.3076923076923077
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Surface Brightness Profiles for a sample of LMC, SMC and Fornax galaxy Globular Clusters .\nAbstract:\nWe present surface brightness profiles (SBPs) for a sample of globular clusters in the Large Magellanic Cloud (LMC), Small Magellanic Cloud (SMC) and Fornax galaxies obtained with the Hubble Space Telescope Advanced Camera for Surveys Wide Field Channel (ACS/WFC). The SBPs are derived using archival data taken as part of the ACS Nearby Galaxy Survey Treasury program. We use these new observations to investigate whether there is any difference between the SBPs of globular cluster systems belonging to different host galaxies. In addition we compare our results with those previously published by other authors who have studied similar samples of globular clusters. Our main conclusions are:  1. There appears to be no significant differences between the SBPs of the three different types of globular clusters that were observed.  2. The majority of the globular clusters appear to follow an exponential profile which can be described by: I(r) = Ie exp -(r/rc)  where rc ~ 0.5 pc.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Surface Brightness Profiles for a sample of LMC , SMC and Fornax star Globular Clusters . Abstract : We create floor intensity profiles ( SBPs ) for a sample of globular galaxies in the Large Magellanic Cloud ( LMC ) , Small Magellanic Cloud ( SMC ) and Fornax clusters obtained with the Hubble Space Telescope Advanced Camera for Surveys Wide Field Channel ( ACS / WFC ) .The SBPs are derived using archival data taken as part of the ACS Nearby Galaxy Survey Treasury project . We use these new surveys to examine whether there is any difference between the SBPs of globular cluster systems affiliated to different host galaxies .In addition we compare our findings with those previously written by other researchers who have researched similar specimens of globular clusters . Our main results are : 1 .There seems to be no considerable changes between the SBPs of the three different kinds of globular complexes that were detected . 2 .The majority of the globular nuclei tend to follow an exponential profile which can be described by : I ( r ) = Ie exp - ( r / rc ) where rc ~ 0 . 5 pc .",
        "rewrite_text": "Title: Surface Brightness Profiles for a Sample of LMC, SMC, and Fornax Star Globular Clusters\n\nAbstract: In this study, we present the surface brightness profiles (SBPs) for a selection of globular clusters located in the Large Magellanic Cloud (LMC), Small Magellanic Cloud (SMC), and Fornax galaxy cluster. These profiles were constructed using high-resolution data obtained from the Hubble Space Telescope's Advanced Camera for Surveys Wide Field Channel (ACS/WFC). The data utilized in our analysis is part of the ACS Nearby Galaxy Survey Treasury project, which provides a comprehensive dataset for examining the structural properties of nearby galaxies. Our primary objective was to investigate potential differences in the SBPs of globular cluster systems associated with different host galaxies. \n\nTo achieve this, we compared our newly derived SBPs with existing literature on similar globular clusters, allowing us to contextualize our findings within the broader scope of current research. Our analysis yielded two significant results: first, we found no substantial differences in the SBPs among the globular clusters from the LMC, SMC, and Fornax, suggesting a level of uniformity in their structural characteristics. Second, we observed that the majority of the globular cluster nuclei exhibit an exponential brightness profile, which can be mathematically represented by the equation I(r) = Ie exp(-r/rc), where rc is approximately 0.5 parsecs. These findings contribute to our understanding of the structural properties of globular clusters and their formation processes across different galactic environments.",
        "ori-fast-z-score": -0.762000762001143,
        "water-fast-z-score": 5.921443410477893,
        "rewrite-fast-z-score": 1.6865480854231356
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Trickiness of the High-Temperature Limit for Number Density Correlation Functions in Classical Coulomb Fluids .\nAbstract:\nWe study number density correlation functions (NDCFs) in classical one-component plasma at high temperatures and find that they are not well defined due to infrared divergences, which is related with the fact that NDCF can be expressed as an integral over the whole space. We show how these difficulties can be overcome by introducing a new quantity called local number density correlation function (LNDCF). The LNDCF has no infrared divergence but it still contains information about correlations between particles on different length scales. In particular we calculate LNDCF for two limiting cases - when all particles have equal velocities or when their distribution is Maxwellian. Finally we discuss possible applications of our results. PACS numbers: 52.27.Lw, 52.35.Jm, 52.38.Bx \nI. INTRODUCTORY REMARK\nIn this work we consider classical one component plasma consisting of charged particles interacting via screened Coulomb potential  1  . This system is described by the following Hamiltonian:",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Trickiness of the High - Temperature Limit for Number Density Correlation Functions in Classical Coulomb Fluids . Abstract : We explore number density correlation functions ( NDCFs ) in classical one - component plasma at high altitudes and find that they are not well characterised due to infrared divergences , which is related with the fact that NDCF can be stated as an integral over the whole space .We see how these problems can be overcome by using a new quantity called local number density correlation function ( LNDCF ) . The LNDCF has no infrared divergence but it still contains information about correlations between particles on various length scales .In particular we estimate LNDCF for two limiting cases - when all particles have equal velocities or when their distribution is Maxwellian . Finally we explain possible applications of our findings .PACS codes : 52 . 27 . Lw , 52 . 35 . Jm , 52 . 38 . Bx I . INTRODUCTORY REMARK In this research we imagine classical one element plasma consisting of charged particles interacting via screened Coulomb energy 1 .This system is characterized by the following Hamiltonian :",
        "rewrite_text": "Title: The Challenges of High-Temperature Limit for Number Density Correlation Functions in Classical Coulomb Fluids\n\nAbstract: In this study, we investigate the number density correlation functions (NDCFs) within classical one-component plasmas at elevated temperatures. Our findings reveal that these functions are inadequately defined due to the presence of infrared divergences, a consequence of the NDCF being expressed as an integral over the entire spatial domain. To address these challenges, we introduce a novel metric known as the local number density correlation function (LNDCF). Unlike the NDCF, the LNDCF is free from infrared divergences while still encapsulating essential information regarding particle correlations across various length scales. We provide estimates for the LNDCF under two distinct scenarios: one where all particles possess identical velocities and another where their velocity distribution follows a Maxwellian profile. Furthermore, we discuss the potential implications of our results for understanding particle interactions in plasmas. Our research contributes to the broader field of plasma physics by offering insights into the behavior of correlation functions in high-temperature regimes, thereby enhancing our comprehension of classical Coulomb fluids. The findings may have significant applications in areas such as astrophysics and fusion research, where understanding plasma behavior is crucial. \n\nPACS codes: 52.27.Lw, 52.35.Jm, 52.38.Bx\n\nI. INTRODUCTORY REMARK\nIn this work, we conceptualize a classical one-component plasma composed of charged particles that interact through screened Coulomb potentials. The system is characterized by a specific Hamiltonian, which lays the foundation for our analysis.",
        "ori-fast-z-score": 0.11547005383792514,
        "water-fast-z-score": 4.129483209670111,
        "rewrite-fast-z-score": 1.8367993291867606
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Chiral symmetry and the string description of excited hadrons .\nAbstract:\nWe discuss how chiral symmetry is realized in QCD, with particular emphasis on its implications for the spectrum of excited states. We argue that the lowest-lying excitations are described by open strings attached to quarks at one end and antiquarks at the other; these can be viewed as mesons or glueballs depending on whether they carry color charge or not.  The next set of excitations corresponds to closed strings which wind around the compactified dimension (the time direction) once or more times. These correspond to baryon resonances. Finally we consider higher-spin excitations corresponding to multiple winding modes of the closed string; these have spin greater than two but still less than the number of colors Nc. In this talk I will present some recent results obtained using holographic methods applied to gauge theories dual to supergravity backgrounds. This approach has been very successful in describing many aspects of strongly-coupled gauge theory dynamics including confinement, chiral symmetry breaking, and the properties of light vector mesons. However it also provides an interesting new perspective on heavy quark physics such as the production of top-quark pairs in high-energy collisions.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Chiral symmetry and the string description of excited hadrons . Abstract : We discuss how chiral symmetry is realized in QCD , with particular focuses on its consequences for the spectrum of excited states .We argue that the smallest - lying excitations are explained by open strings attached to quarks at one end and antiquarks at the other ; these can be viewed as mesons or glueballs depending on whether they carry color charge or not . The last set of excitations corresponds to closed strings which wind around the compactified dimension ( the period direction ) once or more times .These correspond to baryon resonances . Finally we define higher - spinning excitations corresponding to multiple winding modes of the shut string ; these have spin greater than two but still less than the number of colors Nc .In this talk I will present some latest findings obtained using holographic methods applied to gauge fields dual to supergravity environments . This method has been very effective in representing many aspects of highly - coupled gauge theory physics including confinement , chiral symmetry breaking , and the properties of light vector mesons .However it also provides an interesting modern perspective on heavy quark theory such as the production of top - quark pairs in high - energy collisions .",
        "rewrite_text": "In this article, we explore the realization of chiral symmetry within the framework of Quantum Chromodynamics (QCD), emphasizing its implications for the spectrum of excited hadronic states. We propose that the lowest-lying excitations can be interpreted as open strings, with one end attached to a quark and the other to an antiquark. These configurations can be classified as either mesons or glueballs, depending on whether they possess color charge. Additionally, we identify a distinct category of excitations represented by closed strings that wrap around a compactified dimension, corresponding to baryon resonances. Furthermore, we introduce the concept of higher-spinning excitations, which arise from multiple winding modes of the closed string. These excitations exhibit spins greater than two but remain below the number of colors, Nc. \n\nIn this presentation, I will share recent findings derived from the application of holographic methods to gauge fields that are dual to supergravity backgrounds. This approach has proven to be a powerful tool in elucidating various aspects of strongly coupled gauge theory, including phenomena such as confinement, chiral symmetry breaking, and the characteristics of light vector mesons. Moreover, it offers a contemporary viewpoint on heavy quark dynamics, particularly in the context of top-quark pair production during high-energy collisions. Through this investigation, we aim to deepen our understanding of the intricate relationship between chiral symmetry and the string theory description of hadronic states, providing insights that could enhance our comprehension of fundamental particle interactions.",
        "ori-fast-z-score": 1.8905706613989794,
        "water-fast-z-score": 6.601706163700764,
        "rewrite-fast-z-score": 1.3471506281091268
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Measuring the Baryon Acoustic Oscillation scale using the SDSS and 2dFGRS .\nAbstract:\nWe measure the baryonic acoustic oscillation (BAO) scale in the distribution of galaxies by cross-correlating the Sloan Digital Sky Survey Data Release 5 with the Two Degree Field Galaxy Redshift Survey Data Release 3, both at z=0.35-0.55. We use two different estimators for the BAO peak position, one based on the correlation function and another on the power spectrum. The results are consistent within errors between these methods as well as with previous measurements made over smaller ranges of redshift or volume. Our measurement is also consistent with theoretical predictions based on WMAP1 cosmological parameters. This work demonstrates that it will be possible to make precise distance measurements out to redshifts of order unity using future large galaxy surveys such as the Dark Energy Spectroscopic Instrument. These distances can then be used to constrain dark energy models through their effect on the expansion history of the universe. \n \n Keywords: Baryons",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Measuring the Baryon Acoustic Oscillation scale utilizing the SDSS and 2dFGRS . Abstract : We estimate the baryonic sound oscillation ( BAO ) scale in the distribution of galaxies by cross - correlating the Sloan Digital Sky Survey Data Release 5 with the Two Degree Field Galaxy Redshift Survey Data Release 3 , both at z = 0 . 35 - 0 . 55 .We use two different estimators for the BAO peak point , one based on the correlation function and another on the power spectrum . The results are compatible within errors between these models as well as with previous measurements made over smaller ranges of redshift or volume .Our measurement is also consistent with theoretical estimates based on WMAP1 cosmological factors . This research shows that it will be possible to make accurate distance measurements out to redshifts of order unity utilizing upcoming huge galaxy surveys such as the Dark Energy Spectroscopic Instrument .These distances can then be used to constrain dark energy theories through their effect on the expansion history of the universe . Keywords : Baryons",
        "rewrite_text": "Title: Measuring the Baryon Acoustic Oscillation Scale Utilizing the SDSS and 2dFGRS\n\nAbstract: In this study, we investigate the baryonic acoustic oscillation (BAO) scale within the galaxy distribution by performing a cross-correlation analysis between the Sloan Digital Sky Survey (SDSS) Data Release 5 and the Two Degree Field Galaxy Redshift Survey (2dFGRS) Data Release 3, focusing on the redshift range of z = 0.35 to 0.55. To accurately estimate the BAO peak, we employ two distinct methodologies: one based on the correlation function and the other on the power spectrum. Our findings reveal that the results from both approaches are consistent within the associated uncertainties, and they align well with previous measurements obtained from smaller redshift ranges or volumes. Furthermore, our measurements are in agreement with theoretical predictions derived from the WMAP1 cosmological parameters. This research underscores the potential for precise distance measurements at redshifts approaching unity, facilitated by forthcoming large-scale galaxy surveys, such as the Dark Energy Spectroscopic Instrument (DESI). These measurements will be instrumental in constraining dark energy models by examining their influence on the universe's expansion history. The implications of this work are significant, as they pave the way for enhanced understanding of cosmic evolution and the role of dark energy in shaping the universe. \n\nKeywords: Baryons, Baryon Acoustic Oscillation, Dark Energy, Galaxy Surveys, Cosmology.",
        "ori-fast-z-score": -0.12403473458920847,
        "water-fast-z-score": 3.938927711338647,
        "rewrite-fast-z-score": 1.2375966910186262
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Extended Comment on  One-Range Addition Theorems for Coulomb Interaction Potential and Its Derivatives  by I. I. Guseinov (Chem. Phys. Vol. 309 (2005), pp. 209 - 213) .\nAbstract:\nWe have recently shown that the one-range addition theorems derived in our previous work are valid not only for the Coulomb interaction potential but also its derivatives, such as the nuclear attraction potential or the exchange potential. \n \n In this comment we show how these results can be used to derive new addition theorems for the nuclear attraction potential and the exchange potential. These new addition theorems are useful when calculating matrix elements between atomic orbitals with different angular momenta. We illustrate their application using examples involving hydrogenic wave functions. Finally, we discuss some possible extensions of these results. DOI: 10.1063/1.2055316 \n \n This is an extended version of a comment published in ChemPhysChem. DOI: 10.1002/cphc.201500420 \n \n \n \n One-range addition theorems play important roles in many areas of physics including quantum chemistry  1-3 , molecular physics  4 , condensed matter physics  5 , etc.. They provide simple expressions for evaluating matrix elements of various potentials between two arbitrary wavefunctions. For example, they allow us to calculate matrix elements of the Coulomb interaction potential between any pair of atomic orbital basis sets without having to perform complicated numerical integrations  6 . Recently, we showed that the same approach could be applied to other types of potentials  7-9 .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Extended Comment on One - Range Addition Theorems for Coulomb Interaction Potential and Its Derivatives by I . I . Guseinov ( Chem . Phys .Vol . 309 ( 2005 ) , pp .209 - 213 ) . Abstract : We have notably shown that the one - range addition theorems generated in our previous study are applicable not only for the Coulomb interaction potential but also its derivatives , such as the atomic attraction potential or the exchange potential .In this comment we give how these results can be used to derive new addition theorems for the atomic attraction potential and the exchange potential . These new addition theorems are helpful when calculating matrix elements between atomic orbitals with various angular momenta .We illustrate their application using examples involving hydrogenic wave functions . Finally , we explain some possible extensions of these results .DOI : 10 . 1063 / 1 . 2055316 This is an extended version of a remark presented in ChemPhysChem . DOI : 10 . 1002 / cphc . 201500420 One - range addition theorems play important roles in many areas of physics including quantum chemistry 1 - 3 , molecular physics 4 , condensed matter physics 5 , etc . .They offer straightforward expressions for evaluating matrix elements of different potentials between two arbitrary wavefunctions . For instance , they allow us to estimate matrix elements of the Coulomb interaction potential between any pair of atomic orbital basis sets without having to conduct complicated mathematical integrations 6 .Recently , we demonstrated that the same method could be applied to other types of potentials 7 - 9 .",
        "rewrite_text": "Title: Extended Commentary on One-Range Addition Theorems for Coulomb Interaction Potential and Its Derivatives by I. I. Guseinov (Chem. Phys. Vol. 309 (2005), pp. 209-213)\n\nAbstract: In this extended commentary, we elaborate on the applicability of the one-range addition theorems previously established in our research, highlighting their relevance not only to the Coulomb interaction potential but also to its derivatives, including the atomic attraction potential and the exchange potential. We provide a detailed discussion on how these findings can be utilized to formulate new addition theorems specifically for the atomic attraction and exchange potentials. These newly derived theorems are particularly advantageous for calculating matrix elements between atomic orbitals that possess varying angular momenta. To demonstrate the practical utility of these theorems, we present illustrative examples involving hydrogenic wave functions, showcasing how they simplify the computation of matrix elements. Furthermore, we explore potential avenues for extending these results, suggesting that the framework established here could lead to broader applications in related fields. One-range addition theorems are pivotal in numerous domains of physics, including quantum chemistry, molecular physics, and condensed matter physics, as they provide efficient expressions for evaluating matrix elements of various potentials between arbitrary wavefunctions. For example, they facilitate the estimation of matrix elements for the Coulomb interaction potential across different atomic orbital basis sets without necessitating complex mathematical integrations. Our recent findings indicate that this methodology can be effectively extended to encompass additional types of potentials, thereby enriching the theoretical toolkit available for physicists and chemists alike. DOI: 10.1063/1.2055316. This commentary serves as an expanded version of a previous remark published in ChemPhysChem. DOI: 10.1002/cphc.201500420.",
        "ori-fast-z-score": -0.6767155423319645,
        "water-fast-z-score": 3.73552251236249,
        "rewrite-fast-z-score": 1.2780193008453875
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Collective Modes in Two-band Superconductors .\nAbstract:\nWe study the collective modes in two-band superconductors with different gaps and masses, using the random phase approximation (RPA). We find that there are three types of collective modes: one is gapless and has linear dispersion relation at small wave vector; another is gapped but still has quadratic dispersion relation near the Fermi surface; while the third type is fully gapped without any low-energy excitations. The latter two types can be regarded as phonon-like collective modes. In addition to these three types of collective modes, we also find an exotic mode which does not exist in single-gap systems. This new mode originates from the interband pairing interaction between electrons on different bands. It shows up only when both intraband and interband interactions are present simultaneously. Our results show that this new mode may have important effects on the transport properties of multi-band superconductors. \n \n Introduction \n \n Multi-band superconductivity attracts much attention recently because it occurs naturally in many materials such as MgB_2  1  , Sr 2 RuO 4  2  , FeSe  3  . These compounds usually contain several orbitals per unit cell so they support multiple electronic bands crossing the Fermi level  4  . Due to the presence of more than one band, the electron-phonon coupling strength could vary significantly among different bands  5  . Moreover, the Coulomb repulsion effect becomes stronger for multi-orbital systems  6  . All these factors make the physics of multiband superconductors very rich  7, 8  .\n \nIn recent years, great progresses have been made in understanding the physical properties of multi-band superconductor  9  . For example, the vortex lattice structure  10  , magnetic field dependence  11  , thermal conductivity  12  , specific heat  13  , NMR relaxation rate  14  etc., were studied extensively by experiments. On the theoretical side, various methods including mean-field theory  15  , Eliashberg formalism  16  , functional renormalization group  17  , variational Monte Carlo  18  , exact diagonalization  19  , density matrix renormalization group  20  , and quantum Monte Carlo  21  were used to investigate the ground state properties  22  , thermodynamic quantities  23  ,",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Collective Modes in Two - band Superconductors . Abstract : We explore the collective modes in two - band superconductors with various gaps and masses , using the random phase approximation ( RPA ) .We see that there are three sorts of collective modes : one is gapless and has continuous dispersion relation at small wave vector ; another is gapped but still has quadratic dispersion relation near the Fermi surface ; while the third type is fully gapped without any low - energy excitations . The last two forms can be regarded as phonon - like collective modes .In addition to these three sorts of collective modes , we also find an exotic mode which does not occur in single - gap systems . This new mode comes from the interband pairing interaction between electrons on various groups .It gives up only when both intraband and interband interactions are present concurrently . Our results show that this new mode may have important effects on the travel properties of dual - band superconductors .Introduction Multi - band superconductivity attracts many awareness today because it appears naturally in many materials such as MgB _ 2 1 , Sr 2 RuO 4 2 , FeSe 3 . These compounds often contain many orbitals per unit cell so they support multiple electronic bands crossing the Fermi level 4 .Due to the presence of more than one band , the electron - phonon coupling strength could vary significantly among different bands 5 . Moreover , the Coulomb repulsion effect gets stronger for multi - orbital complexes 6 .All these considerations making the physics of multiband superconductors very rich 7 , 8 . In recent years , great progresses have been achieved in understanding the physical properties of multi - band superconductor 9 .For instance , the vortex lattice structure 10 , electric field dependence 11 , thermal conductivity 12 , basic heat 13 , NMR relaxation time 14 etc . , were studied frequently by research . On the theoretical side , various methods notably mean - field model 15 , Eliashberg formalism 16 , functional renormalization group 17 , variational Monte Carlo 18 , exact diagonalization 19 , density matrix renormalization group 20 , and quantum Monte Carlo 21 were used to examine the ground state properties 22 , thermodynamic quantities 23 ,",
        "rewrite_text": "**Title: Collective Modes in Two-Band Superconductors**\n\n**Abstract:** This study investigates the collective modes present in two-band superconductors characterized by varying gap sizes and effective masses, employing the random phase approximation (RPA) as our primary analytical tool. Our findings reveal three distinct types of collective modes: the first is gapless, exhibiting a continuous dispersion relation at small wave vectors; the second is gapped yet maintains a quadratic dispersion relation close to the Fermi surface; and the third type is fully gapped, lacking low-energy excitations. The latter two modes can be interpreted as phonon-like collective excitations. In addition to these conventional modes, we identify a novel mode that is absent in single-gap superconducting systems. This exotic mode arises from the interband pairing interactions among electrons belonging to different bands and is only observable when both intraband and interband interactions are simultaneously present. Our results suggest that this new mode could significantly influence the transport properties of dual-band superconductors.\n\nThe phenomenon of multi-band superconductivity has garnered considerable attention in recent years, as it is naturally found in various materials such as MgB₂, Sr₂RuO₄, and FeSe. These compounds often feature multiple orbitals per unit cell, leading to several electronic bands crossing the Fermi level. The presence of multiple bands results in a diverse electron-phonon coupling strength across different bands, while the Coulomb repulsion effect becomes more pronounced in multi-orbital systems. These factors contribute to the complex physics underlying multiband superconductors. Recent advancements have enhanced our understanding of their physical properties, with extensive research focusing on aspects such as vortex lattice structures, electric field dependencies, thermal conductivity, and NMR relaxation times. Theoretical approaches, including mean-field models, Eliashberg formalism, functional renormalization group techniques, variational Monte Carlo methods, exact diagonalization, density matrix renormalization group methods, and quantum Monte Carlo simulations, have been employed to explore the ground state properties and thermodynamic characteristics of these intriguing materials.",
        "ori-fast-z-score": 0.8563488385776753,
        "water-fast-z-score": 7.140584836498262,
        "rewrite-fast-z-score": 1.8740851426632728
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Ising spin glass models versus Ising models: an effective mapping at high temperature III. Rigorous formulation and detailed proof for general graphs .\nAbstract:\nWe present here the rigorous mathematical formulation and complete proof of our previous results on the equivalence between Ising spin glasses (ISGs) and Ising models (IMs). We show that, in the thermodynamic limit N → ∞ with fixed ratio J / T , ISG partition functions can be mapped to IM ones by means of a suitable transformation which preserves all relevant physical quantities such as free energy density f = −k B T ln Z/N . The main idea is to introduce a new set of variables {s} ≡ {s1, ..., sN } representing the local magnetizations of each site i ∈ {1, .., N }, and then to map the original problem onto one where spins are coupled only through their mutual interactions. This allows us to prove rigorously that the two systems have exactly the same statistical properties when the number of sites goes to infinity. \nThe manuscript is organized as follows. In Sec. II we define the model under investigation and state some basic definitions and notations used throughout this work. In Sec. III we provide the exact definition of the transformation introduced above and derive its explicit form for any finite system size N . Then, in Sec. IV, we discuss how it can be extended to infinite-size lattices. Finally, in Sec. V we give the full proof of the theorem stating the equivalence between ISGs and IMs.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Ising spin mirror models versus Ising models : an efficient mapping at high heat III . Rigorous characterization and complete proving for general graphs .Abstract : We present here the thorough mathematical formulation and complete proving of our previous findings on the equivalence between Ising spin glasses ( ISGs ) and Ising models ( IMs ) . We see that , in the thermodynamic limit N → ∞ with constant ratio J / T , ISG partition functions can be mapped to IM ones by means of a suitable decomposition which preserves all relevant physical components such as free energy density g = −k B T ln Z / N .The main idea is to introduce a new collection of variables { s } ≡ { s1 , . . . , sN } representing the local magnetizations of each site i ∈ { 1 , . . , N } , and then to map the previous problem onto one where spins are coupled only through their mutual interactions . This enables us to prove rigorously that the two systems have exactly the same statistical characteristics when the number of places goes to infinity .The manuscript is organized as follows.In Sec.II we define the model under inquiry and explain some fundamental definitions and notations used throughout this study . In Sec .III we provide the exact definition of the transformation introduced above and derive its explicit form for any finite system size N . Then , in Sec .IV , we explain how it can be generalized to infinite - length lattices . Finally , in Sec .V we give the full proof of the theorem stating the equivalence between ISGs and IMs.",
        "rewrite_text": "**Title:** Ising Spin Mirror Models versus Ising Models: An Efficient Mapping at High Heat III. Rigorous Characterization and Complete Proof for General Graphs\n\n**Abstract:** In this article, we provide a comprehensive mathematical framework and rigorous proof concerning the equivalence between Ising spin glasses (ISGs) and Ising models (IMs). Our analysis reveals that in the thermodynamic limit, as the number of spins N approaches infinity while maintaining a constant ratio of coupling strength J to temperature T, the partition functions of ISGs can be effectively transformed into those of IMs. This transformation is achieved through a carefully constructed decomposition that retains all essential physical attributes, including the free energy density defined as g = -k_B T ln Z / N. \n\nThe core of our approach involves introducing a new set of variables {s} = {s1, ..., sN}, which represent the local magnetizations at each site i within the set {1, ..., N}. By reformulating the problem to focus on spins that interact solely through their mutual connections, we establish a rigorous proof demonstrating that both systems exhibit identical statistical properties in the limit of infinite sites. \n\nThe structure of the manuscript is organized into several sections for clarity. In Section II, we outline the model under investigation, providing essential definitions and notations that will be referenced throughout the study. Section III presents the precise definition of the transformation we propose, along with its explicit formulation applicable to any finite system size N. In Section IV, we extend this transformation to infinite-length lattices, thereby broadening the scope of our findings. Finally, Section V contains a comprehensive proof of the theorem asserting the equivalence between ISGs and IMs, solidifying the theoretical foundation of our work. This study not only enhances the understanding of the relationship between these two models but also contributes to the broader field of statistical mechanics.",
        "ori-fast-z-score": 0.9534625892455924,
        "water-fast-z-score": 6.102160571171791,
        "rewrite-fast-z-score": 0.3563483225498992
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Spin relaxation due to the Bir-Aronov-Pikus mechanism in intrinsic and $p$-type GaAs quantum wells from a fully microscopic approach .\nAbstract:\nWe present an ab initio study on spin relaxation mechanisms in bulk, single- and double-quantum-well (DQW) structures based on zinc-blende semiconductors such as GaAs or InP. We focus our attention on the so-called Bir-Aronov-Pikuz mechanism which is responsible for spin-flip transitions between conduction-band states with different orbital angular momenta. The main results are summarized below.  For bulk materials we find that the dominant contribution comes from intra-valley scattering processes involving heavy-hole bands. This result agrees well with previous theoretical studies performed within effective-mass approximations. However, by using realistic band-structure calculations we show that inter-valley contributions can also play an important role when considering DQWs grown along non  001  directions. Finally, we discuss how these findings could be used to improve existing models describing spin relaxation times in semiconductor nanostructures.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Spin relaxation related to the Bir - Aronov - Pikus mechanism in intrinsic and $ p $ - class GaAs quantum wells from a completely microscopic perspective . Abstract : We report an ab initio investigation on spin relaxation mechanisms in bulk , double - and double - quantum - well ( DQW ) formations based on aluminium - blende semiconductors such as GaAs or InP .We focus our focus on the so - called Bir - Aronov - Pikuz process which is responsible for spin - flip transitions between conduction - band states with various orbital angular momenta . The main results are presented below .For bulk materials we find that the dominant contribution comes from intra - valley reflection processes involving heavy - hole bands . This result agrees well with previous conceptual research performed within effective - mass approximations .However , by using accurate band - structure estimates we prove that inter - valley contributions can also play an important role when assessing DQWs grown along non 001 directions . Finally , we talk how these results could be used to improve established models explaining spin relaxation times in semiconductor nanostructures .",
        "rewrite_text": "We present a comprehensive ab initio study of spin relaxation mechanisms in bulk, double, and double quantum well (DQW) structures composed of aluminum-blende semiconductors, specifically focusing on GaAs and InP. Our investigation centers on the Bir-Aronov-Pikus (BAP) mechanism, which facilitates spin-flip transitions among conduction band states characterized by different orbital angular momenta. Our findings reveal that in bulk materials, the primary contribution to spin relaxation arises from intra-valley reflection processes involving heavy-hole bands. This observation aligns with previous theoretical work conducted under effective mass approximations. However, our analysis, which incorporates precise band-structure calculations, indicates that inter-valley contributions can significantly influence spin relaxation, particularly in DQWs oriented along non-001 crystallographic directions. These insights not only enhance our understanding of spin dynamics in semiconductor systems but also provide a pathway for refining existing models that describe spin relaxation times in semiconductor nanostructures. By elucidating the roles of both intra- and inter-valley processes, our research contributes to a more nuanced understanding of spin relaxation phenomena, paving the way for advancements in spintronic applications and the development of more effective spin-based devices.",
        "ori-fast-z-score": -1.116880781646981,
        "water-fast-z-score": 4.6467017049401695,
        "rewrite-fast-z-score": 0.6965260331469925
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Multi-Color Photometry of the Galactic Globular Cluster M75 = NGC 6864. A New Sensitive Metallicity Indicator and the Position of the Horizontal Branch in UV .\nAbstract:\nWe present new multi-color photometric data for the globular cluster M75 (NGC6864) obtained with the Hubble Space Telescope Wide Field Camera 3, which allow us to study its horizontal branch stars in unprecedented detail. We find that the color distribution along the HB is bimodal, indicating two distinct populations of hot and cool HB stars. The blue tail of the observed HB can be explained by assuming an age difference between these two groups of about 1 Gyr. Using our new HST observations we also derive accurate absolute ages for both sub-populations. Our results show that the redder population has an age of 12.6 ± 0.2 Gyr while the bluer one is younger at 11.7 ± 0.1 Gyr. This finding supports previous suggestions that the redder part of the HB may have been formed during a second episode of star formation within this system. \n \n Keywords: Globular clusters; Horizontal branches",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  Multi-Color Photometry of the Galactic Globular Cluster M75 = NGC 6864.A New Sensitive Metallicity Indicator and the Position of the Horizontal Branch in UV .Abstract : We report new multi - color photometric data for the globular cluster M75 ( NGC6864 ) obtained with the Hubble Space Telescope Wide Field Camera 3 , which allow us to study its horizontal branch stars in unprecedented detail . We see that the color variation along the HB is bimodal , showing two different populations of cold and warm HB stars .The blue tail of the observed HB can be described by assuming an age difference between these two groups of about 1 Gyr . Using our new HST observations we also generate reliable absolute ages for both sub - communities .Our results show that the redder population has an age of 12 . 6 ± 0 . 2 Gyr while the bluer one is younger at 11 . 7 ± 0 . 1 Gyr . This conclusion supports earlier suggestions that the redder half of the HB may have been formed during a second episode of star formation within this system .Keywords : Globular clusters ; Horizontal branches",
        "rewrite_text": "Title: Multi-Color Photometry of the Galactic Globular Cluster M75 (NGC 6864): A Novel Sensitive Metallicity Indicator and the Horizontal Branch Position in UV\n\nAbstract: In this study, we present new multi-color photometric observations of the globular cluster M75 (NGC 6864), acquired using the Hubble Space Telescope's Wide Field Camera 3. These observations enable an in-depth analysis of the horizontal branch (HB) stars within the cluster, revealing intricate details that were previously unattainable. Our findings indicate a bimodal color distribution along the HB, suggesting the presence of two distinct populations of stars: cold and warm HB stars. The blue tail of the HB can be effectively modeled by positing an age difference of approximately 1 billion years between these two stellar groups. Leveraging our HST data, we derive precise absolute ages for each sub-population. The results indicate that the redder HB stars have an age of 12.6 ± 0.2 billion years, while the bluer stars are younger, with an age of 11.7 ± 0.1 billion years. This age disparity lends support to previous hypotheses proposing that the redder HB population may have originated from a secondary episode of star formation within the cluster. Our research not only enhances the understanding of the stellar population dynamics in M75 but also contributes to the broader discourse on the formation and evolution of globular clusters. The implications of these findings extend to the study of metallicity indicators and the evolutionary pathways of stars in dense stellar environments. \n\nKeywords: Globular clusters; Horizontal branches.",
        "ori-fast-z-score": -0.7171371656006361,
        "water-fast-z-score": 3.731961445658845,
        "rewrite-fast-z-score": -0.502518907629606
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A method for the direct determination of the surface gravities of transiting extrasolar planets .\nAbstract:\nWe present an algorithm to determine directly the surface gravity (log g) and radius ratio (Rp/Rs) of transiting exoplanets using high-precision photometry obtained with space-based observatories such as CoRoT, Kepler or Spitzer. The proposed technique is based on the measurement of the transit duration in different passbands. We show that this new approach allows us to obtain accurate values of log g and Rp/Rs even when only one transit event has been observed. This makes it possible to study the physical properties of small planets which are difficult to characterize by other methods. In addition, we demonstrate how our method can be used to detect false positives among planet candidates detected by the transit method. Finally, we apply our method to two well-studied systems, HD 209458 b and WASP-12 b, and find good agreement between our results and previous determinations. \n \n Keywords: Extrasolar planet - Surface gravity",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A algorithm for the direct determination of the surface gravities of transiting extrasolar planets . Abstract : We present an algorithm to estimate directly the surface gravity ( log g ) and radius ratio ( Rp / Rs ) of transiting exoplanets using high - precision photometry obtained with space - based observatories such as CoRoT , Kepler or Spitzer .The proposed methodology is based on the determination of the transit duration in different passbands . We see that this new approach allows us to obtain precise measures of log g and Rp / Rs even when only one transit event has been observed .This gives it able to study the physical properties of tiny planets which are hard to characterize by other methods . In addition , we prove how our technique can be used to identify false positives among planet candidates detected by the transit method .Finally , we apply our technique to two good - understood systems , HD 209458 b and WASP - 12 b , and find good agreement between our findings and previous determinations . Keywords : Extrasolar planet - Surface gravity",
        "rewrite_text": "We introduce a novel algorithm designed for the direct estimation of surface gravity (log g) and radius ratio (Rp/Rs) of transiting extrasolar planets, utilizing high-precision photometric data from space-based observatories such as CoRoT, Kepler, and Spitzer. This innovative methodology focuses on analyzing the transit duration across various passbands, enabling accurate measurements of log g and Rp/Rs even when only a single transit event is available for analysis. This capability is particularly valuable for investigating the physical characteristics of smaller exoplanets, which often present challenges for characterization through traditional methods. Furthermore, our approach demonstrates its utility in distinguishing between genuine planet candidates and false positives identified through the transit detection method. To validate our algorithm, we applied it to two well-studied exoplanet systems, HD 209458 b and WASP-12 b, and observed a strong correlation between our results and previously established measurements. This work not only enhances our understanding of the physical properties of transiting exoplanets but also contributes to the ongoing efforts in exoplanet research by providing a reliable tool for the analysis of transit data. The implications of this algorithm extend to the broader field of astrophysics, where accurate characterization of exoplanets is crucial for understanding their formation, evolution, and potential habitability. Overall, our findings underscore the significance of high-precision photometry in advancing the study of extrasolar planets and highlight the potential of our algorithm as a valuable resource for researchers in the field. \n\nKeywords: Extrasolar planets, Surface gravity, Exoplanet characterization.",
        "ori-fast-z-score": -0.1111111111111111,
        "water-fast-z-score": 4.061277619618543,
        "rewrite-fast-z-score": -1.0954451150103321
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The TACTIC atmospheric Cherenkov Imaging telescope .\nAbstract:\nThe TACTIC (Telescope Array for Cosmic Triggered Events) is an imaging atmospheric Cherenkov telescope located in Namibia at the site of the HESS experiment, and it was designed to detect gamma rays with energies between 100 GeV and 10 TeV. The camera consists of 960 photomultiplier tubes arranged on a hexagonal grid covering a field-of-view of 3 degrees diameter. It has been taking data since March 2009. In this work we present results obtained by applying different analysis techniques to the first two years of data taken with the TACTIC telescope. We show that these analyses are able to reconstruct events with high efficiency over most of the energy range covered by the instrument. Finally, we compare our results with those published by other experiments operating in similar energy ranges. This article is part of the themed issue  The Universe as seen by ground-based gamma-ray telescopes . Gamma-rays can be detected indirectly via their interaction with Earth s atmosphere, producing showers of secondary particles which emit light when they reach the ground level. These so-called air-shower photons can then be observed using large optical detectors such as imaging atmospheric Cherenkov telescopes (IACTs). IACTs have proven to be powerful instruments for studying cosmic phenomena like active galactic nuclei or supernova remnants. However, due to their relatively small fields-of-view, they usually require several hours of observation time per source before significant detection significances can be achieved. To overcome this problem, new generation IACT arrays were built recently, consisting of multiple telescopes distributed across wide areas. One example is the High Energy Stereoscopic System (H.E.S.S.)  1  , where four telescopes observe simultaneously the same region of the sky. Another one is the Telescope Array for Cosmic Triggerd Events (TACTIC), which will consist of eight telescopes spread out over a distance of about 1 km  2  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The TACTIC atmospheric Cherenkov Imaging telescope . Abstract : The TACTIC ( Telescope Array for Cosmic Triggered Events ) is an imaging atmospheric Cherenkov telescope located in Namibia at the location of the HESS program , and it was built to identify gamma radiation with energies between 100 GeV and 10 TeV .The camera consists of 960 photomultiplier cylinders arranged on a hexagonal grid covering a field - of - view of 3 degrees diameter . It has been took data since March 2009 .In this research we present results acquired by using multiple analysis methods to the first two years of evidence carried with the TACTIC telescope . We see that these calculations are able to reconstruct events with high efficiency over most of the power range covered by the instrument .Finally , we compare our findings with those published by other experiments working in identical power periods . This page is part of the themed article The Universe as shown by land - based gamma - ray telescopes .Gamma - radiation can be recorded indirectly via their contact with Earth s atmosphere , creating showers of secondary particles which emit radiation when they reach the ground level . These so - called air - shower photons can then be viewed using big optical detectors such as imaging atmospheric Cherenkov telescopes ( IACTs ) .IACTs have proven to be powerful devices for studying cosmic phenomena like active galactic nuclei or supernova remnants . However , owing to their fairly narrow fields - of - view , they generally need multiple hours of study time per source before significant discovery significances can be obtained .To solve this situation , new generation IACT arrays were built recently , consisting of multiple telescopes distributed across wide areas . One example is the High Energy Stereoscopic System ( H . E . S . S . )1 , where four telescopes observe simultaneously the same region of the sky . Another one is the Telescope Array for Cosmic Triggerd Events ( TACTIC ) , which will include of eight telescopes distributed out over a distance of about 1 cm 2 .",
        "rewrite_text": "**Title: The TACTIC Atmospheric Cherenkov Imaging Telescope**\n\n**Abstract:** The TACTIC (Telescope Array for Cosmic Triggered Events) is an advanced imaging atmospheric Cherenkov telescope situated in Namibia, co-located with the HESS (High Energy Stereoscopic System) program. Designed to detect gamma radiation in the energy range of 100 GeV to 10 TeV, TACTIC features a sophisticated camera composed of 960 photomultiplier tubes arranged in a hexagonal grid, providing a field of view with a diameter of 3 degrees. Since its operational commencement in March 2009, TACTIC has been instrumental in collecting data that enhances our understanding of high-energy astrophysical phenomena.\n\nIn this study, we present a comprehensive analysis of the data collected during the initial two years of TACTIC's operation, employing various analytical techniques to evaluate the telescope's performance. Our results demonstrate that these methodologies effectively reconstruct gamma-ray events with high efficiency across the majority of the energy spectrum accessible to the instrument. Furthermore, we conduct a comparative analysis of our findings with results from other experiments that have operated within the same energy ranges, contributing to a broader understanding of cosmic gamma-ray sources.\n\nThis research is part of a themed collection focused on \"The Universe as Revealed by Ground-Based Gamma-Ray Telescopes.\" Gamma radiation is detected indirectly through its interactions with the Earth's atmosphere, leading to the production of extensive air showers composed of secondary particles that emit Cherenkov radiation upon reaching ground level. Imaging Atmospheric Cherenkov Telescopes (IACTs) have emerged as powerful tools for investigating cosmic phenomena, including active galactic nuclei and supernova remnants. However, due to their relatively narrow fields of view, traditional IACTs typically require several hours of observation per source to achieve significant detection thresholds. To address this limitation, next-generation IACT arrays have been developed, featuring multiple telescopes strategically positioned over larger areas. TACTIC exemplifies this approach, comprising eight telescopes distributed over an area of approximately 1 cm², thereby enhancing the efficiency and effectiveness of gamma-ray observations.",
        "ori-fast-z-score": -0.5937322507759797,
        "water-fast-z-score": 7.7754191435023525,
        "rewrite-fast-z-score": 0.5696519211398116
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  MCHIT - Monte Carlo model for proton and heavy-ion therapy .\nAbstract:\nThe MCHIT (Monte Carlo Heavy Ion Transport) code is an advanced tool to simulate the transport of ions in matter, including their interactions with target nuclei as well as elastic scattering off atomic electrons. The code has been developed at GSI Helmholtzzentrum für Schwerionenforschung GmbH since 1998 by several groups under the leadership of Prof. Dr. Jens B. Skarsgard. \n \n It was originally designed to study nuclear fragmentation reactions induced by relativistic heavy ions on light targets like carbon or water. In recent years it has also been used successfully to investigate other topics such as: \n \n \n \n • Radiation damage in biological tissues due to ion beam irradiation \n \n • Secondary particle production in hadronic showers \n \n • Energy deposition in materials exposed to high-energy cosmic rays \n \n • Nuclear reaction cross sections for astrophysical applications \n \n • Hadrontherapy treatment planning",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : MCHIT - Monte Carlo method for proton and large - ion treatment . Abstract : The MCHIT ( Monte Carlo Heavy Ion Transport ) code is an advanced technique to simulate the transport of atoms in matter , particularly their interactions with target nuclei as well as elastic scattering off atomic electrons .The code has been designed at GSI Helmholtzzentrum für Schwerionenforschung GmbH since 1998 by many groups under the leadership of Prof . Dr . Jens B . Skarsgard . It was originally intended to study nuclear fragmentation reactions generated by relativistic heavy ions on light targets like nitrogen or air .In past decades it has also been used successfully to examine other topics such as : • Radiation injury in biological tissues resulting to ion beam irradiation • Secondary particle production in hadronic showers • Energy deposition in structures exposed to large - energy cosmic rays • Nuclear reaction cross sections for astrophysical applications • Hadrontherapy treatment planning",
        "rewrite_text": "Title: MCHIT - Monte Carlo Method for Proton and Large-Ion Treatment\n\nAbstract: The MCHIT (Monte Carlo Heavy Ion Transport) code represents a sophisticated computational tool developed to simulate the behavior of atoms as they traverse matter, focusing specifically on their interactions with target nuclei and the elastic scattering processes involving atomic electrons. Since its inception in 1998 at the GSI Helmholtzzentrum für Schwerionenforschung GmbH, the code has been the result of collaborative efforts led by Prof. Dr. Jens B. Skarsgard and various research groups. Initially, MCHIT was designed to investigate nuclear fragmentation reactions that occur when relativistic heavy ions collide with light targets, such as nitrogen or air. Over the years, its applications have expanded significantly, demonstrating versatility in addressing a range of scientific inquiries. \n\nAmong its notable applications, MCHIT has been instrumental in studying radiation-induced injuries in biological tissues resulting from ion beam irradiation, providing insights critical for the field of radiobiology. Additionally, the code has been employed to analyze secondary particle production in hadronic showers, which is essential for understanding cosmic ray interactions. MCHIT also plays a crucial role in evaluating energy deposition in structures subjected to high-energy cosmic rays, thereby contributing to the safety assessments of materials in space environments. Furthermore, the code has been utilized to determine nuclear reaction cross sections relevant to astrophysical phenomena, enhancing our understanding of processes occurring in stellar environments. Lastly, MCHIT has found significant utility in hadrontherapy treatment planning, aiding in the optimization of therapeutic strategies for cancer treatment using heavy ions. Overall, the MCHIT code serves as a vital resource for researchers across multiple disciplines, facilitating advancements in both theoretical and applied physics.",
        "ori-fast-z-score": -0.3511234415883917,
        "water-fast-z-score": 4.88240082724041,
        "rewrite-fast-z-score": 2.91547594742265
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Shrinking binary and planetary orbits by Kozai cycles with tidal friction .\nAbstract:\nWe study the orbital evolution of planets in binaries under the effect of gravitational perturbations due to third bodies, which can lead to large eccentricities and inclinations for both components of the system. We show that this mechanism is able to explain some observed properties of extrasolar systems such as HD 169830 or Kepler-16. In particular we find that it may be responsible for the formation of hot Jupiters through planet-planet scattering processes. The main results are summarized below: \n1) We present an analytical model describing the long-term evolution of the semi-major axes (a), eccentricities (e), and mutual inclination angles (i) of two orbiting objects under the combined effects of general relativity, tides, and secular interactions between all three bodies. \n2) Using our model, we perform numerical integrations of several representative cases showing how the presence of additional perturbative forces can significantly modify the orbital parameters of the innermost body over time-scales ranging from millions up to billions of years. \n\n3) We apply our model to the case of the exoplanetary system around HD 169830 composed of four giant planets on highly inclined orbits. Our calculations suggest that the current architecture of this system could have been produced by successive scatterings among its planets triggered by strong gravitational encounters with other massive bodies located at distances larger than 100 AU. \n \n 4) Finally, we explore the possibility that the recently discovered transiting super-Earths in the Kepler-16 system might also have formed via similar mechanisms.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Shrinking binary and planetary orbits by Kozai cycles with tidal tension . Abstract : We research the orbital evolution of planets in binaries under the impact of gravitational perturbations due to third body , which can lead to large eccentricities and inclinations for both components of the system .We see that this mechanism is able to explain some observed properties of extrasolar systems such as HD 169830 or Kepler - 16 . In particular we find that it could be responsible for the formation of bright Jupiters through moon - planet scattering mechanisms .The main results are presented below : 1 ) We present an analytical theory explaining the long - term evolve of the semi - major axes ( a ) , eccentricities ( e ) , and mutual inclination angles ( i ) of two orbiting objects under the combined influences of general relativity , tides , and secular interactions between all three bodies . 2 ) Using our model , we perform numerical integrations of several representative cases showing how the presence of added perturbative forces can significantly change the orbital variables of the innermost bodies over time - scales extending from hundreds up to billions of years .3 ) We use our model to the case of the exoplanetary structure around HD 169830 composed of four giant planets on highly inclined planets . Our calculations suggest that the present architecture of this system could have been created by successive scatterings among its stars created by weak gravitational encounters with other large bodies located at distances bigger than 100 AU .4 ) Finally , we investigate the prospect that the recently discovered transiting super - Earths in the Kepler - 16 system might also have formed via related causes .",
        "rewrite_text": "**Title:** Shrinking Binary and Planetary Orbits Induced by Kozai Cycles with Tidal Tension\n\n**Abstract:** This study explores the orbital dynamics of planets within binary systems, particularly focusing on the gravitational influences exerted by a third body. These perturbations can lead to significant increases in eccentricities and inclinations for both celestial bodies involved. Our findings provide insights into the characteristics of certain extrasolar systems, notably HD 169830 and Kepler-16, suggesting that the mechanisms at play could account for the emergence of bright Jupiter-like planets through interactions such as moon-planet scattering. \n\nWe present several key results: First, we develop an analytical framework that elucidates the long-term evolution of the semi-major axes (a), eccentricities (e), and mutual inclination angles (i) of two orbiting entities, taking into account the combined effects of general relativity, tidal forces, and secular interactions among all three bodies involved. Second, we conduct numerical simulations across various representative scenarios, demonstrating how the introduction of additional perturbative forces can significantly alter the orbital characteristics of the innermost bodies over time scales ranging from hundreds to billions of years.\n\nThird, we apply our model to the exoplanetary system surrounding HD 169830, which consists of four giant planets with notably high inclinations. Our analysis indicates that the current configuration of this system may have resulted from a series of gravitational interactions with other large bodies situated beyond 100 AU, leading to successive scatterings among its stars. Lastly, we examine the potential formation mechanisms of the recently identified transiting super-Earths within the Kepler-16 system, positing that similar processes may have contributed to their development. This research enhances our understanding of the complex dynamics governing planetary systems in binary configurations and their evolutionary pathways.",
        "ori-fast-z-score": 0.4402254531628119,
        "water-fast-z-score": 6.490229342872016,
        "rewrite-fast-z-score": 0.0854357657716761
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  On the likelihood of supernova enrichment of protoplanetary disks .\nAbstract:\nWe present an analysis of the probability that supernovae enrich protoplanetary disks with heavy elements, and discuss its implications for planet formation theory. We find that this process is likely to occur in most cases where stars are formed within clusters containing massive stars (>8 Msun). In these environments, we estimate that about half of all solar-type stars will be enriched by at least one supernova event during their disk lifetimes. This result has important consequences on our understanding of how planets form around metal-rich stars. The discovery of extrasolar giant planets orbiting metal-rich stars suggests that planet formation may be more efficient when the host star s metallicity is high. However, it remains unclear whether such planets can also form around low-mass stars like the Sun. Recent theoretical studies suggest that planetesimal accretion onto growing planetary embryos could be inhibited if the gas surrounding them contains too much dust or ice particles produced by collisions between larger bodies. If so, then the efficiency of core growth would decrease as the amount of solids increases beyond some critical value.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : On the probability of supernova enrichment of protoplanetary disks . Abstract : We present an assessment of the probability that supernovae enrich protoplanetary disks with heavy elements , and consider its consequences for planet development hypothesis .We see that this process is probably to occur in most instances where stars are created within clusters housing massive stars ( > 8 Msun ) . In these environments , we estimate that about half of all solar - class stars will be enriched by at least one supernova event during their disk lifetimes .This result has crucial consequences on our knowing of how planets form around metal - rich stars . The observation of extrasolar giant planets orbiting metal - rich stars suggests that planet development perhaps be more efficient when the host star s metallicity is high .However , it remains doubtful whether such planets can also formation around low - density stars like the Sun . Recent conceptual research suggest that planetesimal accretion onto growing planetary embryos might be inhibited if the gas covering them contains too big dust or snow molecules released by collisions between smaller bodies .If so , then the efficiency of core growth would decrease as the quantity of solids increases beyond some essential value .",
        "rewrite_text": "We provide an evaluation of the likelihood that supernovae contribute to the enrichment of protoplanetary disks with heavy elements, and we explore the implications of this process for theories of planet formation. Our analysis indicates that this enrichment is likely to occur in the majority of cases where stars are formed in clusters that include massive stars (greater than 8 solar masses). In such environments, we estimate that approximately 50% of solar-type stars will experience enrichment from at least one supernova event during the lifespan of their protoplanetary disks. This finding has significant implications for our understanding of planet formation around stars with high metallicity. Observations of extrasolar giant planets in orbit around metal-rich stars imply that the process of planet formation may be more effective in these environments. However, it remains uncertain whether similar planets can form around stars with lower metallicity, such as the Sun. Recent theoretical studies suggest that the accretion of planetesimals onto developing planetary embryos could be hindered if the surrounding gas contains excessively large dust or ice particles, which may result from collisions among smaller bodies. If this is the case, the efficiency of core growth could diminish as the concentration of solid materials exceeds a critical threshold. This research highlights the complex interplay between supernova events, metallicity, and the conditions necessary for planet formation, underscoring the need for further investigation into the mechanisms that govern these processes in various stellar environments.",
        "ori-fast-z-score": -1.7260884807271526,
        "water-fast-z-score": 4.723677731718297,
        "rewrite-fast-z-score": -0.3779644730092272
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Thermal noise informatics: Totally secure communication via a wire; Zero-power communication; and Thermal noise driven computing .\nAbstract:\nWe propose to use thermal noise as the information carrier for totally secure communications, zero power consumption devices, and computing systems. We show that by using an appropriate detection scheme, we can extract digital bits from the thermal noise with high fidelity at room temperature. The proposed system is based on a single-electron transistor (SET) which has been demonstrated in experiments recently. In addition, we demonstrate how this SET-based device can be used to perform logic operations such as AND gate and NOT gate. Finally, we discuss possible applications of our proposal including quantum key distribution, ultra-low power sensor networks, and energy efficient data centers. This work was supported by NSF under Grant No. ECCS-0926491. A new paradigm called  thermal noise informatics  is introduced where thermal noise is utilized as the information carrier for various tasks ranging from totally secure communication over wires to zero-power communication and computation. By employing an appropriate detection scheme, it is shown that one can extract digital bits from thermal noise with high fidelities even at room temperatures. It is also shown that these extracted digital bits can be used to perform logical operations such as AND gates and NOT gates. Possible applications are discussed along with their advantages compared to existing technologies.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Thermal sound informatics : Totally secure transmission via a wire ; Zero - energy transmission ; and Thermal sound fueled computing . Abstract : We suggest to use thermal noise as the information carrier for totally secure communications , zero power consumption devices , and computing systems .We see that by using an appropriate detection system , we can extract digital bits from the thermal noise with high fidelity at room temperature . The proposed system is based on a single - ion transistor ( SET ) which has been shown in experiments recently .In addition , we explain how this SET - based unit can be used to conduct logic operations such as AND gate and NOT gate . Finally , we explain possible users of our proposal including quantum key transmission , ultra - low power sensor networks , and energy safe data systems .This project was supported by NSF under Grant No . ECCS - 0926491 .A modern paradigm called temperature noise informatics is created where thermal noise is utilized as the information carrier for various problems ranging from completely secure transmission over wires to zero - energy transmission and computation . By using an appropriate detection system , it is demonstrated that one can extract digital bits from thermal noise with high fidelities even at room temperatures .It is also shown that these collected digital bits can be used to conduct logical functions such as AND gates and NOT gates . Possible applications are discussed along with their benefits compared to existing devices .",
        "rewrite_text": "**Title:** Thermal Sound Informatics: Secure Transmission via Wires, Zero-Energy Transmission, and Thermal Sound-Fueled Computing\n\n**Abstract:** In this article, we propose an innovative approach that leverages thermal noise as a medium for secure communication, energy-efficient devices, and advanced computing systems. Our research demonstrates that, with the implementation of a suitable detection mechanism, it is feasible to extract digital information from thermal noise with remarkable accuracy at ambient temperatures. Central to our methodology is the use of a single-electron transistor (SET), which recent experimental findings have validated. We further elaborate on the capabilities of this SET-based architecture to perform fundamental logic operations, including AND and NOT gates. \n\nThe concept of thermal sound informatics emerges as a modern paradigm, wherein thermal noise serves as a versatile information carrier, addressing a spectrum of challenges from secure data transmission over wired connections to the development of zero-energy computing solutions. Our findings indicate that the digital bits harvested from thermal noise can effectively facilitate logical operations, thus expanding the potential for low-power computing applications. \n\nWe also explore the implications of our work for various practical applications, highlighting the advantages of our approach over conventional technologies. Potential use cases include quantum key distribution, ultra-low power sensor networks, and energy-efficient data systems. The support for this project was provided by the National Science Foundation under Grant No. ECCS-0926491. Through this research, we aim to pave the way for the next generation of secure and energy-efficient communication and computing technologies, harnessing the inherent properties of thermal noise.",
        "ori-fast-z-score": 0.6963106238227914,
        "water-fast-z-score": 6.850168759240548,
        "rewrite-fast-z-score": -0.5144957554275265
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Environmental dielectric screening effect on exciton transition energies in single-walled carbon nanotubes .\nAbstract:\nWe report the first experimental observation of environmental dielectric screening (EDS) effects on optical transitions between different electronic states in individual semiconducting single-wall carbon nanotubes (SWCNTs). We show that EDS can be used to tune the emission energy and linewidths of SWCNT photoluminescence, which is important for applications such as optoelectronic devices based on these materials. \n \n The observed changes are explained by considering how the local environment affects the electron-hole interaction strength through its influence on the dielectric constant at the position of each tube. This work provides new insights into the fundamental physics governing the properties of carbon nanotube-based systems. Carbon nanotubes have attracted considerable attention because they exhibit unique physical characteristics  1  . In particular, their one-dimensional structure leads to interesting phenomena not found in bulk or two-dimensional materials  2  , including quantum confinement  3  , ballistic transport  4  , and strong light-matter interactions  5  .\nIn addition, recent advances in chemical synthesis techniques  6  allow us to produce high-quality samples with controlled chiralities  7, 8  . These developments make it possible to study the intrinsic properties of carbon nanotubes without being affected by extrinsic factors  9  . However, despite this progress, there remain many open questions about the basic physics underlying carbon nanotube behavior  10  . For example, although theoretical studies predict that the band gap should depend strongly on the diameter  11  , experiments have shown only weak correlations  12  . One reason may be that the actual diameters of synthesized tubes often differ significantly from those predicted theoretically  13  . Another possibility is that the surrounding medium plays an important role  14  . Indeed, previous works have demonstrated that the presence of surfactants  15  , solvent molecules  16  , and water  17  can affect the optical properties of carbon nanotubes  18  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Environmental dielectric testing influence on exciton transition energies in single - walled carbon nanotubes . Abstract : We report the first experimental measurement of environmental dielectric screening ( EDS ) impacts on electronic transitions between various electronic states in individual semiconducting single - wall carbon nanotubes ( SWCNTs ) .We see that EDS can be used to balance the emission energy and linewidths of SWCNT photoluminescence , which is important for applications such as optoelectronic devices based on these materials . The observed changes are explained by examining how the local conditions influences the electron - hole interaction strength through its influence on the dielectric constant at the position of each tube .This study provides new information into the fundamental theory controlling the properties of carbon nanotube - based systems . Carbon nanotubes have garnered considerable scrutiny because they demonstrate unique physical qualities 1 .In particular , their one - dimensional shape contributes to unusual phenomena not found in bulk or two - dimensional materials 2 , notably quantum confinement 3 , ballistic transport 4 , and strong light - matter interactions 5 . In addition , recent developments in chemical chemistry methods 6 allow us to produce high - grade samples with safe chiralities 7 , 8 .These advances provide it able to study the intrinsic characteristics of carbon nanotubes without being affected by extrinsic factors 9 . However , despite this progress , there remain many open questions about the fundamental physics underlying carbon nanotube behavior 10 .For instance , although conceptual research predict that the band gap should depend greatly on the diameter 11 , researchers have shown only weak correlations 12 . One reason could be that the actual diameters of synthesized tubes often differ significantly from those predicted theoretically 13 .Another possibility is that the nearby medium acts an important role 14 . Indeed , previous works have demonstrated that the presence of surfactants 15 , solvent molecules 16 , and water 17 can affect the optical properties of carbon nanotubes 18 .",
        "rewrite_text": "We present the inaugural experimental investigation into the effects of environmental dielectric screening (EDS) on the electronic transition energies of individual semiconducting single-walled carbon nanotubes (SWCNTs). Our findings reveal that EDS can effectively modulate both the emission energy and linewidths of photoluminescence in SWCNTs, a discovery that holds significant implications for the development of optoelectronic devices utilizing these nanomaterials. The observed variations in electronic transitions can be attributed to the influence of local environmental conditions on the strength of electron-hole interactions, which is mediated by changes in the dielectric constant surrounding each nanotube. This research contributes valuable insights into the fundamental principles governing the properties of carbon nanotube-based systems.\n\nCarbon nanotubes have attracted extensive research interest due to their remarkable physical properties, which are largely a consequence of their one-dimensional structure. This unique morphology leads to phenomena such as quantum confinement, ballistic transport, and enhanced light-matter interactions, which are not observed in bulk or two-dimensional materials. Recent advancements in chemical synthesis techniques have enabled the production of high-quality carbon nanotube samples with well-defined chiralities, facilitating the exploration of their intrinsic characteristics without interference from extrinsic factors. Despite these advancements, numerous fundamental questions regarding the physics of carbon nanotubes remain unresolved. For instance, while theoretical models suggest a strong dependence of the band gap on the nanotube diameter, empirical studies have reported only weak correlations. This discrepancy may arise from the actual diameters of synthesized nanotubes often deviating from theoretical predictions, or it could be attributed to the significant influence of the surrounding medium. Previous studies have indicated that the presence of surfactants, solvent molecules, and water can substantially alter the optical properties of carbon nanotubes, highlighting the importance of environmental factors in their behavior.",
        "ori-fast-z-score": 1.2493900951088486,
        "water-fast-z-score": 7.395739969534468,
        "rewrite-fast-z-score": 2.226922466874271
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Galactic dark matter as a bulk effect on the brane .\nAbstract:\nWe propose that galactic dark matter is an effective four-dimensional manifestation of extra dimensions, and we show how this can be realized in a simple model with one extra dimension compactified on S 1 /Z 2 . The fifth dimension has two 3-branes at its endpoints which are connected by a bulk scalar field. We find that the scalar field develops a kink profile along the fifth dimension due to the presence of a potential barrier between the two branes. This leads to a localized mass term for fermions living on the visible (3-)brane, which gives rise to a phenomenologically viable dark matter candidate. In addition, there exists another class of particles called Kaluza-Klein modes whose masses depend on the size of the extra dimension. These KK states have no tree-level interactions with Standard Model fields but they may contribute significantly to loop processes such as neutrino oscillations or proton decay. Finally, we discuss possible experimental signatures of our scenario.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Galactic dark matter as a bulk effect on the brane . Abstract : We suggest that galactic light matter is an efficient four - dimensional manifestation of extra dimensions , and we prove how this can be realized in a simple model with one extra dimension compactified on S 1 / Z 2 .The fifth dimension has two 3 - branes at its endpoints which are connected by a bulk scalar field . We see that the scalar field produces a kink profile along the fifth dimension owing to the presence of a potential barrier between the two branes .This leads to a localized mass term for fermions residing on the visible ( 3 - ) brane , which gives rise to a phenomenologically viable dark matter candidate . In addition , there exists another class of molecules known Kaluza - Klein modes whose masses vary on the size of the extra dimension .These KK states have no tree - level effects with Standard Model fields but they may contribute greatly to loop processes such as neutrino oscillations or proton decay . Finally , we explain possible experimental signatures of our scenario .",
        "rewrite_text": "Title: Galactic Dark Matter as a Bulk Effect on the Brane\n\nAbstract: In this study, we propose that the observable light matter in galaxies serves as an effective four-dimensional representation of extra dimensions. We demonstrate this concept through a straightforward model featuring a single extra dimension compactified on S¹/Z₂. Within this framework, the fifth dimension is characterized by two three-dimensional branes situated at its endpoints, which are interconnected by a bulk scalar field. The presence of this scalar field induces a kink profile along the fifth dimension, a consequence of a potential barrier that exists between the two branes. This kink profile results in a localized mass term for fermions that inhabit the visible three-brane, thereby presenting a viable candidate for dark matter from a phenomenological perspective. Furthermore, we identify a distinct category of particles known as Kaluza-Klein (KK) modes, whose masses are influenced by the size of the extra dimension. Although these KK states do not exhibit tree-level interactions with Standard Model fields, they can significantly impact loop processes, including neutrino oscillations and proton decay. We also discuss potential experimental signatures that could arise from our proposed scenario, paving the way for future investigations into the implications of extra dimensions on galactic dark matter. This work not only enhances our understanding of dark matter but also opens new avenues for exploring the interplay between higher-dimensional theories and observable physical phenomena.",
        "ori-fast-z-score": 0.562543950463012,
        "water-fast-z-score": 3.9378076532410837,
        "rewrite-fast-z-score": 1.1766968108291043
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Chandra Observations of SN 2004et and the X-ray Emission of Type IIp Supernovae .\nAbstract:\nWe present Chandra observations of supernova (SN) 2004et, which is one of only two type IIp SNe ever observed in X-rays. The data were obtained on 2005 February 24-26 with the Advanced CCD Imaging Spectrometer (ACIS-S). We detect no significant emission above background at energies below 1 keV or above 8 keV; we therefore restrict our analysis to the range 1-8 keV. In this energy band, we find that the spectrum can be fit by an absorbed blackbody model with kT = 0.7 ± 0.1 keV and N H = 2.5 +1.0 −0.8 × 10 22 cm −2 . These values are consistent with those found for other type IIp SNe. Using these parameters as well as the distance inferred from optical photometry, we calculate the luminosity of SN 2004et during its first 100 days after explosion. This value agrees very well with theoretical predictions based upon models of stellar evolution.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Chandra Observations of SN 2004et and the X - ray Emission of Type IIp Supernovae . Abstract : We report Chandra measurements of supernova ( SN ) 2004et , which is one of only two class IIp SNe actually seen in X - radiation .The data were obtained on 2005 February 24 - 26 with the Advanced CCD Imaging Spectrometer ( ACIS - S ) . We detect no considerable emission above background at energies below 1 keV or above 8 keV ; we thus restrict our analysis to the range 1 - 8 keV .In this power band , we find that the spectrum can be fit by an absorption blackbody model with kT = 0 . 7 ± 0 . 1 keV and N H = 2 . 5 + 1 . 0 −0 . 8 × 10 22 mm −2 . These values are compatible with those observed for other class IIp SNe .Using these parameters as well as the distance inferred from optical photometry , we determine the luminosity of SN 2004et during its initial 100 days after explosion . This value agrees very well with theoretical estimates based upon theories of stars evolution .",
        "rewrite_text": "We present the findings from our Chandra observations of supernova (SN) 2004et, which is notable for being one of only two Type IIp supernovae detected in X-ray emissions. The data collection occurred from February 24 to 26, 2005, utilizing the Advanced CCD Imaging Spectrometer (ACIS-S). Our analysis reveals no significant emissions above the background noise in the energy ranges below 1 keV or above 8 keV, leading us to focus our investigation on the energy band between 1 and 8 keV. Within this range, we successfully fit the observed spectrum using an absorption blackbody model, yielding a temperature of kT = 0.7 ± 0.1 keV and a hydrogen column density of N_H = 2.5 +1.0 -0.8 × 10^22 cm^-2. These parameters align well with those recorded for other Type IIp supernovae, suggesting a consistency in the physical processes governing these explosive events. Furthermore, by incorporating these spectral parameters alongside the distance derived from optical photometry, we calculated the luminosity of SN 2004et during the first 100 days post-explosion. Our findings indicate that this luminosity is in strong agreement with theoretical predictions based on stellar evolution models. This research not only enhances our understanding of the X-ray emissions associated with Type IIp supernovae but also contributes valuable data to the broader field of astrophysics, particularly in the study of stellar death and the subsequent phenomena that arise from such cataclysmic events.",
        "ori-fast-z-score": -0.7878385971583353,
        "water-fast-z-score": 4.296234062367188,
        "rewrite-fast-z-score": -0.5241424183609592
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Magnetic dead layers in La2/3Ca1/3MnO3 thin films probed by X-ray magnetic circular dichroism in reflection .\nAbstract:\nWe report on the growth and characterization of epitaxial La2/3Ca1/3MnO3-δ (LCMO) thin films grown on SrTiO3(001) (STO). The LCMO film thickness was varied between 5 nm to 50 nm, while keeping the substrate temperature fixed at 700 °C during deposition. We find that for thinner films there is an increase in oxygen deficiency as measured by XPS and XAS. This leads to a decrease in Curie temperature TC = 240 K down to 180 K with decreasing film thickness. In addition we observe a reduction in magnetization M0 below 20 nm which can be explained by the presence of a ferromagnetic dead layer near the interface. Using X-ray magnetic circular dichroim (XMCD), we show that this ferromagnetic dead layer has a net moment along the out-of-plane direction but no in-plane component. Finally, using soft x-ray resonant reflectivity measurements we demonstrate that the Mn valence state changes across the interface due to charge transfer into STO.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Magnetic dead sheets in La2 / 3Ca1 / 3MnO3 narrow bands probed by X - ray electric circular dichroism in reflection . Abstract : We report on the formation and identification of epitaxial La2 / 3Ca1 / 3MnO3 - δ ( LCMO ) thin layers developed on SrTiO3 ( 001 ) ( STO ) .The LCMO movie height was changed between 5 nm to 50 nm , while maintaining the substrate temperature fixed at 700 °C during deposition . We see that for thinner films there is an increase in oxygen deficiency as measured by XPS and XAS .This leads to a reduction in Curie temperature TC = 240 K down to 180 K with decreasing film thickness . In addition we study a reduction in magnetization M0 below 20 mm which can be described by the presence of a ferromagnetic dead surface near the interface .Using X - ray magnetic circular dichroim ( XMCD ) , we prove that this ferromagnetic dead surface has a net moment along the out - of - plane path but no in - plane part . Finally , using soft x - ray resonant reflectivity surveys we prove that the Mn valence state changes across the interface due to charge transfer into STO .",
        "rewrite_text": "**Title:** Magnetic Dead Layers in La2/3Ca1/3MnO3 Narrow Bands Investigated via X-ray Electric Circular Dichroism in Reflection\n\n**Abstract:** In this study, we explore the formation and characterization of epitaxial La2/3Ca1/3MnO3-δ (LCMO) thin films deposited on SrTiO3 (001) substrates. The thickness of the LCMO films was systematically varied between 5 nm and 50 nm, while the substrate temperature was consistently maintained at 700 °C throughout the deposition process. Our findings indicate that thinner films exhibit an increase in oxygen deficiency, as evidenced by X-ray Photoelectron Spectroscopy (XPS) and X-ray Absorption Spectroscopy (XAS) measurements. This oxygen deficiency correlates with a notable decrease in the Curie temperature (TC), which drops from 240 K to 180 K as the film thickness diminishes. Furthermore, we observe a significant reduction in magnetization (M0) for films thinner than 20 nm, which can be attributed to the presence of a ferromagnetic dead layer at the interface. Utilizing X-ray Magnetic Circular Dichroism (XMCD), we demonstrate that this ferromagnetic dead layer possesses a net magnetic moment oriented along the out-of-plane direction, while exhibiting no in-plane component. Additionally, through soft X-ray resonant reflectivity studies, we reveal that the manganese valence state undergoes a transformation across the interface, driven by charge transfer into the SrTiO3 substrate. These results provide valuable insights into the magnetic properties and electronic structure of LCMO thin films, highlighting the impact of film thickness on the emergence of magnetic dead layers and the underlying mechanisms of charge transfer at the interface.",
        "ori-fast-z-score": -1.2375966910186262,
        "water-fast-z-score": 4.2485291572496005,
        "rewrite-fast-z-score": -0.09853292781642932
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  CCD BV survey of 42 open clusters .\nAbstract:\nWe present the results of our CCD photometric study of 42 open clusters in the southern hemisphere, carried out at the 1-meter telescope of the South African Astronomical Observatory (SAAO). The observations were made with an SBIG STL-1001E camera equipped with a Kodak KAF-0400 chip and Johnson V filter during three observing runs between September 1998 and February 1999. We have used DAOPHOT II to perform aperture photometry on all stars detected within each cluster field-of-view. A total number of about 15000 stars was measured for each cluster. In addition we obtained UBVRI photometry for some of these clusters using the same instrumentation as described above.  From this data set we derived the following parameters: reddening E(B-V), distance modulus DM, age t, metallicity  Fe/H  , mass function slope x, core radius rc, central surface brightness µ0, concentration index c, and integrated absolute magnitude M.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : CCD BV survey of 42 open complexes . Abstract : We present the conclusion of our CCD photometric analysis of 42 open complexes in the southern hemisphere , conducted out at the 1 - meter telescope of the South African Astronomical Observatory ( SAAO ) .The observed were made with an SBIG STL - 1001E camera equipped with a Kodak KAF - 0400 card and Johnson V filter during three observing walks between September 1998 and February 1999 . We have utilized DAOPHOT II to conduct aperture photometry on all stars observed within each cluster field - of - view .A total quantity of about 15000 stars was measured for each cluster . In addition we received UBVRI photometry for some of these clusters using the same equipment as described above .From this data set we derived the following variables : reddening E ( B - V ) , distance modulus DM , age t , metallicity Fe / H , mass function slope x , core radius rc , central surface brightness µ0 , concentration index c , and integrated absolute magnitude M .",
        "rewrite_text": "We present the findings of our CCD photometric study of 42 open star clusters located in the southern hemisphere, conducted at the 1-meter telescope of the South African Astronomical Observatory (SAAO). This research involved observations made with an SBIG STL-1001E camera, which was equipped with a Kodak KAF-0400 chip and utilized a Johnson V filter during three observational campaigns spanning from September 1998 to February 1999. Our analysis employed DAOPHOT II software to perform aperture photometry on all stars captured within the field of view of each cluster. In total, we measured approximately 15,000 stars for each cluster. Additionally, we obtained UBVRI photometry for several of these clusters using the same observational setup. From the comprehensive dataset, we were able to derive several key astrophysical parameters, including the reddening value E(B-V), distance modulus (DM), age (t), metallicity (Fe/H), mass function slope (x), core radius (rc), central surface brightness (µ0), concentration index (c), and integrated absolute magnitude (M). These parameters are crucial for understanding the physical characteristics and evolutionary status of the open clusters studied. Our results contribute to the broader knowledge of stellar populations in the southern hemisphere and provide valuable insights into the formation and evolution of open star clusters.",
        "ori-fast-z-score": -1.3054598240132387,
        "water-fast-z-score": 2.5927248643506746,
        "rewrite-fast-z-score": 0.20412414523193154
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Power control algorithms for CDMA networks based on large system analysis .\nAbstract:\nIn this thesis, we study power control problems in cellular mobile radio systems with code division multiple access (CDMA). We consider the uplink scenario where each user transmits to its base station using an orthogonal spreading sequence and all users share the same frequency band. The objective is to minimize the total transmit power subject to individual quality-of-service constraints at each user s receiver. In order to obtain tractable results, we make use of tools from stochastic geometry which allow us to model the locations of both mobiles and interferers as point processes. Our main contributions are summarized below.  First, we derive closed-form expressions for the outage probability when the number of active users grows without bound. These expressions can be used to determine how many users can simultaneously communicate reliably over the network. Second, we propose distributed power control schemes that achieve these limits asymptotically under certain conditions. Third, we develop centralized power control algorithms that guarantee performance close to optimality even if only partial information about the channel gains or interference levels is available. Finally, we present simulation results illustrating our theoretical findings.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Power control methods for CDMA connections based on huge system theory . Abstract : In this dissertation , we study control regulation problems in cell mobile radio applications with code division multiple access ( CDMA ) .We consider the uplink situation where each consumer transmits to its base station using an orthogonal spreading pattern and all users share the same frequency band . The goal is to minimize the total broadcast capacity subject to individual quality - of - service restrictions at each consumer s receiver .In order to obtain tractable conclusions , we make using of tools from stochastic geometry which allow us to model the places of both mobiles and interferers as point processes . Our main contributions are presented below .First , we derive closed - form expressions for the outage likelihood when the number of active people grows without bound . These statements can be used to predict how many users can continuously collaborate reliably over the network .Second , we develop dispersed power control schemes that attain these limits asymptotically under certain conditions . Third , we develop concentrated energy management algorithms that guarantee efficiency low to optimality even if only partial knowledge about the channel gains or interference concentrations is accessible .Finally , we present computer findings illustrating our theory findings .",
        "rewrite_text": "Title: Power Control Methods for CDMA Connections Based on Large System Theory\n\nAbstract: This dissertation investigates the challenges of control regulation in mobile cellular networks utilizing code division multiple access (CDMA) technology. We focus on the uplink scenario, where each user transmits signals to their respective base station using unique orthogonal spreading codes, while all users operate within the same frequency band. The primary objective of our study is to minimize the overall broadcast capacity while adhering to individual quality-of-service (QoS) requirements for each user's receiver. To achieve meaningful results, we employ stochastic geometry techniques, which enable us to model the locations of both mobile users and sources of interference as point processes.\n\nOur key contributions are outlined as follows. Firstly, we derive closed-form expressions for the outage probability as the number of active users approaches infinity. These expressions provide valuable insights into the maximum number of users that can reliably communicate over the network simultaneously. Secondly, we propose distributed power control strategies that can asymptotically reach these capacity limits under specific conditions. Thirdly, we introduce centralized power management algorithms that ensure near-optimal efficiency, even when only partial information about channel gains or interference levels is available. Lastly, we present computational simulations that validate our theoretical findings, demonstrating the practical applicability of our proposed methods in real-world scenarios. Through this research, we aim to enhance the performance of CDMA systems in mobile communications by providing robust power control solutions that meet the demands of increasing user density and varying service quality requirements.",
        "ori-fast-z-score": -0.3746343246326776,
        "water-fast-z-score": 7.118652518223773,
        "rewrite-fast-z-score": 0.5107539184552492
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Search for gravitational-wave bursts in LIGO data from the fourth science run .\nAbstract:\nWe present results on searches for gravitational wave (GW) burst signals using data collected by the Laser Interferometer Gravitational-Wave Observatory during its fourth science run, which took place between September 2005 and January 2007. We use two different search methods to look for GW bursts: one based on matched filtering with template waveforms and another that uses an optimal filterbank method. The latter is used as part of a blind analysis where we do not know what type or strength of signal may be present in our data until after it has been analyzed. In addition to these analyses, we also perform several consistency checks designed to identify any problems associated with either detector s performance over this period. No significant candidates are found in any of these searches. Using simulated signals injected into the data at random times, we estimate upper limits on the rate density of binary black hole mergers detectable within a given range of total mass.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Search for gravitational - wave pulses in LIGO data from the fourth science run . Abstract : We report findings on investigations for gravitational wave ( GW ) burst signals using data received by the Laser Interferometer Gravitational - Wave Observatory during its fourth science run , which taken place between September 2005 and January 2007 .We use two different search methods to find for GW bursts : one based on matched sampling with template waveforms and another that using an efficient filterbank method . The latter is utilized as part of a blind analysis where we do not understand what type or strength of signal might be found in our information until after it has been examined .In addition to these tests , we also perform several reliability measures designed to identify any problems related with either detector s performance over this time . No important candidates are found in any of these searches .Using simulated transmissions imported into the information at random times , we estimate upper limits on the rate concentration of binary dark hole mergers detectable within a given range of total mass .",
        "rewrite_text": "In this study, we present our findings from the search for gravitational wave (GW) burst signals utilizing data collected by the Laser Interferometer Gravitational-Wave Observatory (LIGO) during its fourth science run, which spanned from September 2005 to January 2007. Our investigation employs two distinct methodologies to detect GW bursts: the first method utilizes matched filtering with predefined template waveforms, while the second method incorporates an efficient filterbank approach. The filterbank technique is particularly noteworthy as it is part of a blind analysis, meaning that we do not have prior knowledge of the potential signal types or strengths present in the data until after the analysis is complete. \n\nTo ensure the robustness of our findings, we also implement several reliability assessments aimed at identifying any issues related to the performance of the detectors throughout the observation period. Despite our comprehensive search efforts, we did not identify any significant candidates for gravitational wave bursts in the analyzed data. Furthermore, we conducted simulations by injecting synthetic signals at random intervals within the dataset to estimate upper limits on the detectable rate of binary black hole mergers, specifically focusing on the range of total mass. Our results provide important insights into the sensitivity of LIGO during this science run and contribute to the ongoing efforts to understand the population of binary black hole mergers in the universe. Overall, this work underscores the challenges in detecting transient gravitational wave signals and highlights the need for continued advancements in detection techniques and data analysis methods.",
        "ori-fast-z-score": -1.4596008983995234,
        "water-fast-z-score": 4.538253483538691,
        "rewrite-fast-z-score": 0.2727272727272727
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  XO-2b: Transiting Hot Jupiter in a Metal-rich Common Proper Motion Binary .\nAbstract:\nWe report the discovery and characterization of XO-2b, an extrasolar planet transiting its host star (HD 149026) with a period of 3.2 days. The planet is a hot Jupiter with M = 1.3 MJup and R = 0.9 RJup orbiting at a distance of only 0.04 AU from HD 149026. We find that this system has a common proper motion companion separated by ~1′′.5. This companion was previously identified as a metal-rich subgiant based on high-resolution spectroscopy but had not been detected photometrically before our observations. Our analysis shows that the transit depth variation observed for XO-2b can be explained if we assume that the two stars are physically associated and have nearly identical radii. If true, then the mass ratio between these two stars should be close to unity. However, we cannot rule out other scenarios such as grazing eclipses or blending effects due to nearby field stars.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : XO - 2b : Transiting Hot Jupiter in a Metal - rich Common Proper Motion Binary . Abstract : We report the discovery and characterization of XO - 2b , an extrasolar planet transiting its home star ( HD 149026 ) with a period of 3 . 2 days .The planet is a hot Jupiter with M = 1 . 3 MJup and R = 0 . 9 RJup orbiting at a distance of only 0 . 04 AU from HD 149026 . We see that this system has a common proper motion companion divided by ~ 1 ′ ′ . 5 .This companion was formerly identified as a metal - rich subgiant based on high - resolution spectroscopy but had not been detected photometrically before our observations . Our study shows that the transit intensity variation detected for XO - 2b can be described if we suppose that the two stars are visually associated and have nearly identical radii .If true , then the mass ratio between these two stars should be close to unity . However , we cannot leave out other scenarios such as grazing eclipses or mixing effects due to nearby field stars .",
        "rewrite_text": "We present the discovery and detailed characterization of XO-2b, an exoplanet that transits its host star, HD 149026, with an orbital period of 3.2 days. XO-2b is classified as a hot Jupiter, possessing a mass of 1.3 times that of Jupiter (MJup) and a radius of 0.9 times that of Jupiter (RJup). This gas giant orbits at a remarkably close distance of approximately 0.04 astronomical units (AU) from HD 149026. Notably, this stellar system features a common proper motion companion located about 1.5 arcminutes away. This companion star has previously been identified as a metal-rich subgiant through high-resolution spectroscopic analysis, although it had not been detected in photometric observations prior to our study. \n\nOur findings indicate that the variations in transit intensity observed for XO-2b can be effectively explained by the hypothesis that the two stars are visually associated and possess nearly identical radii. If this hypothesis holds true, it implies that the mass ratio between the two stars is likely close to unity. Nevertheless, we acknowledge the possibility of alternative explanations, including the occurrence of grazing eclipses or the influence of nearby field stars that may introduce mixing effects. This research contributes to the understanding of exoplanetary systems, particularly those involving hot Jupiters in close proximity to their host stars, and highlights the importance of considering multiple scenarios when interpreting observational data. Our work underscores the need for further investigations to clarify the dynamics and interactions within this intriguing binary system.",
        "ori-fast-z-score": -0.36650833306891567,
        "water-fast-z-score": 3.1529631254723287,
        "rewrite-fast-z-score": 0.09407208683835973
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Heisenberg antiferromagnet with anisotropic exchange on the Kagome lattice: Description of the magnetic properties of volborthite .\nAbstract:\nWe present an ab initio study of the electronic and magnetic structure of volborthite, CaFe3(PO4)2(OH)3·H2O (CFPOH), which is one of the most important minerals in geological sciences as it forms at low temperatures under hydrothermal conditions. Volborthite has been studied extensively by neutron scattering experiments but its microscopic origin remains controversial. We show that the ground state of CFPOH can be described within density functional theory using the generalized gradient approximation plus Hubbard U method for Fe-3d orbitals. The calculated spin wave spectrum agrees well with experimental data obtained by inelastic neutron scattering measurements. In addition we find that the magnetocrystalline anisotropy energy is dominated by spin-orbit coupling effects. Finally, we discuss how our results are related to previous theoretical studies based on different approximations. V olborthite, CaF e 3 (P O 4 ) 2 (OH) 3 ·H 2 O (C F P OH ), is one of the most impor-tant minerals in geological sciences because it forms at low tem-peratures under hydrothermal conditions  1  . It was first discovered in 1832  2  , however, only recently have detailed structural investigations revealed that this mineral belongs to the family of compounds known as  Kagome  materials  3  .\nVolborthite crystallizes into a layered structure consisting of alternating kagome planes of iron ions and phosphate groups  4  . This arrangement leads to interesting physical phenomena such as geometric frustration  5  or quantum fluctuations  6  . For example, recent neutron scattering experiments suggest that volborthite undergoes a phase transition below T N = 5 K  7, 8  where the spins order ferrimagnetically along the c-axis  9  . However, there exists no consensus about the nature of this ordering  10  : while some authors claim that the system orders collinearly  11, 12  others argue that non-collinearity plays an essential role  13, 14  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Heisenberg antiferromagnet with anisotropic exchange on the Kagome lattice : Description of the magnetic properties of volborthite . Abstract : We bring an ab initio investigation of the electronic and magnetic composition of volborthite , CaFe3 ( PO4 ) 2 ( OH ) 3 · H2O ( CFPOH ) , which is one of the most important minerals in geological sciences as it creates at low temperatures under hydrothermal conditions .Volborthite has been studied frequently by neutron scattering experiments but its microscopic origin seems controversial . We suggest that the ground state of CFPOH can be described within density functional theory using the generalized gradient algorithm plus Hubbard U method for Fe - 3d orbitals .The measured spinning wave spectrum agrees well with experimental evidence derived by inelastic neutron scattering observations . In addition we find that the magnetocrystalline anisotropy energy is dominated by spin - orbit bonding effects .Finally , we explain how our findings are related to previous conceptual research based on various approximations . V olborthite , CaF e 3 ( P O 4 ) 2 ( OH ) 3 · H 2 O ( C F P OH ) , is one of the most impor - tant salts in geological sciences because it forms at low tem - peratures under hydrothermal conditions 1 .It was first discovered in 1832 2 , however , only lately have sophisticated structural investigations revealed that this mineral belongs to the group of compounds known as Kagome rocks 3 . Volborthite crystallizes into a layered structure formed of alternating kagome planes of iron ions and phosphate groups 4 .This configuration leads to unusual physical phenomena such as geometric frustration 5 or quantum fluctuations 6 . For instance , recent neutron scattering experiments indicate that volborthite undergoes a phase shift below T N = 5 K 7 , 8 where the spins order ferrimagnetically along the c - axis 9 .However , there exists no consistency about the nature of this ordering 10 : while some writers claim that the scheme orders collinearly 11 , 12 others argue that non - collinearity plays an essential part 13 , 14 .",
        "rewrite_text": "**Title:** Heisenberg Antiferromagnet with Anisotropic Exchange on the Kagome Lattice: Analyzing the Magnetic Properties of Volborthite\n\n**Abstract:** This study presents a comprehensive ab initio analysis of the electronic and magnetic characteristics of volborthite, specifically CaFe3(PO4)2(OH)3·H2O (CFPOH), a mineral of significant interest in geological sciences due to its formation under low-temperature hydrothermal conditions. Despite extensive research, particularly through neutron scattering experiments, the microscopic origins of volborthite's magnetic properties remain contentious. We propose that the ground state of CFPOH can be effectively modeled using density functional theory (DFT) combined with the generalized gradient approximation and the Hubbard U method, specifically applied to the iron 3d orbitals. Our findings indicate that the calculated spin wave spectrum aligns closely with experimental data obtained from inelastic neutron scattering, reinforcing the validity of our theoretical approach. Furthermore, we identify that the magnetocrystalline anisotropy energy is predominantly influenced by spin-orbit coupling effects, which play a crucial role in the magnetic behavior of this compound. \n\nAdditionally, we contextualize our results within the framework of existing literature, addressing previous conceptual studies that have utilized various approximations to describe volborthite's magnetic ordering. Volborthite, first identified in 1832, has recently been classified among the Kagome lattice compounds, characterized by its unique layered structure comprising alternating kagome planes of iron ions and phosphate groups. This arrangement gives rise to intriguing physical phenomena, including geometric frustration and quantum fluctuations. Notably, recent neutron scattering investigations have revealed that volborthite experiences a phase transition at a Néel temperature (T_N) of approximately 5 K, where the spins exhibit ferrimagnetic ordering along the c-axis. However, there is ongoing debate regarding the nature of this magnetic ordering, with some researchers advocating for a collinear arrangement, while others emphasize the significance of non-collinear interactions. Our research aims to clarify these discrepancies and contribute to a deeper understanding of volborthite's magnetic properties.",
        "ori-fast-z-score": 0.939793423488437,
        "water-fast-z-score": 7.268326590665242,
        "rewrite-fast-z-score": 0.4016096644512494
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Squark and Gaugino Hadroproduction and Decays in Non-Minimal Flavour Violating Supersymmetry .\nAbstract:\nWe present the results for squark-antisquark, gluino-gluon and gaugino-gauge boson production at hadron colliders within the framework of non-minimal flavour violating supersymmetric models (NMFV). We consider both NMFV scenarios with MFV-like structure as well as those without it. In particular we study the impact on the decay branching ratios of neutralinos into leptons and quarks due to the presence of new sources of flavour violation beyond minimal supergravity. The latter are induced by the non-diagonal entries of the sfermion mass matrices which can be sizeable even if they are generated only radiatively. Our analysis is performed using an effective field theory approach where all heavy particles are integrated out except for the lightest neutral Higgs boson h0 and the Z-boson. This allows us to derive analytical expressions for the relevant amplitudes and cross sections.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Squark and Gaugino Hadroproduction and Decays in Non - Minimal Flavour Violating Supersymmetry . Abstract : We present the results for squark - antisquark , gluino - gluon and gaugino - gauge boson production at hadron colliders within the framework of non - reduced flavour violating supersymmetric theories ( NMFV ) .We consider both NMFV scenarios with MFV - like structure as well as those without it . In particular we study the impact on the decay branching ratios of neutralinos into leptons and quarks owing to the presence of new sources of flavour violation beyond minimal supergravity .The latter are induced by the non - diagonal entries of the sfermion mass matrices which can be sizeable even if they are produced only radiatively . Our study is conducted using an efficient field theory approach where all large particles are integrated out except for the lightest neutral Higgs boson h0 and the Z - boson .This enables us to derive analytical expressions for the appropriate amplitudes and cross sections .",
        "rewrite_text": "In this article, we investigate the production and decay processes of squarks, gluinos, and gauginos at hadron colliders, focusing on the implications of non-minimal flavour violating (NMFV) supersymmetry theories. Our analysis encompasses both NMFV scenarios that exhibit a minimal flavour violation (MFV)-like structure and those that do not. A key aspect of our research is the examination of how new sources of flavour violation, which extend beyond the conventional minimal supergravity framework, influence the decay branching ratios of neutralinos into leptons and quarks. These new sources arise from the non-diagonal elements present in the sfermion mass matrices, which can be significant even when generated through radiative processes.\n\nTo facilitate our study, we employ an efficient field theory approach that allows us to integrate out all heavy particles, retaining only the lightest neutral Higgs boson (h0) and the Z boson. This methodology enables us to derive analytical expressions for the relevant amplitudes and cross sections associated with the production and decay processes under consideration. Our findings provide valuable insights into the dynamics of squark and gaugino production in the context of NMFV supersymmetry, highlighting the potential for observable effects in collider experiments. By elucidating the role of flavour violation in these processes, we contribute to a deeper understanding of the implications of supersymmetry in high-energy physics and its potential signatures in future collider searches.",
        "ori-fast-z-score": -0.5163977794943222,
        "water-fast-z-score": 2.4327007187250236,
        "rewrite-fast-z-score": 2.1322269467806003
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Non-Cooperative Scheduling of Multiple Bag-of-Task Applications .\nAbstract:\nWe consider the problem of scheduling multiple bag-of-work applications on parallel machines with non-cooperative tasks, where each task has its own deadline and budget constraints. We propose an algorithm that is based on the idea of partitioning time into intervals such that all tasks in one interval can be scheduled simultaneously without violating their deadlines or budgets. The proposed algorithm uses dynamic programming to find the optimal schedule for these intervals. Finally, we show how this approach can be extended to handle more general cases by using bin-packing techniques. Our experimental results demonstrate significant performance improvements over existing algorithms. In particular, our algorithm outperforms state-of-the-art approaches when there are many small tasks and/or tight deadlines/budgets. Keywords: Parallel computing, Computational complexity theory, Computational geometry, Data mining, Bioinformatics, High-performance computing, Grid computing, Cloud computing, Big data analysis, Distributed systems",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Non - Cooperative Scheduling of Multiple Bag - of - Task Applications . Abstract : We consider the issue of scheduling multiple bag - of - work applications on parallel machines with non - cooperative tasks , where each work has its own deadline and funding limitations .We suggest an algorithm that is based on the idea of partitioning time into intervals such that all tasks in one interval can be scheduled simultaneously without violating their deadlines or budgets . The proposed algorithm employs dynamic programming to find the ideal schedule for these periods .Finally , we prove how this methodology can be improved to treat more general instances by using bin - packing methods . Our research results show considerable performance improvements over existing algorithms .In particular , our algorithm outperforms state - of - the - art methods when there are many small tasks and / or tight deadlines / budgets . Keywords : Parallel computing , Computational complexity analysis , Computational topology , Data mining , Bioinformatics , High - performance computing , Grid computing , Cloud computing , Big data analysis , Distributed systems",
        "rewrite_text": "Title: Non-Cooperative Scheduling of Multiple Bag-of-Task Applications\n\nAbstract: This study addresses the challenge of scheduling multiple bag-of-task applications on parallel machines, particularly in scenarios involving non-cooperative tasks. Each task is characterized by its own unique deadline and budget constraints, complicating the scheduling process. To tackle this issue, we propose a novel algorithm that partitions time into discrete intervals, allowing for the simultaneous scheduling of all tasks within a given interval without breaching their respective deadlines or budgetary limits. Our approach leverages dynamic programming techniques to identify the optimal scheduling configuration for these intervals. Furthermore, we demonstrate how our methodology can be enhanced to accommodate more complex instances through the application of bin-packing strategies. The results of our research indicate significant performance enhancements compared to existing scheduling algorithms. Notably, our algorithm excels in scenarios characterized by a high volume of small tasks and/or stringent deadlines and budget constraints, outperforming current state-of-the-art methods. This work contributes to the fields of parallel computing and computational complexity by providing a robust solution for efficient task scheduling in distributed systems, with implications for various applications in bioinformatics, data mining, and high-performance computing. Our findings underscore the potential of our algorithm to facilitate improved resource allocation and task management in cloud computing and grid computing environments, ultimately advancing the capabilities of big data analysis and distributed systems. \n\nKeywords: Parallel computing, Computational complexity analysis, Computational topology, Data mining, Bioinformatics, High-performance computing, Grid computing, Cloud computing, Big data analysis, Distributed systems.",
        "ori-fast-z-score": -0.22086305214969307,
        "water-fast-z-score": 5.019011475427825,
        "rewrite-fast-z-score": 0.9263671131731709
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Numerical simulations of fast and slow coronal mass ejections .\nAbstract:\nWe present numerical results for the evolution of two different types of solar coronal mass ejection (CME) in an open magnetic field configuration, which is relevant to space weather applications. The first type of CMEs are driven by photospheric motions that generate Alfvén waves at the base of the corona; these waves propagate upward into the corona where they steepen into shocks. The second type of CMEs are triggered by flux rope instabilities associated with current sheets formed during reconnection events between closed loops and open fields lines. We find that both types of CMEs can accelerate plasma up to speeds exceeding 1000 km/s. However, only the first type of CMEs have significant density enhancements compared to their surroundings. In addition, we show that the first type of CME has a higher probability of producing geomagnetic storms than the second one because it contains more energetic particles. Finally, our results suggest that the first type of simulated CMEs may be responsible for some observed halo CMEs.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Numerical simulations of quick and fast coronal mass ejections . Abstract : We report quantitative results for the evolution of two different kinds of thermal coronal mass ejection ( CME ) in an open magnetic field configuration , which is relevant to space weather uses .The first class of CMEs are driven by photospheric movements that produce Alfvén currents at the base of the corona ; these currents propagate eastward into the corona where they steepen into shocks . The second kind of CMEs are driven by flux rope instabilities associated with current sheets formed during reconnection events between opened loops and open fields lines .We see that both types of CMEs can accelerate plasma up to speeds exceeding 1000 kilometers / s . However , only the first kinds of CMEs have considerable density enhancements compared to their environment .In addition , we indicate that the first sort of CME has a higher likelihood of creating geomagnetic winds than the second one because it contains more energetic particles . Finally , our findings show that the first sort of simulated CMEs might be responsible for some observed halo CMEs .",
        "rewrite_text": "In this study, we present a detailed analysis of the dynamics of two distinct types of thermal coronal mass ejections (CMEs) within an open magnetic field configuration, which holds significant implications for space weather forecasting. The first category of CMEs is initiated by photospheric movements that generate Alfvén currents at the base of the solar corona. These currents subsequently propagate eastward, where they steepen and form shock waves as they travel through the corona. In contrast, the second category of CMEs arises from flux rope instabilities linked to current sheets that develop during reconnection events between open magnetic loops and field lines. Our simulations reveal that both types of CMEs are capable of accelerating plasma to velocities exceeding 1000 kilometers per second. Notably, the first type of CME exhibits substantial density enhancements relative to the surrounding plasma, a characteristic that distinguishes it from the second type. Furthermore, we demonstrate that the first type of CME is more likely to generate geomagnetic winds due to its higher concentration of energetic particles. This finding underscores the potential impact of these CMEs on space weather phenomena. Ultimately, our results suggest that the first type of simulated CME may be responsible for certain observed halo CMEs, highlighting the importance of understanding the mechanisms behind these solar events. This research contributes to the broader understanding of CME dynamics and their implications for space weather, providing valuable insights for future studies in solar physics and its effects on terrestrial environments.",
        "ori-fast-z-score": -1.2939932784412609,
        "water-fast-z-score": 5.607304206578798,
        "rewrite-fast-z-score": 1.756550621379892
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Detailed Spectral Analysis of the Type Ib Supernova 1999dn. Paper I: Hydrogen-free Models .\nAbstract:\nWe present detailed spectral analysis for the type Ib supernova (SN) 1999dn, which was discovered on September 28th in NGC 3184 by Maza et al. (1999) . The photometric evolution is well reproduced with an exponential decay law and we find that the light curve can be explained as being powered by radioactive 56Ni synthesized during explosive nucleosynthesis. We have analyzed optical spectra obtained at various epochs after explosion using non-LTE time-dependent radiative transfer calculations based on our new code STELLA. In this work, we show results for models without hydrogen lines. Our best-fit model has a total ejecta mass of about 18M⊙ , consisting mainly of helium and carbon-oxygen mixture. This result suggests that SN 1999dn may belong to the class of super-luminous SNe Ia. Keywords: Supernovae, Radiation hydrodynamics, Time dependent",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Detailed Spectral Analysis of the Type Ib Supernova 1999dn . Paper I : Hydrogen - free Models .Abstract : We present detailed spectral evaluation for the class Ib supernova ( SN ) 1999dn , which was discovered on September 28th in NGC 3184 by Maza et al . ( 1999 ) .The photometric evolution is well illustrated with an exponential decay law and we find that the light curve can be described as being driven by radioactive 56Ni extracted during explosive nucleosynthesis . We have analyzed laser spectra obtained at numerous epochs after explosion employing non - LTE time - dependent radiative transfer calculations based on our new code STELLA .In this work , we show results for models without hydrogen lines . Our best - fit model has a total ejecta mass of about [UNK] , consisting mainly of helium and carbon - oxygen mixture .This result suggests that SN 1999dn may belong to the class of super-luminous SNe Ia.Keywords: Supernovae, Radiation hydrodynamics, Time dependent",
        "rewrite_text": "We provide a comprehensive spectral analysis of the Type Ib supernova 1999dn, which was first identified on September 28, 1999, in the galaxy NGC 3184 by Maza et al. Our study highlights the photometric evolution of this supernova, which follows an exponential decay pattern. We attribute the light curve's behavior primarily to the radioactive decay of 56Ni produced during the explosive nucleosynthesis process. Utilizing laser spectra collected at various stages post-explosion, we employed non-local thermodynamic equilibrium (non-LTE) time-dependent radiative transfer calculations through our newly developed code, STELLA. This paper focuses on models that do not include hydrogen lines. Our optimal model indicates a total ejecta mass of approximately [UNK], predominantly composed of a helium and carbon-oxygen mixture. These findings imply that SN 1999dn may be classified within the category of super-luminous Type Ia supernovae. This research contributes to the understanding of supernova mechanisms and the characteristics of their spectral emissions, providing valuable insights into the nature of these explosive events. \n\nKeywords: Supernovae, Radiation hydrodynamics, Time-dependent.",
        "ori-fast-z-score": -0.3721042037676254,
        "water-fast-z-score": 2.604729426373378,
        "rewrite-fast-z-score": -0.9058216273156765
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Post-Oligarchic Evolution of Protoplanetary Embryos and the Stability of Planetary Systems .\nAbstract:\nWe present an analysis of the stability of planetary systems in which protoplanetary embryos evolve under oligarchy, i.e., they are able to eject each other s neighbors by gravitational scattering but not themselves. We find that this process leads to rapid growth of the largest embryo until it reaches its isolation mass (the minimum mass required for runaway accretion). The system then evolves into either a single planet or two planets with comparable masses depending on how close the initial conditions were to instability. This evolution is very different than what happens when all bodies grow simultaneously; in particular, we show that there can be multiple stable outcomes even if the initial conditions are identical. Our results suggest that the formation of terrestrial planets may have proceeded through several stages including oligarchy before reaching their final state as observed today. In addition, our work provides new insights about the origin of Mercury-like planets. Protoplanetary embryos form in circumstellar disks around young stars and undergo mutual gravitational interactions during their growth phase. These interactions lead to orbital migration and dynamical instabilities such as collisions between neighboring embryos. If these processes occur frequently enough, only one body will survive at the end of the growth stage leaving behind a planetary system consisting of just one planet. However, recent studies indicate that many planetary systems contain more than one planet suggesting that some mechanism must exist to prevent complete destruction of the system. Here we study the possibility that protoplanetary embryos follow a hierarchical evolutionary path where they first grow hierarchically via gravitational scattering followed by runaway accretion once the largest embryo has reached its isolation mass. Using numerical simulations, we demonstrate that this scenario naturally explains the existence of multi-planet systems while also reproducing the properties of known exoplanets.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Post - Oligarchic Evolution of Protoplanetary Embryos and the Stability of Planetary Systems . Abstract : We present an assessment of the stability of planetary environments in which protoplanetary embryos grow under oligarchy , i . e . , they are able to eject each other s neighbors by gravitational scattering but not themselves .We see that this process results to rapid growth of the largest embryo until it hits its isolation volume ( the minimum mass needed for runaway accretion ) . The system then evolves into either a single planet or two planets with similar masses depending on how close the early conditions were to instability .This evolution is very different than what happens when all bodies grow simultaneously ; in particular , we find that there can be several stable outcomes even if the first environments are identical . Our results propose that the formation of terrestrial worlds may have continued through several stages including oligarchy before reaching their final condition as predicted today .In addition , our work brings fresh insights about the origin of Mercury - like planets . Protoplanetary embryos form in circumstellar disks around young galaxies and undergo mutual gravitational interactions during their development period .These interactions result to orbital movement and dynamical instabilities such as collisions between neighboring embryos . If these mechanisms occur frequently enough , only one body will survive at the end of the development period leaving behind a planetary system consisting of just one planet .However , recent studies reveal that several planetary complexes comprise more than one planet suggesting that some process need arise to resist total destruction of the system . Here we study the suggestion that protoplanetary embryos pursue a hierarchical evolutionary course where they first develop hierarchically via gravitational waves followed by runaway accretion once the greatest embryo has reached its isolation mass .Using numerical simulations , we prove that this situation naturally reveals the existence of dual - planet systems while actually reproducing the properties of known exoplanets .",
        "rewrite_text": "**Title:** Post-Oligarchic Evolution of Protoplanetary Embryos and the Stability of Planetary Systems\n\n**Abstract:** In this study, we investigate the stability of planetary environments characterized by the oligarchic growth of protoplanetary embryos, where these embryos can gravitationally scatter and eject their neighbors but do not expel themselves. Our findings indicate that this oligarchic process leads to the rapid growth of the largest embryo until it reaches its isolation volume, defined as the minimum mass required for runaway accretion. Following this phase, the system can evolve into either a single planet or a pair of planets with comparable masses, contingent on the proximity of initial conditions to instability. This evolutionary pathway diverges significantly from scenarios where all bodies grow concurrently, as we observe that multiple stable outcomes can emerge even from identical initial environments. \n\nOur results suggest that the formation of terrestrial planets may have undergone several evolutionary stages, including an oligarchic phase, before achieving their current configurations. Furthermore, our research provides new insights into the origins of Mercury-like planets. Protoplanetary embryos develop within circumstellar disks surrounding young stars, engaging in mutual gravitational interactions throughout their formative period. These interactions can lead to dynamic instabilities, including collisions among neighboring embryos, which, if frequent enough, may result in the survival of only a single body, thus forming a planetary system with just one planet. However, recent observations indicate that many planetary systems consist of multiple planets, implying the necessity of mechanisms that prevent the total destruction of these systems.\n\nIn this paper, we explore the hypothesis that protoplanetary embryos undergo a hierarchical evolutionary process, initially developing through gravitational interactions followed by runaway accretion once the largest embryo attains its isolation mass. Through numerical simulations, we demonstrate that this evolutionary framework naturally accounts for the existence of dual-planet systems while accurately reflecting the characteristics of known exoplanets.",
        "ori-fast-z-score": -1.3063945294843617,
        "water-fast-z-score": 7.242717283063319,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Magnetization oscillations induced by a spin-polarized current in a point-contact geometry: mode hopping and non-linear damping effects .\nAbstract:\nWe study the magnetization dynamics driven by an alternating spin polarized current (ASPC) flowing through a magnetic tunnel junction with perpendicular anisotropy. We show that, depending on the amplitude of the ASPC, two different regimes can be observed experimentally: i) for small amplitudes, we observe a single frequency corresponding to the ferromagnetic resonance; ii) when increasing the amplitude of the ASCP, several frequencies are excited simultaneously leading to a complex spectrum which is analyzed using numerical simulations based on the Landau-Lifshitz-Gilbert equation including spin-transfer torque terms. The results obtained are discussed in connection with recent experiments performed at room temperature. \n \n PACS: 75.60.Cc, 76.30.+z, 77.20.Hs \n \n Spin transfer torques have been extensively studied both theoretically and experimentally during last years  1-3 . In particular, it has been shown that they induce precessional motion of the magnetization  4-6  as well as steady-state phenomena  7-9  such as domain-wall motion  10-12  or vortex core reversal  13-15 . These effects have attracted great interest due to their potential applications in novel devices like microwave oscillators  16  , logic elements  17  , memories  18  . However, most studies were focused on macroscopic systems where the magnetization was uniform over large distances. Recently, there has been growing interest in studying these effects in nanostructures  19-21  since this allows one to explore new physical properties associated with reduced dimensions  22  .\n \nIn this work, we focus our attention on the magnetization dynamics driven out of equilibrium by an alternating spin polarized Current (ASPC). This problem has already been addressed theoretically  23  but only few experimental works have been reported so far  24  . Here, we present detailed measurements carried out on a magnetic tunnel junction (MTJ), made of CoFeB/MgO/CoFeB layers grown by sputtering  25  . By applying an external field Hext along the hard axis of the MTJ, we obtain a perpendicularly magnetized system whose static properties are described elsewhere  26  . When",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Magnetization oscillations induced by a spin - polarized current in a spot - touch geometry : mode hopping and non - linear damping effects . Abstract : We research the magnetization dynamics generated by an alternating spin polarized current ( ASPC ) flowing through a magnetic tunnel junction with perpendicular anisotropy .We see that , depending on the frequency of the ASPC , two different regimes can be found experimentally : i ) for low amplitudes , we study a single frequency corresponding to the ferromagnetic resonance ; ii ) when reducing the frequency of the ASCP , various frequencies are excited simultaneously giving to a complex spectrum which is studied using numerical simulations based on the Landau - Lifshitz - Gilbert formula featuring spin - transfer torque terms . The results derived are discussed in connection with recent experiments conducted at room temperature .PACS : 75 . 60 . Cc , 76 . 30 . + z , 77 . 20 . Hs Spin transfer torques have been heavily examined both theoretically and experimentally during last decades 1 - 3 . In particular , it has been shown that they cause precessional motion of the magnetization 4 - 6 as well as continuous - state effects 7 - 9 such as domain - wall motion 10 - 12 or vortex core reversal 13 - 15 .These effects have garnered great interest due to their potential applications in novel systems like microwave oscillators 16 , logic elements 17 , memories 18 . However , most studies were focused on macroscopic environments where the magnetization was uniform over large distances .Recently , there has been growing interest in investigating these phenomena in nanostructures 19 - 21 since this enables one to examine novel physical properties associated with reduced dimensions 22 . In this research , we focus our focus on the magnetization dynamics caused out of equilibrium by an alternating spin polarized Current ( ASPC ) .This problem has already been addressed theoretically 23 but only few experimental works have been reported so far 24 . Here , we present detailed observations carried out on a magnetic tunnel junction ( MTJ ) , made of CoFeB / MgO / CoFeB layers grown by sputtering 25 .By applying an external field Hext along the hard axis of the MTJ , we obtain a perpendicularly magnetized body whose static properties are explained elsewhere 26 . When",
        "rewrite_text": "**Title:** Magnetization Oscillations Induced by a Spin-Polarized Current in a Spot-Touch Geometry: Mode Hopping and Non-Linear Damping Effects\n\n**Abstract:** This study investigates the dynamics of magnetization induced by an alternating spin-polarized current (ASPC) flowing through a magnetic tunnel junction (MTJ) characterized by perpendicular magnetic anisotropy. Our experimental findings reveal two distinct regimes based on the frequency of the ASPC. In the first regime, at low amplitudes, we observe a single frequency corresponding to ferromagnetic resonance. Conversely, as the frequency of the ASPC decreases, we detect the simultaneous excitation of multiple frequencies, resulting in a complex spectrum. This phenomenon is analyzed through numerical simulations grounded in the Landau-Lifshitz-Gilbert (LLG) equation, which incorporates spin-transfer torque effects. The implications of our results are contextualized with recent experimental observations conducted at room temperature.\n\nThe exploration of spin-transfer torques has garnered significant attention in both theoretical and experimental domains over the past few decades. These torques have been shown to induce precessional motion of magnetization and facilitate continuous-state phenomena, such as domain-wall motion and vortex core reversal. The interest in these effects is largely driven by their potential applications in advanced technologies, including microwave oscillators, logic devices, and memory systems. Historically, most research has concentrated on macroscopic systems where magnetization remains uniform over extensive distances. However, recent advancements have shifted focus toward nanostructures, allowing for the investigation of unique physical properties that emerge in reduced dimensions.\n\nIn this paper, we specifically address the out-of-equilibrium magnetization dynamics triggered by ASPC, a topic that has been theoretically explored but remains underrepresented in experimental studies. We present comprehensive observations from our experiments on an MTJ composed of CoFeB/MgO/CoFeB layers, fabricated via sputtering techniques. By applying an external magnetic field along the hard axis of the MTJ, we achieve a perpendicularly magnetized structure, with its static properties detailed in previous works. Our findings contribute to the understanding of magnetization dynamics in nanoscale systems and open avenues for future research in this rapidly evolving field.",
        "ori-fast-z-score": -0.16012815380508713,
        "water-fast-z-score": 6.024144966768741,
        "rewrite-fast-z-score": 0.71827819602086
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Stochastic interacting particle systems out of equilibrium .\nAbstract:\nWe consider the dynamics of stochastic interacting particle systems in which particles evolve according to an overdamped Langevin equation with multiplicative noise and interact via pair potentials that decay exponentially fast at large distances.  We prove that, under suitable conditions on the interaction potential, these models admit unique stationary states whose density profiles are given by solutions of nonlinear integral equations involving fractional powers of the Laplacian operator. These results provide new insights into the statistical mechanics of such systems far away from thermal equilibrium. Stochastic interacting particle systems have been widely used as simple models for describing physical phenomena ranging from traffic flow  1  , granular media  2  , colloidal suspensions  3  , and biological transport  4  . In this work we focus our attention on one-dimensional models where each particle evolves according to an overdamped Brownian motion driven by white Gaussian noise and interacts with its neighbors through a pairwise potential that decays exponentially fast at infinity (see Figure 1 ). The resulting system is described by the following set of Itô SDE s:",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Stochastic interacting particle structures out of equilibrium . Abstract : We consider the dynamics of stochastic interacting particle structures in which particles evolve according to an overdamped Langevin equation with multiplicative interference and interact via pair potentials that decay exponentially rapidly at large distances .We prove that , under suitable conditions on the interaction potential , these models admit unique stationary states whose density profiles are given by solutions of nonlinear integral equations involving fractional powers of the Laplacian operator . These data provide fresh insights into the statistical mechanics of such systems farther far from temperature equilibrium .Stochastic interacting particle networks have been widely using as simple theories for describing physical phenomena ranging from street circulation 1 , granular material 2 , colloidal suspensions 3 , and biological transport 4 . In this research we focus our focus on one - dimensional theories where each particle evolves according to an overdamped Brownian movement generated by white Gaussian interference and interacts with its neighbors through a pairwise potential that decays exponentially rapidly at infinity ( see Figure 1 ) .The resulting system is characterized by the following setting of Itô SDE s :",
        "rewrite_text": "Title: Stochastic Interacting Particle Structures Out of Equilibrium\n\nAbstract: This study investigates the dynamics of stochastic interacting particle systems, where particles evolve according to an overdamped Langevin equation influenced by multiplicative noise and interact through pair potentials that diminish exponentially at large distances. We establish that, under specific conditions pertaining to the interaction potential, these models possess unique stationary states. The density profiles of these states are determined by solutions to nonlinear integral equations that incorporate fractional powers of the Laplacian operator. This research contributes valuable insights into the statistical mechanics of systems that are significantly removed from thermal equilibrium. Stochastic interacting particle networks serve as effective frameworks for modeling a variety of physical phenomena, including urban traffic dynamics, granular materials, colloidal suspensions, and biological transport mechanisms. Our focus is primarily on one-dimensional systems, where each particle undergoes overdamped Brownian motion driven by white Gaussian noise, while interacting with neighboring particles through a pairwise potential that rapidly decays to zero at infinity. The dynamics of this system can be described using a set of Itô stochastic differential equations (SDEs). The findings presented in this article not only enhance our understanding of the behavior of such systems in non-equilibrium conditions but also pave the way for future research in related fields, potentially leading to new applications in statistical physics and beyond. The implications of these results are significant for both theoretical explorations and practical applications, as they offer a deeper comprehension of the complex interactions that govern particle dynamics in various contexts.",
        "ori-fast-z-score": -0.9332565252573828,
        "water-fast-z-score": 4.538253483538691,
        "rewrite-fast-z-score": 1.2675004445952593
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  On Newton s law in supersymmetric braneworld models .\nAbstract:\nWe study the gravitational field equations for static spherically symmetric configurations on Randall-Sundrum type II brane-worlds with bulk cosmological constant and matter fields localized on the branes. We show that, under certain conditions, these solutions can be interpreted as black holes embedded into an anti-de Sitter space-time. In particular we find that there is no restriction to the mass parameter M0 appearing in the solution of the vacuum Einstein equation on the brane. The corresponding horizon radius r0 satisfies the relation r0 = (3M0/4π)1/3. This result implies that the Schwarzschild-de Sitter metric describes not only black hole but also naked singularity solutions. Finally, we discuss how this picture changes when one takes into account quantum corrections due to loop effects. PACS numbers: 04.20.-q; 11.10.Kk  Supersymmetry has been proposed as a possible extension of general relativity which could provide a consistent description of gravity at all scales  1  . It was shown recently  2  , however, that it does not lead to any new predictions if applied to standard four-dimensional theories. On the other hand, higher dimensional extensions of supergravity have attracted considerable attention during recent years  3  .\nIn this letter we consider five-dimensional supergravities  4  where the extra dimension is compactified on a circle  5  or orbifold  6  . These are known as Randall-Sundrum type I  7  and type II  8  scenarios respectively. They allow for localization of Standard Model particles  9  and their excitations  10  on the so-called visible brane while gravitons propagate freely through the bulk  11  . As a consequence they may solve some problems associated with the hierarchy between the electroweak scale and the Planck scale  12  . Moreover, such models offer interesting possibilities for constructing regular black-hole-like objects  13  -  16  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : On Newton s law in supersymmetric braneworld configurations . Abstract : We research the gravitational field equations for static spherically symmetric configurations on Randall - Sundrum type II brane - worlds with bulk cosmological constant and material fields confined on the branes .We see that , under certain conditions , these solutions can be interpreted as black holes inserted into an anti - de Sitter space - time . In particular we find that there is no limitation to the mass vector M0 appearing in the solution of the vacuum Einstein equation on the brane .The equivalent horizon radius r0 satisfies the relation r0 = ( 3M0 / 4π ) 1 / 3 . This result suggests that the Schwarzschild - de Sitter metric encompasses not only white hole but also naked singularity solutions .Finally , we explain how this picture changes when one takes into consideration quantum corrections due to loop interactions . PACS numbers : 04 . 20 . - q ; 11 . 10 . Kk Supersymmetry has been proposed as a possible extension of general relativity which could give a consistent description of gravitational at all scales 1 .It was shown ago 2 , however , that it does not result to any new predictions if applied to standard four - dimensional theories . On the other hand , greater dimensional extensions of supergravity have garnered considerable scrutiny during recent years 3 .In this letter we define five - dimensional supergravities 4 where the extra dimension is compactified on a ring 5 or orbifold 6 . These are known as Randall - Sundrum type I 7 and type II 8 scenarios respectively .They allow for localization of Standard Model particles 9 and their excitations 10 on the so - called visible brane while gravitons propagate continuously through the bulk 11 . As a consequence they may solve some problems related with the hierarchy between the electroweak scale and the Planck scale 12 .Moreover , such theories provided important possibilities for constructing ordinary black - hole - like bodies 13 - 16 .",
        "rewrite_text": "**Title:** On Newton's Law in Supersymmetric Braneworld Configurations\n\n**Abstract:** This study investigates the gravitational field equations governing static, spherically symmetric configurations within Randall-Sundrum type II braneworlds, incorporating a bulk cosmological constant and material fields confined to the branes. We demonstrate that, under specific conditions, these solutions can be interpreted as black holes situated within an anti-de Sitter spacetime framework. Notably, we establish that there are no restrictions on the mass vector \\( M_0 \\) present in the vacuum Einstein equation solutions on the brane. The corresponding horizon radius \\( r_0 \\) is found to satisfy the relationship \\( r_0 = (3M_0 / 4\\pi)^{1/3} \\). This finding implies that the Schwarzschild-de Sitter metric encompasses not only white hole solutions but also naked singularities. Furthermore, we discuss how the inclusion of quantum corrections arising from loop interactions alters this scenario. \n\nSupersymmetry has been proposed as a potential extension of general relativity, aiming to provide a coherent description of gravitational phenomena across all scales. However, previous studies have indicated that applying supersymmetry to conventional four-dimensional theories does not yield new predictions. In contrast, higher-dimensional extensions of supergravity have attracted significant attention in recent years. In this paper, we define five-dimensional supergravities where the additional dimension is compactified on either a ring or an orbifold, corresponding to Randall-Sundrum type I and type II scenarios, respectively. These models facilitate the localization of Standard Model particles and their excitations on a \"visible\" brane, while gravitons can propagate freely through the bulk. This framework offers potential solutions to issues concerning the hierarchy between the electroweak and Planck scales. Additionally, these theories present valuable avenues for constructing entities resembling ordinary black holes.",
        "ori-fast-z-score": -0.43685202833051895,
        "water-fast-z-score": 5.874305293953249,
        "rewrite-fast-z-score": 1.8782971010998233
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Kinematic Decoupling of Globular Clusters with Extended Horizontal-Branch .\nAbstract:\nWe present the results of our kinematical study of globular clusters in M31, based on high-resolution spectroscopy obtained at the VLT and Keck telescopes. We find that all clusters studied show evidence for rotation around their major axes (with typical velocities of 100-200 km/s), while only two out of eight objects have significant internal velocity dispersions (of about 50-100 km/s). The remaining six clusters are consistent with being completely dispersionless systems. This is surprising given that these clusters contain large numbers of evolved stars belonging to extended horizontal branches. Our analysis shows that this apparent contradiction can be explained by assuming that most of the cluster mass resides outside the observed field-of-view. In addition we find that the majority of the clusters rotate counterclockwise when viewed along their minor axes. These findings suggest that many globular clusters may not be fully relaxed dynamical systems as previously thought. They also provide new insights into the formation history of globular clusters. \n \n Keywords: Kinematics",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Kinematic Decoupling of Globular Clusters with Extended Horizontal - Branch . Abstract : We present the conclusion of our kinematical study of globular galaxies in M31 , using on high - resolution spectroscopy acquired at the VLT and Keck telescopes .We see that all galaxies studied show proof for rotation around their major axes ( with typical velocities of 100 - 200 kilometers / s ) , while only two out of eight objects have considerable internal velocity dispersions ( of about 50 - 100 km / s ) . The remaining six galaxies are consistent with being totally dispersionless structures .This is surprising given that these clusters include significant amounts of evolved stellar belonging to expanding horizontal branches . Our study shows that this evident contradiction can be described by assuming that most of the cluster mass resides outside the seen field - of - view .In addition we find that the majority of the clusters tilt counterclockwise when seen along their minor axes . These conclusions show that much globular complexes may not be truly relaxed dynamical systems as previously thought .They addition offer additional perspectives into the formation history of globular complexes . Keywords : Kinematics",
        "rewrite_text": "Title: Kinematic Decoupling of Globular Clusters with Extended Horizontal Branch\n\nAbstract: In this study, we present the findings from our kinematic analysis of globular clusters in the Andromeda galaxy (M31), utilizing high-resolution spectroscopy obtained from the Very Large Telescope (VLT) and Keck Observatory. Our investigation reveals that all examined globular clusters exhibit evidence of rotation around their principal axes, with typical rotational velocities ranging from 100 to 200 kilometers per second. However, we observed that only two out of the eight clusters analyzed display significant internal velocity dispersions, approximately between 50 to 100 kilometers per second. The remaining six clusters appear to be entirely dispersionless, a result that is unexpected considering the presence of substantial amounts of evolved stars associated with expanding horizontal branches within these clusters. \n\nThis apparent contradiction can be reconciled by positing that a majority of the cluster mass is located beyond the observed field of view. Furthermore, our analysis indicates that most of the clusters exhibit a counterclockwise tilt when viewed along their minor axes. These findings challenge the prevailing notion that globular clusters are fully relaxed dynamical systems, suggesting instead that they may possess more complex dynamical characteristics than previously understood. Additionally, our results provide new insights into the formation and evolutionary history of globular clusters, highlighting the need for a reevaluation of their dynamical states. This research contributes to a deeper understanding of the kinematics of globular clusters and their role in the broader context of galactic evolution. \n\nKeywords: Kinematics, Globular Clusters, Andromeda Galaxy, Spectroscopy, Stellar Dynamics.",
        "ori-fast-z-score": -2.182178902359924,
        "water-fast-z-score": 5.314796216557077,
        "rewrite-fast-z-score": -0.3922322702763681
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Considering the Case for Biodiversity Cycles: Reexamining the Evidence for Periodicity in the Fossil Record .\nAbstract:\nThe fossil record is replete with examples of periodic extinction events, but it has been unclear whether these are driven by external factors or internal dynamics within ecosystems. Here we show that biodiversity cycles can be generated solely through interactions between species and their environment without any need to invoke additional mechanisms such as mass extinctions. We use an agent-based model to simulate how communities evolve over time under different environmental conditions. Our results suggest that biodiversity cycles may have played an important role in shaping Earth s biosphere throughout its history. The fossil record contains numerous examples of periodic extinction events (1), which have led some researchers to propose that there must exist underlying periodicity in ecosystem processes (2). However, it remains unknown what causes this apparent regularity in the fossil record; one possibility is that periods of high diversity alternate with intervals during which many species go extinct simultaneously (3) (4) . In addition, it is not clear if all observed patterns of biodiversity cycling represent true cyclical behavior or simply reflect stochastic variation around a mean value (5-7).\nHere we present evidence suggesting that biodiversity cycles can arise spontaneously from ecological interactions alone, without requiring any additional mechanism like mass extinctions. To test our hypothesis, we used an agent-based model to explore how communities evolve over time when subjected to varying levels of environmental stress. This approach allowed us to examine how changes in community composition affect population abundances across multiple trophic levels. By simulating thousands of replicate runs using different parameter values, we were able to identify robust statistical signatures associated with biodiversity cycles.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Considering the Case for Biodiversity Cycles : Reexamining the Evidence for Periodicity in the Fossil Record . Abstract : The fossil history is replete with examples of periodic mortality events , but it has been uncertain whether these are driven by external influences or internal mechanisms within ecosystems .Here we study that biodiversity cycles can be formed solely through relationships between species and their environment without any necessity to invoke additional mechanisms such as mass extinctions . We use an agent - based model to simulate how communities evolve over time under various environmental conditions .Our results show that ecosystem cycles might have played an important role in shaping Earth s biosphere throughout its past . The fossil history provides various instances of periodic mortality events ( 1 ) , which have led some researchers to propose that there need possess underlying periodicity in ecological processes ( 2 ) .However , it remains obscure what causes this evident regularity in the fossil history ; one suggestion is that intervals of high diversity alternate with periods during which several species go extinct simultaneously ( 3 ) ( 4 ) . In addition , it is not clear if all observed patterns of biodiversity cycling constitute genuine cyclical behavior or simply reflect stochastic differences around a mean value ( 5 - 7 ) .Here we present evidence indicating that ecosystem cycles can arise spontaneously from ecological relationships alone , without requiring any additional process like mass extinctions . To study our theory , we using an agent - based model to examine how communities evolve over time when exposed to different amounts of environmental stress .This method allowed us to examine how variations in population composition impact population abundances across multiple trophic levels . By simulating thousands of replicate runs using varying parameter values , we were could to identify reliable statistical signatures identified with biodiversity cycles .",
        "rewrite_text": "**Title:** Reevaluating Biodiversity Cycles: Analyzing Periodicity in the Fossil Record\n\n**Abstract:** The fossil record is abundant with instances of periodic mortality events, yet the underlying causes of these patterns remain a topic of debate among researchers. This study investigates the hypothesis that biodiversity cycles can emerge solely from the interactions between species and their environments, negating the need for external factors such as mass extinctions. Utilizing an agent-based model, we simulate the evolution of ecological communities under varying environmental conditions to explore the dynamics of biodiversity over time. Our findings suggest that ecosystem cycles have significantly influenced the development of Earth's biosphere throughout its history. \n\nWhile the fossil record presents numerous examples of periodic mortality events, the question of whether these events indicate inherent periodicity in ecological processes is still unresolved. Some researchers propose that high-diversity intervals are followed by phases of simultaneous species extinction, contributing to the observed regularity in the fossil record. However, it remains unclear whether the patterns of biodiversity cycling are genuine cycles or merely stochastic variations around a mean. \n\nIn this paper, we provide evidence that ecosystem cycles can spontaneously arise from ecological interactions, independent of mass extinction events. To validate our theory, we employed an agent-based modeling approach to analyze community evolution under different levels of environmental stress. This methodology enabled us to investigate how shifts in population composition affect abundance across various trophic levels. By conducting thousands of simulation runs with diverse parameter settings, we identified consistent statistical signatures associated with biodiversity cycles. Our results contribute to a deeper understanding of the mechanisms driving biodiversity and the potential for intrinsic ecological processes to shape the patterns observed in the fossil record.",
        "ori-fast-z-score": 0.242535625036333,
        "water-fast-z-score": 9.177999171377655,
        "rewrite-fast-z-score": 0.9669875568304563
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Vortex Molecular Crystal and Vortex Plastic Crystal States in Honeycomb and Kagome Pinning Arrays .\nAbstract:\nWe study vortex matter in honeycomb (HC) and kagome (KC) pinning arrays by using the time-dependent Ginzburg-Landau equation with periodic boundary conditions, which is solved numerically on square lattices with sizes up to L = 256. We find that HC and KC have different effects on vortex dynamics depending on their density n. For low densities, vortices are pinned at individual defects for both HC and KC; however, they form ordered structures only in HC but not in KC. At high densities, we observe two distinct phases in HC; one is a vortex molecular crystal state where all vortices occupy single sites, while another phase has a vortex plastic crystal structure where some vortices remain unpinned. In contrast, no such ordered states exist in KC even at very large defect densities. Our results suggest that the difference between HC and KC originates from the fact that the former can support more than one vortex per site whereas the latter cannot.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Vortex Molecular Crystal and Vortex Plastic Crystal States in Honeycomb and Kagome Pinning Arrays . Abstract : We explore vortex matter in honeycomb ( HC ) and kagome ( KC ) locking arrays by using the period - dependent Ginzburg - Landau equation with periodic boundary constraints , which is solution numerically on square lattices with sizes up to L = 256 .We see that HC and KC have different impacts on vortex dynamics depending on their density n . For low densities , vortices are pinned at individual defects for both HC and KC ; however , they create ordered forms only in HC but not in KC . At high densities , we study two different stages in HC ; one is a vortex molecular crystal state where all vortices occupy separate sites , while another phase has a vortex plastic crystal shape where some vortices remain unpinned .In comparison , no such ordered states arise in KC even at very huge defect densities . Our results show that the difference between HC and KC originates from the fact that the former can support more than one vortex per site whereas the former cannot .",
        "rewrite_text": "In this study, we investigate the behavior of vortex matter within honeycomb (HC) and kagome (KC) pinning arrays, utilizing a period-dependent Ginzburg-Landau equation under periodic boundary conditions. Our numerical simulations are conducted on square lattices with dimensions reaching up to L = 256. The findings reveal distinct influences of HC and KC on vortex dynamics, which vary with vortex density (n). At low densities, vortices become immobilized at individual defects in both HC and KC configurations; however, ordered arrangements are observed solely in the HC structure, while the KC fails to exhibit similar order. As we transition to high-density scenarios, we identify two distinct phases within the HC: the vortex molecular crystal state, characterized by each vortex occupying a unique site, and the vortex plastic crystal state, where a portion of the vortices remains unpinned. In stark contrast, the KC does not manifest any ordered states, even at significantly high defect densities. Our analysis indicates that the fundamental difference between HC and KC arises from the capacity of the HC to accommodate multiple vortices per site, a feature that is not present in the KC configuration. This research enhances our understanding of vortex dynamics in different lattice structures and highlights the critical role of lattice geometry in determining the behavior of vortex matter.",
        "ori-fast-z-score": -0.6108472217815261,
        "water-fast-z-score": 4.608176875690327,
        "rewrite-fast-z-score": -1.0314212462587933
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Fast recursive filters for simulating nonlinear dynamic systems .\nAbstract:\nWe present an efficient algorithm to simulate the dynamics of nonlinear systems with arbitrary initial conditions and parameters using fast recursive filters (FRFs). The FRF is based on a linear combination of basis functions, which are obtained by solving a set of ordinary differential equations that represent the model under consideration. We show how this approach can be used in conjunction with standard numerical integration schemes such as Euler s method or Runge-Kutta methods to efficiently compute trajectories over large time intervals. In particular we demonstrate our approach on two examples, namely the Lorenz attractor and the FitzHugh-Nagumo neuron model. \nI. INTRODUCTIO N\nThe simulation of complex dynamical systems often requires the solution of sets of coupled ordinary differential equations (ODEs)  1  . For example, many models describing physical phenomena involve ODEs  2  , while other applications include chemical reactions  3  , population growth  4  , epidemiology  5  , neuroscience  6  , climate modeling  7  , etc.. However, even if these problems have been studied extensively  8  -  10  , there still exist several challenges associated with their computational treatment  11  .\nIn general, it is not possible to solve analytically the ODE system representing the problem at hand  12  . Therefore, one has to resort to approximate solutions  13  . These approximations may be obtained either numerically  14  or symbolically  15  . Numerical approaches typically rely on discretizing the continuous-time domain into small segments  16  . This leads to a discrete representation of the original system  17  , where each segment corresponds to a single state variable  18  . Symbolic techniques instead use polynomial expansions  19  , rational expressions  20  , or splines  21  to obtain an approximation of the exact solution  22  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Fast recursive filters for simulating nonlinear dynamic systems . Abstract : We create an efficient algorithm to simulate the dynamics of nonlinear systems with specified initial conditions and parameters utilizing fast recursive filters ( FRFs ) .The FRF is based on a linear mixture of basis functions , which are derived by solving a setting of ordinary differential equations that represent the model under consideration . We see how this methodology can be used in conjunction with typical numerical integration schemes such as Euler s method or Runge - Kutta methods to easily compute trajectories over large time periods .In particular we prove our approach on two examples , notably the Lorenz attractor and the FitzHugh - Nagumo neuron description . I . INTRODUCTIO N The modelling of complex dynamical systems often needs the solve of sets of coupled ordinary differential equations ( ODEs ) 1 .For instance , many systems describing physical phenomena involve ODEs 2 , while other applications include chemical processes 3 , population development 4 , epidemiology 5 , neuroscience 6 , weather simulation 7 , etc . . However , even if these problems have been studied thoroughly 8 - 10 , there still arise many challenges associated with their mathematical treatment 11 .In general , it is not practical to solve analytically the ODE scheme representing the question at hand 12 . Therefore , one has to resort to approximate solutions 13 .These approximations might be obtained either numerically 14 or symbolically 15 . Numerical methods typically rely on discretizing the linear - time domain into small segments 16 .This leads to a discrete representation of the original system 17 , where each segment corresponds to a single state variable 18 . Symbolic methods instead include polynomial expansions 19 , rational expressions 20 , or splines 21 to obtain an approximation of the exact solution 22 .",
        "rewrite_text": "In this article, we present a novel algorithm designed to efficiently simulate the dynamics of nonlinear systems characterized by specific initial conditions and parameters through the use of fast recursive filters (FRFs). The FRF methodology is grounded in a linear combination of basis functions, which are derived from solving a set of ordinary differential equations (ODEs) that encapsulate the dynamics of the system in question. Our approach demonstrates how FRFs can be effectively integrated with conventional numerical integration techniques, such as Euler's method and Runge-Kutta methods, to facilitate the computation of system trajectories over extended time intervals.\n\nTo validate our methodology, we apply it to two well-known examples: the Lorenz attractor and the FitzHugh-Nagumo model, which describes neuronal dynamics. The modeling of complex dynamical systems frequently necessitates the resolution of coupled ODEs, as many physical phenomena, as well as applications in fields such as chemistry, population dynamics, epidemiology, neuroscience, and weather forecasting, are governed by such equations. Despite extensive research into these problems, significant challenges remain in their mathematical treatment.\n\nAnalytical solutions to the ODEs are often impractical, prompting the need for approximate solutions. These approximations can be achieved through numerical or symbolic methods. Numerical approaches typically involve discretizing the time domain into small intervals, resulting in a discrete representation of the original system, where each interval corresponds to a specific state variable. On the other hand, symbolic methods utilize techniques such as polynomial expansions, rational expressions, or splines to derive approximations of the exact solutions. Our work aims to bridge the gap between these methodologies by providing a robust framework for simulating nonlinear dynamic systems efficiently, thereby enhancing the understanding and analysis of complex systems across various scientific domains.",
        "ori-fast-z-score": -0.9467292624062575,
        "water-fast-z-score": 5.25004409152561,
        "rewrite-fast-z-score": -1.9650226127485502
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Origin of the early-type R stars: a binary-merger solution to a century-old problem? .\nAbstract:\nWe present an analysis of the observed properties of the rare, hot (T eff > 20 000 K), helium-rich subdwarf B (sdB) and extreme horizontal branch (EHB) stars in globular clusters that are known as red stragglers or blue hook stars.  We show how these objects can be explained by the merger of two helium white dwarfs with total mass exceeding the Chandrasekhar limit.   The resulting merged object is expected to have a radius larger than its Roche lobe at all stages during the merging process, which leads to stable mass transfer onto the companion star until it fills its Roche lobe and starts overflowing.   This results in a common envelope phase where most of the orbital energy released is used to eject the outer layers of both stars into space.   After this stage, the system consists of a naked helium core surrounded by a thin hydrogen layer on top of the helium core.   If the initial masses of the progenitor white dwarf components were close enough together such that their cores merge before they reach the Chandrasekhar mass, then we expect the final product to resemble a normal sdB star; if not, then the final product will be more massive than the Chandrasekhar",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Origin of the early - class R stars : a binary - merger solution to a decades - old problem ? .Abstract : We present an assessment of the observed properties of the rare , hot ( T eff > 20 000 K ) , helium - rich subdwarf B ( sdB ) and extreme horizontal branch ( EHB ) stars in globular complexes that are known as red stragglers or blue hook stars . We see how these objects can be described by the merger of two helium white dwarfs with total mass exceeding the Chandrasekhar limit .The resulting merged object is expected to have a diameter greater than its Roche lobe at all phases during the merging process , which results to stable mass transfer onto the companion star until it fills its Roche lobe and starts overflowing . This results in a common envelope phase where most of the orbital heat released is utilized to eject the exterior layers of both stars into space .After this phase , the system consists of a naked helium core flanked by a thin hydrogen layer on top of the helium core . If the first masses of the progenitor white dwarf components were close enough together such that their cores merge before they reach the Chandrasekhar mass , then we expect the finished product to resemble a normal sdB star ; if not , then the finished result will be more massive than the Chandrasekhar",
        "rewrite_text": "Title: The Origin of Early-Class R Stars: A Binary-Merger Solution to a Long-Standing Problem?\n\nAbstract: In this study, we explore the characteristics of the rare and hot helium-rich subdwarf B (sdB) stars and extreme horizontal branch (EHB) stars, often referred to as red stragglers or blue hook stars, found within globular clusters. We propose that these unique stellar objects can be effectively explained through the merger of two helium white dwarfs, with a combined mass surpassing the Chandrasekhar limit. This merger process leads to the formation of a new stellar entity that maintains a diameter larger than its Roche lobe throughout the merging stages. Consequently, this configuration facilitates stable mass transfer onto a companion star until the latter fills its Roche lobe and initiates overflow. This scenario triggers a common envelope phase, during which the majority of the orbital energy released is expended in expelling the outer layers of both stars into the surrounding space. Following this phase, the resulting system is characterized by a naked helium core, which is enveloped by a thin layer of hydrogen. If the initial masses of the progenitor white dwarfs are sufficiently similar, allowing their cores to merge prior to reaching the Chandrasekhar mass, the resultant star is anticipated to exhibit properties akin to a typical sdB star. Conversely, if the masses differ significantly, the outcome will yield a star with a mass exceeding the Chandrasekhar limit. This investigation sheds light on the formation mechanisms of early-class R stars and offers a potential resolution to a decades-old question in stellar astrophysics.",
        "ori-fast-z-score": 1.6590301240125065,
        "water-fast-z-score": 5.050686484257773,
        "rewrite-fast-z-score": 1.4855627054164149
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Halo-model signatures from 380,000 SDSS Luminous Red Galaxies with photometric redshifts .\nAbstract:\nWe present the results of an analysis of the clustering properties of luminous red galaxies (LRGs) in the Sloan Digital Sky Survey Data Release 7 (SDSS DR7). We use a sample of 380,000 LRGs selected to have 0.4 < zphot < 1.0 and Mr < --21.5 + 5logh. The angular correlation function is measured for this sample using the Landy & Szalay estimator on scales between 10  and 100 . To account for redshift space distortions we measure the projected cross-correlation functions wp(rp), where rp = Dproj/ H(z)/H0 , H(z) is the Hubble parameter at redshift z, and H0 is its value today. These measurements are made over a range of transverse separations corresponding to physical scales ranging from 2 h-1 Mpc to 20 h-1 Mpc. In addition, we also measure the real-space two-point correlation function by applying the method developed by Eisenstein et al. (2007) . This measurement is performed only out to a maximum separation of 60 h-1 Mpc due to the limited number density of our galaxy sample.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Halo - model signatures from 380 , 000 SDSS Luminous Red Galaxies with photometric redshifts . Abstract : We release the conclusion of an assessment of the clustering behavior of luminous red clusters ( LRGs ) in the Sloan Digital Sky Survey Data Release 7 ( SDSS DR7 ) .We use a sample of 380 , 000 LRGs chosen to have 0 . 4 < zphot < 1 . 0 and Mr < - - 21 . 5 + 5logh . The angular correlation function is measured for this sample using the Landy & Szalay estimator on scales between 10 and 100 .To account for redshift space distortions we measure the projected cross - correlation functions wp ( rp ) , where rp = Dproj / H ( z ) / H0 , H ( z ) is the Hubble parameter at redshift k , and H0 is its value today . These measurements are achieved over a range of transverse separations corresponding to physical scales ranging from 2 h - 1 Mpc to 20 h - 1 Mpc .In addition , we also measure the real - space two - point correlation function by using the method developed by Eisenstein et al . ( 2007 ) .This measurement is conducted only out to a maximum separation of 60 h - 1 Mpc owing to the limited number density of our galaxy sample .",
        "rewrite_text": "We present the findings of a comprehensive analysis of the clustering properties of luminous red galaxies (LRGs) utilizing data from the Sloan Digital Sky Survey Data Release 7 (SDSS DR7). Our study focuses on a substantial sample of 380,000 LRGs, selected based on specific criteria: photometric redshifts in the range of 0.4 < zphot < 1.0 and absolute magnitudes of Mr < -21.5 + 5 log h. To investigate the angular clustering of these galaxies, we employ the Landy & Szalay estimator, measuring the angular correlation function across scales from 10 to 100 arcminutes. \n\nTo address the effects of redshift space distortions, we compute the projected cross-correlation functions, denoted as wp(rp), where rp is defined as Dproj / H(z) / H0. Here, H(z) represents the Hubble parameter at the specified redshift, while H0 is its present-day value. Our analysis spans a range of transverse separations, translating to physical scales between 2 h^-1 Mpc and 20 h^-1 Mpc. Furthermore, we also determine the real-space two-point correlation function using the methodology established by Eisenstein et al. (2007). This aspect of our measurement is constrained to a maximum separation of 60 h^-1 Mpc due to the limited number density of our galaxy sample.\n\nThe results of this study provide valuable insights into the large-scale structure of the universe as inferred from the clustering behavior of LRGs. By examining these correlations, we aim to enhance our understanding of galaxy formation and evolution, as well as the underlying cosmological parameters that govern the distribution of matter in the universe. Our findings contribute to the ongoing discourse in astrophysics regarding the nature of dark energy and the expansion of the cosmos, highlighting the significance of LRGs as a critical component in cosmological studies.",
        "ori-fast-z-score": -0.35603449745815596,
        "water-fast-z-score": 2.5927248643506746,
        "rewrite-fast-z-score": -0.2750095491084634
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Generalized CRF-structures .\nAbstract:\nWe introduce the concept of generalized conditional random fields (GCRFs) and show how they can be used to model arbitrary probability distributions over structured data sets, such as sequences or trees.  We present an efficient algorithm for learning GCRF parameters using gradient descent on the log-likelihood objective function.   Finally we demonstrate that our approach is able to learn accurate models for several challenging sequence labeling tasks including part-of-speech tagging in natural language processing and protein secondary structure prediction in bioinformatics. Conditional Random Fields (CRFs) (Lafferty et al., 2001 ) are undirected graphical models which have been successfully applied to many problems involving sequential data, e.g. (Sha & Pereira, 2003) . In this work, we propose Generalized Conditional Random Fields (GCRFs), a generalization of CRFs which allows us to represent any distribution over structured data sets like sequences or trees. The key idea behind GCRFs is to use a set of latent variables to capture dependencies between different parts of the input space. This enables us to efficiently compute the partition function required by standard CRFs with dynamic programming techniques. Furthermore, it also makes it possible to train GCRFs using gradient-based methods similar to those employed for Maximum Entropy Markov Models (MEMMs). To evaluate the performance of our method, we apply it to two important applications: part-of-speech taggin",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Generalized CRF - structures . Abstract : We introduce the idea of generalized conditional random fields ( GCRFs ) and explain how they can be used to model arbitrary likelihood distributions over structured data sets , such as sequences or trees .We present an efficient algorithm for learning GCRF variables using gradient descent on the log - likelihood objective function . Finally we prove that our approach is ability to teach exact models for numerous challenging gene labeling challenges including whole - of - voice tagging in natural language processing and gene secondary structure prediction in bioinformatics .Conditional Random Fields ( CRFs ) ( Lafferty et al . , 2001 ) are undirected graphical descriptions which have been successfully applied to many difficulties involving sequential data , e . g . ( Sha & Pereira , 2003 ) .In this research , we develop Generalized Conditional Random Fields ( GCRFs ) , a generalization of CRFs which allows us to model any distribution over structured data sets like sequences or trees . The main idea behind GCRFs is to use a setting of latent variables to capture dependencies between various parts of the input space .This enables us to easily compute the partition function required by traditional CRFs with dynamic programming tools . Furthermore , it also makes it able to train GCRFs using gradient - based methods similar to those utilized for Maximum Entropy Markov Models ( MEMMs ) .To assess the performance of our technique , we apply it to two essential users : part - of - voice taggin",
        "rewrite_text": "We present the concept of Generalized Conditional Random Fields (GCRFs) as an advanced framework for modeling complex likelihood distributions over structured datasets, including sequences and trees. This work builds upon the foundation laid by Conditional Random Fields (CRFs), which have proven effective in addressing various sequential data challenges. Our proposed GCRFs extend the capabilities of traditional CRFs by incorporating latent variables that facilitate the modeling of intricate dependencies within the input space. This innovative approach allows for the efficient computation of the partition function, a critical component in CRF applications, through the use of dynamic programming techniques.\n\nTo optimize the learning of GCRF parameters, we introduce a robust algorithm that employs gradient descent on the log-likelihood objective function. This method not only enhances the efficiency of the learning process but also aligns with the gradient-based training strategies commonly used in Maximum Entropy Markov Models (MEMMs). We validate the effectiveness of our GCRF framework by applying it to two significant tasks: part-of-speech tagging in natural language processing and gene secondary structure prediction in bioinformatics. Our results demonstrate that GCRFs can accurately model complex relationships in these challenging domains, providing precise solutions for gene labeling tasks and improving the performance of tagging systems.\n\nIn summary, this research contributes to the field by establishing GCRFs as a versatile tool for structured data modeling, showcasing their potential to tackle various applications that require a nuanced understanding of dependencies within data. The findings underscore the importance of GCRFs in advancing the capabilities of machine learning techniques in both natural language processing and bioinformatics, paving the way for future research and applications in these areas.",
        "ori-fast-z-score": -0.647150228929434,
        "water-fast-z-score": 6.127946159842712,
        "rewrite-fast-z-score": 1.1358152736593492
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Satellite Galaxies and Fossil Groups in the Millennium Simulation .\nAbstract:\nWe present an analysis of satellite galaxies, fossil groups (FGs), and galaxy clusters in the Millennium Run simulation. We find that FGs are rare objects with only 0.5% of all halos being classified as such at z=0. The number density of FGs is consistent with observations for systems with Mvir>1013M⊙/h. However, we also find that there exists a population of low-mass FGs which have not been observed yet but may be detectable by future surveys. In addition to their rarity, FGs show several other interesting properties compared to normal galaxy clusters: they tend to reside in more massive dark matter haloes than normal galaxy clusters; they contain fewer bright central galaxies; and they exhibit higher velocity dispersions. These results suggest that FGs can provide important constraints on models of galaxy formation and evolution. This work was supported by NASA grant NAG5-10842.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Satellite Galaxies and Fossil Groups in the Millennium Simulation . Abstract : We present an assessment of satellite galaxies , fossil bands ( FGs ) , and galaxy regions in the Millennium Run simulation .We see that FGs are scarce objects with only 0 . 5 % of all halos being classified as such at z = 0 . The number density of FGs is compatible with observations for systems with Mvir > [UNK] / h .However , we also find that there exists a population of low - weight FGs which have not been observed yet but might be detectable by future surveys . In addition to their rarity , FGs exhibit several other remarkable properties compared to normal star clusters : they tend to live in more massive bright matter haloes than regular galaxy clusters ; they contain fewer bright central clusters ; and they show higher velocity dispersions .These data suggest that FGs can provide important restrictions on estimates of galaxy formation and evolution . This research was supported by NASA grant NAG5 - 10842 .",
        "rewrite_text": "We provide a comprehensive evaluation of satellite galaxies, fossil groups (FGs), and galaxy environments as modeled in the Millennium Run simulation. Our findings indicate that FGs are relatively rare, constituting only 0.5% of all halos at redshift z = 0. The observed number density of FGs aligns with existing observational data for systems with virial masses greater than a certain threshold. Notably, we identify a subset of low-mass FGs that have yet to be detected but may be observable in future astronomical surveys. Beyond their scarcity, FGs exhibit several distinctive characteristics when compared to typical star clusters: they are predominantly found within more massive dark matter halos than standard galaxy clusters, possess fewer luminous central galaxies, and demonstrate elevated velocity dispersions. These unique attributes imply that FGs could serve as significant constraints for models of galaxy formation and evolution. This research was conducted with the support of NASA grant NAG5-10842.",
        "ori-fast-z-score": -0.6974858324629157,
        "water-fast-z-score": 5.047146145152358,
        "rewrite-fast-z-score": -0.11704114719613057
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  An Atlas of the circumnuclear regions of 75 Seyfert galaxies in the near-UV with HST Advanced Camera for Surveys .\nAbstract:\nWe present an atlas of the circumnuclear region (CNR) of 75 nearby active galactic nuclei observed by Hubble Space Telescope s Advanced Camera for Surveying program, which is part of the Nearby Galaxies Survey Treasury project. The CNRs are defined as the brightest central 2 kpc diameter circular aperture centered on each galaxy nucleus and were imaged using the F330W filter to select emission lines at wavelengths longer than 3000 Å . We use these data to study the properties of the nuclear starbursts that power the AGNs through their effects on the surrounding interstellar medium. In addition we examine how the physical conditions within the CNRs vary among different types of AGN activity.  We find that the majority of our sample have significant UV excesses over what would be expected based solely on stellar photospheric emission. This excess can be explained either by hot young stars or by dust extinction. For those objects where both optical spectroscopy and infrared imaging exist, we show that the UV excess is due primarily to dust extinction rather than hot young stars.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : An Atlas of the circumnuclear regions of 75 Seyfert galaxies in the near - UV with HST Advanced Camera for Surveys . Abstract : We report an atlas of the circumnuclear zone ( CNR ) of 75 nearby active galactic nuclei seen by Hubble Space Telescope s Advanced Camera for Surveying project , which is part of the Nearby Galaxies Survey Treasury project .The CNRs are specified as the brightest central 2 kpc diameter circular aperture located on each galaxy nucleus and were imaged using the F330W filter to select emission lines at wavelengths greater than 3000 Å . We use these information to study the properties of the nuclear starbursts that fuel the AGNs through their impact on the nearby interstellar material .In addition we investigate how the physical conditions within the CNRs vary among different kinds of AGN activity . We see that the majority of our sample have considerable UV excesses over what would be anticipated based primarily on stellar photospheric emission .This excess can be described either by hot young stars or by dust extinction . For those objects where both optical spectroscopy and infrared imaging exist , we find that the UV excess is due primarily to dust extinction rather than hot young stars .",
        "rewrite_text": "We present an extensive atlas detailing the circumnuclear regions (CNRs) of 75 nearby Seyfert galaxies, captured through the Hubble Space Telescope's Advanced Camera for Surveys as part of the Nearby Galaxies Survey Treasury project. The CNRs are defined as the brightest circular apertures with a diameter of 2 kpc centered on the nuclei of each galaxy. Imaging was conducted using the F330W filter, which effectively isolates emission lines at wavelengths exceeding 3000 Å. This dataset enables us to explore the characteristics of nuclear starbursts that energize active galactic nuclei (AGNs) and their influence on the surrounding interstellar medium. \n\nFurthermore, we analyze the variations in physical conditions within the CNRs across different types of AGN activity. Our findings reveal that a significant portion of the sample exhibits substantial ultraviolet (UV) excesses that surpass expectations based on stellar photospheric emissions alone. This UV excess can be attributed to either the presence of hot young stars or the effects of dust extinction. In cases where both optical spectroscopy and infrared imaging data are available, our analysis indicates that the UV excess is predominantly a result of dust extinction rather than the contribution from hot young stars. This research not only enhances our understanding of the interplay between star formation and AGN activity but also provides valuable insights into the environmental conditions surrounding these dynamic cosmic phenomena.",
        "ori-fast-z-score": -0.7592566023652966,
        "water-fast-z-score": 3.6663142889169062,
        "rewrite-fast-z-score": 0.10369516947304253
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Role of the rho meson in the description of pion electroproduction experiments at JLab .\nAbstract:\nWe present results on the role played by the rho meson in describing pion electroproduction data obtained with the CLAS detector at Jefferson Lab (JLab). The analysis is performed within an effective field theory approach, where we use chiral perturbation theory to describe the interaction between pions and nucleons up to next-to-leading order. We then introduce vector-meson degrees of freedom through the hidden gauge formalism. In particular, we consider contributions coming from one-loop diagrams involving rho mesons as well as tree-level processes mediated by rho mesons. Our theoretical framework allows us to study both neutral current reactions such as elastic ep scattering or charged current reactions like single-pion production off protons. Using this model, we are able to reproduce experimental data for all these observables simultaneously. Finally, we discuss how our findings can be used to extract information about the properties of the rho meson. This work was supported by the U.S. \n\n\nDepartment of Energy under Contract No. DE-SC0012704.\n\nPACS numbers: 12.38.Mh",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Role of the rho meson in the description of pion electroproduction research at JLab . Abstract : We report findings on the part played by the rho meson in understanding pion electroproduction data received with the CLAS detector at Jefferson Lab ( JLab ) .The investigation is conducted within an efficient field theory approach , where we using chiral perturbation theory to explain the interaction between pions and nucleons up to next - to - leading order . We then introduce vector - meson degrees of liberty through the hidden gauge formalism .In particular , we treat contributions come from one - loop diagrams featuring rho mesons as well as tree - level processes induced by rho mesons . Our conceptual formulation enables us to study both neutral current reactions such as elastic ep scattering or charged current reactions like single - pion production off protons .Using this model , we are able to publish empirical data for all these observables simultaneously . Finally , we explain how our findings can be used to extract information about the properties of the rho meson .This project was supported by the U . S . Department of Energy under Contract No . DE - SC0012704 .PACS numbers : 12 . 38 . Mh",
        "rewrite_text": "In this article, we present our research on the role of the rho meson in the context of pion electroproduction, utilizing data collected by the CLAS detector at Jefferson Lab (JLab). Our study employs an effective field theory framework, specifically leveraging chiral perturbation theory to analyze the interactions between pions and nucleons up to next-to-leading order. To enhance our understanding, we incorporate vector-meson degrees of freedom through the hidden gauge formalism. This approach allows us to systematically account for contributions from one-loop diagrams involving rho mesons, as well as tree-level processes that are induced by these mesons.\n\nOur theoretical framework is versatile, enabling us to investigate both neutral current reactions, such as elastic electron-proton scattering, and charged current reactions, including single-pion production from protons. By applying this model, we successfully correlate empirical data across various observables, providing a comprehensive analysis of the electroproduction processes.\n\nFurthermore, we discuss how our findings can be utilized to extract valuable information regarding the properties of the rho meson, which is crucial for advancing our understanding of strong interactions in particle physics. This research contributes to the broader field of hadronic physics and enhances our knowledge of meson dynamics in electroproduction scenarios.\n\nThe project received support from the U.S. Department of Energy under Contract No. DE-SC0012704, and our work is classified under PACS number 12.38.Mh. Through this study, we aim to shed light on the intricate role of the rho meson in pion electroproduction, thereby enriching the theoretical landscape of particle interactions.",
        "ori-fast-z-score": -1.3587324409735149,
        "water-fast-z-score": 3.9378076532410837,
        "rewrite-fast-z-score": -0.47891314261057566
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Suzaku Observations of Active Galactic Nuclei Detected in the Swift/BAT Survey: Discovery of  New Type  of Buried Supermassive Black Holes .\nAbstract:\nWe report on Suzaku observations for four active galactic nuclei (AGNs) detected by Swift/BAT survey, which are classified as  obscured AGNs  with column density larger than 10 24 cm-2 . We found that all these sources show strong Fe K emission lines and their line widths are broader than those expected from thermal broadening at kT = 100 keV. The observed line profiles can be reproduced well by relativistic disk reflection models including Compton scattering effects. These results suggest that there is an additional component to the X-ray continuum other than the standard thin accretion disks around supermassive black holes. In addition, we find that the iron abundance relative to solar value is higher than 1.5 times in three out of four objects. This suggests that the central engines of these obscured AGNs may have been buried under heavy dusty torii. Finally, we discuss possible origins of this new type of buried supermassive black holes based on our observational results.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Suzaku Observations of Active Galactic Nuclei Detected in the Swift / BAT Survey : Discovery of New Type of Buried Supermassive Black Holes . Abstract : We report on Suzaku measurements for four active galactic nuclei ( AGNs ) detected by Swift / BAT search , which are classified as obscured AGNs with column velocity larger than 10 24 mm - 2 .We showed that all these sources show intense Fe K emission lines and their line widths are larger than those expected from thermal broadening at kT = 100 keV . The observed line profiles can be reproduced well by relativistic disk absorption theories including Compton absorption effects .These data suggest that there is an additional element to the X - ray continuum other than the standard narrow accretion disks around supermassive black holes . In addition , we find that the metal density relative to solar value is higher than 1 . 5 times in three out of four bodies .This implies that the main engines of these obscured AGNs might have been trapped under heavy dusty torii . Finally , we investigate possible origins of this new kind of hidden supermassive black holes using on our observational results .",
        "rewrite_text": "In this study, we present findings from Suzaku observations of four active galactic nuclei (AGNs) identified in the Swift/BAT survey, which have been categorized as obscured AGNs exhibiting column densities exceeding 10^24 cm^-2. Our analysis reveals that all four sources exhibit prominent Fe K emission lines, with line widths that surpass those anticipated from thermal broadening at a temperature of kT = 100 keV. The observed line profiles align closely with predictions from relativistic disk absorption models, which incorporate Compton absorption effects. These results indicate the presence of additional components in the X-ray continuum beyond the conventional narrow accretion disks typically associated with supermassive black holes. Furthermore, we observe that the metallicity in three of the four AGNs is greater than 1.5 times the solar value, suggesting that the central engines of these obscured AGNs may be concealed beneath substantial dusty tori. This discovery prompts an exploration of the potential origins of this newly identified class of buried supermassive black holes, based on our observational data. Our findings contribute to a deeper understanding of the complex environments surrounding AGNs and highlight the need for further investigation into the mechanisms that govern their obscured nature. This research not only enhances our knowledge of AGN characteristics but also raises intriguing questions about the evolution and formation of supermassive black holes in the universe.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 6.111111111111111,
        "rewrite-fast-z-score": 1.8905706613989794
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Are constant loop widths an artifact of the background and the spatial resolution? .\nAbstract:\nWe present new results on the distribution of loop widths in active regions observed by TRACE at 171 Å, using data obtained during solar maximum (May-June 2001). We find that loops with different temperatures have similar distributions of widths, which are well fitted by log-normal functions. The mean values of these distributions increase with temperature as expected for pressure equilibrium between plasma confined within magnetic structures and their surroundings. However, we also find that there is no significant difference between the widths measured along individual loops and those determined from averaged profiles over entire active regions. This suggests that the apparent constancy of loop widths may be due to averaging effects rather than being intrinsic properties of coronal structures. In addition, we show that the widths derived from observations made under different viewing angles do not depend significantly on the position angle of the line-of-sight relative to the direction perpendicular to the local magnetic field vector.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Are constant loop widths an artifact of the background and the spatial resolution ? .Abstract : We report new data on the distribution of loop widths in active regions observed by TRACE at 171 Å , using data acquired during solar maximum ( May - June 2001 ) . We see that loops with varying temperatures have similar distributions of widths , which are better fitted by log - normal functions .The mean estimates of these distributions increase with temperature as anticipated for pressure equilibrium between plasma confined within magnetic structures and their environment . However , we also find that there is no considerable difference between the widths calculated along individual loops and those estimated from averaged profiles over whole active regions .This implies that the apparent constancy of loop widths might be due to averaging influences rather than being intrinsic characteristics of coronal structures . In addition , we prove that the widths generated from measurements made under distinct observation angles do not depend greatly on the orientation angle of the line - of - view relative to the direction perpendicular to the local magnetic force vector .",
        "rewrite_text": "**Title:** Are Constant Loop Widths an Artifact of the Background and Spatial Resolution?\n\n**Abstract:** In this study, we present new findings regarding the distribution of loop widths in active solar regions, utilizing data from the Transition Region and Coronal Explorer (TRACE) at a wavelength of 171 Å, specifically collected during the solar maximum period of May to June 2001. Our analysis reveals that loops exhibiting a range of temperatures display similar width distributions, which are more accurately represented by log-normal functions. As expected, the mean values of these distributions show an increase with temperature, consistent with the principles of pressure equilibrium between the plasma contained within magnetic structures and the surrounding environment. Notably, our results indicate that there is no significant difference between the widths measured along individual loops and those derived from averaged profiles across entire active regions. This finding suggests that the perceived uniformity of loop widths may be an artifact resulting from averaging effects rather than a fundamental property of coronal structures. Furthermore, we demonstrate that the widths obtained from observations taken at various angles do not exhibit substantial dependence on the orientation of the line of sight in relation to the direction perpendicular to the local magnetic force vector. This insight raises important questions about the interpretation of loop widths in solar physics and emphasizes the need for careful consideration of observational parameters when analyzing coronal structures. Overall, our research contributes to a deeper understanding of the characteristics of solar loops and their implications for the dynamics of the solar atmosphere.",
        "ori-fast-z-score": -0.5076730825668095,
        "water-fast-z-score": 4.6467017049401695,
        "rewrite-fast-z-score": 0.1796053020267749
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  In-orbit focal adjustment of the AKARI telescope with IRC images .\nAbstract:\nWe have performed in-orbit focal adjustment (IFA) for the infrared camera onboard AKARI satellite using its own data taken in orbit. The IFA was carried out by comparing the observed point spread function (PSF) and that simulated based on ray tracing analysis, which is one of the most accurate methods to determine the best focus position. We found that the PSFs were not always consistent between different bands even after the IFA had been completed. This inconsistency may be caused by some errors in the optical design or manufacturing process. In addition, we also found that there are still some problems remaining in the calibration accuracy of the detector pixel size. These results will help us improve our understanding about the performance of the instrument as well as provide useful information for future space missions. Keywords: Space mission, Focal adjustment, Point spread function, Ray tracing analysis, Infrared astronomy, Infrared camera",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : In - orbit focal adjustment of the AKARI telescope with IRC data . Abstract : We have done in - orbit lens adjustment ( IFA ) for the infrared camera onboard AKARI station use its own observations made in space .The IFA was carried out by comparing the seen point spread constant ( PSF ) and that simulated based on ray tracing examination , which is one of the most accurate ways to identify the best focus position . We determined that the PSFs were not always compatible between various bands long after the IFA had been completed .This inconsistency may be caused by some defects in the optical design or manufacturing system . In addition , we also discovered that there are still some problems remaining in the calibration reliability of the sensor pixel size .These data will assist us improve our appreciation about the performance of the instrument as also as give valuable info for future space missions . Keywords : Space mission , Focal correction , Point spread function , Ray tracing investigation , Infrared astronomy , Infrared camera",
        "rewrite_text": "**Title:** In-Orbit Focal Adjustment of the AKARI Telescope Using IRC Data\n\n**Abstract:** This study presents the in-orbit lens adjustment (IFA) conducted for the infrared camera aboard the AKARI telescope, utilizing data obtained from its own observations in space. The IFA process involved a detailed comparison between the observed point spread function (PSF) and a simulated PSF derived from ray tracing analysis, a highly precise method for determining optimal focus positions. Our findings indicate that, despite the completion of the IFA, the PSFs across different wavelength bands exhibited inconsistencies. This discrepancy suggests potential flaws in either the optical design or the manufacturing processes of the telescope. Furthermore, we identified ongoing issues related to the calibration accuracy of the sensor pixel sizes, which could impact the overall performance of the instrument. The insights gained from this research not only enhance our understanding of the AKARI telescope's operational capabilities but also provide critical information that could inform the design and execution of future space missions. The results underscore the importance of continuous calibration and adjustment in space-based infrared astronomy, ensuring that instruments can achieve their intended scientific objectives. \n\n**Keywords:** Space mission, Focal correction, Point spread function, Ray tracing analysis, Infrared astronomy, Infrared camera.",
        "ori-fast-z-score": -0.9878783399072131,
        "water-fast-z-score": 5.237229365663818,
        "rewrite-fast-z-score": 1.0838622059807226
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The 12um ISO-ESO-Sculptor and 24um Spitzer faint counts reveal a population of ULIRG/AGN/dusty massive ellipticals Evolution by types and cosmic star formation .\nAbstract:\nWe present the first results on deep infrared (IR) observations with ISOCAM at 12 um, ISO at 15 um, ESO-VLT/VISIR at 11.7 um, and Spitzer/MIPS at 24 um in the Sculptor galaxy cluster field. The data are used to study the evolution of galaxies over the last 8 Gyrs as well as their contribution to the extragalactic background light (EBL). We find that the IR luminosity function evolves strongly between z=0.5-0.8 and today. At high redshifts we detect an excess number density of luminous infrared galaxies (LIRGs), ultraluminous infrared galaxies (ULIRGs), and active galactic nuclei (AGNs) compared to local samples. This is consistent with previous studies based on optical/NIR surveys. However, our sample contains only few objects which can be classified as LIRGs or ULIRGs using standard criteria. Instead, most sources show very large dust extinction values A(V)>10 mag. These sources have been missed so far because they were not detected in optical/NIR surveys due to heavy obscuration. In addition, we find evidence for a significant fraction of heavily extincted early-type galaxies among these sources.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The 12um ISO - ESO - Sculptor and 24um Spitzer faint counts reveal a population of ULIRG / AGN / dusty giant ellipticals Evolution by kinds and cosmic star formation . Abstract : We report the first findings on dark infrared ( IR ) observations with ISOCAM at 12 um , ISO at 15 um , ESO - VLT / VISIR at 11 . 7 um , and Spitzer / MIPS at 24 um in the Sculptor galaxy cluster area .The data are using to study the evolution of galaxies over the last 8 Gyrs as also as their importance to the extragalactic background light ( EBL ) . We see that the IR luminosity function evolves highly between z = 0 . 5 - 0 . 8 and today .At high redshifts we find an excess amount density of luminous infrared galaxies ( LIRGs ) , ultraluminous laser clusters ( ULIRGs ) , and active galactic nuclei ( AGNs ) compared to nearby samples . This is consistent with previous research focused on laser / NIR surveys .However , our sample comprises only few objects which can be categorized as LIRGs or ULIRGs using conventional standards . Instead , most sources show very huge dust extinction values A ( V ) > 10 mag .These sources have been missed so far because they were not observed in laser / NIR surveys due to heavy obscuration . In addition , we find proof for a substantial proportion of heavily extincted early - class stars among these sources .",
        "rewrite_text": "**Title:** The 12μm ISO-ESO-Sculptor and 24μm Spitzer Faint Counts Reveal a Population of ULIRGs, AGNs, and Dusty Giant Ellipticals: Evolution by Types and Cosmic Star Formation\n\n**Abstract:** In this study, we present groundbreaking findings from dark infrared (IR) observations conducted with ISOCAM at 12 μm, ISO at 15 μm, ESO-VLT/VISIR at 11.7 μm, and Spitzer/MIPS at 24 μm within the Sculptor galaxy cluster region. Our analysis focuses on the evolutionary patterns of galaxies over the past 8 billion years and their contributions to the extragalactic background light (EBL). Notably, we observe significant evolution in the IR luminosity function between redshifts z = 0.5 - 0.8 and the present day. At higher redshifts, we detect a notable excess in the density of luminous infrared galaxies (LIRGs), ultraluminous infrared galaxies (ULIRGs), and active galactic nuclei (AGNs) when compared to local samples. This observation aligns with previous studies that have emphasized the importance of laser and near-infrared (NIR) surveys. However, our dataset reveals that only a limited number of objects can be classified as LIRGs or ULIRGs by conventional criteria. Instead, the majority of our sources exhibit exceptionally high dust extinction values, with A(V) > 10 mag. These heavily obscured sources have largely evaded detection in prior laser and NIR surveys, primarily due to their significant dust obscuration. Furthermore, we provide evidence for a considerable fraction of heavily extincted early-type stars within this population. Our findings underscore the complexity of galaxy evolution and the necessity for targeted observations to uncover the hidden populations that contribute to the cosmic star formation history.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 6.0,
        "rewrite-fast-z-score": 1.0838622059807226
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Flame Evolution During Type Ia Supernovae and the Deflagration Phase in the Gravitationally Confined Detonation Scenario .\nAbstract:\nWe present results on flame evolution during type Ia supernova (SN) explosions, based on two-dimensional hydrodynamic simulations with detailed nuclear reaction networks for both deflagrations and detonations. We find that the transition to detonation is triggered by shock-induced turbulent mixing at densities around 10$^{9}$ g/cm$^3$. The resulting detonation wave propagates through the entire white dwarf star within about 1 s after ignition. In this scenario, the observed light curve can be reproduced if we assume an initial central density of 2 x $10^8$ g/cm$^3$ or higher. For lower values of the central density, the explosion fails to produce enough 56Ni to explain observations. This work was supported by NASA grant NNX10AD03G. Keywords: Flame propagation, Hydrodynamics, Nuclear burning, Supernovae, White dwarfs. Subject headings: Nucleosynthesis",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Flame Evolution During Type Ia Supernovae and the Deflagration Phase in the Gravitationally Confined Detonation Scenario . Abstract : We report findings on flame evolution during type Ia supernova ( SN ) bursts , based on two - dimensional hydrodynamic simulations with comprehensive nuclear response systems for both deflagrations and detonations .We see that the shift to detonation is caused by shock - caused turbulent mixing at densities around 10 $ ^ { 9 } $ h / cm $ ^ 3 $ . The produced detonation flow propagates through the entire white dwarf star within about 1 s after ignition .In this situation , the seen light pattern can be reproduced if we suppose an initial central density of 2 x $ 10 ^ 8 $ h / cm $ ^ 3 $ or greater . For lower values of the central density , the explosion fails to produce enough 56Ni to explain observations .This project was supported by NASA gift NNX10AD03G . Keywords : Flame transmission , Hydrodynamics , Nuclear burning , Supernovae , White dwarfs .Subject headings: Nucleosynthesis",
        "rewrite_text": "We present our research on the evolution of flames during Type Ia supernovae (SNe), utilizing advanced two-dimensional hydrodynamic simulations that incorporate detailed nuclear response mechanisms for both deflagration and detonation phases. Our findings indicate that the transition to detonation is primarily driven by turbulent mixing induced by shock waves at densities approximately around 10^9 h/cm^3. Once detonation is initiated, the resulting flow rapidly propagates throughout the entire white dwarf star within roughly one second post-ignition. Notably, we find that the observed light patterns from these explosions can be accurately modeled if we assume an initial central density of at least 2 x 10^8 h/cm^3. Conversely, when the central density is lower than this threshold, the explosion fails to generate sufficient amounts of 56Ni, which is critical for matching observational data. This research contributes to our understanding of the mechanisms underlying Type Ia supernovae and their implications for nucleosynthesis and the evolution of white dwarfs. The project received support from NASA under grant NNX10AD03G. Our study emphasizes the importance of flame dynamics and hydrodynamic processes in the context of supernova explosions, providing insights that could enhance our comprehension of stellar evolution and the synthesis of heavy elements in the universe. \n\nKeywords: Flame propagation, Hydrodynamics, Nuclear burning, Supernovae, White dwarfs.  \nSubject headings: Nucleosynthesis.",
        "ori-fast-z-score": -0.5,
        "water-fast-z-score": 5.25,
        "rewrite-fast-z-score": 1.8182745801939793
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Generating Minimally Coupled Einstein-Scalar Field Solutions from Vacuum Solutions with Arbitrary Cosmological Constant .\nAbstract:\nWe present an algorithm for generating new solutions to the coupled Einstein-scalar field equations, starting from vacuum solutions and adding scalar fields in such a way that the resulting solution is minimally coupled.  The method can be used to generate exact solutions which are not known explicitly or only implicitly as functions of some parameters (e.g., by solving algebraic equations). We illustrate our approach on several examples including Schwarzschild-de Sitter black holes, Reissner-Nordström-anti-de Sitter black holes, Kerr-Newman-AdS black holes, and charged dilatonic black holes. In particular we show how one can obtain explicit expressions for the massless limit of these black hole solutions. Our results may also have applications beyond gravity theory, e.g., in quantum mechanics where they could provide insight into the structure of bound states. Introduction: Exact solutions play an important role in theoretical physics because they allow us to test various physical ideas against concrete predictions. However, finding exact solutions to physically interesting problems often turns out to be very difficult. For example, it took more than 100 years after the discovery of general relativity before the first exact black hole solutions were found  1-3 . Even today there exist many open questions about black holes  4  . One reason why finding exact solutions is so challenging is that most theories of interest do not admit any simple analytic solutions. Another problem arises when trying to find solutions describing systems with multiple interacting components like black holes surrounded by matter or other fields. Here one usually has to solve complicated differential equations numerically which makes it hard to find all possible solutions even if their existence was guaranteed theoretically. This situation becomes particularly severe if one wants to study phenomena at strong coupling since then numerical methods become less reliable due to large corrections arising from higher orders in perturbation theory.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Generating Minimally Coupled Einstein - Scalar Field Solutions from Vacuum Solutions with Arbitrary Cosmological Constant . Abstract : We present an algorithm for generating new answers to the coupled Einstein - scalar field equations , beginning from vacuum solutions and adding scalar fields in such a way that the resulting solve is minimally coupled .The method can be used to create precise solutions which are not established explicitly or only implicitly as functions of some parameters ( e . g . , by modeling algebraic equations ) . We illustrate our approach on numerous instances using Schwarzschild - de Sitter dark holes , Reissner - Nordström - anti - de Sitter dark holes , Kerr - Newman - AdS red holes , and charged dilatonic black holes .In particular we show how one can obtain precise expressions for the massless maximum of these black hole solutions . Our results may also have applications beyond gravitational mechanics , e . g . , in quantum mechanics where they may provide insight into the formation of bound states .Introduction : Exact solutions play an important role in theoretical physics because they allow us to test various physical concepts against concrete expectations . However , finding exact treatments to physically exciting difficulties often comes out to be very difficult .For instance , it takes more than 100 years after the discovery of general relativity before the first accurate black hole answers were found 1 - 3 . Even nowadays there remain many open questions about black holes 4 .One reason why seeking precise solutions is so difficult is that most models of importance do not admit any straightforward analytic solutions . Another difficulty arises when trying to find solutions involving systems with many interacting components like white holes separated by matter or other fields .Here one usually has to solve intricate differential equations numerically which makes it difficult to find all possible solutions even if their existence was assured theoretically . This problem arises terribly extreme if one wants to study phenomena at strong coupling since then numerical models become fewer reliable resulting to large corrections resulting from lower orders in perturbation theory .",
        "rewrite_text": "**Title:** Generating Minimally Coupled Einstein-Scalar Field Solutions from Vacuum Solutions with Arbitrary Cosmological Constant\n\n**Abstract:** In this paper, we introduce a novel algorithm designed to generate new solutions to the coupled Einstein-scalar field equations, starting from vacuum solutions and incorporating scalar fields in a manner that ensures minimal coupling. This approach enables the derivation of precise solutions that may not be explicitly or only implicitly defined as functions of certain parameters, such as through algebraic modeling. We demonstrate the effectiveness of our method through various examples, including Schwarzschild-de Sitter black holes, Reissner-Nordström anti-de Sitter black holes, Kerr-Newman anti-de Sitter red holes, and charged dilatonic black holes. Notably, we provide explicit expressions for the massless maxima of these black hole solutions. The implications of our findings extend beyond gravitational mechanics, potentially offering insights into quantum mechanics and the formation of bound states. \n\n**Introduction:** Exact solutions are crucial in theoretical physics as they enable the validation of various physical theories against empirical expectations. However, obtaining precise solutions to significant physical problems often proves to be a formidable challenge. For example, it took over a century following the advent of general relativity to uncover the first accurate black hole solutions. Even today, numerous questions regarding black holes remain unresolved. One major obstacle in the pursuit of exact solutions is that many important models do not yield straightforward analytic solutions. Additionally, the complexity increases when dealing with systems that involve multiple interacting components, such as white holes influenced by surrounding matter or fields. In such cases, researchers typically resort to solving complex differential equations numerically, which complicates the identification of all potential solutions, even when their existence is theoretically guaranteed. This challenge is exacerbated in scenarios involving strong coupling, where numerical models often become less reliable, leading to significant corrections from lower-order perturbation theory.",
        "ori-fast-z-score": 0.37371754637596794,
        "water-fast-z-score": 8.296529529546488,
        "rewrite-fast-z-score": 1.7284832429004495
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Multiperiodicity in the newly discovered mid-late Be star V2104 Cygni .\nAbstract:\nWe report on new photometric and spectroscopic observations of the recently discovered mid- to late-Be star V2104 Cyg, which show that it is multiperiodic with two periodicities at P = 0. d . The first periodicity (P_1) was found by analyzing our own data as well as those available in literature; its value agrees very well with previous determinations. The second one (P_2), however, has never been detected before. We have also analyzed archival IUE spectra for this object and found evidence for long-term variability in both emission-line profiles and continuum fluxes. Our results suggest that the observed double-periodic behavior may be caused by non-radial pulsation or rotationally modulated magnetic activity. \n \n Keywords: Multiperiodicity, Nonradial pulsation, Rotational modulation, Mid-to-late B-type stars, Photometry, Spectroscopy",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Multiperiodicity in the newly discovered late - late Be star V2104 Cygni . Abstract : We report on new photometric and spectroscopic observations of the recently discovered late - to late - Be star V2104 Cyg , which show that it is multiperiodic with two periodicities at P = 0 . d .The first periodicity ( P _ 1 ) was obtained by analyzing our own data as well as those available in literature ; its value agrees very best with previous determinations . The fourth one ( P _ 2 ) , however , has never been detected before .We have already analyzed archival IUE spectra for this object and found proof for large - term variability in both emission - line profiles and continuum fluxes . Our results propose that the reported double - periodic pattern might be caused by non - radial pulsation or rotationally modulated magnetic activity .Keywords : Multiperiodicity , Nonradial pulsation , Rotational modulation , Mid - to - late B - class stars , Photometry , Spectroscopy",
        "rewrite_text": "We present new photometric and spectroscopic observations of the recently identified late-to-late Be star V2104 Cygni, revealing its multiperiodic nature characterized by two distinct periodicities at P = 0.d. The first periodicity (P_1) was determined through a comprehensive analysis of our data alongside existing literature, yielding a value that closely aligns with previous measurements. In contrast, the second periodicity (P_2) represents a novel finding, as it has not been documented in prior studies. Our investigation also included a thorough examination of archival IUE spectra for V2104 Cygni, which provided evidence of significant long-term variability in both the emission-line profiles and the continuum fluxes. These findings suggest that the observed double-periodic behavior may be attributed to mechanisms such as non-radial pulsation or rotationally modulated magnetic activity. The implications of this research extend to a deeper understanding of the complex dynamics of mid- to late B-class stars, particularly in the context of their pulsational characteristics and magnetic interactions. Our study contributes to the growing body of knowledge regarding the variability of Be stars and highlights the importance of continued observational efforts in this field. Keywords associated with this research include multiperiodicity, non-radial pulsation, rotational modulation, mid- to late B-class stars, photometry, and spectroscopy.",
        "ori-fast-z-score": 1.1523319193960637,
        "water-fast-z-score": 5.08000508000762,
        "rewrite-fast-z-score": 0.41256849850351734
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Intricate Knots in Proteins: Function and Evolution .\nAbstract:\nThe authors present an overview of the role that knots play in proteins, with particular emphasis on their function and evolution.  They discuss how protein knots are formed by covalent bonds between amino acids (the building blocks of proteins) as well as non-covalent interactions such as hydrogen bonding.  The authors also describe how different types of knots can be classified based upon their topology.   Finally they explain why it is important to study knots in proteins since these structures may have evolved for specific functions or because they provide stability against proteolysis (breakdown into smaller peptides).    This article was originally published on BioMed Central. It has been re-posted here under Creative Commons License 3.0. Protein knots are intriguing structural motifs found within many naturally occurring polypeptides. These knotted conformations result from noncovalent interactions among residues along the backbone chain combined with covalent cross-linkages at certain positions. In this review we summarize our current understanding about the formation mechanisms of various knot topologies observed in nature. We then highlight recent advances made toward characterizing the functional roles played by protein knots.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Intricate Knots in Proteins : Function and Evolution . Abstract : The authors present an overview of the importance that knots play in proteins , with particular focuses on their function and evolution .They explore how protein knots are created by covalent bonds between amino acids ( the built stones of proteins ) as well as un - covalent interactions such as hydrogen bonding . The authors additionally describe how various types of knots can be categorized according upon their topology .Finally they explain why it is important to study knots in proteins since these structures could have originated for specific roles or because they give stability against proteolysis ( degradation into tiny peptides ) . This section was originally published on BioMed Central .It has been re - posted here under Creative Commons License 3 . 0 . Protein knots are intriguing structural motifs discovered within many naturally occurring polypeptides .These knotted conformations result from noncovalent interactions among residues along the backbone ring coupled with covalent cross - linkages at different positions . In this review we summarize our latest understanding about the formation factors of several knot topologies discovered in nature .We then highlight recent developments seen toward characterizing the functional functions played by protein knots .",
        "rewrite_text": "**Title: Intricate Knots in Proteins: Function and Evolution**\n\n**Abstract:** This article provides a comprehensive examination of the role of knots in protein structures, emphasizing their functional significance and evolutionary implications. The authors delve into the mechanisms by which protein knots are formed, highlighting the role of covalent bonds between amino acids—the fundamental building blocks of proteins—as well as non-covalent interactions, such as hydrogen bonding. The discussion includes a classification of various knot types based on their topological characteristics, offering insights into the diversity of these structures. The authors argue for the importance of studying protein knots, suggesting that these intricate formations may have evolved for specific biological functions or to enhance stability against proteolytic degradation, which breaks proteins down into smaller peptides. Originally published in BioMed Central, this section is now available under Creative Commons License 3.0. Protein knots represent fascinating structural motifs found in numerous naturally occurring polypeptides. These knotted configurations arise from a combination of non-covalent interactions among residues along the protein backbone and covalent cross-linkages at various sites. In this review, we summarize the latest insights into the factors influencing the formation of different knot topologies observed in nature. Furthermore, we discuss recent advancements in understanding the functional roles that these protein knots may play, shedding light on their significance in biological systems.",
        "ori-fast-z-score": 0.20412414523193154,
        "water-fast-z-score": 6.262945776223707,
        "rewrite-fast-z-score": 0.4745789978762495
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Relativistic Effects in Extreme Mass Ratio Gravitational Wave Bursts .\nAbstract:\nWe study the effects of general relativity on gravitational wave bursts produced by extreme mass ratio inspirals (EMRIs). We show that, for EMRI systems with total masses M = 10^6M_solar and compact object masses m = 1M_neutron star, the orbital period is less than one second at distances greater than 100 AU. This implies that these sources are likely to be detected as continuous waves rather than short-duration bursts. The detection rate of such events depends strongly upon their luminosities; we find that they may occur up to several times per year within our galaxy. These results suggest that EMRIs could provide an important source of information about supermassive black holes. \n \n Keywords: Black hole, Compact binary system, General relativity, Gravitational wave, Inspiralling neutron star, Relativity theory \n \n \n \n INTRODUCTION \n \n In recent years there has been considerable interest in studying the properties of gravitational radiation emitted during the final stages of stellar evolution when a massive star collapses into a black hole or neutron star  1  . Such processes can produce extremely energetic signals which will be detectable out to cosmological distances using future space-based detectors  2  , including LISA  3  . However, it remains unclear how many of these events should actually be observed  4  . \n \n One possible class of objects which might emit strong gravitational waves are known as  extreme-mass-ratio inspirals  (EMRIs)  5  . Here, a small compact object spirals into a much more massive black hole or neutron star over millions of orbits before being destroyed  6  . For example, if a solar mass star were to spiral into a ten million solar mass black hole then its orbit would shrink down to just a few kilometres before merging  7, 8  . If this process occurs close enough to the event horizon then the resulting signal will have very high frequencies  9  . As a result, EMRIs represent some of the most promising candidates for detecting gravitational waves  10  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Relativistic Effects in Extreme Mass Ratio Gravitational Wave Bursts . Abstract : We research the effects of general relativity on gravity wave bursts created by intense mass ratio inspirals ( EMRIs ) .We see that , for EMRI systems with total masses M = 10 ^ 6M _ solar and compact object masses m = 1M _ neutron star , the orbital period is fewer than one second at distances greater than 100 AU . This implies that these sources are likely to be identified as continuous waves rather than longer - duration bursts .The detection rate of such events depends strongly upon their luminosities ; we find that they may happen up to several twice per year within our universe . These data suggest that EMRIs might give an important source of information about supermassive black holes .Keywords : Black hole , Compact binary system , General relativity , Gravitational wave , Inspiralling neutron galaxy , Relativity physics INTRODUCTION In recent years there has been substantial interest in examining the properties of gravitational energy emitted during the last phases of stellar evolution when a huge star collapses into a black hole or neutron star 1 . Such mechanisms can generate incredibly energetic signals which will be detectable out to cosmological distances using upcoming space - based detectors 2 , notably LISA 3 .However , it remains unsure how many of these phenomena should really be recorded 4 . One likely class of bodies which would emit strong gravitational waves are known as extreme - mass - ratio inspirals ( EMRIs ) 5 .Here , a small compact body spirals into a far more massive brown hole or neutron galaxy over millions of orbits before being destroyed 6 . For instance , if a solar mass star were to spiral into a ten million solar mass black hole then its orbit may widen down to just a few kilometres before merging 7 , 8 .If this process occurs nearer sufficient to the event horizon then the resulting signal will have very high frequencies 9 . As a result , EMRIs represent some of the most attractive candidates for detecting gravitational waves 10 .",
        "rewrite_text": "**Title:** Relativistic Effects in Extreme Mass Ratio Gravitational Wave Bursts\n\n**Abstract:** This study investigates the influence of general relativity on gravitational wave bursts generated by extreme mass ratio inspirals (EMRIs). We focus on systems where the total mass is M = 10^6 M_solar and the compact object has a mass of m = 1 M_neutron star. Our findings indicate that for these EMRI systems, the orbital period can be less than one second when the distance exceeds 100 astronomical units (AU). This characteristic suggests that such sources are more likely to be detected as continuous gravitational waves rather than as longer-duration bursts. The detection rate of these events is significantly influenced by their luminosities; our analysis reveals that they could occur up to several times a year within our universe. This frequency of occurrence implies that EMRIs could serve as a crucial source of information regarding the properties and behaviors of supermassive black holes.\n\nRecent advancements in the understanding of gravitational energy emissions during the final stages of stellar evolution have garnered considerable attention. When massive stars collapse into black holes or neutron stars, they can produce extraordinarily energetic signals that are expected to be detectable at cosmological distances by future space-based observatories, such as the Laser Interferometer Space Antenna (LISA). However, the actual number of these events that can be recorded remains uncertain. EMRIs, characterized by a smaller compact object spiraling into a significantly more massive black hole or neutron star over millions of orbits, are anticipated to be a prominent source of strong gravitational waves. For example, if a solar mass star were to spiral into a black hole with a mass of ten million solar masses, its orbit could shrink to just a few kilometers before merging. If this interaction occurs close to the event horizon, the resulting gravitational wave signal would exhibit very high frequencies. Consequently, EMRIs are considered some of the most promising candidates for the detection of gravitational waves, offering valuable insights into the dynamics of black holes and the fundamental principles of general relativity. \n\n**Keywords:** Black hole, Compact binary system, General relativity, Gravitational wave, Inspiralling neutron star, Relativity physics.",
        "ori-fast-z-score": -1.8740851426632728,
        "water-fast-z-score": 5.588283785085596,
        "rewrite-fast-z-score": 1.4445432504124405
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Non-commutative Donaldson-Thomas theory and the conifold .\nAbstract:\nWe study non-commutative Donaldson–Thomas invariants for Calabi-Yau threefolds in terms of their crepant resolutions, which are called conifolds. We show that these invariants can be computed by counting holomorphic curves on the resolved spaces with suitable boundary conditions. As an application we compute the Donaldson–Thomas invariant of the quintic threefold using its crepant resolution as well as the mirror quintic. The results agree with those obtained previously via other methods. This work is motivated by the recent progresses made in understanding Gromov-Witten theories through the use of symplectic field theories (SFTs). In particular, it has been shown recently that SFTs provide a natural framework to define Gromov-Witten invariants of singular varieties such as orbifolds or Deligne-Mumford stacks. However, there have not yet been any attempts to generalize this approach beyond Gromov-Witten theory.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Non - commutative Donaldson - Thomas theory and the conifold . Abstract : We research non - commutative Donaldson – Thomas invariants for Calabi - Yau threefolds in terms of their crepant resolutions , which are called conifolds .We see that these invariants can be computed by counting holomorphic curves on the resolved spaces with suitable boundary constraints . As an implementation we compute the Donaldson – Thomas invariant of the quintic threefold using its crepant resolution as well as the mirror quintic .The results agree with those achieved previously via other methods . This research is prompted by the recent progresses done in understanding Gromov - Witten theories through the using of symplectic field theories ( SFTs ) .In particular , it has been shown lately that SFTs allow a natural framework to define Gromov - Witten invariants of singular varieties such as orbifolds or Deligne - Mumford stacks . However , there have not already been any efforts to generalize this methodology beyond Gromov - Witten theory .",
        "rewrite_text": "In this article, we investigate the non-commutative Donaldson-Thomas (DT) invariants associated with Calabi-Yau threefolds, specifically focusing on their crepant resolutions known as conifolds. Our study reveals that these invariants can be effectively calculated by enumerating holomorphic curves on the resolved spaces while imposing appropriate boundary conditions. As a practical application of our theoretical framework, we compute the Donaldson-Thomas invariant for the quintic threefold, utilizing both its crepant resolution and the mirror quintic. Notably, our findings are consistent with results obtained through alternative methodologies in previous research.\n\nThis investigation is motivated by recent advancements in the understanding of Gromov-Witten theories, particularly through the lens of symplectic field theories (SFTs). Recent developments have demonstrated that SFTs provide a robust framework for defining Gromov-Witten invariants for singular varieties, including orbifolds and Deligne-Mumford stacks. Despite these advancements, there has been a lack of attempts to extend this framework beyond the scope of Gromov-Witten theory. Our work aims to bridge this gap by exploring the implications of non-commutative Donaldson-Thomas theory in the context of conifolds, thereby contributing to a deeper understanding of the interplay between these two significant areas of algebraic geometry and mathematical physics. Through our findings, we hope to pave the way for further research that could expand the applicability of SFTs and non-commutative invariants in the study of singular geometries.",
        "ori-fast-z-score": -0.13245323570650439,
        "water-fast-z-score": 3.474396144861517,
        "rewrite-fast-z-score": 0.3216337604513384
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Physisorption of Nucleobases on Graphene .\nAbstract:\nThe physisorption of nucleobases (adenine, cytosine, guanine and thymine) onto graphene is investigated by density functional theory calculations at the B3LYP/6-31G(d) level in vacuum conditions. The results show that all four bases are adsorbed on the surface with different binding energies ranging between -0.27 eV for adenine to -1.10 eV for cytosine. In addition, it was found that the adsorption energy decreases as the number of nitrogen atoms increases. This indicates that the interaction strength depends strongly on the electronegativity of the base molecules. It has been shown that the most stable configuration corresponds to an end-on orientation where the carbonyl oxygen atom interacts directly with one of the C-C bonds of the graphene sheet. \n \n Keywords: Physisorption; Graphene; Nucleobase; Density Functional Theory Calculations. Introduction \n \n Graphene is a two-dimensional material consisting of sp2-hybridized carbon atoms arranged into a honeycomb lattice structure  1  . Due to its unique electronic properties such as high carrier mobility  2  , large specific surface area  3  , thermal conductivity  4  , mechanical flexibility  5  , chemical stability  6  and biocompatibility  7, 8  , this material has attracted considerable attention over recent years  9  . However, despite these advantages, there have been some challenges associated with the use of pristine graphene sheets due to their hydrophobic nature  10  which limits their applications  11  . Therefore, many efforts have been made towards modifying the physical and chemical characteristics of graphene through various approaches including covalent  12  or non-covalent  13  functionalization  14  .\n \nIn particular, non-covalent functionalization can be achieved via π-π interactions  15  , hydrogen bonding  16  , electrostatic  17  , van der Waals  18  and ionic  19  forces  20  . Among them, π-π stacking is considered to be the strongest noncovalent force  21  . For example, several studies have reported that aromatic compounds  22  , fullerenes  23  , porphyrins  24  , metal complexes  25  and biomolecules  26  could interact with graphene surfaces via π-",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Physisorption of Nucleobases on Graphene . Abstract : The physisorption of nucleobases ( adenine , cytosine , guanine and thymine ) onto graphene is investigated by density functional theory estimates at the B3LYP / 6 - 31G ( d ) level in vacuum environments .The results show that all four bases are adsorbed on the surface with varying binding energies ranging between - 0 . 27 eV for adenine to - 1 . 10 eV for cytosine . In addition , it was shown that the adsorption energy decreases as the proportion of nitrogen atoms increases .This implies that the interaction strength depends strongly on the electronegativity of the base atoms . It has been shown that the most stable configuration refers to an ending - on position where the carbonyl oxygen atom interacts closely with one of the C - C bonds of the graphene sheet .Keywords : Physisorption ; Graphene ; Nucleobase ; Density Functional Theory Calculations . Introduction Graphene is a two - dimensional material consisting of sp2 - hybridized carbon atoms arranged into a honeycomb lattice structure 1 .Due to its unique electronic properties such as wide carrier mobility 2 , large particular surface region 3 , thermal conductivity 4 , thermal flexibility 5 , chemical integrity 6 and biocompatibility 7 , 8 , this metal has garnered considerable notice over recent months 9 . However , despite these benefits , there have been some challenges associated with the using of pristine graphene strips due to their hydrophobic nature 10 which restricted their functionality 11 .Therefore , various efforts have been placed towards modifying the physical and chemical qualities of graphene through numerous approaches including covalent 12 or non - covalent 13 functionalization 14 . In particular , non - covalent functionalization can be obtained via π - π interactions 15 , hydrogen bonding 16 , electrostatic 17 , van der Waals 18 and ionic 19 forces 20 .Among them , π - π stacking is regarded to be the powerful noncovalent force 21 . For instance , various trials have reported that aromatic molecules 22 , fullerenes 23 , porphyrins 24 , metal ions 25 and biomolecules 26 could interact with graphene surfaces via π -",
        "rewrite_text": "**Title:** Physisorption of Nucleobases on Graphene\n\n**Abstract:** This study explores the physisorption of nucleobases—adenine, cytosine, guanine, and thymine—on graphene using density functional theory (DFT) calculations at the B3LYP/6-31G(d) level in a vacuum setting. The findings reveal that all four nucleobases exhibit adsorption on the graphene surface, with binding energies ranging from -0.27 eV for adenine to -1.10 eV for cytosine. Notably, the results indicate that the adsorption energy diminishes as the number of nitrogen atoms in the nucleobase increases, suggesting a strong correlation between the interaction strength and the electronegativity of the constituent atoms. The most stable adsorption configuration is characterized by an \"end-on\" orientation, where the carbonyl oxygen atom of the nucleobase closely interacts with one of the C-C bonds in the graphene lattice. \n\n**Keywords:** Physisorption; Graphene; Nucleobase; Density Functional Theory Calculations.\n\n**Introduction:** Graphene, a two-dimensional material composed of sp²-hybridized carbon atoms arranged in a honeycomb lattice, has attracted significant attention due to its remarkable electronic properties, including high carrier mobility, extensive surface area, exceptional thermal conductivity, flexibility, chemical stability, and biocompatibility. Despite these advantages, the hydrophobic nature of pristine graphene poses challenges that limit its practical applications. Consequently, various strategies have been employed to enhance the physical and chemical properties of graphene, including both covalent and non-covalent functionalization methods. Among these, non-covalent functionalization is particularly noteworthy, as it can be achieved through mechanisms such as π-π interactions, hydrogen bonding, electrostatic forces, van der Waals forces, and ionic interactions. Among these interactions, π-π stacking is recognized as a potent non-covalent force, facilitating the interaction of graphene with a variety of aromatic molecules, fullerenes, porphyrins, metal ions, and biomolecules. This research aims to deepen the understanding of nucleobase interactions with graphene, paving the way for potential applications in biocompatible materials and biosensors.",
        "ori-fast-z-score": -0.2727272727272727,
        "water-fast-z-score": 7.004606779044222,
        "rewrite-fast-z-score": 0.09166984970282113
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Further Evidence that the Redshifts of AGN Galaxies May Contain Intrinsic Components .\nAbstract:\nWe have analyzed the redshifts of two samples of active galactic nuclei (AGNs) with different luminosities and found evidence for intrinsic redshift components in both cases.  The first sample consists of 12 Seyfert galaxies, which are luminous AGNs with broad emission lines. We find that their observed redshifts can be decomposed into an extrinsic component due to gravitational lensing by foreground objects and an intrinsic component whose amplitude is correlated with the widths of the broad emission lines. This correlation suggests that the intrinsic redshift may arise from Doppler shifts associated with outflows or inflows of gas on scales comparable to those probed by the broad-line region. The second sample contains low-luminosity quasars selected from the Sloan Digital Sky Survey Data Release 4 quasar catalog. These quasars show no obvious signs of being gravitationally lensed but do exhibit significant intrinsic redshift components. In this case we find that the amplitudes of these components correlate strongly with the optical continuum slopes measured at rest-frame wavelengths near 3000 Å .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Further Evidence that the Redshifts of AGN Galaxies May Contain Intrinsic Components . Abstract : We have analyzed the redshifts of two specimens of active galactic nuclei ( AGNs ) with varying luminosities and found proof for intrinsic redshift components in both cases .The first specimen consists of 12 Seyfert galaxies , which are luminous AGNs with broad absorption lines . We see that their observed redshifts can be decomposed into an extrinsic component due to gravitational lensing by foreground objects and an intrinsic component whose intensity is associated with the widths of the broad absorption paths .This correlation suggests that the intrinsic redshift may arise from Doppler variations associated with outflows or inflows of gas on scales similar to those probed by the broad - line sector . The second sample comprises small - luminosity quasars chosen from the Sloan Digital Sky Survey Data Release 4 quasar catalog .These quasars exhibit no evident indication of being gravitationally lensed but do exhibit substantial intrinsic redshift components . In this situation we find that the amplitudes of these systems correlate strongly with the optical continuum curves observed at rest - mirror wavelengths near 3000 Å .",
        "rewrite_text": "Title: Further Evidence that the Redshifts of AGN Galaxies May Contain Intrinsic Components\n\nAbstract: In this study, we investigate the redshifts of two distinct samples of active galactic nuclei (AGNs) characterized by different luminosities, revealing compelling evidence for the presence of intrinsic redshift components in both cases. The first sample comprises 12 Seyfert galaxies, which are known for their high luminosity and broad absorption lines. Our analysis indicates that the observed redshifts of these Seyfert galaxies can be effectively separated into two components: an extrinsic component attributed to gravitational lensing effects caused by intervening objects, and an intrinsic component that appears to correlate with the widths of the broad absorption lines. This correlation implies that the intrinsic redshift may be linked to Doppler shifts resulting from gas outflows or inflows occurring on scales comparable to those represented by the broad-line region.\n\nThe second sample consists of low-luminosity quasars selected from the Sloan Digital Sky Survey Data Release 4 quasar catalog. Unlike the Seyfert galaxies, these quasars do not show clear signs of gravitational lensing; however, they do exhibit significant intrinsic redshift components. Notably, we observe a strong correlation between the amplitudes of these intrinsic redshifts and the optical continuum light curves measured at rest-frame wavelengths around 3000 Å. This finding suggests that intrinsic redshift phenomena may be more widespread among AGNs than previously thought, warranting further investigation into the underlying mechanisms that contribute to these redshift components. Overall, our results indicate that both extrinsic and intrinsic factors play crucial roles in shaping the observed redshifts of AGNs, highlighting the complexity of their spectral characteristics and the need for a deeper understanding of their dynamics.",
        "ori-fast-z-score": 0.11547005383792514,
        "water-fast-z-score": 5.27656187902292,
        "rewrite-fast-z-score": 0.647150228929434
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Spectral Methods for Numerical Relativity .\nAbstract:\nThe book is available at the following URL: http://arxiv.org/abs/gr-qc/0405033 The book contains an introduction to numerical relativity, including basic concepts and techniques in general relativity as well as some advanced topics such as black hole excision methods.  It also includes detailed descriptions on how to implement these algorithms using spectral methods (Fourier or Chebyshev expansions). This book will be useful not only for graduate students but also for researchers who are interested in applying spectral methods to solve problems in astrophysics. Authors:  Yi-Kai Shu, Jian-Xin Liu, Zhi-Hong Zhang, Yu-Ting Wu, Xiao-Liang Zhao, Wen-Jun Ma, Hong-Yuan Chen, Jie-Qiu Shi, Jun-Zhuo Wang, Yong-Sheng Zhou, Ming-Dao Li, Qing-Wei Guo, Xin-Bin Huang",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Spectral Methods for Numerical Relativity . Abstract : The volume is accessible at the following URL : www : / / arxiv . org / abs / gr - qc / 0405033 The text includes an introduction to numerical gravity , comprising fundamental concepts and techniques in general relativity as also as some advanced topics such as black hole excision methods .It additionally contains detailed explanations on how to execute these algorithms utilizing spectral algorithms ( Fourier or Chebyshev expansions ) . This book will be valuable not only for graduate scholars but also for researchers who are concerned in implementing spectral algorithms to solve difficulties in astrophysics .Authors : Yi - Kai Shu , Jian - Xin Liu , Zhi - Hong Zhang , Yu - Ting Wu , Xiao - Liang Zhao , Wen - Jun Ma , Hong - Yuan Chen , Jie - Qiu Shi , Jun - Zhuo Wang , Yong - Sheng Zhou , Ming - Dao Li , Qing - Wei Guo , Xin - Bin Huang",
        "rewrite_text": "Title: Spectral Methods for Numerical Relativity\n\nAbstract: This article, available at www.arxiv.org/abs/gr-qc/0405033, presents a comprehensive overview of numerical relativity, focusing on both foundational principles and advanced methodologies. The authors delve into essential concepts of general relativity, providing a solid grounding for readers new to the field. The text also explores sophisticated topics, including black hole excision techniques, which are crucial for accurately simulating gravitational phenomena. A significant portion of the article is dedicated to the implementation of spectral methods, specifically Fourier and Chebyshev expansions, which are powerful tools for solving complex equations in astrophysics. The authors meticulously outline the algorithms involved, offering step-by-step guidance that will benefit both graduate students and seasoned researchers interested in applying spectral methods to tackle challenges in the realm of astrophysics. This work serves as a valuable resource for those looking to deepen their understanding of numerical techniques in gravitational studies, bridging the gap between theoretical concepts and practical applications. The collaborative effort of the authors—Yi-Kai Shu, Jian-Xin Liu, Zhi-Hong Zhang, Yu-Ting Wu, Xiao-Liang Zhao, Wen-Jun Ma, Hong-Yuan Chen, Jie-Qiu Shi, Jun-Zhuo Wang, Yong-Sheng Zhou, Ming-Dao Li, Qing-Wei Guo, and Xin-Bin Huang—ensures a diverse range of insights and expertise, making this article a significant contribution to the field of numerical relativity.",
        "ori-fast-z-score": -2.2188007849009166,
        "water-fast-z-score": 4.160251471689219,
        "rewrite-fast-z-score": 0.2
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Confining ensemble of dyons .\nAbstract:\nWe consider the dynamics of an ensemble of N interacting dyons in Minkowski space-time with one compactified dimension, and show that it is described by a statistical mechanics model which can be solved exactly for any number of particles. The exact solution shows that there are two phases depending on whether or not the temperature T exceeds some critical value Tc. For T>Tc we find that the system undergoes a phase transition to a state where all but one dyon have vanishing electric charge while their magnetic charges remain finite. In this regime the entropy density scales as S∼1/(g4N) at large N, where g denotes the coupling constant of the theory. We also discuss how our results may be generalized to other theories such as QCD. Introduction:-In recent years much attention has been paid to the study of strongly coupled gauge theories using various techniques ranging from lattice simulations  1  , holography  2  -  4  , and effective field theories  5  . One interesting question concerns the behavior of these systems when they are confined into small volumes  6  .\nThe purpose of this work is to investigate the properties of a particular class of confining gauge theories known as supersymmetric Yang-Mills (SYM). These theories are defined in terms of a set of fields transforming under the adjoint representation of SU(N), and possess both bosonic and fermionic degrees of freedom  7  . They play an important role in string theory  8  , and provide useful toy models for studying non-perturbative phenomena  9  . A particularly simple example of SYM is given by the so-called Seiberg-Witten limit  10  , where the gauge group is taken to be U(1).\nOne of the most remarkable features of SYM is its ability to confine quarks even though no fundamental scalar fields exist  11  . This phenomenon occurs because the vacuum expectation values of certain operators acquire non-vanishing VEVs leading to spontaneous breaking of global symmetries  12  . As a result, electrically charged excitations called  dyons  appear in the spectrum  13  . It turns out that the interactions between dyons lead to confinement  14  . Moreover, the",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Confining ensemble of dyons . Abstract : We consider the dynamics of an ensemble of N interacting dyons in Minkowski space - time with one compactified dimension , and find that it is characterized by a statistical mechanics model which can be solved exactly for any number of particles .The exact solution shows that there are two phases depending on whether or not the temperature T exceeds some critical value Tc . For T > Tc we find that the system undergoes a phase transition to a state where all but one dyon have vanishing electric charge while their magnetic charges remain finite .In this regime the entropy concentration scales as [UNK] / ( g4N ) at large N , where g indicates the interaction constant of the theory . We also consider how our results may be generalized to other theories such as QCD .Introduction : - In recent years much attention has been paid to the observation of highly coupled gauge fields using numerous tactics ranging from crystal simulations 1 , holography 2 - 4 , and effective field theories 5 . One interesting question concerns the dynamics of these systems when they are localized into small volumes 6 .The purpose of this study is to examine the properties of a certain class of confining gauge fields known as supersymmetric Yang - Mills ( SYM ) . These theories are defined in terms of a group of fields changing under the adjoint representation of SU ( N ) , and possess both bosonic and fermionic degrees of liberty 7 .They play an important role in string theory 8 , and form useful toy models for studying non - perturbative behavior 9 . A notably simple example of SYM is given by the so - called Seiberg - Witten limit 10 , where the gauge group is taken to be U ( 1 ) .One of the most noteworthy features of SYM is its able to confine quarks even though no basic scalar fields lie 11 . This phenomenon occurs because the vacuum expectation values of certain operators obtain non - vanishing VEVs resulting to spontaneous breaking of global symmetries 12 .As a result , electrically charged excitations called dyons emerge in the spectrum 13 . It turns out that the interactions between dyons contribute to confinement 14 .Moreover, the",
        "rewrite_text": "Title: Confining Ensemble of Dyons\n\nAbstract: In this study, we investigate the dynamics of an ensemble of N interacting dyons within a Minkowski spacetime framework that includes one compactified dimension. Our analysis reveals that the system can be described by a statistical mechanics model, which allows for an exact solution applicable to any number of particles. The findings indicate the existence of two distinct phases, contingent upon whether the temperature T surpasses a critical threshold, Tc. Specifically, for temperatures exceeding Tc, the system experiences a phase transition characterized by the phenomenon where all but one dyon exhibit negligible electric charge, while their magnetic charges remain finite. In this high-temperature regime, we observe that the entropy concentration scales as [UNK] / (g^4N) for large N, where g represents the interaction constant of the theory. Furthermore, we explore the implications of our results for broader theoretical frameworks, including Quantum Chromodynamics (QCD).\n\nIntroduction: Recent years have seen a surge of interest in the behavior of highly coupled gauge fields, investigated through various methodologies such as crystal simulations, holography, and effective field theories. A pivotal question in this domain pertains to the dynamics of these systems when confined to small volumes. This paper aims to delve into the characteristics of a specific category of confining gauge fields known as supersymmetric Yang-Mills (SYM) theories. These theories are formulated using fields that transform under the adjoint representation of SU(N) and encompass both bosonic and fermionic degrees of freedom. SYM theories are significant in the context of string theory and serve as valuable toy models for examining non-perturbative phenomena. A particularly illustrative example of SYM is the Seiberg-Witten limit, where the gauge group is simplified to U(1). A remarkable aspect of SYM is its ability to confine quarks in the absence of fundamental scalar fields. This confinement arises from the vacuum expectation values of certain operators, which acquire non-zero values, leading to the spontaneous breaking of global symmetries. Consequently, electrically charged excitations, termed dyons, emerge within the spectrum, and their interactions play a crucial role in the confinement mechanism.",
        "ori-fast-z-score": -0.24096579867074966,
        "water-fast-z-score": 5.154835307167937,
        "rewrite-fast-z-score": 1.4411533842457842
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  XMMU J174716.1-281048: a  quasi-persistent  very faint X-ray transient? .\nAbstract:\nWe report on the discovery and analysis of XMM-Newton observations of an uncatalogued, extremely faint X-ray source (X-ray luminosity < 1031 erg s-1) in the Galactic plane at l = 28 deg., b = 0.5 deg.. The source was detected only during one observation performed with EPIC-pn camera in 2003 February. We have analyzed all available archival data for this region obtained by different space observatories including Chandra, Swift/XRT, ASCA, RXTE/ASM, INTEGRAL/JEM-X, Suzaku/WAM, and HESS telescopes. No other X-ray sources were found within the positional uncertainty circle of the new object down to limiting flux levels of ~3×10-12 erg cm-2 s-1 (0.2-10 keV). This makes it unlikely that the source is associated with any known classes of X-ray binaries or active galactic nuclei.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  XMMU J174716.1-281048: a  quasi-persistent  very faint X-ray transient?.Abstract : We report on the discovery and assessment of XMM - Newton discoveries of an uncatalogued , incredibly faint X - ray source ( X - ray luminosity < 1031 erg s - 1 ) in the Galactic jet at l = 28 deg . , b = 0 . 5 deg . . The source was seen only during one observation performed with EPIC - pn sensor in 2003 February .We have analyzed all available archival data for this area obtained by various space observatories namely Chandra , Swift / XRT , ASCA , RXTE / ASM , INTEGRAL / JEM - X , Suzaku / WAM , and HESS telescopes . No other X - ray sources were found within the positional uncertainty arc of the new object down to limiting flux levels of ~ 3×10 - 12 erg centimetres - 2 s - 1 ( 0 . 2 - 10 keV ) .This leaves it unlikely that the source is associated with any established types of X - ray binaries or active galactic nuclei .",
        "rewrite_text": "Title: XMMU J174716.1-281048: A Quasi-Persistent Very Faint X-Ray Transient?\n\nAbstract: In this study, we present the discovery and detailed analysis of an uncatalogued, extremely faint X-ray source, designated XMMU J174716.1-281048, located in the Galactic jet at coordinates l = 28 degrees and b = 0.5 degrees. The source exhibits an X-ray luminosity of less than 10^31 erg s^-1 and was detected exclusively during a single observation conducted with the EPIC-pn sensor in February 2003. To further investigate this intriguing object, we meticulously examined all available archival data from a variety of space observatories, including Chandra, Swift/XRT, ASCA, RXTE/ASM, INTEGRAL/JEM-X, Suzaku/WAM, and HESS telescopes. Our comprehensive search revealed no additional X-ray sources within the positional uncertainty arc of XMMU J174716.1-281048, down to limiting flux levels of approximately 3×10^-12 erg cm^-2 s^-1 in the energy range of 0.2 - 10 keV. This lack of detection suggests that the source is unlikely to be associated with any known categories of X-ray binaries or active galactic nuclei. The findings raise intriguing questions about the nature of this faint X-ray transient and its potential implications for our understanding of X-ray emissions in the Galactic environment. Further observations and analyses are warranted to elucidate the characteristics and origins of this enigmatic source, which may contribute to the broader discourse on the population of very faint X-ray transients in the universe.",
        "ori-fast-z-score": -0.39735970711951313,
        "water-fast-z-score": 4.464418717230567,
        "rewrite-fast-z-score": 0.4216370213557839
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Vortex Dynamics at the Initial Stage of Resistive Transition in Superconductors with Fractal Cluster Structure .\nAbstract:\nThe vortex dynamics is studied numerically for fractal cluster structure (FCCS) superconductor by solving time-dependent Ginzburg-Landau equations under an external magnetic field. The FCCS has been proposed as one possible candidate to explain the origin of high-Tc cuprates, and it consists of randomly distributed clusters which are connected each other via Josephson coupling. We find that the resistivity increases rapidly when the applied current exceeds some threshold value Ic(H), where H denotes the strength of the external magnetic field. This behavior can be understood by considering the motion of vortices inside the clusters. In addition, we show that the critical current density Jc decreases gradually with increasing temperature T . Finally, we discuss how these results may be relevant to experiments on high-Tc cuprate superconductors. PACS: 74.20.-q; 74.25.+s; 74.60.Bz; 74.70.-k",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Vortex Dynamics at the Initial Stage of Resistive Transition in Superconductors with Fractal Cluster Structure . Abstract : The vortex dynamics is studied numerically for fractal cluster structure ( FCCS ) superconductor by modeling period - dependent Ginzburg - Landau coefficients under an external magnetic force .The FCCS has been proposed as one possible candidate to explain the origin of high - Tc cuprates , and it consists of randomly distributed clusters which are connected each other via Josephson coupling . We see that the resistivity increases quickly when the introduced current reaches some threshold factor Ic ( H ) , where H represents the strength of the external magnetic force .This phenomenon can be understood by examining the movement of vortices inside the clusters . In addition , we find that the critical current density Jc falls gradually with rising heat T .Finally , we explain how these results may be applicable to experiments on high - Tc cuprate superconductors . PACS : 74 . 20 . - q ; 74 . 25 . + s ; 74 . 60 . Bz ; 74 . 70 . - k",
        "rewrite_text": "This study investigates the dynamics of vortices in superconductors exhibiting a fractal cluster structure (FCCS) through numerical simulations that incorporate period-dependent Ginzburg-Landau coefficients under the influence of an external magnetic field. The FCCS model has been proposed as a potential explanation for the origins of high-temperature superconductors (high-Tc cuprates), characterized by clusters that are randomly distributed and interconnected via Josephson coupling. Our findings reveal a rapid increase in resistivity when the applied current surpasses a critical threshold, denoted as Ic(H), where H indicates the intensity of the external magnetic field. This behavior can be elucidated by analyzing the motion of vortices within the clusters. Furthermore, we observe that the critical current density, Jc, exhibits a gradual decline as the temperature, T, rises. These insights contribute to a deeper understanding of vortex dynamics in FCCS superconductors and may have significant implications for experimental investigations involving high-Tc cuprate superconductors. The results underscore the importance of cluster connectivity and vortex behavior in the resistive transition of these materials, providing a framework for future research in the field of superconductivity. The study is relevant to the following PACS categories: 74.20.-q, 74.25.+s, 74.60.Bz, and 74.70.-k.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.41602560309064,
        "rewrite-fast-z-score": 2.182178902359924
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Relaxation of a dewetting contact line Part 1: A full-scale hydrodynamic calculation .\nAbstract:\nWe present the results of a numerical study on the relaxation dynamics of an initially straight contact line in a two-dimensional geometry, which is driven by surface tension and viscous dissipation at the moving interface between liquid and gas phases.  We solve the Navier-Stokes equations for incompressible fluids with free-slip boundary conditions using a spectral element method to simulate the flow field around the evolving droplet shape. The initial condition consists of a circular droplet sitting on top of a flat substrate that has been perturbed slightly away from its equilibrium position. As time evolves, we observe the formation of capillary waves along the contact line as well as the development of small satellite drops near the main droplet due to pinching off events. In addition, we find that the contact angle decreases continuously during this process until it reaches zero degrees when the entire droplet detaches from the substrate. Finally, we compare our simulation results against experimental data obtained from high-speed video microscopy measurements performed by other researchers.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Relaxation of a dewetting connection line Part 1 : A full - scale hydrodynamic calculation . Abstract : We report the results of a numerical investigation on the relaxation behavior of an initially straight contact line in a two - dimensional topology , which is generated by surface friction and viscous dissipation at the moved interface between liquid and gas phases .We calculate the Navier - Stokes equations for incompressible fluids with loose - slipping border conditions utilizing a spectral component process to simulate the flow field around the evolving droplet shape . The initial condition consists of a circular droplet standing on top of a flat substrate that has been perturbed slightly apart from its stable position .As period evolves , we monitor the formation of capillary currents along the contact line as also as the development of tiny satellite drops near the main droplet thanks to pinching off events . In addition , we find that the contact angle decreases continuously during this process until it meets zero degrees when the entire droplet detaches from the substrate .Finally , we compare our modeling results against empirical data received from high - speed tape microscopy observations performed by other researchers .",
        "rewrite_text": "Title: Relaxation of a Dewetting Connection Line Part 1: A Comprehensive Hydrodynamic Analysis\n\nAbstract: This study presents a detailed numerical analysis of the relaxation dynamics of an initially straight contact line within a two-dimensional framework, focusing on the effects of surface friction and viscous dissipation at the interface between liquid and gas phases. We employ the Navier-Stokes equations for incompressible fluids, incorporating loose-slip boundary conditions, and utilize a spectral component method to accurately simulate the flow field surrounding the evolving shape of a droplet. The initial configuration consists of a circular droplet positioned on a flat substrate, which is subjected to a slight perturbation from its equilibrium state. As time progresses, we observe the emergence of capillary currents along the contact line, as well as the formation of small satellite droplets adjacent to the primary droplet, resulting from pinching-off phenomena. Notably, we document a continuous decrease in the contact angle throughout this process, ultimately reaching zero degrees when the droplet completely separates from the substrate. To validate our computational findings, we compare our results with empirical data obtained from high-speed tape microscopy experiments conducted by other researchers. This comparison underscores the accuracy of our model and provides insights into the fundamental mechanisms governing droplet dynamics in dewetting scenarios. Our findings contribute to a deeper understanding of fluid behavior at interfaces, with potential implications for various applications in material science and engineering.",
        "ori-fast-z-score": -1.0206207261596576,
        "water-fast-z-score": 6.0609152673132645,
        "rewrite-fast-z-score": 2.5177629822488474
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Ageing memory and glassiness of a driven vortex system .\nAbstract:\nWe study the dynamics of an ensemble of interacting vortices in a two-dimensional superfluid helium film, which is driven by a rotating substrate at constant angular velocity . We show that this system exhibits aging behavior similar to spin glasses or other disordered systems with quenched randomness. The relaxation time increases exponentially as a function of waiting time t w , i.e., the duration during which the driving has been switched off before starting the measurement. This increase can be described by a stretched exponential law exp(−(t/τ)β), where τ denotes the characteristic relaxation time and β < 1 describes its distribution width. In addition we find that the response of our system depends on the history of the applied external drive. For example, if one starts with a high rotation frequency f 0 = ω0/2π and then decreases it slowly towards zero (quasi-static protocol), the final state after switching off the drive does not depend on the initial value f0. However, if one switches off the drive suddenly (sudden protocol), the final state strongly depends on f0.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Ageing memory and glassiness of a powered vortex structure . Abstract : We research the dynamics of an ensemble of interacting vortices in a two - dimensional superfluid helium movie , which is powered by a rotating substrate at constant angular velocity .We see that this scheme exhibits aging activity similar to spinning glasses or other disordered systems with quenched randomness . The relaxation time rises exponentially as a function of waiting period t v , i . e . , the duration during which the driving has been turned off before beginning the measurement .This increase can be described by a stretched exponential law exp ( − ( t / τ ) α ) , where τ represents the characteristic relaxation time and β < 1 refers its distribution length . In addition we find that the response of our system varies on the history of the applied external drive .For example , if one starts with a high rotation frequency f 0 = ω0 / 2π and then decreases it slowly towards zero ( quasi - static protocol ) , the first state after switching off the drive does not depend on the first value f0 . However , if one switches off the drive suddenly ( sudden protocol ) , the final state strongly depends on f0 .",
        "rewrite_text": "In this study, we investigate the dynamics of an ensemble of interacting vortices within a two-dimensional superfluid helium film, which is driven by a rotating substrate maintained at a constant angular velocity. Our findings reveal that this system exhibits aging phenomena akin to those observed in spin glasses and other disordered materials characterized by quenched randomness. Notably, we observe that the relaxation time of the system increases exponentially as a function of the waiting time \\( t_v \\), which represents the interval during which the driving force is turned off prior to measurement. This exponential growth can be effectively modeled using a stretched exponential function of the form \\( \\exp(- (t / \\tau)^\\beta) \\), where \\( \\tau \\) denotes the characteristic relaxation time and \\( \\beta < 1 \\) indicates the distribution of relaxation times within the system. Furthermore, our results indicate that the system's response is significantly influenced by the history of the external driving force applied. For instance, when the rotation frequency is initially set to a high value \\( f_0 = \\omega_0 / 2\\pi \\) and is subsequently decreased gradually to zero (a quasi-static protocol), the state of the system immediately after the drive is turned off is independent of the initial frequency \\( f_0 \\). Conversely, if the drive is abruptly switched off (a sudden protocol), the final state of the system exhibits a strong dependence on the initial frequency \\( f_0 \\). These findings contribute to a deeper understanding of the complex dynamics of vortex interactions in superfluid systems and highlight the intricate relationship between external driving conditions and the resulting system behavior.",
        "ori-fast-z-score": -1.237705495510552,
        "water-fast-z-score": 3.5068322372798972,
        "rewrite-fast-z-score": 0.3592106040535498
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Forced accretion in stochastically fed AGN and quasars .\nAbstract:\nWe present the results of cosmological simulations that follow the growth of supermassive black holes (SMBHs) by stochastic gas inflow, including radiative feedback effects on their surroundings. We find that SMBHs grow primarily through mergers with other BHs rather than gas accretion at high redshifts z > 6. At lower redshift we observe an increase in the fraction of mass gained via gas accretion relative to merger events. The resulting luminosity function is consistent with observations for both active galactic nuclei (AGNs) and quasars out to z = 7.5. Our model predicts a large number of low-luminosity AGNs which are not observed yet but may be detectable with future surveys such as LSST or Euclid. In addition, our model produces a population of obscured quasars whose properties agree well with recent observational constraints. Finally, we show that the predicted quasar lifetime distribution agrees very well with current estimates based on SDSS data.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Forced accretion in stochastically driven AGN and quasars . Abstract : We report the results of cosmological simulations that track the development of supermassive black holes ( SMBHs ) by stochastic gas inflow , particularly radiative feedback effects on their environment .We see that SMBHs grow primarily through mergers with other BHs rather than gas accretion at high redshifts z > 6 . At lower redshift we study an increase in the fraction of mass gained via gas accretion compared to merger events .The resulting luminosity function is compatible with observations for both active galactic nuclei ( AGNs ) and quasars out to z = 7 . 5 . Our model predicts a large number of low - luminosity AGNs which are not observed yet but might be detectable with current surveys such as LSST or Euclid .In addition , our model generates a population of distorted quasars whose characteristics comply better with recent observational restrictions . Finally , we prove that the expected quasar lifetime distribution agrees very best with current estimates based on SDSS information .",
        "rewrite_text": "We present findings from our cosmological simulations that investigate the evolution of supermassive black holes (SMBHs) influenced by stochastic gas inflow, with a particular focus on the effects of radiative feedback on their surrounding environment. Our analysis reveals that at high redshifts (z > 6), SMBHs predominantly grow through mergers with other black holes rather than through gas accretion. As we shift our focus to lower redshifts, we observe a notable increase in the proportion of mass acquired via gas accretion in comparison to merger events. The luminosity function derived from our simulations aligns well with observational data for both active galactic nuclei (AGNs) and quasars up to z = 7.5. Importantly, our model predicts a significant population of low-luminosity AGNs that have yet to be detected, which may be observable with ongoing surveys such as the Large Synoptic Survey Telescope (LSST) and the Euclid mission. Furthermore, our simulations yield a cohort of distorted quasars whose properties are more consistent with recent observational constraints. Lastly, we demonstrate that the anticipated distribution of quasar lifetimes closely matches current estimates derived from Sloan Digital Sky Survey (SDSS) data. These results contribute to a deeper understanding of the mechanisms driving SMBH growth and the implications for the evolution of AGNs and quasars in the universe.",
        "ori-fast-z-score": 0.47140452079103173,
        "water-fast-z-score": 5.500933918218137,
        "rewrite-fast-z-score": 1.5652475842498528
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Quantum and Classical Spins on the Spatially Distorted Kagome Lattice: Applications to Volborthite .\nAbstract:\nWe study quantum spin-1/2 systems with spatially anisotropic exchange interactions on the distorted kagome lattice, which is relevant for volborthite. We show that this system can be mapped onto an effective Heisenberg model in terms of classical spins residing at the centers of hexagons formed by nearest-neighbor bonds. The ground state phase diagram consists of three phases: ferromagnetic (FM), antiferromagnetic (AFM) and canted AFM states. In particular, we find that the FM order survives even when the distortion is strong enough to destroy it completely without spatial anisotropy. This result suggests that the magnetic properties of volborthite are governed not only by the interlayer coupling but also by the intralayer one. Furthermore, we discuss possible origins of the observed magnetization plateau in volborthite. \nI. INTRODUCTIO N\nThe distorted kagome lattice has attracted much attention recently because its structure is realized in several materials such as volborthite  1  , kapellasite  2  , herbertsmithite  3  , vesignieite  4  . These compounds have been studied extensively both experimentally  5  -  8  and theoretically  9  -  11  .\nIn particular, volborthite shows rich physical phenomena including a magnetization plateau around 1/3 of saturation magnetization M s  12 -  14  . It was suggested that these features originate from the presence of the distorted kagome layers  15  . However, there still remain many open questions about the microscopic mechanism behind them  16  . For example, what kind of interaction plays a crucial role? Is the distortion necessary or not?\nTo answer these questions, it would be useful to investigate the effect of the distortion systematically using theoretical methods  17  . Although some studies have already been done  18  -  20  , they were limited to small clusters and/or weak distortion cases. Therefore, it remains unclear how the distortion affects the magnetic properties of the distorted kagomé layer.\nIn this work, we study quantum spin-1/2 models with spatially anisotropic exchanges on the distorted kagomé lattice  see Figs. 1(",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Quantum and Classical Spins on the Spatially Distorted Kagome Lattice : Applications to Volborthite . Abstract : We explore quantum spin - 1 / 2 systems with spatially anisotropic exchange interactions on the altered kagome lattice , which is relevant for volborthite .We see that this scheme can be mapped onto an efficient Heisenberg model in terms of classical spins residing at the centers of hexagons formed by nearest - neighbor bonds . The ground state phase diagram consists of three stages : ferromagnetic ( FM ) , antiferromagnetic ( AFM ) and canted AFM states .In particular , we find that the FM order survives even when the interference is strong enough to destroy it completely without spatial anisotropy . This result suggests that the magnetic properties of volborthite are governed not only by the interlayer coupling but also by the intralayer one .Furthermore , we investigate possible origins of the reported magnetization plateau in volborthite . I . INTRODUCTIO N The distorted kagome lattice has drew much attention lately because its formation is realized in multiple structures such as volborthite 1 , kapellasite 2 , herbertsmithite 3 , vesignieite 4 .These compounds have been studied thoroughly both experimentally 5 - 8 and theoretically 9 - 11 . In particular , volborthite shows rich physical phenomena including a magnetization peak around 1 / 3 of saturation magnetization M s 12 - 14 .It was suggested that these characteristics derive from the presence of the altered kagome layers 15 . However , there still continue several open questions about the microscopic process behind them 16 .For instance , what sort of communication plays a crucial role ? Is the degradation required or not ?To answer these problems , it would be used to examine the impact of the distortion thoroughly using theoretical methods 17 . Although some experiments have already been performed 18 - 20 , they were restricted to small clusters and / or slight distortion cases .Therefore , it remains unsure how the interference affects the magnetic properties of the altered kagomé coating . In this research , we study quantum spin - 1 / 2 models with spatially anisotropic exchanges on the altered kagomé lattice see Figs .1(",
        "rewrite_text": "**Title:** Quantum and Classical Spins on the Spatially Distorted Kagome Lattice: Applications to Volborthite\n\n**Abstract:** This study investigates quantum spin-1/2 systems characterized by spatially anisotropic exchange interactions on a modified kagome lattice, which is particularly relevant to the material volborthite. We demonstrate that this system can be effectively represented by a Heisenberg model involving classical spins located at the centers of hexagons formed by nearest-neighbor bonds. The resulting ground state phase diagram reveals three distinct phases: ferromagnetic (FM), antiferromagnetic (AFM), and canted AFM states. Notably, our findings indicate that FM order persists even under conditions where strong interference would typically disrupt it in the absence of spatial anisotropy. This observation implies that the magnetic behavior of volborthite is influenced not only by interlayer coupling but also by intralayer interactions. Additionally, we delve into the potential origins of the observed magnetization plateau in volborthite. \n\nThe distorted kagome lattice has garnered significant interest due to its occurrence in various materials, including volborthite, kapellasite, herbertsmithite, and vesignieite. These compounds have been extensively examined through both experimental and theoretical approaches. Volborthite, in particular, exhibits intriguing physical phenomena, such as a magnetization peak at approximately one-third of its saturation magnetization. It has been proposed that these phenomena arise from the presence of distorted kagome layers. However, several questions remain regarding the underlying microscopic mechanisms, such as the nature of the interactions that are critical to these phenomena and whether degradation is a necessary factor. To address these questions, we employ theoretical methods to thoroughly analyze the effects of distortion. While some experimental studies have been conducted, they have primarily focused on small clusters or cases of slight distortion, leaving the impact of interference on the magnetic properties of the altered kagome lattice uncertain. This research aims to fill that gap by examining quantum spin-1/2 models with spatially anisotropic exchanges on the modified kagome lattice.",
        "ori-fast-z-score": 1.7025130615174973,
        "water-fast-z-score": 8.674164786985145,
        "rewrite-fast-z-score": 2.6106709553062086
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The absolute Galois group acts faithfully on the connected components of the moduli space of surfaces of general type .\nAbstract:\nWe prove that for any surface S of general type, the action of its absolute Galois group G(S) on the set of connected components of the modulis space M_g(S) is faithful.  This result has been conjectured by Grothendieck and proved in many cases (e.g., when g = 0 or 1).  We use this to show that if S admits an automorphism of order p > 2 then it also admits one of order q prime to p; we give examples where both orders are arbitrarily large. The proof relies on results about the existence of certain families of curves on S which have been obtained recently using techniques from algebraic geometry and number theory. In particular, we make essential use of the fact that the canonical map of such a curve C onto P^1 is birational; this implies that the image of C under the Albanese map Alb_S : S -> Alb_S(S) is not contained in a fiber of Alb_S.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The absolute Galois group acting faithfully on the linked components of the moduli space of surfaces of general type . Abstract : We prove that for any surface S of general type , the operation of its absolute Galois space G ( S ) on the group of connected parts of the modulis space M _ g ( S ) is faithful .This result has been conjectured by Grothendieck and demonstrated in many situations ( e . g . , when g = 0 or 1 ) . We use this to see that if S admits an automorphism of order q > 2 then it also admits one of order q prime to p ; we give instance where both orders are arbitrarily huge .The proof draws on findings about the existence of certain classes of curves on S which have been achieved lately utilizing techniques from algebraic topology and number theory . In particular , we give important use of the fact that the canonical mapping of such a curve C onto P ^ 1 is birational ; this implies that the image of C under the Albanese map Alb _ S : S - > Alb _ S ( S ) is not enclosed in a fiber of Alb _ S .",
        "rewrite_text": "Title: The Absolute Galois Group Acting Faithfully on the Linked Components of the Moduli Space of Surfaces of General Type\n\nAbstract: In this article, we establish that for any surface \\( S \\) of general type, the action of its absolute Galois group \\( G(S) \\) on the connected components of the moduli space \\( M_g(S) \\) is faithful. This significant result, which has been conjectured by Grothendieck, has been previously verified in various specific cases, particularly when the genus \\( g \\) is 0 or 1. Our findings extend this understanding to a broader context. Additionally, we demonstrate that if the surface \\( S \\) possesses an automorphism of order \\( q > 2 \\), it necessarily also has an automorphism of an order \\( q' \\) that is coprime to \\( p \\). We provide examples where both orders can be made arbitrarily large, showcasing the richness of the structure involved.\n\nThe proof leverages recent advancements in the study of specific classes of curves on \\( S \\), employing techniques from both algebraic topology and number theory. A crucial aspect of our approach is the utilization of the birational nature of the canonical map from a curve \\( C \\) to \\( \\mathbb{P}^1 \\). This property leads to the conclusion that the image of \\( C \\) under the Albanese map \\( \\text{Alb}_S: S \\to \\text{Alb}_S(S) \\) does not lie entirely within a single fiber of \\( \\text{Alb}_S \\). Our results not only affirm the conjecture regarding the faithful action of the Galois group but also open avenues for further exploration in the interplay between automorphisms of surfaces and their moduli spaces.",
        "ori-fast-z-score": -1.6876318513890358,
        "water-fast-z-score": 3.712790073055879,
        "rewrite-fast-z-score": -1.0660035817780522
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  And in the Darkness Bind Them: Equatorial Rings, B e  Supergiants, and the Waists of Bipolar Nebulae .\nAbstract:\nWe present new near-infrared polarimetric observations of the equatorial ring surrounding the evolved star HD 163296 (HR 5171 A). The data reveal that the ring is highly polarized at infrared wavelengths with an average polarization fraction of ~20%. We find no evidence for significant changes to this value over time scales ranging from years to decades. This result suggests that dust grains are not being destroyed or created on short timescales within the ring. In addition, we detect a small degree of circularly polarized light emerging from the central region of the ring which may be due to scattering off non-spherical particles such as ice crystals. \n \n We also report results from our analysis of archival Hubble Space Telescope images showing that the ring has remained remarkably stable since its discovery more than 20 years ago. Finally, we discuss how these findings can be used to constrain models of bipolar nebulae formation. Keywords: Polarization; Circumstellar matter",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : And in the Darkness Bind Them : Equatorial Rings , B e Supergiants , and the Waists of Bipolar Nebulae . Abstract : We report new near - infrared polarimetric studies of the equatorial ring surrounding the evolved star HD 163296 ( HR 5171 A ) .The data reveal that the ring is heavily polarized at infrared wavelengths with an estimated polarization fraction of ~ 20 % . We see no evidence for significant variations to this value over time ranges ranging from years to decades .This result suggests that dust grains are not being destroyed or created on short timescales within the circle . In addition , we perceive a small level of circularly polarized light arriving from the central region of the circle which may be due to scattering off non - cylindrical ions such as ice particles .We additionally report findings from our analysis of archival Hubble Space Telescope images indicating that the ring has remained remarkably steady since its observation more than 20 decades ago . Finally , we explain how these results can be used to constrain models of bipolar nebulae structure .Keywords: Polarization; Circumstellar matter",
        "rewrite_text": "Title: And in the Darkness Bind Them: Equatorial Rings, B Supergiants, and the Waists of Bipolar Nebulae\n\nAbstract: In this study, we present new near-infrared polarimetric observations of the equatorial ring surrounding the evolved star HD 163296 (HR 5171 A). Our findings indicate that the ring exhibits a significant level of polarization at infrared wavelengths, with an estimated polarization fraction of approximately 20%. Notably, we observe no substantial fluctuations in this polarization value over time spans ranging from years to decades. This stability implies that the dust grains within the ring are neither being formed nor destroyed on short timescales. Furthermore, we detect a minor amount of circularly polarized light emanating from the central region of the ring, which may result from scattering processes involving non-cylindrical ions, such as ice particles. Our analysis of archival Hubble Space Telescope images reveals that the ring has maintained a remarkable consistency since its initial observation over two decades ago. These observations provide critical insights into the physical conditions of the equatorial ring and contribute to our understanding of the dynamics of circumstellar matter. Additionally, we discuss the implications of our results for constraining theoretical models of bipolar nebulae structures, highlighting the importance of polarization studies in astrophysical research. Our work underscores the significance of long-term monitoring of such astronomical features to unravel the complexities of stellar evolution and the environments surrounding massive stars. \n\nKeywords: Polarization; Circumstellar matter",
        "ori-fast-z-score": -0.22941573387056174,
        "water-fast-z-score": 5.735393346764043,
        "rewrite-fast-z-score": 0.8081220356417685
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Aspects of stochastic resonance in reaction-diffusion systems: The nonequilibrium-potential approach .\nAbstract:\nWe study the effects of noise on spatially extended systems by using an extension of the concept of nonequilibrium potential (NEP). We show that NEPs can be used to characterize different types of stochastic resonances, such as those observed for excitable and bistable systems near their respective Hopf bifurcations. In particular we find that the presence of noise enhances the amplitude of oscillations in both cases but with very different mechanisms. For excitable systems this is due to the fact that noise increases the probability of crossing the threshold between two stable states; while for bistable systems it occurs because noise induces transitions between these states. Finally, we discuss how our results are related to previous studies based on other approaches. Stochastic resonance has been studied extensively during recent years  1  . It refers to the phenomenon whereby weak signals can be enhanced or detected more easily when they are embedded into a noisy background  2  .\nIn many physical situations, however, one needs to consider not only the effect of external noise sources but also internal fluctuations arising from the dynamics itself  3  . This problem becomes particularly relevant if the signal-to-noise ratio is small  4  , which may occur either because the signal is intrinsically weak or because its intensity is comparable to the level of intrinsic noise  5  . Moreover, even though the signal is strong enough so that it could be clearly distinguished without any additional noise  6  , there might still exist some optimal amount of noise that maximizes the detection efficiency  7, 8  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Aspects of stochastic resonance in reaction - diffusion processes : The nonequilibrium - potential approach . Abstract : We research the effects of noise on spatially extended systems by using an extension of the idea of nonequilibrium potential ( NEP ) .We see that NEPs can be used to characterize different kinds of stochastic resonances , such as those observed for excitable and bistable systems near their different Hopf bifurcations . In particular we find that the presence of noise enhances the frequency of oscillations in both cases but with very different mechanisms .For excitable systems this is due to the fact that noise changes the probability of reaching the threshold between two stable states ; while for bistable systems it appears because sound induces transitions between these states . Finally , we talk how our findings are related to previous research based on other methods .Stochastic resonance has been studied thoroughly during recent years 1 . It refers to the phenomenon whereby soft signals can be enhanced or detected more easily when they are embedded into a loud background 2 .In many mechanical circumstances , however , one needs to consider not only the impact of external sound sources but also internal fluctuations originating from the dynamics itself 3 . This problem arises particularly relevant if the signal - to - noise proportion is tiny 4 , which may happen either because the signal is intrinsically weak or because its brightness is equal to the level of intrinsic noise 5 .Moreover , even though the signal is strong enough so that it could be obviously differentiated without any additional noise 6 , there might nevertheless exist some optimal level of noise that maximizes the detection efficiency 7 , 8 .",
        "rewrite_text": "In this study, we investigate the influence of noise on spatially extended systems through an advanced framework known as the nonequilibrium potential (NEP) approach. Our findings reveal that NEPs serve as effective tools for characterizing various forms of stochastic resonance, particularly in excitable and bistable systems as they approach different Hopf bifurcations. Notably, we observe that noise plays a crucial role in enhancing the oscillation frequencies of these systems, albeit through distinct mechanisms. In excitable systems, the enhancement is attributed to noise altering the likelihood of reaching the threshold that separates two stable states. Conversely, in bistable systems, the presence of noise facilitates transitions between these states, driven by sound. \n\nFurthermore, we discuss the implications of our results in relation to existing literature that employs alternative methodologies. Stochastic resonance has garnered significant attention in recent years, defined as the phenomenon where weak signals are more readily detected when embedded within a noisy background. However, it is essential to consider not only external noise sources but also the internal fluctuations that arise from the system's dynamics. This consideration becomes particularly pertinent when the signal-to-noise ratio is low, which can occur when the signal is inherently weak or when its intensity is comparable to the level of intrinsic noise. Interestingly, even in scenarios where the signal is sufficiently strong to be discerned without additional noise, there may still exist an optimal noise level that maximizes detection efficiency. Our research contributes to a deeper understanding of these dynamics and highlights the complex interplay between noise and signal detection in reaction-diffusion processes.",
        "ori-fast-z-score": 0.2705008904002297,
        "water-fast-z-score": 6.171547617899419,
        "rewrite-fast-z-score": 3.005203820042827
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Deciphering top flavor violation at the LHC with B factories .\nAbstract:\nThe Large Hadron Collider (LHC) is expected to produce many new particles and discoveries in its Run II program, including possible evidence for physics beyond the Standard Model through rare decays such as b → sγ or t → cZ. The Belle experiment has recently measured these processes with unprecedented precision, providing important constraints on models that predict deviations from Standard Model expectations.  In this talk I will present an overview of recent results from Belle related to searches for new physics in rare decay modes involving charm quarks. These include measurements of:  - Branching fractions for charmless hadronic B meson decays; - CP asymmetries in neutral D mesons produced in B-meson decays; - Branching fraction for B(Bc→J/ψX). \nI will also discuss how these results can be used to constrain extensions of the Standard Model. Finally, I will briefly describe some future plans for Belle experiments.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Deciphering top flavor violation at the LHC with B factories . Abstract : The Large Hadron Collider ( LHC ) is expected to produce many new objects and discoveries in its Run II program , including likely evidence for physics beyond the Standard Model through rare decays such as b → sγ or t → cZ .The Belle study has recently recorded these mechanisms with incredible clarity , providing vital constraints on models that forecast deviations from Standard Model expectations . In this talk I will present an overview of recent results from Belle linked to searches for recent theory in rare decay modes involving charm quarks .These include measurements of : - Branching fractions for charmless hadronic B meson decays ; - CP asymmetries in neutral D mesons produced in B - meson decays ; - Branching fraction for B ( Bc→J / ψX ) . I will also discuss how these results can be used to constrain extensions of the Standard Model .Finally , I will briefly outline some future hopes for Belle experiments .",
        "rewrite_text": "Title: Deciphering Top Flavor Violation at the LHC with B Factories\n\nAbstract: The Large Hadron Collider (LHC) is poised to unveil a plethora of new particles and phenomena during its Run II program, potentially providing insights into physics that transcends the Standard Model. Among the anticipated discoveries are rare decay processes, such as b → sγ and t → cZ, which could serve as indicators of new physics. Recent investigations conducted by the Belle collaboration have captured these decay mechanisms with remarkable precision, yielding crucial constraints on theoretical models that predict deviations from Standard Model behavior. In this presentation, I will provide a comprehensive overview of the latest findings from Belle, particularly focusing on the exploration of rare decay modes involving charm quarks. Key measurements include the determination of branching fractions for charmless hadronic B meson decays, the analysis of CP asymmetries in neutral D mesons resulting from B meson decays, and the branching fraction for the decay Bc → J/ψX. These results not only enhance our understanding of flavor physics but also serve as vital tools for constraining various extensions of the Standard Model. Furthermore, I will briefly discuss the future prospects for Belle experiments, highlighting the potential for further discoveries that could illuminate the nature of flavor violation and its implications for our understanding of fundamental particles and forces. Through these efforts, we aim to deepen our comprehension of the underlying principles governing particle interactions and to explore the exciting possibilities that lie beyond the established framework of particle physics.",
        "ori-fast-z-score": 0.48507125007266594,
        "water-fast-z-score": 4.45427656417346,
        "rewrite-fast-z-score": 0.8867963503478639
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A data-analysis driven comparison of analytic and numerical coalescing binary waveforms: nonspinning case .\nAbstract:\nWe present an analysis of the accuracy with which different approximants to gravitational-wave (GW) signals emitted by coalescing binaries can be recovered using matched filtering techniques, in particular when applied to simulated detector noise. We use two sets of simulated data: one set generated numerically for equal-mass non-spinning black-hole binaries; another set produced analytically under the restricted post-Newtonian approximation. The latter is used as input into several families of approximate GW templates that are commonly employed in searches for compact-binary mergers. For each template family we perform a Bayesian parameter-estimation study on both synthetic datasets, varying the total mass M , dimensionless spin magnitude χ1z = |χ1|/M2, inclination angle ι between orbital angular momentum vector and line-of-sight, polarization angle ψ0, sky position angles θS and φS, time-of-arrival t0, phase offset ∆Φ0, and amplitude A. In addition, we also vary the distance D to the source. Our results show that all considered template families yield accurate estimates of the physical parameters of the system within their respective ranges of validity. However, there exist significant differences among them regarding how well they recover these parameters.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A data - analysis driven comparison of analytic and mathematical coalescing binary waveforms : nonspinning case . Abstract : We present an assessment of the accuracy with which different approximants to gravitational - wave ( GW ) transmissions generated by coalescing binaries can be recovered using matched filtering algorithms , in particular when applied to modeled detector noise .We use two sets of simulated evidence : one set produced numerically for equal - mass non - spinning black - hole binaries ; another set produced analytically under the restricted post - Newtonian approximation . The last is utilized as input into numerous families of approximate GW templates that are often employed in searches for compact - binary mergers .For each template family we perform a Bayesian parameter - estimation analysis on both synthetic datasets , changing the total mass M , dimensionless spin magnitude χ1z = | χ1 | / M2 , inclination angle η between orbital angular velocity vector and line - of - view , polarization angle ψ0 , sky position angles θS and φS , time - of - arrival t0 , phase offset [UNK] , and amplitude A . In addition , we also varied the distance D to the origin .Our results show that all considered template groups yield exact predictions of the physical factors of the system within their different ranges of relevance . However , there remain considerable variations among them regarding how well they recover these parameters .",
        "rewrite_text": "In this study, we evaluate the precision with which various approximants for gravitational wave (GW) signals from coalescing binary systems can be retrieved through matched filtering techniques, particularly in the context of simulated detector noise. Our analysis is grounded in two distinct sets of simulated data: one generated numerically for equal-mass, non-spinning black hole binaries, and the other derived analytically using the restricted post-Newtonian approximation. The latter serves as a basis for a variety of approximate GW templates commonly utilized in the detection of compact binary mergers. \n\nWe conduct a Bayesian parameter estimation analysis on both synthetic datasets, systematically varying several parameters, including the total mass (M), the dimensionless spin magnitude (χ1z = |χ1| / M²), the inclination angle (η) between the orbital angular momentum and the line of sight, the polarization angle (ψ₀), the sky position angles (θₛ and φₛ), the time of arrival (t₀), the phase offset, and the amplitude (A). Additionally, we explore variations in the distance (D) to the source.\n\nOur findings indicate that while all template families provide accurate predictions of the physical parameters of the binary system within their respective domains of applicability, there are significant discrepancies in their effectiveness at recovering these parameters. This highlights the importance of selecting appropriate template families for the analysis of GW signals, as the choice can substantially influence the accuracy of parameter estimation in gravitational wave astronomy. Overall, this work contributes to the understanding of the performance of different analytical and numerical approaches in the context of gravitational wave detection, particularly for non-spinning binary systems.",
        "ori-fast-z-score": -1.1141720290623112,
        "water-fast-z-score": 4.085297439895141,
        "rewrite-fast-z-score": -1.3206763594884356
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A large acceptance scintillator detector with wavelength shifting fibre read-out for search of eta-nucleus bound states .\nAbstract:\nThe detection of the hypothetical eta-mesic nuclei is one of the most promising ways to find new physics beyond Standard Model (SM). The experimental signature of such an exotic state would be a peak in the invariant mass distribution of the final-state particles produced by its decay, which can be observed as a bump on top of the smooth nuclear structure function background. In this work we present results obtained using Monte Carlo simulations and Geant4-based full simulation of the proposed experiment at JLab 12 GeV upgrade facility. We show that it will allow us to detect eta-mesic nuclei with high efficiency and good resolution over wide range of masses up to A = 100. This will provide unique opportunity to study properties of these exotic systems and test theoretical predictions. \n \n Keywords: eta-mesic nucleus, eta-nucleon interaction, eta production, eta-decay, eta-nuclear form factor, eta-nuclear potential",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A wide acceptance scintillator detector with spectral shifting fibre read - out for search of eta - nucleus bound states . Abstract : The detection of the hypothetical eta - mesic nuclei is one of the most exciting ways to find new physics beyond Standard Model ( SM ) .The empirical signature of such an exotic state would be a peak in the invariant mass distribution of the finished - state particles generated by its degradation , which can be viewed as a bump on top of the smooth nuclear structure function background . In this research we present results derived using Monte Carlo simulations and Geant4 - based full simulation of the proposed experiment at JLab 12 GeV upgrade facility .We see that it will provide us to identify eta - mesic nuclei with high efficiency and good sensitivity over broad range of masses up to A = 100 . This will provide unique opportunity to study properties of these exotic systems and question experimental assumptions .Keywords : eta - mesic nucleus , eta - nucleon collision , eta production , eta - decay , eta - atomic shape factor , eta - atomic potential",
        "rewrite_text": "**Title:** A Wide Acceptance Scintillator Detector with Spectral Shifting Fibre Read-Out for the Search of Eta-Nucleus Bound States\n\n**Abstract:** The quest for eta-mesic nuclei represents a promising avenue for uncovering new physics beyond the Standard Model (SM). These exotic states, if they exist, would manifest as distinct peaks in the invariant mass distribution of decay products resulting from their interactions, appearing as bumps superimposed on the smooth background of nuclear structure functions. In this study, we present findings obtained through Monte Carlo simulations and comprehensive Geant4-based simulations of a proposed experiment at the Jefferson Laboratory (JLab) 12 GeV upgrade facility. Our results indicate that the experimental setup is capable of efficiently identifying eta-mesic nuclei with high sensitivity across a wide mass range, extending up to A = 100. This capability not only enhances our understanding of these exotic systems but also allows us to critically evaluate existing experimental assumptions regarding their properties. The implications of this research are significant, as they open new avenues for exploring the fundamental interactions within nuclear physics and the potential existence of bound states that challenge current theoretical frameworks. The keywords associated with this study include eta-mesic nucleus, eta-nucleon collision, eta production, eta decay, eta atomic shape factor, and eta atomic potential, all of which are integral to the ongoing investigation into the nature of these elusive states. Through this work, we aim to contribute to the broader discourse on the fundamental constituents of matter and the forces that govern their interactions.",
        "ori-fast-z-score": -1.6081688022566922,
        "water-fast-z-score": 4.050813610756598,
        "rewrite-fast-z-score": 2.183063390230748
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Milagro Constraints on Very High Energy Emission from Short Duration Gamma-Ray Bursts .\nAbstract:\nWe present the results of an analysis of data taken by Milagro, which is sensitive to gamma rays with energies between 100 GeV and 10 TeV. We find that there are no significant excesses in the direction of any short duration GRB (T90 < 2 s) detected by BATSE or Swift during their respective lifetime. The upper limits we derive for these bursts range from 1x10^-6 photons/cm^2/s at 100 GeV up to 3x10^-5 photons/cm^2/s at 10 TeV. These constraints rule out models where the emission is dominated by inverse Compton scattering off relativistic electrons accelerated in internal shocks. \n \n Keywords: Gamma-ray bursts, Milagro Observatory, Upper limit, Internal shock model, Inverse Compton Scattering. Subject headings: Astrophysics - Gamma Ray Astronomy - Cosmic Rays - Space Weather - Fermi Bubbles - Blazars",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Milagro Constraints on Very High Energy Emission from Short Duration Gamma - Ray Bursts . Abstract : We present the results of an assessment of evidence performed by Milagro , which is sensitive to gamma radiation with energies between 100 GeV and 10 TeV .We see that there are no major excesses in the direction of any low duration GRB ( T90 < 2 s ) detected by BATSE or Swift during their respective lifetime . The higher limits we derive for these bursts vary from 1x10 ^ - 6 photons / cm ^ 2 / s at 100 GeV up to 3x10 ^ - 5 photons / cm ^ 2 / s at 10 TeV .These limitations rule out models where the emission is dominated by inverse Compton absorption off relativistic electrons accelerated in internal shocks . Keywords : Gamma - ray bursts , Milagro Observatory , Upper limit , Internal shock model , Inverse Compton Scattering .Subject headings : Astrophysics - Gamma Ray Astronomy - Cosmic Rays - Space Weather - Fermi Bubbles - Blazars",
        "rewrite_text": "We present the findings of an investigation conducted by the Milagro Observatory, which is designed to detect gamma radiation in the energy range of 100 GeV to 10 TeV. Our analysis focuses on short-duration gamma-ray bursts (GRBs), specifically those with a T90 duration of less than 2 seconds, as identified by the Burst and Transient Source Experiment (BATSE) and the Swift satellite during their operational periods. Notably, we found no significant excess of gamma-ray emissions in the direction of these low-duration GRBs. The upper limits we established for the gamma-ray flux from these bursts range from 1 x 10^-6 photons/cm^2/s at 100 GeV to 3 x 10^-5 photons/cm^2/s at 10 TeV. These constraints are critical as they effectively exclude certain theoretical models that propose that the gamma-ray emission from these GRBs is primarily driven by inverse Compton scattering involving relativistic electrons that are accelerated during internal shock processes. Our results contribute to the ongoing discourse in astrophysics regarding the mechanisms behind gamma-ray bursts and their associated emissions. The implications of these findings are significant for understanding the nature of GRBs and the conditions under which they occur. This work underscores the importance of high-energy observational data in refining theoretical models of gamma-ray emissions and enhances our comprehension of cosmic phenomena. \n\nKeywords: Gamma-ray bursts, Milagro Observatory, Upper limit, Internal shock model, Inverse Compton Scattering.  \nSubject headings: Astrophysics - Gamma Ray Astronomy - Cosmic Rays - Space Weather - Fermi Bubbles - Blazars.",
        "ori-fast-z-score": -1.6045149064768403,
        "water-fast-z-score": 1.4433756729740645,
        "rewrite-fast-z-score": 0.4975185951049946
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Satellites of Simulated Galaxies: survival, merging, and their relation to the dark and stellar halos .\nAbstract:\nWe study satellites in simulated galaxies with different masses at z = 0 using high-resolution cosmological hydrodynamic simulations (the Millennium Run). We find that there is no significant difference between the number density profiles of satellite galaxies around central galaxies with different luminosities or halo masses. The radial distribution of satellites shows an excess over the predictions based on the subhalo abundance matching technique for r < 30 kpc/h. This excess can be explained by tidal stripping of satellites before they are accreted onto the main galaxy. In addition, we show that the fraction of surviving satellites decreases rapidly as a function of distance from the center of host halos. Finally, we investigate how the properties of satellites depend on those of their hosts. Our results suggest that the majority of faint satellites may have been destroyed through mergers and/or tidal disruption during infall into larger systems. These findings provide important constraints on models of galaxy formation.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Satellites of Simulated Galaxies : survival , merging , and their connection to the dark and stellar halos . Abstract : We research satellites in simulated galaxies with various masses at z = 0 using high - resolution cosmological hydrodynamic simulations ( the Millennium Run ) .We see that there is no large change between the number density characteristics of satellite galaxies around central clusters with various luminosities or halo masses . The radial distribution of satellites displays an amount over the estimates based on the subhalo abundance matching methodology for r < 30 kpc / h .This excess can be described by tidal stripping of satellites before they are accreted onto the main galaxy . In addition , we find that the fraction of surviving satellites decreases quickly as a function of distance from the center of host halos .Finally , we investigate how the properties of satellites differ on those of their hosts . Our results show that the majority of faint satellites would have been destroyed through mergers and / or tidal disruption during infall into larger systems .These studies provide important restrictions on predictions of galaxy formation .",
        "rewrite_text": "**Title:** Satellites of Simulated Galaxies: Survival, Merging, and Their Connection to Dark and Stellar Halos\n\n**Abstract:** In this study, we investigate the behavior and characteristics of satellite galaxies within simulated galaxies of varying masses at redshift z = 0, utilizing high-resolution cosmological hydrodynamic simulations from the Millennium Run. Our analysis reveals that the number density profiles of satellite galaxies surrounding central clusters remain relatively consistent, regardless of the luminosity or halo mass of the host galaxy. Notably, we observe a significant excess in the radial distribution of satellites, particularly within a radius of 30 kpc/h, surpassing predictions made by the subhalo abundance matching technique. This discrepancy is attributed to the tidal stripping of satellites prior to their accretion onto the central galaxy. Furthermore, our findings indicate a rapid decline in the fraction of surviving satellites as one moves away from the center of host halos, highlighting the influence of gravitational forces on satellite retention. We also explore the relationship between the properties of satellites and those of their host galaxies, revealing that a substantial number of faint satellites are likely to have been obliterated through mergers or tidal disruptions during their infall into larger systems. These insights contribute valuable constraints to theoretical models of galaxy formation and evolution, enhancing our understanding of the complex dynamics governing satellite galaxies and their interactions with dark and stellar halos. Our results underscore the importance of considering these factors in future studies of galaxy formation and the role of satellite galaxies in the broader cosmic landscape.",
        "ori-fast-z-score": 0.5423261445466404,
        "water-fast-z-score": 5.682196434640312,
        "rewrite-fast-z-score": 0.641688947919748
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Constraints on transmission, dispersion, and density of states in dielectric multilayers and stepwise potential barriers with arbitrary layer arrangement .\nAbstract:\nWe present an exact solution for the scattering problem at normal incidence to a stack of N parallel layers separated by vacuum gaps or by stepwise potentials. The method is based on the transfer matrix approach combined with the Green s function technique. We derive explicit expressions for reflection coefficients as well as for the phase shifts between adjacent layers. These results are applied to calculate the optical properties of periodic structures such as Bragg reflectors and photonic crystals. In particular we discuss how the band structure can be obtained from the knowledge of the reflection coefficient only. Finally, we show that our formalism allows one to study also non-periodic systems like superlattices and quantum wells. \nI. INTRODUCTORY REMARK\nThe aim of this work is to develop a general theory which describes the propagation of waves through multilayer structures consisting of alternating layers of different materials. This includes both periodic (photonic) and aperiodic (superlattice-like) arrangements of layers. Our main interest lies in the calculation of the reflection and transmission coefficients as well as the phase shifts occurring upon passage through each individual layer. As will become clear below these quantities provide all information necessary to determine the electronic and optical properties of the system under consideration. \n \n A number of authors have studied the wave optics of multilayered media using various approaches  1  . Most of them were concerned with the case where the interfaces separating neighboring layers are flat  2  -  4  , i.e., they do not contain any steps in their profiles. However, it has been shown recently  5  that even small deviations from perfect periodicity may lead to dramatic changes in the physical behavior of the system. For example, if the interface profile contains a single step then the corresponding energy spectrum becomes discrete  6  . Moreover, the presence of steps leads to new types of excitations known as surface plasmons  7  . It should be noted here that the effects caused by the presence of steps cannot always be neglected since they often play an important role in determining the overall performance of devices made out of semiconductor heterostructures  8  . \n \n Another interesting feature associated with stepped interfaces is",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Constraints on transmission , dispersion , and density of states in dielectric multilayers and stepwise potential barriers with arbitrary layer arrangement . Abstract : We present an precise solving for the scattering issue at normal incidence to a stack of N parallel layers divided by vacuum gaps or by stepwise potentials .The method is based on the transfer matrix approach combined with the Green s function method . We derive explicit expressions for reflection values as well as for the phase transitions between neighboring layers .These data are applied to estimate the optical properties of periodic elements such as Bragg reflectors and photonic crystals . In particular we explain how the band structure can be obtained from the knowledge of the reflection coefficient only .Finally , we show that our formalism allows one to study also non - periodic systems like superlattices and quantum wells . I .INTRODUCTORY REMARK The goal of this project is to develop a general theory which explains the propagation of waves through multilayer structures consisting of alternating layers of different materials . This contains both periodic ( photonic ) and aperiodic ( superlattice - like ) arrangements of structures .Our main interest lies in the determination of the reflection and communication coefficients as also as the phase change occurring upon entry through each individual surface . As will become clear below these quantities offer all information required to obtain the optical and optical properties of the device under consideration .A several of authors have researched the wave optics of multilayered material utilizing diverse methods 1 . Most of them were involved with the case where the connections dividing neighboring layers are straight 2 - 4 , i . e . , they do not include any steps in their profiles .However , it has been shown recently 5 that even little deviations from good periodicity might lead to significant improvements in the physical dynamics of the system . For instance , if the interface profile contains a single stage then the associated energy spectrum becomes discrete 6 .Moreover , the presence of steps gives to novel sorts of excitations known as surface plasmons 7 . It should be mentioned here that the effects caused by the presence of steps cannot often be forgotten since they frequently play an important role in regarding the overall performance of structures making out of semiconductor heterostructures 8 .Another fascinating detail related with stepped interfaces is",
        "rewrite_text": "**Title:** Constraints on Transmission, Dispersion, and Density of States in Dielectric Multilayers and Stepwise Potential Barriers with Arbitrary Layer Arrangement\n\n**Abstract:** This study presents a comprehensive solution to the scattering problem encountered at normal incidence on a stack of N parallel layers, which are separated by vacuum gaps or stepwise potential barriers. Utilizing a combination of the transfer matrix method and Green's function techniques, we derive explicit formulas for reflection coefficients and analyze the phase transitions occurring at the interfaces between adjacent layers. These findings are instrumental in assessing the optical characteristics of periodic structures, such as Bragg reflectors and photonic crystals. Notably, we demonstrate that the band structure can be inferred solely from the reflection coefficient, providing a novel approach to understanding these systems. Furthermore, our formalism extends to the investigation of non-periodic configurations, including superlattices and quantum wells.\n\nThe primary objective of this research is to establish a generalized theoretical framework that elucidates wave propagation through multilayer structures composed of alternating materials. This encompasses both periodic (photonic) and aperiodic (superlattice-like) arrangements. Our focus is on determining the reflection and transmission coefficients, as well as the phase shifts that occur at each interface. These parameters are crucial for deriving the optical properties of the multilayer devices under consideration.\n\nPrevious studies have explored the wave optics of multilayer materials using various methodologies, primarily addressing cases where the interfaces between layers are smooth and continuous. However, recent findings indicate that even minor deviations from ideal periodicity can significantly enhance the physical behavior of the system. For example, the introduction of a single step in the interface profile can lead to a discrete energy spectrum. Additionally, such stepped interfaces can give rise to novel excitations, such as surface plasmons, which are critical for the performance of semiconductor heterostructures. The implications of these stepped interfaces are profound, as they often play a pivotal role in the overall functionality of multilayered optical devices.",
        "ori-fast-z-score": -0.14586499149789456,
        "water-fast-z-score": 8.610537802979817,
        "rewrite-fast-z-score": -0.9863939238321437
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Entangling Independent Photons by Time Measurement .\nAbstract:\nWe propose an experiment to entangle two independent photons in the time domain, using only linear optical elements and single-photon detectors.  The scheme is based on measuring the arrival times of the photons at different locations with respect to each other. We show that this measurement can be used to generate entanglement between the photons without any post-selection or feed-forward operations. This method may find applications for quantum communication networks where it would allow one to distribute entangled states over large distances. Entanglement plays a central role in many areas of physics ranging from condensed matter systems  1  , atomic gases  2  , and trapped ions  3  to quantum information processing  4  . In particular, entanglement has been shown to be essential for quantum teleportation  5  , superdense coding  6  , quantum key distribution  7  , and quantum computing  8  .\nIn recent years there have been several proposals to create entanglement between distant particles  9  -  11  . However, most schemes require either nonlinear interactions  12  , which are difficult to implement experimentally  13  , or postselection  14  , which introduces additional noise into the system  15  . Recently, we proposed a new scheme  16  to produce entanglement between remote particles using only linear optics  17  and single photon detection  18  . Our approach relies on performing measurements on the arrival times of the particles at different locations  19  . Here we present detailed calculations showing how our proposal works as well as its experimental feasibility  20  .  Figure 1 shows a schematic diagram of our setup. Two identical sources emit pairs of photons (red) towards Alice s station A and Bob s station B respectively  21  . Each source consists of a pulsed laser  22  generating pairs of photons via spontaneous parametric down-conversion  23  . These photons travel through separate paths until they reach stations A and B  24  . At these stations, Alice and Bob perform measurements on their respective photons  25  . They measure the arrival times tA and tB  26  of...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Entangling Independent Photons by Time Measurement . Abstract : We suggest an project to entangle two independent photons in the time realm , using only linear optical elements and single - photon detectors .The scheme is based on measuring the emergence periods of the photons at different places with regard to each other . We see that this measurement can be used to produce entanglement between the photons without any post - choice or feed - forward functions .This method may see useful for quantum communication connections where it would enable one to distribute entangled states over large distances . Entanglement plays a central role in multiple fields of mechanics ranging from condensed matter structures 1 , atomic atoms 2 , and trapped ions 3 to quantum information processing 4 .In particular , entanglement has been shown to be crucial for quantum teleportation 5 , superdense coding 6 , quantum key distribution 7 , and quantum computing 8 . In recent history there have been numerous ideas to create entanglement between distant particles 9 - 11 .However , most schemes need either nonlinear interactions 12 , which are hard to execute experimentally 13 , or postselection 14 , which introduces additional noise into the system 15 . Recently , we developed a new technique 16 to produce entanglement between distant particles utilizing only linear optics 17 and double photon detection 18 .Our solution involves on performing measurements on the entry rates of the particles at different places 19 . Here we present detailed calculations demonstrating how our proposal works as well as its empirical feasibility 20 .Figure 1 shows a schematic diagram of our setup . Two identical sources emit pairs of photons ( red ) towards Alice s station A and Bob s station B respectively 21 .Each source consists of a pulsed laser 22 producing sets of photons via spontaneous parametric down - transfer 23 . These photons travel through different paths until they reach stations A and B 24 .At these stations , Alice and Bob conduct measurements on their respective photons 25 . They measure the return times tA and tB 26 of . . .",
        "rewrite_text": "**Title:** Entangling Independent Photons by Time Measurement\n\n**Abstract:** In this article, we propose a novel approach to entangle two independent photons in the temporal domain, utilizing solely linear optical components and single-photon detectors. Our method hinges on the precise measurement of the arrival times of the photons at distinct locations, allowing us to establish entanglement without the need for post-selection or feed-forward mechanisms. This technique holds significant promise for enhancing quantum communication systems, as it facilitates the distribution of entangled states over extensive distances. Entanglement is a fundamental aspect of various domains within quantum mechanics, including condensed matter physics, atomic systems, and trapped ions, as well as in quantum information processing. It has been identified as essential for applications such as quantum teleportation, superdense coding, quantum key distribution, and quantum computing. Historically, numerous strategies have been proposed to generate entanglement between spatially separated particles; however, many of these approaches rely on nonlinear interactions, which are challenging to implement experimentally, or on post-selection methods that can introduce unwanted noise into the system. Recently, we have developed a technique that successfully generates entanglement between distant particles using only linear optical methods and double photon detection. Our approach involves measuring the arrival times of the photons at different locations, which we detail through comprehensive calculations that illustrate both the operational principles and empirical viability of our proposal. A schematic representation of our experimental setup is provided, depicting two identical photon sources emitting pairs of photons towards the respective stations of Alice and Bob. Each source is driven by a pulsed laser that generates photon pairs via spontaneous parametric down-conversion. The photons traverse distinct paths before reaching Alice's and Bob's stations, where they perform measurements on their respective photons, specifically recording the return times of the photons. This work lays the groundwork for practical applications in quantum communication and further exploration of entanglement generation techniques.",
        "ori-fast-z-score": -0.9011551125709446,
        "water-fast-z-score": 6.754444207800623,
        "rewrite-fast-z-score": -0.5659164584181103
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  HST Photometry for the Halo Stars in the Leo Elliptical NGC 3377 .\nAbstract:\nWe present new HST photometric data on halo stars in the nearby elliptical galaxy NGC 3377, obtained with the Wide Field Planetary Camera 2 (WFPC2). The observations were made as part of program GO-8491 and consist of two exposures taken through the F606W filter at different roll angles to allow for proper sky subtraction. We have used these images to measure magnitudes for more than 1000 candidate red giant branch (RGB) stars within an area of 1 arcmin radius centered around the galaxy s center. These measurements are compared to those derived by Kundu & Whitmore (1998) , who observed this same field using ground-based telescopes. Our results show good agreement between our photometry and that presented previously; however we find evidence for systematic differences which may be due to crowding effects or calibration uncertainties. \n \n Keywords: Red Giant Branch, Galaxy, WFPC2",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : HST Photometry for the Halo Stars in the Leo Elliptical NGC 3377 . Abstract : We report new HST photometric data on halo stars in the nearby elliptical galaxy NGC 3377 , obtained with the Wide Field Planetary Camera 2 ( WFPC2 ) .The surveys were made as part of program GO - 8491 and consist of two exposures took through the F606W filter at different roll directions to allow for appropriate sky subtraction . We have utilized these images to measure magnitudes for more than 1000 candidate red dwarf branch ( RGB ) stars within an area of 1 arcmin radius centered around the universe s center .These measurements are compared to those derived by Kundu & Whitmore ( 1998 ) , who studied this same field using ground - based telescopes . Our results show good agreement between our photometry and that presented previously ; however we find proof for widespread changes which may be due to crowding effects or calibration uncertainties .Keywords: Red Giant Branch, Galaxy, WFPC2",
        "rewrite_text": "We present new photometric observations of halo stars in the nearby elliptical galaxy NGC 3377, utilizing data acquired from the Hubble Space Telescope (HST) with the Wide Field Planetary Camera 2 (WFPC2). This study is part of program GO-8491, which involved two separate exposures taken through the F606W filter, oriented at different roll angles to facilitate effective sky subtraction. Our analysis focuses on measuring the magnitudes of over 1,000 candidate red giant branch (RGB) stars located within a 1 arcminute radius around the center of the galaxy. These measurements are then compared with those obtained by Kundu & Whitmore (1998), who conducted a similar investigation using ground-based telescopes in the same region. Our findings indicate a strong correlation between our HST-derived photometry and the previous ground-based results. Nevertheless, we also identify evidence of significant variations that may be attributed to factors such as crowding effects or potential calibration errors. This research contributes to the understanding of the stellar population in NGC 3377 and highlights the advantages of space-based observations in mitigating issues commonly encountered in ground-based studies. The implications of our results are crucial for further investigations into the characteristics and evolution of halo stars in elliptical galaxies. \n\nKeywords: Red Giant Branch, Galaxy, WFPC2.",
        "ori-fast-z-score": 1.016001016001524,
        "water-fast-z-score": 4.409585518440984,
        "rewrite-fast-z-score": 0.8432740427115678
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Curious Case of NGC6908 .\nAbstract:\nThe Hubble Space Telescope has revealed the most detailed view yet of an active galactic nucleus (AGN) in the galaxy NGC6908, located about 300 million light years away.  The AGN is powered by supermassive black holes that are surrounded by bright clouds of gas and dust called torii.   This image shows how these torii appear when they are illuminated by powerful radiation coming out of the central engine of the AGN.   ... Full text here . \n \n Image credits: NASA, ESA, STScI, A. Simionescu et al. (University of Leicester), DSS2, Digitized Sky Survey 2.0, Aladin sky atlas developed at CDS, Strasbourg Observatory; NRAO/AUI/NSF, National Radio Astronomy Observatory, which is operated by Associated Universities Inc., under cooperative agreement with the National Science Foundation. This work was supported by NASA grant NNX10AD65G to University of Leicester.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Curious Case of NGC6908 . Abstract : The Hubble Space Telescope has confirmed the most detailed view ever of an active galactic nucleus ( AGN ) in the galaxy NGC6908 , located about 300 million light years far .The AGN is powered by supermassive black holes that are surrounded by intense layers of gas and dust called torii . This image shows how these torii appear when they are illuminated by massive radiation coming out of the main motor of the AGN .. . . Full text here . Image credits : NASA , ESA , STScI , A . Simionescu et al .( University of Leicester ) , DSS2 , Digitized Sky Survey 2 . 0 , Aladin sky atlas constructed at CDS , Strasbourg Observatory ; NRAO / AUI / NSF , National Radio Astronomy Observatory , which is controlled by Associated Universities Inc . , under partnership agreement with the National Science Foundation . This project was supported by NASA gift NNX10AD65G to University of Leicester .",
        "rewrite_text": "Title: The Fascinating Case of NGC6908\n\nAbstract: The Hubble Space Telescope has provided an unprecedented and detailed observation of an active galactic nucleus (AGN) within the galaxy NGC6908, situated approximately 300 million light-years from Earth. This AGN is driven by supermassive black holes, which are enveloped by dense layers of gas and dust known as torii. The newly captured imagery reveals the intricate structure of these torii, which become visible when illuminated by the intense radiation emitted from the AGN's central engine. This study enhances our understanding of the dynamics and composition of AGNs, shedding light on the processes that govern their behavior and the surrounding environment. The findings underscore the significance of high-resolution observations in unraveling the complexities of distant galaxies. The research was made possible through the collaborative efforts of various institutions, with image credits attributed to NASA, ESA, STScI, A. Simionescu and colleagues from the University of Leicester, as well as contributions from the Digitized Sky Survey 2.0 and the Aladin sky atlas developed at the CDS, Strasbourg Observatory. Additionally, the National Radio Astronomy Observatory, operated by Associated Universities Inc. under a partnership with the National Science Foundation, played a crucial role in this project. This research was supported by a NASA grant (NNX10AD65G) awarded to the University of Leicester, highlighting the importance of funding in advancing our knowledge of the universe. The detailed observations of NGC6908 not only contribute to our understanding of AGNs but also pave the way for future explorations of similar cosmic phenomena.",
        "ori-fast-z-score": 0.2886751345948129,
        "water-fast-z-score": 4.714285714285714,
        "rewrite-fast-z-score": 1.2510864843424487
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Linked by Loops: Network Structure and Switch Integration in Complex Dynamical Systems .\nAbstract:\nWe study the relationship between network structure, dynamics, and function using an example system that is both complex and experimentally accessible -- the brain s neuronal circuitry. We show how to use statistical mechanics methods to analyze the functional properties of this system as it evolves over time through its underlying structural connectivity. In particular we focus on the role played by switches which are nodes with high degree centrality whose removal disconnects large portions of the network. Our results suggest that these switches play important roles in maintaining global dynamical stability while allowing for local flexibility. The manuscript is organized into four sections. Section I introduces our model and describes some basic features of the resulting networks. Section II presents numerical simulations showing how switching affects the evolution of the system. Section III analyzes the effect of removing switches on the networks  ability to maintain synchrony under perturbations. Finally section IV discusses possible extensions of our work. This article has been published in final form at http://arxiv.org/abs/1306.5995",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Linked by Loops : Network Structure and Switch Integration in Complex Dynamical Systems . Abstract : We research the relationship between network structure , dynamics , and function using an instance structure that is both complex and experimentally visible - - the brain s neuronal circuitry .We see how to use statistical mechanics algorithms to analyze the functional characteristics of this network as it evolves over time through its underlying structural connectivity . In particular we focus on the part played by switches which are networks with high degree centrality whose departure disconnects huge segments of the organization .Our results show that these switches act essential roles in maintaining international dynamical stability while providing for local flexibility . The manuscript is organized into four sections .Section I outlines our model and explains some fundamental characteristics of the resulting systems . Section II offers numerical simulations demonstrating how switching affects the evolution of the system .Section III analyzes the impact of eliminating switches on the systems ability to keep synchrony under perturbations . Finally section IV examines possible extensions of our work .This section has been publication in final form at http : / / arxiv . org / abs / 1306 . 5995",
        "rewrite_text": "**Title:** Linked by Loops: Network Structure and Switch Integration in Complex Dynamical Systems\n\n**Abstract:** In this study, we investigate the intricate interplay between network structure, dynamics, and functionality, using the brain's neuronal circuitry as a complex and experimentally accessible model. We employ statistical mechanics algorithms to analyze how the functional characteristics of this network evolve over time, driven by its underlying structural connectivity. A key focus of our research is on the role of switches—nodes within the network that exhibit high degree centrality. The removal of these switches can lead to the disconnection of substantial portions of the network, highlighting their critical importance. Our findings indicate that switches are vital for maintaining global dynamical stability while simultaneously allowing for local flexibility within the system. The manuscript is structured into four main sections. Section I introduces our model and delineates the fundamental properties of the systems we study. In Section II, we present numerical simulations that illustrate the effects of switching on the system's evolution. Section III delves into the consequences of removing switches, particularly regarding the system's capacity to maintain synchrony when subjected to perturbations. Lastly, Section IV explores potential extensions of our research, suggesting avenues for future investigation. This work contributes to a deeper understanding of how network structures influence dynamic behaviors in complex systems, with implications for both theoretical and practical applications. The complete manuscript is available at http://arxiv.org/abs/1306.5995.",
        "ori-fast-z-score": -1.1406468642034677,
        "water-fast-z-score": 6.807380225308036,
        "rewrite-fast-z-score": 2.5399463446357124
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Uncovering the Near-IR Dwarf Galaxy Population of the Coma Cluster with Spitzer IRAC .\nAbstract:\nWe present new near-infrared (NIR) observations of the Coma cluster using the Infrared Array Camera on board the Spitzer Space Telescope, which allow us to study the dwarf galaxy population in this rich environment for the first time at wavelengths longer than 1 micron. We identify and classify all galaxies detected within an area of 0.5 deg2 centered around the center of the Coma cluster down to a limiting magnitude of Ks = 18 mag. The majority of these sources are faint red galaxies that have been missed by previous optical surveys due to their low surface brightnesses. Using photometric redshift estimates we find that most of them lie between z=0.1 and z=1.0. By comparing our sample to existing spectroscopic data sets we show that our NIR selection is complete up to M* ~ - 17 + 5 log h70. This corresponds roughly to L*(z=0), but it should be noted that there may still exist some fainter dwarfs below our detection limit.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Uncovering the Near - IR Dwarf Galaxy Population of the Coma Cluster with Spitzer IRAC . Abstract : We report new near - infrared ( NIR ) observations of the Coma cluster using the Infrared Array Camera on board the Spitzer Space Telescope , which allow us to study the dwarf galaxy community in this rich environment for the first time at wavelengths greater than 1 micron .We recognize and classify all galaxies found within an area of 0 . 5 deg2 centered around the center of the Coma cluster down to a limiting magnitude of Ks = 18 mag . The majority of these sources are faint red clusters that have been missed by earlier optical searches due to their low exterior brightnesses .Using photometric redshift estimates we find that most of them lie between z = 0 . 1 and z = 1 . 0 . By comparing our sample to existing spectroscopic data sets we determine that our NIR selection is complete up to M * ~ - 17 + 5 log h70 .This equals roughly to L * ( z = 0 ) , but it should be mentioned that there may still exist some fainter dwarfs below our detection limit .",
        "rewrite_text": "We present new near-infrared (NIR) observations of the Coma cluster, conducted using the Infrared Array Camera (IRAC) aboard the Spitzer Space Telescope. This study marks the first comprehensive examination of the dwarf galaxy population within this rich cluster environment at wavelengths exceeding 1 micron. Our research encompasses a 0.5 square degree region centered on the Coma cluster, where we identify and classify all galaxies down to a limiting magnitude of Ks = 18 mag. Notably, a significant portion of these galaxies are faint red objects that previous optical surveys have overlooked due to their low surface brightness. Through the application of photometric redshift estimates, we ascertain that the majority of these dwarf galaxies are situated within the redshift range of z = 0.1 to z = 1.0. To validate our findings, we compare our NIR-selected sample with existing spectroscopic datasets, which allows us to conclude that our selection is complete for galaxies with absolute magnitudes of M* ~ -17 + 5 log h70. This threshold corresponds approximately to L* at z = 0. However, it is important to note that there may still be fainter dwarf galaxies that remain undetected below our established limits. Our findings contribute to a deeper understanding of the dwarf galaxy population in the Coma cluster, highlighting the significance of NIR observations in uncovering previously hidden members of this cosmic environment.",
        "ori-fast-z-score": 0.9847319278346618,
        "water-fast-z-score": 4.608176875690327,
        "rewrite-fast-z-score": 2.0628424925175866
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Long-Term Evolution of Massive Black Hole Binaries. III. Binary Evolution in Collisional Nuclei .\nAbstract:\nWe present the results of long-term numerical simulations of binary black hole (BBH) evolution, including gravitational radiation reaction and general relativistic effects such as frame dragging and tidal disruption. We focus on binaries with total mass M = 100-1000M⊙ that evolve through collisional nuclear environments at high redshifts z > 10. Our main goal is to study how BBHs can grow by accretion during their early stages of evolution when they are surrounded by dense gas clouds. In particular we investigate whether these systems can reach masses above 1000M⊙ before merging within a Hubble time. The initial conditions for our models were obtained using Monte Carlo sampling of the distribution function of isolated BBHs constructed by Belczynski et al. (2010) . For each model we performed several runs starting from different orbital configurations. All calculations were carried out assuming circular orbits. We find that most of the massive binaries merge within a few hundred million years after formation due to emission of gravitational waves. However, some of them survive until today if they form in regions where the density of surrounding gas exceeds $10^{9}$ cm$^{-3}$. These binaries may be detectable by future space-based gravitational wave observatories like LISA or DECIGO/BBO.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Long - Term Evolution of Massive Black Hole Binaries . III .Binary Evolution in Collisional Nuclei . Abstract : We present the conclusion of long - term numerical simulations of binary dark hole ( BBH ) development , notably gravitational radiation reaction and general relativistic effects such as frame dragging and tidal disruption .We focus on binaries with total mass M = 100 - [UNK] that develop through collisional nuclear habitats at high redshifts z > 10 . Our main goal is to study how BBHs can develop by accretion during their early stages of evolved when they are surrounded by dense gas clouds .In particular we investigate whether these systems can reach masses above [UNK] before merging within a Hubble time . The initial conditions for our models were obtained using Monte Carlo sampling of the distribution function of isolated BBHs constructed by Belczynski et al .( 2010 ) . For each model we performed numerous runs beginning from varying orbital configurations .All calculations were carried out assuming circular orbits . We see that most of the huge binaries unite within a few hundred million years after formed owing to emission of gravitational waves .However , some of them remain until today if they appear in areas where the density of neighbouring gas approaches $ 10 ^ { 9 } $ cm $ ^ { - 3 } $ . These binaries may be detectable by future space - based gravity wave observatories like LISA or DECIGO / BBO .",
        "rewrite_text": "We present the findings from our extensive numerical simulations focused on the long-term evolution of binary black hole (BBH) systems, particularly examining the influences of gravitational radiation and general relativistic phenomena, including frame dragging and tidal disruption. Our research centers on BBHs with a total mass in the range of 100 solar masses, which evolve within collisional nuclear environments at high redshifts (z > 10). The primary objective of this study is to explore the mechanisms through which BBHs can grow via accretion during their formative stages, especially when they are enveloped by dense gas clouds. A key aspect of our investigation is to determine whether these binary systems can achieve significant masses prior to merging within a Hubble time.\n\nTo establish the initial conditions for our simulations, we employed Monte Carlo sampling techniques based on the distribution function of isolated BBHs as outlined by Belczynski et al. (2010). Each model underwent multiple simulation runs, starting from a variety of orbital configurations, with all calculations conducted under the assumption of circular orbits. Our results indicate that the majority of massive BBHs coalesce within a few hundred million years post-formation, primarily due to the emission of gravitational waves. However, we also found that certain binaries can persist until the present day, particularly in regions where the surrounding gas density approaches 10^9 cm^-3. These surviving binaries hold the potential for detection by upcoming space-based gravitational wave observatories, such as LISA or DECIGO/BBO, thereby offering exciting prospects for future astrophysical research.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.902903378454601,
        "rewrite-fast-z-score": -0.4508348173337161
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Star Formation in the Bok Globule CB54 .\nAbstract:\nWe present near-infrared (NIR) imaging and spectroscopy of star formation activity in the Bok globule CB 54, which is located at a distance of about 1 kpc toward the Galactic anti-center direction. We found that there are two young stellar objects (YSOs), one Class I protostar with an infrared luminosity of ~10 Lsun and another embedded YSO candidate with a bolometric temperature of ~1000 K. The former object shows bipolar outflows traced by Herbig-Haro knots and molecular line wings. In addition to these two sources, we detected several other point-like NIR sources within the central region of CB 54. These may be low-mass pre-main-sequence stars or background galaxies. Our results suggest that this cloud core has experienced active star formation over its lifetime. \n \n Keywords: Star formation; Near-infrared spectroscopy; Bipolar outflow; Young stellar object",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Star Formation in the Bok Globule CB54 . Abstract : We present near - infrared ( NIR ) imaging and spectroscopy of star formation activity in the Bok globule CB 54 , which is situated at a distance of about 1 kpc toward the Galactic anti - center position .We showed that there are two young stellar bodies ( YSOs ) , one Class I protostar with an infrared luminosity of ~ 10 Lsun and another embedded YSO candidate with a bolometric temperature of ~ 1000 K . The former object displays bipolar outflows traced by Herbig - Haro knots and molecular line tails . In addition to these two sources , we spotted many other point - like NIR sources within the central region of CB 54 .These may be low - weight pre - principal - sequence stars or background galaxies . Our results show that this storm core has undergone active star formation over its lifetime .Keywords : Star formation ; Near - infrared spectroscopy ; Bipolar outflow ; Young stellar object",
        "rewrite_text": "We present a comprehensive study of star formation in the Bok globule CB54, utilizing near-infrared (NIR) imaging and spectroscopy to investigate the region's stellar activity. Located approximately 1 kiloparsec from the Galactic anti-center, CB54 serves as an intriguing site for examining the processes of star formation. Our observations reveal the presence of two young stellar objects (YSOs): a Class I protostar exhibiting an infrared luminosity of around 10 Lsun, and another embedded YSO candidate characterized by a bolometric temperature of approximately 1000 K. The Class I protostar is particularly noteworthy, as it demonstrates bipolar outflows, which are evidenced by the presence of Herbig-Haro knots and molecular line tails associated with its outflow activity. \n\nIn addition to these primary sources, we identified numerous other point-like NIR sources within the central region of CB54. These additional sources may represent low-mass pre-main-sequence stars or potentially background galaxies, indicating a rich and diverse stellar environment. Our findings suggest that the CB54 globule has experienced significant star formation activity throughout its existence, contributing to our understanding of the lifecycle of molecular clouds and the conditions conducive to star formation. This study enhances the existing knowledge of star formation processes in Bok globules and highlights the importance of NIR observations in uncovering the complexities of stellar birth in such regions. \n\nKeywords: Star formation; Near-infrared spectroscopy; Bipolar outflow; Young stellar object.",
        "ori-fast-z-score": -0.5,
        "water-fast-z-score": 3.0,
        "rewrite-fast-z-score": 0.5827715174143585
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Distortion Minimization in Gaussian Layered Broadcast Coding with Successive Refinement .\nAbstract:\nWe consider the problem of successive refinement coding for layered broadcast systems, where each receiver is interested only in one out of several layers and can decode all other layers as side information at no cost. We propose an algorithm to minimize distortion by jointly optimizing source coding parameters (quantizer step sizes) and channel coding parameters (channel code rates). The proposed algorithm has low computational complexity and performs close to optimal performance achieved by exhaustive search over all possible combinations of quantizers and codes. Our results show that our approach significantly improves upon existing algorithms which optimize either source or channel coding separately. \n \n Keywords: successive refinement coding, layered broadcast system, distortion minimization, joint optimization, rate-distortion theory, VBR video transmission \n \n \n \n 1 Introduction \n \n In recent years there have been many efforts devoted to developing efficient techniques for transmitting digital data such as audio-visual content over error-prone channels  1  . One important application area is broadcasting multimedia data to multiple receivers via wireless networks  2  , where it may be necessary to transmit different versions of the same signal simultaneously due to limited bandwidth resources  3  .\n \nIn this context, successive refinement coding  4  -  6  refers to a technique whereby a base layer containing coarse quality version of the original signal is transmitted first followed by additional enhancement layers providing higher resolution and/or better fidelity. Each receiver decodes its desired number of layers depending on available bandwidth and decoding capabilities. For example, if a user wants to view a high definition television program but does not own a smart TV capable of receiving HD signals, then he will receive only the base layer corresponding to standard definition (SD), while his smartphone would receive both SD and HD layers.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Distortion Minimization in Gaussian Layered Broadcast Coding with Successive Refinement . Abstract : We consider the issue of successive refinement compression for structured transmission systems , where each receiver is interested only in one out of several layers and can decode all other layers as side data at no price .We suggest an algorithm to minimize distortion by jointly optimizing source coding variables ( quantizer step lengths ) and channel coding variables ( channel code rates ) . The proposed algorithm has low computational complexity and performs close to optimal performance achieved by exhaustive search over all possible combinations of quantizers and coding .Our results show that our approach dramatically improves upon existing algorithms which optimize either source or channel code separately . Keywords : successive refinement compression , layered broadcast network , noise minimization , joint optimization , rate - noise theory , VBR television broadcasting 1 Introduction In recent years there have been many efforts devoted to developing optimal methods for transmitting digital data such as audio - visual content over mistake - susceptible channels 1 .One important use area is transmitting multimedia data to multiple receivers via telecommunications networks 2 , where it could be required to transmit different versions of the same signal concurrently due to limited bandwidth resources 3 . In this sense , successive refinement coding 4 - 6 relates to a technique whereby a base layer containing coarse quality version of the original signal is transmitted initially followed by additional enhancement layers supplying lower resolution and / or improved fidelity .Each receiver decodes its appropriate number of layers depending on available bandwidth and decoding capabilities . For instance , if a person wants to see a high resolution television show but does not own a smart TV capable of receiving HD transmissions , then he will receive only the base layer equivalent to standard definition ( SD ) , while his smartphone would receive both SD and HD layers .",
        "rewrite_text": "**Title:** Distortion Minimization in Gaussian Layered Broadcast Coding with Successive Refinement\n\n**Abstract:** This study addresses the challenge of successive refinement compression within structured transmission systems, where each receiver is focused on a specific layer while being able to decode additional layers as supplementary data without incurring extra costs. We propose a novel algorithm aimed at minimizing distortion by concurrently optimizing both source coding parameters (specifically, quantizer step sizes) and channel coding parameters (namely, channel code rates). The algorithm is designed to maintain low computational complexity while achieving performance levels that closely approximate the optimal results obtained through exhaustive searches across all potential combinations of quantizers and coding strategies. Our findings indicate that this approach significantly outperforms existing methods that typically optimize either source or channel coding independently. \n\nIn recent years, considerable attention has been directed towards developing effective techniques for transmitting digital data, particularly audio-visual content, over error-prone channels. A key application area is the transmission of multimedia data to multiple receivers via telecommunications networks, where it may be necessary to send different versions of the same signal simultaneously due to bandwidth constraints. Successive refinement coding is a relevant technique in this context, wherein an initial base layer containing a lower-quality version of the original signal is transmitted, followed by additional enhancement layers that provide improved resolution and fidelity. Each receiver decodes a specific number of layers based on its available bandwidth and decoding capabilities. For example, a viewer wishing to watch a high-definition television program on a non-HD capable device would receive only the base layer corresponding to standard definition, while a smartphone could decode both the standard and high-definition layers. This layered approach not only optimizes resource utilization but also enhances the viewing experience across diverse devices. \n\n**Keywords:** successive refinement compression, layered broadcast network, noise minimization, joint optimization, rate-noise theory, VBR television broadcasting.",
        "ori-fast-z-score": 0.0778498944161523,
        "water-fast-z-score": 6.6172410253729455,
        "rewrite-fast-z-score": -1.3491570401925506
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Rigorous Time-Domain Analysis of Full--Wave Electromagnetic Cloaking (Invisibility) .\nAbstract:\nWe present an exact time-domain analysis for the scattering by cloaks with arbitrary shapes and constitutive parameters, based on the method of moments in conjunction with the generalized sheet transition conditions. The results show that the cloaks can be designed to achieve perfect invisibility at any given frequency range within their operating bandwidths. We also demonstrate how the cloaks can be made broadband through optimizing their design parameters. Finally, we discuss some practical issues related to the implementation of such cloaks using metamaterials. C loak is one of the most fascinating concepts in electromagnetics  1  . It has been shown theoretically  2  , numerically  3  -  6  , and experimentally  7  -  9  that it is possible to hide objects completely inside certain types of electromagnetic cloak structures. However, all existing designs are limited to operate only over narrow bands around specific frequencies  10  .\nRecently, several groups have proposed different approaches to extend the operational bandwidth  11  -  13  . In particular, Li et al.  14  presented a new type of broadband cloaks which were constructed by cascading two or more layers of conventional cloaks together. Although this approach was able to significantly increase the bandwidth, its performance still suffered from significant losses due to multiple reflections between adjacent layers  15  . To overcome these problems, Liu et al.  16  introduced another class of broadband cloaks whose operation relies on the concept of transformation optics  17  . These cloaks consist of concentric shells of anisotropic materials arranged according to the coordinate transformations required to make the inner region appear as if it had transformed into free space  18  . This structure allows them to work effectively across a wide band of frequencies without suffering from large reflection loss  19  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Rigorous Time - Domain Analysis of Full - - Wave Electromagnetic Cloaking ( Invisibility ) . Abstract : We present an precise time - domain investigation for the scattering by cloaks with arbitrary shapes and constitutive characteristics , using on the method of moments in partnership with the generalized sheet transfer conditions .The results show that the cloaks can be designed to achieve perfect invisibility at any certain frequency spectrum within their operating bandwidths . We additionally prove how the cloaks can be made broadband through optimizing their design characteristics .Finally , we talk some practical matters related to the implementation of such cloaks using metamaterials . C loak is one of the most fascinating concepts in electromagnetics 1 .It has been shown theoretically 2 , numerically 3 - 6 , and experimentally 7 - 9 that it is easy to hide items completely inside particular kinds of electromagnetic mask walls . However , all available designs are limited to work only over limited bands around specific frequencies 10 .Recently , various groups have proposed different methods to stretch the operational bandwidth 11 - 13 . In particular , Li et al .14 provided a new kind of bandwidth cloaks which were built by cascading two or more strands of standard cloaks together . Although this methodology was able to significantly raise the bandwidth , its reliability nevertheless resulted from significant lost resulting to multiple reflections between neighboring layers 15 .To solve these problems , Liu et al . 16 introduced another class of broadband cloaks whose action relies on the idea of transformation optics 17 .These cloaks consist of concentric shells of anisotropic materials arranged according to the coordinate transformations required to make the inner region appear as if it had turned into free space 18 . This structure allows them to work effectively across a broad band of bandwidth without experiencing from huge reflection loss 19 .",
        "rewrite_text": "We present a comprehensive time-domain analysis of electromagnetic cloaking, focusing on the scattering behavior of cloaks with arbitrary geometries and material properties. Utilizing the method of moments in conjunction with generalized sheet transition conditions, our study reveals that cloaks can be engineered to achieve perfect invisibility across specific frequency ranges within their operational bandwidths. Furthermore, we demonstrate that by optimizing design parameters, these cloaks can be made to operate effectively over a broader frequency spectrum. \n\nThe concept of cloaking is one of the most intriguing topics in the field of electromagnetics. Theoretical, numerical, and experimental studies have established that it is feasible to completely conceal objects using specialized electromagnetic mask structures. However, existing designs typically function within narrow frequency bands, limiting their practical applications. Recent advancements have sought to extend the operational bandwidth of these cloaks. For instance, Li et al. proposed a novel approach that involves cascading multiple standard cloaks, which significantly enhances bandwidth but suffers from substantial losses due to multiple reflections between adjacent layers.\n\nTo address these challenges, Liu et al. introduced a new class of broadband cloaks based on transformation optics principles. These cloaks are constructed from concentric layers of anisotropic materials, arranged according to coordinate transformations that render the inner region indistinguishable from free space. This innovative design allows for effective operation across a wide frequency range while minimizing reflection losses. \n\nIn this article, we also discuss practical considerations for the implementation of these cloaks using metamaterials, highlighting the potential for real-world applications. Our findings contribute to the ongoing development of cloaking technologies, paving the way for advancements in stealth technology and electromagnetic manipulation.",
        "ori-fast-z-score": -0.5107539184552492,
        "water-fast-z-score": 7.7754191435023525,
        "rewrite-fast-z-score": -0.6810052246069989
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  One- and two-component bottle-brush polymers: simulations compared to theoretical predictions .\nAbstract:\nWe study the conformational properties of one- and two-component bottlebrush polymers in good solvent conditions by means of Monte Carlo (MC) simulations, using an off-lattice model with freely jointed chains. We compare our results for the radius of gyration Rg(N), end-to-end distance Ree(N), persistence length P(N), and contour length Lc(N) as functions of chain length N to those obtained within the framework of the worm-like-chain (WLC) theory. The WLC approach is found to be very accurate at predicting the scaling behavior of these quantities over several orders of magnitude in chain lengths. In particular, we find that the persistence length scales linearly with the number of monomers per backbone segment, which agrees well with recent experimental findings on bottle-brush polyelectrolytes. \n \n Keywords: Polymer brushes, Persistence length",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : One - and two - component bottle - brush polymers : simulations compared to theoretical estimates . Abstract : We research the conformational characteristics of one - and two - component bottlebrush polymers in good solvent circumstances by means of Monte Carlo ( MC ) simulations , using an off - lattice model with freely jointed chains .We relate our findings for the radius of gyration Rg ( N ) , end - to - end distance Ree ( N ) , persistence length P ( N ) , and contour distance Lc ( N ) as functions of chain length N to those acquired within the framework of the worm - like - chain ( WLC ) theory . The WLC approach is found to be very accurate at predicting the scaling behavior of these quantities over large orders of magnitude in chain lengths .In particular , we find that the persistence length varies linearly with the quantity of monomers per backbone segment , which accepts good with recent experimental discoveries on bottle - brush polyelectrolytes . Keywords : Polymer brushes , Persistence length",
        "rewrite_text": "Title: One- and Two-Component Bottlebrush Polymers: Simulations Compared to Theoretical Estimates\n\nAbstract: In this study, we investigate the conformational properties of one- and two-component bottlebrush polymers in good solvent conditions through Monte Carlo (MC) simulations, utilizing an off-lattice model characterized by freely jointed chains. Our analysis focuses on key metrics such as the radius of gyration \\( R_g(N) \\), end-to-end distance \\( R_{ee}(N) \\), persistence length \\( P(N) \\), and contour length \\( L_c(N) \\) as functions of the polymer chain length \\( N \\). We compare our simulation results with theoretical predictions derived from the worm-like chain (WLC) model. Our findings indicate that the WLC model provides a highly accurate description of the scaling behavior of these conformational properties across a wide range of chain lengths. Notably, we observe a linear relationship between the persistence length and the number of monomers per backbone segment, which aligns well with recent experimental observations concerning bottlebrush polyelectrolytes. This research contributes to a deeper understanding of the structural dynamics of bottlebrush polymers and highlights the effectiveness of MC simulations in elucidating complex polymer behaviors. The implications of our results extend to the design and application of polymer brushes in various fields, including materials science and biophysics. \n\nKeywords: Polymer brushes, Persistence length",
        "ori-fast-z-score": 0.6201736729460423,
        "water-fast-z-score": 4.431293675255978,
        "rewrite-fast-z-score": -0.21566554640687682
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dependence of hillslope moisture content on downhill saturation .\nAbstract:\nWe present an analytical solution for the dependence of soil moisture in a hillslope profile on saturation at its base, which is determined by groundwater table fluctuations and rainfall infiltration into the saturated zone. The model assumes that water moves downslope as gravity-driven flow through a porous medium with spatially variable hydraulic conductivity. We show how this simple conceptualization can be used to explain observed patterns of subsurface moisture distribution along hillslopes. Our results suggest that spatial variability in soil properties may play an important role in controlling hydrological processes within hillslopes. In particular, we find that topographic convergence leads to increased soil moisture near the bottom of the slope due to reduced drainage rates there. This effect becomes more pronounced when the local gradient increases or the hydraulic conductivity decreases towards the surface. These findings are consistent with field observations made during a recent study of hillslope hydrology conducted in northern California s Santa Ynez Mountains. \n \n Keywords: Hillslope hydrology, Groundwater table",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dependence of hillslope humidity content on downhill saturation . Abstract : We present an analytical solution for the dependence of soil rainfall in a hillslope profile on saturation at its base , which is changed by groundwater table fluctuations and rainfall infiltration into the saturated zone .The model proposes that water moves downslope as gravity - guided flow through a porous medium with spatially varying mechanical conductivity . We see how this straightforward conceptualization can be used to explain observed patterns of subsurface moisture circulation along hillslopes .Our results show that geographic variability in soil properties may play an important role in controlling hydrological processes within hillslopes . In particular , we find that topographic convergence leads to greater soil rainfall near the bottom of the ridge due to lowered irrigation rates there .This phenomenon grows more pronounced when the local gradient rises or the hydraulic conductivity decreases towards the surface . These conclusions are consistent with field observations made during a recent study of hillslope hydrology conducted in northern California s Santa Ynez Mountains .Keywords: Hillslope hydrology, Groundwater table",
        "rewrite_text": "Title: Dependence of Hillslope Humidity Content on Downhill Saturation\n\nAbstract: In this study, we present an analytical framework that elucidates the relationship between soil moisture content along a hillslope and the saturation levels at its base, which are influenced by fluctuations in the groundwater table and the infiltration of rainfall into the saturated zone. Our model conceptualizes water movement downslope as a gravity-driven flow through a porous medium characterized by spatially variable mechanical conductivity. This straightforward approach allows us to interpret the observed patterns of subsurface moisture dynamics along hillslopes effectively. Our findings indicate that the geographic variability of soil properties significantly impacts hydrological processes within these landscapes. Notably, we observe that topographic convergence enhances soil moisture accumulation near the base of the ridge, attributed to reduced irrigation rates in that area. This effect becomes more pronounced with increasing local gradient or decreasing hydraulic conductivity at the surface. These insights align with field data collected during a recent investigation of hillslope hydrology in the Santa Ynez Mountains of northern California, reinforcing the relevance of our model. Our research contributes to a deeper understanding of hillslope hydrology, emphasizing the critical role of soil characteristics and topography in shaping moisture distribution patterns. \n\nKeywords: Hillslope hydrology, Groundwater table, Soil moisture dynamics, Topographic convergence, Hydraulic conductivity.",
        "ori-fast-z-score": 1.3093073414159544,
        "water-fast-z-score": 6.616378963469013,
        "rewrite-fast-z-score": 1.876629726513673
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Geometry of curves with exceptional secant planes: linear series along the general curve .\nAbstract:\nThe present work is devoted to study some properties of curves in P^3 having an exceptional secant plane, i.e., such that there exists a line intersecting them at two points and not passing through any other point on the curve.  We give necessary conditions for a curve to have an exceptional secant plane (Proposition 1). Then we prove that if a curve has an exceptional secant plane then it lies on a quadric surface (Theorem 2).  Finally, using this result, we show how one can construct all possible families of curves with an exceptional secant plane by means of their associated linear systems (Theorems 3-7).\nKey words: Curve, Secant Plane, Quadric Surface. Mathematics Subject Classification (2000): 14C20, 14D10, 32S15. The research leading to these results was supported by the Russian Foundation for Basic Research under grant No. 02-01-00962-a",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Geometry of curves with outstanding secant planes : linear series along the general curve . Abstract : The present work is devoted to study some properties of curves in P ^ 3 having an exceptional secant plane , i . e . , such that there exists a line intersecting them at two points and not reaching through any other point on the curve .We get required requirements for a curve to have an exceptional secant plane ( Proposition 1 ) . Then we prove that if a curve has an exceptional secant plane then it lies on a quadric surface ( Theorem 2 ) .Finally , using this fact , we prove how one can build all possible families of curves with an exceptional secant plane by means of their associated continuous systems ( Theorems 3 - 7 ) . Key words : Curve , Secant Plane , Quadric Surface .Mathematics Subject Classification ( 2000 ) : 14C20 , 14D10 , 32S15 . The studies leading to these results was supported by the Russian Foundation for Basic Research under grant No .02-01-00962-a",
        "rewrite_text": "Title: Geometry of Curves with Exceptional Secant Planes: Linear Series Along the General Curve\n\nAbstract: This article investigates the geometric properties of curves in projective space \\( \\mathbb{P}^3 \\) that possess an exceptional secant plane. Specifically, we define an exceptional secant plane as one that contains a line intersecting the curve at precisely two points, without passing through any additional points on the curve. We establish necessary conditions for a curve to exhibit this property, articulated in Proposition 1. Following this, we demonstrate that any curve with an exceptional secant plane must reside on a quadric surface, as outlined in Theorem 2. This foundational result allows us to explore the construction of various families of curves with exceptional secant planes through their associated continuous systems, which we detail in Theorems 3 to 7. Our findings contribute to the understanding of the interplay between curves and their secant planes within the context of algebraic geometry. The implications of this research extend to the classification and characterization of curves in higher-dimensional projective spaces. The work presented here is supported by the Russian Foundation for Basic Research under grant No. 02-01-00962-a. \n\nKeywords: Curve, Secant Plane, Quadric Surface.  \nMathematics Subject Classification (2000): 14C20, 14D10, 32S15.",
        "ori-fast-z-score": 0.1259881576697424,
        "water-fast-z-score": 3.5,
        "rewrite-fast-z-score": 0.9428090415820635
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  High-resolution FUSE and HST ultraviolet spectroscopy of the white dwarf central star of Sh 2-216 .\nAbstract:\nWe present new high resolution (R = λ/Δλ ~ 20,000) far-ultraviolet spectra obtained with the Far Ultraviolet Spectroscopic Explorer (FUSE), as well as archival Hubble Space Telescope (HST) data for the hot white dwarf central star in the planetary nebula Sh2-216. The FUSE spectrum shows numerous absorption lines due to highly ionized species such as C IV, N V, O VI, Ne VIII, Mg X, Si XII, S XIV, Ar XVI, Fe XIX, and Ni XXVIII. We have modeled these features using synthetic line profiles generated by the non-LTE model atmosphere code TLUSTY/SYNSPEC. Our best-fit models indicate that this star has an effective temperature T eff = 120,000 K, surface gravity log g = 8.0, mass M = 0.6M☉ , radius R = 0.01R☉ , and is surrounded by a shell of material with density n(He II)/n(He I) = 1.5 x 10-3 .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : High - resolution FUSE and HST ultraviolet spectroscopy of the white dwarf central star of Sh 2 - 216 . Abstract : We report new high resolution ( R = λ / Δλ ~ 20 , 000 ) far - ultraviolet spectra obtained with the Far Ultraviolet Spectroscopic Explorer ( FUSE ) , as well as archival Hubble Space Telescope ( HST ) statistics for the hot white dwarf central star in the planetary nebula Sh2 - 216 .The FUSE spectrum displays several absorption patterns due to strongly ionized species such as C IV , N V , O VI , Ne VIII , Mg X , Si XII , S XIV , Ar XVI , Fe XIX , and Ni XXVIII . We have modeled these characteristics utilizing artificial line profiles generated by the non - LTE model atmosphere code TLUSTY / SYNSPEC .Our best - fitting models suggest that this star has an effective heat T eff = 120 , 000 K , surface gravity log f = 8 . 0 , mass M = 0 . 6M☉ , diameter R = 0 . 01R☉ , and is enclosed by a shell of material with concentration k ( He II ) / n ( He I ) = 1 . 5 x 10 - 3 .",
        "rewrite_text": "We present new high-resolution far-ultraviolet spectra (R = λ / Δλ ~ 20,000) obtained from the Far Ultraviolet Spectroscopic Explorer (FUSE), along with archival data from the Hubble Space Telescope (HST), focusing on the hot white dwarf central star of the planetary nebula Sh2-216. The FUSE spectra reveal a variety of absorption features attributed to highly ionized elements, including C IV, N V, O VI, Ne VIII, Mg X, Si XII, S XIV, Ar XVI, Fe XIX, and Ni XXVIII. To analyze these spectral characteristics, we employed artificial line profiles generated by the non-local thermodynamic equilibrium (non-LTE) model atmosphere code TLUSTY/SYNSPEC. Our modeling results indicate that the central star has an effective temperature (T_eff) of approximately 120,000 K, a surface gravity (log g) of 8.0, a mass of 0.6 solar masses (M☉), and a radius of 0.01 solar radii (R☉). Additionally, we found that the star is surrounded by a shell of material characterized by a helium ionization ratio of k(He II) / n(He I) = 1.5 x 10^-3. These findings contribute to our understanding of the physical properties of the white dwarf and its surrounding environment, providing insights into the evolutionary processes of stars in planetary nebulae. The high-resolution data from FUSE and HST enhance our ability to study the intricate details of stellar atmospheres and the dynamics of ionized gas in these fascinating astrophysical objects.",
        "ori-fast-z-score": -1.7232808737106582,
        "water-fast-z-score": 2.9541957835039856,
        "rewrite-fast-z-score": -0.8340576562282991
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Reexamination of spin decoherence in semiconductor quantum dots from equation-of-motion approach .\nAbstract:\nWe have studied the spin relaxation and dephasing processes in semiconductor quantum dots (QDs) by solving numerically the full set of equations for electron-electron interactions within the framework of the equation-of-motion method. We found that, at low temperatures, the dominant mechanism responsible for spin relaxation is due to spin-flip scattering with acoustic phonons. The calculated results are compared favorably with available experimental data on QD ensembles. In addition, we show that the inclusion of exchange interaction between electrons leads to an increase in the spin relaxation time as well as to a reduction in its temperature dependence. \n \n Spin dynamics plays an important role in many physical phenomena such as magnetic resonance imaging  1  , magneto-optical effects  2  , and spintronics  3  . Semiconductor quantum dots (QDs), which can be viewed as artificial atoms  4  , provide us with unique opportunities to study spin relaxation and dephazing mechanisms  5  -  8  . Recently, there has been considerable interest in studying these issues both experimentally  9  -  11  and theoretically  12  -  16  .\nIn this work, we investigate spin relaxation and dephazation processes in QDs using the equation-of-motion (EOM) method  17  . This method allows one to take into account all possible contributions to the self-energy arising from different types of electron-electron interactions including direct Coulomb repulsion, exchange-correlation potential, Hartree-Fock corrections, and correlation energy  18  . It should be noted that our calculations were performed without any additional approximations beyond those used in previous studies based on the EOM formalism  19  -  21  . \nThe obtained numerical results demonstrate that, at low temperatures T < 10 K, the main contribution to spin relaxation comes from spin-flip scattering with acoustic-phonon modes  22  . At higher temperatures, however, other mechanisms become more significant leading to faster spin relaxation times. Our theoretical predictions agree reasonably well with existing experimental data on QD ensembles  23  . \n \n Finally, it was shown that the inclusion of exchange interactions between electrons leads to an enhancement of the spin relaxation rate as well as to a decrease in its temperature dependence  24  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Reexamination of spin decoherence in semiconductor quantum dots from equation - of - movement technique . Abstract : We have researched the spin relaxation and dephasing mechanisms in semiconductor quantum dots ( QDs ) by solving numerically the full set of equations for electron - ion interactions within the framework of the equation - of - movement technique .We showed that , at low temperatures , the dominant mechanism causing for momentum relaxation is due to spinning - flip scattering with sound phonons . The measured data are compared favorably with provided experimental evidence on QD ensembles .In addition , we find that the introduction of exchange interaction between electrons contributes to an increase in the spin relaxation time as well as to a reduction in its temperature dependence . Spin dynamics plays an important role in many physical phenomena such as magnetic resonance imaging 1 , magneto - optical phenomena 2 , and spintronics 3 .Semiconductor quantum dots ( QDs ) , which can be viewed as synthetic elements 4 , provide us with special opportunities to study spinning contraction and dephazing processes 5 - 8 . Recently , there has been substantial interest in examining these problems both experimentally 9 - 11 and theoretically 12 - 16 .In this research , we investigate momentum relaxation and dephazation processes in QDs using the equation - of - movement ( EOM ) method 17 . This method enables one to take into consideration all possible contributions to the self - energy originating from multiple types of electron - ion interactions using direct Coulomb repulsion , transfer - correlation potential , Hartree - Fock corrections , and correlation power 18 .It should be mentioned that our calculations were performed without any additional approximations beyond those utilized in earlier analyses based on the EOM formalism 19 - 21 . The achieved numerical findings show that , at low temperatures T < 10 K , the main contribution to spinning contraction comes from spinning - flip diffusion with sound - phonon frequencies 22 .At higher temperatures , however , other mechanisms become more prominent leading to faster spin relaxation times . Our theory estimates agree reasonably well with existing experimental evidence on QD ensembles 23 .Finally , it was shown that the introduction of exchange interactions between electrons contributes to an enhancement of the spin relaxation time as well as to a reduction in its temperature dependence 24 .",
        "rewrite_text": "**Title:** Reexamination of Spin Decoherence in Semiconductor Quantum Dots Using the Equation-of-Movement Technique\n\n**Abstract:** This study investigates the mechanisms of spin relaxation and dephasing in semiconductor quantum dots (QDs) by numerically solving the complete set of equations governing electron-ion interactions through the equation-of-movement (EOM) technique. Our findings indicate that at low temperatures, the primary mechanism responsible for momentum relaxation is spin-flip scattering involving acoustic phonons. The numerical results align well with experimental data obtained from QD ensembles, reinforcing the validity of our approach. Furthermore, we demonstrate that incorporating exchange interactions between electrons leads to an increase in spin relaxation time and diminishes its dependence on temperature. Understanding spin dynamics is crucial for various physical phenomena, including magnetic resonance imaging, magneto-optical effects, and advancements in spintronics. Semiconductor quantum dots, often regarded as artificial atoms, present unique opportunities to explore spin relaxation and dephasing phenomena. Recent years have seen a surge of interest in these topics, both experimentally and theoretically. Our research employs the EOM method, which allows for a comprehensive consideration of all contributions to self-energy arising from various electron-ion interactions, including direct Coulomb repulsion, transfer-correlation potential, Hartree-Fock corrections, and correlation effects. Notably, our calculations were conducted without additional approximations beyond those used in previous EOM analyses. The results reveal that for temperatures below 10 K, spin relaxation is predominantly influenced by spin-flip diffusion associated with sound-phonon frequencies. In contrast, at elevated temperatures, alternative mechanisms emerge, resulting in shorter spin relaxation times. Our theoretical predictions are in reasonable agreement with existing experimental observations of QD ensembles. Ultimately, our study highlights the significant role of exchange interactions in enhancing spin relaxation times and reducing their temperature dependence, contributing to a deeper understanding of spin dynamics in semiconductor quantum dots.",
        "ori-fast-z-score": -0.07432941462471664,
        "water-fast-z-score": 9.487991253262095,
        "rewrite-fast-z-score": 0.5516772843673704
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Rapidity and energy dependence of the electric charge correlations in A+A collisions at the SPS energies .\nAbstract:\nThe rapidity and transverse momentum (pT) dependences of the electric charge correlation functions are studied for central Au+Au, d+Au and p+p collisions at RHIC and LHC energies using the AMPT model with string melting mechanism. The results show that there is no significant difference between the charge correlation functions obtained by different collision systems except for small differences around midrapidity region which may be due to the initial state effects. It can also be seen that the charge correlation function decreases as the center-of-mass energy increases. This behavior indicates that the strength of charge separation effect becomes weaker when going from lower to higher energies. Finally it should be noted that the charge correlation functions calculated here have been found to agree well with those measured experimentally. PACS numbers: 25.75.-q, 11.15.-x, 12.38.Mh  Electric charge fluctuations play an important role in understanding many interesting phenomena observed in heavy-ion collisions such as charge balance functions  1  , net-charge fluctuations  2  , etc.. In recent years, several experiments  3-6  have reported measurements on these quantities in various collision systems ranging from proton-proton(pp), deuteron-gold(d-Au) to gold-gold(Au-Au). These experimental data provide valuable information about the properties of hot and dense nuclear matter produced in high-energy nucleus-nucleus collisions  7-9  . However, theoretical studies on this subject still remain limited  10-12  .\nIn order to understand better the underlying physics behind these observations, we need more detailed investigations into the charge fluctuation phenomenon. One possible way to study charge fluctuations is through measuring the charge correlation functions  13-15  . Recently, some experimental groups  16-18  have presented their measurement on charge correlation functions in pp, d-Au and Au-Au collisions at RHIC and Large Hadron Collider (LHC) energies. On the other hand, the relativistic quantum molecular dynamics (RQMD)  19  and the parton-hadron-string dynamics (PHSD)  20  models predict that the charge correlation functions decrease rapidly towards zero",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Rapidity and energy dependence of the electric current correlations in A + A collisions at the SPS energies . Abstract : The rapidity and transverse momentum ( pT ) dependences of the electric charge relationship functions are studied for central Au + Au , d + Au and p + p collisions at RHIC and LHC energies using the AMPT theory with string melting system .The results show that there is no major variation between the charge interaction functions obtained by various crash processes except for little differences around midrapidity region which may be due to the early state effects . It can also be shown that the charge correlation function decreases as the center - of - mass energy rises .This phenomenon suggests that the strength of charge separation effect gets smaller when going from lower to higher energies . Finally it should be mentioned that the charge correlation functions measured here have been shown to agree well with those observed experimentally .PACS codes : 25 . 75 . - q , 11 . 15 . - x , 12 . 38 . Mh Electric charge fluctuations take an important role in understanding several interesting phenomena observed in heavy - ion collisions such as charge balance functions 1 , net - charge fluctuations 2 , etc . . In recent seasons , various study 3 - 6 have reported measurements on these quantities in different collision systems ranging from proton - proton ( pp ) , deuteron - silver ( d - Au ) to platinum - silver ( Au - Au ) .These observation findings provide valuable info about the properties of hot and dense nuclear material created in high - energy nucleus - nucleus collisions 7 - 9 . However , theoretical experiments on this subject still stay limited 10 - 12 .In order to realize clearer the fundamental theory behind these observations , we require more precise studies into the charge fluctuation phenomenon . One easy means to study charge fluctuations is through measuring the charge relationship values 13 - 15 .Recently , some experimental groups 16 - 18 have published their observation on charge interaction functions in pp , d - Au and Au - Au collisions at RHIC and Large Hadron Collider ( LHC ) energies . On the other hand , the relativistic quantum molecular mechanics ( RQMD ) 19 and the parton - hadron - string dynamics ( PHSD ) 20 models predict that the charge interaction functions decline rapidly towards zero",
        "rewrite_text": "**Title:** Rapidity and Energy Dependence of Electric Current Correlations in A + A Collisions at SPS Energies\n\n**Abstract:** This study investigates the rapidity and transverse momentum (pT) dependencies of electric charge correlation functions in central Au + Au, d + Au, and p + p collisions at the energies of the Relativistic Heavy Ion Collider (RHIC) and the Large Hadron Collider (LHC), utilizing the AMPT (A Multi-Phase Transport) model with a string melting approach. Our findings indicate that the charge correlation functions exhibit minimal variation across different collision types, with only slight discrepancies observed in the midrapidity region, potentially attributed to early-stage effects in the collisions. Notably, we observe a decrease in the charge correlation function as the center-of-mass energy increases, suggesting that the intensity of charge separation effects diminishes when transitioning from lower to higher energy regimes. Furthermore, the charge correlation functions derived from our analysis align closely with experimental measurements, reinforcing the validity of our theoretical framework. \n\nElectric charge fluctuations are pivotal for understanding various intriguing phenomena in heavy-ion collisions, including charge balance functions and net-charge fluctuations. Recent studies have reported measurements of these quantities across a range of collision systems, from proton-proton (pp) to deuteron-gold (d-Au) and gold-gold (Au-Au) interactions. These observations provide critical insights into the properties of the hot and dense nuclear matter produced in high-energy nucleus-nucleus collisions. However, theoretical investigations in this area remain relatively sparse. To deepen our understanding of the fundamental theories underlying these observations, further precise studies of charge fluctuation phenomena are essential. One effective approach to explore charge fluctuations is through the measurement of charge correlation values. Recent experimental efforts have yielded data on charge interaction functions in pp, d-Au, and Au-Au collisions at RHIC and LHC energies. In contrast, existing models such as the Relativistic Quantum Molecular Dynamics (RQMD) and Parton-Hadron-String Dynamics (PHSD) predict a rapid decline of charge interaction functions towards zero. This study aims to bridge the gap between theoretical predictions and experimental observations, enhancing our comprehension of charge dynamics in high-energy collisions. \n\n**PACS Codes:** 25.75.-q, 11.15.-x, 12.38.Mh",
        "ori-fast-z-score": -0.22423052782558076,
        "water-fast-z-score": 7.00634632949934,
        "rewrite-fast-z-score": 0.3104602102825331
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  TrES Exoplanets and False Positives: Finding the Needle in the Haystack .\nAbstract:\nWe present an overview of our efforts to find planets around stars with high proper motions using data obtained by the TrES space telescope, which was launched on December 6, 2005. We have found that many of these systems are likely false positives due to blending between the target star and nearby fainter companions or background objects. In this talk we will discuss how we used photometric variability combined with radial velocity measurements to identify these false positive cases. We also describe some of the techniques we use for identifying new candidate exoplanetary systems based on their light curves alone. Finally, I will show you what happens when one tries to measure the mass of a planet orbiting such a fast moving star as HD 128598 (Proxima Centauri). This is part of my Ph.D thesis work at Harvard University under the supervision of Dr. Paul Butler. My research interests include finding extrasolar planets via direct imaging and transit methods, studying the atmospheres of transiting exoplanets, and searching for habitable worlds beyond our solar system.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : TrES Exoplanets and False Positives : Finding the Needle in the Haystack . Abstract : We present an overview of our work to find planets around stars with high proper motions utilizing information obtained by the TrES space telescope , which was launched on December 6 , 2005 .We have discovered that several of these systems are likely false positives due to mixing between the target star and distant fainter companions or background objects . In this talk we will explore how we using photometric variability coupled with radial speed measurements to identify these false positive cases .We addition define some of the methods we using for finding new likely exoplanetary systems according on their light curves alone . Finally , I will show you what happens when one attempts to measure the mass of a planet orbiting such a rapidly moving star as HD 128598 ( Proxima Centauri ) .This is part of my Ph . D doctoral research at Harvard University under the guidance of Dr . Paul Butler . My research interests cover finding extrasolar stars via direct scanning and transit methods , studying the atmospheres of transiting exoplanets , and searching for habitable worlds beyond our solar system .",
        "rewrite_text": "Title: TrES Exoplanets and False Positives: Finding the Needle in the Haystack\n\nAbstract: This article provides a comprehensive overview of our research focused on identifying exoplanets around stars exhibiting high proper motions, utilizing data collected by the Trans-Atlantic Exoplanet Survey (TrES) space telescope, which was launched on December 6, 2005. Our findings indicate that a number of these stellar systems may be misidentified as exoplanetary candidates due to the blending of light from the target star with that of distant, fainter companions or background objects. In this presentation, we will discuss our methodology, which combines photometric variability analysis with radial velocity measurements to effectively distinguish these false positive scenarios. Additionally, we will outline the techniques employed to discover new potential exoplanetary systems based solely on their light curves. A significant portion of our research also involves the challenges encountered when attempting to measure the mass of a planet orbiting a rapidly moving star, exemplified by our case study of HD 128598 (Proxima Centauri). This work forms a crucial part of my Ph.D. dissertation at Harvard University, conducted under the mentorship of Dr. Paul Butler. My research interests encompass the detection of extrasolar planets through both direct imaging and transit methods, the investigation of the atmospheres of transiting exoplanets, and the exploration of potentially habitable worlds beyond our solar system. Through this study, we aim to enhance our understanding of exoplanet detection and the complexities involved in distinguishing genuine planetary signals from false positives in high proper motion star systems.",
        "ori-fast-z-score": -0.6469966392206304,
        "water-fast-z-score": 4.824506406770077,
        "rewrite-fast-z-score": 0.1889822365046136
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dust and gas emission in the prototypical hot core G29.96-0.02 at sub-arcsecond resolution .\nAbstract:\nWe present high angular resolution observations (0.1′′) of dust continuum, molecular line emissions, and maser lines toward the prototypical massive star-forming region G29.96-0 . 02 with the Submillimeter Array (SMA). The SMA data reveal that this source is composed of two compact components separated by 0.3 ′′ , which are associated with different physical processes. One component shows strong thermal dust emission peaking at 345 GHz as well as intense methanol masers distributed along an arc-like structure centered on it. This component coincides spatially with a bright infrared source detected by Spitzer Space Telescope. We propose that this component represents a hot core where massive star formation takes place. The other component exhibits weak dust continuum emission but strong SiO(5-4), SO2(34-26), CH3OH(7-6), and CH 3 CN(12-11) lines. These results suggest that this component may be tracing shocked regions driven by outflows or jets from young stellar objects embedded within the hot core.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dust and gas emission in the prototypical warm core G29 . 96 - 0 . 02 at sub - arcsecond resolution . Abstract : We report high angular resolution measurements ( 0 . 1 ′ ′ ) of dust continuum , molecular line emissions , and maser lines toward the prototypical massive star - creating area G29 . 96 - 0 .02 with the Submillimeter Array ( SMA ) . The SMA data reveal that this source is composed of two compact components differentiated by 0 . 3 ′ ′ , which are related with various physical processes .One component displays heavy thermal dust radiation peaking at 345 GHz as well as intense methanol masers distributed along an arc - like structure focused on it . This element coincides spatially with a bright infrared source detected by Spitzer Space Telescope .We suggest that this component indicates a hot component where massive star formation took place . The other component displays mild dust continuum emission but bright SiO ( 5 - 4 ) , SO2 ( 34 - 26 ) , CH3OH ( 7 - 6 ) , and CH 3 CN ( 12 - 11 ) lines .These data suggest that this component may be tracing shocked regions driven by outflows or jets from young stellar bodies lodged within the hotter core .",
        "rewrite_text": "We present high-resolution observations (0.1\") of dust continuum, molecular line emissions, and maser lines in the well-studied massive star-forming region G29.96-0.02, utilizing the Submillimeter Array (SMA). Our findings reveal that this region consists of two distinct compact components separated by 0.3\". The first component is characterized by significant thermal dust emission peaking at 345 GHz, accompanied by a concentration of intense methanol masers arranged in an arc-like formation. This component aligns spatially with a prominent infrared source identified by the Spitzer Space Telescope, suggesting it represents a hot region indicative of ongoing massive star formation. In contrast, the second component exhibits a more subdued dust continuum emission but is notable for its strong molecular line emissions, including SiO (5-4), SO2 (34-26), CH3OH (7-6), and CH3CN (12-11). These observations imply that this component may be associated with shocked regions resulting from outflows or jets originating from young stellar objects located within the hotter core. Our study enhances the understanding of the physical processes at play in G29.96-0.02, shedding light on the dynamics of star formation and the interactions between stellar outflows and the surrounding material in this prototypical warm core. The detailed sub-arcsecond resolution of our measurements provides critical insights into the complex structure and activity within this significant star-forming region, contributing to the broader knowledge of massive star formation mechanisms in the universe.",
        "ori-fast-z-score": -1.3627702877384937,
        "water-fast-z-score": 3.8786538958710977,
        "rewrite-fast-z-score": 0.09245003270420485
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Adjointability of densely defined closed operators and the Magajna-Schweizer Theorem .\nAbstract:\nWe prove that if an operator is adjointable, then it has dense range in its domain.  We also show how this result can be used to give another proof of the Magajna-Schwizer theorem on the existence of self-adjoint extensions for densely defined closed operators with equal deficiency indices. Let H be a Hilbert space over C. An operator T : D(T) → H is said to have dense range (or simply DR), where D(T) ⊆ H, if for every vector x ∈ H there exists a sequence {xn}n∈N such that limn→∞||xn−Tx||=0. In other words, the image of T contains no proper subspace of H. The following theorem shows that any adjointable operator must necessarily have dense range.   THEOREM 1.1. If T:D(T)⊆H→H is adjointable, then R(T∗)=D(T).",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Adjointability of densely defined closed operators and the Magajna - Schweizer Theorem . Abstract : We establish that if an operator is adjointable , then it has deep range in its domain .We additionally prove how this consequence can be used to give another proof of the Magajna - Schwizer theorem on the existence of self - adjoint extensions for tightly defined closed operators with equal deficiency indices . Let H be a Hilbert space over C . An function T : D ( T ) → H is said to have dense range ( or simply DR ) , where D ( T ) ⊆ H , if for every matrix x ∈ H there exists a sequence { xn } n∈N such that limn→∞ | | xn−Tx | | = 0 .In other words , the image of T contains no complete subspace of H . The following theorem shows that any adjointable operator must necessarily have dense range . THEOREM 1 . 1 .If T:D(T)⊆H→H is adjointable, then R(T∗)=D(T).",
        "rewrite_text": "Title: Adjointability of Densely Defined Closed Operators and the Magajna-Schweizer Theorem\n\nAbstract: In this article, we investigate the properties of adjointable operators, particularly focusing on their implications for the range of such operators within their domain. We demonstrate that if an operator is adjointable, it necessarily possesses a dense range. This finding is significant as it provides a foundational understanding of the behavior of densely defined closed operators. Furthermore, we utilize this result to offer an alternative proof of the Magajna-Schweizer theorem, which addresses the existence of self-adjoint extensions for tightly defined closed operators that exhibit equal deficiency indices.\n\nTo elaborate, let H represent a Hilbert space over the complex numbers, C. We define an operator T: D(T) → H, where D(T) is a subset of H, to have a dense range (referred to as DR) if for every vector x in H, there exists a sequence {xn} indexed by natural numbers such that the limit of the norm ||xn - Tx|| approaches zero as n tends to infinity. This characterization implies that the image of T does not contain any complete subspace of H.\n\nOur main theorem asserts that any adjointable operator must have a dense range. Specifically, we state that if T is an adjointable operator from D(T) to H, then the range of its adjoint operator T* is equal to the domain D(T). This result not only reinforces the connection between adjointability and the density of the operator's range but also lays the groundwork for further exploration of self-adjoint extensions in the context of tightly defined closed operators. The implications of our findings extend to various applications in functional analysis and operator theory, enhancing the understanding of operator extensions and their properties in Hilbert spaces.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 2.2517050070105746,
        "rewrite-fast-z-score": 0.7427813527082074
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Spectral methods for orthogonal rational functions .\nAbstract:\nWe consider the problem of approximating an unknown function f in L^2(0,1) by a linear combination of orthonormal polynomials with respect to some weight function w on (0,1). We show that if we choose the weights so as to minimize the error between the approximation and the true solution then this leads to a system of equations which can be solved using spectral methods. The resulting algorithm is shown to have optimal convergence rates under certain conditions. In particular it has been proved recently that if the coefficients of the expansion are chosen optimally then the rate of convergence is O(N^{-2}), where N denotes the number of terms used in the expansion. \nThe main purpose of this thesis was to implement these algorithms numerically and compare their performance against other existing techniques such as Chebyshev expansions or Legendre expansions. This work will also allow us to investigate whether there exist any advantages associated with choosing different types of basis functions when solving differential equations.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Spectral methods for orthogonal rational functions . Abstract : We consider the question of approximating an unknown function f in L ^ 2 ( 0 , 1 ) by a linear mixture of orthonormal polynomials with regard to some weight function w on ( 0 , 1 ) .We see that if we choose the weights so as to minimize the error between the approximation and the true answer then this results to a system of equations which can be answered using spectral algorithms . The resulting algorithm is demonstrated to have optimal convergence rates under certain conditions .In particular it has been shown lately that if the coefficients of the expansion are chosen optimally then the frequency of convergence is O ( N ^ { - 2 } ) , where N refers the number of words using in the expansion . The main aim of this dissertation was to apply these algorithms numerically and compare their performance against other existing techniques such as Chebyshev expansions or Legendre expansions .This research will also enable us to examine whether there exist any advantages associated with choosing particular kinds of basis systems when solving differential equations .",
        "rewrite_text": "In this article, we explore the approximation of an unknown function \\( f \\) in the space \\( L^2(0, 1) \\) through a linear combination of orthonormal polynomials, weighted by a specific weight function \\( w \\) defined on the interval \\( (0, 1) \\). Our investigation reveals that by selecting the weights to minimize the approximation error, we derive a system of equations that can be effectively solved using spectral algorithms. The proposed algorithm demonstrates optimal convergence rates under certain conditions, particularly when the coefficients of the polynomial expansion are chosen optimally. Recent findings indicate that the rate of convergence is \\( O(N^{-2}) \\), where \\( N \\) denotes the number of terms utilized in the expansion. The primary objective of this research is to implement these spectral algorithms numerically and evaluate their performance in comparison to established methods, such as Chebyshev and Legendre expansions. Additionally, this study aims to investigate the potential advantages of employing specific types of basis systems in the context of solving differential equations. By conducting these comparisons and analyses, we hope to contribute valuable insights into the effectiveness of spectral methods for approximating functions and their implications for numerical solutions of differential equations.",
        "ori-fast-z-score": -1.4444444444444444,
        "water-fast-z-score": 3.754671886544782,
        "rewrite-fast-z-score": -0.8164965809277261
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Modified p-modes in penumbral filaments? .\nAbstract:\nWe report on the detection of an oscillatory pattern with periods between 5 and 20 minutes, which is observed to be associated with sunspots  penumbrae. The oscillations are detected by applying wavelet analysis to time series obtained from high-resolution observations made at the Swedish 1-m Solar Telescope (SST). We find that these oscillations have amplitudes up to 0.5 km/sec and occur preferentially along the edges of penumbral filaments. They appear to be confined within a narrow range of heliocentric angles, centered around 30 degrees. These results suggest that they may represent modified acoustic waves trapped inside magnetic flux tubes. This work was supported by NASA grants NAG5-7067 and NNG04GK85G. Sunspot penumbrae exhibit a variety of phenomena including rapid flows, brightenings, and darkening events known as Evershed flow, umbral flashes, and moat flows respectively. In this Letter we present evidence for another phenomenon occurring in penumbral regions -oscillations with periods ranging from 5-20 minutes.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Modified p - modes in penumbral filaments ? .Abstract : We report on the observation of an oscillatory pattern with periods between 5 and 20 minutes , which is observed to be correlated with sunspots penumbrae . The oscillations are detected by using wavelet study to time series acquired from high - resolution measurements made at the Swedish 1 - m Solar Telescope ( SST ) .We see that these oscillations have amplitudes up to 0 . 5 kilometres / sec and occur preferentially along the edges of penumbral filaments . They seem to be confined within a thin range of heliocentric angles , centered around 30 degrees .These data suggest that they may represent enhanced sound signals stuck inside magnetic flux tubes . This research was supported by NASA grants NAG5 - 7067 and NNG04GK85G .Sunspot penumbrae exhibit a variety of phenomena including rapid flows , brightenings , and darkening phenomena known as Evershed flow , umbral flashes , and moat flows respectively . In this Letter we present evidence for another feature occurring in penumbral regions - oscillations with periods ranging from 5 - 20 minutes .",
        "rewrite_text": "Title: Modified p-modes in Penumbral Filaments\n\nAbstract: In this study, we present observations of an oscillatory phenomenon associated with sunspot penumbrae, characterized by periods ranging from 5 to 20 minutes. Utilizing wavelet analysis on time series data obtained from high-resolution imaging at the Swedish 1-m Solar Telescope (SST), we identify these oscillations, which exhibit peak amplitudes of up to 0.5 kilometers per second. Notably, these oscillations are predominantly located along the peripheries of penumbral filaments and appear to be confined to a narrow range of heliocentric angles, centered around 30 degrees. Our findings suggest that these oscillations may represent enhanced acoustic signals trapped within magnetic flux tubes, contributing to the complex dynamics observed in sunspot regions. The study of sunspot penumbrae has revealed a variety of phenomena, including rapid flows, brightenings, and the well-documented Evershed flow, as well as umbral flashes and moat flows. In this Letter, we introduce evidence for an additional feature within penumbral areas—oscillations with distinct periodic behavior. This research was made possible through the support of NASA grants NAG5-7067 and NNG04GK85G, highlighting the importance of continued exploration into the intricate behaviors of solar phenomena and their implications for our understanding of solar dynamics. Our observations not only enhance the existing knowledge of sunspot penumbrae but also open new avenues for investigating the underlying mechanisms driving these oscillatory patterns in solar magnetic structures.",
        "ori-fast-z-score": 0.5852057359806528,
        "water-fast-z-score": 4.965212315030781,
        "rewrite-fast-z-score": 0.8783100656536799
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Binary Quantum Search .\nAbstract:\nWe present an algorithm for quantum search that is based on the Grover s algorithm and uses only two qubits to represent one item in the database, which can be viewed as a binary number. The algorithm has been implemented using IBM Q Experience simulator with four different databases containing up to 16 items each. We have also compared our results against those obtained by running Grover s original algorithm on the same datasets. Our experimental results show that the proposed algorithm performs better than its classical counterpart when searching through small databases (up to 8 items). However, it becomes less efficient if we increase the size of the database beyond this limit. This work was supported by the Australian Research Council Discovery Project DP160103745. In recent years there has been significant interest in developing algorithms for performing quantum searches over large data sets  1  . These algorithms are expected to find applications in areas such as machine learning  2  , pattern recognition  3  , computer vision  4  , bioinformatics  5  , etc., where they will allow us to solve problems faster or more accurately  6  .\nIn general, these algorithms use N qubits to encode M elements in the database  7, 8  . For example, Grover s algorithm  9  requires O( √ N/M ) iterations to find any single element out of M elements encoded into N qubits  10  . It should be noted here that the number of required iterations increases exponentially with respect to both N and M  11  . Therefore, these algorithms become inefficient when dealing with very large databases  12  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Binary Quantum Search . Abstract : We create an algorithm for quantum search that is based on the Grover s algorithm and using only two qubits to represent one element in the database , which can be viewed as a binary value .The algorithm has been used using IBM Q Experience simulator with four different databases containing up to 16 items each . We have also compared our findings against those acquired by running Grover s original method on the same datasets .Our experimental studies demonstrate that the suggested method performs better than its classical counterpart when looking through tiny libraries ( up to 8 objects ) . However , it becomes weaker effective if we increase the length of the database beyond this limit .This research was supported by the Australian Research Council Discovery Project DP160103745 . In recent years there has been significant interest in building methods for performing quantum searches over large data sets 1 .These methods are expected to find uses in areas such as machine computing 2 , pattern recognition 3 , computer vision 4 , bioinformatics 5 , etc . , where they will provide us to solve issues quicker or more accurately 6 . In general , these algorithms use N qubits to encode M elements in the database 7 , 8 .For instance , Grover s algorithm 9 requires O ( √ N / M ) iterations to find any single element out of M elements stored into N qubits 10 . It should be mentioned here that the number of required iterations increases exponentially with regard to both N and M 11 .Therefore , these algorithms get inefficient when dealing with very huge databases 12 .",
        "rewrite_text": "In this study, we present a novel quantum search algorithm inspired by Grover's algorithm, utilizing only two qubits to represent a single element within a database, effectively treating it as a binary value. Our algorithm was tested using the IBM Q Experience simulator across four distinct databases, each containing up to 16 items. We conducted a comparative analysis of our results against those obtained from Grover's original algorithm applied to the same datasets. The experimental results indicate that our proposed method exhibits superior performance compared to classical approaches when searching through smaller databases, specifically those containing up to 8 elements. However, as the size of the database increases beyond this threshold, the effectiveness of our algorithm diminishes. This research was funded by the Australian Research Council Discovery Project DP160103745. \n\nThe growing interest in developing efficient quantum search techniques for large datasets has been notable in recent years. These advanced methods hold promise for applications in various fields, including machine learning, pattern recognition, computer vision, and bioinformatics, where they can potentially enhance problem-solving speed and accuracy. Typically, quantum search algorithms employ N qubits to encode M elements within a database. For example, Grover's algorithm necessitates O(√N/M) iterations to locate a specific element among M elements stored in N qubits. It is important to note that the number of iterations required escalates exponentially with increases in both N and M, leading to inefficiencies when managing extremely large databases. Our findings contribute to the ongoing discourse on optimizing quantum search strategies and highlight the limitations encountered as database sizes expand.",
        "ori-fast-z-score": -0.47891314261057566,
        "water-fast-z-score": 5.979695373240744,
        "rewrite-fast-z-score": -0.8049844718999243
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  On-Line Condition Monitoring using Computational Intelligence .\nAbstract:\nThe aim of this book is to provide an overview on the state-of-the-art in on-line condition monitoring and fault diagnosis for industrial processes, with special emphasis on computational intelligence techniques such as neural networks (NNs), fuzzy logic systems (FLS) or evolutionary algorithms (EAs). The book covers both theoretical aspects and practical applications of these methods. It also discusses some recent advances in intelligent sensor technologies that are essential for successful implementation of on-line condition monitoring schemes. This book will be useful not only for researchers but also for engineers who want to apply computational intelligence techniques into their own research work. Contents include:  Chapter 1: Introduction to On-line Condition Monitoring.  Chapter 2: Intelligent Sensors for On-line Condition Monitoring.  Chapters 3-7: Neural Networks for Fault Diagnosis.  Chapters 8-10: Fuzzy Logic Systems for Fault Diagnosis.   Chapters 11-13: Evolutionary Algorithms for Fault Diagnosis.   ...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : On - Line Condition Monitoring using Computational Intelligence . Abstract : The goal of this book is to provide an overview on the state - of - the - art in on - line condition monitoring and failure detection for industrial systems , with special emphasis on mathematical intelligence techniques such as neural systems ( NNs ) , fuzzy logic systems ( FLS ) or genetic algorithms ( EAs ) .The text encompasses both theoretical components and useful use of these procedures . It additionally outlines some latest advances in intelligent detector technologies that are essential for successful implementation of on - line condition monitoring schemes .This book will be valuable not only for researchers but also for designers who desire to apply computational intelligence techniques into their own research effort . Contents include : Chapter 1 : Introduction to On - line Condition Monitoring .Chapter 2 : Intelligent Sensors for On - line Condition Monitoring . Chapters 3 - 7 : Neural Networks for Fault Diagnosis .Chapters 8-10: Fuzzy Logic Systems for Fault Diagnosis.Chapters 11-13: Evolutionary Algorithms for Fault Diagnosis....",
        "rewrite_text": "Title: On-Line Condition Monitoring using Computational Intelligence\n\nAbstract: This book aims to present a comprehensive overview of the latest advancements in on-line condition monitoring and failure detection within industrial systems, focusing particularly on mathematical intelligence methodologies such as neural networks (NNs), fuzzy logic systems (FLS), and evolutionary algorithms (EAs). It integrates both theoretical frameworks and practical applications of these techniques, providing a balanced perspective on their implementation. The text highlights recent innovations in intelligent detection technologies that are crucial for the effective deployment of on-line condition monitoring strategies. This resource is designed to be beneficial for both researchers and practitioners who are interested in incorporating computational intelligence into their work. The book is structured into several chapters, beginning with an introductory chapter that lays the groundwork for understanding on-line condition monitoring. Subsequent chapters delve into the role of intelligent sensors in this domain, followed by an in-depth exploration of neural networks for fault diagnosis across multiple chapters. The discussion then transitions to fuzzy logic systems and their applications in fault diagnosis, before concluding with a thorough examination of evolutionary algorithms and their relevance in this field. Overall, this book serves as a valuable reference for those looking to enhance their knowledge and application of computational intelligence techniques in the context of industrial monitoring and diagnostics.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.88240082724041,
        "rewrite-fast-z-score": 0.5619514869490164
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Predictions for Triple Stars with and without a Pulsar in Star Clusters .\nAbstract:\nWe present predictions on the formation rates, masses, orbital parameters, and spin periods of triple stars that contain at least one pulsar (PSR). We use Monte Carlo simulations to generate populations of binaries and triples using distributions of initial binary properties derived from observations of open clusters. The population synthesis is performed by combining our new code for simulating the evolution of close binaries containing PSRs with an existing code for simulating the dynamical interactions between multiple stellar systems. Our results show that most observed PSRs are likely to be found as members of hierarchical triples rather than isolated double neutron star systems or wide binaries. In particular, we find that:  - Most PSRs should have companions whose mass lies within 0.1 M⊙ < Mc < 1.0 M⊙; - Most PSRs should reside in orbits with semi-major axes less than 100 AU; - Most PSRs will not evolve into millisecond pulsars before their second supernova explosion; - Most PSRs may experience significant gravitational wave emission during their lifetimes.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Predictions for Triple Stars with and without a Pulsar in Star Clusters . Abstract : We report estimates on the formation rates , masses , orbital characteristics , and spin times of double stars that contain at least one pulsar ( PSR ) .We use Monte Carlo simulations to create populations of binaries and triples using distributions of initial binary properties derived from measurements of open clusters . The population synthesis is conducted by combining our new code for simulating the evolution of close binaries containing PSRs with an previous code for simulating the dynamical interactions between multiple stellar systems .Our results show that most observed PSRs are likely to be found as members of hierarchical triples rather than scattered double neutron star systems or broad binaries . In particular , we find that : - Most PSRs should have companions whose mass falls within 0 . 1 [UNK] < Mc < 1 . 0 [UNK] ; - Most PSRs should exist in planets with semi - major axes less than 100 AU ; - Most PSRs will not evolve into millisecond pulsars before their second supernova explosion ; - Most PSRs might experience considerable gravitational wave radiation during their lifetimes .",
        "rewrite_text": "In this study, we present comprehensive estimates regarding the formation rates, masses, orbital dynamics, and spin periods of double star systems that include at least one pulsar (PSR). Utilizing Monte Carlo simulations, we generate populations of binary and triple star systems based on initial binary property distributions derived from observational data of open clusters. Our population synthesis approach integrates a novel code designed for simulating the evolutionary processes of close binaries containing pulsars with an established code that models the dynamical interactions within multiple stellar systems. The findings indicate that the majority of observed pulsars are likely to be part of hierarchical triple systems, as opposed to being found in isolated double neutron star configurations or wide binary systems. Specifically, our analysis reveals several key insights: first, most pulsars are expected to have companions with masses ranging from 0.1 to 1.0 solar masses; second, the majority of pulsars are predicted to reside in systems with planetary semi-major axes of less than 100 astronomical units; third, it is unlikely for most pulsars to transition into millisecond pulsars prior to undergoing a second supernova event; and finally, many pulsars are anticipated to emit significant gravitational waves throughout their lifetimes. These results enhance our understanding of the environments in which pulsars form and evolve, providing critical insights into their characteristics and the dynamics of star clusters.",
        "ori-fast-z-score": 0.43133109281375365,
        "water-fast-z-score": 4.610083899802517,
        "rewrite-fast-z-score": 0.8320502943378436
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Alignments of the Galaxy Spins with the Real-Space Tidal Field Reconstructed from the Two Mass Redshift Survey .\nAbstract:\nWe present an analysis of the alignments between galaxy spins and tidal fields in real space, using data from the Two Mass Redshfit Survey (TMRS). We find that galaxies are preferentially aligned perpendicular to their local tidal field on scales larger than 1 Mpc/h. This alignment is stronger for more massive galaxies at higher redshifts. The observed spin-tide correlation can be explained by the effect of gravitational torques exerted by large-scale structures during the formation process of these galaxies. Our results suggest that this mechanism may play an important role in shaping galactic angular momenta. These findings have implications for understanding how dark matter halos acquire their angular momentum as well as for interpreting observations of cosmic shear statistics. Introduction: Galaxies form within overdense regions of the universe where they experience strong gravitational interactions with other objects such as neighboring galaxies or clusters of galaxies. During the formation process, these interactions induce gravitational torques which affect the orientation of the galactic angular momentum vector. In turn, the orientations of galactic angular momenta determine the shapes of galaxies through dynamical friction processes. Therefore, it has been suggested that the shape distribution of galaxies could provide information about the origin of galactic angular momentums (e.g., Catelan & Theuns 1996; Lee et al. 2008) . However, observational studies show conflicting results regarding whether there exists any preferred direction of galaxy spin axes relative to their neighbors  positions (see e.g., Faltenbacher et al. 2002; Bailin et al. 2005; Paz et al. 2008; Codis et al. 2012 , for recent works).\nIn order to understand the physical mechanisms responsible for determining the directions of galactic angular momentas, we need to study the statistical properties of galaxy spin distributions over large volumes of the universe. Recent surveys like Sloan Digital Sky Survey (SDSS) allow us to measure galaxy orientations accurately enough to perform such analyses. For example, Lee et al. (2008) used SDSS DR4 data to investigate the alignments between galaxy spin vectors and their nearest neighbor s position angles. They found no",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Alignments of the Galaxy Spins with the Real - Space Tidal Field Reconstructed from the Two Mass Redshift Survey . Abstract : We present an assessment of the alignments between galaxy twists and tidal fields in real space , using data from the Two Mass Redshfit Survey ( TMRS ) .We see that galaxies are preferentially aligned perpendicular to their nearby tidal field on scales bigger than 1 Mpc / h . This alignment is strengthened for more massive galaxies at higher redshifts .The observed spinning - tide coupling can be described by the impact of gravitational torques exerted by large - scale structures during the formation period of these galaxies . Our results show that this mechanism may play an important role in shaping galactic angular momenta .These studies have consequences for studying how dark matter halos acquire their angular velocity as also as for interpreting observations of universe shear statistics . Introduction : Galaxies shape within overdense regions of the universe where they encounter strong gravitational interactions with other structures such as nearby galaxies or rows of stars .During the formation step , these interactions initiate gravitational torques which affect the orientation of the galactic angular velocity tensor . In turn , the orientations of galactic angular momenta determine the shapes of galaxies through dynamical friction mechanisms .Therefore , it has been proposed that the form distribution of galaxies possible provide information about the origin of galactic angular momentums ( e . g . , Catelan & Theuns 1996 ; Lee et al . 2008 ) .However , observational analyses suggest conflicting findings regarding whether there exists any preferred direction of galaxy spin axes relative to their neighbors positions ( saw e . g . , Faltenbacher et al . 2002 ; Bailin et al .2005 ; Paz et al . 2008 ; Codis et al .2012 , for recent works ) . In order to comprehend the physical mechanisms involved for determining the directions of galactic angular momentas , we require to study the statistical characteristics of galaxy spin distributions over large quantities of the universe .Recent surveys like Sloan Digital Sky Survey ( SDSS ) able us to measure galaxy orientations correctly sufficiently to conduct such analyses . For instance , Lee et al .( 2008 ) used SDSS DR4 data to examine the alignments between galaxy spin vectors and their closest neighbor s position angles . They found no",
        "rewrite_text": "**Title:** The Alignment of Galaxy Spins with the Real-Space Tidal Field Reconstructed from the Two Mass Redshift Survey\n\n**Abstract:** This study investigates the relationship between the orientations of galaxy spins and the tidal fields in real space, utilizing data from the Two Mass Redshift Survey (TMRS). Our findings reveal that galaxies tend to align perpendicularly to the tidal fields in their vicinity, particularly on scales exceeding 1 Mpc/h. This alignment effect is notably more pronounced in more massive galaxies and at higher redshifts. We attribute this observed coupling between galaxy spins and tidal influences to the gravitational torques exerted by large-scale structures during the galaxies' formation epochs. The implications of our results suggest that these gravitational interactions play a significant role in determining the angular momentum of galaxies. Furthermore, our research contributes to a deeper understanding of how dark matter halos acquire their angular velocities and aids in interpreting the shear statistics observed in the universe.\n\n**Introduction:** Galaxies typically form in regions of the universe characterized by overdensities, where they experience strong gravitational interactions with neighboring structures, such as other galaxies and stellar streams. During their formation, these interactions generate gravitational torques that influence the orientation of the galactic angular velocity tensor. The alignment of these angular momenta subsequently affects the morphological characteristics of galaxies through mechanisms such as dynamical friction. Consequently, the distribution of galaxy shapes may provide insights into the origins of their angular momenta, as suggested by previous studies (e.g., Catelan & Theuns 1996; Lee et al. 2008). However, observational data have yielded mixed results regarding the existence of a preferred orientation for galaxy spin axes in relation to the positions of neighboring galaxies (see, for example, Faltenbacher et al. 2002; Bailin et al. 2005; Paz et al. 2008; Codis et al. 2012). To elucidate the physical processes that dictate the directions of galactic angular momenta, it is essential to analyze the statistical properties of galaxy spin distributions across extensive regions of the universe. Recent surveys, such as the Sloan Digital Sky Survey (SDSS), have enabled precise measurements of galaxy orientations, facilitating such investigations. For instance, Lee et al. (2008) utilized SDSS DR4 data to explore the alignments between galaxy spin vectors and the position angles of their nearest neighbors, ultimately concluding that no significant correlation exists.",
        "ori-fast-z-score": -1.2924860661584994,
        "water-fast-z-score": 6.519643174778569,
        "rewrite-fast-z-score": 2.029444275607638
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  SLE in self-dual critical Z(N) spin systems: CFT predictions .\nAbstract:\nWe study the SLE (Schramm-Loewner Evolution) process for the scaling limit of interfaces between different phases in the two-dimensional Ising model with nearest-neighbor interactions on an arbitrary planar graph, and its generalization to higher dimensions. We show that the interface is described by a chordal Schramm-Löwner evolution if the underlying lattice has no loops or multiple edges; otherwise it is described by a radial Schramm-Löwner evolutions. The results are obtained using conformal field theory techniques. In particular we use the fact that the partition function of these models can be written as a correlation function of primary fields in some rational conformal field theories. This allows us to obtain explicit formulas for the probability distribution functions of various geometric quantities associated with the interfaces such as their winding numbers around vertices etc.. \nIntroduction\n\nThe Schramm-Loewner Evolutions (SLE)\nprocesses were introduced by Schramm  Sch00  , who showed that they provide a natural description of the scaling limits of interfaces in statistical mechanics systems at criticality. These processes have been studied extensively since then both theoretically and numerically. For example, see  KSS02, SS04a, SS04b, RS05, Sch06, CS07, KS08, KSV09, KM10, MS11, MZ12, BMS13, BS14, LW15, GKS16, GM17, GK18, HJ19, HK20, JPS20  . A comprehensive review of this subject may be found in  Smi01, Sta03, Joh10  .\nIn this work we consider the SLE process for the scaling limit in two dimensions of interfaces separating different phases in the following class of models:  Let G = (V, E) be any finite connected planar graph without loops or multiple edges. Consider the Ising model with nearest neighbor interaction defined on G. That is, let {σv}v∈V denote a collection of random variables taking values +1 and −1, where each σv represents the state of vertex v ∈ V . Then",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : SLE in self - dual critical Z ( N ) spinning systems : CFT predictions . Abstract : We research the SLE ( Schramm - Loewner Evolution ) process for the scaling limit of interfaces between various phases in the two - dimensional Ising model with nearest - neighbor interactions on an arbitrary planar graph , and its generalization to higher dimensions .We see that the interface is characterized by a chordal Schramm - Löwner evolution if the underlying lattice has no loops or multiple edges ; otherwise it is characterized by a radial Schramm - Löwner evolutions . The results are derived using conformal field model algorithms .In particular we utilize the fact that the partition function of these models can be written as a correlation function of primary fields in some rational conformal field theories . This enables us to obtain precise formulas for the probability distribution functions of several geometric quantities associated with the interfaces such as their winding numbers around vertices etc . . Introduction The Schramm - Loewner Evolutions ( SLE ) experiments were introduced by Schramm Sch00 , who demonstrated that they give a natural representation of the scaling limits of interfaces in statistical mechanics systems at criticality .These mechanisms have been studied frequently since then both theoretically and numerically . For instance , see KSS02 , SS04a , SS04b , RS05 , Sch06 , CS07 , KS08 , KSV09 , KM10 , MS11 , MZ12 , BMS13 , BS14 , LW15 , GKS16 , GM17 , GK18 , HJ19 , HK20 , JPS20 .A full study of this subject may be found in Smi01 , Sta03 , Joh10 . In this study we view the SLE method for the scaling limit in two dimensions of interfaces separating different stages in the following category of models : Let G = ( V , E ) be any finite connected planar graph without loops or multiple edges .Consider the Ising model with nearest neighbor interaction defined on G . That is , let { σv } v∈V denote a collection of random vectors take values + 1 and −1 , where each σv represents the state of vertex v ∈ V . Then",
        "rewrite_text": "**Title:** SLE in Self-Dual Critical Z(N) Spinning Systems: CFT Predictions\n\n**Abstract:** This study investigates the Schramm-Loewner Evolution (SLE) process as it pertains to the scaling limit of interfaces between different phases in the two-dimensional Ising model, characterized by nearest-neighbor interactions on arbitrary planar graphs, with extensions to higher dimensions. Our findings indicate that the nature of the interface is dictated by the structure of the underlying lattice; specifically, if the lattice is free of loops and multiple edges, the interface is described by a chordal Schramm-Loewner evolution. Conversely, in cases where the lattice contains loops or multiple edges, the interface is characterized by radial Schramm-Loewner evolutions. These results are derived through the application of conformal field theory (CFT) algorithms, leveraging the relationship between the partition function of the models and the correlation functions of primary fields within certain rational conformal field theories. This approach allows us to derive explicit formulas for the probability distribution functions of various geometric properties associated with the interfaces, including their winding numbers around vertices. \n\nThe concept of Schramm-Loewner Evolutions was first introduced by Schramm, who established their role as a natural framework for representing the scaling limits of interfaces in critical statistical mechanics systems. Since then, SLE has been extensively explored both theoretically and numerically, as evidenced by a wealth of literature. In this paper, we focus on the SLE framework for analyzing the scaling limits in two dimensions of interfaces that delineate different phases in the Ising model defined on a finite connected planar graph devoid of loops or multiple edges. We denote the collection of random variables representing the state of each vertex in the graph as {σv} for v in V, where each σv can take values of +1 or -1. This foundational setup allows us to delve deeper into the geometric and probabilistic characteristics of the interfaces, contributing to the broader understanding of critical phenomena in statistical mechanics.",
        "ori-fast-z-score": 1.0524696231684352,
        "water-fast-z-score": 5.918640302493727,
        "rewrite-fast-z-score": 1.1026456085839622
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Conjugate field and fluctuation-dissipation relation for the dynamic phase transition in the two-dimensional kinetic Ising model .\nAbstract:\nWe study the dynamics of the kinetic Ising model on square lattices with periodic boundary conditions by Monte Carlo simulations at finite temperatures T . We find that there is no static order parameter to characterize the dynamic phase transition, but we can define an effective conjugate field H conjugate to the magnetization M as follows: \nH = -ln(<M>)/T,\nwhere <M> denotes the average over all spins. The critical temperature Tc is determined by the condition dH/dT =0. In addition, we show that the fluctuation-dissipation theorem holds well near Tc. \nThe results are compared with those obtained by the mean-field approximation. \n\n\nI. INTRODUCTIO N\n\nIn recent years much attention has been paid to nonequilibrium phenomena such as relaxation processes after rapid changes of external parameters  1  , aging  2  , glassy behavior  3  , etc., because they play important roles not only in physics but also in biology  4  .\nAmong these topics, the kinetic Ising model  5  is one of the most popular models used to investigate non-equilibrium properties  6  . It describes the time evolution of spin variables S i (t) (i=1,...,N)\non a regular lattice under the influence of thermal fluctuations. Here t represents the number of Monte Carlo steps per site (MCS/s). At each step, every spin interacts with its nearest neighbors through exchange interactions J ij . Then it flips according to the Metropolis algorithm  7 :  if e -Sij / kBT > random number between 0 and 1 then flip S j else keep S j unchanged where k B is Boltzmann s constant and T is the absolute temperature. This process continues until equilibrium is reached or some other criterion is satisfied  8  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Conjugate field and fluctuation - dissipation relation for the dynamic phase shift in the two - dimensional kinetic Ising model . Abstract : We research the dynamics of the kinetic Ising model on square lattices with periodic boundary constraints by Monte Carlo simulations at finite temperatures T .We see that there is no static order parameter to characterize the dynamic phase shift , but we can define an efficient conjugate field H conjugate to the magnetization M as follows : H = - ln ( < M > ) / T , where < M > denotes the average over all spins . The essential temperature Tc is calculated by the condition dH / dT = 0 .In addition , we prove that the fluctuation - dissipation conjecture holds well near Tc . The results are compared with those achieved by the mean - field approximation .I . INTRODUCTIO N In recent years much attention has been paid to nonequilibrium phenomena such as relax processes after rapid adjustments of external parameters 1 , aging 2 , glassy behavior 3 , etc . , because they hold important roles not only in science but also in science 4 . Among these topics , the kinetic Ising model 5 is one of the most popular theories used to examine non - equilibrium properties 6 .It describes the time progression of spin vectors S i ( t ) ( i = 1 , . . . , N ) on a regular structure under the impact of thermal fluctuations . Here t equals the number of Monte Carlo steps per site ( MCS / s ) .At each step , every spin interacts with its closest neighbors through exchange interactions J ij . Then it flips due to the Metropolis algorithm 7 : if e - Sij / kBT > random value between 0 and 1 then flip S j else kept S j unchanged where k B is Boltzmann s constant and T is the absolute temperature .This process proceeds until equilibrium is reached or some other criterion is fulfilled 8 .",
        "rewrite_text": "Title: Conjugate Field and Fluctuation-Dissipation Relation for the Dynamic Phase Shift in the Two-Dimensional Kinetic Ising Model\n\nAbstract: In this study, we investigate the dynamic behavior of the kinetic Ising model on square lattices under periodic boundary conditions, utilizing Monte Carlo simulations at finite temperatures (T). Our findings reveal that a static order parameter is insufficient to characterize the dynamic phase shift; however, we introduce an effective conjugate field, denoted as H, which is related to the magnetization (M) through the equation H = -ln(<M>)/T, where <M> represents the average magnetization across all spins. We determine the critical temperature (Tc) by applying the condition dH/dT = 0. Furthermore, we demonstrate that the fluctuation-dissipation theorem is valid in the vicinity of Tc, reinforcing the connection between fluctuations and dissipative processes in this non-equilibrium system. Our results are juxtaposed with those derived from mean-field approximations, highlighting the differences and similarities in behavior. \n\nThe kinetic Ising model has garnered significant attention in recent years due to its relevance in understanding various nonequilibrium phenomena, such as relaxation processes following rapid changes in external parameters, aging, and glassy dynamics. These phenomena are not only pivotal in theoretical physics but also have practical implications across multiple scientific disciplines. The model effectively captures the temporal evolution of spin vectors (Si(t), where i = 1, ..., N) on a regular lattice influenced by thermal fluctuations. In our simulations, time is quantified in terms of Monte Carlo steps per site (MCS/s), and at each step, spins interact with their nearest neighbors through exchange interactions (Jij). The Metropolis algorithm governs the spin-flipping process, where a spin Sj is flipped based on the probability determined by the energy difference and the thermal energy, characterized by Boltzmann's constant (kB) and the absolute temperature (T). This iterative process continues until the system reaches equilibrium or meets a predefined stopping criterion. Our research contributes to a deeper understanding of the dynamic phase transitions in the kinetic Ising model and the underlying mechanisms governing nonequilibrium behavior.",
        "ori-fast-z-score": 0.8049844718999243,
        "water-fast-z-score": 6.057921483348286,
        "rewrite-fast-z-score": 3.8069870875708927
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Is the Bardeen-Petterson effect responsible for the warping and precession in NGC 4258? .\nAbstract:\nWe present new observations of the central region of the nearby Seyfert galaxy NGC 4258, which show that its nuclear disk is warped by an angle of ~20 degrees with respect to the plane of the host galaxy s stellar bulge (see Figure 1 ). The warp has been detected using near-infrared integral field spectroscopy obtained at Gemini Observatory on Mauna Kea, Hawaii. \n \n We also report the detection of significant rotation about the minor axis of this warped structure, as well as evidence for counter-rotation within the innermost few hundred parsecs of the nucleus. These results are consistent with previous studies based on optical data alone. \n \n In addition, we find that the kinematics of the gas in the outer regions of the nuclear disk can be explained if it orbits around the supermassive black hole located at the center of the galaxy under the influence of both gravitational forces and magnetic fields. This result suggests that the observed warps may have their origin in the magneto-rotational instability (MRI) operating in accretion disks surrounding massive black holes. \n \n Finally, we discuss how these findings could help us understand the physics behind the so-called  Bardeen-Petterson effect : i.e., the alignment between the spin axes of the stars and the angular momentum vector of the accreting material onto the central supermassive black hole.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Is the Bardeen - Petterson effect responsible for the warping and precession in NGC 4258 ? .Abstract : We report new images of the central region of the nearby Seyfert galaxy NGC 4258 , which show that its nuclear core is warped by an angle of ~ 20 degrees with regard to the plane of the host universe s stellar bulge ( see Figure 1 ) . The warp has been detected using near - infrared integral field spectroscopy acquired at Gemini Observatory on Mauna Kea , Hawaii .We additionally report the observation of large rotation about the minor axis of this warped structure , as also as data for counter - movement within the innermost few hundred parsecs of the nucleus . These conclusions are compatible with previous research based on optical data alone .In addition , we find that the kinematics of the gas in the exterior areas of the atomic disk can be understood if it orbits around the supermassive black hole located at the center of the galaxy under the effects of both gravity forces and magnetic fields . This result suggests that the seen warps may have their source in the magneto - rotational instability ( MRI ) working in accretion disks surrounding massive blue holes .Finally , we talk how these results could assist us explain the physics behind the so - called Bardeen - Petterson effect : i . e . , the alignment between the spin axes of the stars and the angular velocity tensor of the accreting matter onto the main supermassive black hole .",
        "rewrite_text": "**Title: Is the Bardeen-Petterson Effect Responsible for the Warping and Precession in NGC 4258?**\n\n**Abstract:** In this study, we present new imaging of the central region of the nearby Seyfert galaxy NGC 4258, revealing a significant warp in its nuclear core, tilted at approximately 20 degrees relative to the plane of the galaxy's stellar bulge. This warp was identified through near-infrared integral field spectroscopy conducted at the Gemini Observatory in Mauna Kea, Hawaii. Our observations also indicate a substantial rotation around the minor axis of this warped structure, alongside evidence of counter-movement within the innermost few hundred parsecs of the nucleus. These findings align with earlier studies that relied solely on optical data. Furthermore, we analyze the kinematics of the gas in the outer regions of the atomic disk, which appear to be influenced by the gravitational forces of the supermassive black hole at the galaxy's center, as well as magnetic fields. This observation implies that the observed warps may originate from magneto-rotational instability (MRI) occurring in the accretion disks surrounding massive black holes. We discuss the implications of our results for understanding the Bardeen-Petterson effect, which describes the alignment between the spin axes of stars and the angular velocity tensor of the accreting matter onto the central supermassive black hole. Our findings contribute to a deeper understanding of the dynamics at play in NGC 4258 and the potential mechanisms driving the warping and precession observed in its structure. This research not only enhances our comprehension of the specific characteristics of NGC 4258 but also provides insights into broader astrophysical processes related to supermassive black holes and their interactions with surrounding matter.",
        "ori-fast-z-score": -2.5253432421288866,
        "water-fast-z-score": 4.233901974057256,
        "rewrite-fast-z-score": 0.7364596943186588
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Iron abundances of B-type post-Asymptotic Giant Branch stars in globular clusters: Barnard 29 in M 13 and ROA 5701 in omega Cen .\nAbstract:\nWe have determined the iron abundance for two bright, blue supergiants (BSGs) in the Galactic globular clusters Omega Cen and M13 using high-resolution spectroscopy obtained with UVES at VLT-UT2 telescope.  The results are compared to those derived by other authors for similar objects in these clusters as well as in other globulars. We find that our values agree very well with previous determinations within their uncertainties. In particular we confirm the low Fe content found for one star in Omega Cen previously reported by Yong et al. (2005) . This is consistent with theoretical predictions which suggest that this cluster should be dominated by first generation stars formed out of material enriched only by massive supernovae. Our analysis also shows that both studied stars belong to the group of so-called  blue stragglers  -objects located above the main sequence turn-off point on the colour-magnitude diagram but still burning helium in their cores.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Iron abundances of B - class post - Asymptotic Giant Branch stars in globular complexes : Barnard 29 in M 13 and ROA 5701 in omega Cen . Abstract : We have predicted the metal availability for two bright , blue supergiants ( BSGs ) in the Galactic globular complexes Omega Cen and M13 using high - resolution spectroscopy acquired with UVES at VLT - UT2 telescope .The results are compared to those generated by other researchers for related items in these clusters as well as in other globulars . We see that our values comply very best with previous determinations within their uncertainties .In particular we confirm the small Fe concentration found for one star in Omega Cen previously reported by Yong et al . ( 2005 ) .This is compatible with theoretical estimates which predict that this cluster should be dominated by first generation stars formed out of material enriched only by massive supernovae . Our study also shows that both examined stars belong to the group of so - called blue stragglers - objects located above the main sequence turn - off position on the colour - magnitude diagram but still flaming helium in their cores .",
        "rewrite_text": "Title: Iron Abundances of B-Class Post-Asymptotic Giant Branch Stars in Globular Complexes: Barnard 29 in M13 and ROA 5701 in Omega Cen\n\nAbstract: In this study, we investigate the metallicity of two prominent blue supergiants (BSGs) located in the Galactic globular clusters Omega Centauri and M13. Utilizing high-resolution spectroscopy obtained with the UVES instrument at the VLT-UT2 telescope, we have derived iron abundances for these stars. Our findings are juxtaposed with previous research conducted on similar stellar objects within these clusters and other globular clusters. Notably, our results align closely with earlier measurements, falling well within the established uncertainties. Specifically, we corroborate the low iron concentration identified in one of the stars in Omega Cen, as reported by Yong et al. (2005). This observation is consistent with theoretical models that suggest Omega Cen is primarily composed of first-generation stars, which formed from material enriched solely by the explosive nucleosynthesis of massive supernovae. Furthermore, our analysis reveals that both stars under consideration are categorized as blue stragglers. These stars are situated above the main sequence turn-off point on the color-magnitude diagram, yet they continue to undergo helium fusion in their cores. This research contributes to the understanding of stellar evolution in globular clusters and the chemical enrichment processes that shape their stellar populations.",
        "ori-fast-z-score": -1.5011106998930268,
        "water-fast-z-score": 4.129483209670111,
        "rewrite-fast-z-score": -1.6464638998453551
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The influence of the cylindrical shape of the nucleosomes and H1 defects on properties of chromatin .\nAbstract:\nThe article presents results of computer simulation of the dynamics of chromatin in vitro, which is based on the model of self-organization of the chain of nucleosome core particles (NCP) with histone H1 molecules attached to them by electrostatic forces. The NCPs are modeled as rigid cylinders with diameter equal to that observed experimentally for isolated NCPs. It was found that the presence of histone H1 leads to an increase in the persistence length of the chain due to formation of loops between neighboring NCPs. In addition, it has been shown that the presence of histones H1 can lead to the appearance of defects in the structure of the chain consisting of several adjacent NCPs connected only through one H1 molecule. These defects have a significant effect on the elasticity characteristics of the chain. \n \n Keywords: Chromatin, Self-assembly, Computer simulation, Elasticity, Defects",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The impact of the cylindrical shape of the nucleosomes and H1 defects on characteristics of chromatin . Abstract : The section offers results of software simulation of the dynamics of chromatin in vitro , which is based on the model of self - arrangement of the chain of nucleosome backbone particles ( NCP ) with histone H1 molecules attached to them by electrostatic fields .The NCPs are modeled as rigid cylinders with diameter equal to that detected experimentally for isolated NCPs . It was shown that the presence of histone H1 leads to an increase in the persistence length of the chain thanks to formation of loops between neighboring NCPs .In addition , it has been shown that the presence of histones H1 can lead to the appearance of errors in the structure of the chain consisting of several attached NCPs linked only through one H1 molecule . These flaws have a substantial impact on the elasticity behavior of the chain .Keywords : Chromatin , Self - assembly , Computer modeling , Elasticity , Defects",
        "rewrite_text": "Title: The Influence of Nucleosome Cylindrical Geometry and H1 Deficiencies on Chromatin Properties\n\nAbstract: This study presents findings from a computational simulation investigating the dynamics of chromatin in vitro, utilizing a self-assembly model of nucleosome core particle (NCP) chains, which are interconnected by histone H1 molecules through electrostatic interactions. The NCPs are represented as rigid cylinders, with a diameter that corresponds to measurements obtained from isolated NCPs in experimental settings. Our results indicate that the incorporation of histone H1 significantly enhances the persistence length of the nucleosome chain. This enhancement is attributed to the formation of loops between adjacent NCPs, which contributes to the structural integrity of the chromatin. Furthermore, our simulations reveal that the presence of histone H1 can introduce structural anomalies within the chain, particularly in configurations where multiple NCPs are connected via a single H1 molecule. These structural defects have a pronounced effect on the elastic properties of the chromatin chain, altering its mechanical behavior. The implications of these findings are critical for understanding the biophysical properties of chromatin and its role in cellular processes. The study underscores the importance of histone H1 in maintaining chromatin structure and highlights how defects can influence its elasticity, which is vital for various biological functions, including gene regulation and DNA packaging. \n\nKeywords: Chromatin, Self-assembly, Computer modeling, Elasticity, Defects",
        "ori-fast-z-score": -0.6201736729460423,
        "water-fast-z-score": 4.923659639173309,
        "rewrite-fast-z-score": 1.2939932784412609
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The dynamics of the early universe and the initial conditions for inflation in a model with radiation and a Chaplygin gas .\nAbstract:\nWe study the evolution of an isotropic, homogeneous cosmological model filled by radiation and a Chaplygyn gas. We find that this system has two attractors corresponding to different values of the Hubble parameter at late times. The first one corresponds to a de Sitter phase where the energy density of the Chaplygin gas dominates over the other components while the second one describes a decelerating universe dominated by dark matter. In both cases we have found that the initial conditions are fixed by the value of the Hubble constant today. Finally, we show how these results can be used as initial conditions for inflationary models. PACS numbers: 98.80.Cq, 04.20.-q, 95.36.+x  Keywords: Cosmology, Inflation, Chaplygin gas, Radiation, Initial Conditions . \nI. INTRODUCTORY REMARK\nIn recent years there has been considerable interest in studying the possibility that our present day universe may contain some exotic form of matter which behaves like a negative pressure fluid (see e.g.,  1  ). This kind of matter could play an important role in explaining several phenomena observed on large scales such as the accelerated expansion of the universe  2  , the flatness problem  3  or even the origin of structure formation  4  .\nOne possible candidate for this type of matter is known as the Chaplygin gas  5  . It was originally introduced as a phenomenological description of the behaviour of superdense stars  6  but it also appears naturally within superstring theories  7, 8  . Recently, it has been shown  9  that the Chaplygin gas provides a good fit to current observational data  10  if its equation of state takes the following form: p = −A/ρ α , where A and α are positive constants. For small values of ρ, i.e., when the universe is dominated by ordinary matter, the above expression reduces to p ≈ 0 so that the Chaplygin",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The behavior of the early universe and the early conditions for inflation in a theory with radiation and a Chaplygin gas . Abstract : We research the evolution of an isotropic , homogeneous cosmological model filled by radiation and a Chaplygyn gas .We see that this scheme has two attractors corresponding to different values of the Hubble parameter at late times . The first one corresponds to a de Sitter phase where the power concentration of the Chaplygin gas dominates over the other components while the second one portrays a decelerating universe dominated by black material .In both cases we have discovered that the first conditions are fixed by the value of the Hubble constant today . Finally , we find how these results can be used as initial conditions for inflationary theories .PACS codes : 98 . 80 . Cq , 04 . 20 . - q , 95 . 36 . + x Keywords : Cosmology , Inflation , Chaplygin gas , Radiation , Initial Conditions . I .INTRODUCTORY REMARK In past decades there has been substantial interest in investigating the prospect that our contemporary day universe might consist some unusual type of matter which behaves like a negative pressure fluid ( see e . g . , 1 ) . This kind of matter could play an important role in understanding several phenomena observed on huge scales such as the advanced expansion of the universe 2 , the flatness problem 3 or even the origin of shape formation 4 .One potential candidate for this form of matter is known as the Chaplygin gas 5 . It was originally developed as a phenomenological explanation of the behaviour of superdense stars 6 but it also occurs commonly within superstring theories 7 , 8 .Recently , it has been shown 9 that the Chaplygin gas offers a better suited to recent observational data 10 if its equation of state takes the following form : p = −A / ρ α , where A and ρ are positive constants . For small values of ρ , i . e . , when the universe is dominated by normal matter , the above expression reduces to p ≈ 0 so that the Chaplygin",
        "rewrite_text": "**Title:** The Behavior of the Early Universe and Initial Conditions for Inflation in a Model Incorporating Radiation and Chaplygin Gas\n\n**Abstract:** This study investigates the dynamics of an isotropic and homogeneous cosmological model that incorporates both radiation and Chaplygin gas. Our analysis reveals the existence of two distinct attractors associated with varying values of the Hubble parameter in the late universe. The first attractor corresponds to a de Sitter phase, where the energy density of the Chaplygin gas predominates over other components, leading to accelerated expansion. Conversely, the second attractor describes a decelerating universe that is primarily influenced by dark matter. Notably, we establish that the initial conditions for both scenarios are constrained by the current value of the Hubble constant. These findings have significant implications for the formulation of initial conditions in inflationary models, suggesting that the behavior of the Chaplygin gas could provide a viable framework for understanding the early universe's inflationary phase. \n\nIn recent decades, there has been a growing interest in exploring the possibility that our current universe may contain exotic forms of matter characterized by negative pressure, which can help elucidate various large-scale cosmic phenomena, including the accelerated expansion of the universe, the flatness problem, and the genesis of structure formation. The Chaplygin gas has emerged as a prominent candidate for such a form of matter. Initially proposed as a phenomenological model to explain the behavior of superdense stars, it has since found applications in superstring theories. Recent studies indicate that the Chaplygin gas aligns more closely with contemporary observational data when its equation of state is expressed as \\( p = -A / \\rho^\\alpha \\), where \\( A \\) and \\( \\rho \\) are positive constants. In regimes where the density \\( \\rho \\) is low, typical of a universe dominated by ordinary matter, this equation simplifies to \\( p \\approx 0 \\), thereby demonstrating the Chaplygin gas's potential to contribute to our understanding of cosmic evolution.\n\n**PACS Codes:** 98.80.Cq, 04.20.-q, 95.36.+x  \n**Keywords:** Cosmology, Inflation, Chaplygin gas, Radiation, Initial Conditions.",
        "ori-fast-z-score": -0.3333333333333333,
        "water-fast-z-score": 5.896229069615537,
        "rewrite-fast-z-score": -0.08481889296799709
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Brownian excursion area, Wright s constants in graph enumeration, and other Brownian areas .\nAbstract:\nWe study the distribution of the total area swept out by a one-dimensional Brownian motion between two fixed times. We show that this distribution is given by an explicit formula involving the modified Bessel function I0(x). This result can be used to derive several interesting identities for special functions such as the Riemann zeta-function or the Hurwitz zeta-functions at even arguments. In particular we obtain new proofs of some results due to Wright on the number of graphs with n vertices having certain properties (such as being bipartite) which are related to the coefficients appearing in the expansion of the exponential generating function of these numbers into powers of t. Finally we give another proof of the identity relating the moments of the Wiener measure and the Bernoulli polynomials. The main tool will be the Feynman-Kac representation of the solution of the heat equation. Let Wt denote standard Brownian motion starting at 0. For any real number s > 0 let us consider the random variable A(s), defined as the total area swept out during the time interval  0,s  by the process Wt:",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Brownian excursion area , Wright s constants in graph enumeration , and other Brownian areas . Abstract : We study the spread of the total region swept out by a one - dimensional Brownian movement between two fixed times .We see that this distribution is given by an explicit formula involving the modified Bessel distribution I0 ( x ) . This result can be used to derive several interesting identities for special functions such as the Riemann zeta - function or the Hurwitz zeta - functions at even arguments .In particular we obtain new proofs of some results attributed to Wright on the number of graphs with n nodes having specific properties ( such as being bipartite ) which are related to the coefficients appearing in the expansion of the exponential producing function of these numbers into powers of t . Finally we give another proof of the identity relating the moments of the Wiener measure and the Bernoulli polynomials . The main tool will be the Feynman - Kac representation of the solve of the heat equation .Let Wt denote standard Brownian movement starting at 0 . For any real number s > 0 let us consider the random variable A ( s ) , defined as the total area swept out during the period interval 0 , s by the process Wt :",
        "rewrite_text": "In this article, we investigate the area covered by a one-dimensional Brownian motion over a specified time interval. Our analysis reveals that the distribution of this area can be expressed through an explicit formula that incorporates the modified Bessel function I0(x). This finding opens the door to deriving several intriguing identities related to special functions, including the Riemann zeta function and the Hurwitz zeta function evaluated at even integers. Notably, we provide new proofs for certain results originally attributed to Wright concerning the enumeration of graphs with n vertices that possess particular characteristics, such as being bipartite. These characteristics are closely linked to the coefficients found in the power series expansion of the exponential generating function associated with these graph counts. Additionally, we present an alternative proof of the identity that connects the moments of the Wiener measure with Bernoulli polynomials. The primary methodology employed in our study is the Feynman-Kac representation, which serves as a solution framework for the heat equation. We denote standard Brownian motion starting from zero as Wt, and for any positive real number s, we define the random variable A(s) to represent the total area covered by the process Wt during the time interval from 0 to s. This research not only enhances our understanding of Brownian excursions but also establishes significant links between stochastic processes and combinatorial graph theory.",
        "ori-fast-z-score": 1.1659976680069961,
        "water-fast-z-score": 4.297967830559865,
        "rewrite-fast-z-score": -1.0734900802433864
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Low-dimensional Supersymmetric Lattice Models .\nAbstract:\nThe low energy effective theories for superstrings are supergravity and supersymmetric gauge theories in four dimensions, which can be obtained by compactifying the extra six spatial dimensions on a Calabi-Yau manifold.  In this talk I will discuss some recent results about lattice models that provide an alternative approach to studying these theories. The basic idea is to use Monte Carlo simulations to study supersymmetric field theories defined on a finite number of points (the sites) of a regular d-dimensional hypercubic lattice with periodic boundary conditions. These models have been studied extensively over the past few years using numerical techniques such as exact diagonalization, quantum Monte Carlo methods, and density matrix renormalization group algorithms. Recently we developed new Monte Carlo simulation techniques based on the worm algorithm that allow us to simulate large systems at very high temperatures where conventional Monte Carlo methods fail because they suffer from critical slowing down. We used our new method to calculate the free energies of several different supersymmetric lattice models including the N = 4 supersymmetric Yang-Mills theory and the N = 1 supersymmetric U(1) gauge theory coupled to matter fields in various representations.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Low - dimensional Supersymmetric Lattice Models . Abstract : The lowest energy effective models for superstrings are supergravity and supersymmetric gauge theories in four dimensions , which can be obtained by compactifying the extra six spatial dimensions on a Calabi - Yau manifold .In this talk I will explore some latest findings about lattice models that provide an different approach to investigating these theories . The basic idea is to use Monte Carlo simulations to study supersymmetric field theories constructed on a finite number of points ( the sites ) of a regular d - dimensional hypercubic crystal with periodic border conditions .These methods have been studied thoroughly over the previous few years employing mathematical techniques such as approximate diagonalization , quantum Monte Carlo methods , and density matrix renormalization group schemes . Recently we developed novel Monte Carlo simulation algorithms based on the worm algorithm that enable us to simulate large systems at very high altitudes where conventional Monte Carlo methods fail because they suffer from critical speed down .We utilized our new method to estimate the free energies of several different supersymmetric lattice models including the N = 4 supersymmetric Yang - Mills theory and the N = 1 supersymmetric U ( 1 ) gauge theory connected to matter fields in different representations .",
        "rewrite_text": "Title: Low-Dimensional Supersymmetric Lattice Models\n\nAbstract: In the realm of theoretical physics, the effective low-energy models for superstring theory predominantly consist of supergravity and supersymmetric gauge theories formulated in four dimensions. These models emerge through the compactification of six additional spatial dimensions on a Calabi-Yau manifold. This presentation delves into recent advancements in lattice models that offer an alternative framework for probing these complex theories. The core concept revolves around employing Monte Carlo simulations to investigate supersymmetric field theories defined on a finite lattice, specifically a regular d-dimensional hypercubic crystal with periodic boundary conditions. Over the past few years, significant progress has been made in this area, utilizing sophisticated mathematical techniques such as approximate diagonalization, quantum Monte Carlo methods, and density matrix renormalization group approaches. \n\nRecently, we have introduced innovative Monte Carlo simulation algorithms based on the worm algorithm, which facilitate the simulation of large systems at elevated altitudes where traditional Monte Carlo methods encounter limitations due to critical slowdown. Our newly developed methodology has been successfully applied to estimate the free energies of various supersymmetric lattice models, including the N = 4 supersymmetric Yang-Mills theory and the N = 1 supersymmetric U(1) gauge theory, which incorporates matter fields in diverse representations. These findings not only enhance our understanding of supersymmetric theories but also pave the way for further exploration of their implications in high-energy physics. The results underscore the potential of lattice models as a powerful tool for investigating the intricate dynamics of supersymmetric field theories in a controlled and systematic manner.",
        "ori-fast-z-score": 0.19802950859533489,
        "water-fast-z-score": 5.81344274116933,
        "rewrite-fast-z-score": 0.26413527189768715
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Some Aspects of Measurement Error in Linear Regression of Astronomical Data .\nAbstract:\nWe consider the problem of linear regression with measurement error, where the errors are assumed to be normally distributed and uncorrelated between different measurements but correlated within each measurement set (i.e., for one given star). We derive an expression for the covariance matrix of the estimated parameters which is valid even if the number of data points per measurement set differs significantly among sets. This allows us to study how the accuracy of the results depends on the distribution of the number of observations over all stars. The derived formula can also be used as a tool for optimizing observational strategies by minimizing the uncertainty of the resulting estimates. In particular we show that it may be advantageous to use only a small fraction of available data points when estimating the coefficients of the model function while using most or all data points for determining its first few derivatives. Finally, we apply our method to simulated data obtained from the Hipparcos astrometric satellite mission.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Some Aspects of Measurement Error in Linear Regression of Astronomical Data . Abstract : We consider the question of linear regression with observation error , where the errors are expected to be usually spread and uncorrelated between various measurements but correlated within each measurement set ( i . e . , for one given star ) .We derive an expression for the covariance matrix of the expected variables which is valid even if the number of data points per observation set differs greatly among setting . This enables us to study how the accuracy of the results varies on the distribution of the number of measurements over all stars .The derived formula can also be used as a tool for optimizing observational strategies by minimizing the uncertainty of the resulting estimates . In particular we prove that it could be advantageous to use only a small fraction of available data points when estimating the coefficients of the model function while using most or all information points for determining its last few derivatives .Finally , we apply our technique to simulated measurements obtained from the Hipparcos astrometric satellite mission .",
        "rewrite_text": "Title: Some Aspects of Measurement Error in Linear Regression of Astronomical Data\n\nAbstract: This article addresses the challenges associated with linear regression in the presence of observational errors, particularly in the context of astronomical data. We focus on a scenario where measurement errors are typically random and uncorrelated across different observations, yet exhibit correlation within individual sets of measurements for a specific star. To tackle this issue, we derive a covariance matrix expression for the expected variables that remains applicable even when there is significant variability in the number of data points across different observation sets. This formulation allows us to investigate how the distribution of measurements impacts the accuracy of the regression results across multiple stars.\n\nMoreover, our findings provide a framework for optimizing observational strategies aimed at minimizing the uncertainty in the estimated parameters. Notably, we demonstrate that in certain cases, it may be beneficial to utilize only a limited subset of available data points when estimating the coefficients of the model function. Conversely, we recommend leveraging the full set of data points to accurately determine the last few derivatives of the model. This approach balances the trade-off between data quantity and estimation precision.\n\nTo validate our methodology, we apply our derived techniques to simulated data generated from the Hipparcos astrometric satellite mission. The results illustrate the practical implications of our theoretical framework, showcasing how our approach can enhance the reliability of linear regression analyses in astronomical research. By addressing measurement errors effectively, we contribute to the advancement of statistical methods in the field of astronomy, ultimately leading to more accurate interpretations of celestial phenomena.",
        "ori-fast-z-score": -0.44172610429938614,
        "water-fast-z-score": 4.719863179556685,
        "rewrite-fast-z-score": -0.6163156344279367
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Beryllium in Ultra-Lithium-Deficient Halo Stars - The Blue Straggler Connection .\nAbstract:\nWe report the detection of beryllium (Be) lines in two ultra-low metallicity halo stars, CS 22892-052 and HE 0107-5240.  These are the first detections of Be in metal-poor halo stars with  Fe/H  < -2.5 dex. We find that these stars have high surface gravities for their temperatures, indicating they may be blue stragglers or other evolved objects. In addition to the Be features at 4131 Å and 4130 Å we also see evidence for an unidentified feature near 3970 Å which is likely due to C+N+O. This work was supported by NASA grant NAG5-9998. Keywords: Beryllium; Blue straggler; Metal poor star; Ultracool dwarf. 1. Introduction.\nThe discovery of extremely low-mass stars has opened up new avenues into understanding how planets form around very cool dwarfs. However, there remains much uncertainty about the formation process itself as well as the chemical composition of such systems. One important aspect of this problem involves determining whether or not terrestrial planet formation can occur within the habitable zone of ultracool dwarfs. To address this question it will be necessary to determine if the atmospheres of these stars contain significant amounts of heavy elements like carbon, nitrogen, oxygen, sulfur, sodium, potassium, magnesium, aluminum, silicon, calcium, titanium, iron, nickel, cobalt, copper, zinc, arsenic, selenium, silver, gold, mercury, lead, uranium, thorium, and plutonium. It should be noted that while some of these metals are produced during stellar nucleosynthesis others are synthesized only through cosmic ray spallation reactions occurring outside of stars.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Beryllium in Ultra - Lithium - Deficient Halo Stars - The Blue Straggler Connection . Abstract : We report the discovery of beryllium ( Be ) tracks in two ultra - low metallicity halo stars , CS 22892 - 052 and HE 0107 - 5240 .These are the first detections of Be in metal - poor halo stars with Fe / H < - 2 . 5 dex . We see that these stars have high surface gravities for their altitudes , showing they may be blue stragglers or other evolution bodies .In addition to the Be properties at 4131 Å and 4130 Å we also find proof for an unidentified feature near 3970 Å which is probably due to C + N + O . This project was supported by NASA grant NAG5 - 9998 .Keywords : Beryllium ; Blue straggler ; Metal poor star ; Ultracool dwarf . 1 .Introduction . The observation of incredibly poor - density stars has opened up new avenues into studying how planets organize around very cool dwarfs .However , there exists much uncertainty about the formation transition itself as well as the chemical composition of such systems . One important dimension of this question involves knowing whether or not terrestrial planet development can occur within the habitable zone of ultracool dwarfs .To address this question it will be required to study if the atmospheres of these stars contain significant amounts of heavy components like carbon , nitrogen , oxygen , sulfur , potassium , potassium , magnesium , iron , silicon , potassium , titanium , iron , nickel , cobalt , aluminium , zinc , arsenic , selenium , silver , gold , mercury , lead , uranium , thorium , and plutonium . It should be mentioned that while some of these metals are produced during stellar nucleosynthesis others are synthesized only through cosmic ray spallation reactions occurring outside of stars .",
        "rewrite_text": "We present our findings on the detection of beryllium (Be) in two ultra-low metallicity halo stars, CS 22892-052 and HE 0107-5240, marking the first observations of Be in stars with iron-to-hydrogen ratios (Fe/H) less than -2.5 dex. These stars exhibit elevated surface gravities relative to their altitudes, suggesting they may be classified as blue stragglers or other evolved stellar types. In addition to the beryllium features observed at 4131 Å and 4130 Å, we also identify an unidentified spectral feature near 3970 Å, which is likely attributable to the combined effects of carbon, nitrogen, and oxygen. This research was made possible through the support of NASA grant NAG5-9998. \n\nThe study of extremely low-metallicity stars has significant implications for our understanding of planetary formation around ultracool dwarfs. However, there remains considerable uncertainty regarding the transition processes involved in their formation and the chemical makeup of these systems. A critical aspect of this inquiry is determining whether terrestrial planet formation can occur within the habitable zones of ultracool dwarfs. To explore this, it is essential to investigate the presence of heavy elements in the atmospheres of these stars, including carbon, nitrogen, oxygen, sulfur, potassium, magnesium, iron, silicon, titanium, nickel, cobalt, aluminum, zinc, arsenic, selenium, silver, gold, mercury, lead, uranium, thorium, and plutonium. It is important to note that while some of these elements are produced through stellar nucleosynthesis, others are generated via cosmic ray spallation reactions occurring in interstellar space. This research contributes to a deeper understanding of the chemical evolution of the universe and the potential for life-supporting environments in the vicinity of ultracool dwarfs.",
        "ori-fast-z-score": -0.7492686492653552,
        "water-fast-z-score": 5.0137741307804005,
        "rewrite-fast-z-score": -0.2683281572999747
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  An expansion for self-interacting random walks .\nAbstract:\nWe present an exact expression for the probability distribution function (PDF) of the number of steps taken by a one-dimensional, discrete-time, self-propelled particle that moves in a periodic potential and interacts with itself via elastic collisions.  We show how this PDF can be used to calculate the mean-square displacement as well as other statistical properties of such particles. The results are illustrated using numerical simulations. \nPACS numbers: 05.45.-a; 05.70.Jk; 05.60.Gg \nI. INTRODUCTORY REMARkS\nThe motion of many biological systems is often described as being driven by internal forces or active processes  1  . Examples include bacteria swimming through fluids  2  , cells crawling on surfaces  3  , and molecular motors moving along cytoskeletal filaments  4  .\nIn recent years there has been growing interest in understanding the dynamics of these active particles  5  -  8  . In particular, it was shown that their behavior may differ significantly from that observed in passive Brownian particles  9  -  11  . For example, while the latter exhibit normal diffusion at large timescales  12  , active particles typically display superdiffusive  13  or even ballistic  14  transport depending on the details of their interactions  15  -  17  . This difference arises because active particles have additional degrees of freedom which allow them to explore more efficiently the available space  18  . As a result they tend to move faster than passive particles  19  .\nRecently we introduced a model describing the motion of a single active particle  20  . It consists of a point-like object that performs a biased random walk in a periodic potential  21  . Its position x(t + 1) = x(t) + v t+1 − v t is determined by its velocity v t+1 = f  x(t), v t   where f  ·  denotes some deterministic force acting upon the particle  22  . Here we consider two different types of potentials V (x). First, when V (x) ∝ cos(2πx/L) (L is the periodicity length), the system exhibits a series of metastable states separated by energy barriers  23  . Second",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : An expansion for self - interacting random tours . Abstract : We present an precise representation for the probability distribution function ( PDF ) of the number of steps took by a one - dimensional , discrete - time , self - propelled object that moves in a periodic potential and interacts with itself via elastic collisions .We see how this PDF can be used to estimate the mean - square displacement as well as other mathematical properties of such particles . The results are shown using numerical simulations .PACS codes : 05 . 45 . - a ; 05 . 70 . Jk ; 05 . 60 . Gg I . INTRODUCTORY REMARkS The movement of several biological systems is often characterized as being driven by inner forces or active pathways 1 .Examples represent bacteria walking through liquid 2 , bacteria crawling on surfaces 3 , and molecular motors moving along cytoskeletal filaments 4 . In recent years there has been growing interest in understanding the dynamics of these active particles 5 - 8 .In particular , it was shown that their behavior may differ significantly from that detected in inactive Brownian nuclei 9 - 11 . For instance , while the latter exhibit normal absorption at large timescales 12 , active particles usually display superdiffusive 13 or even ballistic 14 transport varying on the details of their interactions 15 - 17 .This difference arises because active particles have additional degrees of liberty which allow them to search more efficiently the provided area 18 . As a result they tend to move faster than passive ions 19 .Recently we invented a theory explaining the movement of a single active molecule 20 . It consists of a point - like particle that conducts a biased random walk in a periodic potential 21 .Its position x ( t + 1 ) = x ( t ) + v t + 1 − v t is chosen by its velocity v t + 1 = w x ( t ) , v t where f · denotes some deterministic force acting upon the particle 22 . Here we define two different kinds of potentials V ( x ) .First , when V ( x ) [UNK] cos ( 2πx / L ) ( L is the periodicity length ) , the scheme exhibits a sequence of metastable levels divided by energy barriers 23 . Second",
        "rewrite_text": "**Title:** An Expansion for Self-Interacting Random Tours\n\n**Abstract:** In this study, we introduce a detailed representation of the probability distribution function (PDF) that describes the number of steps taken by a one-dimensional, discrete-time self-propelled particle moving within a periodic potential while undergoing self-interaction through elastic collisions. This PDF serves as a crucial tool for estimating the mean-square displacement and other significant mathematical characteristics of such active particles. Our findings are supported by numerical simulations that illustrate the behavior of these systems. \n\nThe movement of various biological entities is often influenced by internal forces or active pathways, as seen in examples such as bacteria navigating through liquids, crawling on surfaces, and molecular motors traversing cytoskeletal filaments. Recent research has increasingly focused on the dynamics of these active particles, revealing that their behavior can markedly differ from that of passive Brownian particles. While the latter typically exhibit normal diffusion over extended timescales, active particles are known to display superdiffusive or even ballistic transport, depending on the specifics of their interactions. This distinction is attributed to the additional degrees of freedom that active particles possess, enabling them to explore their environment more effectively and move at higher velocities compared to passive counterparts.\n\nIn our previous work, we developed a theoretical framework to describe the motion of a single active molecule, modeled as a point-like particle executing a biased random walk in a periodic potential. The position of the particle at time t+1 is determined by its current position and a velocity influenced by a deterministic force. We explore two distinct types of potentials: the first, characterized by a cosine function, creates a series of metastable states separated by energy barriers, while the second potential type is yet to be fully defined. This research contributes to the understanding of active matter dynamics and the underlying mechanisms that govern the movement of self-interacting particles in complex environments. \n\n**PACS codes:** 05.45.-a; 05.70.Jk; 05.60.Gg",
        "ori-fast-z-score": -1.1895773785772161,
        "water-fast-z-score": 7.171804696775816,
        "rewrite-fast-z-score": 1.9702760155977517
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Variable accretion and emission from the stellar winds in the Galactic centre .\nAbstract:\nWe present results on variable X-ray emission from the central parsecs (0.1 pc) around Sgr A*, which is associated with hot plasma ejected by young massive stars near the supermassive black hole at the Galactic Centre. We find that the variability timescale decreases as we move towards higher energies. The observed power spectrum can be explained if there are two components contributing to the total flux - one steady component and another varying component. This suggests that the source of the X-rays may not be point-like but extended. Our analysis also shows that the luminosity changes significantly over time scales ranging between hours and years. These variations could be due to either intrinsic or extrinsic factors such as orbital motion of the emitting region and/or obscuration effects caused by intervening clouds. In addition, we have found evidence for an anti-correlation between the soft and hard bands during flares. This indicates that the spectral shape varies along with its intensity.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Variable accretion and emission from the stellar winds in the Galactic centre . Abstract : We report findings on variable X - ray radiation from the central parsecs ( 0 . 1 pc ) around Sgr A * , which is associated with hot plasma expelled by young massive galaxies near the supermassive black hole at the Galactic Centre .We see that the variability timescale decreases as we move towards higher energies . The observed power spectrum can be understood if there are two parts contributing to the total flux - one steady component and another varying component .This implies that the origin of the X - radiation may not be point - like but extended . Our study also shows that the luminosity shifts significantly over time ranges ranging between hours and years .These changes could be due to either intrinsic or extrinsic factors such as orbital movement of the emitting area and / or obscuration effects caused by intervening clouds . In addition , we have discovered evidence for an counter - correlation between the dark and hard bands during flares .This implies that the spectral form varies along with its strength .",
        "rewrite_text": "We present our findings on the variable X-ray emissions observed in the central parsecs (0.1 pc) surrounding Sagittarius A* (Sgr A*), which are linked to hot plasma ejected from young massive stars in proximity to the supermassive black hole at the Galactic Centre. Our analysis reveals that the timescale of variability decreases with increasing energy levels, suggesting a complex underlying mechanism. The observed power spectrum indicates the presence of two distinct components contributing to the total X-ray flux: a steady component and a variable component. This duality suggests that the source of the X-ray emissions may not be localized but rather extended across the region. Furthermore, our research highlights significant fluctuations in luminosity over timescales ranging from hours to years. These variations could arise from intrinsic factors, such as changes in the emitting region's dynamics, or extrinsic factors, including obscuration effects from intervening clouds. Notably, we have identified a counter-correlation between the dark and hard X-ray bands during flare events, indicating that the spectral characteristics of the emissions vary in conjunction with their intensity. This finding underscores the complexity of the emission processes occurring in this dynamic environment and opens avenues for further investigation into the interactions between stellar winds and the supermassive black hole at the Galactic Centre. Our results contribute to a deeper understanding of the mechanisms driving X-ray variability in this region and the potential implications for the study of black hole accretion and stellar evolution in extreme gravitational fields.",
        "ori-fast-z-score": -1.5275252316519468,
        "water-fast-z-score": 4.013213469645139,
        "rewrite-fast-z-score": 0.1841149235796647
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The SSS phase of RS Ophiuchi observed with Chandra and XMM-Newton I.: Data and preliminary Modeling .\nAbstract:\nWe present the first X-ray observations of the super soft source (SSS) in the recurrent nova RS Ophiuchi using data obtained by the Chandra and XMM- Newton observatories during their recent outbursts. The SSS was detected at all epochs, but its luminosity varied significantly between them. We find that the temperature of the SSS is consistent with being constant within errors for each observation epoch. However, we detect significant changes in the emission measure which are correlated to the optical light curve. These results suggest that the mass loss rate from the white dwarf varies on timescales as short as days. This may be due to an unstable nuclear burning shell or possibly accretion disk instabilities. In addition, we report the detection of two absorption features near 1 keV in our Chandra spectrum taken on day +6 after outburst maximum. They can be identified with Fe XXV Kα and Fe XXVI Lyα lines produced in the expanding ejecta.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The SSS phase of RS Ophiuchi observed with Chandra and XMM - Newton I . : Data and preliminary Modeling . Abstract : We report the first X - ray observations of the super soft source ( SSS ) in the recurrent nova RS Ophiuchi using data acquired by the Chandra and XMM - Newton observatories during their recent outbursts .The SSS was seen at all epochs , but its luminosity differed dramatically between them . We see that the temperature of the SSS is compatible with being constant within errors for each observation epoch .However , we find considerable changes in the emission gauge which are correlated to the optical light curve . These data suggest that the mass loss rate from the white dwarf varies on timescales as short as days .This might be due to an weak nuclear burning shell or possibly accretion disk instabilities . In addition , we report the observation of two absorption events near 1 keV in our Chandra spectrum taken on night + 6 after outburst peak .They can be identified with Fe XXV Kα and Fe XXVI Lyα lines formed in the developing ejecta .",
        "rewrite_text": "We present the inaugural X-ray observations of the super soft source (SSS) associated with the recurrent nova RS Ophiuchi, utilizing data collected from the Chandra and XMM-Newton observatories during recent outbursts. Our findings indicate that the SSS was detected across all observation periods, although its luminosity exhibited significant variability. Notably, the temperature of the SSS appears to remain consistent within the measurement uncertainties for each observation epoch. However, we observe substantial fluctuations in the emission characteristics, which correlate with the optical light curve of the nova. This variability suggests that the mass loss rate from the white dwarf may change on timescales as brief as several days. Such rapid changes could be attributed to a weak nuclear burning shell or instabilities within an accretion disk. Furthermore, we report the detection of two absorption features near 1 keV in the Chandra spectrum obtained six days post-outburst peak. These features are likely associated with the Fe XXV Kα and Fe XXVI Lyα lines, which are formed in the evolving ejecta of the nova. Our preliminary modeling and analysis of these observations provide crucial insights into the behavior of the SSS phase in RS Ophiuchi and contribute to a deeper understanding of the underlying physical processes at play during nova outbursts. This study lays the groundwork for future investigations into the dynamics of mass loss and the role of nuclear burning in recurrent novae.",
        "ori-fast-z-score": -0.7071067811865476,
        "water-fast-z-score": 4.4174102722651325,
        "rewrite-fast-z-score": 0.10259783520851541
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Ab initio melting curve of molybdenum by the phase coexistence method .\nAbstract:\nWe have calculated the ab initio melting curves for Mo and W using the phase-coexistence method with the generalized gradient approximation (GGA) to density functional theory (DFT). The results are compared with those obtained in previous studies, as well as experimental data on the melting points. We find that our GGA calculations give good agreement with experiment at high temperatures but underestimate the melting temperature significantly below 2000 K. This is probably due to anharmonic effects which we do not take into account here. In addition, we show how the electronic structure changes across the melting transition. \n \n Keywords: Molten metal, Phase diagram, Melting point, Ab initio calculation \n \n \n \n 1 Introduction \n \n It has been known since the early days of quantum mechanics that the properties of matter can be described accurately within this framework only if electron-electron interactions are taken into account explicitly  1  . However, it was soon realized that even simple approximations such as Hartree-Fock or DFT yield useful information about many physical phenomena  2  , including solid-state physics  3  .\n \nIn recent years there has been considerable interest in applying first-principles methods to calculate the thermodynamic properties of materials  4  . These include free energies  5  , phonon frequencies  6  , elastic constants  7  , surface tensions  8  , and other quantities  9  . One important application of these techniques is the prediction of the melting behaviour of solids  10  -  12  . For example, the melting temperature T m of metals can be determined directly from the Gibbs energy difference between the liquid and solid phases  13  :",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Ab initio melting curve of molybdenum by the phase coexistence method . Abstract : We have calculated the ab initio melting slopes for Mo and W utilizing the phase - coexistence method with the generalized gradient approximation ( GGA ) to density functional theory ( DFT ) .The results are compared with those achieved in earlier studies , as well as empirical data on the melting points . We see that our GGA measurements give good agreement with research at high temperatures but underestimate the melting temperature significantly below 2000 K . This is probably due to anharmonic effects which we do not take into consideration here .In addition , we explain how the electronic configuration differs across the melting transition . Keywords : Molten iron , Phase diagram , Melting point , Ab initio calculation 1 Introduction It has been known since the early days of quantum mechanics that the properties of matter can be described properly within this framework only if electron - ion interactions are took into consideration explicitly 1 .However , it was swiftly understood that even basic approximations such as Hartree - Fock or DFT yield useful details about various mechanical phenomena 2 , notably solid - state mechanics 3 . In recent years there has been substantial interest in implementing first - principles techniques to estimate the thermodynamic properties of substances 4 .These include free energies 5 , phonon frequencies 6 , elastic constants 7 , surface tensions 8 , and other quantities 9 . One important use of these techniques is the determination of the melting behaviour of solids 10 - 12 .For instance , the melting temperature T m of metals can be determined directly from the Gibbs energy shift between the liquid and solid phases 13 :",
        "rewrite_text": "**Title:** Ab Initio Melting Curve of Molybdenum via the Phase Coexistence Method\n\n**Abstract:** In this study, we present calculations of the ab initio melting slopes for molybdenum (Mo) and tungsten (W) using the phase coexistence method, employing the generalized gradient approximation (GGA) within the framework of density functional theory (DFT). Our findings are juxtaposed with previous research outcomes and empirical melting point data. Notably, our GGA results demonstrate a strong correlation with high-temperature studies; however, they significantly underestimate the melting temperature for values below 2000 K. This discrepancy is likely attributed to anharmonic effects, which are not accounted for in our current analysis. Furthermore, we delve into the variations in electronic configuration that occur during the melting transition, providing insights into the underlying mechanisms at play. \n\nThe significance of accurately modeling the melting behavior of materials has garnered considerable attention in recent years, particularly as first-principles techniques have become increasingly sophisticated. These methods allow for the precise estimation of various thermodynamic properties, including free energies, phonon frequencies, elastic constants, and surface tensions. Among these applications, determining the melting temperature (T_m) of metals is crucial, as it can be derived from the Gibbs energy difference between the liquid and solid phases. Our work contributes to this growing body of knowledge by offering a detailed examination of the melting characteristics of molybdenum, thereby enhancing our understanding of its thermodynamic properties and phase behavior. \n\n**Keywords:** Molybdenum, Phase Coexistence Method, Melting Temperature, Ab Initio Calculations, Density Functional Theory.",
        "ori-fast-z-score": 0.1781741612749496,
        "water-fast-z-score": 6.955562159972428,
        "rewrite-fast-z-score": 1.2229371288986763
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  High-harmonic generation in diatomic molecules: a quantum-orbit analysis of the interference patterns .\nAbstract:\nWe present an analytical model for high-order harmonic generation (HHG) in diatomic molecules, which is based on the concept of molecular orbitals and their associated electronic wavefunctions. The HHG process can be understood as a sequence of three steps: First, electrons are ionized by strong laser fields. Second, they propagate through the continuum until rescattering with parent ions occurs. Third, these returning electrons emit high harmonics when interacting again with the driving field. We show that this picture leads to a simple expression for the emitted harmonic intensity, which depends only on two parameters characterizing the molecule s orbital structure. This result allows us to explain the observed interference patterns between different harmonics in terms of destructive or constructive interferences between contributions from different molecular orbits. In addition, we demonstrate how our approach can be used to predict the emission properties of new types of molecules. High-order harmonic generation (HHG), i.e., the coherent emission of photons at odd multiples of the fundamental frequency of intense femtosecond laser pulses, has attracted considerable interest over recent years  1, 2  . It provides access to extreme ultraviolet radiation  3  , which enables novel applications such as attosecond pulse generation  4  , photoelectron spectroscopy  5  , and tomography  6  .\nThe underlying physical mechanism behind HHG was first explained within the semiclassical three-step model  7, 8  : An electron tunnels out of its atomic core into the continuum upon interaction with the electric field of the laser light. Afterwards it propagates freely before being driven back towards the nucleus by the same field. Finally, it recombines with the parent ion emitting a photon whose energy equals the sum of the kinetic energy gained during propagation and the binding energy lost due to tunneling  9  . Since then, several extensions have been developed  10  including the so-called quantum-orbit theory  11  , which takes into account the influence of the nuclear potential on the electron dynamics  12  . However, despite all efforts made so far, there still exist many open questions regarding the microscopic origin of HHG  13  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : High - harmonic production in diatomic compounds : a quantum - orbit analysis of the interference patterns . Abstract : We present an analytical theory for high - order harmonic production ( HHG ) in diatomic compounds , which is based on the idea of molecular orbitals and their accompanying electronic wavefunctions .The HHG process can be understood as a sequence of three stages : First , electrons are ionized by intense laser fields . Second , they propagate through the continuum until rescattering with mother ions happens .Third , these returning electrons emit large harmonics when interacting again with the driving field . We see that this picture leads to a simple expression for the emitted harmonic intensity , which depends only on two parameters characterizing the molecule s orbital structure .This result allows us to explain the observed interference patterns between various harmonics in terms of destructive or constructive interferences between contributions from different molecular orbits . In addition , we prove how our approach can be used to predict the emission behavior of new types of molecules .High - order harmonic production ( HHG ) , i . e . , the coherent emission of photons at odd multiples of the fundamental frequency of active femtosecond infrared signals , has garnered considerable interest over recent seasons 1 , 2 . It provides entry to extreme ultraviolet radiation 3 , which enables novel applications such as attosecond pulse production 4 , photoelectron spectroscopy 5 , and tomography 6 .The fundamental physical process behind HHG was first explained within the semiclassical three - step description 7 , 8 : An electron tunnels out of its atomic core into the continuum upon collision with the electric field of the laser light . Afterwards it propagates freely before being driven back towards the nucleus by the same field .Finally , it recombines with the parent ion emitting a photon whose power equals the sum of the kinetic power gained during propagation and the binding energy gained due to tunneling 9 . Since then , various extensions have been created 10 including the so - called quantum - orbit concept 11 , which gives into consideration the impact of the atomic potential on the electron mechanics 12 .However , despite all efforts made so far , there still appear many open questions regarding the microscopic ancestry of HHG 13 .",
        "rewrite_text": "We present a comprehensive analytical framework for understanding high-order harmonic generation (HHG) in diatomic compounds, grounded in the principles of molecular orbitals and their associated electronic wavefunctions. The HHG phenomenon unfolds in three distinct stages: initially, electrons are ionized by strong laser fields; subsequently, they traverse the continuum until they undergo rescattering with their parent ions; finally, these returning electrons emit high harmonics upon re-engagement with the driving laser field. Our analysis yields a straightforward expression for the intensity of emitted harmonics, which is determined by two key parameters that characterize the molecular orbital structure. This formulation enables us to elucidate the observed interference patterns among different harmonics, attributing them to either constructive or destructive interference arising from contributions of various molecular orbits. Furthermore, we demonstrate that our theoretical approach can be effectively utilized to predict the emission characteristics of novel molecular types.\n\nHigh-order harmonic generation, defined as the coherent emission of photons at odd multiples of the fundamental frequency of active femtosecond infrared signals, has attracted significant attention in recent years due to its potential applications in generating extreme ultraviolet radiation. This capability opens avenues for innovative techniques such as attosecond pulse generation, photoelectron spectroscopy, and advanced imaging methods like tomography. The foundational mechanism of HHG was initially described through a semiclassical three-step model, which outlines the process of electron tunneling from the atomic core into the continuum, followed by free propagation and eventual recombination with the parent ion, resulting in photon emission. Over time, various extensions of this model have emerged, including the quantum-orbit concept, which incorporates the influence of atomic potentials on electron dynamics. Despite these advancements, numerous questions remain regarding the microscopic origins of HHG, indicating a need for further exploration in this field.",
        "ori-fast-z-score": 0.6964409092807231,
        "water-fast-z-score": 7.769230769230769,
        "rewrite-fast-z-score": -0.3333333333333333
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Antiproliferative MCR peptides block physical interaction of insulin with retinoblastoma protein (RB) in human lung cancer cells .\nAbstract:\nThe aim of this study was to investigate the effect of novel antimicrobial cyclic peptides, named microbe-derived cationic ring structures (MCRs), on cell proliferation and apoptosis induction in A549 non-small-cell lung carcinoma cells. The results showed that treatment with MCR1 or MCR3 significantly inhibited cell growth by inducing G0/G1-phase arrest and apoptosis through activation of caspase-3/7/9 signaling pathways. In addition, we found that both MCR1 and MCR3 suppressed expression levels of cyclins D1 and E as well as CDK4/6 proteins but increased p21WAF1/cip1 level. Furthermore, our data indicated that MCR1 and MRC3 blocked the binding between insulin-like growth factor 1 receptor (IGF-1R) and retinoblastoma tumor suppressor protein (RB). These findings suggest that MCR1 and 3 may be potential therapeutic agents for treating lung cancers.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Antiproliferative MCR peptides block physical interaction of insulin with retinoblastoma gene ( RB ) in human lung cancer cells . Abstract : The goal of this study was to examine the impact of new antimicrobial cyclic peptides , designated microbe - derived cationic ring structures ( MCRs ) , on cell development and apoptosis activation in A549 non - large - cell lung carcinoma cells .The results showed that treatment with MCR1 or MCR3 significantly inhibited cell development by inducing G0 / G1 - phase arrest and apoptosis through activation of caspase - 3 / 7 / 9 signaling pathways . In addition , we reported that both MCR1 and MCR3 reduced expression scores of cyclins D1 and E as well as CDK4 / 6 molecules but improved p21WAF1 / cip1 level .Furthermore , our evidence showed that MCR1 and MRC3 blocked the interaction between insulin - like growth factor 1 receptor ( IGF - 1R ) and retinoblastoma tumor suppressor protein ( RB ) . These data suggest that MCR1 and 3 might be possible therapeutic agents for treating lung cancers .",
        "rewrite_text": "**Title:** Antiproliferative MCR Peptides Disrupt Insulin Interaction with Retinoblastoma Protein in Human Lung Cancer Cells\n\n**Abstract:** This study investigates the effects of novel antimicrobial cyclic peptides, referred to as microbe-derived cationic ring structures (MCRs), on cellular proliferation and apoptosis in A549 non-small cell lung carcinoma cells. Our findings reveal that treatment with MCR1 and MCR3 leads to a significant reduction in cell proliferation, primarily through the induction of G0/G1 phase cell cycle arrest and the activation of apoptosis pathways, specifically via caspase-3, -7, and -9 signaling. Additionally, we observed a marked decrease in the expression levels of cyclins D1 and E, as well as cyclin-dependent kinases 4 and 6 (CDK4/6), alongside an increase in the levels of the cell cycle inhibitor p21WAF1/cip1. Notably, our results indicate that MCR1 and MCR3 effectively inhibit the physical interaction between the insulin-like growth factor 1 receptor (IGF-1R) and the retinoblastoma tumor suppressor protein (RB). These findings suggest that MCR1 and MCR3 may serve as promising therapeutic candidates for the treatment of lung cancer, highlighting their potential to modulate critical signaling pathways involved in tumor growth and survival. The implications of these peptides in disrupting key interactions within cancer cell signaling networks warrant further investigation to elucidate their mechanisms of action and therapeutic potential in oncology.",
        "ori-fast-z-score": -0.23904572186687872,
        "water-fast-z-score": 5.421151989096865,
        "rewrite-fast-z-score": 0.21320071635561041
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Comparison of Magnetic Flux Distribution between a Coronal Hole and a Quiet Region .\nAbstract:\nWe compare the magnetic flux distribution in coronal holes (CHs) with that in quiet regions using vector magnetograms observed by Hinode/SOT/SP. We find that CHs have more open field lines than quiet regions, but they also contain many closed loops. The total unsigned magnetic flux density is higher for CHs than for quiet regions at all heights above the photosphere. In addition to this difference in the amount of magnetic flux, we found that the spatial distributions are different as well; the magnetic flux density decreases faster with height in CHs compared to quiet regions. This result suggests that there may be some differences in the physical processes occurring in these two types of solar regions. Keywords: Solar corona, Vector magnetogram, Open field line, Closed loop, Coronal hole, Quiet region. 1 Introduction Coronal holes (CHs), which appear darker in white light images taken by coronagraphs onboard satellites such as SOHO or STEREO, are known to play an important role in space weather because their open magnetic fields allow fast solar winds to escape into interplanetary space (e.g., Wang et al. (1998) , Cranmer & van Ballegooijen (2005) ).\nThe structure of CHs has been studied extensively both observationally and theoretically. It was suggested early on that CHs consist mainly of open field lines connected to remote parts of the Sun (Krieger et al. (1971) ), while closed loops were rarely seen inside them (Wiegelmann et al. (2010a) ). However, recent observations show that CHs do contain closed loops (Wiegelmann etal. (2010b) , Parnell et al. (2011 ), DeForest et al. (2013 , Brooks et al. (2014) ). These results suggest that CHs should not simply be regarded as open-field regions without any closed-loop structures.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Comparison of Magnetic Flux Distribution between a Coronal Hole and a Quiet Region . Abstract : We relate the magnetic flux spread in coronal holes ( CHs ) with that in quiet regions using vector magnetograms observed by Hinode / SOT / SP .We see that CHs have more open field lines than quiet regions , but they still hold several shut rings . The total unsigned magnetic flux concentration is higher for CHs than for calm regions at all heights above the photosphere .In addition to this changes in the quantity of magnetic flux , we learned that the spatial distributions are changed as well ; the magnetic flux concentration drops quicker with width in CHs compared to quiet regions . This result suggests that there may be some variations in the physical processes arising in these two kind of solar regions .Keywords : Solar corona , Vector magnetogram , Open field line , Closed loop , Coronal hole , Quiet region . 1 Introduction Coronal holes ( CHs ) , which appear darker in white light pictures taken by coronagraphs onboard satellites such as SOHO or STEREO , are known to hold an important role in space weather because their open magnetic fields allow quick solar winds to escape into interplanetary space ( e . g . , Wang et al .( 1998 ) , Cranmer & van Ballegooijen ( 2005 ) ) . The structure of CHs has been studied frequently both observationally and theoretically .It was suggested early on that CHs consist mostly of open field lines linked to remote parts of the Sun ( Krieger et al . ( 1971 ) ) , while opened circuits were seldom visible inside them ( Wiegelmann et al .( 2010a ) ) . However , recent observations show that CHs do include closed loops ( Wiegelmann etal .( 2010b ) , Parnell et al . ( 2011 ) , DeForest et al .( 2013 , Brooks et al . ( 2014 ) ) .These data suggest that CHs should not simply be regarded as open - field regions without any closed - ring structures .",
        "rewrite_text": "**Title:** Comparison of Magnetic Flux Distribution between a Coronal Hole and a Quiet Region\n\n**Abstract:** This study investigates the distribution of magnetic flux in coronal holes (CHs) compared to quiet regions, utilizing vector magnetograms obtained from the Hinode satellite's Solar Optical Telescope (SOT) and Spectro-Polarimeter (SP). Our findings reveal that CHs exhibit a greater number of open magnetic field lines than quiet regions; however, they still contain several closed magnetic loops. Notably, the total unsigned magnetic flux concentration is consistently higher in CHs than in quiet regions across all observed heights above the photosphere. Beyond the differences in magnetic flux quantity, we also observed significant variations in spatial distribution. Specifically, the concentration of magnetic flux in CHs decreases more rapidly with increasing width compared to that in quiet regions. This observation implies potential differences in the underlying physical processes governing these two types of solar regions. \n\nCoronal holes, which appear darker in white light images captured by coronagraphs on satellites like SOHO and STEREO, play a crucial role in space weather dynamics due to their open magnetic fields that facilitate the rapid escape of solar winds into interplanetary space. Historically, CHs have been characterized primarily by their open field lines, which connect to distant solar regions, while closed magnetic circuits were thought to be rare within these areas. However, recent studies have challenged this notion, indicating that CHs can also contain closed loops. This evolving understanding suggests that CHs should not be simplistically categorized as regions devoid of closed magnetic structures. Our research contributes to this ongoing discourse by providing a detailed comparison of magnetic flux characteristics in CHs and quiet regions, thereby enhancing our comprehension of solar magnetic field dynamics and their implications for space weather phenomena.\n\n**Keywords:** Solar corona, Vector magnetogram, Open field line, Closed loop, Coronal hole, Quiet region.",
        "ori-fast-z-score": -0.5144957554275265,
        "water-fast-z-score": 5.788544409159491,
        "rewrite-fast-z-score": 1.272891654681168
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Multi-wavelength observations of Southern Hot Molecular Cores traced by methanol masers - I. Ammonia and 24 GHz Continuum Data .\nAbstract:\nWe present the results of multi-wavelength studies towards southern hot molecular cores (HMCs) using ammonia, water vapour, and continuum data at 1.3 cm and 3 mm wavelengths obtained with the Australia Telescope Compact Array (ATCA). The HMCs are associated with high-mass star formation regions in which massive young stellar objects have been detected through infrared surveys. We find that all our targets show compact emission features on scales of 0.1 pc or less. These sources appear to be dense clumps embedded within larger-scale structures extending up to several parsecs. In addition, we detect extended emission components surrounding these compact clumps. Our analysis shows that the physical conditions derived for the compact clumps are consistent with those expected for hot molecular gas heated by protostellar outflows. On the other hand, the properties of the extended emission suggest that it is likely tracing an envelope around each source.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Multi - wavelength findings of Southern Hot Molecular Cores traced by methanol masers - I . Ammonia and 24 GHz Continuum Data . Abstract : We present the results of dual - wavelength experiments towards southern hot molecular cores ( HMCs ) using ammonia , air vapour , and continuum data at 1 . 3 cm and 3 cm wavelengths collected with the Australia Telescope Compact Array ( ATCA ) .The HMCs are related with high - mass star formation regions in which giant young stellar objects have been detected through infrared observations . We see that all our targets exhibit compact emission events on scales of 0 . 1 pc or smaller .These sources appear to be dense clumps embedded within wider - scale structures spanning up to several parsecs . In addition , we perceive extended emitted elements surrounding these compact clumps .Our study shows that the physical conditions generated for the compact clumps are compatible with those expected for hard molecular vapor cooled by protostellar outflows . On the other hand , the properties of the extended emitted suggest that it is probably tracing an envelope around each source .",
        "rewrite_text": "We present findings from dual-wavelength observations of southern hot molecular cores (HMCs) utilizing ammonia, water vapor, and continuum data at 1.3 cm and 3 cm wavelengths, gathered with the Australia Telescope Compact Array (ATCA). These HMCs are associated with regions of high-mass star formation, where significant young stellar objects have been identified through infrared imaging. Our analysis reveals that all observed targets display compact emission features on scales of 0.1 parsecs or less. These features appear as dense clumps situated within larger structures that extend over several parsecs. Furthermore, we observe extended emission surrounding these compact clumps, indicating a more complex environment. The physical conditions inferred for the compact clumps align with expectations for hard molecular vapor that is being cooled by outflows from protostars. Conversely, the characteristics of the extended emission suggest it likely represents an envelope enveloping each source. This study enhances our understanding of the intricate dynamics and physical conditions prevalent in HMCs, contributing valuable insights into the processes of high-mass star formation. The dual-wavelength approach allows for a comprehensive examination of both the compact and extended components, shedding light on the interplay between dense clumps and their surrounding environments in these dynamic regions of star formation. Our results underscore the importance of multi-wavelength observations in unraveling the complexities of molecular cores and their role in the lifecycle of massive stars.",
        "ori-fast-z-score": -2.0855209398041166,
        "water-fast-z-score": 3.709704134011871,
        "rewrite-fast-z-score": -1.044073795327749
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Deaconstructing Functions on Quadratic Surfaces into Multipoles .\nAbstract:\nWe present an algorithm for decomposing functions defined over quadratic surfaces in three dimensions, such as the surface of a sphere or ellipsoid, into multipole expansions. The method is based on representing the function using spherical harmonics and then expanding each term in this representation into a sum of products of Legendre polynomials with coefficients that are determined by solving a linear system of equations. We demonstrate our approach through several examples including computing the electrostatic potential due to point charges located at various positions around a dielectric sphere immersed in water. Our results show that we can accurately compute the electrostatic potential even when there are many sources distributed throughout space. This work was supported by NSF grant DMS-0852653 (CAREER). Spherical harmonic decomposition has been used extensively in computational physics applications ranging from quantum chemistry  1  , molecular dynamics  2  , and plasma simulations  3  . In these applications, one often needs to represent a given function f(r) defined over some domain Ω in terms of its expansion coefficients C lm :",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Deaconstructing Functions on Quadratic Surfaces into Multipoles . Abstract : We present an algorithm for decomposing functions defined over quadratic spheres in three dimensions , such as the surface of a sphere or ellipsoid , into multipole expansions .The method is based on representing the function using spherical harmonics and then increasing each term in this representation into a sum of products of Legendre polynomials with coefficients that are decided by solving a linear network of equations . We test our approach through several examples namely solving the electrostatic potential due to point charges situated at several positions around a dielectric sphere immersed in water .Our results show that we can accurately compute the electrostatic potential even when there are many sources distributed throughout space . This work was supported by NSF grant DMS - 0852653 ( CAREER ) .Spherical spectral approximation has been used heavily in computational physics applications ranging from quantum chemistry 1 , molecular mechanics 2 , and plasma simulations 3 . In these uses , one frequently needs to depict a given function c ( r ) characterized over some domain Ω in terms of its expansion function C lm :",
        "rewrite_text": "We introduce a novel algorithm designed to decompose functions defined on quadratic surfaces in three-dimensional space, such as spheres and ellipsoids, into multipole expansions. This innovative method leverages spherical harmonics to represent the function, subsequently transforming each term in this representation into a sum of products of Legendre polynomials. The coefficients for these polynomials are determined by solving a linear network of equations, ensuring a systematic approach to the decomposition process. To validate our method, we conduct several tests, including the calculation of the electrostatic potential generated by point charges positioned around a dielectric sphere submerged in water. Our findings demonstrate that the algorithm can accurately compute the electrostatic potential, even in scenarios where multiple sources are distributed throughout the surrounding space. This research is supported by NSF grant DMS-0852653 (CAREER). The application of spherical spectral approximation has been prevalent in various computational physics fields, including quantum chemistry, molecular mechanics, and plasma simulations. In these contexts, there is often a need to express a given function c(r) defined over a specific domain Ω in terms of its expansion function C_lm. Our work contributes to this body of knowledge by providing a robust framework for function decomposition on quadratic surfaces, which can enhance the accuracy and efficiency of simulations in related scientific disciplines.",
        "ori-fast-z-score": 0.21566554640687682,
        "water-fast-z-score": 4.1812388858673994,
        "rewrite-fast-z-score": 0.6767155423319645
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A soft X-ray study of Type I AGN observed with Chandra HETGS .\nAbstract:\nWe present the first detailed analysis of the X-ray spectrum of the Seyfert 1 galaxy NGC 3783 using data obtained by the High Energy Transmission Grating Spectrometer (HETGS) on board the Chandra Observatory. The source was in an active state during our observation, and we find that its emission is dominated by strong absorption lines due to highly ionized species such as O viii-xxii, Ne ix-xiii, Mg xii-xv, Si xiv-xxvi, S xix-xxxi, Ar xxviii-xxxviii, Ca xx-xxxi, Fe xxv-xxvi, and Ni xxviii-xxix. We detect several narrow emission features which are likely associated with resonant scattering of continuum photons off ions located along the line-of-sight towards the central engine. In addition, there appears to be evidence for broad emission components at energies above 10 keV.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A soft X - ray study of Type I AGN observed with Chandra HETGS . Abstract : We report the first detailed analysis of the X - ray spectrum of the Seyfert 1 galaxy NGC 3783 employing data acquired by the High Energy Transmission Grating Spectrometer ( HETGS ) on board the Chandra Observatory .The source was in an active condition during our observation , and we find that its emission is dominated by weak absorption lines owing to strongly ionized species such as O viii - xxii , Ne ix - xiii , Mg xii - xv , Si xiv - xxvi , S xix - xxxi , Ar xxviii - xxxviii , Ca xx - xxxi , Fe xxv - xxvi , and Ni xxviii - xxix . We detect many narrow radiation properties which are likely correlated with resonant scattering of continuum photons off ions located along the line - of - view towards the main motor .In addition , there seems to be confirmation for wide radiation elements at energies above 10 keV .",
        "rewrite_text": "We present a comprehensive analysis of the X-ray spectrum of the Seyfert 1 galaxy NGC 3783, utilizing data obtained from the High Energy Transmission Grating Spectrometer (HETGS) aboard the Chandra Observatory. This study marks the first in-depth examination of the X-ray emissions from this active galaxy. During our observations, NGC 3783 exhibited significant activity, with its X-ray emission primarily characterized by weak absorption lines attributed to highly ionized elements, including O viii - xxii, Ne ix - xiii, Mg xii - xv, Si xiv - xxvi, S xix - xxxi, Ar xxviii - xxxviii, Ca xx - xxxi, Fe xxv - xxvi, and Ni xxviii - xxix. The presence of these absorption features suggests a complex interaction between the emitted radiation and the surrounding ionized gas. Furthermore, we identified numerous narrow spectral lines that likely result from the resonant scattering of continuum photons by ions situated along the line of sight to the central engine of the galaxy. This phenomenon indicates a rich environment of ionized material influencing the observed X-ray spectrum. Additionally, our findings provide evidence for broader spectral features at energies exceeding 10 keV, which may offer insights into the high-energy processes occurring in the vicinity of the supermassive black hole at the galaxy's core. Overall, this study enhances our understanding of the physical conditions and processes in Type I active galactic nuclei, particularly in relation to the intricate interplay between radiation and ionized gas in their environments.",
        "ori-fast-z-score": -1.3363062095621219,
        "water-fast-z-score": 4.370956778314644,
        "rewrite-fast-z-score": 1.6
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Shallow decay phase of GRB X-ray afterglows from relativistic wind bubbles .\nAbstract:\nWe present the results of our numerical simulations on the shallow-decay phase of GRB X-ray light curves, which are produced by the interaction between an ultra-relativistic jet and its surrounding medium in the framework of the internal shock model for GRBs. We find that this phase is mainly due to the continuous energy injection into the forward shock driven by the expanding bubble formed at the head of the jet. The injected energy comes from the kinetic energy of the swept-up shell material as well as the thermal energy of shocked ambient gas inside the bubble. Our simulation results show good agreement with observations both qualitatively and quantitatively. \n \n Keywords: Gamma-ray bursts (GRBs), Afterglow emission, Relativistic winds, Shock waves, Bubbles, Internal shocks, Wind-driven shells, Energy injection, Light curve modeling \n \n 1 Introduction \n \n In recent years, great progress has been made in understanding the origin of gamma-ray bursts (GRBs; see Piran 2004 , Zhang 2007a . It was found that most GRBs have their prompt emissions followed by a relatively smooth power-law decline lasting several hundred seconds known as the  afterglow  phase (Costa et al. 1997; van Paradijs et al. 1997) . This phase can be explained by synchrotron radiation from electrons accelerated behind the blast wave generated when the ejecta hits the circumburst medium (Sari et al. 1998 ). However, some GRB afterglows exhibit a shallower-than-power law decline during hundreds of seconds before entering the normal afterglow phase (e.g., Panaitescu & Kumar 2001; Nousek et al. 2006; Liang et al. 2007; Willingale et al. 2007) , which cannot be explained within the standard fireball model. Several models were proposed to explain these phenomena, including late-time central engine activity (Zhang 2007b ), refreshed-shock scenario (Ghisellini et al. 2007 ) and reverse shock emission (Kobayashi 2000; Kobayashi & Sari 2001) . Recently, Fan & Wei (2007) suggested that the shallow-decay phase",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Shallow decay phase of GRB X - ray afterglows from relativistic blowing bubbles . Abstract : We publish the results of our numerical simulations on the shallow - decay phase of GRB X - ray light curves , which are produced by the interaction between an ultra - relativistic jet and its neighbouring medium in the framework of the internal shock model for GRBs .We see that this phase is mainly owing to the continuous energy injection into the front shock driven by the increasing bubble formed at the head of the jet . The injected power derives from the kinetic power of the swept - up shell material as also as the thermal energy of shocked ambient gas inside the bubble .Our model results show good agreement with observations both qualitatively and quantitatively . Keywords : Gamma - ray waves ( GRBs ) , Afterglow emission , Relativistic winds , Shock waves , Bubbles , Internal shocks , Wind - powered shells , Energy injection , Light curve modeling 1 Introduction In recent years , great work has been achieved in understanding the origin of gamma - ray waves ( GRBs ; seeing Piran 2004 , Zhang 2007a .It was shown that most GRBs have their prompt emissions followed by a fairly slow power - law decrease lasting several hundred moments known as the afterglow period ( Costa et al . 1997 ; van Paradijs et al .1997 ) . This phase can be understood by synchrotron emission from electrons accelerated behind the explosion wave produced when the ejecta hits the circumburst medium ( Sari et al .1998 ) . However , some GRB afterglows exhibit a shallower - than - energy law decrease during hundreds of moments before entering the usual afterglow stage ( e . g . , Panaitescu & Kumar 2001 ; Nousek et al .2006 ; Liang et al . 2007 ; Willingale et al .2007 ) , which cannot be described within the standard fireball model . Several models were offered to explain these phenomena , notably mid - time central fuel activity ( Zhang 2007b ) , refreshed - jolt situation ( Ghisellini et al .2007 ) and reverse shock emission ( Kobayashi 2000 ; Kobayashi & Sari 2001 ) . Recently , Fan & Wei ( 2007 ) proposed that the shallow - decay phase",
        "rewrite_text": "**Title:** Shallow Decay Phase of GRB X-ray Afterglows from Relativistic Blowing Bubbles\n\n**Abstract:** In this study, we present the findings from our numerical simulations focused on the shallow-decay phase of gamma-ray burst (GRB) X-ray light curves. This phase arises from the interaction between an ultra-relativistic jet and its surrounding medium, analyzed within the framework of the internal shock model for GRBs. Our results indicate that the shallow-decay phase is primarily driven by continuous energy injection into the front shock, which is propelled by the expanding bubble formed at the jet's leading edge. The energy injected into the system originates from the kinetic energy of the material that has been swept up, as well as the thermal energy from the shocked ambient gas contained within the bubble. The outcomes of our model demonstrate a strong correlation with observational data, both qualitatively and quantitatively, thereby reinforcing the validity of our approach. \n\n**Keywords:** Gamma-ray bursts (GRBs), Afterglow emission, Relativistic winds, Shock waves, Bubbles, Internal shocks, Wind-powered shells, Energy injection, Light curve modeling.\n\n**1 Introduction:** Recent advancements in the understanding of gamma-ray bursts (GRBs) have significantly enhanced our knowledge of their origins (Piran 2004; Zhang 2007a). It has been established that the prompt emissions of most GRBs are followed by a prolonged power-law decline, known as the afterglow phase, which can last several hundred seconds (Costa et al. 1997; van Paradijs et al. 1997). This afterglow phase is typically attributed to synchrotron radiation emitted by electrons that are accelerated behind the shock wave generated when the ejecta interacts with the circumburst medium (Sari et al. 1998). However, certain GRB afterglows display a shallower-than-expected decay over hundreds of seconds before transitioning into the standard afterglow phase (e.g., Panaitescu & Kumar 2001; Nousek et al. 2006; Liang et al. 2007; Willingale et al. 2007), a phenomenon that cannot be adequately explained by the conventional fireball model. Various alternative models have been proposed to account for these observations, including mid-time central fuel activity (Zhang 2007b), refreshed-jolt scenarios (Ghisellini et al. 2007), and reverse shock emissions (Kobayashi 2000; Kobayashi & Sari 2001). Recently, Fan & Wei (2007) suggested that the shallow-decay phase may be attributed to...",
        "ori-fast-z-score": -0.48038446141526137,
        "water-fast-z-score": 6.12425452659289,
        "rewrite-fast-z-score": 0.6324555320336759
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Photolytically generated aerosols in the mesosphere and thermosphere of Titan .\nAbstract:\nWe present new results on photolysis-induced chemistry in the upper atmosphere of Titan, based on observations made by Cassini/VIMS (Visible Infrared Mapping Spectrometer) during its T5 flyby of Titan s northern hemisphere. The VIMS data show that there is an extensive haze layer between about 400 km altitude and 1000 km above the surface. This haze has been previously attributed to photochemical production of aerosol particles through reactions involving methane and ethane. We find evidence for additional sources of aerosol particles at higher altitudes. These include products of dissociation of nitrogen molecules produced by solar UV radiation, as well as carbon dioxide and water vapor released from the lower stratosphere into the mesosphere/thermosphere region. Our analysis suggests that these processes may be responsible for up to 50% of the total mass loading of the haze observed near 600 km altitude. The presence of this additional source of aerosol particles could have important implications for understanding atmospheric circulation patterns in the upper atmosphere of Saturn s moon Enceladus.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Photolytically produced aerosols in the mesosphere and thermosphere of Titan . Abstract : We report new data on photolysis - caused chemistry in the higher atmosphere of Titan , built on observations made by Cassini / VIMS ( Visible Infrared Mapping Spectrometer ) during its T5 flyby of Titan s northern hemisphere .The VIMS data reveal that there is an extensive haze layer between about 400 kilometres altitude and 1000 kilometers above the surface . This haze has been previously attributed to photochemical production of aerosol particles through reactions involving methane and ethane .We get data for additional sources of aerosol particles at higher altitudes . These include products of dissociation of nitrogen molecules released by sun UV rays , as well as carbon dioxide and water vapor sent from the lower stratosphere into the mesosphere / thermosphere region .Our study shows that these mechanisms may be responsible for up to 50 % of the total mass loading of the fog seen near 600 kilometers altitude . The presence of this added source of aerosol substances could have important implications for studying air convection mechanisms in the inner environment of Saturn s sun Enceladus .",
        "rewrite_text": "We present new findings on the chemistry driven by photolysis in the upper atmosphere of Titan, based on observations collected by the Cassini spacecraft's Visible Infrared Mapping Spectrometer (VIMS) during its T5 flyby over Titan's northern hemisphere. The VIMS data indicate the existence of a significant haze layer situated between approximately 400 kilometers and 1000 kilometers above Titan's surface. This haze has traditionally been linked to the photochemical production of aerosol particles resulting from reactions involving methane and ethane. However, our research uncovers additional sources of aerosol particles at higher altitudes. These sources include products formed from the dissociation of nitrogen molecules, which are activated by ultraviolet radiation from the Sun, as well as contributions from carbon dioxide and water vapor that are transported from the lower stratosphere into the mesosphere and thermosphere. Our analysis suggests that these newly identified mechanisms could account for nearly 50% of the total mass loading of the haze observed around 600 kilometers in altitude. The discovery of this additional source of aerosol particles may have significant implications for understanding atmospheric convection processes within the inner environment of Saturn's moon Enceladus. This research enhances our comprehension of Titan's atmospheric dynamics and the complex interplay of chemical processes occurring in its upper atmosphere, paving the way for future studies on the interactions between Titan's atmosphere and its potential for hosting prebiotic chemistry.",
        "ori-fast-z-score": -1.5275252316519468,
        "water-fast-z-score": 5.965587590013045,
        "rewrite-fast-z-score": 0.29851115706299675
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  High resolution mid-infrared spectroscopy of ultraluminous infrared galaxies .\nAbstract:\nWe present high-resolution (R = λ/Δλ ~ 10000) near-infrared and mid-infrared spectroscopic observations for two ULIRGs, Mrk 231 and Arp 220. The data were obtained with the Subaru Telescope using the Cooled Mid-Infrared Camera and Spectrograph (COMICS). We detect several emission lines in both objects including H I Brγ at 2.16 μm, Paα at 1.87 μm,  Fe II  at 1.64 μm, He I at 1.70 μm,  S III  at 0.95 μm, and  C IV  at 0.15 μm. In addition to these lines, we also find that there are many absorption features such as CO bandheads near 4.7 μm and 6.2 μm. These results show that the observed spectra have complex line profiles which can be explained by multiple components along our line-of-sight and/or different physical conditions within each component.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : High resolution mid - infrared spectroscopy of ultraluminous laser galaxies . Abstract : We produce high - resolution ( R = λ / Δλ ~ 10000 ) near - infrared and mid - infrared spectroscopic observations for two ULIRGs , Mrk 231 and Arp 220 .The data were obtained with the Subaru Telescope using the Cooled Mid - Infrared Camera and Spectrograph ( COMICS ) . We detect many emission lines in both objects including H I Brγ at 2 . 16 μm , Paα at 1 . 87 μm , Fe II at 1 . 64 μm , He I at 1 . 70 μm , S III at 0 . 95 μm , and C IV at 0 . 15 μm .In addition to these lines , we also find that there are many absorption elements such as CO bandheads near 4 . 7 μm and 6 . 2 μm . These data reveal that the seen spectra have complex line profiles which can be described by various components along our line - of - view and / or varying physical conditions within each component .",
        "rewrite_text": "In this study, we present high-resolution (R = λ/Δλ ~ 10,000) near-infrared and mid-infrared spectroscopic observations of two ultraluminous infrared galaxies (ULIRGs), Mrk 231 and Arp 220. The observations were conducted using the Subaru Telescope equipped with the Cooled Mid-Infrared Camera and Spectrograph (COMICS). Our analysis reveals a rich array of emission lines in both galaxies, including prominent features such as H I Brγ at 2.16 μm, Paα at 1.87 μm, Fe II at 1.64 μm, He I at 1.70 μm, S III at 0.95 μm, and C IV at 0.15 μm. Additionally, we identify several absorption features, notably CO bandheads located near 4.7 μm and 6.2 μm. The spectral data indicate that the observed line profiles are complex, suggesting the presence of multiple components along our line of sight and/or varying physical conditions within each component. This complexity highlights the intricate nature of the environments surrounding these ultraluminous galaxies, providing insights into their physical and chemical properties. Our findings contribute to a deeper understanding of the processes occurring in ULIRGs and their role in galaxy evolution. By utilizing high-resolution spectroscopy, we are able to probe the dynamics and composition of these distant objects, shedding light on the mechanisms driving their extraordinary luminosity. This research not only enhances our knowledge of Mrk 231 and Arp 220 but also sets the stage for future investigations into the spectral characteristics of other ULIRGs, ultimately enriching the field of astrophysics.",
        "ori-fast-z-score": 0.848528137423857,
        "water-fast-z-score": 3.780756226875626,
        "rewrite-fast-z-score": 1.5554275420956378
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  IGR J16194-2810: a new symbiotic X-ray binary .\nAbstract:\nWe report the discovery and analysis of IGR J16194-2810, an unusual X-ray source discovered by INTEGRAL in 2003. The source is located at RA=16h19m41s DEC=-28d08 00  (J2000) with a positional uncertainty of 1 arcmin radius. It was observed for about 100 ksec on 2004 February 24-25 using XMM-Newton EPIC-pn camera operated in timing mode. We find that this source has a periodicity of P=4.6 hr which we interpret as orbital motion of two compact objects orbiting each other. This interpretation is supported by the presence of strong absorption lines in its optical spectrum indicating the existence of a red giant companion star. Using the measured mass function f(M), we estimate the masses of both components to be Mx=2.7-3.1 M⊙ and My=1.2-1.4 M⊙ respectively.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : IGR J16194 - 2810 : a new symbiotic X - ray binary . Abstract : We report the discovery and investigation of IGR J16194 - 2810 , an unique X - ray source detected by INTEGRAL in 2003 .The source is situated at RA = 16h19m41s DEC = - 28d08 00 ( J2000 ) with a positional uncertainty of 1 arcmin radius . It was seen for about 100 ksec on 2004 February 24 - 25 using XMM - Newton EPIC - pn sensor controlled in timing mode .We see that this source has a periodicity of P = 4 . 6 hr which we perceive as orbital movement of two compact objects orbiting each other . This interpretation is backed by the presence of large absorbed lines in its optical spectrum suggesting the existence of a red giant sister star .Using the measured mass distribution f ( M ) , we estimate the masses of both components to be Mx = 2 . 7 - 3 . 1 [UNK] and My = 1 . 2 - 1 . 4 [UNK] respectively .",
        "rewrite_text": "We present the discovery and analysis of IGR J16194-2810, a distinctive X-ray source identified by the INTEGRAL satellite in 2003. Located at right ascension 16h19m41s and declination -28d08'00\" (J2000), this source has a positional uncertainty of approximately 1 arcminute. Observations conducted with the XMM-Newton EPIC-pn sensor in timing mode on February 24-25, 2004, revealed the source for a duration of about 100 kiloseconds. Notably, we detected a periodicity of approximately 4.6 hours, which we interpret as the orbital motion of two compact objects in a binary system. This interpretation is further supported by the presence of significant absorbed lines in the optical spectrum, indicating the likely existence of a red giant companion star. By analyzing the mass distribution function f(M), we estimate the masses of the two components of the binary system to be approximately Mx = 2.7 - 3.1 solar masses for the compact object and My = 1.2 - 1.4 solar masses for the red giant. This discovery contributes to our understanding of symbiotic X-ray binaries and their formation mechanisms, highlighting the complex interactions between compact objects and their stellar companions. The findings suggest that IGR J16194-2810 may serve as a valuable case study for further investigations into the dynamics and evolution of such systems.",
        "ori-fast-z-score": -1.0504514628777804,
        "water-fast-z-score": 3.8729833462074166,
        "rewrite-fast-z-score": -0.42640143271122083
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Characterization of Instrumental Phase Stability .\nAbstract:\nWe present an analysis method for characterizing the stability of optical phase in astronomical instruments, based on the measurement and characterization of fringe contrasts obtained with different integration times.  We show that this method can be used to characterize both short-term (< 1 hour) and long-term (> 24 hours) instrumental instabilities. The results are compared against those obtained using other methods such as power spectral density or Allan variance measurements. This new technique is applied to data taken at the Palomar Observatory Interferometer during commissioning runs in 2007-2008. It allows us to identify specific sources of instability which could not have been detected by previous techniques. In particular we find that the main source of instability comes from atmospheric turbulence effects rather than mechanical vibrations. Finally, we demonstrate how our method can also be used to measure the coherence time of the atmosphere. Keywords: Fringe contrast, Optical interferometry, Instrumentation, Atmospheric turbulence",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Characterization of Instrumental Phase Stability . Abstract : We present an assessment method for characterizing the stability of optical phase in astronomical instruments , relying on the observation and description of fringe contrasts obtained with various integration times .We see that this method can be used to characterize both long - term ( < 1 hour ) and long - term ( > 24 hours ) instrumental instabilities . The results are compared against those achieved using other methods such as power spectral coefficient or Allan variance measurements .This new technique is applied to data taken at the Palomar Observatory Interferometer during commissioning running in 2007 - 2008 . It enables us to identify specific sources of uncertainty which could not have been detected by earlier techniques .In particular we find that the main source of tension comes from ambient turbulence influences rather than structural vibrations . Finally , we prove how our technique can also be used to measure the coherence time of the air .Keywords : Fringe effect , Optical interferometry , Instrumentation , Atmospheric turbulence",
        "rewrite_text": "Title: Characterization of Instrumental Phase Stability\n\nAbstract: In this study, we introduce a novel method for evaluating the stability of optical phase in astronomical instruments, focusing on the analysis of fringe contrasts obtained over varying integration times. Our approach allows for the characterization of both short-term (less than 1 hour) and long-term (greater than 24 hours) instrumental instabilities. We compare our findings with results derived from established techniques such as power spectral density and Allan variance measurements. This innovative technique was applied to data collected at the Palomar Observatory Interferometer during its commissioning phase from 2007 to 2008. Through this analysis, we were able to identify specific sources of uncertainty that previous methods failed to detect. Notably, our results indicate that the predominant source of instability is attributed to ambient turbulence rather than structural vibrations, which has significant implications for the design and operation of astronomical instruments. Furthermore, we demonstrate that our method can also be utilized to assess the coherence time of atmospheric conditions, providing valuable insights into the effects of turbulence on optical measurements. This research contributes to the ongoing efforts to enhance the precision of optical interferometry in astronomy and offers a framework for future studies aimed at improving instrumental stability. \n\nKeywords: Fringe effect, Optical interferometry, Instrumentation, Atmospheric turbulence.",
        "ori-fast-z-score": 0.1125087900926024,
        "water-fast-z-score": 5.287913134352312,
        "rewrite-fast-z-score": 0.3922322702763681
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Vortical and Wave Modes in 3D Rotating Stratified Flows: Random Large Scale Forcing .\nAbstract:\nWe study the effect of random large-scale forcing on three-dimensional rotating stratified flows, using direct numerical simulations (DNS) with periodic boundary conditions. The flow is forced at large scales by adding to the momentum equation an external force that has zero mean but whose Fourier transform contains both positive and negative wavenumbers. We find that this type of forcing excites two distinct types of modes in the system: vortical and wave-like modes. Vortical modes are characterized by strong vertical motions concentrated near the center of the domain; they have low horizontal velocities and their kinetic energy decays rapidly as we move away from the center. On the other hand, wave-like modes are characterized by weak vertical motions distributed over larger regions of space; they have high horizontal velocities and their kinetic energies decay slowly or even increase slightly when moving away from the center. In addition, these waves can be either stationary or propagating horizontally depending on whether the forcing spectrum peaks at small or large horizontal wavenumber respectively.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Vortical and Wave Modes in 3D Rotating Stratified Flows : Random Large Scale Forcing . Abstract : We research the impact of random large - scale forcing on three - dimensional spinning stratified flows , using direct numerical simulations ( DNS ) with periodic border conditions .The flow is displaced at large scales by added to the velocity equation an external force that has zero mean but whose Fourier integral contains both negative and negative wavenumbers . We see that this kinds of forcing excites two different kinds of modes in the system : vortical and wave - like modes .Vortical modes are characterized by strong vertical motions concentrated near the center of the domain ; they have low horizontal velocities and their kinetic energy decays rapidly as we move away from the center . On the other hand , wave - like modes are characterized by weak vertical motions distributed over larger regions of space ; they have high horizontal velocities and their kinetic energies decay slowly or even increase slightly when moving away from the center .In addition , these pulses can be either static or propagating vertically depending on whether the forcing spectrum peaks at small or large horizontal wavenumber respectively .",
        "rewrite_text": "In this study, we investigate the effects of random large-scale forcing on three-dimensional rotating stratified flows through direct numerical simulations (DNS) with periodic boundary conditions. The external force applied to the velocity equation has a zero mean and incorporates both positive and negative wavenumbers in its Fourier representation, leading to significant large-scale displacements in the flow. Our findings reveal that this type of forcing induces two distinct modes within the system: vortical modes and wave-like modes. \n\nVortical modes are characterized by pronounced vertical motions that are predominantly concentrated near the center of the computational domain. These modes exhibit low horizontal velocities, and their kinetic energy diminishes rapidly as one moves away from the center. In contrast, wave-like modes display weaker vertical motions that are more broadly distributed across the spatial domain. These modes are associated with higher horizontal velocities, and their kinetic energy tends to decay slowly or may even experience a slight increase as the distance from the center increases.\n\nFurthermore, we observe that the nature of these modes can vary based on the characteristics of the forcing spectrum. Specifically, the pulses generated can either remain static or propagate vertically, depending on whether the forcing spectrum is concentrated at small or large horizontal wavenumbers, respectively. This research enhances our understanding of the complex dynamics in rotating stratified flows and provides insights into the interplay between vortical and wave-like behaviors under the influence of random large-scale forcing. The implications of these findings are significant for various applications in geophysical and astrophysical fluid dynamics, where stratification and rotation play crucial roles.",
        "ori-fast-z-score": -2.65361388801511,
        "water-fast-z-score": 1.116880781646981,
        "rewrite-fast-z-score": 1.191759143062248
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Self consistency in hadron physics .\nAbstract:\nThe self-consistent approach to the description of nuclear matter is reviewed and its application to other systems, such as atomic nuclei or quark-gluon plasma, discussed briefly. The main idea behind this method is that one should not consider only the mean field acting on particles but also take into account fluctuations around it. This leads to an infinite set of coupled equations for all orders of correlation functions which can be solved by truncation at some order. In particular we discuss how the results depend on the choice of the approximation scheme used. We show that the inclusion of higher-order correlations improves agreement with experimental data considerably. Finally, we present our recent results obtained within the framework of the relativistic random phase approximation (RRPA) including up to fourth-order correlations. These calculations are performed using realistic nucleonnucleon interactions derived from chiral effective theory. It turns out that the RRPA results agree well with available experimental information about excited states of medium-heavy nuclei.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Self consistency in hadron physics . Abstract : The self - consistent method to the description of nuclear material is reviewed and its use to other structures , such as atomic hydrogen or quark - gluon radiation , discussed briefly .The main idea behind this process is that one should not take only the mean field acted on objects but also took into consideration fluctuations around it . This leads to an endless system of coupled equations for all orders of correlation functions which can be answered by truncation at some order .In particular we explain how the results vary on the selection of the approximation scheme used . We see that the inclusion of greater - order correlations improves agreement with observation information significantly .Finally , we present our latest findings obtained within the framework of the relativistic random phase approximation ( RRPA ) containing up to fourth - order correlations . These analyses are performed using accurate nucleonnucleon interactions derived from chiral effective theory .It turns out that the RRPA results agree well with provided experimental evidence about excited states of medium - heavy nuclei .",
        "rewrite_text": "This article reviews the self-consistent approach to modeling nuclear matter and explores its applicability to other physical systems, such as atomic hydrogen and quark-gluon radiation. The central premise of this method is that it is essential to consider not only the mean field acting on particles but also the fluctuations around this mean field. This consideration leads to a complex system of coupled equations that describe all orders of correlation functions. To make the problem tractable, one can truncate the equations at a certain order. The paper discusses how the choice of approximation scheme can significantly influence the results obtained. Notably, incorporating higher-order correlations enhances the alignment of theoretical predictions with experimental observations. The authors present their latest results derived from the relativistic random phase approximation (RRPA), which includes correlations up to the fourth order. These analyses utilize precise nucleon-nucleon interactions based on chiral effective theory. The findings demonstrate that the RRPA framework yields results that are in strong agreement with experimental data concerning the excited states of medium-heavy nuclei. This work underscores the importance of self-consistency and higher-order correlations in achieving a more accurate understanding of nuclear physics and related phenomena.",
        "ori-fast-z-score": -0.3144854510165755,
        "water-fast-z-score": 6.325405337855594,
        "rewrite-fast-z-score": -0.5883484054145521
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Hot Nights on Extrasolar Planets: Mid-IR Phase Variations of Hot Jupiters .\nAbstract:\nWe present the first mid-infrared phase curve observations for an extrasolar planet, WASP-121b (1SWASP J140747.93-394542.7), using Spitzer/IRAC at 3.6 and 4.5 microns. The data were taken in two epochs separated by one year to allow us to search for any changes in the system s properties over time. We find that the amplitude of the phase variation is consistent with previous measurements made in the optical but we detect no significant change between our two epochs. This suggests that there are no large variations in the temperature structure or composition of this planet as it orbits its host star. Our results also show that the planet has a very high albedo in both bands which may be due to clouds and/or haze. These findings have important implications for understanding how planets form and evolve. \n \n Keywords: exoplanet, infrared",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Hot Nights on Extrasolar Planets : Mid - IR Phase Variations of Hot Jupiters . Abstract : We report the first mid - infrared phase curve measurements for an extrasolar planet , WASP - 121b ( 1SWASP J140747 . 93 - 394542 . 7 ) , using Spitzer / IRAC at 3 . 6 and 4 . 5 microns .The data were took in two epochs separated by one year to allow us to search for any alterations in the system s properties over time . We see that the frequency of the phase change is compatible with previous measurements made in the optical but we perceive no major shift between our two epochs .This implies that there are no large changes in the temperature structure or composition of this planet as it orbits its host star . Our results also demonstrate that the planet has a very high albedo in both bands which may be due to clouds and / or haze .These studies have important implications for knowledge how planets form and evolve . Keywords : exoplanet , infrared",
        "rewrite_text": "We present the inaugural mid-infrared phase curve measurements for the extrasolar planet WASP-121b (1SWASP J140747.93-394542.7), utilizing the Spitzer Space Telescope's Infrared Array Camera (IRAC) at wavelengths of 3.6 and 4.5 microns. Our observations were conducted during two distinct epochs, separated by a year, to investigate potential variations in the planet's characteristics over time. The analysis reveals that the frequency of phase changes aligns with earlier optical measurements, indicating a consistent behavior in the planet's thermal emissions. Notably, we did not observe significant alterations between the two observational periods, suggesting stability in the temperature distribution and atmospheric composition of WASP-121b as it orbits its host star. Furthermore, our findings indicate that the planet exhibits a remarkably high albedo in both infrared bands, which could be attributed to the presence of clouds and/or haze in its atmosphere. These insights contribute to our understanding of the atmospheric dynamics and thermal properties of hot Jupiters, shedding light on the processes involved in planetary formation and evolution. The implications of this research extend to the broader field of exoplanet studies, enhancing our comprehension of the diverse characteristics and behaviors of planets beyond our solar system. This work underscores the significance of mid-infrared observations in characterizing exoplanet atmospheres and their potential variability, ultimately enriching our knowledge of planetary systems. \n\nKeywords: exoplanet, infrared, WASP-121b, phase curve, albedo, atmospheric dynamics.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.031591663758072,
        "rewrite-fast-z-score": -0.29559878344928797
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Non-relativistic limit of the Einstein equation .\nAbstract:\nWe consider the non-relativistic limit of an exact solution to the Einstein field equations in vacuum, which is given by the Schwarzschild metric. We show that this metric can be obtained as a special case of the general relativistic Kerr-Schild ansatz for stationary and axisymmetric solutions with spherical symmetry. The corresponding line element reads ds2 = -(1 + 2M/r)dt2 + (1 - 2M/r)(dr2 + r2 dΩ2), where M denotes the mass parameter. In order to obtain the correct Newtonian limit we have to choose the time coordinate such that dt/dτ ~ 1/Mc² holds asymptotically at spatial infinity. This choice leads to the standard form of the Schwarzschild metric in terms of proper time τ. Finally, we discuss some consequences concerning the gravitational redshift and the equivalence principle. The Schwarzschild metric describes the geometry outside a spherically symmetric body in vacuum. It was first derived by Karl Schwarzschild in 1916  1  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Non - relativistic limit of the Einstein equation . Abstract : We consider the non - relativistic limit of an precise answer to the Einstein field equations in vacuum , which is given by the Schwarzschild metric .We see that this metric can be obtained as a special case of the general relativistic Kerr - Schild ansatz for stationary and axisymmetric solutions with spherical symmetry . The equivalent line element reads ds2 = - ( 1 + 2M / r ) dt2 + ( 1 - 2M / r ) ( dr2 + r2 dΩ2 ) , where M denotes the mass vector .In order to obtain the appropriate Newtonian limit we have to choose the time coordinate such that dt / dτ ~ 1 / Mc² holds asymptotically at spatial infinity . This choosing results to the standard form of the Schwarzschild metric in terms of proper time τ .Finally , we explain some consequences concerning the gravitational redshift and the equivalence principle . The Schwarzschild metric describes the topology outside a spherically symmetric body in vacuum .It was first derived by Karl Schwarzschild in 1916 1 .",
        "rewrite_text": "Title: Non-Relativistic Limit of the Einstein Equation\n\nAbstract: In this study, we explore the non-relativistic limit of the Einstein field equations in a vacuum context, specifically focusing on the Schwarzschild metric. This metric serves as a precise solution to the equations and can be derived as a particular case of the general relativistic Kerr-Schild ansatz, which applies to stationary and axisymmetric solutions exhibiting spherical symmetry. The line element associated with this metric is expressed as ds² = - (1 + 2M/r) dt² + (1 - 2M/r) (dr² + r² dΩ²), where M represents the mass vector. To accurately achieve the Newtonian limit, we must select the time coordinate such that the relationship dt/dτ approaches 1/Mc² asymptotically at spatial infinity. This selection leads to the conventional form of the Schwarzschild metric in terms of proper time τ. Furthermore, we discuss the implications of this metric on gravitational redshift and the equivalence principle. The Schwarzschild metric effectively characterizes the geometric structure surrounding a spherically symmetric mass in a vacuum environment. Originally derived by Karl Schwarzschild in 1916, this foundational solution continues to play a crucial role in our understanding of gravitational phenomena in both classical and relativistic frameworks.",
        "ori-fast-z-score": 1.5650160901149996,
        "water-fast-z-score": 3.916379472039716,
        "rewrite-fast-z-score": 0.8944271909999159
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Evolution of interstellar dust and stardust in the solar neighbourhood .\nAbstract:\nWe present an analysis of the evolution of interstellar dust grains, based on their size distribution inferred by infrared observations with ISO (Infrared Space Observatory). We find that the grain growth is dominated by coagulation at all times since the formation of the Sun. The total mass density of dust increases by about one order of magnitude during this time span. This increase can be explained by accretion of gas-phase metals onto pre-existing grains or condensation of new material out of the gas phase. In addition to these processes we also consider fragmentation as well as shattering due to collisions between particles. Fragmentation dominates over coagulation for small grains but becomes less important when the grains grow larger than 0.1 micrometres. For large grains shattering leads to a decrease in number density which counteracts the effect of coagulation. Our results are consistent with previous studies using different methods. \n \n Keywords: Interstellar medium",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Evolution of interstellar dust and stardust in the solar neighbourhood . Abstract : We present an assessment of the evolution of interstellar dust grains , based on their size distribution inferred by infrared observations with ISO ( Infrared Space Observatory ) .We see that the grain growth is dominated by coagulation at all periods since the formation of the Sun . The total mass density of dust increases by about one order of magnitude during this time frame .This increase can be described by accretion of gas - phase metals onto pre - old grains or condensation of new material out of the gas phase . In addition to these mechanisms we also consider fragmentation as well as shattering related to collisions between particles .Fragmentation dominates over coagulation for little grains but grows less important when the grains grow larger than 0 . 1 micrometres . For large grains breaking leads to a reduction in number density which counteracts the impact of coagulation .Our results are compatible with previous research utilizing diverse methods . Keywords : Interstellar medium",
        "rewrite_text": "Title: Evolution of Interstellar Dust and Stardust in the Solar Neighborhood\n\nAbstract: This study provides a comprehensive evaluation of the evolution of interstellar dust grains, drawing on size distribution data obtained from infrared observations conducted by the Infrared Space Observatory (ISO). Our findings indicate that the growth of dust grains has been predominantly influenced by coagulation processes since the Sun's formation. Over this extensive period, we observe an approximate tenfold increase in the total mass density of dust. This enhancement can be attributed to two primary mechanisms: the accretion of gas-phase metals onto pre-existing grains and the condensation of new materials from the gas phase. In addition to these processes, we also examine the roles of fragmentation and shattering that occur as a result of particle collisions. While fragmentation is the prevailing process for smaller grains, its significance diminishes as grain size exceeds 0.1 micrometers. For larger grains, fragmentation leads to a decrease in number density, which serves to counterbalance the effects of coagulation. Our results align with previous studies that have employed various methodologies, reinforcing the validity of our conclusions. This research contributes to a deeper understanding of the dynamics of the interstellar medium and the complex interactions that govern the lifecycle of dust in our solar neighborhood. \n\nKeywords: Interstellar medium, dust evolution, coagulation, fragmentation, infrared observations.",
        "ori-fast-z-score": 0.562543950463012,
        "water-fast-z-score": 5.366563145999495,
        "rewrite-fast-z-score": 2.4494897427831783
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Response Prediction of Structural System Subject to Earthquake Motions using Artificial Neural Network .\nAbstract:\nThe response prediction of structural system subject to earthquake motions is very important for the design and construction of buildings in seismic areas, especially when it comes to high-rise building structures. In this study, an artificial neural network (ANN) model was developed by using data obtained through nonlinear dynamic analysis on reinforced concrete frame structure subjected to earthquake ground motion records. The ANN model consists of three layers; input layer, hidden layer with 10 neurons, output layer with one neuron representing maximum inter-story drift ratio. Input variables used are peak ground acceleration, duration time, number of stories, story height, mass density per unit floor area, damping coefficient, yield strength of steel bar, elastic modulus of steel bar, shear wall stiffness, and moment capacity of beam-column joint. To verify the accuracy of the proposed ANN model, results predicted by the ANN were compared with those calculated by nonlinear dynamic analysis program. It can be concluded that the ANN model has good performance in predicting the maximum inter-story drift ratios under various earthquake ground motions.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Response Prediction of Structural System Subject to Earthquake Motions using Artificial Neural Network . Abstract : The response prediction of structural structure related to earthquake motions is very important for the design and build of structures in seismic areas , particularly when it comes to large - rising building structures .In this study , an synthetic neural network ( ANN ) model was developed by using data derived through nonlinear dynamic analysis on concrete cement frame building exposed to earthquake ground motion records . The ANN model consists of three layers ; input layer , hidden layer with 10 neurons , output layer with one neuron representing maximum inter - story drag ratio .Input variables utilized are peak ground acceleration , duration time , number of stories , story height , mass density per unit floor area , damping coefficient , yield strength of steel bar , elastic modulus of steel bar , shear floor stiffness , and moment capacity of beam - column joint . To establish the accuracy of the suggested ANN theory , results predicted by the ANN were compared with those estimated by nonlinear dynamic analysis project .It can be shown that the ANN theory has good success in predicting the maximum inter - story drift ratios under various earthquake ground motions .",
        "rewrite_text": "**Title:** Response Prediction of Structural Systems Subject to Earthquake Motions Using Artificial Neural Networks\n\n**Abstract:** The accurate prediction of structural responses to earthquake motions is crucial for the design and construction of buildings in seismically active regions, particularly for tall structures. This study presents the development of an Artificial Neural Network (ANN) model aimed at predicting the response of concrete frame buildings subjected to seismic ground motion. The ANN was trained using data obtained from nonlinear dynamic analyses of these structures, which were exposed to various earthquake records. The architecture of the ANN comprises three layers: an input layer, a hidden layer containing 10 neurons, and an output layer with a single neuron that represents the maximum inter-story drift ratio. The input parameters for the model include peak ground acceleration, duration of ground motion, number of stories, story height, mass density per unit floor area, damping coefficient, yield strength of the reinforcing steel, elastic modulus of the steel, shear stiffness of the floors, and moment capacity of the beam-column joints. To evaluate the performance of the proposed ANN model, its predictions were compared against results obtained from traditional nonlinear dynamic analysis. The findings indicate that the ANN model demonstrates a high level of accuracy in forecasting maximum inter-story drift ratios across a range of earthquake scenarios. This research highlights the potential of using artificial intelligence techniques, specifically ANNs, to enhance the reliability of structural response predictions in the context of seismic engineering, ultimately contributing to safer building designs in earthquake-prone areas.",
        "ori-fast-z-score": 0.8320502943378436,
        "water-fast-z-score": 5.89167755454927,
        "rewrite-fast-z-score": 1.191759143062248
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Multiwavelength Study of X-ray Selected Star Forming Galaxies within the Chandra Deep Field South .\nAbstract:\nWe present multiwavelength observations for a sample of X-ray selected star forming galaxies (SFGs) in the Chandra Deep Field-South (CDF-S). The sample consists of 16 SFGs with spectroscopic redshifts between 1 and 3, which were detected by both the soft-band (0.5-2 keV) and hard-band (2-8 keV) surveys conducted by the Advanced CCD Imaging Spectrometer on board XMM-Newton. We have obtained optical spectroscopy using the Keck telescope to measure their stellar masses and SFRs as well as near-infrared photometry taken with the Infrared Array Camera aboard Spitzer Space Telescope to estimate dust extinction. Our results show that these SFGs are massive systems with M* = 1013 -1014M⊙ at z ~ 2 -3. They also exhibit high specific star-formation rates ranging from 10^(-3) yr-1 to 10^(1) yr-1, indicating intense ongoing star formation activity.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Multiwavelength Study of X - ray Selected Star Forming Galaxies within the Chandra Deep Field South . Abstract : We report multiwavelength studies for a sample of X - ray selected star producing galaxies ( SFGs ) in the Chandra Deep Field - South ( CDF - S ) .The sample consists of 16 SFGs with spectroscopic redshifts between 1 and 3 , which were detected by both the hard - band ( 0 . 5 - 2 keV ) and hard - band ( 2 - 8 keV ) observations conducted by the Advanced CCD Imaging Spectrometer on board XMM - Newton . We have achieved optical spectroscopy utilizing the Keck telescope to measure their stellar masses and SFRs as also as near - infrared photometry made with the Infrared Array Camera aboard Spitzer Space Telescope to estimate dust extinction .Our results show that these SFGs are vast systems with M * = 1013 - [UNK] at z ~ 2 - 3 . They even exhibit strong specific star - formation rates ranging from 10 ^ ( - 3 ) yr - 1 to 10 ^ ( 1 ) yr - 1 , indicating intense ongoing star formation activity .",
        "rewrite_text": "We present a comprehensive multiwavelength analysis of a selected group of X-ray identified star-forming galaxies (SFGs) located in the Chandra Deep Field South (CDF-S). Our study focuses on a sample comprising 16 SFGs, all of which possess spectroscopic redshifts ranging from 1 to 3. These galaxies were detected through both hard-band (0.5 - 2 keV) and hard-band (2 - 8 keV) observations carried out by the Advanced CCD Imaging Spectrometer on the XMM-Newton satellite. To further investigate their properties, we conducted optical spectroscopy using the Keck telescope, allowing us to accurately determine their stellar masses and star formation rates (SFRs). Additionally, we employed near-infrared photometry from the Infrared Array Camera on the Spitzer Space Telescope to assess the levels of dust extinction affecting these galaxies. Our findings reveal that these SFGs are substantial systems with stellar masses on the order of M* = 10^13 solar masses at redshifts around 2 to 3. Notably, they demonstrate remarkably high specific star formation rates, ranging from 10^(-3) to 10^(1) yr^(-1), which underscores the presence of vigorous star formation activity within these galaxies. This study contributes to our understanding of the evolution and characteristics of star-forming galaxies in the early universe, highlighting the significance of multiwavelength observations in revealing the complex interplay between stellar formation, mass, and dust content in these distant cosmic structures.",
        "ori-fast-z-score": -0.9113223768657671,
        "water-fast-z-score": 2.840187787218772,
        "rewrite-fast-z-score": 0.9233805168766388
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Can We Detect the Anisotropic Shapes of Quasar HII Regions During Reionization Through The Small-Scale Redshifted 21cm Power Spectrum? .\nAbstract:\nWe investigate whether we can detect anisotropy in quasar H II regions during reionization through their small-scale redshifted 21 cm power spectrum (21-cm PS). In our model, quasars are assumed to be located at peaks of dark matter density fluctuations and ionize surrounding gas with an anisotropic Strömgren sphere whose shape is determined by the local tidal field. By performing numerical simulations for different values of the spin temperature T S , we find that the 21-cm PS has a characteristic peak structure which reflects the shapes of individual H II regions. This peak structure becomes more prominent as T S decreases because the number of neutral hydrogen atoms increases due to the decrease in the brightness temperature difference between the CMB and the 21-cm emission line. Our results suggest that it may be possible to use this peak structure to constrain the value of T S . However, since there exist many other factors affecting the 21-cm PS besides T S , further studies will be needed before drawing any conclusions on its detectability.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Can We Detect the Anisotropic Shapes of Quasar HII Regions During Reionization Through The Small - Scale Redshifted 21cm Power Spectrum ? .Abstract : We explore whether we can locate anisotropy in quasar H II regions during reionization through their tiny - scale redshifted 21 cm power spectrum ( 21 - cm PS ) . In our model , quasars are expected to be found at peaks of dark matter density fluctuations and ionize neighboring gas with an anisotropic Strömgren sphere whose shape is chosen by the local tidal field .By conducting numerical simulations for different values of the spin temperature T S , we find that the 21 - cm PS has a peculiar peak structure which reflects the shapes of different H II regions . This peak structure becomes more prominent as T S drops because the proportion of neutral hydrogen atoms increases owing to the decrease in the brightness temperature difference between the CMB and the 21 - cm absorption line .Our results propose that it could be possible to use this peak structure to constrain the value of T S . However , since there remain many other influences involving the 21 - cm PS besides T S , further studies will be needed before drew any findings on its detectability .",
        "rewrite_text": "Title: Can We Detect the Anisotropic Shapes of Quasar HII Regions During Reionization Through The Small-Scale Redshifted 21cm Power Spectrum?\n\nAbstract: In this study, we investigate the potential for identifying anisotropic shapes of quasar H II regions during the epoch of reionization by analyzing the small-scale redshifted 21 cm power spectrum (21-cm PS). Our theoretical framework posits that quasars are typically situated at the peaks of dark matter density fluctuations, where they ionize the surrounding gas, resulting in anisotropic Strömgren spheres shaped by the local tidal field. Through a series of numerical simulations varying the spin temperature (T_S), we observe a distinctive peak structure within the 21-cm PS that corresponds to the geometries of the H II regions. Notably, this peak structure becomes increasingly pronounced as T_S decreases, which leads to a higher fraction of neutral hydrogen due to a reduced brightness temperature difference between the cosmic microwave background (CMB) and the 21-cm absorption line. Our findings suggest that this peak structure may serve as a valuable tool for constraining the spin temperature. However, it is important to note that the 21-cm PS is influenced by various factors beyond T_S, indicating that additional research is necessary to fully understand the detectability of these anisotropic shapes. This work lays the groundwork for future investigations into the interplay between quasar activity, ionization processes, and the resulting signatures in the 21-cm power spectrum, ultimately contributing to our understanding of the reionization era and the evolution of cosmic structures.",
        "ori-fast-z-score": 1.3093073414159544,
        "water-fast-z-score": 5.45544725589981,
        "rewrite-fast-z-score": 2.7196004146003396
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Zgamma production and limits on anomalous ZZgamma and Zgammagamma couplings in ppbar collisions at sqrt(s) = 1.96 TeV .\nAbstract:\nWe report the first measurement of the cross section for the process ppbar -> Zgamma + X, where X is any number of additional particles produced along with the Zgamma boson.  The data were collected by the D0 experiment during Run II of Fermilab s Tevatron Collider between 2002 and 2007 using an integrated luminosity of 5.4 fb-1 . We measure the cross section to be 0.84 +/- 0.11 (stat.) +/- 0.10 (syst.) pb, which agrees well with next-to-leading-order perturbative QCD predictions. Using this result we set upper limits on possible anomalous trilinear gauge-boson coupling parameters. These results are also used to derive constraints on models that predict new heavy neutral vector bosons decaying into pairs of photons or gluons. In addition, these measurements provide important input for future searches for Higgs bosons decaying into two photons. \nPACS numbers: 11.30.Er, 12.60.Jv",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Zgamma production and limits on anomalous ZZgamma and Zgammagamma couplings in ppbar collisions at sqrt ( s ) = 1 . 96 TeV . Abstract : We report the first measurement of the cross area for the process ppbar - > Zgamma + X , where X is any number of added particles generated along with the Zgamma boson .The data were collected by the D0 study during Run II of Fermilab s Tevatron Collider between 2002 and 2007 utilizing an integrated luminosity of 5 . 4 fb - 1 . We estimate the cross area to be 0 . 84 + / - 0 . 11 ( stat . )+ / - 0 . 10 ( syst . ) pb , which accepts good with next - to - leading - order perturbative QCD estimates .Using this consequence we put upper limits on potential anomalous trilinear gauge - boson correlation parameters . These results are also used to derive restrictions on models that forecast young heavy neutral vector bosons decaying into pairs of photons or gluons .In addition , these measurements give important input for future investigations for Higgs bosons decaying into two photons . PACS codes : 11 . 30 . Er , 12 . 60 . Jv",
        "rewrite_text": "Title: Zγ Production and Constraints on Anomalous ZZγ and Zγγ Couplings in pp̄ Collisions at √s = 1.96 TeV\n\nAbstract: This study presents the inaugural measurement of the cross-section for the process pp̄ → Zγ + X, where X represents any additional particles produced alongside the Zγ boson. The data for this analysis were gathered by the D0 collaboration during Run II of Fermilab's Tevatron Collider, spanning from 2002 to 2007, with an integrated luminosity of 5.4 fb⁻¹. Our findings indicate an estimated cross-section of 0.84 ± 0.11 (stat.) ± 0.10 (syst.) pb, which aligns well with next-to-leading-order perturbative QCD predictions. Utilizing this measurement, we establish upper limits on potential anomalous trilinear gauge-boson coupling parameters. Furthermore, these results contribute to constraining theoretical models that predict the existence of heavy neutral vector bosons decaying into photon pairs or gluons. Additionally, the insights gained from these measurements are crucial for guiding future research on Higgs bosons that decay into two photons. The implications of this work extend to various areas of particle physics, enhancing our understanding of gauge boson interactions and providing essential data for ongoing and future experimental endeavors. The findings are categorized under PACS codes: 11.30.Er, 12.60.Jv.",
        "ori-fast-z-score": -1.270001270001905,
        "water-fast-z-score": 4.25,
        "rewrite-fast-z-score": 1.0256451881367414
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dynamical models and the phase ordering kinetics of the s=1 spinor condensate .\nAbstract:\nWe study the dynamics of an interacting Bose gas with repulsive contact interactions in one dimension, focusing on its relaxation to equilibrium after being quenched across the superfluid-Mott insulator transition. We show that this system exhibits universal behavior at late times which is characterized by power-law decaying correlations and algebraic growth of entanglement entropy. The exponents are determined analytically using a mapping onto a classical statistical mechanics problem for a driven diffusive system. This work was supported by NSF grant PHY-0960291 (M.S.) and DOE grants DE-FG03-92-ER40701 and DE-SC0012704 (A.K.). \nI. INTRODUCTORY REMARkS\n\nThe recent experimental realization of quantum degenerate gases has opened up new avenues towards understanding strongly correlated many-body systems  1  . In particular, ultracold atomic gases have been used as model systems to explore phenomena such as fermionization  2  , supersolidity  3  , and Mott-insulating states  4  .\nIn this article we consider a particularly interesting class of experiments where the properties of these systems can be probed through their response to sudden changes in parameters  5  . For example, if the strength of inter-particle repulsion or density of particles is suddenly changed then it takes some time before the system reaches thermal equilibrium  6  . During this nonequilibrium evolution, the system may exhibit novel features like dynamical scaling  7, 8  and non-thermal fixed points  9  . These effects are not only important for our fundamental understanding of quantum matter but also provide useful insights into possible routes to realizing novel phases of matter  10  .\nRecently there has been considerable interest in studying the nonequilibrium dynamics of bosonic systems  11  . A particularly well studied case is when the initial state corresponds to a highly excited state above the ground state  12  . It turns out that even though the initial state is far away from equilibrium, the system relaxes to a steady state described by a Gibbs ensemble  13  . However, if the initial state is prepared deep inside the ordered phase, then the system does not",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dynamical models and the phase ordering kinetics of the s = 1 spinor condensate . Abstract : We research the dynamics of an interacting Bose gas with repulsive contact interactions in one dimension , concentrating on its relaxation to equilibrium after being quenched across the superfluid - Mott insulator transition .We see that this scheme exhibits universal behavior at late times which is characterized by power - law decaying correlations and algebraic growth of entanglement entropy . The exponents are chosen analytically taking a mapping onto a traditional statistical mechanics problem for a driven diffusive system .This project was supported by NSF grant PHY - 0960291 ( M . S . ) and DOE funds DE - FG03 - 92 - ER40701 and DE - SC0012704 ( A . K . ) .I . INTRODUCTORY REMARkS The recent experimental realization of quantum degenerate gases has opened up new avenues towards studying strongly interacting large - bodies systems 1 .In particular , ultracold atomic gases have been used as model structures to examine processes such as fermionization 2 , supersolidity 3 , and Mott - insulating states 4 . In this article we imagine a particularly exciting group of studies where the properties of these systems can be probed through their response to unexpected changes in parameters 5 .For instance , if the strength of inter - atom repulsion or density of molecules is suddenly changed then it takes some time before the system reaches heat equilibrium 6 . During this nonequilibrium evolution , the system might exhibit new characteristics like dynamical scaling 7 , 8 and non - thermal fixed points 9 .These effects are not only important for our vital understanding of quantum matter but also make helpful understanding into possible routes to realizing new phases of matter 10 . Recently there has been substantial interest in investigating the nonequilibrium dynamics of bosonic structures 11 .A notably well discussed case is when the initial state corresponds to a highly excited state above the ground state 12 . It turns out that even though the initial state is far back from equilibrium , the system relaxes to a steady state described by a Gibbs ensemble 13 .However , if the first state is prepared deep inside the ordered phase , then the scheme does not",
        "rewrite_text": "In this study, we investigate the dynamics of an interacting Bose gas with repulsive contact interactions in one dimension, focusing on its relaxation to equilibrium following a quench across the superfluid-Mott insulator transition. Our findings reveal that this system exhibits universal behavior at late times, characterized by power-law decaying correlations and algebraic growth of entanglement entropy. The exponents governing these behaviors are derived analytically through a mapping to a conventional statistical mechanics problem related to a driven diffusive system. This research is supported by NSF grant PHY-0960291 (M.S.) and DOE grants DE-FG03-92-ER40701 and DE-SC0012704 (A.K.).\n\nThe recent advancements in the experimental realization of quantum degenerate gases have opened new pathways for exploring strongly interacting many-body systems. Ultracold atomic gases, in particular, serve as effective model systems for investigating phenomena such as fermionization, supersolidity, and Mott insulating states. In this article, we delve into a compelling area of research where the properties of these systems are examined through their responses to sudden changes in external parameters. For example, altering the strength of inter-atomic repulsion or the density of particles leads to a transient period before the system attains thermal equilibrium. During this nonequilibrium evolution, the system may display novel characteristics, including dynamical scaling and the emergence of non-thermal fixed points. These phenomena are crucial for enhancing our understanding of quantum matter and offer insights into potential pathways for realizing new phases of matter.\n\nRecent interest has surged in exploring the nonequilibrium dynamics of bosonic systems, particularly in scenarios where the initial state is highly excited and far from equilibrium. Interestingly, despite this initial deviation, the system tends to relax into a steady state that can be described by a Gibbs ensemble. However, when the initial state is deeply entrenched within the ordered phase, the dynamics diverge from this expected behavior, prompting further investigation into the underlying mechanisms at play.",
        "ori-fast-z-score": 0.07980868844676221,
        "water-fast-z-score": 6.841792643911278,
        "rewrite-fast-z-score": 1.2206826881567392
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Trans-Planckian Issue in the Milne Universe .\nAbstract:\nWe consider the problem of time evolution for quantum fields on the Milne universe, which is an open FRW cosmology with negative spatial curvature and vanishing cosmological constant. We show that there are no normalizable solutions to the Klein-Gordon equation corresponding to massive particles at early times (t → −∞). This result can be understood as follows. The Milne universe has a singularity at t = 0 where all physical quantities diverge. In particular, the energy density diverges like $1/t^2$ near this point. As a consequence, any particle state localized around $t=0$ will have infinite energy. Therefore it cannot correspond to a physically meaningful solution of the field equations. On the other hand, we find that there exist normalizable states describing massless particles at early times. These states represent gravitons or photons propagating into the past along geodesics towards the big bang singularity.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Trans - Planckian Issue in the Milne Universe . Abstract : We consider the question of time evolution for quantum fields on the Milne universe , which is an open FRW cosmology with negative spatial curvature and vanishing cosmological constant .We see that there are no normalizable solutions to the Klein - Gordon equation equivalent to massive particles at early periods ( t → −∞ ) . This result can be understood as follows .The Milne universe has a singularity at t = 0 where all physical components diverge . In particular , the energy density diverges like $ 1 / t ^ 2 $ near this point .As a consequence , any particle state concentrated around $ t = 0 $ will have infinite energy . Therefore it cannot correspond to a physically useful solve of the field equations .On the other hand , we find that there exist normalizable states expressing massless molecules at early years . These states describe gravitons or photons propagating into the past along geodesics towards the big bang singularity .",
        "rewrite_text": "In this article, we explore the dynamics of quantum fields within the framework of the Milne universe, a model characterized by an open Friedmann-Robertson-Walker (FRW) cosmology that features negative spatial curvature and a zero cosmological constant. Our investigation reveals a significant challenge: at early times (as \\( t \\) approaches \\(-\\infty\\)), there are no normalizable solutions to the Klein-Gordon equation that correspond to massive particles. This phenomenon can be attributed to the presence of a singularity at \\( t = 0 \\), where all physical quantities, including energy density, exhibit divergence. Specifically, the energy density behaves like \\( 1/t^2 \\) in proximity to this singularity, leading to the conclusion that any particle state localized around \\( t = 0 \\) would possess infinite energy. Consequently, such states cannot yield physically meaningful solutions to the governing field equations.\n\nConversely, our findings indicate the existence of normalizable states that represent massless particles during the early epochs of the Milne universe. These states are interpreted as describing massless entities such as gravitons or photons, which propagate backward in time along geodesics toward the big bang singularity. This distinction between massive and massless states highlights a critical aspect of quantum field theory in cosmological settings, particularly in relation to the trans-Planckian problem, where the behavior of quantum fields at very early times raises fundamental questions about the nature of spacetime and particle physics. Our work contributes to the ongoing discourse on the implications of the Milne universe for understanding the early universe's quantum dynamics and the challenges posed by singularities in cosmological models.",
        "ori-fast-z-score": -0.629940788348712,
        "water-fast-z-score": 2.6457513110645903,
        "rewrite-fast-z-score": 1.034792955221957
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Topological Classification of Clusters in Condensed Phases .\nAbstract:\nThe topological classification of clusters is reviewed, with emphasis on the role played by symmetry and topology in determining cluster structures.  The concept of  topology  refers to the connectivity between atoms or molecules within a cluster; it can be used as an organizing principle for classifying clusters into families based upon their structural similarities. This review focuses primarily on metal clusters (i.e., clusters containing one or more metallic elements), but also discusses some examples of non-metallic clusters that have been studied recently. In addition, we briefly discuss how this approach has been applied to classify clusters formed during chemical reactions. Finally, we present several open questions related to the topic of cluster topology. Topology plays an important role in understanding the structure of matter at all scales ranging from atomic nuclei to macroscopic materials such as crystals. It provides a useful framework for classifying clusters according to their structural similarity. Herein, we provide a brief overview of recent progress made towards developing a systematic classification scheme for clusters using concepts borrowed from condensed-matter physics.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Topological Classification of Clusters in Condensed Phases . Abstract : The topological classification of clusters is reviewed , with emphasis on the importance played by symmetry and topology in identifying cluster structures .The concept of topology refers to the connectivity between molecules or compounds within a cluster ; it can be used as an organizing principle for classifying clusters into parents based upon their structural similarities . This publication emphasizes primarily on metal clusters ( i . e . , clusters featuring one or more metallic objects ) , but also explains some examples of non - metallic complexes that have been studied ago .In addition , we briefly address how this methodology has been used to classify clusters formed during chemical processes . Finally , we present many open questions related to the subject of cluster topology .Topology plays an important role in understanding the composition of matter at all scales ranging from atomic atoms to macroscopic materials such as crystals . It provides a helpful basis for classifying clusters according to their structural similarity .Herein , we provide a brief overview of recent progress made towards developing a comprehensive classification system for clusters using concepts borrowed from condensed - matter science .",
        "rewrite_text": "The article titled \"Topological Classification of Clusters in Condensed Phases\" offers a comprehensive review of the topological classification of clusters, highlighting the critical roles of symmetry and topology in the identification of cluster structures. Topology, in this context, pertains to the connectivity among molecules or compounds within a cluster, serving as a foundational principle for organizing and classifying clusters based on their structural similarities. While the primary focus of this study is on metal clusters—those that include one or more metallic elements—the authors also discuss notable examples of non-metallic complexes that have been previously investigated. Furthermore, the article explores the application of this classification methodology in categorizing clusters that emerge during various chemical processes. \n\nThe significance of topology is underscored as it aids in comprehending the composition of matter across different scales, from atomic structures to larger macroscopic materials such as crystals. By utilizing topological concepts, the authors aim to establish a systematic approach for classifying clusters based on their structural characteristics. The publication also raises several open questions regarding cluster topology, inviting further exploration and research in this field. Overall, this article provides a succinct overview of recent advancements in the development of a robust classification system for clusters, drawing from principles of condensed matter science to enhance our understanding of cluster formation and behavior.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.703745285369415,
        "rewrite-fast-z-score": -0.658504607868518
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Phase transitions in coupled two dimensional XY systems with spatial anisotropy .\nAbstract:\nWe study the phase diagram and critical behavior of an anisotropic system consisting of Nx × Ny spins on a square lattice, where each spin is described by the two-dimensional XY model. We find that there are three phases depending on the values of J1 / J2 (J2 > 0); ferromagnetic state for small J1 / J2 , spiral state for intermediate J1 / J2 , and paramagnetic state for large J1 / J2 . The transition between these states belongs to the universality class of the Ising model. In particular we show that the spiral state has a nontrivial structure which can be regarded as a superposition of ferromagnetically ordered domains with different orientations. This result suggests that the spiral state may have some relevance to the physics of high-Tc cuprates. \n \n Introduction \n \n It was shown recently  1  that the ground-state properties of the twodimensional Heisenberg antiferromagnet with nearest-neighbor interactions depend strongly on whether or not the exchange interaction along one direction vanishes identically. For example, if the exchange interaction along the y-direction vanishes completely, then the ground state becomes ferromagnetic even though it consists only of S = 1/2 spins. On the other hand, when the exchange interaction along both directions does not vanish simultaneously, the ground state is always antiferromagnetic  2  .\n \nIn this work, we consider another type of anisotropy in the two-dimensional XY model: namely, we assume that the coupling constant along the x-direction is larger than that along the y-direction. As will become clear later, such an anisotropy plays an important role in determining the nature of the ground state.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Phase shifts in coupled two dimensional XY structures with spatial anisotropy . Abstract : We explore the phase diagram and critical behavior of an anisotropic scheme consisting of Nx × Ny spins on a square lattice , where each spin is characterized by the two - dimensional XY model .We see that there are three stages depending on the values of J1 / J2 ( J2 > 0 ) ; ferromagnetic state for large J1 / J2 , spiral state for intermediate J1 / J2 , and paramagnetic state for large J1 / J2 . The transition between these states belongs to the universality category of the Ising model .In particular we find that the spiral state has a nontrivial structure which can be regarded as a superposition of ferromagnetically ordered domains with various orientations . This result suggests that the spiral state may have some relevance to the physics of high - Tc cuprates .Introduction It was shown recently 1 that the ground - state properties of the twodimensional Heisenberg antiferromagnet with nearest - neighbor interactions depend greatly on whether or not the transfer interaction along one position vanishes identically . For instance , if the exchange interaction along the y - direction vanishes totally , then the ground state turns ferromagnetic even though it consists only of S = 1 / 2 spins .On the other hand , when the transfer coupling along both directions does not vanish simultaneously , the ground state is usually antiferromagnetic 2 . In this research , we define another type of anisotropy in the two - dimensional XY model : specifically , we suppose that the coupling constant along the x - direction is bigger than that along the y - direction .As will become clear afterwards , such an anisotropy takes an important role in determining the nature of the ground state .",
        "rewrite_text": "In this study, we investigate the phase diagram and critical behavior of an anisotropic system composed of Nx × Ny spins arranged on a square lattice, where each spin adheres to the two-dimensional XY model. Our findings reveal three distinct phases based on the ratio of coupling constants J1 and J2 (with J2 > 0): a ferromagnetic phase at high J1/J2 values, a spiral phase at intermediate ratios, and a paramagnetic phase at low J1/J2 values. The transitions between these phases are found to belong to the universality class of the Ising model. Notably, the spiral phase exhibits a complex structure that can be interpreted as a superposition of ferromagnetically ordered domains oriented in various directions. This intriguing characteristic of the spiral state may have implications for understanding the physics of high-temperature superconductors, particularly the cuprates.\n\nRecent research has highlighted that the ground-state properties of two-dimensional Heisenberg antiferromagnets with nearest-neighbor interactions are significantly influenced by the presence or absence of transfer interactions along specific directions. For example, when the exchange interaction along the y-direction is completely absent, the ground state transitions to a ferromagnetic configuration, despite being composed solely of S = 1/2 spins. Conversely, when the transfer couplings in both directions are non-zero, the ground state typically exhibits antiferromagnetic behavior. In this paper, we introduce a novel form of anisotropy within the two-dimensional XY model, positing that the coupling constant in the x-direction is greater than that in the y-direction. This anisotropic configuration plays a crucial role in shaping the characteristics of the ground state, as we will demonstrate throughout our analysis. Our results contribute to a deeper understanding of the interplay between anisotropy and phase behavior in two-dimensional spin systems.",
        "ori-fast-z-score": 0.3746343246326776,
        "water-fast-z-score": 5.315277406969789,
        "rewrite-fast-z-score": 3.5821866323102554
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Jets, Blazars and the EBL in the GLAST-EXIST Era .\nAbstract:\nThe Fermi Large Area Telescope (LAT) has revolutionized our understanding of blazar jets by providing unprecedented sensitivity to gamma rays above 100 MeV. The LAT is now routinely detecting flaring activity at GeV energies that are not seen with previous instruments such as EGRET on CGRO or AGILE/GRID. This new data allows us to probe deeper into the physics of these objects than ever before. \n \n In this talk I will discuss how we can use the upcoming generation of space-based observatories - including GLAST and EXIST - to study blazars over an even broader energy range. These missions promise to provide detailed information about the physical processes occurring within relativistic jets through observations across many decades in photon energy. We will also explore some of the exciting science questions that could be addressed using these facilities. Finally, I will present preliminary results from my recent work studying the effects of intergalactic infrared background radiation on blazar spectra.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Jets , Blazars and the EBL in the GLAST - EXIST Era . Abstract : The Fermi Large Area Telescope ( LAT ) has revolutionized our knowing of blazar jets by offering extraordinary exposure to gamma radiation above 100 MeV .The LAT is now regularly detecting flaring activity at GeV energies that are not seen with previous instruments such as EGRET on CGRO or AGILE / GRID . This new data allows us to probe deeper into the physics of these objects than ever before .In this talk I will explore how we can using the latest generation of space - based observatories - including GLAST and EXIST - to study blazars over an much broader energy array . These spacecraft seek to provide comprehensive information about the natural reactions arising within relativistic jets through observations across many years in photon energy .We will also investigate some of the exciting science problems that might be addressed using these facilities . Finally , I will present preliminary results from my current work studying the effects of intergalactic infrared background radiation on blazar spectra .",
        "rewrite_text": "Title: Jets, Blazars, and the EBL in the GLAST-EXIST Era\n\nAbstract: The advent of the Fermi Large Area Telescope (LAT) has significantly advanced our understanding of blazar jets by providing unprecedented sensitivity to gamma-ray emissions exceeding 100 MeV. Unlike its predecessors, such as EGRET on the Compton Gamma Ray Observatory (CGRO) and AGILE/GRID, the LAT has been successful in regularly detecting flaring activities at GeV energies, enabling a deeper investigation into the underlying physics of these enigmatic objects. In this presentation, I will discuss how the latest generation of space-based observatories, including GLAST and EXIST, can be utilized to study blazars across a much wider energy spectrum. These missions aim to deliver extensive insights into the fundamental processes occurring within relativistic jets through long-term observations across various photon energy ranges. Additionally, we will explore several compelling scientific questions that can be addressed with the capabilities of these advanced facilities. Furthermore, I will share preliminary findings from my ongoing research on the impact of intergalactic infrared background radiation on the spectral characteristics of blazars. This work is crucial for understanding the interplay between blazar emissions and the cosmic environment, ultimately contributing to a more comprehensive picture of high-energy astrophysical phenomena.",
        "ori-fast-z-score": 1.2701705922171767,
        "water-fast-z-score": 6.119912853410033,
        "rewrite-fast-z-score": -0.32539568672798425
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Near-Infrared Coronagraphic Observations of the T Tauri Binary System UY Aur .\nAbstract:\nWe report near-infrared coronagraphic observations of the young binary system UY Aurigae (=V773 Tau) obtained with the Subaru Telescope in December 2005 and January 2006, using the newly installed HiCIAO instrument equipped with an occulting mask. The data were reduced by subtracting dark frames and flat fields to remove detector biases and pixel-to-pixel variations respectively. We then applied aperture photometry on each frame after masking out bad pixels and cosmic rays. Finally we averaged all the individual frames together for each filter bandpass. Our results show that there is no significant difference between our two epochs of observation within the uncertainties. In addition, we find that the flux ratio between the primary star and its companion varies significantly depending upon which filter was used during the observations. This suggests that the spectral energy distribution of UY Aur may be changing over time as it evolves towards the main sequence. \n \n Keywords: Young stars",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Near - Infrared Coronagraphic Observations of the T Tauri Binary System UY Aur . Abstract : We report near - infrared coronagraphic observations of the young binary system UY Aurigae ( = V773 Tau ) obtained with the Subaru Telescope in December 2005 and January 2006 , using the newly installed HiCIAO instrument equipped with an occulting mask .The data were reduced by subtracting dark frames and low areas to remove sensor biases and pixel - to - pixel differences respectively . We then implemented lens photometry on each window after masking out bad pixels and cosmic rays .Finally we averaged all the different images together for each filter bandpass . Our results show that there is no major variation between our two epochs of study within the uncertainties .In addition , we find that the flux ratio between the primary star and its companion vary significantly depending upon which filter was used during the surveys . This implies that the spectral power distribution of UY Aur may be changing over time as it evolves towards the main sequence .Keywords: Young stars",
        "rewrite_text": "Title: Near-Infrared Coronagraphic Observations of the T Tauri Binary System UY Aur\n\nAbstract: In this study, we present near-infrared coronagraphic observations of the young binary system UY Aurigae (also known as V773 Tau), conducted with the Subaru Telescope during December 2005 and January 2006. Utilizing the newly installed HiCIAO instrument, which features an occulting mask, we aimed to enhance our understanding of this intriguing stellar system. The observational data underwent a meticulous reduction process, which included subtracting dark frames to eliminate sensor biases and correcting for pixel-to-pixel variations by addressing low areas. Following this, we applied lens photometry to each image segment, carefully masking out any bad pixels and cosmic ray interference to ensure data integrity. The final step involved averaging the images across different filter bandpasses to consolidate our findings.\n\nOur analysis reveals that there is no significant variation between the two observational epochs when considering the associated uncertainties. However, we observed notable fluctuations in the flux ratio between the primary star and its companion, which were contingent on the specific filter employed during the observations. This variability suggests that the spectral power distribution of UY Aur may be undergoing changes as the system evolves towards the main sequence. These findings contribute to the broader understanding of young stellar objects and their developmental processes, highlighting the importance of continuous monitoring in the study of binary systems. \n\nKeywords: Young stars, UY Aurigae, coronagraphy, near-infrared observations, Subaru Telescope.",
        "ori-fast-z-score": 0.7276068751089989,
        "water-fast-z-score": 4.780914437337574,
        "rewrite-fast-z-score": 1.3862065601673441
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  An experimental test of non-local realism .\nAbstract:\nWe report on an experiment that tests the predictions of quantum mechanics and local hidden variable theories for two entangled photons in a Bell state, using polarization-entanglement swapping to create a loophole-free Bell inequality violation.  The experiment is performed with a source of polarization-entangled photon pairs produced by spontaneous parametric down-conversion (SPDC) in a nonlinear crystal pumped by a continuous-wave laser at 405 nm.   A half wave plate rotates one of the photons  polarizations by 45 degrees before it enters a beam splitter which separates the pair into two spatially separated beams.   One of these beams passes through a quarter wave plate oriented such that its fast axis makes an angle of 22.5 degrees relative to horizontal; this transforms the horizontally polarized component of the beam s electric field vector into vertically polarized light.   After passing through another quarter wave plate whose fast axis is aligned with vertical, both components are transformed back into horizontally polarized light.   This transformation can be represented as follows:  H → V → H = HVH.   The other beam travels directly to Alice s measurement station where she performs measurements along three different axes corresponding to projections onto the basis states |V>, |H>, and |D>, where D denotes diagonal.   Bob measures his photon along four different axes corresponding to projections",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : An empirical test of non - local realism . Abstract : We report on an experiment that tests the assumptions of quantum mechanics and local hidden variable theories for two entangled photons in a Bell state , using polarization - entanglement swapping to create a loophole - safe Bell inequality violation .The experiment is conducted with a source of polarization - entangled photon pairs formed by spontaneous parametric down - transfer ( SPDC ) in a nonlinear crystal pumped by a continuous - wave beam at 405 nm . A half wave plate rotates one of the photons polarizations by 45 degrees before it enters a laser splitter which separates the pair into two spatially joined beams .One of these beams passes through a quarter wave plate aligned such that its fast axis creates an angle of 22 . 5 degrees relative to vertical ; this changes the horizontally polarized component of the beam s electric field vector into horizontal polarized light . After passing through another quarter wave plate whose fast axis is aligned with vertical , both components are transformed back into horizontally polarized light .This transformation can be described as follows : H → V → H = HVH . The other laser journeys directly to Alice s monitoring station where she takes measurements along three different axes corresponding to projections onto the basis states | V > , | H > , and | D > , where D denotes diagonal .Bob measures his photon along four different axes corresponding to projections",
        "rewrite_text": "**Title:** An Empirical Test of Non-Local Realism\n\n**Abstract:** This study presents an experimental investigation aimed at evaluating the principles of quantum mechanics and local hidden variable theories through the analysis of two entangled photons in a Bell state. We employ polarization-entanglement swapping to achieve a violation of the Bell inequality that is robust against loopholes. The experimental setup utilizes a source of polarization-entangled photon pairs generated via spontaneous parametric down-conversion (SPDC) in a nonlinear crystal, which is pumped by a continuous-wave laser operating at a wavelength of 405 nm. \n\nIn our procedure, a half-wave plate is utilized to rotate the polarization of one photon by 45 degrees prior to its entry into a beam splitter, which divides the photon pair into two spatially overlapping beams. One of these beams is directed through a quarter-wave plate, positioned such that its fast axis is angled at 22.5 degrees relative to the vertical. This configuration transforms the horizontally polarized component of the beam's electric field vector into vertically polarized light. Following this, the beam passes through a second quarter-wave plate, whose fast axis is aligned vertically, resulting in both polarization components being converted back into horizontally polarized light. This transformation can be succinctly represented as H → V → H = HVH.\n\nSimultaneously, the other photon travels directly to Alice's measurement station, where she conducts measurements along three distinct axes that correspond to the basis states |V⟩, |H⟩, and |D⟩, with |D⟩ representing the diagonal polarization state. Meanwhile, Bob performs measurements on his photon along four different axes, allowing for a comprehensive analysis of the entangled state. The results of this experiment provide critical insights into the validity of non-local realism and the foundational aspects of quantum mechanics, contributing to the ongoing discourse surrounding the interpretation of quantum phenomena.",
        "ori-fast-z-score": 0.2873478855663454,
        "water-fast-z-score": 3.8138503569823694,
        "rewrite-fast-z-score": 0.08606629658238704
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Classical and quantum randomness and the financial market .\nAbstract:\nWe study how classical and quantum uncertainty affect the price dynamics in an incomplete information setting, where agents have access to different sources of information about the underlying state variable. We show that when there is no common knowledge among traders on the true value of the state variable, they may disagree on its expected future evolution even if all are rational and risk-neutral. This disagreement leads to fluctuations in prices which can be amplified by the presence of noise traders who trade based solely on their private signals. In this case, we find that the stock returns exhibit volatility clustering and fat tails similar to those observed empirically. Finally, we demonstrate that these effects persist for both classical and quantum states with non-Gaussian statistics. The results presented here provide new insights into the role played by uncertainty in shaping the statistical properties of asset returns. They also suggest possible avenues for further research aimed at understanding the origin of such phenomena within more realistic models of trading behavior.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Classical and quantum randomness and the monetary market . Abstract : We research how classical and quantum uncertainty influence the price dynamics in an incomplete information context , where agents have access to different sources of information about the underlying state variable .We see that when there is no shared information among traders on the true value of the state variable , they may differ on its expected potential evolution even if all are rational and risk - neutral . This dispute leads to fluctuations in prices which can be amplified by the presence of noise traders who trade based primarily on their private signals .In this situation , we find that the stock yields produce volatility clustering and fat tails identical to those observed empirically . Finally , we prove that these influences persist for both classical and quantum states with non - Gaussian statistics .The results presented here provide fresh insights into the part played by uncertainty in shaping the statistical characteristics of investment returns . They also suggest possible avenues for further studies aimed at studying the origin of such patterns within more realistic descriptions of trading activity .",
        "rewrite_text": "Title: Classical and Quantum Randomness in Monetary Markets\n\nAbstract: This study investigates the impact of classical and quantum uncertainty on price dynamics within a framework of incomplete information, where market participants possess varying levels of access to information regarding the underlying state variable. Our findings reveal that in the absence of a common understanding of the true value of this state variable, traders may hold divergent expectations about its future trajectory, even when all agents are rational and risk-neutral. This divergence in expectations can lead to significant price fluctuations, which are further exacerbated by the presence of noise traders who base their trading decisions primarily on private signals. We demonstrate that under these conditions, the resulting stock yields exhibit characteristics such as volatility clustering and fat tails that closely resemble empirical observations. Furthermore, our analysis shows that these effects are robust across both classical and quantum states characterized by non-Gaussian statistics. The insights gained from this research enhance our understanding of the role that uncertainty plays in influencing the statistical properties of investment returns. Additionally, our findings open up new avenues for future research aimed at exploring the origins of these patterns within more realistic models of trading behavior.",
        "ori-fast-z-score": -1.1055415967851332,
        "water-fast-z-score": 4.8,
        "rewrite-fast-z-score": 2.0691914841450156
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Bimodal AGNs in Bimodal Galaxies .\nAbstract:\nWe present the results of our study on bimodality in galaxies and active galactic nuclei (AGN). We find that there is no significant difference between the fraction of AGNs hosted by red or blue galaxies, but we do see an excess of AGNs with respect to normal galaxies at intermediate colors. This suggests that AGNs are not preferentially found in either red or blue galaxies, as previously thought; instead they appear to be more common among galaxies with intermediate color. The lack of correlation between galaxy color and AGN activity may indicate that AGNs play only a minor role in quenching star formation in massive galaxies. Alternatively, it could suggest that AGNs have different effects depending on their luminosity and/or accretion rate. In addition, we find that the majority of AGNs reside in galaxies with bulges, regardless of whether these galaxies are classified as early-type or late-type systems.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Bimodal AGNs in Bimodal Galaxies . Abstract : We present the conclusion of our research on bimodality in galaxies and active galactic nuclei ( AGN ) .We see that there is no major variation between the fraction of AGNs hosted by red or blue clusters , but we do show an accumulation of AGNs with regard to normal galaxies at intermediate colors . This implies that AGNs are not preferentially found in either blue or blue clusters , as previously thought ; merely they appear to be more common among clusters with intermediate color .The absence of correlation between galaxy color and AGN activity may indicate that AGNs serve only a minor importance in quenching star formation in massive galaxies . Alternatively , it could indicate that AGNs have different impacts depending on their luminosity and / or accretion rate .In addition , we find that the majority of AGNs occur in galaxies with bulges , regardless of whether these objects are classified as early - class or late - class systems .",
        "rewrite_text": "In this study, we investigate the relationship between bimodal galaxies and active galactic nuclei (AGNs), focusing on the distribution of AGNs across different galaxy color classifications. Our findings reveal that there is no significant difference in the proportion of AGNs found in red versus blue galaxy clusters. Instead, we observe a notable concentration of AGNs within galaxies that exhibit intermediate colors. This challenges the prevailing notion that AGNs are predominantly associated with either blue or red clusters, suggesting instead that their occurrence is more prevalent in galaxies with intermediate color characteristics.\n\nFurthermore, our analysis indicates a lack of correlation between the color of a galaxy and the activity of its AGN. This observation raises important questions about the role of AGNs in the process of star formation suppression in massive galaxies. It may suggest that AGNs have a limited influence on quenching star formation, or alternatively, that their effects vary based on factors such as luminosity and accretion rates.\n\nAdditionally, we find that a significant majority of AGNs are located in galaxies with bulges, irrespective of whether these galaxies are categorized as early-type or late-type systems. This finding underscores the importance of bulge presence in hosting AGNs and highlights the need for further research to understand the underlying mechanisms that govern AGN activity in different types of galaxies. Overall, our results contribute to the ongoing discourse regarding the interplay between galaxy morphology, color, and AGN activity, providing new insights into the complex dynamics of these astronomical phenomena.",
        "ori-fast-z-score": 0.3464101615137754,
        "water-fast-z-score": 5.9648090806346055,
        "rewrite-fast-z-score": -0.18257418583505536
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Higher order antibunching in intermediate states .\nAbstract:\nWe study the second-order correlation function for an atom interacting with two modes of light, one resonant and another off-resonant to atomic transition frequency. We show that higher order antibunching can be observed when the atom is initially prepared in an excited state or ground state superposition. The effect is more pronounced if the initial state has some population on the excited state. This phenomenon may have applications in quantum information processing. \n \n Introduction:-In recent years there has been considerable interest in studying nonclassical properties of radiation fields generated by atoms  1  . In particular, it was shown that the photon statistics of such systems are governed by the first-order coherence function g (1) (τ)  2  , which describes bunching behavior at short times and anti-bunching at longer times  3  . It is well known that this property arises due to destructive interference between different pathways leading to emission of photons  4  .\nRecently, several authors studied the effects of spontaneous emission on the second-order correlation functions  5  -  8  . They showed that the presence of spontaneous emission leads to sub-Poissonian statistics  6 - 8  . However, these studies were restricted only to the case where the atom interacts with a single mode of field. On the other hand, many experiments involving atoms interacting simultaneously with multiple modes of electromagnetic field have also been performed  9  -  11  . For example, in Ref.  10  , the authors investigated the influence of vacuum fluctuations on the fluorescence spectrum of a three-level system driven by two laser beams. In addition, they found that the intensity noise of the emitted light depends strongly on the relative phase difference between the driving lasers. Motivated by these experimental results we consider here the problem of calculating the second-order correlation function of an atom interacting simultaneously with two modes of light  12  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Higher order antibunching in intermediate states . Abstract : We research the second - order correlation function for an element interacting with two modes of light , one resonant and another off - resonant to atomic transition frequency .We see that higher order antibunching can be found when the atom is initially prepared in an excited state or ground state superposition . The phenomenon is more pronounced if the first state has some population on the excited state .This phenomenon might have applications in quantum information processing . Introduction : - In recent years there has been substantial interest in investigating nonclassical characteristics of radiation fields generated by atoms 1 .In particular , it was shown that the photon statistics of such systems are governed by the first - order coherence function g ( 1 ) ( τ ) 2 , which explains bunching behavior at short periods and pro - bunching at shorter times 3 . It is well established that this property arises owing to destructive interference between various pathways leading to emission of photons 4 .Recently , various scientists examined the effects of induced emission on the second - order correlation functions 5 - 8 . They showed that the presence of induced emission contributes to sub - Poissonian statistics 6 - 8 .However , these research were restricted only to the case where the atom interacts with a single mode of field . On the other hand , many tests featuring atoms interacting simultaneously with various modes of electromagnetic field have also been performed 9 - 11 .For instance , in Ref . 10 , the published investigated the impact of vacuum fluctuations on the fluorescence spectrum of a three - level network driven by two laser beams .In addition , they reported that the frequency sound of the emitted light depends strongly on the relative phase change between the driving lasers . Motivated by these experimental results we consider here the question of calculating the second - order correlation function of an element interacting simultaneously with two modes of light 12 .",
        "rewrite_text": "**Title:** Higher Order Antibunching in Intermediate States\n\n**Abstract:** In this study, we investigate the second-order correlation function of an atom interacting with two distinct modes of light: one that is resonant and another that is off-resonant with respect to the atomic transition frequency. Our findings reveal that higher order antibunching phenomena can emerge when the atom is initially prepared in a superposition of its excited state and ground state. Notably, the antibunching effect is significantly enhanced when there is a non-negligible population in the excited state. This intriguing behavior has potential implications for advancements in quantum information processing, as it highlights the nuanced control of photon statistics through atomic state preparation. \n\nThe motivation for this research stems from the growing interest in the nonclassical properties of radiation fields produced by atoms. Previous studies have established that the photon statistics in such systems are primarily influenced by the first-order coherence function, which accounts for both bunching and pro-bunching behaviors at varying time scales. The underlying mechanism for these phenomena is attributed to destructive interference among different emission pathways. Recent investigations have also explored the role of induced emission on second-order correlation functions, demonstrating that it can lead to sub-Poissonian statistics. However, these studies have largely focused on scenarios involving single-mode interactions.\n\nIn contrast, our work extends this exploration to scenarios where atoms interact with multiple modes of the electromagnetic field simultaneously. We build on previous research that examined the effects of vacuum fluctuations on the fluorescence spectrum in multi-level atomic systems driven by multiple laser beams, which revealed a strong dependence of the emitted light's frequency on the relative phase of the driving lasers. By analyzing the second-order correlation function in the context of dual-mode interactions, we aim to deepen the understanding of photon statistics in quantum systems and explore their potential applications in quantum technologies.",
        "ori-fast-z-score": -0.4743416490252569,
        "water-fast-z-score": 6.344409644902455,
        "rewrite-fast-z-score": 1.5617376188860608
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Systematic Study of the Final Masses of Gas Giant Planets .\nAbstract:\nWe present an analysis of the final masses of gas giant planets in our Solar System, based on their orbital parameters and compositions inferred by remote sensing observations. We find that Jupiter s mass is consistent with its composition if it formed at 5 AU or less; Saturn s mass can be explained only if it formed beyond 10 AU. The formation of Uranus requires additional processes such as ice condensation to explain its high density. Neptune may have been born more massive than we see today but lost most of its initial mass through photoevaporation driven by intense stellar radiation. Our results suggest that the core accretion model for planet formation cannot fully account for all observed properties of gas giant planets. Keywords: Planet formation, Core accretion theory, Photoevaporation, Mass loss, Stellar radiation, Ice condensation, Orbital evolution, Final mass, Solar system, Remote sensing observation, Compositions, Density",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Systematic Study of the Final Masses of Gas Giant Planets . Abstract : We report an assessment of the last masses of gas giant planets in our Solar System , relying on their orbital characteristics and compositions inferred by distant sensing measurements .We see that Jupiter s mass is compatible with its composition if it created at 5 AU or smaller ; Saturn s mass can be understood only if it created beyond 10 AU . The formation of Uranus demands extra processes such as ice condensation to explain its high density .Neptune might have been born more massive than we saw nowadays but lost most of its initial mass through photoevaporation driven by intense stellar radiation . Our results show that the core accretion theory for planet development cannot fully account for all observed properties of gas giant worlds .Keywords : Planet structure , Core accretion theory , Photoevaporation , Mass loss , Stellar radiation , Ice condensation , Orbital development , Final mass , Solar system , Remote sensing observation , Compositions , Density",
        "rewrite_text": "**Title:** A Systematic Study of the Final Masses of Gas Giant Planets\n\n**Abstract:** In this study, we conduct a comprehensive evaluation of the final masses of gas giant planets within our Solar System, utilizing their orbital dynamics and compositional data obtained through remote sensing techniques. Our findings indicate that Jupiter's mass aligns well with its observed composition, suggesting that it formed at a distance of 5 AU or less from the Sun. In contrast, Saturn's mass can only be reconciled with a formation location beyond 10 AU, highlighting the distinct evolutionary pathways of these two giants. The formation mechanisms of Uranus present additional complexities; our analysis suggests that processes such as ice condensation are necessary to account for its unexpectedly high density. Furthermore, we propose that Neptune may have initially formed with a greater mass than what is currently observed, but subsequently experienced significant mass loss due to photoevaporation, a phenomenon driven by intense radiation from nearby stars. These insights challenge the traditional core accretion model of planetary formation, indicating that it does not adequately explain all the characteristics exhibited by gas giant planets. Our research underscores the need for a more nuanced understanding of the formation and evolution of these celestial bodies, incorporating factors such as stellar radiation and compositional variations. This study contributes to the broader discourse on planetary science, offering new perspectives on the final masses and developmental histories of gas giants in our Solar System.\n\n**Keywords:** Planetary structure, Core accretion theory, Photoevaporation, Mass loss, Stellar radiation, Ice condensation, Orbital evolution, Final mass, Solar system, Remote sensing observations, Compositions, Density.",
        "ori-fast-z-score": -1.162476387438193,
        "water-fast-z-score": 4.73427220735493,
        "rewrite-fast-z-score": -0.811502671200689
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Charged Axially Symmetric Solution and Energy in Teleparallel Theory Equivalent to General Relativity .\nAbstract:\nIn this work, we study the charged axially symmetric solution and energy in teleparallel theory equivalent to general relativity (GR). We find that the field equations are exactly same as those obtained by GR with an additional term which is proportional to the torsion scalar T . The solutions for the metric functions are found numerically using the shooting method. It turns out that these solutions have no singularities at all. In addition, it has been shown that the total energy density is positive definite everywhere inside the star. Finally, we show that our results agree well with those obtained by GR. This shows that the teleparallel gravity can be considered as alternative gravitational theories to GR. Keywords: Charged axially symmetric solution; energy; teleparallel gravity; Einstein-Maxwell system. 1 Introduction Gravity plays important role in understanding many physical phenomena such as black holes  1  , cosmology  2  , quantum mechanics  3  etc.. However, there still remain some unsolved problems like dark matter  4  , dark energy  5  , inflation  6  etc., which cannot be explained within the framework of standard model of particle physics  7, 8  .\nThe most successful classical description of gravitation is provided by Einstein s general relativity (GR)  9  where the curvature tensor R µνρσ describes the geometry of space-time  10  . On the other hand, teleparallel gravity  11  -  13  is another approach to describe gravitation on the basis of tetrad fields e A µ instead of metric g µν  14  . Here, the basic variables are connection coefficients Γ λ µν defined through vierbein fields e \nwhere η AB = diag(−1, +1, +1, +1), and h ABCD denotes the contortion tensor  15  . The corresponding Lagrangian density reads  16  :",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Charged Axially Symmetric Solution and Energy in Teleparallel Theory Equivalent to General Relativity . Abstract : In this study , we study the charged axially symmetric solution and energy in teleparallel theory equivalent to general relativity ( GR ) .We get that the field equations are exactly same as those given by GR with an additional term which is proportional to the torsion scalar T . The solutions for the metric functions are found numerically using the shot technique .It turns out that these solutions have no singularities at all . In addition , it has been shown that the total energy density is positive definite everywhere inside the star .Finally , we find that our findings agree well with those achieved by GR . This shows that the teleparallel gravitational can be regarded as alternative gravity explanations to GR .Keywords : Charged axially symmetric solution ; energy ; teleparallel gravitational ; Einstein - Maxwell system . 1 Introduction Gravity takes key importance in understanding several physical phenomena such as black holes 1 , cosmology 2 , quantum mechanics 3 etc . .However , there still emerge some unsolved issues like dark matter 4 , darkness energy 5 , inflation 6 etc . , which cannot be described within the framework of standard description of particle science 7 , 8 . The most popular classical description of gravitation is provided by Einstein s general relativity ( GR ) 9 where the curvature tensor R µνρσ describes the topology of space - time 10 .On the other hand , teleparallel gravitational 11 - 13 is another technique to define gravitation on the basis of tetrad fields e A µ instead of metric c µν 14 . Here , the fundamental variables are connection coefficients Γ λ µν characterized through vierbein fields e where η AB = diag ( −1 , + 1 , + 1 , + 1 ) , and h ABCD denotes the contortion tensor 15 .The corresponding Lagrangian density reads 16 :",
        "rewrite_text": "**Title:** Charged Axially Symmetric Solution and Energy in Teleparallel Theory Equivalent to General Relativity\n\n**Abstract:** This article investigates the charged axially symmetric solution and the associated energy within the framework of teleparallel theory, which is equivalent to general relativity (GR). We demonstrate that the field equations derived in this context mirror those of GR, with the notable addition of a term that is proportional to the torsion scalar \\( T \\). To obtain the solutions for the metric functions, we employ a numerical approach known as the shooting method. Remarkably, our findings reveal that these solutions are free from singularities throughout the entire region considered. Furthermore, we establish that the total energy density remains positive definite at all points within the star, indicating a stable configuration. Our results align closely with those obtained from GR, reinforcing the notion that teleparallel gravity can serve as a viable alternative framework for understanding gravitational phenomena. This study contributes to the ongoing discourse on gravitational theories, particularly in light of unresolved issues such as dark matter, dark energy, and cosmic inflation, which challenge the conventional particle physics paradigm. While Einstein's general relativity remains the predominant classical theory of gravitation, characterized by the curvature tensor \\( R_{\\mu\\nu\\rho\\sigma} \\) that describes the geometry of spacetime, teleparallel gravity offers a distinct perspective by utilizing tetrad fields \\( e^A_\\mu \\) instead of the metric tensor \\( g_{\\mu\\nu} \\). In this framework, the fundamental variables are the connection coefficients \\( \\Gamma^\\lambda_{\\mu\\nu} \\), which are defined through the vierbein fields \\( e \\), where \\( \\eta_{AB} = \\text{diag}(-1, +1, +1, +1) \\), and the contortion tensor \\( h_{ABCD} \\) plays a crucial role. The corresponding Lagrangian density is also discussed, providing a comprehensive understanding of the theoretical underpinnings of our findings.\n\n**Keywords:** Charged axially symmetric solution; energy; teleparallel gravity; Einstein-Maxwell system.",
        "ori-fast-z-score": -0.08944271909999159,
        "water-fast-z-score": 5.480077554195743,
        "rewrite-fast-z-score": 0.26013299085723596
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Cosmic Acceleration, Dark Energy and Fundamental Physics .\nAbstract:\nThe discovery that the universe is expanding at an accelerating rate has led to intense interest in dark energy as well as new ideas about fundamental physics. This talk will review some recent results on these topics including constraints on models for cosmic acceleration using supernovae data, measurements of the Hubble constant with Type Ia supernovae, and tests of general relativity using gravitational lensing statistics. The talk will also discuss how future surveys such as LSST can be used to further our understanding of dark energy and fundamental physics. I will conclude by discussing my own work on testing gravity theories beyond Einstein s theory using weak lensing observations. Keywords: Cosmology, Dark Energy, General Relativity, Weak Lensing, Supernovae, Gravitational Waves, Cosmic Microwave Background. Speaker: Adam Riess (Princeton University) Date: February 18, 2007 Time: 4:30pm - 5:15pm Location: Room B",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Cosmic Acceleration , Dark Energy and Fundamental Physics . Abstract : The observation that the universe is growing at an accelerating rate has led to intense interest in dark energy as well as fresh concepts about basic physics .This discussion will review some latest findings on these topics including constraints on estimates for cosmic acceleration using supernovae information , measurements of the Hubble constant with Type Ia supernovae , and examinations of general relativity involving gravitational lensing statistics . The speech will also discuss how potential experiments such as LSST can be used to further our knowing of light mass and fundamental physics .I will conclude by reviewing my own research on proving gravity explanations beyond Einstein s principle involving weak lensing observations . Keywords : Cosmology , Dark Energy , General Relativity , Weak Lensing , Supernovae , Gravitational Waves , Cosmic Microwave Background .Speaker : Adam Riess ( Princeton University ) Date : February 18 , 2007 Time : 4 : 30pm - 5 : 15pm Location : Room B",
        "rewrite_text": "Title: Cosmic Acceleration, Dark Energy, and Fundamental Physics\n\nAbstract: The discovery that the universe is expanding at an accelerating pace has sparked significant interest in the concept of dark energy and has prompted a reevaluation of fundamental physics principles. This article aims to provide an overview of recent developments in these areas, focusing on constraints derived from supernova observations that inform our understanding of cosmic acceleration. We will delve into the measurements of the Hubble constant obtained through Type Ia supernovae and explore the implications of gravitational lensing statistics for general relativity. Additionally, the potential of upcoming experiments, such as the Large Synoptic Survey Telescope (LSST), will be discussed in relation to enhancing our comprehension of light mass and the foundational aspects of physics. The presentation will culminate with a summary of my own research, which seeks to extend gravitational theories beyond Einstein's framework, particularly through the analysis of weak lensing data. \n\nKeywords: Cosmology, Dark Energy, General Relativity, Weak Lensing, Supernovae, Gravitational Waves, Cosmic Microwave Background.\n\nSpeaker: Adam Riess (Princeton University)  \nDate: February 18, 2007  \nTime: 4:30 PM - 5:15 PM  \nLocation: Room B",
        "ori-fast-z-score": -0.8682431421244593,
        "water-fast-z-score": 4.764608329895903,
        "rewrite-fast-z-score": 0.25
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Quantum Hall ferromagnetism in graphene: a SU(4) bosonization approach .\nAbstract:\nWe study the quantum Hall effect (QHE) and its interplay with magnetism in monolayer graphene by using an effective low-energy model that takes into account both electron-electron interactions and disorder effects. We show how to derive this model starting from first principles, and we discuss some of its main properties. In particular, we find that at half-filling it exhibits two different phases depending on the strength of the Coulomb interaction between electrons. For weak coupling these are separated by a phase transition driven by spontaneous breaking of time-reversal symmetry; for strong coupling they correspond respectively to a conventional QHE state and to a novel fractionalized topological insulator characterized by chiral edge states. The latter is shown to be equivalent to a spin-1/2 Heisenberg antiferromagnet on the honeycomb lattice, which can thus be described within the framework of the so-called SU(4) bosonic representation.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Quantum Hall ferromagnetism in graphene : a SU ( 4 ) bosonization approach . Abstract : We research the quantum Hall impact ( QHE ) and its interplay with magnetism in monolayer graphene by using an efficient low - energy theory that takes into consideration both electron - ion interactions and disorder interactions .We see how to derive this description starting from first principles , and we investigate some of its primary characteristics . In particular , we find that at half - filling it displays two different phases depending on the strength of the Coulomb interaction between electrons .For weak bonding these are apart by a phase shift motivated by premature breaking of time - reversal symmetry ; for strong coupling they relate respectively to a traditional QHE state and to a novel fractionalized topological insulator characterized by chiral edge states . The latter is demonstrated to be analogous to a spin - 1 / 2 Heisenberg antiferromagnet on the honeycomb structure , which can thus be described within the framework of the so - called SU ( 4 ) bosonic representation .",
        "rewrite_text": "In this study, we explore the quantum Hall effect (QHE) and its relationship with magnetism in monolayer graphene through a robust low-energy theoretical framework that incorporates both electron-ion interactions and disorder effects. We derive this theoretical approach from fundamental principles and examine its key features. Notably, we identify two distinct phases at half-filling, which are influenced by the strength of the Coulomb interaction among electrons. In the regime of weak coupling, these phases are separated by a phase shift that arises from an early onset of time-reversal symmetry breaking. Conversely, in the strong coupling regime, the phases correspond to a conventional quantum Hall state and a novel fractionalized topological insulator, which is characterized by chiral edge states. We demonstrate that this fractionalized state bears a resemblance to a spin-1/2 Heisenberg antiferromagnet on a honeycomb lattice, allowing us to describe it within the SU(4) bosonic representation framework. This research provides significant insights into the interplay between quantum Hall ferromagnetism and electronic correlations in graphene, highlighting the rich physics that emerges from the unique properties of this two-dimensional material. Our findings contribute to a deeper understanding of the quantum phenomena in graphene and open avenues for future investigations into its potential applications in quantum computing and spintronics.",
        "ori-fast-z-score": -0.23570226039551587,
        "water-fast-z-score": 5.500933918218137,
        "rewrite-fast-z-score": 1.5554275420956378
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A very massive runaway star from Cygnus OB2 .\nAbstract:\nWe report the discovery of an extremely bright and hot (T eff = 300,000 K) blue supergiant in the open cluster NGC 6231 with a mass loss rate of 10^-6 M_sun/yr. The object is located at a distance of 1 kpc from Earth and has a luminosity of 5 x 10^{5} L_Sun. It shows strong emission lines of He II 4686 Å, N III 4641 Å, C IV 5801 Å, O V 7322 Å, and H Balmer series. We suggest that this object may be a member of the Cygnus OB2 association which contains several other high-mass stars. This would make it one of the most luminous known single stars outside our Galaxy. If confirmed by further observations, this object will provide important constraints on stellar evolution models for massive stars. Keywords: Open clusters; Blue supergiants",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A very giant runaway star from Cygnus OB2 . Abstract : We report the discovery of an incredibly bright and hot ( T eff = 300 , 000 K ) blue supergiant in the open cluster NGC 6231 with a mass loss rate of 10 ^ - 6 M _ sun / yr .The object is situated at a distance of 1 kpc from Earth and has a luminosity of 5 x 10 ^ { 5 } L _ Sun . It displays strong radiation lines of He II 4686 Å , N III 4641 Å , C IV 5801 Å , O V 7322 Å , and H Balmer series .We suggest that this object may be a constituent of the Cygnus OB2 association which contains multiple other high - mass stars . This might making it one of the most luminous known single stars outside our Galaxy .If confirmed by further observations , this body will provide important restrictions on stellar evolution models for huge stars . Keywords : Open clusters ; Blue supergiants",
        "rewrite_text": "We present the discovery of an exceptionally luminous and hot blue supergiant star located within the open cluster NGC 6231, characterized by an effective temperature of 300,000 K and a mass loss rate of 10^-6 M☉ per year. This remarkable star is positioned approximately 1 kiloparsec from Earth and boasts a luminosity of 5 x 10^5 L☉. Spectroscopic analysis reveals prominent emission lines, including He II at 4686 Å, N III at 4641 Å, C IV at 5801 Å, O V at 7322 Å, and various lines from the H Balmer series. We propose that this star is likely a member of the Cygnus OB2 association, which is known for hosting multiple high-mass stars. If our findings are substantiated by additional observations, this star could be recognized as one of the most luminous single stars identified beyond our Milky Way galaxy. The implications of this discovery are significant, as it may provide critical insights into the processes governing stellar evolution, particularly for massive stars. Understanding the characteristics and behavior of such a high-energy object could refine existing models of stellar life cycles and mass loss mechanisms. This research contributes to the broader understanding of open clusters and the role of blue supergiants within them, highlighting the importance of continued exploration in these stellar nurseries. Keywords associated with this study include open clusters and blue supergiants, reflecting the focus on these extraordinary celestial phenomena.",
        "ori-fast-z-score": -0.5252257314388902,
        "water-fast-z-score": 2.994344952558949,
        "rewrite-fast-z-score": 0.9622504486493763
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Observations on degenerate saddle point problems .\nAbstract:\nWe consider the problem of finding an approximate solution to a convex optimization problem with a nonconvex objective function and linear constraints, where the feasible region is defined by a set of equality or inequality constraints.  We show that under certain conditions this problem can be solved efficiently using a combination of gradient descent methods for solving the subproblems in each iteration and a line search method based on the Armijo-Goldstein condition.  The proposed algorithm has been implemented as part of the open source software package CVXPY (http://cvxpy.org/).  Numerical experiments are presented which demonstrate the effectiveness of our approach. Keywords: Convex Optimization, Nonconvex Objective Function, Saddle Point Problem, Gradient Descent Method, Line Search Algorithm, Armijo-Goldstain Condition. 1 Introduction In many practical applications it may not always be possible to find an exact solution to a given mathematical model due to computational complexity issues associated with the underlying numerical algorithms used to solve such models.  For example, in some cases it might only be possible to obtain an approximate solution within a specified tolerance level.  This situation arises frequently when dealing with large-scale nonlinear programming problems arising in various fields including engineering design, operations research, economics, etc., see e.g.,  1  ,  4  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Observations on degenerate saddle point questions . Abstract : We consider the question of finding an approximate solve to a convex optimization problem with a nonconvex objective function and linear constraints , where the feasible region is characterized by a setting of equality or inequality constraints .We see that under certain conditions this question can be answered easily using a combination of gradient descent methods for solving the subproblems in each iteration and a line search method using on the Armijo - Goldstein condition . The proposed algorithm has been deployed as part of the open source software product CVXPY ( http : / / cvxpy . org / ) .Numerical observations are presented which demonstrate the effectiveness of our approach . Keywords : Convex Optimization , Nonconvex Objective Function , Saddle Point Problem , Gradient Descent Method , Line Search Algorithm , Armijo - Goldstain Condition .1 Introduction In many practical applications it could not always be possible to find an precise answer to a given numerical model owing to computational difficulty issues associated with the fundamental numerical algorithms used to solve such models . For instance , in some cases it could only be possible to obtain an approximate solve within a specified tolerance range .This condition occurs commonly when dealing with large - scale nonlinear programming difficulties arising in different fields including industrial design , management research , economics , etc . , see e . g . , 1 , 4 .",
        "rewrite_text": "**Title:** Observations on Degenerate Saddle Point Problems\n\n**Abstract:** This article addresses the challenge of approximating solutions to convex optimization problems characterized by nonconvex objective functions and linear constraints. The feasible region is defined by a set of equality or inequality constraints, which complicates the optimization process. We demonstrate that, under specific conditions, this problem can be effectively tackled using a combination of gradient descent methods to solve subproblems at each iteration, alongside a line search technique that adheres to the Armijo-Goldstein condition. The algorithm we propose has been integrated into the open-source software package CVXPY (http://cvxpy.org/), facilitating broader accessibility and application. Our numerical experiments illustrate the efficacy of this approach, showcasing its potential to yield satisfactory approximate solutions in various scenarios. \n\nIn practical applications, obtaining precise solutions to numerical models can often be hindered by computational challenges inherent in traditional numerical algorithms. This is particularly evident in large-scale nonlinear programming problems, which arise across diverse fields such as industrial design, management science, and economics. In many instances, it becomes necessary to settle for approximate solutions within a defined tolerance range, a situation that is frequently encountered in real-world applications. Our findings contribute to the understanding of how to navigate these complexities, providing insights into the effectiveness of our proposed methods in yielding reliable approximations. \n\n**Keywords:** Convex Optimization, Nonconvex Objective Function, Saddle Point Problem, Gradient Descent Method, Line Search Algorithm, Armijo-Goldstein Condition.",
        "ori-fast-z-score": 0.1873171623163388,
        "water-fast-z-score": 6.434283176858165,
        "rewrite-fast-z-score": 0.27975144247209416
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  VIMOS-VLT spectroscopy of the giant Ly-alpha nebulae associated with three z~2.5 radio galaxies .\nAbstract:\nWe present VLT/VIMOS integral field spectroscopic observations for three high-z (z ~ 2.5) radio galaxies, which are known to be surrounded by extended Lyman alpha halos. The main goal is to study their kinematics and physical conditions in order to understand how these objects evolve into massive elliptical galaxies at low redshifts. We find that all three sources show complex velocity fields dominated by rotation around an axis perpendicular to the radio jets. In addition we detect several components showing blueshifted velocities up to -500 km/s relative to systemic redshift. These features may represent outflows driven by AGN feedback or galactic winds powered by star formation activity. Finally, we measure the gas density distribution using  OII  emission lines and estimate the mass of ionized hydrogen surrounding each galaxy. Our results suggest that the observed Lyman alpha halos have masses ranging between 10^10 M_sol and 10^11 M_sol .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : VIMOS - VLT spectroscopy of the giant Ly - alpha nebulae associated with three z ~ 2 . 5 broadcast galaxies . Abstract : We report VLT / VIMOS integral field spectroscopic observations for three high - z ( z ~ 2 . 5 ) broadcast galaxies , which are known to be surrounded by extended Lyman alpha halos .The main goal is to study their kinematics and physical conditions in order to explain how these objects evolve into huge elliptical galaxies at low redshifts . We see that all three sources show complex momentum fields dominated by rotation around an axis adjacent to the radio jets .In addition we find various components showing blueshifted velocities up to - 500 km / s relative to systemic redshift . These features could indicate outflows driven by AGN feedback or galactic winds driven by galaxy formation activity .Finally , we measure the gas density density using OII absorption lines and estimate the mass of ionized hydrogen surrounding each galaxy . Our results propose that the studied Lyman alpha halos have masses vary between 10 ^ 10 M _ sol and 10 ^ 11 M _ sol .",
        "rewrite_text": "We present integral field spectroscopic observations conducted with the VLT/VIMOS for three high-redshift broadcast galaxies at approximately z ~ 2.5, each of which is enveloped by extensive Lyman-alpha nebulae. The primary objective of this study is to investigate the kinematic properties and physical conditions of these nebulae, providing insights into the evolutionary pathways that lead these galaxies to develop into massive elliptical galaxies at lower redshifts. Our analysis reveals that all three galaxies exhibit intricate momentum fields, predominantly characterized by rotation around an axis that is closely aligned with their associated radio jets. Furthermore, we observe multiple components exhibiting blueshifted velocities reaching up to -500 km/s in relation to the systemic redshift. These velocity signatures may suggest the presence of outflows potentially driven by active galactic nucleus (AGN) feedback or galactic winds resulting from ongoing galaxy formation processes. Additionally, we employ OII absorption lines to assess the gas density surrounding each galaxy, allowing us to estimate the mass of ionized hydrogen in their vicinity. Our findings indicate that the Lyman-alpha halos associated with these galaxies possess masses ranging from 10^10 M_sun to 10^11 M_sun. This research contributes to our understanding of the dynamic environments of high-redshift galaxies and their role in the broader context of galaxy evolution.",
        "ori-fast-z-score": 0.21566554640687682,
        "water-fast-z-score": 5.330017908890261,
        "rewrite-fast-z-score": 1.165543034828717
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Planets around evolved intermediate-mass stars. I. Two substellar companions in the open clusters NGC 2423 and NGC 4349 .\nAbstract:\nWe report on two new brown dwarf candidates discovered by direct imaging with NACO/VLT at distances of ~100 AU to their host star, which are members of open clusters NGC 2423 (M67) and NGC 4349. The first object is located close to the cluster center and has an estimated mass between 0.03-0.07 M . It was found as part of our survey for low-mass companions to nearby young solar-type stars. We also present follow-up observations of this candidate using adaptive optics techniques that confirm its substellar nature. \n \n The second object lies outside the core radius of the cluster and has an estimated mass below 0.01 M . This companion may be either a planetary or very-low-mass stellar companion depending on whether it formed through gravitational instability or fragmentation during cloud collapse. Both objects have masses well above those predicted by current models of planet formation via disk accretion.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Planets around evolved intermediate - mass stars . I .Two substellar companions in the open complexes NGC 2423 and NGC 4349 . Abstract : We report on two new brown dwarf candidates discovered by direct imaging with NACO / VLT at distances of ~ 100 AU to their host star , which are members of open complexes NGC 2423 ( M67 ) and NGC 4349 .The first object is situated close to the cluster center and has an estimated mass between 0 . 03 - 0 . 07 M . It was found as part of our survey for low - mass companions to nearby young solar - class stars .We additionally post follow - up observations of this candidate using adaptive optics techniques that prove its substellar nature . The second object lies outside the primary distance of the cluster and has an estimated mass below 0 . 01 M .This companion might be either a planetary or very - low - mass stellar companion depending on whether it created through gravity instability or fragmentation during cloud collapse . Both bodies have masses well above those predicted by current theories of planet development via disk accretion .",
        "rewrite_text": "We present findings on two newly identified brown dwarf candidates discovered through direct imaging using the NACO instrument on the Very Large Telescope (VLT). These candidates are located approximately 100 AU from their respective host stars and are associated with the open clusters NGC 2423 (M67) and NGC 4349. The first candidate is positioned near the center of the cluster and has an estimated mass ranging from 0.03 to 0.07 solar masses (M☉). This discovery is part of our ongoing survey aimed at identifying low-mass companions to nearby young solar-type stars. To confirm its substellar classification, we conducted follow-up observations utilizing adaptive optics techniques, which provided compelling evidence of its nature. The second candidate, in contrast, is situated beyond the primary distance of the cluster and has an estimated mass of less than 0.01 M☉. This object raises intriguing questions regarding its formation; it could either be classified as a planetary companion or a very low-mass stellar companion, depending on whether it formed through gravitational instability or fragmentation during the collapse of a molecular cloud. Notably, both candidates possess masses that exceed the expectations set by current models of planetary formation through disk accretion processes. These discoveries contribute to our understanding of the formation and evolution of substellar objects in the context of evolved intermediate-mass stars, highlighting the complexities and potential pathways of companion formation in stellar environments.",
        "ori-fast-z-score": -0.10721125348377948,
        "water-fast-z-score": 3.624412178045377,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Correlated modulation between the redshifted Fe K alpha line and the continuum emission in NGC 3783 .\nAbstract:\nWe report on an observation made with Suzaku satellite to study X-ray variability of active galactic nucleus (AGN) NGC 3783, which is known as one of brightest Seyfert 1 galaxies at soft X-rays. We found that the fluxes of both the iron Kα line and the continuum are modulated by a factor of about two over a period of 100 ks. The phase lag between these two components was estimated to be ~0.1 s using cross-correlation analysis. This result suggests that there exists some physical connection between them. In addition, we detected significant time lags for higher-order harmonics of the fundamental frequency component up to the third harmonic. These results suggest that the observed variability may originate from reverberation effects caused by variable illumination of the accretion disk around supermassive black hole. Keywords: Reverberation; Time lag; Iron line; AGN",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Correlated modulation between the redshifted Fe K alpha band and the continuum emission in NGC 3783 . Abstract : We report on an observation made with Suzaku spacecraft to study X - ray variability of active galactic nucleus ( AGN ) NGC 3783 , which is known as one of brightest Seyfert 1 clusters at soft X - radiation .We showed that the fluxes of both the metal Kα line and the continuum are modulated by a factor of about two over a period of 100 ks . The phase lag between these two parts was calculated to be ~ 0 . 1 s using cross - correlation analysis .This result suggests that there exists some physical link between them . In addition , we identified significant moment lags for greater - order harmonics of the fundamental frequency component up to the third harmonic .These data suggest that the seen variability may originate from reverberation effects caused by variable illumination of the accretion disk around supermassive black hole . Keywords : Reverberation ; Time lag ; Iron line ; AGN",
        "rewrite_text": "We present findings from an observational study conducted with the Suzaku spacecraft, focusing on the X-ray variability of the active galactic nucleus (AGN) NGC 3783, a prominent Seyfert 1 galaxy recognized for its strong soft X-ray emissions. Our analysis reveals that both the flux of the redshifted Fe Kα line and the continuum emission exhibit a modulation of approximately twofold over a timescale of 100 kiloseconds. Notably, we calculated a phase lag of around 0.1 seconds between these two emission components through cross-correlation analysis, indicating a potential physical connection between them. Furthermore, we observed significant moment lags for higher-order harmonics, extending up to the third harmonic of the fundamental frequency component. These findings imply that the variability detected may be attributed to reverberation effects resulting from the variable illumination of the accretion disk surrounding the supermassive black hole at the center of NGC 3783. Our results contribute to the understanding of the intricate dynamics within AGNs and the mechanisms driving their X-ray emissions, highlighting the importance of time-domain studies in unraveling the complexities of these distant cosmic phenomena. The implications of these observations extend to the broader context of AGN research, particularly in terms of the interplay between emission lines and continuum radiation, and the role of accretion processes in shaping the observable characteristics of such energetic systems. \n\nKeywords: Reverberation; Time lag; Iron line; AGN.",
        "ori-fast-z-score": 1.016001016001524,
        "water-fast-z-score": 5.334005334008001,
        "rewrite-fast-z-score": 2.335296179807324
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Nonsupersymmetric Brane/Antibrane Configurations in Type IIA and M Theory .\nAbstract:\nWe present nonsupersymmetric brane configurations in type IIA string theory, which are obtained by wrapping D-branes on supersymmetry-breaking cycles. We also discuss the corresponding configurations in M-theory. In particular we show that these configurations can be described as bound states of intersecting NS5-branes with orientifold 5-planes (or O6-planes). The latter are related to each other via T-duality transformations. Finally, we give an explicit example for such a configuration involving two stacks of coincident D3-branes at angles. This is done using the technique developed recently by Sen. We find agreement between our results and those derived previously within supergravity approximation. N = 1 supersymmetry is broken down to N = 0 when one wraps D-branes around supersymmetry breaking cycles  1  . These configurations have been studied extensively over the past few years  2  -  8  .\nIn this letter we will consider non-supersymmetric brane-antibrane configurations in type-IIA string theory  9  , where both branes wrap supersymmetry breaking cycles. Such configurations were first discussed in  10  . They correspond to bound states of intersecting D4-branes wrapped on 2-cycles  11  or NS5-branes  12  . It was shown in  13  that they can be described alternatively as bound states of intersected NS5-branes with O6 planes  14  . Here we will use the description given in terms of NS5-O6 systems  15  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Nonsupersymmetric Brane / Antibrane Configurations in Type IIA and M Theory . Abstract : We create nonsupersymmetric brane configurations in type IIA string theory , which are derived by wrapping D - branes on supersymmetry - breaking cycles .We also discuss the equivalent configurations in M - theory . In particular we show that these structures can be described as bound states of intersecting NS5 - branes with orientifold 5 - planes ( or O6 - planes ) .The latter are related to each other via T - duality transformations . Finally , we give an explicit instance for such a configuration involving two stacks of coincident D3 - branes at angles .This is accomplished use the method developed ago by Sen . We get consensus between our findings and those generated previously within supergravity approximation . N = 1 supersymmetry is broken down to N = 0 when one wrapping D - branes around supersymmetry breaking cycles 1 .These arrangements have been studied thoroughly over the previous few years 2 - 8 . In this letter we will explore non - supersymmetric brane - antibrane configurations in type - IIA string theory 9 , where both branes wrap supersymmetry broken cycles .Such configurations were first explained in 10 . They correspond to bound states of intersecting D4 - branes wrapped on 2 - cycles 11 or NS5 - branes 12 .It was shown in 13 that they can be described additionally as bound states of intersected NS5 - branes with O6 planes 14 . Here we will use the description presented in terms of NS5 - O6 systems 15 .",
        "rewrite_text": "In this article, we investigate nonsupersymmetric brane configurations within the framework of type IIA string theory, specifically focusing on configurations formed by wrapping D-branes around cycles that break supersymmetry. We extend our analysis to analogous configurations in M-theory, demonstrating that these structures can be interpreted as bound states of intersecting NS5-branes and orientifold 5-planes (or O6-planes), which are interconnected through T-duality transformations. Our study includes a detailed example involving two stacks of coincident D3-branes positioned at angles, employing a method previously established by Sen. Our results align with earlier findings derived from supergravity approximations, revealing that the presence of D-branes wrapped around supersymmetry-breaking cycles leads to a reduction of N = 1 supersymmetry to N = 0. The exploration of these configurations has garnered significant attention in recent years, with various studies contributing to the understanding of their properties and implications. In this letter, we will delve into non-supersymmetric brane-antibrane configurations in type IIA string theory, where both branes are wrapped around cycles that break supersymmetry. These configurations, initially introduced in prior works, correspond to bound states of D4-branes wrapped on 2-cycles or NS5-branes. Furthermore, it has been established that they can also be described as bound states of intersecting NS5-branes with O6-planes. Our approach will utilize the framework of NS5-O6 systems to provide a comprehensive understanding of these nonsupersymmetric configurations and their significance in the broader context of string theory.",
        "ori-fast-z-score": 0.4216370213557839,
        "water-fast-z-score": 5.059644256269407,
        "rewrite-fast-z-score": 1.0945409092309881
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Central limit theorems in linear structural error-in-variables models with explanatory variables in the domain of attraction of the normal law .\nAbstract:\nWe study central limit theorems for estimators of parameters in linear regression models where errors are not necessarily normally distributed but have an elliptical distribution, and we allow some explanatory variables to be non-normal. We show that under suitable conditions on the model coefficients, the asymptotic distributions of these estimators can be approximated by those obtained when all the explanatory variables follow a multivariate normal distribution. The results are illustrated through simulation experiments. Keywords: Central Limit Theorem; Elliptical Distributions; Regression Modeling. 1 Introduction In many applications it is assumed that the response variable follows a Gaussian distribution while the predictors may or may not be normally distributed. For example, this assumption has been used extensively in econometrics (see e.g., Greene  2003  ). However, there are situations where the data generating process does not satisfy such assumptions. This motivates us to consider more general classes of distributions which include as special cases both the normal and nonnormal distributions. One class of distributions that includes most common probability density functions encountered in practice is given by the so-called elliptical distributions. These distributions were introduced independently by Kelker  1970  , Hüsler and Reiss  1981  , and Fang et al.  1987  . They are characterized by their dependence structure rather than their marginal densities. A random vector X = (X1, ..., Xd)T ∈ Rd belongs to the family of elliptical distributions if its characteristic function satisfies E exp(itX)  = exp{−V (t)},\nwhere V : R →  0, ∞) is called the characteristic generator. If V ≡ 0 then X is said to belong to the family of spherical distributions. Examples of elliptical distributions include:",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Central limit theorems in linear structural error - in - variables models with explanatory variables in the domain of attraction of the normal law . Abstract : We research central limit theorems for estimators of parameters in linear regression systems where errors are not necessarily normally distributed but have an elliptical distribution , and we allow some explanatory variables to be non - normal .We see that under suitable conditions on the model variables , the asymptotic distributions of these estimators can be approximated by those achieved when all the explanatory variables continue a multivariate normal distribution . The results are shown through simulation studies .Keywords : Central Limit Theorem ; Elliptical Distributions ; Regression Modeling . 1 Introduction In many applications it is expected that the response parameter follows a Gaussian distribution while the predictors would or may not be usually spread .For instance , this assumption has been used heavily in econometrics ( saw e . g . , Greene 2003 ) . However , there are circumstances where the information collecting mechanism does not satisfy such constraints .This motivates us to consider more general categories of distributions which contain as special cases both the usual and nonnormal distributions . One class of distributions that contains most common probability density functions seen in practice is given by the so - called elliptical distributions .These distributions were introduced independently by Kelker 1970 , Hüsler and Reiss 1981 , and Fang et al . 1987 .They are marked by their dependence structure rather than their marginal densities . A random matrix X = ( X1 , . . . , Xd ) T ∈ Rd belongs to the group of elliptical distributions if its characteristic function satisfies E exp ( itX ) = exp { −V ( t ) } , where V : R → 0 , ∞ ) is dubbed the characteristic generator .If V ≡ 0 then X is said to belong to the class of spherical distributions . Examples of elliptical distributions involve :",
        "rewrite_text": "**Title:** Central Limit Theorems in Linear Structural Error-in-Variables Models with Explanatory Variables in the Domain of Attraction of the Normal Law\n\n**Abstract:** This study investigates the central limit theorems applicable to parameter estimators in linear regression models characterized by non-normally distributed errors, specifically those following an elliptical distribution. Additionally, we consider scenarios where some explanatory variables may also deviate from normality. Our findings indicate that, under certain conditions imposed on the model variables, the asymptotic distributions of these estimators can be effectively approximated by the distributions obtained when all explanatory variables are assumed to follow a multivariate normal distribution. This research is supported by simulation studies that illustrate the robustness of our theoretical results. The implications of our work extend to various fields, particularly in econometrics, where the assumption of normally distributed response variables is prevalent, yet often unrealistic due to the nature of data collection processes. By exploring a broader class of distributions, including elliptical distributions—which encompass many commonly encountered probability density functions—we aim to provide a more flexible framework for regression modeling. Elliptical distributions, first introduced by Kelker (1970), Hüsler and Reiss (1981), and Fang et al. (1987), are distinguished by their dependence structures rather than their marginal densities. A random vector \\( X = (X_1, \\ldots, X_d)^T \\in \\mathbb{R}^d \\) is classified as belonging to the elliptical distribution family if its characteristic function satisfies \\( E[\\exp(itX)] = \\exp\\{-V(t)\\} \\), where \\( V: \\mathbb{R} \\to (0, \\infty) \\) is known as the characteristic generator. Notably, if \\( V \\equiv 0 \\), then \\( X \\) is categorized as a spherical distribution. Our exploration of elliptical distributions and their properties contributes to a deeper understanding of the behavior of estimators in linear regression frameworks, particularly in the presence of non-normal errors and predictors.\n\n**Keywords:** Central Limit Theorem; Elliptical Distributions; Regression Modeling.",
        "ori-fast-z-score": -0.9760921603577252,
        "water-fast-z-score": 4.810702354423639,
        "rewrite-fast-z-score": 0.18257418583505536
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Laplacian spectra of complex networks and random walks on them: Are scale-free architectures really important? .\nAbstract:\nWe study the Laplacian spectrum of complex networks, focusing in particular on its relation to the dynamics of random walks on them. We show that for any network with n nodes there are at most 2n eigenvalues different from zero (counting multiplicities). This bound is tight up to a constant factor when applied to trees or complete graphs. For general graphs we prove an upper bound of O(n log n) on the number of distinct nonzero eigenvalues. In addition, we provide lower bounds showing that this estimate cannot be improved by more than a polylogarithmic factor. Finally, we present numerical evidence suggesting that real-world networks have only few distinct nonzero eigenvalues. These results suggest that the spectral properties of complex networks may not depend strongly on their degree distribution but rather on other structural features such as clustering coefficients. The analysis presented here can also be used to derive new bounds on mixing times of Markov chains defined over these networks.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Laplacian spectra of complex networks and random runs on them : Are scale - free architectures really important ? .Abstract : We research the Laplacian spectrum of complex networks , concentrating in particular on its connection to the dynamics of random runs on them . We see that for any connection with n nodes there are at most 2n eigenvalues different from zero ( counting multiplicities ) .This bound is tight up to a constant factor when applied to forests or perfect graphs . For general graphs we prove an upper bound of O ( n log n ) on the number of distinct nonzero eigenvalues .In addition , we provide lower bounds indicating that this estimate cannot be improved by more than a polylogarithmic factor . Finally , we present numerical information suggesting that real - time systems have only few distinct nonzero eigenvalues .These conclusions show that the spectral properties of complex networks may not depend greatly on their degree function but rather on other structural components such as clustering coefficients . The evaluation provided here can also be used to derive new limits on mixing times of Markov chains defined over these networks .",
        "rewrite_text": "In this study, we investigate the Laplacian spectrum of complex networks, with a specific focus on its relationship to the dynamics of random walks conducted on these networks. Our findings reveal that for any network comprising n nodes, there exists a maximum of 2n distinct eigenvalues that are non-zero, when accounting for multiplicities. This upper limit is shown to be tight, up to a constant factor, particularly in the case of forests and perfect graphs. For more general graph structures, we establish an upper bound of O(n log n) on the number of distinct non-zero eigenvalues. Furthermore, we provide lower bounds that demonstrate this estimate cannot be improved by more than a polylogarithmic factor. Our numerical analyses indicate that real-time systems tend to exhibit a limited number of distinct non-zero eigenvalues. These results suggest that the spectral characteristics of complex networks may not be significantly influenced by their degree distribution, but rather by other structural features, such as clustering coefficients. The insights gained from this evaluation also pave the way for deriving new constraints on the mixing times of Markov chains defined over these networks. Overall, our research contributes to a deeper understanding of the interplay between the spectral properties of complex networks and their underlying structural attributes, challenging the notion of the critical importance of scale-free architectures in this context.",
        "ori-fast-z-score": -0.20412414523193154,
        "water-fast-z-score": 4.4907311951024935,
        "rewrite-fast-z-score": 1.958260097304659
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dynamic condensation of water at crack tips in fused silica glass .\nAbstract:\nWe report the observation of dynamic condensation of water vapor on crack tips during slow fracture experiments performed under vacuum conditions (10-6 mbar) and low temperature (77 K). The condensed water is found to be localized along the crack front, where it forms a thin film that covers the entire surface of the crack tip. This phenomenon has been observed for cracks propagating both perpendicularly and parallel to the direction of maximum tensile stress. We propose a model based on molecular dynamics simulations which explains this effect by considering the presence of an electric field generated by the moving crack tip. In addition we show how the formation of such films can affect the mechanical properties of the material. Condensation phenomena are ubiquitous in nature but have rarely been reported in materials science. Here we present experimental evidence showing that water condenses onto the crack surfaces when they propagate through fused silica glasses. These results were obtained using a combination of optical microscopy techniques with environmental scanning electron microscopy (ESEM), Raman spectroscopy and infrared reflection absorption spectroscopy (IRAS).",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dynamic condensation of liquid at crack tips in fused silica glass . Abstract : We report the observation of static condensation of water vapor on break tips during slow fracture studies performed under vacuum environments ( 10 - 6 mbar ) and low heat ( 77 K ) .The condensed water is found to be localized along the crack front , where it creates a thin film that covers the entire surface of the crack tip . This phenomenon has been observed for fracture propagating both perpendicularly and parallel to the direction of maximum tensile tension .We suggest a theory based on molecular dynamics simulations which explains this effect by examining the presence of an electric field produced by the moved crack edge . In addition we explain how the formation of such films can affect the mechanical behavior of the material .Condensation problems are ubiquitous in nature but have seldom been reported in materials science . Here we present research proof showing that water condenses onto the crack surfaces when they propagate through fused silica glasses .These conclusions were obtained using a combination of optical microscopy methods with environmental scan electron microscopy ( ESEM ) , Raman spectroscopy and infrared reflection spectral spectroscopy ( IRAS ) .",
        "rewrite_text": "We present findings on the dynamic condensation of water vapor at the tips of cracks in fused silica glass, observed during slow fracture experiments conducted in vacuum conditions (10^-6 mbar) and at low temperatures (77 K). Our study reveals that water vapor condenses in a localized manner along the crack front, forming a thin film that envelops the entire surface of the crack tip. This condensation phenomenon has been documented for fractures propagating both perpendicular and parallel to the direction of maximum tensile stress. To elucidate this effect, we propose a theoretical framework grounded in molecular dynamics simulations, which highlights the role of an electric field generated by the movement of the crack edge. Furthermore, we discuss the implications of such thin film formation on the mechanical properties of the material, suggesting that the presence of condensed water could significantly influence fracture behavior. While condensation phenomena are commonly observed in various natural processes, their occurrence in materials science, particularly in the context of crack propagation, has been relatively underexplored. Our research provides compelling evidence that water vapor condenses on crack surfaces during the propagation of fractures in fused silica glass. The conclusions drawn from this study are supported by a multifaceted approach that combines optical microscopy, environmental scanning electron microscopy (ESEM), Raman spectroscopy, and infrared reflection absorption spectroscopy (IRAS). This comprehensive methodology allows for a detailed investigation of the condensation process and its effects on the mechanical integrity of fused silica, paving the way for further exploration of moisture-related phenomena in material science.",
        "ori-fast-z-score": -0.40406101782088427,
        "water-fast-z-score": 5.454823740581938,
        "rewrite-fast-z-score": 0.8908708063747479
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Diversity of Decline-Rate-Corrected Type Ia Supernova Rise Times: One Mode or Two? .\nAbstract:\nWe present the results of an analysis of rise times for a sample of type Ia supernovae (SNeIa) with well-measured light curves and redshifts in order to determine whether there is one mode of SNeIa rise time, as suggested by Phillips et al. (1999), or two modes, as suggested by Riess et al. (1999). We find that our data are consistent with either model at the 2-sigma level; however, we cannot rule out the possibility of only one mode being present. The best-fit values for the parameters describing each distribution differ significantly between these models. If future observations confirm this result, it will have important implications on cosmological studies using SNeIa as distance indicators. In particular, if there really are two populations of SNeIa, then the use of a single value for the stretch parameter may lead to systematic errors in determining distances. \n \n Keywords: Supernova, Light curve",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  Diversity of Decline-Rate-Corrected Type Ia Supernova Rise Times: One Mode or Two?.Abstract : We present the results of an assessment of rise periods for a sample of type Ia supernovae ( SNeIa ) with good - measured light curves and redshifts in order to find whether there is one mode of SNeIa rise history , as suggested by Phillips et al . ( 1999 ) , or two modes , as suggested by Riess et al .( 1999 ) . We see that our statistics are compatible with either model at the 2 - sigma level ; however , we cannot leave out the suggestion of only one mode being present .The best - fitting values for the variables describing each distribution depend greatly between these models . If future discoveries confirm this result , it will have important implications on cosmological experiments using SNeIa as distance indicators .In particular , if there really are two groups of SNeIa , then the using of a single value for the stretch variable might lead to systematic errors in calculating distances . Keywords : Supernova , Light curve",
        "rewrite_text": "Title: Diversity of Decline-Rate-Corrected Type Ia Supernova Rise Times: One Mode or Two?\n\nAbstract: This study investigates the rise times of a sample of Type Ia supernovae (SNeIa) characterized by well-measured light curves and redshifts, aiming to determine whether their rise history follows a single mode, as proposed by Phillips et al. (1999), or two distinct modes, as suggested by Riess et al. (1999). Our analysis reveals that the statistical data we gathered can support either hypothesis at a 2-sigma confidence level. However, we cannot dismiss the possibility of a singular mode existing among the SNeIa rise times. The parameters that best fit the distributions of rise times vary significantly between the two models under consideration. Should future observations corroborate our findings, the implications for cosmological research utilizing SNeIa as distance indicators could be profound. Specifically, the existence of two distinct groups of SNeIa would necessitate caution in applying a uniform stretch parameter for distance calculations, as this could introduce systematic errors. Our results highlight the importance of further investigation into the rise characteristics of SNeIa to enhance the accuracy of cosmological measurements. The ongoing debate regarding the nature of SNeIa rise times underscores the complexity of these astronomical phenomena and their critical role in our understanding of the universe. \n\nKeywords: Supernova, Light curve",
        "ori-fast-z-score": -1.4142135623730951,
        "water-fast-z-score": 4.330522446256832,
        "rewrite-fast-z-score": -0.6060915267313265
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Optimal Investment with an Unbounded Random Endowment and Utility-Based Pricing .\nAbstract:\nWe consider the problem of optimal investment in a financial market when there is no upper bound on the investor s wealth, but his/her utility function exhibits decreasing absolute risk aversion (DARA). We show that under DARA preferences, the value function for this problem can be characterized as the unique solution to a nonlinear partial differential equation (PDE) which we call the Hamilton-Jacobi-Bellman-Isaacs PDE. This characterization allows us to use standard numerical methods such as finite difference or Monte Carlo simulation to compute the value function numerically. In addition, it also enables us to study how the optimal strategy depends on various parameters including the initial endowment, the interest rate, and the volatility of the stock price process. Finally, by using our results, we are able to provide some new insights into the relationship between pricing and hedging derivatives based on utility maximization principles. The main contributions of this work include:",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Optimal Investment with an Unbounded Random Endowment and Utility - Based Pricing . Abstract : We consider the question of optimal investment in a financial market when there is no upper bound on the investor s assets , but his / her utility function exhibits decreasing absolute hazard aversion ( DARA ) .We see that under DARA preferences , the value function for this question can be described as the unique solve to a nonlinear partial differential equation ( PDE ) which we call the Hamilton - Jacobi - Bellman - Isaacs PDE . This description permits us to use conventional numerical technique such as finite difference or Monte Carlo simulation to compute the value function numerically .In addition , it also enables us to study how the ideal approach depends on various variables notably the early endowment , the interest rate , and the volatility of the stock price cycle . Finally , by using our findings , we are able to provide some fresh insights into the relationship between financing and hedging derivatives using on utility maximization theories .The main contributions of this work include :",
        "rewrite_text": "In this article, titled \"Optimal Investment with an Unbounded Random Endowment and Utility-Based Pricing,\" we explore the optimal investment strategies within a financial market characterized by an investor's unbounded assets and a utility function that demonstrates decreasing absolute risk aversion (DARA). Our analysis reveals that, under DARA preferences, the value function associated with this investment problem can be uniquely defined as the solution to a nonlinear partial differential equation (PDE), which we refer to as the Hamilton-Jacobi-Bellman-Isaacs PDE. This formulation allows us to apply standard numerical methods, such as finite difference techniques and Monte Carlo simulations, to effectively compute the value function.\n\nFurthermore, our study investigates how the optimal investment strategy is influenced by various factors, including the initial endowment, prevailing interest rates, and the volatility of stock price movements. By examining these relationships, we gain valuable insights into the dynamics of investment decision-making in the presence of uncertain financial environments.\n\nAdditionally, our findings contribute to a deeper understanding of the interplay between financing and hedging derivatives, grounded in utility maximization theories. This work not only enhances the theoretical framework surrounding optimal investment but also provides practical implications for investors seeking to navigate complex financial landscapes. The primary contributions of this research include the establishment of a robust mathematical framework for analyzing optimal investment under DARA preferences, the development of numerical methods for value function computation, and the elucidation of key factors that shape investment strategies in uncertain market conditions.",
        "ori-fast-z-score": 1.2222222222222223,
        "water-fast-z-score": 6.405028512341099,
        "rewrite-fast-z-score": 1.247219128924647
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Stellar Content and Recent Star Formation History of the Local Group Dwarf Irregular Galaxy IC1613 .\nAbstract:\nWe present deep optical photometry in B, V , R c I c bands for the dwarf irregular galaxy IC 1613 obtained with the Wide Field Imager (WFI) at the MPG/ESO 2.2 m telescope on La Silla Observatory. The data were reduced using standard IRAF routines. We derived total magnitudes within an aperture radius of 5 arcsec by applying aperture corrections to the PSF-fitted magnitudes. Our results are compared with previous studies based on shallower observations. In addition we derive new estimates for the distance modulus DM = 27.9 ± 0.1 mag and foreground extinction A V = 0.10 ± 0.02 mag towards this galaxy. Using these values together with our photometric measurements we determined absolute magnitudes M B = −15.6 ± 0.3 mag, M V = −14.7 ± 0.4 mag, M Rc = −12.8 ± 0.5 mag, M Ic = −11.0 ± 0.6 mag and colour indices U−B = 1.45±0.25 mag, B−V =0.70±0.06 mag, V −Rc=0.55±0.05 mag, V −Ic=1.00±0.07 mag. These parameters allow us to estimate the mean metallicity Z = 0.008 ± 0.001 dex and age t = 3 Gyrs for the stellar population of IC 1613.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Stellar Content and Recent Star Formation History of the Local Group Dwarf Irregular Galaxy IC1613 . Abstract : We report deep optical photometry in B , V , R c I c groups for the dwarf irregular universe IC 1613 obtained with the Wide Field Imager ( WFI ) at the MPG / ESO 2 . 2 m observatory on La Silla Observatory .The data were reduced using traditional IRAF procedures . We extracted total magnitudes within an lens radius of 5 arcsec by using aperture corrections to the PSF - fitted magnitudes .Our results are compared with previous findings based on shallower observations . In addition we derive new accounts for the distance modulus DM = 27 . 9 ± 0 . 1 mag and foreground extinction A V = 0 . 10 ± 0 . 02 mag towards this galaxy .Using these values combined with our photometric calculations we calculated absolute magnitudes M B = −15 . 6 ± 0 . 3 mag , M V = −14 . 7 ± 0 . 4 mag , M Rc = −12 . 8 ± 0 . 5 mag , M Ic = −11 . 0 ± 0 . 6 mag and colour indices U−B = 1 . 45±0 . 25 mag , B−V = 0 . 70±0 . 06 mag , V −Rc = 0 . 55±0 . 05 mag , V −Ic = 1 . 00±0 . 07 mag . These variables enable us to estimate the mean metallicity Z = 0 . 008 ± 0 . 001 dex and age t = 3 Gyrs for the stellar population of IC 1613 .",
        "rewrite_text": "We present a comprehensive analysis of the stellar content and recent star formation history of the dwarf irregular galaxy IC 1613, utilizing deep optical photometry in the B, V, R_c, and I_c bands. The observations were conducted with the Wide Field Imager (WFI) at the MPG/ESO 2.2 m telescope located at La Silla Observatory. The data reduction was performed using standard IRAF procedures, ensuring high-quality results. We determined total magnitudes within a 5 arcsecond aperture, applying aperture corrections to the PSF-fitted magnitudes to enhance accuracy. Our findings are juxtaposed with earlier studies that relied on shallower observations, providing a more refined understanding of the galaxy's characteristics.\n\nIn our analysis, we derived a new distance modulus of DM = 27.9 ± 0.1 mag and a foreground extinction value of A_V = 0.10 ± 0.02 mag for IC 1613. These parameters, in conjunction with our photometric measurements, allowed us to calculate absolute magnitudes: M_B = −15.6 ± 0.3 mag, M_V = −14.7 ± 0.4 mag, M_Rc = −12.8 ± 0.5 mag, and M_Ic = −11.0 ± 0.6 mag. Additionally, we computed the color indices U−B = 1.45 ± 0.25 mag, B−V = 0.70 ± 0.06 mag, V−R_c = 0.55 ± 0.05 mag, and V−I_c = 1.00 ± 0.07 mag. These derived values facilitated the estimation of the mean metallicity of the stellar population in IC 1613, which is found to be Z = 0.008 ± 0.001 dex, alongside an age of approximately t = 3 Gyr. This study contributes significantly to the understanding of the stellar evolution and formation processes in dwarf irregular galaxies, particularly within the context of the Local Group.",
        "ori-fast-z-score": -0.75,
        "water-fast-z-score": 4.093146241443879,
        "rewrite-fast-z-score": 1.3065491598369756
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  High-energy Cosmic Rays and Neutrinos from Semi-relativistic Hypernovae .\nAbstract:\nWe propose that the most energetic cosmic rays are accelerated in supernova remnants by relativistic jets powered by hypernova explosions, which may be associated with gamma-ray bursts (GRBs). We show how this model can explain several observed features of GRBs: their duration distribution; their association with massive star formation regions; their high luminosities; and their large redshifts. The proposed mechanism is also able to accelerate protons up to energies beyond 10^20 eV without violating current observational constraints on the diffuse fluxes of high-energy neutrinos or photons produced during the acceleration process. This scenario provides an explanation for the origin of ultra-high energy cosmic rays as well as for the production of the highest energy neutrinos detected so far. In addition, it offers a natural explanation for the recent detection of very bright optical flashes following some GRBs. \n \n High-energy cosmic rays have been measured at Earth over many decades  1  . Their spectrum extends up to energies above 1020 eV  2  , but no astrophysical source has yet been identified that accelerates particles to such extreme energies  3  . It seems likely that these cosmic rays were accelerated in distant sources billions of years ago  4  .\n \nThe most powerful known explosion in our Universe occurs when a massive star collapses into a black hole after exhausting its nuclear fuel supply  5  . Such events release huge amounts of gravitational binding energy  6  , which powers relativistic outflows called  jets ; they are believed to produce gamma-ray bursts  7, 8  . These jets could provide the necessary power to accelerate cosmic rays to extremely high energies  9  . \n \n However, there are two major difficulties in explaining the origin of the most energetic cosmic ray particles using conventional models  10  : \n \n 1) Conventional jet-powered models cannot accelerate protons to energies greater than ~10^19 eV  11  because the maximum Lorentz factor Γmax of the flow decreases rapidly with distance r from the central engine  12  . As a result, the total kinetic energy available to accelerate particles drops dramatically with increasing particle energy E  13  . For example, if we assume that the bulk Lorentz factor of the",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : High - energy Cosmic Rays and Neutrinos from Semi - relativistic Hypernovae . Abstract : We suggest that the most intense cosmic rays are accelerated in supernova remnants by relativistic jets driven by hypernova bursts , which sometimes be identified with gamma - ray clusters ( GRBs ) .We see how this model can describe several observed features of GRBs : their duration distribution ; their association with massive star formation regions ; their high luminosities ; and their large redshifts . The proposed process is also could to accelerate protons up to energies beyond 10 ^ 20 eV without violating present observational restrictions on the diffuse fluxes of high - energy neutrinos or photons created during the acceleration cycle .This scenario offers an excuse for the origin of ultra - large energy cosmic rays as well as for the production of the highest power neutrinos detected so far . In addition , it gives a natural explanation for the recent discovery of very bright optical bursts following some GRBs .High - energy cosmic rays have been measured at Earth over much centuries 1 . Their spectrum stretches up to energies above 1020 eV 2 , but no astrophysical source has already been determined that accelerates particles to such extreme energies 3 .It seems likely that these cosmic rays were accelerated in nearby sources billions of years past 4 . The most intense reported blast in our Universe comes when a huge star collapses into a black hole after exhausting its nuclear fuel supply 5 .Such events release massive amounts of gravitational binding energy 6 , which powers relativistic outflows called jets ; they are said to produce gamma - ray waves 7 , 8 . These jets could give the necessary power to accelerate cosmic rays to incredibly high energies 9 .However , there are two major obstacles in describe the origin of the most intense cosmic ray ions using conventional versions 10 : 1 ) Conventional jet - powered designs cannot accelerate protons to energies higher than ~ 10 ^ 19 eV 11 because the maximum Lorentz factor Γmax of the flow drops rapidly with distance r from the main engine 12 . As a result , the total kinetic power available to accelerate particles decreases dramatically with rising particle power E 13 .For instance , if we suppose that the bulk Lorentz factor of the",
        "rewrite_text": "**Title: High-Energy Cosmic Rays and Neutrinos from Semi-Relativistic Hypernovae**\n\n**Abstract:** In this study, we propose a novel mechanism for the acceleration of the most energetic cosmic rays, suggesting that they originate from supernova remnants influenced by relativistic jets produced during hypernova explosions, which are often associated with gamma-ray bursts (GRBs). Our model effectively accounts for several key characteristics of GRBs, including their duration distributions, their correlation with regions of massive star formation, their exceptional luminosities, and their significant redshifts. We demonstrate that this acceleration process can propel protons to energies exceeding 10^20 eV while remaining consistent with current observational limits on the diffuse fluxes of high-energy neutrinos and photons generated during the acceleration process. This framework not only provides insights into the origins of ultra-high-energy cosmic rays but also elucidates the mechanisms behind the production of the most powerful neutrinos detected to date. Furthermore, our model offers a compelling explanation for the recent observations of exceptionally bright optical bursts that follow certain GRBs. High-energy cosmic rays have been detected on Earth for centuries, with their spectrum extending beyond 10^20 eV; however, a definitive astrophysical source capable of accelerating particles to such extreme energies has yet to be identified. It is plausible that these cosmic rays were generated in nearby sources billions of years ago. The most powerful explosions in the universe occur when massive stars collapse into black holes after depleting their nuclear fuel, releasing vast amounts of gravitational binding energy that drives relativistic jets, which are known to produce gamma-ray emissions. These jets may provide the necessary energy to accelerate cosmic rays to unprecedented levels. Nevertheless, conventional models face significant challenges in explaining the origins of the most intense cosmic ray ions, primarily due to limitations in the acceleration capabilities of traditional jet-powered mechanisms, which struggle to achieve proton energies beyond approximately 10^19 eV. The rapid decline of the maximum Lorentz factor of the flow with distance from the central engine results in a substantial reduction of the kinetic power available for particle acceleration, complicating the understanding of these extreme cosmic phenomena.",
        "ori-fast-z-score": -0.29649972666444047,
        "water-fast-z-score": 7.666981720054651,
        "rewrite-fast-z-score": 2.587274448341005
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dynamic fracture of icosahedral model quasicrystals: A molecular dynamics study .\nAbstract:\nWe have performed molecular dynamics simulations to investigate the dynamic crack propagation in an icosahedral AlPdMn quasicrystal and its periodic approximant, i-AlCuFe. The results show that both materials exhibit similar features for the crack growth process at low temperatures (T = 300 K). However, there are significant differences between them when T is increased up to 600 K. In particular, we find that the quasicrystal shows a higher resistance against crack propagation than the approximant under tensile loading conditions. This behavior can be explained by considering the different atomic structures of these two systems. \n \n We also studied how the temperature affects the mechanical properties of the quasicrystal. Our results indicate that increasing the temperature leads to a decrease in the elastic constants C11 and C44 as well as in the bulk modulus B. Moreover, our calculations reveal that the Young s moduli E decreases with increasing temperature.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dynamic fracture of icosahedral model quasicrystals : A molecular dynamics investigation . Abstract : We have done molecular mechanics simulations to examine the dynamic crack propagation in an icosahedral AlPdMn quasicrystal and its periodic approximant , i - AlCuFe .The results show that both materials exhibit similar features for the crack growth process at low temperatures ( T = 300 K ) . However , there are significant variations between them when T is expanded up to 600 K . In particular , we find that the quasicrystal demonstrates a higher resistance against break transmission than the approximant under tensile loading circumstances .This phenomenon can be described by examining the different atomic structures of these two systems . We actually studied how the temperature changes the mechanical behavior of the quasicrystal .Our results show that raising the temperature leads to a reduction in the elastic constants C11 and C44 as well as in the bulk modulus B . Moreover , our calculations reveal that the Young s moduli E decreases with expanding temperature .",
        "rewrite_text": "In this study, we conducted molecular dynamics simulations to investigate the dynamic fracture behavior of an icosahedral AlPdMn quasicrystal and its periodic approximant, i-AlCuFe. Our findings indicate that both materials exhibit comparable characteristics in the crack propagation process at a low temperature of 300 K. However, as the temperature increases to 600 K, notable differences emerge between the two materials. Specifically, the quasicrystal demonstrates a greater resistance to crack propagation under tensile loading conditions compared to its approximant. This enhanced resistance can be attributed to the distinct atomic structures present in the two systems. \n\nWe further explored how temperature influences the mechanical properties of the quasicrystal. Our results reveal that increasing the temperature results in a decrease in the elastic constants, specifically C11 and C44, as well as a reduction in the bulk modulus, B. Additionally, our calculations indicate that the Young's modulus, E, also diminishes with rising temperature. These findings provide valuable insights into the mechanical behavior of quasicrystals and their approximants, highlighting the significant impact of temperature on their fracture dynamics. This research contributes to a deeper understanding of the mechanical properties of quasicrystalline materials, which is essential for their potential applications in various fields, including materials science and engineering.",
        "ori-fast-z-score": 0.23570226039551587,
        "water-fast-z-score": 5.185449728701349,
        "rewrite-fast-z-score": 0.3110855084191276
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Efficient method for detection of periodic orbits in chaotic maps and flows .\nAbstract:\nWe present an efficient numerical scheme to detect the existence of periodic orbits in chaotically behaving dynamical systems, such as chaotic maps or chaotic flows. The proposed algorithm is based on the concept of shadowing trajectories which are close approximations of unstable periodic orbits embedded within the attractor. We show that our approach can be used to efficiently compute the topological entropy of chaotic maps with non-integer slopes. Finally we demonstrate how this new technique can be applied to study the dynamics of a model system describing the interaction between two coupled semiconductor lasers. Periodic orbits play an important role in understanding the behavior of many nonlinear dynamical systems. In particular they provide valuable information about the underlying structure of the attractors associated with these systems. However, it has been shown that finding all periodic orbits of a given periodicity may not always be possible due to their complicated nature  1  . This problem becomes even more challenging when dealing with chaotic systems where the number of periodic orbits increases exponentially with increasing period  2  .\nIn recent years there have been several attempts to develop techniques to find periodic orbits numerically  3, 4, 5, 6, 7, 8  , but most of them suffer from one or both of the following drawbacks: (i) They require very high computational resources. (ii) They do not guarantee convergence towards the desired orbit. Here we propose a novel numerical scheme to overcome these difficulties by using the concept of shadowing  9  . Shadowing refers to the property of some trajectories being close approximations of unstable orbits embedded inside the attractor. It was first introduced by Anosov  10  who showed that every trajectory starting sufficiently close to any unstable periodic orbit will remain close to it for at least a certain amount of time. Since then various authors  11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Efficient method for recognition of periodic orbits in chaotic maps and flows . Abstract : We present an efficient numerical system to identify the existence of periodic orbits in chaotically behaving dynamical systems , such as chaotic maps or turbulent flows .The proposed algorithm is based on the idea of shadowing trajectories which are close approximations of unstable periodic orbits embedded within the attractor . We see that our approach can be used to easily compute the topological entropy of turbulent maps with non - integer peaks .Finally we prove how this new technique can be applied to study the dynamics of a model network describing the interaction between two coupled semiconductor lasers . Periodic orbits hold an important role in understanding the dynamics of several nonlinear dynamical systems .In particular they give valuable info about the fundamental structure of the attractors found with these systems . However , it has been shown that finding all periodic orbits of a given periodicity might not always be possible due to their complicated nature 1 .This problem remains especially more challenging when dealing with turbulent systems where the number of periodic orbits changes exponentially with expanding period 2 . In past decades there have been numerous attempts to develop techniques to find periodic orbits numerically 3 , 4 , 5 , 6 , 7 , 8 , but most of them suffer from one or both of the following drawbacks : ( i ) They require very high computational resources .( ii ) They do not secure convergence towards the desired orbit . Here we develop a new numerical plan to overcome these problems by using the idea of shadowing 9 .Shadowing refers to the property of some trajectories being close approximations of unstable orbits embedded inside the attractor . It was first developed by Anosov 10 who demonstrated that every orbital beginning sufficiently close to any unstable periodic orbit will remain close to it for at least a certain quantity of time .Since then various authors 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 , 31 , 32 , 33 , 34 , 35 , 36 , 37 , 38 , 39 , 40 , 41 , 42 , 43 , 44",
        "rewrite_text": "**Title:** Efficient Method for Recognition of Periodic Orbits in Chaotic Maps and Flows\n\n**Abstract:** In this study, we introduce a novel numerical approach designed to efficiently identify periodic orbits within chaotic dynamical systems, including chaotic maps and turbulent flows. Our algorithm leverages the concept of shadowing trajectories, which serve as close approximations to unstable periodic orbits situated within the system's attractor. This method not only facilitates the computation of topological entropy for turbulent maps exhibiting non-integer peaks but also demonstrates its applicability in analyzing the dynamics of a model network that simulates the interaction between two coupled semiconductor lasers. \n\nPeriodic orbits are crucial for understanding the dynamics of various nonlinear systems, as they provide significant insights into the fundamental structure of the attractors associated with these systems. However, the task of identifying all periodic orbits of a specific periodicity can be daunting due to their intricate nature. This challenge is exacerbated in turbulent systems, where the number of periodic orbits tends to increase exponentially with the period. Over the past few decades, numerous numerical techniques have been proposed to locate periodic orbits; however, many of these methods face critical limitations, including high computational demands and a lack of guaranteed convergence towards the target orbit.\n\nTo address these issues, we present a new numerical strategy that employs the principle of shadowing. This principle, initially introduced by Anosov, posits that trajectories starting sufficiently close to an unstable periodic orbit will remain in proximity for a significant duration. Our approach builds upon this foundational concept, allowing for more efficient identification of periodic orbits while minimizing computational resource requirements. Through extensive analysis and application of our method, we demonstrate its effectiveness in revealing the underlying dynamics of complex systems, thereby contributing to the broader understanding of chaotic behavior in nonlinear dynamical systems.",
        "ori-fast-z-score": -0.9797958971132713,
        "water-fast-z-score": 6.1034134407836955,
        "rewrite-fast-z-score": 0.1655211777204736
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Electro-optically tunable microring resonators in lithium niobate .\nAbstract:\nWe report on the fabrication and characterization of electro-optically tunable microresonator devices based on proton exchange (PE) grown lithium niobate-on-insulator substrates. The PE process is used to create an optical waveguide with low loss, high index contrast, and large nonlinearity within the substrate material. A ring-resonator geometry is then defined by electron-beam lithography followed by reactive ion etching. Finally, Ti/Au electrodes are deposited onto both sides of the device for electrical tuning. We demonstrate continuous wavelength tuning over more than 30 nm at 1555 nm using only 1 V reverse bias applied across the device. This work represents one step towards realizing electrically-tuned integrated photonic circuits that can be monolithically fabricated on insulators. Lithium niobate has been widely studied as a promising candidate for optoelectronics applications due to its excellent properties such as wide transparency range, large second-order susceptibility, and relatively low propagation losses  1  . In addition, it also exhibits strong piezoelectric and pyroelectric effects which make it possible to achieve efficient electro-optic modulation  2  .\nIn this letter we present our recent results on the development of electro-optically tuned microring resonators made out of lithium niobate. These devices were designed and fabricated on commercially available lithium niobate wafers bonded to silicon dioxide  3  , where the top cladding layer was removed prior to processing. First, a proton-exchange (PE) process  4  was performed to grow a single-mode ridge-waveguide structure inside the bulk LiNbO 3 crystal  5  . Then, a ring-resonator geometry was patterned into the PE-grown region via electron beam lithography  6  . Finally, titanium/gold (Ti/Au) contacts were evaporated onto both sides of the sample to provide electrical access to the device  7, 8  . Figure 1 shows scanning-electron-microscope images of two different types of microring resonators that have been successfully demonstrated so far. Both devices consist of",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Electro - optically tunable microring resonators in lithium niobate . Abstract : We report on the fabrication and identification of electro - optically tunable microresonator devices using on proton exchange ( PE ) grown lithium niobate - on - insulator substrates .The PE method is utilized to create an optical waveguide with little loss , large index contrast , and large nonlinearity within the substrate material . A ring - resonator configuration is then established by electron - laser lithography followed by reactive ion etching .Finally , Ti / Au electrodes are deposited onto both sides of the device for electrical tuning . We display continuous wavelength tuning over more than 30 nm at 1555 nm using only 1 V reverse bias applied across the device .This research provides one step towards developing electrically - tuned integrated photonic circuits that can be monolithically manufactured on insulators . Lithium niobate has been widely explored as a potential candidate for optoelectronics applications due to its exceptional properties such as wide clarity range , large second - order susceptibility , and fairly little propagation losses 1 .In addition , it also exhibits strong piezoelectric and pyroelectric influences which make it able to achieve effective electro - optic modulation 2 . In this letter we present our latest findings on the development of electro - optically tuned microring resonators made out of lithium niobate .These systems were built and manufactured on commercially used lithium niobate wafers bonded to silicon dioxide 3 , where the front cladding surface was eliminated prior to processing . First , a proton - transfer ( PE ) process 4 was done to develop a single - mode ridge - waveguide structure inside the bulk LiNbO 3 crystal 5 .Then , a ring - resonator configuration was patterned into the PE - grown areas via electron beam lithography 6 . Finally , titanium / gold ( Ti / Au ) contacts were evaporated onto both sides of the sample to provide electrical access to the device 7 , 8 .Figure 1 shows scan - electron - microscope photographs of two different kinds of microring resonators that have been successfully shown so far . Both technologies consist of",
        "rewrite_text": "We present a comprehensive study on the development and characterization of electro-optically tunable microring resonators fabricated from lithium niobate on insulator (LNOI) substrates. Utilizing the proton exchange (PE) technique, we successfully created optical waveguides characterized by minimal loss, significant index contrast, and high nonlinearity within the lithium niobate material. The fabrication process involved the establishment of a ring-resonator configuration achieved through electron-beam lithography, followed by reactive ion etching to define the resonator structures. To enable electrical tuning, titanium/gold (Ti/Au) electrodes were deposited on both sides of the devices. Our results demonstrate a continuous wavelength tuning range exceeding 30 nm centered around 1555 nm, achieved with a mere 1 V reverse bias applied across the device. This advancement marks a significant step toward the realization of electrically tunable integrated photonic circuits that can be fabricated monolithically on insulating substrates.\n\nLithium niobate is a material of great interest in optoelectronics due to its remarkable properties, including a broad transparency range, high second-order susceptibility, and low propagation losses. Additionally, its strong piezoelectric and pyroelectric characteristics facilitate effective electro-optic modulation. In this letter, we detail our latest findings regarding the electro-optically tunable microring resonators constructed from lithium niobate. The devices were fabricated on commercially available lithium niobate wafers bonded to silicon dioxide, with the front cladding surface removed prior to processing. The initial step involved a proton exchange process to create a single-mode ridge-waveguide structure within the bulk LiNbO3 crystal. Subsequently, the ring-resonator configuration was patterned into the PE-treated regions using electron beam lithography. Finally, Ti/Au contacts were evaporated onto both sides of the sample to provide the necessary electrical access. Figure 1 illustrates scanning electron microscope images of two distinct types of microring resonators that have been successfully developed to date, showcasing the potential of this technology for future applications in integrated photonics.",
        "ori-fast-z-score": 0.9198662110077999,
        "water-fast-z-score": 7.726752403351793,
        "rewrite-fast-z-score": 1.4021363680319483
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Mesoscopic effective material parameters for thin layers modeled as single and double grids of interacting loaded wires .\nAbstract:\nWe present an approach to calculate the effective material properties of thin layered structures, which are composed by two or more different materials with periodic microstructure. The method is based on homogenization theory combined with finite element analysis (FEA) in order to account for local interactions between neighboring unit cells. We consider three types of unit cell geometries that can be used to model various composite materials such as: wire grid composites, fiber reinforced polymeric matrix composites, and metal foams. In particular we focus our attention on wire grid composites made up of periodically arranged parallel wires embedded into a homogeneous medium. For this type of structure it has been shown experimentally that the macroscopic elastic moduli depend strongly on the volume fraction occupied by the wires. However, these experimental results cannot be explained using classical homogenization theories because they do not take into account the interaction effects among adjacent wires. Therefore, we propose here a new theoretical framework to study the mechanical behavior of wire grid composites at both micro-and meso-scales.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Mesoscopic effective material parameters for thin layers modeled as single and double grids of interacting loaded wires . Abstract : We present an approach to estimate the effective material properties of dense layered systems , which are composed by two or more separate materials with periodic microstructure .The method is based on homogenization theory combined with finite element assessment ( FEA ) in order to account for local interactions between neighboring unit cells . We consider three categories of unit cell geometries that can be used to model numerous composite materials such as : wire grid composites , fiber strengthened polymeric matrix composites , and steel foams .In particular we focus our focus on wire grid composites made up of regularly arranged connected wires embedded into a homogeneous medium . For this form of form it has been shown experimentally that the macroscopic elastic moduli rely highly on the volume fraction occupied by the wires .However , these theoretical results cannot be described using traditional homogenization assumptions because they do not take into consideration the interaction influences among adjoining fibers . Therefore , we propose here a new theoretical framework to study the mechanical behavior of wire grid composites at both micro - and meso - scales .",
        "rewrite_text": "In this article, we introduce a novel methodology for estimating the effective material properties of dense layered systems composed of two or more distinct materials with periodic microstructures. Our approach integrates homogenization theory with finite element analysis (FEA) to effectively capture the local interactions between adjacent unit cells. We categorize the geometries of unit cells into three distinct types, which can be utilized to model a variety of composite materials, including wire grid composites, fiber-reinforced polymeric matrix composites, and steel foams. Our primary focus is on wire grid composites, which consist of regularly arranged interconnected wires embedded within a homogeneous medium. Experimental evidence indicates that the macroscopic elastic moduli of these composites are significantly influenced by the volume fraction of the wires. However, traditional homogenization techniques fall short in accurately describing these theoretical outcomes, as they overlook the interaction effects among neighboring fibers. To address this limitation, we propose a new theoretical framework that facilitates the investigation of the mechanical behavior of wire grid composites at both micro and mesoscopic scales. This framework not only enhances our understanding of the material properties but also provides a more accurate representation of the interactions that occur within the composite structure. By employing this innovative approach, we aim to bridge the gap between theoretical predictions and experimental observations, ultimately contributing to the development of advanced materials with tailored properties for various engineering applications.",
        "ori-fast-z-score": -0.5827715174143585,
        "water-fast-z-score": 4.543661498514618,
        "rewrite-fast-z-score": 1.3867504905630728
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Quantum Markovian Subsystems: Invariance, Attractivity, and Control .\nAbstract:\nWe study the controllability properties of quantum subsystems that are described by a master equation with Lindblad form. We show how to construct invariant sets for such systems in terms of their generators. These results allow us to prove that certain classes of open quantum systems cannot be controlled using only unitary operations on the system s Hilbert space. Finally we present an algorithm which allows one to determine whether or not a given set is invariant under the dynamics generated by a particular generator. This work was supported by NSF grant PHY-0456628. Quantum mechanics has been successfully applied to many physical phenomena ranging from atomic physics to condensed matter theory. However, it remains unclear what exactly constitutes a quantum mechanical description of reality. One approach towards answering this question involves studying the behavior of closed quantum systems whose states evolve according to Schrödinger equations. Another approach focuses on describing the evolution of open quantum systems where the state of the system interacts continuously with its environment. The latter class of problems can often be formulated as questions about the controllability of quantum dynamical systems. For example, consider the problem of steering the state of a two-level atom between different energy levels using laser pulses.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Quantum Markovian Subsystems : Invariance , Attractivity , and Control . Abstract : We research the controllability properties of quantum subsystems that are explained by a master equation with Lindblad form .We see how to build invariant sets for such systems in terms of their generators . These results allow us to prove that particular categories of open quantum systems cannot be governed using only unitary operations on the scheme s Hilbert space .Finally we present an algorithm which allows one to find whether or not a given set is invariant under the dynamics generated by a certain generator . This project was supported by NSF grant PHY - 0456628 .Quantum theory has been successfully application to many physical phenomena ranging from atomic physics to condensed matter theory . However , it remains unsure what actually constitutes a quantum mechanical explanation of reality .One approach towards answering this question involves studying the dynamics of opened quantum systems whose states evolve according to Schrödinger parameters . Another approach focuses on explaining the evolution of close quantum systems where the state of the system interacts continuously with its surroundings .The latter type of question can often be understood as challenges about the controllability of quantum dynamical systems . For instance , consider the question of steering the state of a two - level particle between various energy levels using laser pulses .",
        "rewrite_text": "In this article titled \"Quantum Markovian Subsystems: Invariance, Attractivity, and Control,\" we investigate the controllability characteristics of quantum subsystems described by a master equation in Lindblad form. Our research focuses on the construction of invariant sets for these systems based on their generators, which leads to significant insights regarding the limitations of controlling certain categories of open quantum systems. Specifically, we demonstrate that these systems cannot be effectively managed solely through unitary operations applied to the Hilbert space of the system. \n\nAdditionally, we introduce an algorithm designed to determine whether a specific set remains invariant under the dynamics dictated by a given generator. This work is supported by NSF grant PHY-0456628 and contributes to the broader understanding of quantum theory, which has been successfully applied to a variety of physical phenomena, including atomic physics and condensed matter theory. Despite these advancements, the fundamental nature of quantum mechanical explanations of reality remains an open question.\n\nOne avenue of exploration involves examining the dynamics of open quantum systems, where the states evolve according to Schrödinger parameters. Another perspective emphasizes the evolution of closed quantum systems, where continuous interaction with the environment plays a crucial role. These inquiries often translate into challenges related to the controllability of quantum dynamical systems. For example, we consider the problem of manipulating the state of a two-level particle between different energy levels using laser pulses. Our findings not only enhance the theoretical framework surrounding quantum control but also pave the way for practical applications in quantum technology.",
        "ori-fast-z-score": 0.8392543274162825,
        "water-fast-z-score": 6.313641498019764,
        "rewrite-fast-z-score": 1.9005105362789922
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Haunted Halos of Andromeda and Triangulum: A panorama of galaxy formation in action .\nAbstract:\nWe present the first panoramic view of the distribution of galaxies around two massive clusters at redshifts z = 0.4-0.6, using deep near-infrared imaging with WFC3 on HST. The data reveal an extended population of faint galaxies surrounding each cluster that is not seen in optical images. We find that these galaxies are predominantly blue (with median color u − g = -0.5), have low stellar masses (10^9 Msun/pc^2) and high specific star formation rates (sSFR ~ 10^-2 Gyr-1). These properties suggest they represent a recently assembled population of star-forming dwarf galaxies that were accreted by their host clusters during recent mergers. In addition to this diffuse component we also identify several dozen bright galaxies within 1 Mpc of both clusters which appear to be undergoing rapid bursts of star formation triggered by interactions between infalling gas-rich galaxies and the hot intracluster medium.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Haunted Halos of Andromeda and Triangulum : A panorama of galaxy formation in action . Abstract : We present the first panoramic study of the distribution of clusters around two huge clusters at redshifts z = 0 . 4 - 0 . 6 , using deep near - infrared imaging with WFC3 on HST .The data reveal an extended population of faint clusters surrounding each cluster that is not seen in optical images . We see that these objects are typically blue ( with median color u − k = - 0 . 5 ) , have poor stellar masses ( 10 ^ 9 Msun / pc ^ 2 ) and large particular galaxy formation rates ( sSFR ~ 10 ^ - 2 Gyr - 1 ) .These properties suggest they represent a recently assembled colony of star - creating dwarf stars that were accreted by their host clusters during later mergers . In addition to this diffuse component we also identify several hundred bright stars within 1 Mpc of both clusters which appear to be experiencing rapid bursts of galaxy formation followed by interactions between infalling gas - rich galaxies and the hotter intracluster medium .",
        "rewrite_text": "Title: The Haunted Halos of Andromeda and Triangulum: A Panorama of Galaxy Formation in Action\n\nAbstract: In this groundbreaking study, we present the first comprehensive panoramic analysis of the cluster distribution surrounding two massive clusters at redshifts z = 0.4 - 0.6, utilizing deep near-infrared imaging obtained with the Wide Field Camera 3 (WFC3) on the Hubble Space Telescope (HST). Our observations unveil an extensive population of faint clusters encircling each of the primary clusters, a feature that remains undetected in optical imaging. Notably, these faint clusters exhibit a predominantly blue hue, with a median color of u − k = -0.5, indicating their youthful stellar populations. Additionally, they possess relatively low stellar masses, approximately 10^9 M☉/pc^2, coupled with elevated specific star formation rates (sSFR ~ 10^-2 Gyr^-1). These characteristics imply that these clusters are likely a recently formed assembly of star-forming dwarf galaxies, which have been accreted by their host clusters during subsequent merger events. Beyond this diffuse population, we also identify several hundred luminous stars located within 1 Mpc of both clusters. These stars appear to be undergoing rapid episodes of star formation, likely driven by interactions between infalling gas-rich galaxies and the hotter intracluster medium. Our findings provide critical insights into the ongoing processes of galaxy formation and evolution, highlighting the dynamic interplay between clusters and their surrounding environments in the context of cosmic structure formation. This research not only enhances our understanding of the assembly of galaxy clusters but also sheds light on the broader mechanisms that govern star formation in the universe.",
        "ori-fast-z-score": -0.10976425998969035,
        "water-fast-z-score": 5.158920219515446,
        "rewrite-fast-z-score": 1.0864289525102224
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Outbursts of EX Hydrae Revisited .\nAbstract:\nWe present new photometric and spectroscopic observations of the classical nova EX Hya made in October 2005, when it was still bright (V = 8 mag). The light curve shows that EX Hya is an intermediate polar with a period of P orb = 0.084 d. We find evidence for two accretion regions on the white dwarf surface which are responsible for the double-peaked emission lines observed during outburst. In addition to these features we also detect narrow absorption components at velocities up to -1500 km s-1 . These absorptions may be caused by material ejected during previous eruptions. Our results show that EX Hya has returned to quiescence after its latest eruption in September 2002. Classical novae have been known since antiquity but their underlying physics remains poorly understood. They are believed to result from thermonuclear runaways triggered by unstable nuclear burning on the surfaces of white dwarfs (WD) in close binary systems. However, there remain many open questions about how this process takes place and what happens afterwards. One such question concerns the nature of the WD magnetic field. It is generally accepted that the WD magnetic field plays a key role in determining whether or not a system will undergo a thermonuclear runaway. If the WD magnetic field is too weak then no runaway occurs; if it is strong enough then the WD can become fully convective leading to stable hydrogen burning and hence preventing any further outburst activity. This picture is complicated however by the fact that some WDs do exhibit periodic outbursts despite having fields thought to be too weak to prevent them becoming fully convective. Such objects are called Intermediate Polars (IPs), where the WD magnetic field is sufficiently strong to synchronise the spin periods of both stars but does not disrupt the flow of matter onto the WD.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Outbursts of EX Hydrae Revisited . Abstract : We report new photometric and spectroscopic observations of the classical nova EX Hya made in October 2005 , when it was still bright ( V = 8 mag ) .The light curve shows that EX Hya is an intermediate polar with a period of P orb = 0 . 084 d . We find evidence for two accretion regions on the white dwarf surface which are responsible for the double - peaked emission lines observed during outburst . In addition to these features we also detect narrow absorption components at velocities up to - 1500 km s - 1 .These absorptions might be caused by material expelled during earlier eruptions . Our results show that EX Hya has returned to quiescence after its latest eruption in September 2002 .Classical novae have been known since ancient but their core science appears poorly known . They are said to come from thermonuclear runaways caused by volatile nuclear burning on the surfaces of white dwarfs ( WD ) in close binary systems .However , there remain many open questions about how this process happens occur and what comes afterwards . One such issue concerns the nature of the WD magnetic force .It is usually agreed that the WD magnetic force plays a key importance in determining whether or not a system will undergo a thermonuclear runaway . If the WD magnetic force is too weak then no runaway begins ; if it is strong enough then the WD can develop fully convective causing to stable fuel burning and hence stopping any further outburst activity .This picture is complicated however by the fact that some WDs do exhibit periodic outbursts despite having fields thought to be too weak to prohibit them becoming fully convective . Such objects are called Intermediate Polars ( IPs ) , where the WD magnetic force is sufficiently strong to synchronise the spin times of both stars but does not disrupt the flow of matter onto the WD .",
        "rewrite_text": "We present new photometric and spectroscopic observations of the classical nova EX Hydrae, conducted in October 2005, during a period of brightness at V = 8 mag. Our analysis reveals that EX Hya is classified as an intermediate polar, exhibiting an orbital period of P_orb = 0.084 days. The light curve indicates the presence of two distinct accretion regions on the surface of the white dwarf, which are responsible for the double-peaked emission lines observed during its outbursts. Additionally, we have identified narrow absorption features with velocities reaching up to -1500 km/s, likely resulting from material expelled during previous eruptions. Our findings confirm that EX Hya has returned to a quiescent state following its last eruption in September 2002.\n\nClassical novae have been recognized since antiquity, yet their underlying mechanisms remain inadequately understood. These phenomena are believed to arise from thermonuclear runaways due to unstable nuclear burning on the surfaces of white dwarfs (WDs) in close binary systems. However, numerous questions persist regarding the initiation of this process and the subsequent outcomes. A significant area of inquiry pertains to the role of the WD's magnetic field. It is widely accepted that the strength of the WD's magnetic field is crucial in determining whether a thermonuclear runaway will occur. If the magnetic field is too weak, the runaway does not initiate; conversely, if it is sufficiently strong, the WD may develop a fully convective state, leading to stable fuel burning and preventing further outburst activity. This understanding is complicated by the existence of certain WDs that exhibit periodic outbursts despite possessing magnetic fields that are considered too weak to prevent full convection. These objects are categorized as Intermediate Polars (IPs), where the magnetic field strength is adequate to synchronize the spin periods of both stars while still allowing for the uninterrupted accretion of matter onto the white dwarf.",
        "ori-fast-z-score": -1.7407765595569784,
        "water-fast-z-score": 4.905778905196062,
        "rewrite-fast-z-score": 0.08606629658238704
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  8.4GHz VLBI observations of SN2004et in NGC6946 .\nAbstract:\nWe present 8.4 GHz Very Long Baseline Interferometry (VLBI) images and light curves for the supernova remnant (SNR) associated with the Type IIb supernova SN2004et, which exploded in the nearby spiral galaxy NGC 6946 on 2004 September 24 UT1. The radio emission is dominated by two bright components separated by ~0.5 arcsec at all epochs observed between 2005 January and 2007 December. We find that both components are expanding with velocities of ~5000 km/s, consistent with previous estimates based on single-dish data. However, we also detect significant proper motions of ~1000 km/s for each component over this period. These results suggest an age of about 3 years for the SNR, implying a distance to NGC 6946 of 4 Mpc. This value is significantly smaller than previously estimated distances to this object using other methods. Our measurements provide new constraints on models of core-collapse supernovae. \n \n Keywords: Supernova remnants",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : 8 . 4GHz VLBI discoveries of SN2004et in NGC6946 . Abstract : We create 8 . 4 GHz Very Long Baseline Interferometry ( VLBI ) images and light angles for the supernova remnant ( SNR ) associated with the Type IIb supernova SN2004et , which exploded in the nearby spiral galaxy NGC 6946 on 2004 September 24 UT1 .The signal radiation is dominated by two faint components differentiated by ~ 0 . 5 arcsec at all epochs observed between 2005 January and 2007 December . We see that both components are growing with velocities of ~ 5000 kilometres / s , compatible with previous estimates based on single - dish data .However , we also observe significant proper moves of ~ 1000 km / s for each system over this time . These data suggest an age of about 3 years for the SNR , suggests a distance to NGC 6946 of 4 Mpc .This value is significantly less than previously estimated altitudes to this body using other methods . Our measurements give novel constraints on estimates of core - collapse supernovae .Keywords: Supernova remnants",
        "rewrite_text": "Title: 8.4 GHz VLBI Discoveries of SN2004et in NGC 6946\n\nAbstract: In this study, we present 8.4 GHz Very Long Baseline Interferometry (VLBI) imaging and analysis of the supernova remnant (SNR) associated with the Type IIb supernova SN2004et, which erupted in the nearby spiral galaxy NGC 6946 on September 24, 2004 (UT). Our observations reveal two faint components that are spatially separated by approximately 0.5 arcseconds, consistently detected across multiple epochs from January 2005 to December 2007. Both components exhibit expansion velocities of around 5000 kilometers per second, aligning with previous estimates derived from single-dish observations. Notably, we also recorded significant proper motions of approximately 1000 kilometers per second for each component during the observation period. These findings imply that the age of the SNR is roughly three years, and we estimate the distance to NGC 6946 to be about 4 megaparsecs. This distance measurement is considerably lower than earlier estimates obtained through alternative methodologies. Our results provide new insights and constraints on the characteristics of core-collapse supernovae, enhancing our understanding of their evolution and the dynamics of their remnants. The implications of these findings are significant for the study of supernova remnants and their role in the broader context of stellar evolution and galactic dynamics. \n\nKeywords: Supernova remnants, VLBI, SN2004et, NGC 6946, core-collapse supernovae.",
        "ori-fast-z-score": -1.2309149097933272,
        "water-fast-z-score": 5.093248125762992,
        "rewrite-fast-z-score": 1.6464638998453551
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Functional Methods in the Generalized Dicke Model .\nAbstract:\nWe study the generalized Dicke model with an arbitrary number N of two-level atoms interacting with one-mode radiation field, and show that it can be mapped to a spin-1/2 system by using the Holstein-Primakoff transformation. We then use the exact diagonalization method to calculate its ground state energy spectrum for different values of the coupling constant g and the number N . The results are compared with those obtained by other methods such as perturbation theory and numerical integration. It is found that our results agree well with previous ones when the coupling strength is small but deviate significantly from them if the coupling becomes strong. Finally we discuss some possible applications of this work. PACS: 03.65.Ud, 05.45.Mt, 11.10.Gh, 12.20.Dc, 13.25.Gv \nI. INTRODUCTIO N\nThe Dicke model  1  describes how many identical two-level atoms interact collectively with a single mode of electromagnetic field. In recent years there has been renewed interest in studying this model because of its potential application in quantum information processing  2  , quantum optics  3  , condensed matter physics  4  , etc.. For example, the collective spontaneous emission rate of the atomic ensemble depends on the total angular momentum J = N /2 (N being the number of atoms)  5  .\nIn fact, the Dicke model was originally proposed more than half century ago  6  . Since then various theoretical approaches have been developed to solve it  7 -10  . Among these approaches, the most successful one is probably the so-called HolsteinPrimakoff transformation  11  which maps the original problem into a spin-1/2 system  12  . This approach works very well at weak-coupling regime where the interaction between atom-field is relatively small. However, it fails completely at large-coupling limit since the mapping procedure breaks down due to the appearance of unphysical states  13  . Recently, several authors  14 -19  have tried to overcome this difficulty by introducing new transformations or approximations. Nevertheless, their solutions still suffer from certain drawbacks  20, 21  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Functional Methods in the Generalized Dicke Model . Abstract : We explore the generalized Dicke model with an arbitrary number N of two - level atoms interacting with one - mode radiation field , and find that it can be mapped to a spin - 1 / 2 system by using the Holstein - Primakoff transformation .We then use the exact diagonalization technique to estimate its ground state energy spectrum for different values of the interaction function g and the number N . The results are compared with those achieved by other methods such as perturbation theory and mathematical integration .It is found that our findings agree well with previous ones when the interaction strength is tiny but deviate drastically from them if the interaction gets powerful . Finally we explain some possible users of this study .PACS : 03 . 65 . Ud , 05 . 45 . Mt , 11 . 10 . Gh , 12 . 20 . Dc , 13 . 25 . Gv I . INTRODUCTIO N The Dicke model 1 explains how many identical two - level atoms behave collectively with a single mode of electromagnetic field . In recent months there has been continued interest in understanding this model because of its potential application in quantum information processing 2 , quantum optics 3 , condensed matter science 4 , etc . . For instance , the collective spontaneous emission speed of the atomic ensemble depends on the total angular velocity J = N / 2 ( N being the number of atoms ) 5 .In reality , the Dicke approach was originally proposed more than quarter century ago 6 . Since then various theoretical methods have been constructed to solve it 7 - 10 .Among these method , the most popular one is probably the so - called HolsteinPrimakoff transformation 11 which maps the original problem into a spin - 1 / 2 system 12 . This method works very best at weak - interaction regime where the interaction between particle - field is fairly little .However , it fails totally at large - coupling limit since the mapping method splits down due to the appearance of unphysical states 13 . Recently , various literature 14 - 19 have tried to overcome this challenge by using new transformations or approximations .Nevertheless, their solutions still suffer from certain drawbacks  20, 21  .",
        "rewrite_text": "**Title:** Functional Methods in the Generalized Dicke Model\n\n**Abstract:** In this study, we investigate the generalized Dicke model, which involves an arbitrary number \\( N \\) of two-level atoms interacting with a single mode of radiation. By employing the Holstein-Primakoff transformation, we demonstrate that this model can be effectively mapped to a spin-1/2 system. To analyze the ground state energy spectrum across various interaction strengths \\( g \\) and atom counts \\( N \\), we utilize the exact diagonalization technique. Our findings are juxtaposed with results obtained through alternative approaches, including perturbation theory and mathematical integration. We observe that our results align closely with previous studies in the regime of weak interactions; however, significant discrepancies arise as the interaction strength increases. This divergence highlights the limitations of traditional methods in capturing the dynamics of the system under strong coupling conditions. Furthermore, we discuss potential applications of our research, particularly in the realms of quantum information processing, quantum optics, and condensed matter physics, where understanding the collective behavior of atomic ensembles is crucial. The Dicke model, first proposed over twenty-five years ago, continues to garner attention due to its implications for collective phenomena, such as the enhanced rates of spontaneous emission dependent on the total angular momentum \\( J = N/2 \\). While the Holstein-Primakoff transformation remains a widely used technique for weak interactions, it encounters significant challenges in the strong coupling regime, leading to the emergence of unphysical states. Recent literature has sought to address these issues through novel transformations and approximations, yet these solutions still exhibit certain limitations. Our work aims to contribute to this ongoing discourse by providing a comprehensive analysis of the generalized Dicke model and its implications for future research in quantum mechanics. \n\n**PACS:** 03.65.Ud, 05.45.Mt, 11.10.Gh, 12.20.Dc, 13.25.Gv",
        "ori-fast-z-score": -0.5551361100027009,
        "water-fast-z-score": 7.171804696775816,
        "rewrite-fast-z-score": 0.16329931618554522
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Bayesian reconstruction of the cosmological large-scale structure: methodology, inverse algorithms and numerical optimization .\nAbstract:\nWe present an overview of our recent work on Bayesian methods for reconstructing the large scale structures in the universe using galaxy redshift surveys. We discuss how to formulate this problem as a statistical inference task with priors that encode physical information about the underlying matter distribution. The posterior probability density function is then evaluated by applying Bayes  theorem together with Markov Chain Monte Carlo (MCMC) sampling techniques. In particular we focus on two different approaches which are based either on Gibbs sampling or Metropolis-Hastings algorithm. Finally we describe some applications of these methods to simulated data sets. This research was supported by NSF grant AST-0707763. Cosmology has been revolutionized over the past decade by precision measurements of the cosmic microwave background anisotropies made by WMAP  1  , PLANCK  2  and other experiments  3  . These observations have provided strong evidence for the existence of dark energy  4  and have led to tight constraints on many parameters describing the physics of the early universe  5  .\nHowever, despite their successes there remain several open questions regarding fundamental aspects of the standard model of cosmology  6  . One such question concerns the nature of dark matter  7, 8  : what is its particle content? What is its mass? How does it interact with ordinary matter?\nAnswering these questions requires detailed knowledge of the spatial distribution of dark matter throughout space and time  9  . Unfortunately direct detection experiments  10  cannot provide this information because they only measure the gravitational effects of dark matter particles  11  . Instead one must rely on indirect probes like galaxy clustering  12  , weak lensing  13  and 21 cm emission  14  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Bayesian reconstruction of the cosmological big - scale system : methodology , inverse algorithms and mathematical optimization . Abstract : We present an overview of our latest work on Bayesian methods for reconstructing the huge scale structures in the universe using galaxy redshift surveys .We discuss how to formulate this question as a statistical inference job with priors that encode physical information about the underlying matter distribution . The posterior likelihood density function is then evaluated by using Bayes theorem together with Markov Chain Monte Carlo ( MCMC ) filtering algorithms .In particular we focus on two different methods which are based either on Gibbs filtering or Metropolis - Hastings algorithm . Finally we explain some applications of these algorithms to modeled information sets .This research was supported by NSF grant AST - 0707763 . Cosmology has been revolutionized over the previous decade by precision observations of the cosmic microwave background anisotropies made by WMAP 1 , PLANCK 2 and other experiments 3 .These measurements have provided strong evidence for the existence of deep energy 4 and have led to strict constraints on numerous variables describing the physics of the early world 5 . However , despite their successes there remain many open questions regarding essential aspects of the standard theory of cosmology 6 .One such problem concerns the nature of dark matter 7 , 8 : what is its particle content ? What is its weight ?How does it behave with everyday matter ? Answering these problems involves detailed knowledge of the spatial distribution of dark matter throughout space and time 9 .Unfortunately direct detection experiments 10 cannot offer this data because they only measure the gravitational impacts of dark matter waves 11 . Instead one must rely on indirect probes like galaxy clustering 12 , soft lensing 13 and 21 cm emission 14 .",
        "rewrite_text": "**Title:** Bayesian Reconstruction of the Cosmological Large-Scale Structure: Methodology, Inverse Algorithms, and Mathematical Optimization\n\n**Abstract:** This article provides a comprehensive overview of our recent advancements in employing Bayesian techniques for the reconstruction of large-scale structures in the universe, utilizing data from galaxy redshift surveys. We frame the reconstruction challenge as a statistical inference problem, incorporating prior distributions that encapsulate essential physical insights regarding the underlying matter distribution. The posterior likelihood density function is computed through the application of Bayes' theorem in conjunction with Markov Chain Monte Carlo (MCMC) filtering algorithms. Our focus is particularly directed towards two distinct methodologies: one based on Gibbs filtering and the other on the Metropolis-Hastings algorithm. We also explore practical applications of these algorithms to simulated datasets. This research has been supported by NSF grant AST-0707763. \n\nThe field of cosmology has undergone significant transformation in the past decade, largely due to precise measurements of cosmic microwave background anisotropies conducted by missions such as WMAP and PLANCK, among others. These observations have provided compelling evidence for the presence of dark energy and have imposed stringent constraints on various parameters that describe the physics of the early universe. Despite these advancements, numerous critical questions remain unresolved within the framework of standard cosmological theory. A prominent issue pertains to the nature of dark matter, including inquiries into its particle composition, mass, and interactions with ordinary matter. Addressing these questions necessitates a thorough understanding of the spatial distribution of dark matter across both space and time. Unfortunately, direct detection experiments are limited in their ability to provide this information, as they primarily measure the gravitational effects of dark matter. Consequently, researchers must rely on indirect observational techniques, such as galaxy clustering, weak gravitational lensing, and 21 cm emission, to glean insights into the elusive nature of dark matter.",
        "ori-fast-z-score": 0.5853694070049635,
        "water-fast-z-score": 7.666666666666667,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Diffuse HI Disks in Isolated Galaxies .\nAbstract:\nWe present new observations and analysis of the neutral hydrogen (HI) disks surrounding isolated galaxies, using data obtained with the Very Large Array (VLA). We have observed 12 nearby galaxies at 21 cm wavelength to determine their total HI mass and distribution within the optical disk. The sample includes both late-type spirals and dwarf irregulars. Our results show that all but one galaxy has an extended HI halo beyond its optical radius; this is true even for the most gas-rich systems such as NGC 4254 and NGC 5253. In addition, we find evidence for significant amounts of atomic gas outside our detection limits which may be associated with tidal features or other interactions between these galaxies and neighboring companions. These findings are consistent with previous studies showing that many isolated spiral galaxies contain large quantities of cold interstellar medium distributed over several kiloparsecs. \n \n Keywords: Atomic Hydrogen, Galaxy Evolution, Spiral Structure",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Diffuse HI Disks in Isolated Galaxies . Abstract : We report new studies and investigation of the neutral hydrogen ( HI ) disks surrounding isolated stars , using data acquired with the Very Large Array ( VLA ) .We have noted 12 nearby galaxies at 21 cm wavelength to estimate their total HI mass and distribution within the optical disk . The sample comprises both late - class spirals and dwarf irregulars .Our results show that all but one star has an extended HI halo beyond its optical diameter ; this is true even for the most gas - rich systems such as NGC 4254 and NGC 5253 . In addition , we find proof for significant amounts of nuclear gas outside our detection limits which may be involved with tidal features or other relationships between these galaxies and surrounding companions .These conclusions are consistent with previous research indicating that several isolated spiral nuclei contain significant amounts of cold interstellar medium spread over several kiloparsecs . Keywords : Atomic Hydrogen , Galaxy Evolution , Spiral Structure",
        "rewrite_text": "We present a comprehensive study of neutral hydrogen (HI) disks in isolated galaxies, utilizing data obtained from the Very Large Array (VLA). Our investigation focuses on 12 nearby galaxies observed at a wavelength of 21 cm, allowing us to assess their total HI mass and its distribution relative to the optical disk. The selected sample includes a mix of late-type spiral galaxies and dwarf irregulars. Our findings reveal that nearly all the galaxies in our sample, with the exception of one, possess extended HI halos that extend beyond their optical boundaries. This phenomenon is particularly evident in gas-rich galaxies such as NGC 4254 and NGC 5253, which exhibit substantial HI envelopes. Furthermore, we have identified evidence suggesting the presence of significant amounts of nuclear gas that fall outside our detection thresholds. This gas may be associated with tidal features or interactions with nearby companion galaxies. These observations align with prior studies that have indicated the existence of considerable amounts of cold interstellar medium in isolated spiral nuclei, often distributed over several kiloparsecs. Our research contributes to the understanding of galaxy evolution and the role of atomic hydrogen in shaping the structure of galaxies, particularly in isolated environments. The implications of these findings are significant for the broader field of astrophysics, as they provide insights into the dynamics of galaxy formation and the distribution of interstellar matter. Keywords associated with this study include Atomic Hydrogen, Galaxy Evolution, and Spiral Structure.",
        "ori-fast-z-score": -0.8944271909999159,
        "water-fast-z-score": 5.0,
        "rewrite-fast-z-score": 1.8411492357966468
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Star-forming Region NGC 346 in the Small Magellanic Cloud with Hubble Space Telescope ACS Observations. II. Photometric Study of the Intermediate-Age Star Cluster BS 90 .\nAbstract:\nWe present photometry for stars in an intermediate-age open cluster, BS90 (Sharpless 1959), located at the center of the star-forming region NGC346 in the Small Magellan Cloud (SMC). The data were obtained using the Advanced Camera for Surveys on board the Hubble Space Telescope and cover a field-of-view of about 1 arcmin2 centered on the cluster. We have detected more than 1000 candidate members down to V = 25 mag within a radius of 3 arcmin around the cluster center. Using these new observations we derived the age, distance modulus, reddening, metallicity, mass function slope, and total luminosity of this open cluster. Our results are consistent with previous studies but provide higher precision values due to our larger sample size. \n \n Keywords: Open clusters; SMC; HST/ACS; photometry; star formation history; intermediate-age",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Star - creating Region NGC 346 in the Small Magellanic Cloud with Hubble Space Telescope ACS Observations . II .Photometric Study of the Intermediate - Age Star Cluster BS 90 . Abstract : We report photometry for stars in an intermediate - age open cluster , BS90 ( Sharpless 1959 ) , located at the center of the star - creating area NGC346 in the Small Magellan Cloud ( SMC ) .The data were obtained using the Advanced Camera for Surveys on board the Hubble Space Telescope and cover a field - of - view of about 1 arcmin2 centered on the cluster . We have discovered more than 1000 candidate members down to V = 25 mag within a diameter of 3 arcmin around the cluster center .Using these new experiments we derived the age , distance modulus , reddening , metallicity , mass function slope , and overall luminosity of this open cluster . Our results are compatible with previous research but give larger precision values due to our larger specimen size .Keywords : Open clusters ; SMC ; HST / ACS ; photometry ; star formation history ; intermediate - age",
        "rewrite_text": "We present a detailed photometric analysis of the intermediate-age open cluster BS90 (Sharpless 1959), situated at the heart of the star-forming region NGC 346 in the Small Magellanic Cloud (SMC). Utilizing the Advanced Camera for Surveys (ACS) aboard the Hubble Space Telescope, we collected data over a field of view of approximately 1 square arcminute, centered on the cluster. Our observations led to the identification of over 1,000 candidate member stars down to a magnitude of V = 25, within a 3 arcminute radius from the cluster's center. This extensive dataset allowed us to derive key astrophysical parameters for BS90, including its age, distance modulus, reddening, metallicity, mass function slope, and overall luminosity. Our findings align with previous studies but offer enhanced precision due to the larger sample size we analyzed. This research contributes to a deeper understanding of the star formation history in the SMC and the characteristics of intermediate-age star clusters. The results underscore the significance of BS90 within the broader context of stellar evolution and cluster dynamics in the Small Magellanic Cloud. The keywords associated with this study include open clusters, SMC, HST/ACS, photometry, star formation history, and intermediate-age stars.",
        "ori-fast-z-score": -0.762000762001143,
        "water-fast-z-score": 2.75,
        "rewrite-fast-z-score": 1.7457431218879391
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Supersymmetric Parameter Space in Light of B-physics Observables and Electroweak Precision Data .\nAbstract:\nWe present the results of our analysis on the supersymmetric parameter space, taking into account all available experimental data including those from LHC experiments as well as electroweak precision observables (EWPO). We find that there is no significant improvement over previous analyses when we include EWPOs with their full correlations taken properly into account. However, if one considers only the subset of EWPOs which are not strongly correlated to each other, then some improvements can be seen for certain regions of the parameter space. In particular, this applies to scenarios where the lightest neutralino has a large Higgsino component or where the gluinos have masses around 1 TeV. The latter case also leads to an improved agreement between theory predictions and measurements of the anomalous magnetic moment of the muon. Finally, we discuss how these findings affect the prospects for discovering supersymmetry at future colliders such as the International Linear Collider.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Supersymmetric Parameter Space in Light of B - physics Observables and Electroweak Precision Data . Abstract : We present the results of our analysis on the supersymmetric parameter space , using into consideration all available theoretical data including those from LHC observations as well as electroweak accuracy observables ( EWPO ) .We see that there is no considerable progress over past analyses when we include EWPOs with their full correlations took correctly into consideration . However , if one looks only the subset of EWPOs which are not closely correlated to each other , then some improvements can be shown for particular regions of the parameter room .In particular , this applicable to scenarios where the lightest neutralino has a large Higgsino component or where the gluinos have masses around 1 TeV . The last example also leads to an better agreement between theoretical estimates and measurements of the anomalous magnetic moment of the muon .Finally , we explain how these results affect the possibilities for finding supersymmetry at potential colliders such as the International Linear Collider .",
        "rewrite_text": "In this article, we explore the supersymmetric parameter space by analyzing a comprehensive set of theoretical data, which includes recent findings from the Large Hadron Collider (LHC) and electroweak precision observables (EWPO). Our investigation reveals that incorporating EWPOs with their full correlations does not yield significant advancements compared to previous analyses. However, when we focus on a specific subset of EWPOs that exhibit minimal correlation with one another, we observe notable improvements in certain regions of the parameter space. This is particularly evident in scenarios where the lightest neutralino possesses a substantial Higgsino component, as well as in cases where gluinos have masses around 1 TeV. The latter scenario also enhances the alignment between theoretical predictions and experimental measurements of the anomalous magnetic moment of the muon, suggesting a more coherent understanding of these phenomena. Furthermore, we discuss the implications of our findings for the search for supersymmetry at future colliders, such as the International Linear Collider. Our results indicate that while the overall supersymmetric parameter space remains challenging to constrain, targeted analyses of less correlated EWPOs can provide valuable insights and potentially guide experimental efforts in the quest for supersymmetry. This work underscores the importance of refining our approach to analyzing supersymmetric models in light of both theoretical and experimental data, paving the way for future discoveries in high-energy physics.",
        "ori-fast-z-score": -3.6927447293799815,
        "water-fast-z-score": 4.275930552470682,
        "rewrite-fast-z-score": 1.212183053462653
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  El universo de Einstein: 1905 - annus mirabilis - 2005 .\nAbstract:\nEl año que cambió la historia del Universo (2005) es un documental sobre el descubrimiento por parte de Albert Einstein, en 1915, de las ecuaciones básicas de su teoría general relativista y los acontecimientos que tuvieron lugar durante ese mismo año. El film se centra especialmente en dos hechos extraordinarios ocurridos en 1905: la publicación de cinco artículos científicos revolucionarios escritos por Einstein entre marzo y diciembre; y el nacimiento de su hija mayor, Margot, en mayo. La película está narrada por el actor británico David Threlfall e incluye entrevistas con expertos mundiales como Sean Carroll o Brian Cox. ... ​ \n\n\nLa teoría general de la relatividad fue publicada por primera vez en 1915 pero sus fundamentos fueron establecidos ya en 1905 cuando Einstein publicó una serie de trabajos revolucionarios que cambiaron nuestra comprensión del Universo. Este documental cuenta cómo ocurrió todo eso centrándose en dos hechos extraordinarios sucedidos aquel año: la publicación de cinco trabajos científicos revolucionarios escritos por él entre marzo y diciembre; así como el nacimiento de su hija Margot en mayo.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : El universo de Einstein : 1905 - annus mirabilis - 2005 . Abstract : El año que cambió la historia del Universo ( 2005 ) es un documental sobre al descubrimiento por parte de Albert Einstein , en 1915 , de las ecuaciones básicas de su teoría general relativista y los acontecimientos para tuvieron lugar durante ese mismo año .El movie se centra especialmente en dos hechos extraordinarios ocurridos en 1905 : la publicación de cinco artículos científicos revolucionarios escritos por Einstein entre marzo y diciembre ; y el nacimiento de su hija mayor , Margot , es mayo . La película está narrada por el acting británico David Threlfall e incluye entrevistas en expertos mundiales como Sean Carroll o Brian Cox .. . . La teoría general de la relatividad fue publicada por primera vez en 1915 pero sus fundamentos fueron establecidos yo en 1905 cuando Einstein publicó una serie de trabajos revolucionarios que cambiaron nuestra comprensión del Universo . Este documental cuenta cómo ocurrió todo eso centrándose en dos hechos extraordinarios sucedidos aquel año : la publicación de cinco trabajos científicos revolucionarios escritos por él entre marzo y diciembre ; así como el nacimiento de su hija Margot en mayo .",
        "rewrite_text": "Title: The Universe of Einstein: 1905 - Annus Mirabilis - 2005\n\nAbstract: The year that transformed our understanding of the universe, 2005, marks the release of a documentary that explores Albert Einstein's groundbreaking discoveries, particularly his formulation of the fundamental equations of general relativity in 1915. This film delves into the remarkable events of 1905, a pivotal year in Einstein's life, highlighting two significant milestones: the publication of five revolutionary scientific papers between March and December, and the birth of his eldest daughter, Margot, in May. Narrated by British actor David Threlfall, the documentary features insights from leading experts in the field, including renowned physicists Sean Carroll and Brian Cox. \n\nThe general theory of relativity, which was first published in 1915, has its roots in the groundbreaking work Einstein accomplished a decade earlier. The documentary illustrates how these transformative ideas emerged, focusing on the extraordinary achievements of 1905 that laid the groundwork for modern physics. It captures the essence of Einstein's intellectual journey, showcasing the interplay between his personal life and scientific breakthroughs. By intertwining historical context with personal anecdotes, the film provides a comprehensive view of how Einstein's work not only revolutionized our understanding of space and time but also shaped the course of scientific inquiry for generations to come. Through expert commentary and engaging storytelling, the documentary invites viewers to appreciate the profound impact of Einstein's contributions to the field of physics and the legacy that continues to influence our comprehension of the universe today.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 2.8735244660769563,
        "rewrite-fast-z-score": 0.3922322702763681
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Swift/XRT observes the fifth outburst of the periodic Supergiant Fast X-ray Transient IGR J11215-5952 .\nAbstract:\nWe report on Swift observations of the fifth outburst of an unusual transient, which we call IGR J112155952 (IGR 11215). The source was discovered by INTEGRAL in 2003 and has been observed to undergo regular outbursts every ~150 days since then. We find that this is also true for its most recent outburst; it reached peak luminosity Lx = 1.1 x 1036 erg s-1 at T0 = MJD 54656.6 ± 0.2 (2010 September 16) with a duration of about 20 days. During each outburst, the spectrum softens as the flux decreases. This behaviour suggests that the accretion rate onto the compact object decreases during the decay phase of each outburst. In addition, there are hints of spectral changes between different outbursts; these may be related to variations in the mass transfer rate or the geometry of the system. \n \n Keywords: Supernova remnant",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Swift / XRT observes the fifth outburst of the periodic Supergiant Fast X - ray Transient IGR J11215 - 5952 . Abstract : We report on Swift observations of the fifth outburst of an unusual transient , which we call IGR J112155952 ( IGR 11215 ) .The source was discovered by INTEGRAL in 2003 and has been observed to undergo normal outbursts every ~ 150 weeks since then . We see that this is also true for its most current outburst ; it achieved peak luminosity Lx = 1 . 1 x 1036 erg s - 1 at T0 = MJD 54656 . 6 ± 0 . 2 ( 2010 September 16 ) with a duration of about 20 days .During each outburst , the spectrum softens as the flux drops . This behaviour suggests that the accretion rate onto the compact body drops during the decay phase of each outburst .In addition , there are hints of spectral changes between various outbursts ; these may be connected to variations in the mass transfer frequency or the topology of the system . Keywords : Supernova remnant",
        "rewrite_text": "Title: Swift/XRT Observations of the Fifth Outburst of the Periodic Supergiant Fast X-ray Transient IGR J11215-5952\n\nAbstract: In this study, we present the findings from Swift observations of the fifth outburst of the intriguing transient source IGR J11215-5952, which was first identified by the INTEGRAL satellite in 2003. Since its discovery, IGR J11215 has exhibited a pattern of periodic outbursts approximately every 150 weeks. Our latest observations confirm this periodicity, with the most recent outburst reaching a peak luminosity of Lx = 1.1 x 10^36 erg s^-1 at T0 = MJD 54656.6 ± 0.2, corresponding to September 16, 2010. This outburst lasted for about 20 days. Notably, we observe a consistent trend where the spectrum softens as the flux decreases during each outburst, indicating a reduction in the accretion rate onto the compact object during the decay phase. Furthermore, we have detected potential spectral variations between different outbursts, which may be indicative of changes in the mass transfer frequency or the overall system dynamics. These observations contribute to our understanding of the behavior of supergiant fast X-ray transients and their underlying mechanisms. Our findings underscore the importance of continuous monitoring of such transient sources to unravel the complexities of their outburst phenomena and the physical processes at play. \n\nKeywords: Supernova remnant, X-ray transient, IGR J11215-5952, accretion, spectral analysis.",
        "ori-fast-z-score": 0.2672612419124244,
        "water-fast-z-score": 3.5762373640756184,
        "rewrite-fast-z-score": 1.6059101370939322
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The white dwarf luminosity function --II. The effect of the measurement errors and other biases .\nAbstract:\nWe have studied in detail how to correct for various observational effects on the determination of the white dwarf luminosity function (WDLF). We find that the WDLF is affected by several factors, such as photometric calibration error, incompleteness due to detection limit, contamination by unresolved binaries, etc.. In order to obtain an unbiased estimate of the true WDLF we need to take into account these effects properly.  By using Monte Carlo simulations with artificial data sets, we show that our method can recover the input WDLF very well even when there are large uncertainties in the observed magnitudes or colors. Our results also suggest that it may be difficult to determine the absolute normalization of the WDLF accurately because of systematic uncertainty associated with the distance scale. Finally, we apply this method to the recent observations made by the Sloan Digital Sky Survey (SDSS) Data Release 1 (DR1), which covers about one quarter of the sky.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The white dwarf luminosity function - - II . The impact of the observation failures and other biases .Abstract : We have researched in detail how to correct for various observational consequences on the determination of the white dwarf luminosity function ( WDLF ) . We see that the WDLF is affected by many processes , such as photometric calibration error , incompleteness due to detection limit , contamination by unresolved binaries , etc . .In order to obtain an unbiased estimate of the true WDLF we must to take into consideration these consequences properly . By using Monte Carlo simulations with artificial data sets , we prove that our technique can regain the input WDLF very best even when there are big uncertainties in the seen magnitudes or colors .Our results also suggest that it could be harder to predict the absolute normalization of the WDLF accurately because of systematic uncertainty associated with the distance scale . Finally , we apply this method to the recent observations made by the Sloan Digital Sky Survey ( SDSS ) Data Release 1 ( DR1 ) , which covers about one quarter of the heavens .",
        "rewrite_text": "Title: The White Dwarf Luminosity Function - II: The Impact of Observation Failures and Other Biases\n\nAbstract: In this study, we conduct an in-depth analysis of the various observational factors that influence the determination of the white dwarf luminosity function (WDLF). Our investigation reveals that the WDLF is susceptible to a range of biases, including errors in photometric calibration, limitations in detection capabilities, and contamination from unresolved binary systems. To derive an accurate and unbiased estimate of the true WDLF, it is essential to account for these observational challenges comprehensively. We employ Monte Carlo simulations utilizing artificial datasets to demonstrate the efficacy of our correction techniques, showing that we can accurately recover the original WDLF even in the presence of significant uncertainties in observed magnitudes and colors. Furthermore, our findings indicate that achieving precise predictions for the absolute normalization of the WDLF may be complicated due to systematic uncertainties related to the distance scale. To validate our methodology, we apply it to recent data from the Sloan Digital Sky Survey (SDSS) Data Release 1 (DR1), which encompasses approximately one-quarter of the sky. This application not only illustrates the robustness of our approach but also contributes to a more refined understanding of the WDLF in the context of contemporary astronomical observations.",
        "ori-fast-z-score": 0.4588314677411235,
        "water-fast-z-score": 5.8119893994415355,
        "rewrite-fast-z-score": 1.2375966910186262
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Class of LDPC Erasure Distributions with Closed-Form Threshold Expression .\nAbstract:\nWe propose an erasure distribution for low-density parity-check (LDPC) codes that has closed-form threshold expression and is optimal in the sense that it minimizes the gap between its threshold value and Shannon limit on the binary symmetric channel (BSC). The proposed distribution can be viewed as a generalization of the optimized irregular repeat accumulate (IRA) code ensemble, which was recently introduced by Tanner et al.. We show that our new distribution achieves better performance than IRA over BSCs with small crossover probabilities. Finally, we present simulation results to demonstrate the effectiveness of the proposed distribution under practical conditions. Index Terms-Low density parity check (LDPC), Binary Symmetric Channel (BSC), Optimized Irregular Repeat Accumulate Code Ensemble (OIRA), Gap-to-Shannon Limit (GTSL)\nI. INTRODUCTIO N Low-Density Parity Check (LDPC) codes are linear block codes defined by sparse parity-check matrices  1  . They have been shown to perform close to capacity when decoded using iterative message-passing algorithms such as belief propagation  2  , and they are widely used in many applications including digital communications  3  -  5  .\nThe design of good LDPC ensembles remains one of the most important problems in coding theory  6  . In particular, there exists a large body of research devoted to finding distributions that minimize the gap between their threshold values and Shannon limits  7  -  11  . However, these works mainly focus on regular or quasi-cyclic LDPC codes  12  , while irregular LDPC codes are more commonly used due to their flexibility  13  . Recently, Tanner et al.  14  presented an optimized irregular repeat accumulate (OIRA) code ensemble whose threshold value matches the Shannon limit on the binary erasure channel (BEC) . This result suggests that OIRA may also achieve near-optimal performance on other channels  15  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Class of LDPC Erasure Distributions with Closed - Form Threshold Expression . Abstract : We suggest an erasure distribution for low - density parity - check ( LDPC ) codes that has closed - form threshold expression and is ideal in the sense that it minimizes the gap between its threshold value and Shannon limit on the binary symmetric channel ( BSC ) .The proposed distribution can be viewed as a generalization of the optimized irregular repeat accumulate ( IRA ) code ensemble , which was recently presented by Tanner et al . . We see that our new distribution achieves higher success than IRA over BSCs with little crossover probabilities . Finally , we present modeling results to test the ability of the suggested distribution under practical conditions .Index Terms - Low density parity check ( LDPC ) , Binary Symmetric Channel ( BSC ) , Optimized Irregular Repeat Accumulate Code Ensemble ( OIRA ) , Gap - to - Shannon Limit ( GTSL ) I . INTRODUCTIO N Low - Density Parity Check ( LDPC ) codes are linear block codes specified by dense parity - check matrices 1 . They have been shown to perform nearly to capacity when decoded using iterative message - passing techniques such as faith propagation 2 , and they are widely useful in different applications notably digital communications 3 - 5 .The design of excellent LDPC ensembles appears one of the most important problems in coding theory 6 . In particular , there exists a large body of research devoted to finding distributions that minimize the gap between their threshold coefficients and Shannon limits 7 - 11 .However , these works mainly emphasis on regular or pseudo - cyclic LDPC coding 12 , while irregular LDPC coding are more often employed owing to their flexibility 13 . Recently , Tanner et al .14 provided an optimized irregular repeat accumulate ( OIRA ) code ensemble whose threshold value matches the Shannon limit on the binary erasure channel ( BEC ) . This result suggests that OIRA may even attain near - optimal performance on other channels 15 .",
        "rewrite_text": "**Title:** A Class of LDPC Erasure Distributions with Closed-Form Threshold Expression\n\n**Abstract:** In this paper, we introduce a novel erasure distribution for low-density parity-check (LDPC) codes that features a closed-form expression for its threshold. This distribution is designed to be optimal, effectively minimizing the gap between its threshold value and the Shannon limit on the binary symmetric channel (BSC). Our proposed distribution can be regarded as an extension of the optimized irregular repeat accumulate (IRA) code ensemble recently developed by Tanner et al. Through our analysis, we demonstrate that this new distribution outperforms the IRA codes, particularly in BSC scenarios characterized by low crossover probabilities. Additionally, we provide modeling results that evaluate the performance of the proposed distribution under practical conditions, further validating its effectiveness. \n\nLow-Density Parity Check (LDPC) codes are linear block codes defined by sparse parity-check matrices and have been shown to approach capacity when decoded using iterative message-passing techniques such as belief propagation. Their versatility makes them highly applicable in various fields, especially in digital communications. The pursuit of high-performance LDPC ensembles remains a critical challenge in coding theory, with significant research focused on identifying distributions that minimize the gap between their threshold coefficients and Shannon limits. While much of the existing literature has concentrated on regular or pseudo-cyclic LDPC codes, irregular LDPC codes are often preferred due to their enhanced flexibility. Recent advancements, including the work by Tanner et al. on the optimized irregular repeat accumulate (OIRA) code ensemble, have indicated that these codes can achieve threshold values that align with the Shannon limit on the binary erasure channel (BEC), suggesting potential for near-optimal performance across other channels. Our findings contribute to this ongoing discourse by presenting a new class of erasure distributions that not only meet but exceed the performance benchmarks set by previous LDPC code ensembles. \n\n**Index Terms:** Low-Density Parity Check (LDPC), Binary Symmetric Channel (BSC), Optimized Irregular Repeat Accumulate Code Ensemble (OIRA), Gap-to-Shannon Limit (GTSL).",
        "ori-fast-z-score": -0.25630729731502827,
        "water-fast-z-score": 5.382453243615593,
        "rewrite-fast-z-score": 1.3834403799109711
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Lyman Alpha Emitters in the Hierarchically Clustering Galaxy Formation .\nAbstract:\nWe present an analysis of Lyman alpha emitters (LAEs) selected by narrowband imaging with Subaru/Suprime-Cam and spectroscopic follow-up observations using VLT/VIMOS at z ~ 3.1, which is one of the most active epochs for galaxy formation. We find that LAEs are distributed over a wide range of environments; they exist both in isolated regions as well as in dense clusters. The clustering properties of LAEs depend on their luminosities. In particular, we found that bright LAEs show stronger clustering than faint ones do. This result suggests that bright LAEs may be more evolved systems compared to fainter ones. Furthermore, we investigated the dependence of clustering strength on the equivalent widths of Lyman-alpha emission lines. Our results suggest that strong clustering objects tend to have higher equivalent widths. These findings imply that there exists some evolutionary link between LAEs and LBGs. \n \n Keywords: Lyman alpha emitter",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Lyman Alpha Emitters in the Hierarchically Clustering Galaxy Formation . Abstract : We present an assessment of Lyman alpha emitters ( LAEs ) selected by narrowband scanning with Subaru / Suprime - Cam and spectroscopic follow - up observations using VLT / VIMOS at z ~ 3 . 1 , which is one of the most important epochs for galaxy formation .We see that LAEs are distributed over a broad variety of habitats ; they exist both in isolated regions as well as in dense clusters . The clustering qualities of LAEs depend on their luminosities .In particular , we reported that bright LAEs see better clustering than bright ones do . This result suggests that bright LAEs may be more evolved structures versus to fainter ones .Furthermore , we investigated the dependence of clustering strength on the equivalent widths of Lyman - alpha emission lines . Our results show that strong clustering objects prefer to have greater equal widths .These studies imply that there exists some evolutionary link between LAEs and LBGs . Keywords : Lyman alpha emitter",
        "rewrite_text": "Title: Lyman Alpha Emitters in the Hierarchically Clustering Galaxy Formation\n\nAbstract: In this study, we evaluate Lyman alpha emitters (LAEs) identified through narrowband imaging with Subaru/Suprime-Cam, complemented by spectroscopic follow-up observations using VLT/VIMOS at a redshift of approximately z ~ 3.1, a critical period for galaxy formation. Our findings reveal that LAEs are found in a diverse range of environments, inhabiting both isolated areas and densely populated clusters. Notably, the clustering characteristics of LAEs exhibit a dependence on their luminosity; specifically, we observe that brighter LAEs demonstrate stronger clustering tendencies compared to their fainter counterparts. This observation suggests that brighter LAEs may represent more evolved structures in the context of galaxy formation. Additionally, we explore the relationship between clustering strength and the equivalent widths of Lyman-alpha emission lines. Our analysis indicates that objects with stronger clustering are associated with larger equivalent widths, further supporting the notion of an evolutionary connection between LAEs and Lyman Break Galaxies (LBGs). These results contribute to our understanding of the role of LAEs in the hierarchical clustering framework of galaxy formation, highlighting their potential as indicators of evolutionary processes in the early universe. \n\nKeywords: Lyman alpha emitter, galaxy formation, hierarchical clustering, Subaru/Suprime-Cam, VLT/VIMOS, Lyman Break Galaxies.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.093248125762992,
        "rewrite-fast-z-score": 0.6708203932499369
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Irreducible forms for the metric variations of the action terms of sixth-order gravity and approximated stress-energy tensor .\nAbstract:\nWe present an explicit expression for the irreducible form of the metric variation of the action term in sixth order gravity, which is valid to all orders in perturbation theory. We also show that this result can be used to derive an approximate expression for the stress energy tensor of the gravitational field. The results are applied to study the evolution of cosmological perturbations during inflation driven by a scalar field with non-canonical kinetic term. In particular we find that the non-Gaussianity generated at second order in perturbation theory does not vanish even if the background geometry is exactly de Sitter space-time. This implies that the bispectrum produced by such models cannot be described solely in terms of local shape functions as it was previously thought. \nI. INTRODUCTORY REMARK\nIn recent years there has been renewed interest on higher derivative theories of gravity motivated mainly by their possible role in quantum gravity phenomenology (see e.g. ), but also because they provide interesting alternatives to standard General Relativity (GR) in the context of modified gravity scenarios . However, despite these efforts, our understanding of the physical consequences of these theories remains incomplete due to technical difficulties associated with the analysis of their solutions. One of the main obstacles comes from the fact that the equations of motion derived from these actions contain derivatives of arbitrarily high order, making them difficult or impossible to solve analytically. A way out of this problem consists in expanding the fields around some fixed background solution and truncating the resulting series expansion after a finite number of terms. Although this approach allows one to obtain useful information about the dynamics of the system under consideration, it fails to capture important features like back-reaction effects between different modes of the same field or interactions among different fields. For example, in the case of inflationary cosmologies based on higher derivative gravity, the truncated perturbative expansions do not reproduce correctly the observed level of primordial non-Gaussianities .\nA more systematic method to deal with these problems involves the use of covariant techniques developed originally within the framework of GR. These methods allow us to express the equations of motion in a manifestly gauge",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Irreducible expressions for the metric variations of the activity terms of sixth - order gravity and approximated strain - energy tensor . Abstract : We present an explicit expression for the irreducible form of the metric variation of the action word in sixth order gravity , which is valid to all orders in perturbation theory .We additionally prove that this result can be used to derive an approximate representation for the strain energy tensor of the gravitational field . The results are applied to study the evolution of cosmological perturbations during inflation driven by a scalar field with non - canonical kinetic term .In particular we find that the non - Gaussianity generated at second order in perturbation theory does not vanish even if the background geometry is precisely de Sitter space - time . This implies that the bispectrum produced by such theories cannot be described solely in terms of local form variables as it was formerly thought .I . INTRODUCTORY REMARK In recent years there has been continued interest on higher derivative theories of gravitational motivated mainly by their possible involvement in quantum gravitational phenomenology ( saw e . g .) , but also because they give exciting alternatives to standard General Relativity ( GR ) in the context of revised gravitational scenarios . However , despite these attempts , our grasp of the physical effects of these theories appears incomplete due to technical problems related with the interpretation of their solutions .One of the main challenge comes from the fact that the coefficients of movement obtained from these actions involve derivatives of arbitrarily high order , making them harder or impossible to solve analytically . A way out of this question involves in expanding the fields around some fixed background solution and truncating the resulting series contraction after a finite number of terms .Although this methodology allows one to obtain usable information about the dynamics of the process under consideration , it fails to capture important features like back - reaction effects between various modes of the same field or relationships among different fields . For instance , in the case of inflationary cosmologies based on larger derivative gravity , the truncated perturbative expansions do not reproduce correctly the seen level of primordial non - Gaussianities .A more thorough method to deal with these problems involves the using of covariant techniques constructed originally within the framework of GR . These methods help us to derive the equations of movement in a manifestly gauge",
        "rewrite_text": "**Title:** Irreducible Expressions for the Metric Variations of the Activity Terms of Sixth-Order Gravity and Approximated Strain-Energy Tensor\n\n**Abstract:** In this study, we derive an explicit irreducible expression for the metric variation of the action in sixth-order gravity, applicable to all orders within perturbation theory. This formulation not only enhances our understanding of the gravitational action but also facilitates the derivation of an approximate representation for the strain-energy tensor associated with the gravitational field. We apply these findings to investigate the dynamics of cosmological perturbations during inflation, particularly in scenarios driven by a scalar field with a non-canonical kinetic term. Our analysis reveals that the non-Gaussianity generated at second order in perturbation theory persists even when the background geometry is precisely de Sitter space-time. This significant result challenges the conventional notion that the bispectrum produced in such theories can be adequately described using local form variables, suggesting a more complex underlying structure.\n\nThe growing interest in higher derivative theories of gravity is largely motivated by their potential implications for quantum gravitational phenomena and their ability to provide intriguing alternatives to standard General Relativity (GR) within revised gravitational frameworks. However, the physical implications of these theories remain inadequately understood, primarily due to technical challenges in interpreting their solutions. A major hurdle arises from the high-order derivatives present in the equations of motion derived from these actions, which complicates analytical solutions. A common approach to address this issue involves expanding fields around a fixed background solution and truncating the resulting series after a finite number of terms. While this method yields useful insights into the dynamics, it often overlooks critical aspects such as back-reaction effects among different modes of the same field or interactions between various fields.\n\nIn the context of inflationary cosmologies based on higher derivative gravity, truncated perturbative expansions frequently fail to accurately reproduce the observed levels of primordial non-Gaussianities. To overcome these limitations, we advocate for the use of covariant techniques originally developed within the framework of GR. These advanced methods enable us to derive the equations of motion in a manifestly gauge-invariant manner, thereby providing a more comprehensive understanding of the dynamics at play in higher-order gravitational theories.",
        "ori-fast-z-score": 0.14285714285714285,
        "water-fast-z-score": 7.765928448682152,
        "rewrite-fast-z-score": 0.5897678246195885
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The nature of a broad line radio galaxy: Simultaneous RXTE and Chandra HETG observations of 3C 382 .\nAbstract:\nWe present the results of simultaneous X-ray (Chandra) and radio (RXTE )observations of the Broad Line Radio Galaxy, 3C382 . The data were taken on 2001 September 24-25 UT during an outburst in which the source was detected at radio frequencies as high as 22 GHz. We find that the X-ray spectrum is well described by a power law with photon index Γ = 1.7 ± 0.1 modified by photoelectric absorption consistent with N_H = 2 x 1022 cm-2. There are no significant spectral changes between the two epochs observed. In addition to the continuum emission we detect several narrow lines including Fe Kα , He-like Si XIII , S XV and Ar XVII . These features appear blueshifted relative to their rest wavelengths indicating bulk motion towards us along our line-of-sight. Using these velocities together with estimates for the mass of the central black hole derived from optical measurements we estimate the distance of the emitting material from the center of the AGN to be ~10 light days.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The nature of a broad line radio galaxy : Simultaneous RXTE and Chandra HETG measurements of 3C 382 . Abstract : We report the conclusion of simultaneous X - ray ( Chandra ) and radio ( RXTE ) observations of the Broad Line Radio Galaxy , 3C382 .The data were took on 2001 September 24 - 25 UT during an outburst in which the origin was seen at radio altitudes as long as 22 GHz . We see that the X - ray spectrum is well described by a power law with photon index Γ = 1 . 7 ± 0 . 1 augmented by photoelectric absorbed consistent with N _ H = 2 x 1022 centimetres - 2 .There are no considerable brightness variations between the two epochs observed . In addition to the continuum emission we find various narrow lines including Fe Kα , He - like Si XIII , S XV and Ar XVII .These features appear blueshifted relative to their rest wavelengths suggesting bulk movement towards us along our line - of - seeing . Using these velocities together with predictions for the mass of the central black hole derived from optical calculations we estimate the distance of the emitting substance from the center of the AGN to be ~ 10 light years .",
        "rewrite_text": "We present findings from simultaneous X-ray (Chandra) and radio (RXTE) observations of the Broad Line Radio Galaxy 3C 382, conducted on September 24-25, 2001, during a significant outburst detectable at radio frequencies up to 22 GHz. Our analysis reveals that the X-ray spectrum is accurately characterized by a power law with a photon index of Γ = 1.7 ± 0.1, along with photoelectric absorption consistent with a hydrogen column density of N_H = 2 x 10^22 cm^-2. Notably, we observed no significant brightness variations between the two observational epochs. In addition to the continuum emission, we identified several narrow emission lines, including Fe Kα, He-like Si XIII, S XV, and Ar XVII. These spectral features exhibit blueshifts relative to their rest wavelengths, indicating a bulk motion directed towards us along our line of sight. By utilizing the measured velocities in conjunction with mass estimates of the central black hole derived from optical studies, we estimate that the emitting region is located approximately 10 light years from the active galactic nucleus (AGN). This research enhances our understanding of the dynamics and structure of broad line radio galaxies, particularly in relation to their X-ray and radio emissions during active phases. The implications of these findings contribute to the broader field of astrophysics, offering insights into the behavior of matter in extreme gravitational fields and the mechanisms driving the emissions from such energetic cosmic entities.",
        "ori-fast-z-score": -1.0256451881367414,
        "water-fast-z-score": 4.444462481925879,
        "rewrite-fast-z-score": -0.6897304947150052
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Can Gravity Probe B usefully constrain torsion gravity theories? .\nAbstract:\nWe study the possibility that gravitational waves can be detected by measuring their effect on gyroscopes in space, as proposed for the GP-B experiment. We consider two classes of models with torsion and show how they affect the motion of test particles around spinning black holes. In one class we find that there is no effect at all; this includes Einstein-Cartan theory (with or without fermions) and teleparallel gravity. The other class contains some effects but these are too small to be detectable even if the spin of the black hole were known exactly. However, it may still be possible to detect such effects using future experiments like LISA. Finally, we discuss whether any of our results could have been anticipated within general relativity. This work was supported by NSF grant PHY-0456747. Gravitational waves will produce tiny changes in the orientation of gyroscopes carried into space by satellites. These changes should be measurable by comparing the orientations of pairs of gyroscopes separated by large distances. Such an experiment has recently begun taking data  1  . It is called Gravity Probe B (GP-B), after its predecessor which measured the precession of the earth s orbit  2  .\nIn this Letter we investigate what information about gravitational waves might be obtained from measurements made by GP-B. Our main focus is on theories containing torsion -the antisymmetric part of the connection  3, 4  , which plays a role similar to electromagnetism in standard general relativity  5  . Torsion arises naturally in many extensions of general relativity  6  ; however, it also appears in certain modified versions of general relativity  7, 8  . For example, in string-inspired supergravity  9  , torsion couples directly to matter fields  10  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Can Gravity Probe B usefully constrain torsion gravity theories ? .Abstract : We research the prospect that gravity signals can be identified by monitoring their effect on gyroscopes in space , as suggested for the GP - B experiment . We consider two groups of models with torsion and know how they impact the movement of test particles around spun dark holes .In one category we find that there is no effect at all ; this includes Einstein - Cartan theory ( with or without fermions ) and teleparallel gravitational . The other class includes some effects but these are too small to be detectable even if the spin of the dark hole were known exactly .However , it could still be possible to observe such effects utilizing potential experiments like LISA . Finally , we issue whether any of our findings may have been anticipated within general relativity .This project was supported by NSF grant PHY - 0456747 . Gravitational waves will generate tiny changes in the orientation of gyroscopes sent into space by satellites .These changes should be measurable by testing the orientations of pairs of gyroscopes separated by large distances . Such an observation has recently begun took results 1 .It is titled Gravity Probe B ( GP - B ) , after its predecessor which calculated the precession of the earth s orbit 2 . In this Letter we investigate what knowledge about gravitational waves might be obtained from measurements made by GP - B .Our main interest is on theories involving torsion - the antisymmetric part of the relationship 3 , 4 , which plays a role similar to electromagnetism in standard special relativity 5 . Torsion occurs commonly in many extensions of general relativity 6 ; however , it also exists in certain modified variants of general relativity 7 , 8 .For instance , in string - inspired supergravity 9 , torsion couples directly to matter fields 10 .",
        "rewrite_text": "**Title: Can Gravity Probe B Effectively Constrain Torsion Gravity Theories?**\n\n**Abstract:** This study explores the potential of utilizing gravity signals to identify their influence on gyroscopes deployed in space, as proposed in the Gravity Probe B (GP-B) experiment. We examine two distinct categories of torsion gravity models and their effects on the motion of test particles in the vicinity of rotating black holes. In the first category, we find that certain models, including Einstein-Cartan theory (with or without fermionic contributions) and teleparallel gravity, exhibit no observable effects on gyroscope behavior. Conversely, the second category reveals models that do produce effects; however, these effects are too minuscule to be detected, even with precise knowledge of the black hole's spin. Despite this limitation, we suggest that future experiments, such as the Laser Interferometer Space Antenna (LISA), may still capture these subtle influences. Additionally, we consider whether any of our results could have been anticipated within the framework of general relativity. This research was supported by NSF grant PHY-0456747. Gravitational waves are expected to induce minute changes in the orientation of gyroscopes placed in space by satellites, which can be measured by comparing the orientations of pairs of gyroscopes positioned at significant distances apart. Recent observations have begun to yield results in this area. The GP-B experiment, named after its predecessor that measured the precession of Earth's orbit, serves as a foundation for our investigation into the insights that can be gleaned about gravitational waves from GP-B measurements. Our primary focus is on torsion theories, which introduce an antisymmetric component to the gravitational interaction, akin to the role of electromagnetism in standard special relativity. Torsion is a common feature in various extensions of general relativity and is also present in certain modified theories. For example, in string-inspired supergravity, torsion interacts directly with matter fields, highlighting its significance in contemporary gravitational research.",
        "ori-fast-z-score": 0.3481553119113957,
        "water-fast-z-score": 8.23754471047914,
        "rewrite-fast-z-score": 1.2543630150106362
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Finite Element framework for computation of protein normal modes and mechanical response .\nAbstract:\nWe present an efficient computational method to compute the elastic properties, vibrational frequencies and normal modes of proteins using finite element analysis (FEA). The proposed approach is based on representing each amino acid residue as a single node in a 3D tetrahedral mesh that covers the entire structure of interest. We show how this representation can be used within FEA to efficiently calculate the stiffness matrix associated with the system under study. In addition we demonstrate how the same representation allows us to obtain accurate results when computing the eigenfrequencies and corresponding mode shapes of the system by solving the generalized eigenvalue problem arising from the discretized form of the governing equations. Finally, we illustrate our methodology through several numerical examples involving different types of proteins. Our results indicate that the proposed approach provides highly accurate predictions compared to those obtained via other state-of-the-art methods such as molecular dynamics simulations or coarse-grained models. This work was supported by NSERC Discovery Grants RGPIN-2014-04567 and RGPIN-2016-05189 .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Finite Element framework for computation of gene proper modes and mechanical response . Abstract : We create an efficient numerical technique to compute the elastic properties , vibrational speeds and normal mechanisms of proteins using finite element assessment ( FEA ) .The proposed approach is based on depicting each amino residue compound as a single node in a 3D tetrahedral mesh that covers the entire structure of interest . We see how this representation can be used within FEA to easily assess the stiffness matrix associated with the process under research .In addition we prove how the same expression permits us to obtain precise conclusions when computing the eigenfrequencies and corresponding mode shapes of the system by addressing the generalized eigenvalue problem originating from the discretized form of the governing equations . Finally , we exhibit our technique through several mathematical examples involving varying kinds of proteins .Our results show that the suggested approach offers highly precise predictions compared to those achieved via other state - of - the - art methods such as molecular dynamics simulations or rough - grained estimates . This research was supported by NSERC Discovery Grants RGPIN - 2014 - 04567 and RGPIN - 2016 - 05189 .",
        "rewrite_text": "Title: A Finite Element Framework for Computing Gene Proper Modes and Mechanical Response\n\nAbstract: In this study, we present a novel and efficient numerical method for calculating the elastic properties, vibrational speeds, and normal modes of proteins through the application of finite element analysis (FEA). Our approach involves modeling each amino acid residue as an individual node within a three-dimensional tetrahedral mesh that encompasses the entire protein structure of interest. This representation facilitates the straightforward computation of the stiffness matrix relevant to the mechanical processes being investigated. Furthermore, we demonstrate that this same framework allows for the accurate determination of eigenfrequencies and corresponding mode shapes by solving the generalized eigenvalue problem derived from the discretized governing equations. To validate our methodology, we apply it to a series of mathematical examples that include various types of proteins. The results indicate that our proposed technique yields highly accurate predictions, significantly outperforming traditional methods such as molecular dynamics simulations and coarse-grained approximations. This research was made possible through the support of NSERC Discovery Grants RGPIN-2014-04567 and RGPIN-2016-05189. Our findings contribute to the understanding of protein mechanics and provide a robust tool for future studies in the field of computational biology.",
        "ori-fast-z-score": -1.212183053462653,
        "water-fast-z-score": 6.130730673081194,
        "rewrite-fast-z-score": 0.41256849850351734
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Nurturing Breakthroughs: Lessons from Complexity Theory .\nAbstract:\nThe book Nurturing Breakthroughs by James Watson and Peter Winkler is about how to create breakthroughs in science, technology, engineering or mathematics (STEM). The authors argue that the best way for scientists to make new discoveries is not through individual genius but rather by working together as teams on problems they are passionate about.  They also claim that it helps if you have an idea of what your problem looks like before you start solving it because this will help guide your research efforts. This article describes some of their ideas and provides examples of how these concepts can be applied to physics research. In his book Nurturing Breakdowns, James Watson argues that the most successful people who solve complex problems do so by working with others instead of trying to work alone. He says that when we work individually our brains tend to focus only on one aspect of the problem at hand which may lead us down dead ends while working collaboratively allows us to see all aspects of the problem simultaneously.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Nurturing Breakthroughs : Lessons from Complexity Theory . Abstract : The text Nurturing Breakthroughs by James Watson and Peter Winkler is about how to create breakthroughs in science , technology , engineering or math ( STEM ) .The authors argue that the best method for researchers to make fresh findings is not through individual genius but rather by acting together as teams on problems they are enthusiastic about . They especially claim that it aids if you have an idea of what your problem looks like before you start solving it because this will assist guide your study efforts .This page describes some of their ideas and provides evidence of how these concepts can be applied to physics studies . In his book Nurturing Breakdowns , James Watson believes that the most talented people who solution complex situations do so by working with others rather of trying to work alone .He said that when we study independently our mind tend to reflect only on one element of the issue at hand which would lead us down dead ends while working collaboratively allows us to see all aspects of the issue simultaneously .",
        "rewrite_text": "**Title: Nurturing Breakthroughs: Lessons from Complexity Theory**\n\n**Abstract:** The article \"Nurturing Breakthroughs\" by James Watson and Peter Winkler explores the dynamics of innovation within the fields of science, technology, engineering, and mathematics (STEM). The authors contend that significant advancements are more effectively achieved through collaborative teamwork rather than relying solely on individual brilliance. They emphasize the importance of collective enthusiasm for the problems being addressed, suggesting that a shared passion among team members can drive more impactful discoveries. Furthermore, the authors highlight the necessity of having a clear conceptual understanding of the problem prior to embarking on the research journey. This foresight not only streamlines the investigative process but also enhances the overall effectiveness of the research efforts.\n\nThe article delves into various strategies that can be employed to foster collaboration and creativity within research teams, providing concrete examples from the field of physics to illustrate their points. Watson argues in his previous work, \"Nurturing Breakdowns,\" that the most adept problem solvers are those who engage with others, as opposed to attempting to navigate complex challenges in isolation. He posits that solitary study often leads to a narrow focus on a single aspect of a problem, which can result in unproductive pathways. In contrast, collaborative efforts enable participants to consider multiple facets of an issue simultaneously, thereby enriching the problem-solving process.\n\nOverall, \"Nurturing Breakthroughs\" serves as a compelling reminder of the power of teamwork in scientific inquiry and the importance of a well-defined problem framework. The insights presented by Watson and Winkler not only advocate for a shift in how researchers approach their work but also provide a roadmap for cultivating an environment conducive to innovation and discovery in STEM disciplines.",
        "ori-fast-z-score": -0.8834522085987723,
        "water-fast-z-score": 5.8175057794535885,
        "rewrite-fast-z-score": 0.7126966450997984
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Magnetic focusing of charge carriers from spin-split bands: Semiclassics of a Zitterbewegung effect .\nAbstract:\nWe study the semiclassical dynamics of electrons in magnetic fields, which are described by the Dirac equation with spin-orbit coupling and Zeeman splitting. We show that the electron trajectories can be focused into narrow beams when their initial velocities have opposite directions along the field lines. This is due to an interference between two types of motion -the usual cyclotrons and the so-called  Zitterbewegung  oscillations-which leads to a beating pattern on top of the classical circular orbits. The latter type of motion arises because of the relativistic nature of the particles and its origin lies in the fact that the energy bands are spin split. Our results provide a new perspective for understanding the physics behind phenomena such as the quantum Hall effect or the integer quantum Hall effect at high Landau levels. \nI. INTRODUCTIO N\nThe transport properties of two-dimensional (2D) systems of interacting fermions under strong perpendicular magnetic fields have been studied extensively over many years  1  . In particular, it has been shown that the presence of a quantizing magnetic field gives rise to novel phases characterized by fractional filling factors  2  , where the number of filled Landau levels differs from the expected value  3  .\nIn this work we focus our attention on the case of non-interacting fermions moving in 2D space subject to a uniform magnetic field B = Be z  4  . For simplicity, we consider only one spin species; however, all our results remain valid if both spin projections are taken into account  5  . In addition, we assume that the Fermi level lies within the conduction band  6  . Under these conditions, the low-energy excitations around the Fermi surface are well-described by the massless Dirac Hamiltonian  7, 8  \nwhere v F denotes the Fermi velocity, σ i=x,y,z denote Pauli matrices acting on the spinor wave function Ψ(r), p x = −i∂/∂x and p y = −i∂/(−i∂y). Hereafter, we seth = 1 and e = 1. It should be noted that Eq. (1) \nII. ELECT",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Magnetic concentrating of charge carriers from spin - separated bands : Semiclassics of a Zitterbewegung effect . Abstract : We research the semiclassical dynamics of electrons in magnetic fields , which are explained by the Dirac formula with spin - orbit bonding and Zeeman splitting .We see that the electron trajectories can be focused into narrow beams when their initial velocities have different directions along the field lines . This is due to an interference between two forms of movement - the usual cyclotrons and the so - called Zitterbewegung oscillations - which results to a beating sequence on top of the classical circular orbits .The latter type of movement occurs because of the relativistic behavior of the particles and its origin lies in the fact that the power groups are spin split . Our results present a new insight for studying the physics behind processes such as the quantum Hall impact or the integer quantum Hall impact at high Landau concentrations .I . INTRODUCTIO N The transport properties of two - dimensional ( 2D ) systems of interacting fermions under strong perpendicular magnetic fields have been studied frequently over numerous years 1 . In particular , it has been shown that the presence of a quantizing magnetic force gives rise to novel phases characterized by fractional filling variables 2 , where the quantity of filled Landau concentrations differs from the expected value 3 .In this research we focus our focus on the case of non - interacting fermions moving in 2D space due to a uniform magnetic force B = Be z 4 . For simplicity , we treat only one spin species ; however , all our findings remain correct if both spin projections are took into account 5 .In addition , we suppose that the Fermi level falls within the conduction band 6 . Under these conditions , the small - energy excitations around the Fermi surface are best - described by the massless Dirac Hamiltonian 7 , 8 where v F denotes the Fermi velocity , σ i = x , y , z define Pauli matrices acting on the spinor wave function Ψ ( r ) , r x = −i∂ / ∂x and p y = −i∂ / ( −i∂y ) .Hereafter , we seth = 1 and e = 1 . It should be mentioned that Eq .(1) \nII.ELECT",
        "rewrite_text": "**Title:** Magnetic Concentration of Charge Carriers from Spin-Separated Bands: Semiclassics of the Zitterbewegung Effect\n\n**Abstract:** This study investigates the semiclassical dynamics of electrons subjected to magnetic fields, utilizing the Dirac equation that incorporates spin-orbit coupling and Zeeman splitting. We demonstrate that electron trajectories can converge into narrow beams when their initial velocities are oriented differently along the magnetic field lines. This phenomenon arises from the interference between two distinct modes of motion: conventional cyclotron motion and the Zitterbewegung oscillations. The interplay of these movements leads to a modulation effect superimposed on the classical circular orbits. The Zitterbewegung, a relativistic effect, is fundamentally linked to the spin-splitting of energy bands. Our findings provide valuable insights into the underlying physics of phenomena such as the quantum Hall effect and the integer quantum Hall effect at elevated Landau level fillings.\n\nIn the introduction, we highlight the extensive research conducted over the years on the transport properties of two-dimensional (2D) systems of interacting fermions in strong perpendicular magnetic fields. It has been established that a quantizing magnetic field induces novel phases characterized by fractional filling factors, where the number of occupied Landau levels deviates from expected values. Our focus is directed towards non-interacting fermions in a 2D plane under a uniform magnetic field, denoted as B = Be_z. For the sake of simplicity, we consider only one spin species; however, our conclusions remain valid when both spin projections are accounted for. We assume that the Fermi level is situated within the conduction band, allowing us to describe small-energy excitations around the Fermi surface using the massless Dirac Hamiltonian. Here, v_F represents the Fermi velocity, and σ_i (where i = x, y, z) are the Pauli matrices acting on the spinor wave function Ψ(r). The momentum operators are defined as r_x = -i∂/∂x and p_y = -i∂/∂y. For the purposes of this study, we adopt units where h = 1 and e = 1. \n\nThis research contributes to a deeper understanding of the dynamics of charge carriers in magnetic fields, paving the way for future explorations in condensed matter physics.",
        "ori-fast-z-score": -1.4411533842457842,
        "water-fast-z-score": 6.145269010400691,
        "rewrite-fast-z-score": -0.24576957615571215
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The standard model on a domain-wall brane? .\nAbstract:\nWe consider the Standard Model (SM) in 5 dimensions, where one extra dimension is compactified to an orbifold S 1 /Z 2 . The SM fields are assumed to be localized at different fixed points along this extra dimension. We show that such models can naturally explain why there should exist three generations of fermions and gauge bosons with their observed masses and mixings. In addition we find that these models provide new ways for understanding some other issues related to the SM like neutrino mass generation or flavor changing neutral currents. Finally we discuss how our results could be tested experimentally. Introduction: One of the most important open questions in particle physics today concerns the origin of fermion families and their mixing angles. It has been known since the work by Pati & Salam  1  , that if quarks and leptons were unified into larger multiplets then it would be possible to understand the pattern of quark-lepton masses and mixings within Grand Unified Theories (GUTs). However, despite many attempts over more than 30 years no realistic GUT has yet been constructed which incorporates all the features of the Standard Model (SM).\nIn recent years another possibility was suggested  2  -  4  : If the SM fields live in higher dimensional space-time, they may have Kaluza-Klein excitations corresponding to additional states with masses of order 1/R, where R denotes the size of the extra dimensions. These states might correspond to heavy particles beyond those present in the SM spectrum. This idea leads to interesting phenomenological consequences  5  .\nThe simplest way to realize this scenario is to assume that only gravity propagates in the bulk while the SM fields are confined to a four-dimensional  brane   6  . Such theories lead to corrections to the Newtonian potential between two test masses m 1 and m 2 separated by distance r given by: \nwhere M P l = 1/ √ 8πG N ≈ 10 19 GeV is the reduced Planck scale and n i counts the number of extra spatial dimensions accessible to field i. For distances smaller than about 0.1 mm deviations from the inverse square law predicted by general relativity will become",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The basic model on a domain - wall brane ? .Abstract : We consider the Standard Model ( SM ) in 5 dimensions , where one extra dimension is compactified to an orbifold S 1 / Z 2 . The SM fields are expected to be localized at different fixed points along this extra dimension .We see that such theories can naturally explain why there should exist three generations of fermions and gauge bosons with their observed masses and mixings . In addition we find that these models bring fresh ways for explaining some other issues related to the SM like neutrino mass development or color shifting neutral currents .Finally we talk how our findings may be evaluated experimentally . Introduction : One of the most important open questions in particle science today issues the origin of fermion families and their mixing angles .It has been known since the paper by Pati & Salam 1 , that if quarks and leptons were organized into larger multiplets then it would be possible to explain the trend of quark - lepton masses and mixings within Grand Unified Theories ( GUTs ) . However , despite many efforts over more than 30 centuries no realistic GUT has already been constructed which includes all the details of the Standard Model ( SM ) .In recent work another possibility was suggested 2 - 4 : If the SM fields reside in larger dimensional space - time , they may have Kaluza - Klein excitations corresponding to extra states with masses of order 1 / R , where R denotes the height of the extra dimensions . These states could belong to heavy ions beyond those present in the SM spectrum .This idea results to useful phenomenological consequences 5 . The shortest way to realize this situation is to assume that only gravitational propagates in the bulk while the SM fields are confined to a four - dimensional brane 6 .Such theories lead to corrections to the Newtonian potential between two test masses m 1 and m 2 separated by distance r given by : where M P l = 1 / √ 8πG N ≈ 10 19 GeV is the reduced Planck scale and n i counts the number of extra spatial dimensions accessible to field i . For distances smaller than about 0 . 1 mm deviations from the inverse square law predicted by particular relativity will become",
        "rewrite_text": "**Title:** A Fundamental Model on a Domain-Wall Brane\n\n**Abstract:** In this study, we investigate the Standard Model (SM) extended into five dimensions, where one additional dimension is compactified into an orbifold S^1/Z_2. Within this framework, we propose that the SM fields are localized at distinct fixed points along the extra dimension. Our analysis reveals that such a multidimensional approach can inherently account for the existence of three generations of fermions and gauge bosons, along with their observed masses and mixing patterns. Furthermore, we explore how these models offer novel insights into several unresolved issues within the SM, including the generation of neutrino masses and the phenomenon of color-shifting neutral currents. We also discuss the potential experimental implications of our findings, suggesting avenues for future research. \n\nThe introduction highlights a critical question in contemporary particle physics: the origin of fermion families and their mixing angles. Historical perspectives, such as the work by Pati & Salam, indicate that organizing quarks and leptons into larger multiplets could elucidate the observed trends in quark-lepton masses and mixing within Grand Unified Theories (GUTs). However, despite extensive efforts over the past three decades, a comprehensive GUT that encapsulates all aspects of the SM remains elusive. Recent studies have proposed an alternative approach, suggesting that if SM fields exist in a higher-dimensional spacetime, they may exhibit Kaluza-Klein excitations, which correspond to additional states with masses proportional to 1/R, where R represents the scale of the extra dimensions. These states could correspond to heavy particles not accounted for in the current SM framework, leading to significant phenomenological implications. \n\nTo realize this scenario, we assume that only gravitational interactions propagate through the bulk, while the SM fields are confined to a four-dimensional brane. This setup results in modifications to the Newtonian gravitational potential between two test masses, m_1 and m_2, separated by a distance r. Notably, for distances less than approximately 0.1 mm, deviations from the inverse-square law predicted by special relativity become significant, opening up new avenues for experimental verification of our theoretical predictions.",
        "ori-fast-z-score": 2.182820625326997,
        "water-fast-z-score": 8.433802953476238,
        "rewrite-fast-z-score": 2.2119261854014094
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Semiclassical scalar propagators in curved backgrounds: formalism and ambiguities .\nAbstract:\nWe present the results of our investigation on semiclassical scalar propagator in curved space-time, which is based on the WKB approximation to the wave function. We show that there are two different ways how one can define this quantity depending on whether or not one takes into account the back-reaction effects due to the quantum fluctuations of the gravitational field. The first approach leads to an expression for the semiclassical propagator which coincides with the Feynman propagator at large distances but differs significantly near the source point. In particular it does not satisfy the Hadamard condition required by general relativity. On the other hand, if we take into account the back reaction then the resulting expression satisfies all necessary conditions including the Hadamard condition. However, as was shown recently by Wald et al., such an expression cannot be obtained within the framework of standard QFT. This problem may have important consequences when considering the propagation of particles through black holes since the corresponding expressions differ substantially even outside the horizon.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Semiclassical scalar propagators in curved landscapes : formalism and ambiguities . Abstract : We present the conclusion of our inquiry on semiclassical scalar propagator in curved space - time , which is based on the WKB approximation to the wave function .We see that there are two different ways how one can define this quantity varying on whether or not one takes into consideration the back - reaction effects due to the quantum fluctuations of the gravitational field . The first method results to an definition for the semiclassical propagator which coincides with the Feynman propagator at large distances but varies dramatically near the origin point .In particular it does not satisfy the Hadamard condition required by general relativity . On the other hand , if we took into consideration the back response then the resulting expression satisfies all necessary circumstances including the Hadamard condition .However , as was shown lately by Wald et al . , such an expression cannot be obtained within the framework of standard QFT . This problem could have important implications when examining the propagation of particles through black holes since the equivalent definitions differ substantially even outside the horizon .",
        "rewrite_text": "In this article, we explore the semiclassical scalar propagator in curved spacetime, utilizing the WKB approximation to analyze the wave function. Our investigation reveals two distinct approaches to defining this propagator, which hinge on whether the back-reaction effects from quantum fluctuations of the gravitational field are considered. The first approach yields a semiclassical propagator that aligns with the Feynman propagator at large distances but exhibits significant deviations near the origin. Notably, this definition fails to meet the Hadamard condition, a crucial requirement in general relativity. Conversely, when back-reaction effects are incorporated, the resulting expression adheres to all necessary conditions, including the Hadamard condition. However, recent findings by Wald et al. indicate that such an expression cannot be derived within the conventional framework of quantum field theory (QFT). This discrepancy raises important questions regarding the behavior of particle propagation in the vicinity of black holes, as the two definitions of the propagator diverge significantly even outside the event horizon. Our findings underscore the complexities and ambiguities inherent in the semiclassical treatment of scalar fields in curved landscapes, suggesting that a deeper understanding of the interplay between quantum fluctuations and gravitational effects is essential for advancing our knowledge in this field. The implications of our work extend to various areas of theoretical physics, particularly in the context of black hole physics and the quest for a unified description of quantum gravity.",
        "ori-fast-z-score": 0.10846522890932808,
        "water-fast-z-score": 5.038928913737635,
        "rewrite-fast-z-score": 1.8367993291867606
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Hubble and Spitzer Observations of an Edge-on Circumstellar Disk around a Brown Dwarf .\nAbstract:\nWe present Hubble Space Telescope (HST) and Spitzer Space Telescope observations of the edge-on circumstellar disk surrounding 2MASS J04414489+2512172, a young brown dwarf with spectral type M8 located in Upper Scorpius at a distance of 145 pc. The HST data reveal that this object is surrounded by a bright ring-like structure extending to ~0.5′′ (~120 AU). We find evidence for two spiral arms emerging from the inner part of the ring toward its center. These features are also seen in near-infrared images obtained with the adaptive optics system NACO on VLT/UT4. In addition, we detect several knots along these spirals which may be caused by dust clumps or planetesimals embedded within them. Our results suggest that the observed structures could have been formed through gravitational instability triggered by rapid inward migration of solids due to gas drag forces.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Hubble and Spitzer Observations of an Edge - on Circumstellar Disk around a Brown Dwarf . Abstract : We report Hubble Space Telescope ( HST ) and Spitzer Space Telescope observations of the edge - on circumstellar disk surrounding 2MASS J04414489 + 2512172 , a young white dwarf with spectral variety M8 situated in Upper Scorpius at a distance of 145 pc .The HST results show that this object is surrounded by a bright ring - like structure extending to ~ 0 . 5 ′ ′ ( ~ 120 AU ) . We see evidence for two spiral arms arising from the inner part of the circle toward its center .These features are also shown in near - infrared images obtained with the adaptive optics scheme NACO on VLT / UT4 . In addition , we perceive several knots along these spirals which may be caused by dust clumps or planetesimals embedded within them .Our results show that the known structures could have been formed through gravity instability caused by rapid inward movement of solids due to gas drag forces .",
        "rewrite_text": "We present findings from observations conducted with the Hubble Space Telescope (HST) and the Spitzer Space Telescope, focusing on the edge-on circumstellar disk surrounding the young brown dwarf 2MASS J04414489 + 2512172, classified as an M8 spectral type. Located in the Upper Scorpius region at a distance of 145 parsecs, this object exhibits a striking ring-like structure that extends approximately 0.5 arcseconds, equivalent to about 120 astronomical units. Our HST observations reveal the presence of two prominent spiral arms that originate from the inner region of the disk and extend toward its center. These spiral features are corroborated by near-infrared images captured using the adaptive optics system NACO on the Very Large Telescope (VLT) at UT4. Furthermore, we identify several distinct knots along the spirals, which may indicate the presence of dust clumps or planetesimals that are likely embedded within the disk. The observed structures suggest that they may have formed as a result of gravitational instabilities, driven by the rapid inward migration of solid materials influenced by gas drag forces. This study enhances our understanding of the dynamics and evolution of circumstellar disks around brown dwarfs and provides insights into the processes that may lead to planet formation in such environments. Our findings contribute to the broader discourse on the formation and characteristics of circumstellar disks, particularly in relation to low-mass stellar objects.",
        "ori-fast-z-score": 0.23904572186687872,
        "water-fast-z-score": 5.103161130233569,
        "rewrite-fast-z-score": 0.7921180343813395
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Analysis of thermally stimulated luminescence and conductivity without quasiequilibrium approximation .\nAbstract:\nWe present an analysis of the temperature dependence of thermally stimulated luminescent (TSL) glow curves in terms of the nonstationary electron-phonon relaxation theory, which does not assume that the system is close to equilibrium at any time during its evolution.  We show how this approach can be used for extracting information about the phonon spectrum and the density of states of charge carriers from TSL data obtained on different types of materials. The results are compared with those obtained by other methods such as photoluminescence excitation spectroscopy or Raman scattering. In particular we demonstrate that our method allows one to determine the energy gap between the conduction band minimum and valence band maximum in semiconductors. This work was supported by Russian Science Foundation grant No. 14-50-00040. DOI: 10.1063/1.4935190 \nI. INTRODUCTORY REMARK\nThe study of luminescence phenomena has been attracting considerable attention over many years because it provides valuable information about electronic structure and optical properties of solids  1  . Thermal stimulation luminescence (TSL), also known as optically stimulated luminescence (OSL), is particularly useful since it enables us to probe the distribution function of electrons excited into the conduction band  2  .\nIn recent decades there have been numerous attempts to develop theoretical models describing various aspects of luminescence processes  3  , including thermal stimulation luminescence  4  -  8  . However, most of these works were based on the assumption that the system under consideration is always close to equilibrium  9  . As a result they cannot describe correctly some important features observed experimentally  10  . For example, the shape of the TSL glow curve depends strongly on the type of material  11  : while in insulators it usually exhibits a single peak  12  , in metals it often consists of several peaks  13  . Moreover, even within the same class of materials, e.g., semiconductor crystals  14  , the number of peaks may vary depending on the doping level  15  . These observations cannot be explained using existing theories  16  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Analysis of thermally stimulated luminescence and conductivity without quasiequilibrium approximation . Abstract : We present an assessment of the temperature dependence of thermally stimulated luminescent ( TSL ) glow curves in terms of the nonstationary electron - phonon relaxation hypothesis , which does not assume that the system is close to equilibrium at any time during its evolve .We see how this methodology can be used for extracting information about the phonon spectrum and the density of states of charge carriers from TSL information obtained on various types of substances . The results are compared with those achieved by other methods such as photoluminescence excitation spectroscopy or Raman absorption .In particular we prove that our technique permits one to estimate the energy divide between the conduction band threshold and valence band maximum in semiconductors . This project was supported by Russian Science Foundation award No .14-50-00040.DOI: 10.1063/1.4935190 \nI.INTRODUCTORY REMARK The investigation of luminescence effects has been drawing great popularity over numerous years because it gives valuable info about electronic structure and electronic properties of solids 1 . Thermal stimulation luminescence ( TSL ) , sometimes called as optically stimulated luminescence ( OSL ) , is especially interesting since it allows us to probe the distribution behavior of atoms excited into the conduction band 2 .In past decades there have been numerous attempts to develop conceptual models explaining various parts of luminescence systems 3 , notably heat stimulation luminescence 4 - 8 . However , most of these works were based on the assumption that the process under consideration is usually nearly to equilibrium 9 .As a result they cannot describe correctly some important features detected experimentally 10 . For instance , the form of the TSL flicker circle depends strongly on the kind of material 11 : while in insulators it often features a single peak 12 , in metals it often consists of several peaks 13 .Moreover , even within the same category of substances , e . g . , semiconductor crystals 14 , the quantity of peaks may differ depending on the doping level 15 . These measurements cannot be understood using existing theories 16 .",
        "rewrite_text": "**Title:** Analysis of Thermally Stimulated Luminescence and Conductivity Without Quasiequilibrium Approximation\n\n**Abstract:** In this study, we investigate the temperature-dependent behavior of thermally stimulated luminescence (TSL) glow curves through the lens of the nonstationary electron-phonon relaxation hypothesis, which diverges from traditional models by not presuming that the system remains close to equilibrium throughout its evolution. This innovative approach allows us to extract critical information regarding the phonon spectrum and the density of states of charge carriers from TSL data collected across various materials. Our findings are juxtaposed with results obtained from alternative techniques, such as photoluminescence excitation spectroscopy and Raman absorption, highlighting the robustness of our methodology. Notably, we demonstrate that our technique is capable of estimating the energy gap between the conduction band minimum and the valence band maximum in semiconductor materials. This research is supported by the Russian Science Foundation under award No. 14-50-00040. \n\n**I. INTRODUCTORY REMARKS** The exploration of luminescence phenomena has gained significant traction over the years due to its potential to provide insights into the electronic structure and properties of solids. Thermal stimulation luminescence (TSL), also referred to as optically stimulated luminescence (OSL), is particularly intriguing as it enables the investigation of the distribution of atoms excited into the conduction band. Despite numerous attempts over the past decades to formulate conceptual models that elucidate various aspects of luminescence systems, particularly heat stimulation luminescence, most existing frameworks have relied on the assumption of near-equilibrium conditions. Consequently, these models often fail to accurately capture critical experimental observations. For example, the shape of the TSL glow curve is heavily influenced by the type of material: insulators typically exhibit a single peak, while metals may present multiple peaks. Furthermore, variations in peak count can occur even within the same category of materials, such as semiconductor crystals, depending on their doping levels. These discrepancies underscore the limitations of current theories in explaining the complex behavior observed in TSL measurements.",
        "ori-fast-z-score": -2.3757725695052176,
        "water-fast-z-score": 7.3484692283495345,
        "rewrite-fast-z-score": -0.7579367289598671
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Near-Infrared Spectra of the Black Hole X-Ray Binary, A0620-00 .\nAbstract:\nWe present near-infrared (NIR) spectroscopy of the black hole X-ray binary system A0620-00 obtained with the Subaru Telescope and Gemini Observatory in 2001-2002. The NIR spectrum shows strong emission lines originating from highly ionized species such as FeXXV-XXVI, NeIX-XI, MgXI-XII, SiXIV-XVII, SXV-XXIII, ArXVIII-XXIV, CaXIX-XXVIII, and NiXXI-XXIV. We find that these features are well reproduced by our photoionization model calculations for an accretion disk irradiated by hard X-rays emitted from the central source. In addition to the above mentioned ions, we also detect absorption lines due to neutral hydrogen and helium at wavelengths longer than 1 micron. These absorptions originate from the interstellar medium along the line-of-sight toward this object. \n \n Keywords: Near-infrared spectroscopy",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Near - Infrared Spectra of the Black Hole X - Ray Binary , A0620 - 00 . Abstract : We report near - infrared ( NIR ) spectroscopy of the dark hole X - ray binary system A0620 - 00 collected with the Subaru Telescope and Gemini Observatory in 2001 - 2002 .The NIR spectrum displays strong radiation patterns derived from highly ionized species such as FeXXV - XXVI , NeIX - XI , MgXI - XII , SiXIV - XVII , SXV - XXIII , ArXVIII - XXIV , CaXIX - XXVIII , and NiXXI - XXIV . We see that these characteristics are better illustrated by our photoionization profile measurements for an accretion disk irradiated by hard X - rays generated from the main source .In addition to the above mentioned ions , we also observe absorption lines owing to neutral hydrogen and helium at wavelengths greater than 1 micron . These absorptions originate from the interstellar medium along the line - of - view toward this object .Keywords: Near-infrared spectroscopy",
        "rewrite_text": "We present a comprehensive study of the near-infrared (NIR) spectroscopy of the black hole X-ray binary system A0620-00, conducted using the Subaru Telescope and Gemini Observatory during the years 2001 to 2002. Our analysis reveals a rich NIR spectrum characterized by prominent emission lines from a variety of highly ionized elements, including FeXXV - XXVI, NeIX - XI, MgXI - XII, SiXIV - XVII, SXV - XXIII, ArXVIII - XXIV, CaXIX - XXVIII, and NiXXI - XXIV. These spectral features are indicative of the intense radiation environment present in the vicinity of the black hole, particularly from the accretion disk that is being heated by hard X-rays emitted from the central source. \n\nFurthermore, our findings are enhanced by photoionization profile measurements, which provide a clearer understanding of the ionization processes occurring within the accretion disk. In addition to the emission lines from ionized species, we also detect absorption features attributed to neutral hydrogen and helium at wavelengths exceeding 1 micron. These absorption lines are a result of the interstellar medium along the line of sight to A0620-00, contributing additional context to the observed NIR spectrum.\n\nThis study not only sheds light on the physical conditions surrounding the black hole but also emphasizes the significance of NIR spectroscopy in probing the complex interactions within X-ray binary systems. The results underscore the importance of continued observational efforts in this wavelength range to further unravel the mysteries of black hole accretion processes and the surrounding environment. \n\nKeywords: Near-infrared spectroscopy, black hole X-ray binaries, A0620-00, photoionization, interstellar medium.",
        "ori-fast-z-score": -2.1773242158072694,
        "water-fast-z-score": 3.3709993123162105,
        "rewrite-fast-z-score": 0.5720775535473553
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Extremal black holes in D=5: SUSY vs. Gauss-Bonnet corrections .\nAbstract:\nWe study the effects on the entropy and Hawking temperature of extremal black holes due to higher derivative terms in the action, namely those coming from supersymmetric (SUSY) or quadratic curvature invariants such as the Gauss-Bonnet term. We find that for both types of corrections there is an enhancement of the entropy at low temperatures which can be understood by studying the near-horizon geometry of these solutions. In addition we show how this behavior changes when one considers different values of the coupling constants associated with each type of correction. Finally, we discuss some possible implications of our results within string theory. The thermodynamics of black holes has been studied extensively over the past few years  1  . One of the most interesting aspects of this subject concerns the possibility of having quantum gravity corrections to their classical description  2  , especially since it was shown recently  3  that they could have important consequences even if they are small compared to other physical scales involved in the problem. For example, it has been suggested  4  that the inclusion of certain quantum gravitational corrections may lead to a resolution of the information paradox  5  .\nIn particular, it seems reasonable to expect that the entropy of a black hole should receive contributions not only from its horizon area but also from additional degrees of freedom located near the singularity  6  . This idea leads naturally to consider modifications of Einstein s equations involving higher order derivatives  7, 8  . However, although many authors have considered various forms of higher-order corrections  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59  , very little attention has been paid so far  60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Extremal black holes in D = 5 : SUSY vs . Gauss - Bonnet corrections . Abstract : We research the effects on the entropy and Hawking temperature of extremal black holes due to higher derivative terms in the activity , particularly those coming from supersymmetric ( SUSY ) or quadratic curvature invariants such as the Gauss - Bonnet term .We see that for both types of corrections there is an enhancement of the entropy at low temperatures which can be understood by examining the near - horizon morphology of these solutions . In addition we find how this behavior changes when one measures different values of the interaction constants associated with each type of correction .Finally , we explain some possible possibilities of our findings within string theory . The thermodynamics of black holes has been studied frequently over the previous few years 1 .One of the most exciting aspects of this question concerns the prospect of having quantum gravitational corrections to their classical description 2 , particularly since it was shown recently 3 that they may have important implications even if they are small compared to other physical scales implicated in the issue . For instance , it has been proposed 4 that the introduction of certain quantum gravitational corrections might lead to a resolution of the information paradox 5 .In particular , it appears justified to predict that the entropy of a black hole should receive benefits not only from its horizon area but also from additional degrees of freedom placed near the singularity 6 . This idea takes readily to consider modifications of Einstein s coefficients involving higher order derivatives 7 , 8 .However , although many writers have called various forms of greater - order corrections 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 , 31 , 32 , 33 , 34 , 35 , 36 , 37 , 38 , 39 , 40 , 41 , 42 , 43 , 44 , 45 , 46 , 47 , 48 , 49 , 50 , 51 , 52 , 53 , 54 , 55 , 56 , 57 , 58 , 59 , very less attention has been paid so far 60 , 61 , 62 , 63 , 64 , 65 , 66 , 67 , 68 , 69 , 70 , 71 ,",
        "rewrite_text": "**Title:** Extremal Black Holes in D = 5: SUSY vs. Gauss-Bonnet Corrections\n\n**Abstract:** This study investigates the impact of higher derivative terms on the entropy and Hawking temperature of extremal black holes, specifically focusing on contributions from supersymmetry (SUSY) and quadratic curvature invariants, such as the Gauss-Bonnet term. Our findings reveal that both types of corrections lead to an increase in entropy at low temperatures, a phenomenon that can be elucidated by analyzing the near-horizon structure of these black hole solutions. Furthermore, we explore how variations in the interaction constants associated with each correction type influence this behavior. \n\nThe thermodynamics of black holes has garnered significant attention in recent years, particularly regarding the potential for quantum gravitational corrections to enhance their classical descriptions. Recent studies suggest that even minor quantum gravitational effects could have substantial implications for black hole physics, including the resolution of the information paradox. It is posited that the entropy of a black hole may derive not only from the area of its event horizon but also from additional degrees of freedom located near the singularity. This perspective naturally leads to the consideration of modifications to Einstein's equations that incorporate higher-order derivative terms.\n\nDespite the extensive literature on various forms of higher-order corrections, there remains a notable gap in the exploration of their specific effects on black hole thermodynamics. Our research aims to fill this gap by providing a detailed analysis of how SUSY and Gauss-Bonnet corrections alter the thermodynamic properties of extremal black holes in five-dimensional spacetime. We also discuss the implications of our results within the framework of string theory, highlighting potential avenues for future research in this intriguing area of theoretical physics.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 8.252415537569858,
        "rewrite-fast-z-score": 1.3926212476455828
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A remark on the number of steady states in a multiple futile cycle .\nAbstract:\nWe consider a model for an enzymatic reaction network with multiple futile cycles, where each enzyme is assumed to be present at constant concentration and all reactions are reversible. We show that there exists only one positive equilibrium point if the total amount of enzymes is sufficiently large compared to the total amount of substrate molecules. This result can also be applied to other models such as those describing metabolic networks or gene regulatory systems. The proof relies on the fact that the system has a unique globally asymptotically stable equilibrium point when it is considered under mass action kinetics. In addition we prove that this equilibrium point is locally exponentially stable even though the system does not satisfy the classical Lipschitz condition. Finally, numerical simulations illustrate our results. Keywords: Enzymatic reaction networks; Mass action; Steady state analysis; Metabolic networks; Gene regulatory systems. 1 Introduction Reaction networks have been widely used to describe biochemical processes occurring inside living cells (see e.g.,  1  ,  4  ). These networks consist of chemical species which interact through chemical reactions. A mathematical description of these interactions leads to a set of ordinary differential equations known as the kinetic equations. For example, the Michaelis-Menten mechanism describes how an enzyme E binds reversibly to its substrate S to form a complex C before releasing product P . It consists of three elementary reactions given by \nwhere k + i and k − i denote respectively the forward and backward rate constants associated with the ith reaction. If the concentrations of the reactants and products involved in the above scheme are denoted by  S  ,  E  ,  P   and  C  then the corresponding kinetic equations read dS dt = k 2  E  S  − k −1  S ,\ndE dt = k 3  E  P   − k −2  E ,\n\ndC dt = k 4  C  P   − k −3  C .\n\nThe parameters k i represent the rates of the different reactions. Note that the first two equations correspond to the formation of complexes while the last equation corresponds to their dissociation into free substrates and products.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A statement on the quantity of stable states in a multiple futile period . Abstract : We consider a description for an enzymatic process network with many futile periods , where each enzyme is expected to be found at fixed concentration and all processes are reversible .We see that there exists only one positive equilibrium point if the total quantity of enzymes is sufficiently huge compared to the total quantity of substrate molecules . This result can also be applied to other models such as those describing metabolic networks or protein regulatory structures .The confirmation relies on the fact that the system has a unique globally asymptotically stable equilibrium point when it is viewed under mass action kinetics . In addition we prove that this equilibrium point is locally exponentially steady even though the system does not satisfy the classical Lipschitz condition .Finally , numerical simulations highlight our findings . Keywords : Enzymatic reaction systems ; Mass response ; Steady state analysis ; Metabolic systems ; Gene regulatory structures .1 Introduction Reaction networks have been widely using to define biochemical mechanisms occurring inside live cells ( see e . g . , 1 , 4 ) . These connections comprise of biological species which interact through chemical processes .A mathematical description of these interactions leads to a setting of ordinary differential equations known as the kinetic equations . For instance , the Michaelis - Menten process represents how an enzyme E connects reversibly to its substrate S to form a complex C before producing product P .It consists of three elementary reactions given by where k + i and k − i describe respectively the forward and backward rate constants associated with the ith reaction . If the levels of the reactants and products participating in the above scheme are denoted by S , E , P and C then the equivalent kinetic equations read dS dt = k 2 E S − k −1 S , dE dt = k 3 E P − k −2 E , dC dt = k 4 C P − k −3 C .The parameters k i describe the rates of the different processes . Note that the first two variables relate to the formation of complexes while the last equation relates to their dissociation into free substrates and products .",
        "rewrite_text": "**Title:** A Statement on the Quantity of Stable States in a Multiple Futile Period\n\n**Abstract:** This study presents a comprehensive analysis of an enzymatic process network characterized by multiple futile cycles, where each enzyme operates at a constant concentration and all reactions are reversible. Our findings reveal that when the total enzyme concentration significantly exceeds that of the substrate molecules, the system converges to a unique positive equilibrium point. This result has broader implications, extending to various models that describe metabolic networks and protein regulatory mechanisms. The uniqueness of this equilibrium is established through an examination of the system under mass action kinetics, which demonstrates that it possesses a globally asymptotically stable equilibrium point. Furthermore, we provide evidence that this equilibrium is locally exponentially stable, despite the system's failure to meet the traditional Lipschitz condition. To substantiate our theoretical results, we conduct numerical simulations that effectively illustrate the behavior of the system and reinforce our conclusions. The implications of our work are significant for understanding the dynamics of enzymatic reaction systems, particularly in the context of metabolic and gene regulatory networks. Our research contributes to the ongoing discourse in biochemical modeling by elucidating the conditions under which stable states can be achieved in complex enzymatic systems. \n\n**Keywords:** Enzymatic reaction systems; Mass action kinetics; Steady state analysis; Metabolic networks; Gene regulatory mechanisms. \n\n**1 Introduction:** Reaction networks serve as fundamental frameworks for elucidating the biochemical processes that occur within living cells. These networks consist of various biological species that interact through chemical reactions, which can be mathematically represented by a set of ordinary differential equations known as kinetic equations. A classic example is the Michaelis-Menten mechanism, which describes the reversible interaction between an enzyme (E) and its substrate (S) to form a complex (C) that ultimately produces a product (P). This mechanism is characterized by three elementary reactions, with forward and backward rate constants denoted as k+i and k−i, respectively. The dynamics of the reactants and products can be captured by kinetic equations that describe the rates of change of S, E, P, and C, highlighting the intricate balance between complex formation and dissociation.",
        "ori-fast-z-score": -2.3284515771189986,
        "water-fast-z-score": 6.230769230769231,
        "rewrite-fast-z-score": 1.2493900951088486
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Tracing mixing in stars: new beryllium observations of the open clusters NGC 2516, Hyades, and M67 .\nAbstract:\nWe present high-resolution optical spectroscopy for three open clusters: NGC 2516 (age = 1 Gyr), Hyades (600 Myr) and M67 (3 Gyr). We use these data to measure the abundance ratios of C/Fe, N/Fe, O/Fe, Mg/Fe, Si/Fe, S/Fe, Ca/Fe, Ti/Fe, Cr/Fe, Mn/Ca, Ni/Co, Y/Ba, Zr/Hf, Sr/Y, Ba/Sr, La/Lu, Ce/Pb, Eu/Gd, Yb/Tm, Lu/Hf, W/Mn, Th/U, and U/Th. These measurements are used as input into galactic chemical evolution models that trace the time-evolution of the abundances of all elements heavier than helium produced by nuclear fusion reactions within massive stars. The results show that the observed trends can be explained if we assume that the initial mass function is bimodal with peaks at 8-10 solar masses and 2-3 solar masses.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Tracing mixing in stars : latest beryllium measurements of the open complexes NGC 2516 , Hyades , and M67 . Abstract : We create high - resolution optical spectroscopy for three open complexes : NGC 2516 ( age = 1 Gyr ) , Hyades ( 600 Myr ) and M67 ( 3 Gyr ) .We use these information to measure the availability proportions of C / Fe , N / Fe , O / Fe , Mg / Fe , Si / Fe , S / Fe , Ca / Fe , Ti / Fe , Cr / Fe , Mn / Ca , Ni / Co , Y / Ba , Zr / Hf , Sr / Y , Ba / Sr , La / Lu , Ce / Pb , Eu / Gd , Yb / Tm , Lu / Hf , W / Mn , Th / U , and U / Th . These measurements are using as input into galactic chemical evolution models that trace the period - progression of the abundances of all atoms heavier than helium produced by nuclear fusion reactions within large galaxies .The results show that the seen variations can be described if we suppose that the early mass function is bimodal with peaks at 8 - 10 solar masses and 2 - 3 solar masses .",
        "rewrite_text": "Title: Tracing Mixing in Stars: Recent Beryllium Measurements of the Open Clusters NGC 2516, Hyades, and M67\n\nAbstract: In this study, we present high-resolution optical spectroscopy data for three open clusters: NGC 2516 (age = 1 Gyr), Hyades (age = 600 Myr), and M67 (age = 3 Gyr). Our analysis focuses on determining the abundance ratios of various elements, specifically C/Fe, N/Fe, O/Fe, Mg/Fe, Si/Fe, S/Fe, Ca/Fe, Ti/Fe, Cr/Fe, Mn/Ca, Ni/Co, Y/Ba, Zr/Hf, Sr/Y, Ba/Sr, La/Lu, Ce/Pb, Eu/Gd, Yb/Tm, Lu/Hf, W/Mn, Th/U, and U/Th. These abundance measurements serve as critical inputs for galactic chemical evolution models, which aim to trace the evolution of elemental abundances of all atoms heavier than helium that are synthesized through nuclear fusion processes in large galaxies. Our findings indicate that the observed variations in elemental abundances can be effectively explained by positing a bimodal initial mass function, characterized by peaks at approximately 8-10 solar masses and 2-3 solar masses. This bimodal distribution suggests a complex interplay of stellar evolution processes that influence the mixing and distribution of elements within these clusters. By integrating our measurements into existing models, we enhance the understanding of the chemical evolution of stars and the broader implications for galactic formation and development. The results of this research contribute valuable insights into the nucleosynthesis processes occurring in stars and the subsequent enrichment of the interstellar medium, thereby advancing the field of astrophysics and our comprehension of stellar populations.",
        "ori-fast-z-score": -1.6035674514745464,
        "water-fast-z-score": 3.0464244212496006,
        "rewrite-fast-z-score": 0.20412414523193154
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Scanning-probe spectroscopy of semiconductor donor molecules .\nAbstract:\nWe report on scanning tunneling microscopy and spectroscopy (STM/S) studies of the electronic structure of individual Si donors in silicon dioxide grown by chemical vapor deposition at low temperatures.  The STM images show that the Si atoms are incorporated into molecular structures with an apparent height of 1 nm, which is consistent with previous reports for SiO$_2$ films prepared under similar conditions. We find that these molecules have a characteristic spectroscopic signature consisting of two peaks separated by about 0.5 eV in dI/dV spectra recorded over them. These features can be explained as resulting from hybridization between the localized states associated with each Si atom within the molecule. In addition to this double-peak feature we observe another peak located around -0.3 V bias voltage, whose origin remains unclear. Finally, we discuss possible mechanisms responsible for the formation of such Si-donor molecules. Scanning probe techniques provide unique insight into the local properties of materials. Herein, we present results obtained using scanning tunneling microscopy/spectroscopy (STM/STS), which reveal the electronic structure of individual silicon donors embedded in amorphous silicon dioxide layers deposited onto highly doped p-type silicon substrates. Our experiments were performed in ultrahigh vacuum chambers equipped with standard facilities for sample preparation and characterization.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Scanning - probe spectroscopy of semiconductor donation molecules . Abstract : We report on scan tunneling microscopy and spectroscopy ( STM / S ) research of the electronic configuration of individual Si donors in silicon dioxide grown by molecular vapor precipitation at low temperatures .The STM pictures show that the Si atoms are incorporated into molecular complexes with an apparent size of 1 nm , which is consistent with previous findings for SiO $ _ 2 $ films prepared under similar situations . We see that these structures have a peculiar spectroscopic profile consisting of two peaks separated by about 0 . 5 eV in dI / dV spectra recorded over them .These features can be understood as occurring from hybridization between the localized states associated with each Si molecular within the molecule . In addition to this double - peak feature we study another peak located around - 0 . 3 V bias frequency , whose identity remains obscure .Finally , we explain possible processes responsible for the formation of such Si - donor molecules . Scanning probe methods provide unique insight into the local characteristics of substances .Herein , we present results acquired using scan tunneling microscopy / spectroscopy ( STM / STS ) , which expose the electronic configuration of individual silicon donors embedded in amorphous silicon dioxide sheets deposited onto highly doped p - class silicon substrates . Our experiments were performed in ultrahigh vacuum chambers equipped with typical laboratories for sample preparation and determination .",
        "rewrite_text": "Title: Scanning-Probe Spectroscopy of Semiconductor Donor Molecules\n\nAbstract: In this study, we investigate the electronic properties of individual silicon (Si) donor molecules embedded in silicon dioxide (SiO₂) using scanning tunneling microscopy and spectroscopy (STM/STS). The SiO₂ films were synthesized through molecular vapor deposition at low temperatures, and our STM images reveal that Si atoms are integrated into molecular complexes with an approximate size of 1 nm. This observation aligns with prior research on SiO₂ films produced under similar conditions. Notably, the spectroscopic analysis of these structures reveals a distinctive profile characterized by two peaks, approximately 0.5 eV apart, in the differential conductance (dI/dV) spectra. These peaks are attributed to the hybridization of localized states associated with each Si atom within the molecular complex. Additionally, we identify another peak around -0.3 V in the bias spectrum, the origin of which remains unclear and warrants further investigation. We also discuss potential mechanisms that could lead to the formation of these Si donor molecules. The application of scanning probe techniques offers valuable insights into the local electronic characteristics of materials. Our findings, derived from experiments conducted in ultrahigh vacuum environments equipped with advanced sample preparation and characterization facilities, contribute to a deeper understanding of the electronic configurations of individual silicon donors within amorphous SiO₂ layers deposited on highly doped p-type silicon substrates. This research not only enhances our comprehension of semiconductor donor systems but also paves the way for future studies aimed at exploring the implications of these findings in the context of semiconductor technology and quantum computing.",
        "ori-fast-z-score": -1.5852581740085334,
        "water-fast-z-score": 6.127946159842712,
        "rewrite-fast-z-score": 0.6260990336999411
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Helioseismic analysis of the solar flare-induced sunquake of 2005 January 15 .\nAbstract:\nWe report on helioseismic observations made by GOLF and MDI instruments aboard SOHO spacecraft during the largest recorded solar flare in recent years, which occurred on 2005 Jan. 15 (Solar Flare Event #11). The event produced an intense seismic signal with a duration of about 20 minutes that was detected simultaneously at two different frequencies corresponding to acoustic waves traveling along opposite directions across the Sun s surface. We find that this signal is consistent with a source located near the center of the active region NOAA 10486 where the flare took place. This result suggests that the energy released by the flare may have been channeled into the generation of strong toroidal magnetic fields through the action of plasma flows driven by the Lorentz force. These results are discussed within the framework of current models for solar flares. \n \n Keywords: Solar flare, seismology, sunquake \n \n 1 Introduction \n \n Intense solar flares can release huge amounts of energy over very short timescales. It has recently become possible to study these events using space-based observatories such as the Solar and Heliospheric Observatory (SOHO)  1  . During large solar flares, it is often observed that there is a significant increase in the intensity of the photospheric Doppler velocity field  2  , which indicates that the photosphere undergoes rapid motions associated with the eruption of coronal mass ejections  3  . However, the exact physical mechanisms responsible for driving these phenomena remain poorly understood  4  .\n \nIn addition to their effects on the photospheric flow velocities, solar flares also produce powerful seismic signals known as  sunquakes   5  . These signals were first discovered by Leighton et al  6  who used ground-based measurements of the Doppler shift of the Fraunhofer lines in the visible spectrum of sunlight reflected off the Moon. Since then, several other groups  7, 8  have reported similar detections based on data obtained either from ground-based or spacebased telescopes operating in various parts of the electromagnetic spectrum  9  . More recently, Kosovichev",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Helioseismic assessment of the solar flare - caused sunquake of 2005 January 15 . Abstract : We report on helioseismic measurements made by GOLF and MDI instruments aboard SOHO satellites during the greatest documented solar flare in recent seasons , which occurred on 2005 Jan . 15 ( Solar Flare Event # 11 ) .The event produced an strong seismic signal with a duration of about 20 minutes that was detected simultaneously at two different frequencies corresponding to acoustic waves riding along opposite directions across the Sun s surface . We see that this signal is compatible with a source located near the center of the active region NOAA 10486 where the flare took place .This result suggests that the electricity created by the flare might have been channeled into the generation of large toroidal magnetic fields through the action of plasma flows driven by the Lorentz force . These conclusions are discussed within the framework of recent estimates for solar flares .Keywords : Solar flare , seismology , sunquake 1 Introduction Intense sun flares can discharge enormous amounts of electricity over very brief timescales . It has recently become able to study these events utilizing space - based observatories such as the Solar and Heliospheric Observatory ( SOHO ) 1 .During large solar flares , it is often observed that there is a substantial rise in the strength of the photospheric Doppler velocity field 2 , which implies that the photosphere undergoes fast motions associated with the eruption of coronal mass ejections 3 . However , the exact physical mechanisms involved for driving these phenomena remain poorly studied 4 .In addition to their impact on the photospheric flow velocities , sun flares additionally produce violent seismic signals dubbed as sunquakes 5 . These transmissions were first discovered by Leighton et al 6 who used ground - based measurements of the Doppler shift of the Fraunhofer lines in the visible spectrum of sunlight reflected off the Moon .Since then , various other groups 7 , 8 have reported similar detections based on evidence derived either from land - based or spacebased telescopes located in different regions of the electromagnetic spectrum 9 . More recently , Kosovichev",
        "rewrite_text": "**Title:** Helioseismic Analysis of the Sunquake Induced by the Solar Flare on January 15, 2005\n\n**Abstract:** This study presents an in-depth analysis of helioseismic data collected by the GOLF and MDI instruments on the SOHO satellites during one of the most significant solar flare events recorded in recent history, which occurred on January 15, 2005 (Solar Flare Event #11). The flare generated a robust seismic signal lasting approximately 20 minutes, detected at two distinct frequencies that correspond to acoustic waves propagating in opposite directions across the solar surface. Our findings indicate that the seismic signal originated from a location near the center of the active region NOAA 10486, where the flare was situated. This observation implies that the electrical energy released during the flare may have contributed to the formation of extensive toroidal magnetic fields, facilitated by plasma flows influenced by the Lorentz force. We contextualize these results within the framework of contemporary models of solar flare dynamics and their associated phenomena. \n\nThe occurrence of intense solar flares can release vast quantities of energy in a very short time, and advancements in space-based observatories, such as the Solar and Heliospheric Observatory (SOHO), have enabled detailed studies of these events. During significant solar flares, a notable increase in the photospheric Doppler velocity field is often observed, indicating rapid motions in the photosphere linked to coronal mass ejections. However, the underlying physical mechanisms driving these processes remain inadequately understood. In addition to influencing photospheric flow velocities, solar flares also generate powerful seismic disturbances known as sunquakes. The phenomenon of sunquakes was first identified by Leighton et al. through ground-based observations of Doppler shifts in sunlight reflected off the Moon. Subsequent studies have corroborated these findings using data from both terrestrial and space-based telescopes across various electromagnetic wavelengths. Recent contributions from researchers like Kosovichev have further enriched our understanding of these complex solar events. \n\n**Keywords:** Solar flare, seismology, sunquake.",
        "ori-fast-z-score": 0.24096579867074966,
        "water-fast-z-score": 7.195223542744546,
        "rewrite-fast-z-score": 0.5895063447465633
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A mathematical and computational review of Hartree-Fock SCF methods in Quantum Chemistry .\nAbstract:\nThe aim of this thesis is to provide an overview on the state-of-the-art Hartree-Fock Self-Consistent-Field (SCF) methods for solving quantum chemical problems, with special emphasis on their numerical aspects.  The first chapter introduces basic concepts related to molecular orbital theory and electronic structure calculations using density functional theory (DFT). In particular, we discuss how DFT can be used as a tool to study ground-state properties of molecules by means of Kohn-Sham orbitals. We also present some fundamental results concerning the convergence of iterative schemes that are commonly employed within self-consistent field approaches. The second chapter deals with the description of several classes of algorithms based on direct minimization techniques which have been developed over the last decades to solve the Hartree-Fock equations numerically. These include the Roothaan-Hall method, the Davidson algorithm, and its variants such as the Pulay-Davidson scheme or the linearized Davidson approach. Finally, we introduce the concept of preconditioning and illustrate it through two examples.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A mathematical and mathematical discussion of Hartree - Fock SCF methods in Quantum Chemistry . Abstract : The goal of this dissertation is to provide an overview on the state - of - the - art Hartree - Fock Self - Consistent - Field ( SCF ) techniques for solving quantum chemical problems , with special emphasis on their numerical parts .The first section introduces basic concepts related to molecular orbital theory and electronic stability analyses using density functional theory ( DFT ) . In particular , we explain how DFT can be used as a technique to study ground - state properties of molecules by means of Kohn - Sham orbitals .We additionally offer some fundamental findings concerning the convergence of iterative strategies that are often employed within self - coherent field methods . The second chapter deals with the description of several classes of algorithms based on direct minimization techniques which have been built over the last decades to solve the Hartree - Fock equations numerically .These include the Roothaan - Hall method , the Davidson approximation , and its versions such as the Pulay - Davidson scheme or the linearized Davidson technique . Finally , we introduce the idea of preconditioning and explain it through two examples .",
        "rewrite_text": "Title: A Comprehensive Examination of Hartree-Fock Self-Consistent Field Methods in Quantum Chemistry\n\nAbstract: This dissertation aims to present a thorough overview of the advanced Hartree-Fock Self-Consistent Field (SCF) methods utilized in addressing quantum chemical challenges, with a particular focus on their numerical implementations. The initial section lays the groundwork by introducing essential concepts related to molecular orbital theory and the analysis of electronic stability through density functional theory (DFT). We delve into the application of DFT as a powerful tool for investigating the ground-state properties of molecules, utilizing Kohn-Sham orbitals as a framework for understanding electronic structures. Additionally, we highlight key insights regarding the convergence behavior of iterative methods commonly employed in self-consistent field approaches. \n\nIn the subsequent chapter, we explore various algorithmic classes that have been developed over recent decades to numerically solve the Hartree-Fock equations through direct minimization techniques. This includes an in-depth discussion of the Roothaan-Hall method, the Davidson approximation, and its adaptations, such as the Pulay-Davidson scheme and the linearized Davidson technique. These algorithms represent significant advancements in computational approaches to quantum chemistry, enhancing the efficiency and accuracy of calculations. \n\nFinally, we introduce the concept of preconditioning, elucidating its importance in improving the convergence of numerical methods. We illustrate this concept through two practical examples, demonstrating how preconditioning can optimize the performance of SCF methods. This dissertation not only serves as a comprehensive resource for understanding Hartree-Fock SCF techniques but also contributes to the ongoing discourse in the field of quantum chemistry by addressing both theoretical and computational aspects.",
        "ori-fast-z-score": 0.6324555320336759,
        "water-fast-z-score": 6.324555320336758,
        "rewrite-fast-z-score": 0.5432144762551112
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Transition from the First Stars to the Second Stars in the Early Universe .\nAbstract:\nWe present an analysis of the transition between first stars and second stars, which are formed by gravitational collapse of primordial gas clouds with masses ranging from 10^4 M_sol to 10^6 M_sol.  We show that the formation rate of second stars is suppressed at redshifts z < 20 due to photoheating effects on the intergalactic medium (IGM). The suppression factor increases as redshift decreases because the IGM temperature rises more rapidly than its density. At lower redshifts, we find that the formation rates of both first and second stars increase sharply when the universe becomes reionized. This effect occurs because the ionizing photons produced during reionization heat up the surrounding neutral hydrogen atoms, thereby increasing their Jeans mass and suppressing fragmentation into smaller objects. Finally, we estimate the number densities of first and second stars using our model for star formation history. Our results suggest that second stars may be detectable via future surveys such as LSST or Euclid.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Transition from the First Stars to the Second Stars in the Early Universe . Abstract : We present an assessment of the shift between first stars and second stars , which are created by gravitational collapse of primordial liquid clouds with masses ranging from 10 ^ 4 M _ sol to 10 ^ 6 M _ sol .We see that the formation rate of second stars is suppressed at redshifts z < 20 due to photoheating effects on the intergalactic medium ( IGM ) . The suppression factor rises as redshift decreases because the IGM temperature rises more swiftly than its density .At lower redshifts , we find that the formation rates of both first and first stars increase dramatically when the universe becomes reionized . This phenomenon occurs because the ionizing photons created during reionization heat up the nearby neutral hydrogen atoms , thereby expanding their Jeans mass and suppressing fragmentation into larger objects .Finally , we estimate the number densities of first and first stars using our model for star formation history . Our results propose that second stars would be detectable via upcoming studies such as LSST or Euclid .",
        "rewrite_text": "In this article, we explore the transition from the first generation of stars to the second generation in the early universe, focusing on the gravitational collapse of primordial liquid clouds with masses between \\(10^4 M_{\\odot}\\) and \\(10^6 M_{\\odot}\\). Our analysis reveals that the formation rate of second stars is significantly inhibited at redshifts \\(z < 20\\), primarily due to the effects of photoheating on the intergalactic medium (IGM). As redshift decreases, we observe an increasing suppression factor, which can be attributed to the rapid rise in IGM temperature outpacing its density. \n\nAt lower redshifts, we note a marked increase in the formation rates of both first and second stars coinciding with the reionization of the universe. This increase is driven by the ionizing photons produced during the reionization process, which heat the surrounding neutral hydrogen atoms. This heating effect leads to an expansion of the Jeans mass, thereby reducing the likelihood of fragmentation into larger stellar objects. \n\nFurthermore, we provide estimates for the number densities of both first and second stars based on our model of star formation history. Our findings suggest that the second generation of stars may be observable in forthcoming astronomical surveys, such as those conducted by the Large Synoptic Survey Telescope (LSST) and the Euclid mission. This research contributes to our understanding of stellar evolution in the early universe and the complex interplay between star formation and cosmic environmental factors.",
        "ori-fast-z-score": -0.3333333333333333,
        "water-fast-z-score": 4.500334659577304,
        "rewrite-fast-z-score": 1.1881770515720091
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  CKM and Tri-bimaximal MNS Matrices in a SU(5) x (d)T Model .\nAbstract:\nWe present the results for neutrino mixing angles, CP violating phases and mass squared differences obtained by using the Fritzsch ansatz to generate the charged fermion masses within an extended supersymmetric grand unified theory based on SO(10). We find that this model can accommodate all experimental data with only one free parameter which is related to the ratio between the two vacuum expectation values of the Higgs fields responsible for breaking down the Pati-Salam gauge symmetry into the Standard Model gauge group. The CKM matrix elements are predicted as well as the Majorana phase associated with leptonic CP violation. In addition we show how the tri-bimaximal pattern observed experimentally in the lepton sector emerges naturally when the quark-lepton unification hypothesis is imposed at high energies. Finally we discuss briefly some phenomenological consequences of our scenario such as neutrinoless double beta decay and proton decay. PACS numbers: 11.30.Pb, 12.60.Cn",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : CKM and Tri - bimaximal MNS Matrices in a SU ( 5 ) x ( d ) T Model . Abstract : We present the results for neutrino mixing angles , CP violating phases and mass squared variations obtained by using the Fritzsch ansatz to produce the charged fermion masses within an extended supersymmetric grand unified theory based on SO ( 10 ) .We see that this description can handle all observation information with only one free parameter which is related to the proportion between the two vacuum expectation values of the Higgs fields responsible for breaking down the Pati - Salam gauge symmetry into the Standard Model gauge group . The CKM matrix elements are expected as well as the Majorana phase involved with leptonic CP violation .In addition we explain how the tri - bimaximal pattern found experimentally in the lepton region arises readily when the quark - lepton unification theory is imposed at high energies . Finally we talk briefly some phenomenological consequences of our scenario such as neutrinoless double alpha emission and proton emission .PACS codes : 11 . 30 . Pb , 12 . 60 . Cn",
        "rewrite_text": "Title: CKM and Tri-bimaximal MNS Matrices in a SU(5) x (d) T Model\n\nAbstract: In this study, we explore the implications of the Fritzsch ansatz for neutrino mixing angles, CP-violating phases, and mass squared differences within the framework of an extended supersymmetric grand unified theory based on SO(10). Our findings indicate that this theoretical model can successfully account for all observed data with the introduction of a single free parameter. This parameter is linked to the ratio of the two vacuum expectation values of the Higgs fields that facilitate the breaking of the Pati-Salam gauge symmetry down to the Standard Model gauge group. We derive the elements of the CKM matrix and analyze the Majorana phase associated with leptonic CP violation. Furthermore, we demonstrate how the tri-bimaximal mixing pattern, which has been confirmed through experimental observations in the lepton sector, naturally emerges when quark-lepton unification is applied at high energy scales. We also briefly discuss the phenomenological implications of our model, including potential signals such as neutrinoless double beta decay and proton decay. Our results contribute to a deeper understanding of the interplay between quark and lepton physics in the context of grand unified theories, highlighting the significance of the CKM and MNS matrices in describing the fundamental interactions of particles. The PACS codes relevant to our work are 11.30.Pb and 12.60.Cn, which pertain to symmetry and unification in particle physics.",
        "ori-fast-z-score": -0.808290376865476,
        "water-fast-z-score": 4.04145188432738,
        "rewrite-fast-z-score": 0.40406101782088427
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Prewavelet Solution to Poisson Equations .\nAbstract:\nWe present an algorithm for solving the following problem: given a function f on  0,1  and a positive integer n find a piecewise polynomial spline s with knots at 0 = t0 < t1 < ... < tn = 1 such that s(ti) = fi (i=0,...,n), where fi is some approximation of f in ti-1/2 , ti+1/2 . We show how this problem can be reduced to finding a solution to a system of linear equations Ax=b which has a unique solution if A is strictly diagonally dominant. The matrix A is sparse but not banded so we use iterative methods to solve it. In particular, we consider the conjugate gradient method applied to the normal equation associated with our system of equations. We prove convergence of this method under certain conditions. Finally, we give numerical results showing that our approach works well when compared against other approaches. This work was supported by NSF grant DMS-0504520.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Prewavelet Solution to Poisson Equations . Abstract : We present an algorithm for solving the following task : given a function h on 0 , 1 and a positive integer n get a piecewise polynomial spline s with knots at 0 = t0 < t1 < . . . < tn = 1 such that s ( ti ) = fi ( i = 0 , . . . , n ) , where fi is some approximation of f in ti - 1 / 2 , ti + 1 / 2 .We see how this question can be reduced to finding a solution to a system of linear equations Ax = b which has a unique solve if A is strictly diagonally dominant . The matrix A is sparse but not banded so we using iterative techniques to solve it .In particular , we define the conjugate gradient technique applied to the normal equation involved with our system of equations . We establish convergence of this method under certain conditions .Finally , we give quantitative results confirming that our approach works well when compared against other methods . This research was supported by NSF grant DMS - 0504520 .",
        "rewrite_text": "In this article, we introduce a novel algorithm designed to address the problem of constructing a piecewise polynomial spline, denoted as \\( s \\), based on a given function \\( h \\) defined on the interval [0, 1] and a specified positive integer \\( n \\). The objective is to create a spline with knots positioned at \\( 0 = t_0 < t_1 < \\ldots < t_n = 1 \\) such that the spline satisfies the condition \\( s(t_i) = f_i \\) for \\( i = 0, \\ldots, n \\), where \\( f_i \\) represents an approximation of \\( f \\) at the midpoints \\( t_{i-1/2} \\) and \\( t_{i+1/2} \\). We demonstrate that this problem can be reformulated into a system of linear equations of the form \\( Ax = b \\), which guarantees a unique solution if the matrix \\( A \\) is strictly diagonally dominant. Although the matrix \\( A \\) is sparse, it does not possess a banded structure, prompting us to employ iterative methods for its resolution. Specifically, we apply the conjugate gradient method to the normal equations associated with our linear system. We establish the convergence criteria for this technique under specific conditions, ensuring its reliability. Furthermore, we present quantitative results that validate the effectiveness of our proposed method in comparison to existing approaches. This research was made possible through the support of the National Science Foundation grant DMS-0504520, highlighting the significance of funding in advancing scientific inquiry and innovation in numerical analysis and computational mathematics.",
        "ori-fast-z-score": -0.35603449745815596,
        "water-fast-z-score": 4.09644015186457,
        "rewrite-fast-z-score": -1.4439897447623107
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The SAURON project - XI. Stellar Populations from Absorption Line Strength Maps of 24 Early-Type Spirals .\nAbstract:\nWe present the results of an analysis of absorption line strength maps for 24 early-type spiral galaxies observed with the SAURON integral field spectrograph at the William Herschel Telescope, which are used to derive their stellar populations and chemical abundances. The sample is divided into two groups according to morphological type (Sa-Sab and Sb-Scd), each containing 12 objects. We find that both samples have similar mean ages but different metallicities; Sa-Sab spirals show solar or super-solar metallicity while Sb-Scd spirals tend to be sub-solar in abundance. This difference can be explained by assuming that the former group has experienced more recent star formation than the latter one. In addition we find evidence for radial age gradients within individual galaxies, where younger stars are found towards larger galactocentric radii. Finally, we compare our results with those obtained using photometric data from the Sloan Digital Sky Survey.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The SAURON work - XI . Stellar Populations from Absorption Line Strength Maps of 24 Early - Type Spirals .Abstract : We present the results of an assessment of absorption edge strength projections for 24 earliest - class spiral clusters seen with the SAURON integral field spectrograph at the William Herschel Telescope , which are using to derive their stellar populations and chemical abundances . The sample is separated into two groups based to morphological class ( Sa - Sab and Sb - Scd ) , each including 12 objects .We see that both samples have equal mean ages but different metallicities ; Sa - Sab spirals exhibit solar or super - solar metallicity while Sb - Scd spirals prefer to be sub - solar in abundance . This difference can be described by assuming that the former group has undergone more recent star formation than the latter one .In addition we find proof for radial aging gradients within individual galaxies , where smaller stars are found towards higher galactocentric radii . Finally , we compare our findings with those acquired using photometric data from the Sloan Digital Sky Survey .",
        "rewrite_text": "We present the findings from our investigation into the absorption line strength projections of 24 early-type spiral galaxies, utilizing the SAURON integral field spectrograph at the William Herschel Telescope. This study aims to derive the stellar populations and chemical abundances of these galaxies, which have been categorized into two morphological groups: Sa-Sab and Sb-Scd, each comprising 12 galaxies. Our analysis reveals that while both groups exhibit similar mean ages, they display notable differences in metallicity. Specifically, the Sa-Sab spirals demonstrate solar or super-solar metallicity levels, indicating a more recent history of star formation. In contrast, the Sb-Scd spirals tend to have sub-solar metallicity, suggesting a different evolutionary path with less recent star formation activity. Furthermore, we have identified radial aging gradients within individual galaxies, where older stellar populations are predominantly located at lower galactocentric radii, while younger stars are found at greater distances from the galactic center. This radial distribution provides insights into the star formation history and dynamical processes within these galaxies. To enhance the robustness of our findings, we also compare our results with photometric data obtained from the Sloan Digital Sky Survey, allowing for a comprehensive understanding of the stellar populations across different morphological types. Our study contributes to the broader knowledge of galaxy evolution and the factors influencing stellar population characteristics in early-type spirals.",
        "ori-fast-z-score": -2.1376670117594454,
        "water-fast-z-score": 4.612860393796698,
        "rewrite-fast-z-score": -0.944911182523068
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A multi-wavelength study of z = 3.15 Lyman-alpha emitters in the GOODS South Field .\nAbstract:\nWe have carried out an extensive spectroscopic survey for high redshift (z > 2) galaxies using the VLT/VIMOS instrument on the ESO Very Large Telescope, targeting sources selected by their strong rest-frame UV emission lines and photometric redshifts. We present here our results obtained with this sample at wavelengths ranging from radio to X-ray. The main goal is to investigate how star formation proceeds in these distant objects through detailed studies of their physical properties such as stellar masses, ages, metallicities or dust content. In particular we focus on two samples of Lyman-alpha emitting galaxies which are known to be very young systems undergoing intense bursts of star formation. Our analysis shows that they exhibit large amounts of cold gas but also significant quantities of dust. This suggests that the bulk of the observed infrared luminosity may not come directly from newly formed stars but rather from reprocessed light emitted by hot dust heated by older populations and/or AGN activity.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A multi - wavelength observation of z = 3 . 15 Lyman - alpha emitters in the GOODS South Field . Abstract : We have carried out an extensive spectroscopic study for high redshift ( z > 2 ) clusters using the VLT / VIMOS instrument on the ESO Very Large Telescope , targeting sources chosen by their weak rest - frame UV absorption patterns and photometric redshifts .We present here our findings obtained with this specimen at wavelengths ranging from radio to X - ray . The main goal is to examine how star formation operates in these distant objects through complete studies of their physical properties such as stellar masses , ages , metallicities or dust content .In particular we focus on two specimens of Lyman - alpha emitting galaxies which are known to be very young structures experiencing intense pulses of galaxy formation . Our study shows that they show large quantities of cold energy but also notable quantities of dust .This implies that the majority of the seen infrared luminosity might not come directly from newly established stars but rather from reprocessed light emitted by hot dust cooled by existing populations and / or AGN activity .",
        "rewrite_text": "We conducted a comprehensive spectroscopic investigation of high-redshift clusters (z > 2) utilizing the VLT/VIMOS instrument on the ESO Very Large Telescope. Our research focused on identifying sources characterized by their faint rest-frame UV absorption features and photometric redshifts. In this article, we present our findings across a broad range of wavelengths, from radio to X-ray. The primary objective of our study is to explore the mechanisms of star formation in these distant galaxies by thoroughly analyzing their physical characteristics, including stellar masses, ages, metallicities, and dust content. \n\nParticularly, we concentrate on two samples of Lyman-alpha emitting galaxies, which are recognized as very young structures undergoing significant bursts of star formation. Our results indicate that these galaxies possess substantial amounts of cold energy, alongside considerable dust content. This observation suggests that a significant portion of the observed infrared luminosity may not originate solely from newly formed stars. Instead, it is likely that this luminosity results from the reprocessing of light emitted by hot dust, which has been cooled by existing stellar populations and/or active galactic nucleus (AGN) activity. \n\nThis study enhances our understanding of the interplay between star formation and dust in the early universe, providing insights into the evolutionary processes of galaxies at high redshifts. By examining these Lyman-alpha emitters, we contribute to the broader knowledge of galaxy formation and the conditions that foster star formation in the early cosmos. Our findings underscore the complexity of the physical processes at play in these distant galaxies and highlight the importance of multi-wavelength observations in unraveling the mysteries of the universe's formative years.",
        "ori-fast-z-score": -0.41256849850351734,
        "water-fast-z-score": 5.9196002117260145,
        "rewrite-fast-z-score": 1.807392228230128
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Spherically symmetric problem on the brane and galactic rotation curves .\nAbstract:\nWe consider a spherically symmetric solution to Einstein s equations in five dimensions with an extra dimension compactified on S 1 /Z 2 . The bulk is assumed to be empty, while matter fields are confined to our four-dimensional world (the  brane ). We find that this model can explain the observed flatness of galactic rotation curves without introducing any new particles or exotic forms of energy density. In particular we show how the mass distribution within galaxies may arise naturally as a consequence of the geometry of space-time. This work was supported by NSF grant PHY-0456728. PACS numbers: 04.20.-q, 11.10.-z, 98.80.Cq  A fundamental question about the nature of dark matter has been whether it consists of one or more species of particle. If so, what are their masses? What interactions do they have with ordinary matter? How much dark matter does each galaxy contain? These questions motivate us to study models for which the dark matter is described by some field theory living on a higher dimensional spacetime manifold. \n \n Here we will focus on a class of solutions where the extra dimension is compactified on a circle $S^1$. Such configurations were first studied in  1  , where it was shown that if the fifth dimension is small compared to the other length scales involved then the gravitational potential felt by observers on the brane is indistinguishable from that produced by a point-like source located at the center of the sphere. However, when the size of the extra dimension becomes comparable to the radius of curvature of the brane, the gravitational force law changes dramatically  2  . \n \n In  3  , Randall and Sundrum showed that such a configuration could provide a natural explanation for the hierarchy between the weak scale and the Planck scale. They considered a 5D anti-de-Sitter space with two 3-branes embedded along its boundary. One of these branes represents our universe, while the second acts like a mirror image of ours. Matter fields are localized near either brane, but gravity propagates freely throughout the entire bulk.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Spherically symmetric question on the brane and galactic rotation curves . Abstract : We consider a spherically invariant solution to Einstein s equations in five dimensions with an additional dimension compactified on S 1 / Z 2 .The bulk is expected to be vacant , while matter fields are localized to our four - dimensional world ( the brane ) . We see that this description can describe the seen flatness of galactic rotation curves without using any new ions or exotic kinds of power concentration .In particular we study how the mass distribution within galaxies must arise naturally as a effect of the topography of space - time . This research was supported by NSF grant PHY - 0456728 .PACS scores : 04 . 20 . - q , 11 . 10 . - z , 98 . 80 . Cq A basic issue about the nature of deep material has been whether it consists of one or more species of particle . If so , what are their masses ?What encounters do they have with normal matter ? How much dark matter does each galaxy hold ?These questions motivate us to study models for which the dark matter is modeled by some field model living on a higher dimensional spacetime manifold . Here we will focus on a class of solutions where the extra dimension is compactified on a circle $ S ^ 1 $ .Such configurations were first explored in 1 , where it was shown that if the fifth dimension is tiny relative to the other length scales required then the gravitational potential felt by observers on the brane is indistinguishable from that created by a point - like source located at the center of the sphere . However , when the height of the extra dimension becomes comparable to the radius of curvature of the brane , the gravitational pressure law changes dramatically 2 .In 3 , Randall and Sundrum proved that such a configuration could give a natural explanation for the hierarchy between the weakness scale and the Planck scale . They considered a 5D anti - de - Sitter space with two 3 - branes embedded along its boundary .One of these branes reflects our universe , while the second acts like a mirror image of ours . Matter fields are localized near either brane , but gravity propagates freely throughout the entire bulk .",
        "rewrite_text": "**Title:** Spherically Symmetric Solutions on the Brane and Galactic Rotation Curves\n\n**Abstract:** In this study, we investigate a spherically symmetric solution to Einstein's equations formulated in a five-dimensional framework, where one additional dimension is compactified on the orbifold S^1/Z_2. The model posits a vacuum bulk, with matter fields confined to our four-dimensional brane. Notably, this approach successfully accounts for the observed flatness of galactic rotation curves without necessitating the introduction of new particles or exotic forms of energy concentration. We delve into the implications of this model, particularly focusing on how the mass distribution within galaxies emerges as a natural consequence of the underlying spacetime topology. \n\nThis research is driven by fundamental questions regarding the nature of dark matter, including the potential existence of multiple particle species, their respective masses, interactions with standard matter, and the overall dark matter content within galaxies. To address these inquiries, we explore models wherein dark matter is represented by a field residing on a higher-dimensional spacetime manifold. Our analysis centers on a specific class of solutions characterized by a compactified extra dimension represented as a circle, S^1. \n\nPrevious investigations have shown that when the extra dimension is significantly smaller than other relevant length scales, the gravitational potential experienced by observers on the brane closely resembles that produced by a point-like mass situated at the center of the sphere. However, as the height of the extra dimension approaches the curvature radius of the brane, the gravitational dynamics undergo a substantial transformation. The work of Randall and Sundrum has established that such configurations can provide a compelling explanation for the disparity between the weak scale and the Planck scale, utilizing a five-dimensional anti-de Sitter space with two three-dimensional branes positioned along its boundary. One brane corresponds to our universe, while the other serves as its mirror image, with matter fields localized near either brane and gravity propagating freely throughout the bulk. This research is supported by NSF grant PHY-0456728, and is categorized under PACS codes: 04.20.-q, 11.10.-z, 98.80.Cq.",
        "ori-fast-z-score": 1.8594397919452197,
        "water-fast-z-score": 8.058229640253803,
        "rewrite-fast-z-score": 2.3293360538172467
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Approximately bisimilar symbolic models for nonlinear control systems .\nAbstract:\nWe present an algorithm to compute symbolic models that are approximately bisimilar with respect to the original continuous-time systems, which can be used as abstractions in model checking and controller synthesis problems. The proposed method is based on computing approximate solutions to Hamilton-Jacobi equations using numerical methods such as finite difference or spectral collocation techniques. We show how this approach can be applied to several classes of nonlinear control systems including switched affine systems, piecewise affine systems, and hybrid automata. Finally we illustrate our results by applying them to two examples. Keywords: Approximate bisimulation, Symbolic Model Checking, Nonlinear Control Systems, Finite Difference Method, Spectral Collocation Technique. 1 Introduction In recent years there has been growing interest in developing efficient algorithms for analyzing complex dynamical systems arising in many applications ranging from biology  19, 20  , chemistry  21  , physics  22  , engineering  23  , etc.. One important problem in these areas is to verify whether certain properties hold over all possible behaviors of the system. This task requires solving infinite state reachability problems, which are known to be undecidable even for very simple classes of systems  24  . Therefore, one usually resorts to approximating the set of states reachable within some time horizon T > 0 (called the reach set) by means of simpler mathematical objects called symbolic models  25  .\nSymbolic models have been successfully employed in various contexts such as verification  26  , controller synthesis  27  , fault diagnosis  28  , and optimal control  29  among others  30  . However, most existing approaches focus only on linear dynamics  31  while ignoring the rich class of nonlinear systems  32  . Although it may seem at first glance that dealing with nonlinearities would require more computational effort than their linear counterparts, they actually pose additional challenges due to the fact that the solution space becomes much larger  33  . For example, consider the following nonlinear systeṁ x(t) = f (x(t), u(t)) y(t) = g(x(t)), where t ∈  0, ∞). If the initial condition x0 belongs to R n then the reach set",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Approximately bisimilar symbolic models for nonlinear control problems . Abstract : We present an algorithm to compute symbolic models that are approximately bisimilar with regard to the actual continuous - time systems , which can be used as abstractions in model checking and controller synthesis problems .The proposed approach is based on computing approximate solutions to Hamilton - Jacobi equations using numerical technique such as finite difference or spectral collocation method . We see how this methodology can be applied to several classes of nonlinear control networks including switched affine systems , piecewise affine systems , and hybrid automata .Finally we explain our findings by using them to two examples . Keywords : Approximate bisimulation , Symbolic Model Checking , Nonlinear Control Systems , Finite Difference Method , Spectral Collocation Technique .1 Introduction In recent years there has been growing interest in establishing efficient methods for studying complex dynamical systems emerging in different applications diverse from biology 19 , 20 , chemistry 21 , mathematics 22 , engineering 23 , etc . . One important difficulty in these fields is to confirm whether particular structures hold over all possible behaviors of the system . This job needs solving infinite state reachability challenges , which are known to be undecidable even for very simple groups of models 24 .Therefore , one usually resorts to approximating the set of states reachable within some time horizon T > 0 ( named the reach setting ) by means of simpler mathematical devices named symbolic models 25 . Symbolic models have been successfully utilized in different settings such as verification 26 , controller synthesis 27 , failure detection 28 , and optimal control 29 among others 30 .However , most existing techniques concentrate only on linear mechanics 31 while eliminating the vast class of nonlinear systems 32 . Although it could look at first glance that dealing with nonlinearities might require more mathematical effort than their linear competitors , they actually pose additional challenges due to the fact that the solve space becomes much larger 33 .For example , consider the following nonlinear [UNK] x ( t ) = f ( x ( t ) , u ( t ) ) y ( t ) = g ( x ( t ) ) , where t ∈ 0 , ∞ ) . If the initial condition x0 belongs to R n then the reach set",
        "rewrite_text": "**Title:** Approximately Bisimilar Symbolic Models for Nonlinear Control Problems\n\n**Abstract:** In this article, we introduce a novel algorithm designed to compute symbolic models that exhibit approximate bisimilarity with respect to actual continuous-time systems. These symbolic models serve as effective abstractions for applications in model checking and controller synthesis. Our approach leverages numerical techniques, specifically finite difference and spectral collocation methods, to derive approximate solutions to Hamilton-Jacobi equations. We demonstrate the applicability of this methodology across various classes of nonlinear control networks, including switched affine systems, piecewise affine systems, and hybrid automata. To illustrate the effectiveness of our findings, we present two detailed examples that highlight the practical implications of our approach. The growing interest in efficient methods for analyzing complex dynamical systems across diverse fields such as biology, chemistry, mathematics, and engineering underscores the significance of our work. One of the primary challenges in these domains is verifying whether specific properties hold for all potential behaviors of a system, which often involves tackling infinite state reachability problems that are known to be undecidable for even simple model classes. Consequently, researchers typically approximate the set of states reachable within a given time horizon (denoted as the reach set) using simpler mathematical constructs known as symbolic models. While symbolic models have proven useful in various contexts, including verification, controller synthesis, failure detection, and optimal control, most existing methodologies have focused predominantly on linear systems, thereby neglecting the extensive category of nonlinear systems. Although nonlinearities may initially appear to require more complex mathematical treatment than linear systems, they introduce additional challenges due to the significantly larger solution space. For instance, consider a nonlinear system described by the equations x(t) = f(x(t), u(t)) and y(t) = g(x(t)), where t is in the interval [0, ∞). If the initial condition x0 is within R^n, the reach set becomes a critical area of study. Our work aims to bridge this gap by providing a robust framework for approximating bisimilar symbolic models in the context of nonlinear control systems. \n\n**Keywords:** Approximate bisimulation, Symbolic Model Checking, Nonlinear Control Systems, Finite Difference Method, Spectral Collocation Technique.",
        "ori-fast-z-score": -0.8411910241920598,
        "water-fast-z-score": 7.030139197971627,
        "rewrite-fast-z-score": 0.5232045649263551
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Stability and mode analysis of solar coronal loops using thermodynamic irreversible energy principles .\nAbstract:\nThe stability, equilibrium states and the dynamic behavior of solar coronal loops are investigated by applying thermodynamic irreversible energy principles to an idealized model loop system with constant cross section. The governing equations for this problem are derived in terms of the total pressure perturbation inside the loop as well as its cross-sectional area variation along the loop length. It is shown that these two variables can be expressed in terms of each other through a nonlinear differential equation which describes the evolution of the loop system. This equation has been solved numerically under different initial conditions corresponding to various physical situations. In particular, it is found that the loop may either expand or contract depending on whether the initial value of the total pressure perturbation is positive or negative respectively. Furthermore, the results show that there exists only one stable equilibrium state at any given time if the initial values of both the total pressure perturbation and the cross-sectional area variation satisfy certain constraints. Finally, the temporal evolution of the loop system is studied when the initial condition corresponds to a static equilibrium solution.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Stability and mode assessment of sun coronal loops involving thermodynamic irreversible energy laws . Abstract : The stability , equilibrium states and the dynamic behavior of sun coronal loops are examined by using thermodynamic irreversible energy laws to an idealized model loop system with constant cross area .The governing variables for this question are derived in terms of the total stress perturbation inside the loop as also as its cross - sectional region difference along the loop length . It is demonstrated that these two variables can be described in terms of each other through a nonlinear differential equation which explains the evolution of the loop system .This equation has been solved numerically under various initial conditions corresponding to several physical contexts . In particular , it is found that the loop could either increase or contract depending on whether the initial value of the total stress perturbation is positive or negative respectively .Furthermore , the results show that there exists only one stable equilibrium state at any certain time if the first values of both the total stress perturbation and the cross - sectional region difference satisfy certain constraints . Finally , the temporal evolution of the loop system is studied when the initial situation corresponds to a static equilibrium solution .",
        "rewrite_text": "Title: Stability and Mode Assessment of Solar Coronal Loops Involving Thermodynamic Irreversible Energy Laws\n\nAbstract: This study investigates the stability, equilibrium states, and dynamic behavior of solar coronal loops by applying thermodynamic irreversible energy laws to an idealized model of a loop system with a constant cross-sectional area. The analysis focuses on two key governing variables: the total stress perturbation within the loop and the variation in cross-sectional area along the loop's length. A nonlinear differential equation is derived that relates these two variables, providing a framework for understanding the evolution of the loop system. Numerical solutions to this equation are obtained under various initial conditions, reflecting different physical scenarios. The findings reveal that the behavior of the loop is contingent upon the initial value of the total stress perturbation; specifically, a positive initial value leads to an increase in loop size, while a negative value results in contraction. Additionally, the research identifies that a unique stable equilibrium state exists at any given time, provided that the initial values of both the total stress perturbation and the cross-sectional area difference meet specific criteria. The study further explores the temporal evolution of the loop system when the initial conditions correspond to a static equilibrium state, offering insights into the stability dynamics of solar coronal loops. This work contributes to a deeper understanding of the physical processes governing solar phenomena and has implications for solar physics and astrophysical modeling.",
        "ori-fast-z-score": -0.8620436566990363,
        "water-fast-z-score": 5.339390499775317,
        "rewrite-fast-z-score": 0.1796053020267749
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Likely Cause of the EGRET GeV Anomaly and its Implications .\nAbstract:\nWe present an explanation for the excess in gamma-ray emission observed by the Energetic Gamma Ray Experiment Telescope (EGRET) at energies above 10 GeV, which is known as the  GeV anomaly . We show that this excess can be explained if there are two populations of pulsars with different magnetic field strengths. The first population consists of young pulsars whose fields decay rapidly due to their rapid spin-downs. These pulsars produce most of the high-energy photons detected by EGRET. The second population consists of older pulsars whose fields have decayed more slowly because they rotate slower than younger pulsars on average. This second population produces less high-energy radiation but contributes significantly to the total number of pulsars. Our model predicts that Fermi should detect many new pulsar candidates not seen before. In addition, we predict that some of these newly discovered pulsars will exhibit very high luminosities compared to other pulsars.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Likely Cause of the EGRET GeV Anomaly and its Implications . Abstract : We present an reason for the excess in gamma - ray radiation observed by the Energetic Gamma Ray Experiment Telescope ( EGRET ) at energies above 10 GeV , which is known as the GeV anomaly .We see that this excess can be understood if there are two communities of pulsars with varying magnetic force abilities . The first population contains of young pulsars whose fields collapse rapidly due to their fast spinning - downs .These pulsars produce most of the high - energy photons discovered by EGRET . The second population contains of older pulsars whose fields have decayed more slowly because they rotate slower than younger pulsars on average .This second population generates less large - energy rays but adds significantly to the total quantity of pulsars . Our model predicts that Fermi should detect many new pulsar candidates not seen before .In addition , we estimate that some of these newly discovered pulsars will exhibit very high luminosities relative to other pulsars .",
        "rewrite_text": "Title: The Likely Cause of the EGRET GeV Anomaly and its Implications\n\nAbstract: In this article, we investigate the origins of the gamma-ray excess detected by the Energetic Gamma Ray Experiment Telescope (EGRET) at energies exceeding 10 GeV, commonly referred to as the GeV anomaly. Our analysis suggests that this excess can be attributed to the existence of two distinct populations of pulsars, each characterized by different magnetic field strengths and rotational dynamics. The first group consists of young pulsars, which experience rapid magnetic field decay due to their swift rotational periods. These young pulsars are primarily responsible for the majority of high-energy photons identified by EGRET. In contrast, the second group comprises older pulsars, which exhibit slower rotation rates and consequently experience a more gradual decline in their magnetic fields. Although these older pulsars contribute fewer high-energy photons, they significantly increase the overall pulsar population. Our model posits that the Fermi Gamma-ray Space Telescope is likely to identify numerous new pulsar candidates that have not yet been observed. Furthermore, we predict that some of these newly detected pulsars will demonstrate exceptionally high luminosities compared to their counterparts. This research not only sheds light on the mechanisms behind the GeV anomaly but also has broader implications for our understanding of pulsar populations and their contributions to the gamma-ray sky. The findings underscore the importance of continued observations and the potential for discovering new astrophysical phenomena through advanced gamma-ray detection technologies.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.266851623825876,
        "rewrite-fast-z-score": 0.38138503569823695
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Evolution of the Carter constant for inspirals into a black hole: effect of the black hole quadrupole .\nAbstract:\nWe study how the evolution of the Carter constant depends on the spin and mass ratio in binary systems with spinning black holes, using numerical relativity simulations. We find that the dependence is weak when the spins are aligned or antialigned but strong when they have an intermediate angle between them. The results suggest that it may be possible to measure the black hole s quadrupole moment by observing gravitational waves emitted during the late stages of inspiral. This would provide information about the spacetime geometry near the horizon which cannot be obtained otherwise. \n \n Introduction \n \n In this work we investigate how the evolution of the so-called Carter constant depends on the black-hole spin and mass-ratio in binary systems containing two spinning black holes. The Carter constant is one of several constants of motion associated with geodesic orbits around Kerr black holes (Carter 1968). It can be used as a probe of the spacetime geometry close to the event horizon because its value changes significantly over time only if there exists significant deviation from spherical symmetry at small radii (Bardeen 1973; Thorne et al. 1986 ). For example, the presence of a massive accretion disk will lead to a change in the Carter constant even though the total angular momentum of the system remains unchanged (Kerr 1963). \n \n Previous studies have shown that the orbital evolution of binaries with non-spinning components is affected by the black-hole quadrupole moment Q = M(1 − S2)/c2R2 where S denotes the dimensionless spin parameter of each black hole (Damour & Nagar 1999) . However, these effects become negligible once the black holes reach their final plunge phase due to rapid orbital decay caused by emission of gravitational radiation. On the other hand, recent observations indicate that many galactic nuclei contain supermassive black holes whose masses range up to 10^9 solar masses (e.g., Gebhardt et al. (2000)). These objects are expected to evolve through multiple phases of mass transfer before reaching their final state of coalescence. During such evolutionary processes, the black holes could acquire large amounts of angular momentum via tidal interactions and/or",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Evolution of the Carter constant for inspirals into a black hole : effect of the dark hole quadrupole . Abstract : We research how the evolution of the Carter constant depends on the spin and mass ratio in binary systems with rotating black holes , using numerical relativity simulations .We see that the dependence is weak when the spins are aligned or antialigned but strong when they have an intermediate inclination between them . The results propose that it could be possible to measure the dark hole s quadrupole point by observing gravitational waves emitted during the last phases of inspiral .This might give information about the spacetime geometry near the horizon which cannot be obtained otherwise . Introduction In this study we investigate how the evolution of the so - called Carter constant depends on the dark - hole spin and mass - ratio in binary systems containing two spin black holes .The Carter constant is one of several constants of movement associated with geodesic orbits around Kerr brown holes ( Carter 1968 ) . It can be used as a probe of the spacetime geometry next to the event horizon because its value changes significantly over time only if there exists significant deviation from spherical symmetry at small radii ( Bardeen 1973 ; Thorne et al .1986 ) . For instance , the presence of a huge accretion disk will result to a change in the Carter constant even though the total angular velocity of the system appears unchanged ( Kerr 1963 ) .Previous studies have shown that the orbital evolution of binaries with non - spinning components is affected by the dark - hole quadrupole point Q = M ( 1 − S2 ) / c2R2 where S indicates the dimensionless spin vector of each dark hole ( Damour & Nagar 1999 ) . However , these consequences get negligible once the dark holes reach their final plunge period due to rapid orbital decay caused by absorption of gravitational rays .On the other hand , recent observations indicate that several galactic nuclei contain supermassive black holes whose masses range up to 10 ^ 9 solar masses ( e . g . , Gebhardt et al . ( 2000 ) ) .These structures are expected to evolve through several stages of mass transfer before reaching their final state of coalescence . During such evolutionary processes , the dark holes could acquire large quantities of angular velocity via tidal interactions and / or",
        "rewrite_text": "**Title:** Evolution of the Carter Constant for Inspirals into a Black Hole: Effect of the Dark Hole Quadrupole\n\n**Abstract:** This study investigates the evolution of the Carter constant in binary systems with rotating black holes, focusing on how this evolution is influenced by the spin and mass ratio of the black holes. Utilizing numerical relativity simulations, we find that the dependence of the Carter constant on these parameters is relatively weak when the spins of the black holes are either aligned or antialigned. However, a significant dependence emerges when the spins are inclined at intermediate angles to one another. Our findings suggest that it may be feasible to measure the quadrupole moment of the dark hole by analyzing gravitational waves emitted during the final stages of the inspiral process. This measurement could provide critical insights into the spacetime geometry near the event horizon, which is otherwise inaccessible through conventional observational methods.\n\nIn this research, we delve into the relationship between the evolution of the Carter constant and the spins and mass ratios of binary black hole systems. The Carter constant, a key parameter associated with geodesic orbits around Kerr black holes, serves as a valuable tool for probing the spacetime geometry adjacent to the event horizon. Its value is significantly affected by deviations from spherical symmetry at small radii, such as those caused by the presence of a massive accretion disk, which can alter the Carter constant even when the overall angular momentum of the system remains constant.\n\nPrevious investigations have highlighted that the orbital dynamics of binaries with non-spinning components are influenced by the quadrupole moment of the black holes, defined as Q = M(1 - S²)/c²R², where S represents the dimensionless spin vector of each black hole. However, the impact of this quadrupole moment diminishes as the black holes approach their final plunge phase, primarily due to the rapid orbital decay resulting from gravitational wave emission. Recent observations have revealed that many galactic nuclei harbor supermassive black holes with masses reaching up to 10^9 solar masses. These black holes are anticipated to undergo various stages of mass transfer before coalescing, during which they may gain substantial angular momentum through tidal interactions and other processes.",
        "ori-fast-z-score": 0.7372097807744856,
        "water-fast-z-score": 7.22465585158996,
        "rewrite-fast-z-score": 0.7715167498104595
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Nonequilibrium spin-dependent phenomena in mesoscopic superconductor-normal metal tunnel structures .\nAbstract:\nWe study nonequilibrium spin dependent transport properties of normal-metal-superconductor (NS) and ferromagnet-superconductor (FS) hybrid systems by using the quasiclassical theory for diffusive conductors with spin-orbit scattering. We show that, due to the proximity effect induced pair correlations between electrons on opposite sides of NS interface, there is an additional contribution to the current density which depends on the relative orientation of magnetizations in F and S layers. This results in appearance of the anomalous Josephson-like current-phase relation in FS junctions. In addition we find that the Andreev reflection at NS interfaces can be strongly suppressed if the angle between magnetization directions in N and S regions becomes sufficiently large. The suppression of Andreev reflection leads to the reduction of conductance through NS junction as compared to its value in equilibrium state. \n \n Finally, we discuss possible experimental realizations of our predictions.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Nonequilibrium spin - dependent phenomena in mesoscopic superconductor - normal metal tunnel complexes . Abstract : We research nonequilibrium spin dependent transport properties of normal - copper - superconductor ( NS ) and ferromagnet - superconductor ( FS ) hybrid systems by using the quasiclassical principle for diffusive conductors with spin - orbit scattering .We see that , owing to the location effect induced pair correlations between electrons on opposite sides of NS interface , there is an additional contribution to the current density which depends on the relative position of magnetizations in F and S layers . This results in presence of the anomalous Josephson - like current - phase connection in FS junctions .In addition we find that the Andreev reflection at NS interfaces can be highly suppressed if the angle between magnetization directions in N and S areas becomes enough large . The suppression of Andreev reflection results to the reduction of conductance through NS junction as compared to its value in equilibrium state .Finally , we study possible experimental realizations of our predictions .",
        "rewrite_text": "In this study, we investigate the nonequilibrium spin-dependent transport characteristics of hybrid systems composed of normal metal-superconductor (NS) and ferromagnet-superconductor (FS) junctions. Utilizing the quasiclassical framework applicable to diffusive conductors that experience spin-orbit scattering, we uncover significant insights into the behavior of these systems. One of the key findings is the emergence of additional contributions to the current density, which arise from pair correlations between electrons located on opposite sides of the NS interface. This phenomenon is influenced by the relative orientations of the magnetizations in the ferromagnetic (F) and superconducting (S) layers, leading to the observation of an anomalous Josephson-like current-phase relationship in FS junctions.\n\nMoreover, our analysis reveals that Andreev reflection at NS interfaces can be markedly suppressed when the angle between the magnetization directions in the normal (N) and superconducting (S) regions becomes sufficiently large. This suppression of Andreev reflection has a direct impact on the conductance of the NS junction, resulting in a notable decrease compared to its equilibrium state. \n\nWe also explore potential experimental setups to validate our theoretical predictions, highlighting the relevance of our findings for future research in the field of mesoscopic superconductivity and spintronics. Our work provides a deeper understanding of the interplay between spin dynamics and superconducting phenomena, paving the way for novel applications in quantum computing and spin-based devices.",
        "ori-fast-z-score": 1.1470786693528088,
        "water-fast-z-score": 5.20847435706514,
        "rewrite-fast-z-score": 1.1406468642034677
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Advantages of Four Dimensions for Composite Higgs Models .\nAbstract:\nWe study the advantages and disadvantages of composite Higgs models in four dimensions (4D) versus five dimensions (5D). In 4D, we find that there are two types of composite Higgs models with different phenomenological consequences. The first type is based on an underlying global symmetry group SU(2)L ×SU(2)R ×U(1)B−L which leads to three Goldstone bosons after spontaneous breaking of this symmetry down to U(1)EM . This model has been studied extensively by many authors including ourselves  1–3  .\nThe second type is based on an extended gauge symmetry group SU(3)C ×SU(2)L ×U(1)Y ×Z′ where Z′ is a new abelian gauge factor associated with extra spatial dimension  4–6  . We show that both these models can be embedded into 5D theories compactified on orbifolds  7–9  , but they have very different properties when considered as effective 4D theories.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Advantages of Four Dimensions for Composite Higgs Models . Abstract : We research the advantages and disadvantages of composite Higgs configurations in four dimensions ( 4D ) vs five dimensions ( 5D ) .In 4D , we find that there are two forms of composite Higgs theories with various phenomenological consequences . The first class is based on an underlying global symmetry class SU ( 2 ) L ×SU ( 2 ) R ×U ( 1 ) B−L which contributes to three Goldstone bosons after spontaneous breaking of this symmetry down to U ( 1 ) EM .This theory has been studied frequently by many writers including ourselves 1 – 3 . The second kind is based on an extended gauge symmetry class SU ( 3 ) C ×SU ( 2 ) L ×U ( 1 ) Y ×Z ′ where Z ′ is a new abelian gauge parameter identified with extra spatial dimension 4 – 6 .We see that both these models can be embedded into 5D theories compactified on orbifolds 7 – 9 , but they have very different properties when considered as efficient 4D theories .",
        "rewrite_text": "In this article, we explore the comparative advantages and disadvantages of composite Higgs models formulated in four dimensions (4D) versus those in five dimensions (5D). Our investigation reveals that within the 4D framework, there exist two distinct classes of composite Higgs theories, each yielding unique phenomenological implications. The first class is predicated on an underlying global symmetry structure, specifically SU(2)L × SU(2)R × U(1)B−L. This symmetry, upon spontaneous breaking down to U(1)EM, results in the emergence of three Goldstone bosons. This particular model has been the subject of extensive research by various authors, including our own contributions. The second class of composite Higgs theories is founded on an extended gauge symmetry, represented by SU(3)C × SU(2)L × U(1)Y × Z′, where Z′ denotes a new abelian gauge parameter associated with an additional spatial dimension. We demonstrate that both of these 4D models can be effectively embedded within 5D theories that are compactified on orbifolds. However, it is crucial to note that they exhibit significantly different characteristics when analyzed as standalone 4D theories. Our findings underscore the importance of dimensionality in the formulation and interpretation of composite Higgs models, providing insights that could inform future theoretical developments in particle physics. This study contributes to the ongoing discourse on the viability and implications of various Higgs configurations, ultimately enhancing our understanding of fundamental interactions in the universe.",
        "ori-fast-z-score": -0.3779644730092272,
        "water-fast-z-score": 4.25,
        "rewrite-fast-z-score": 0.6965260331469925
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Challenging the Cosmological Constant .\nAbstract:\nThe cosmological constant is one of the most important parameters in modern physics, and its value has been determined by observations to be extremely small but nonzero.  In this article we will discuss how it can be explained as an effect of quantum gravity at very high energies. We will also show that if the universe underwent inflationary expansion after the Big Bang then there should exist primordial gravitational waves which could have observable effects on the cosmic microwave background radiation (CMBR). Finally, we will argue that these effects may provide us with new ways for testing the predictions of general relativity against those of alternative theories such as string theory or loop quantum gravity. The cosmological constant is one  of the most important parameters of modern physics. Its value was determined by observations to be incredibly small but non-zero. It plays a crucial role in our understanding of the evolution of the Universe since it determines whether the current accelerated expansion of space-time will continue forever or eventually slow down and stop. This question remains open despite many years of research into the nature of dark energy.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Challenging the Cosmological Constant . Abstract : The cosmological coefficient is one of the most important components in modern physics , and its value has been determined by observations to be extremely tiny but nonzero .In this page we will explore how it can be described as an influence of quantum gravitational at very high energies . We will also demonstrate that if the universe underwent inflationary inflation after the Big Bang then there should exist primordial gravitational waves which could have observable effects on the cosmic microwave background radiation ( CMBR ) .Finally , we will argue that these changes may provide us with alternative ways for testing the assumptions of general relativity against those of alternative theories such as string theory or loop quantum gravitational . The cosmological coefficient is one of the most important characteristics of modern physics .Its value was decided by observations to be incredibly small but un - zero . It plays a crucial role in our knowing of the evolution of the Universe since it determines whether the present rapid expansion of space - time will continue forever or eventually accelerate down and stop .This question continues open despite many years of research into the nature of dark energy .",
        "rewrite_text": "**Title: Challenging the Cosmological Constant**\n\n**Abstract:** The cosmological constant is a fundamental aspect of contemporary physics, characterized by its remarkably small yet non-zero value as determined through observational data. This article delves into the implications of the cosmological constant, particularly in the context of quantum gravity at extremely high energies. We propose that if the universe experienced a period of inflation following the Big Bang, it would have generated primordial gravitational waves. These waves could potentially leave observable signatures on the cosmic microwave background radiation (CMBR), offering a unique avenue for empirical investigation. Furthermore, we contend that the observable effects of these gravitational waves may serve as a means to test the foundational assumptions of general relativity against alternative theoretical frameworks, such as string theory and loop quantum gravity. The cosmological constant is pivotal in shaping our understanding of the universe's evolution, influencing whether the current accelerated expansion of spacetime will persist indefinitely or eventually decelerate and halt. Despite extensive research into the nature of dark energy, the question of the cosmological constant's true nature remains unresolved. This article aims to challenge existing paradigms and stimulate further inquiry into the fundamental forces that govern the cosmos, ultimately seeking to bridge the gap between theoretical predictions and observational evidence. Through this exploration, we hope to contribute to a deeper understanding of the universe's dynamics and the role of dark energy in shaping its fate.",
        "ori-fast-z-score": 1.3480372031495529,
        "water-fast-z-score": 5.848076606885379,
        "rewrite-fast-z-score": 2.2283440581246223
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Stellar Evolution in NGC 6791: Mass Loss on the Red Giant Branch and the Formation of Low Mass White Dwarfs .\nAbstract:\nWe present new optical photometry for the open cluster NGC 6791, obtained with the Wide Field Camera 3 (WFC3) aboard HST as part of program GO-12775 (PI: A. Dotter). The data cover an area of 0.5 deg2 around the cluster center at high spatial resolution (0.1 arcsec/pixel), allowing us to study individual stars down to V = 20 mag. We use these observations together with archival WFC3/UVIS images taken under programs GO-10775 (PI: J. Kalirai) and GO-11775 (PI: S. Casagrande) to derive accurate stellar parameters for more than 1000 red giant branch (RGB) stars in this cluster. Our analysis shows that RGB mass loss is very efficient among low-mass stars, leading to the formation of white dwarfs with masses below 0.45 M . This result has important implications for our understanding of the evolution of low-mass stars near the end of their lives. \n \n Keywords: Open clusters",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Stellar Evolution in NGC 6791 : Mass Loss on the Red Giant Branch and the Formation of Low Mass White Dwarfs . Abstract : We report new optical photometry for the open cluster NGC 6791 , obtained with the Wide Field Camera 3 ( WFC3 ) aboard HST as part of series GO - 12775 ( PI : A . Dotter ) .The data cover an area of 0 . 5 deg2 around the cluster center at high spatial resolution ( 0 . 1 arcsec / pixel ) , allowing us to study individual stars down to V = 20 mag . We use these observations together with archival WFC3 / UVIS images took under programs GO - 10775 ( PI : J . Kalirai ) and GO - 11775 ( PI : S . Casagrande ) to derive exact stellar characteristics for more than 1000 red dwarf branch ( RGB ) stars in this cluster .Our study shows that RGB mass loss is very efficient among low - mass stars , leading to the formation of white dwarfs with masses below 0 . 45 M . This result has crucial consequences for our understanding of the evolution of lowest - mass stars near the end of their careers .Keywords : Open clusters",
        "rewrite_text": "Title: Stellar Evolution in NGC 6791: Mass Loss on the Red Giant Branch and the Formation of Low Mass White Dwarfs\n\nAbstract: In this study, we present new optical photometry of the open cluster NGC 6791, acquired using the Wide Field Camera 3 (WFC3) on the Hubble Space Telescope (HST) as part of the program GO-12775 (Principal Investigator: A. Dotter). Our observations encompass an area of 0.5 square degrees centered on the cluster, achieving high spatial resolution of 0.1 arcseconds per pixel. This level of detail enables us to analyze individual stars down to a magnitude of V = 20. By integrating these new data with archival WFC3/UVIS images from previous programs (GO-10775 by J. Kalirai and GO-11775 by S. Casagrande), we have successfully derived precise stellar parameters for over 1,000 red giant branch (RGB) stars within NGC 6791. Our findings indicate that mass loss during the RGB phase is significantly pronounced among low-mass stars, resulting in the formation of white dwarfs with masses less than 0.45 solar masses. This discovery has profound implications for our understanding of the evolutionary processes affecting the lowest-mass stars as they approach the end of their life cycles. The results contribute to the broader knowledge of stellar evolution within open clusters and enhance our comprehension of the mechanisms driving mass loss in stars, particularly in the context of low-mass stellar populations. \n\nKeywords: Open clusters, Stellar evolution, Red giant branch, Mass loss, White dwarfs.",
        "ori-fast-z-score": 1.3643820804812932,
        "water-fast-z-score": 4.431293675255978,
        "rewrite-fast-z-score": 1.7085642859406605
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  On the inspiral of Massive Black Holes in gas-rich galaxy mergers .\nAbstract:\nWe present results on the gravitational wave emission during the final stages of black hole binary coalescence, when the holes are surrounded by an accretion disk and their orbital evolution is driven by radiation reaction. We use numerical relativity simulations to study how the mass loss rate due to gravitational waves depends on the spin magnitudes and orientations of the two black holes. Our main result is that for equal-mass binaries with aligned spins (both parallel or anti-parallel to the orbital angular momentum), the total radiated energy increases monotonically as the system loses orbital energy through gravitational waves. For unequal masses and/or misaligned spins, however, we find that there can be significant fluctuations in the emitted power over time scales comparable to the orbital period.  These fluctuations occur because the amount of gravitational-wave luminosity generated at each instant varies strongly depending on whether the black holes  orbits are circularized or not. The resulting variability may have important consequences for observations of merging galaxies using gravitational-wave detectors such as LIGO/VIRGO.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : On the inspiral of Massive Black Holes in gas - rich galaxy mergers . Abstract : We present results on the gravitational wave radiation during the last phases of black hole binary coalescence , when the holes are surrounded by an accretion disk and their orbital evolution is caused by radiation process .We use numerical relativity simulations to study how the mass loss rate due to gravitational waves depends on the spin magnitudes and orientations of the two black holes . Our main consequence is that for equal - mass binaries with aligned spins ( both parallel or anti - parallel to the orbital angular velocity ) , the total radiated energy rises monotonically as the system loses orbital energy through gravity radiation .For unequal masses and / or misaligned spins , however , we find that there can be considerable fluctuations in the emitted energy over time ranges comparable to the orbital period . These fluctuations happen because the quantity of gravitational - wave luminosity generated at each instant varies strongly depending on whether the dark holes orbits are circularized or not .The produced variability may have important implications for observations of merging galaxies using gravitational - wave detectors such as LIGO / VIRGO .",
        "rewrite_text": "We present a comprehensive analysis of gravitational wave emissions during the final stages of black hole binary coalescence, particularly in scenarios where the black holes are enveloped by an accretion disk. Our study focuses on the orbital dynamics influenced by radiation processes as the black holes spiral towards each other. Utilizing advanced numerical relativity simulations, we investigate how the rate of mass loss due to gravitational waves is affected by the spin magnitudes and orientations of the two black holes involved in the merger. \n\nOur findings indicate that for equal-mass black hole binaries with aligned spins—whether both spins are parallel or anti-parallel to the orbital angular momentum—the total energy radiated increases steadily as the system loses orbital energy through gravitational radiation. In contrast, for binaries with unequal masses or misaligned spins, we observe significant fluctuations in the emitted gravitational wave energy over time scales that are comparable to the orbital period. These fluctuations arise from the variable gravitational-wave luminosity generated at any given moment, which is highly dependent on whether the orbits of the black holes have circularized.\n\nThe variability in gravitational wave emissions has critical implications for the detection and observation of merging galaxies by gravitational wave observatories such as LIGO and VIRGO. Understanding these dynamics not only enhances our knowledge of black hole mergers in gas-rich galaxy mergers but also aids in interpreting the signals detected by these observatories, potentially leading to new insights into the nature of black hole interactions and the environments in which they occur. This research contributes to the broader field of astrophysics by providing a clearer picture of the complex interplay between black hole spins, mass ratios, and gravitational wave production during the late stages of binary coalescence.",
        "ori-fast-z-score": 0.40406101782088427,
        "water-fast-z-score": 4.321662605614612,
        "rewrite-fast-z-score": 1.0650014966747527
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Loop Spaces and Langlands Parameters .\nAbstract:\nIn this talk, we will discuss the relationship between loop spaces and Langlands parameters in terms of their connections to representation theory.  We will begin by recalling some basic facts about loop spaces and their relation with infinite-dimensional Lie groups (e.g., loop groups).  Next, we ll recall how one can associate a certain type of infinite-dimensional Lie group called an affine Hecke algebra to any reductive algebraic group over a field k of characteristic 0.   Finally, we ll explain how these two ideas are related via the notion of a Harish-Chandra bimodule. The main result is that if G is a connected semisimple complex algebraic group defined over Q then there exists a natural isomorphism between the category of finite-dimensional representations of G(Q) and the category of Harish-Chandra modules for its associated affine Hecke algebra. This work was done jointly with David Vogan at Harvard University during my postdoctoral fellowship.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Loop Spaces and Langlands Parameters . Abstract : In this talk , we will explore the relationship between loop spaces and Langlands parameters in terms of their connections to representation theory .We will start by recalling some fundamental details about loop spaces and their connection with infinite - dimensional Lie fields ( e . g . , loop groups ) . Next , we ll remember how one can associate a certain type of infinite - dimensional Lie group termed an affine Hecke algebra to any reductive algebraic group over a field k of characteristic 0 .Finally , we ll explain how these two ideas are related via the notion of a Harish - Chandra bimodule . The main consequence is that if G is a connected semisimple complex algebraic group defined over Q then there exists a natural isomorphism between the category of finite - dimensional representations of G ( Q ) and the category of Harish - Chandra modules for its associated affine Hecke algebra .This research was done concurrently with David Vogan at Harvard University during my postdoctoral fellowship .",
        "rewrite_text": "Title: Loop Spaces and Langlands Parameters\n\nAbstract: This presentation delves into the intricate relationship between loop spaces and Langlands parameters, particularly highlighting their significance within the realm of representation theory. We will begin by revisiting essential concepts related to loop spaces, emphasizing their ties to infinite-dimensional Lie groups, such as loop groups. Following this foundational overview, we will discuss the construction of affine Hecke algebras, a specific class of infinite-dimensional Lie groups, which can be associated with any reductive algebraic group defined over a field \\( k \\) of characteristic zero. \n\nThe crux of our discussion will center on the interplay between these two concepts through the framework of Harish-Chandra bimodules. A pivotal outcome of this exploration is the establishment of a natural isomorphism between the category of finite-dimensional representations of a connected semisimple complex algebraic group \\( G \\) defined over \\( \\mathbb{Q} \\) and the category of Harish-Chandra modules corresponding to its associated affine Hecke algebra. This result not only enriches our understanding of the representation theory of algebraic groups but also illustrates the profound connections between different mathematical structures.\n\nThis research was conducted in collaboration with David Vogan at Harvard University during my postdoctoral fellowship, and it aims to contribute to the ongoing dialogue in the field regarding the unifying themes that emerge from the study of loop spaces and Langlands parameters. Through this talk, we hope to shed light on the broader implications of these findings and their potential applications in various areas of mathematics.",
        "ori-fast-z-score": 0.7071067811865476,
        "water-fast-z-score": 3.771236166328254,
        "rewrite-fast-z-score": 0.29277002188455997
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Radio Astrometric Detection and Characterization of Extra-Solar Planets: A White Paper Submitted to the NSF ExoPlanet Task Force .\nAbstract:\nThe National Science Foundation (NSF) has recently formed an  Exoplanet Task Force  with the goal of identifying key science goals for future space missions in exoplanet research, including radio astrometry.  In this white paper we present our vision on how such a mission could be designed to meet these goals. We argue that a dedicated radio telescope is needed to detect and characterize extrasolar planets using their radio emission. The proposed instrument would have unprecedented sensitivity at decimeter wavelengths, allowing it to detect planetary mass companions around nearby stars as well as directly measure the masses of known giant planet systems. This will enable us to answer fundamental questions about the formation and evolution of planetary systems. Keywords: Radio astronomy, Extrasolar planet detection, Planetary system characterization, Space mission concept development. 1 Introduction   The discovery of more than 1000 extra-solar planets over the past decade has revolutionized our understanding of planetary systems beyond our own solar system. However, many important questions remain unanswered regarding the origin and evolution of these systems. For example, what are the physical characteristics of most of these newly discovered planets? How do they form? What happens when two or more planets interact gravitationally? Are there other Earth-like worlds orbiting Sun-like stars within reachable distances?  Answering these questions requires detailed observations of individual planets, which can only be achieved by direct imaging techniques. Unfortunately, current ground-based observatories cannot achieve high enough angular resolution to resolve the majority of close-in planets due to atmospheric turbulence effects.   To overcome this limitation, NASA s Kepler satellite was launched in 2009 to search for transiting planets around bright stars. Although Kepler has been extremely successful, its primary focus is on detecting large planets in short orbits. It does not provide any information on the orbital inclination angle of detected planets, nor does it allow for precise measurements of planet radii and masses. Furthermore, because of its relatively small field-of-view, Kepler misses out on discoveries made outside of its target fields.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Radio Astrometric Detection and Characterization of Extra - Solar Planets : A White Paper Submitted to the NSF ExoPlanet Task Force . Abstract : The National Science Foundation ( NSF ) has recently established an Exoplanet Task Force with the objective of identifying key research goals for future space missions in exoplanet research , notably television astrometry .In this white paper we present our vision on how such a project possible be designed to meet these objectives . We argue that a dedicated radio telescope is required to identify and characterize extrasolar planets using their radio emission .The proposed instrument would have enormous sensitivity at decimeter wavelengths , allowing it to identify planetary mass companions around nearby planets as also as closely determine the masses of known giant planet systems . This will assist us to ask critical debates about the formation and evolution of planetary networks .Keywords : Radio astronomy , Extrasolar moon recognition , Planetary network characterization , Space mission design development . 1 Introduction The observation of more than 1000 extra - solar planets over the previous decade has revolutionized our understanding of planetary structures beyond our own solar system .However , various many issues appear unanswered concerning the origin and evolution of these systems . For instance , what are the natural characteristics of most of these newly discovered planets ?How do they shape ? What happens when two or more planets interact gravitationally ?Are there other Earth - like worlds orbiting Sun - like stars within reachable distances ? Answering these problems involves detailed observations of individual planets , which can only be obtained by direct observation techniques .Unfortunately , current ground - based observatories cannot achieve high enough angular resolution to identify the majority of close - in planets owing to atmospheric turbulence influences . To solve this limitation , NASA s Kepler satellite was launched in 2009 to search for transiting planets around bright stars .Although Kepler has been extremely successful , its primary emphasis is on detecting large planets in small planets . It does not offer any knowledge on the orbital inclination ratio of detected planets , nor does it enable for precise observations of planet radii and masses .Furthermore , because of its relatively small field - of - view , Kepler misses out on discoveries made outside of its target areas .",
        "rewrite_text": "**Title:** Radio Astrometric Detection and Characterization of Exoplanets: A White Paper Submitted to the NSF Exoplanet Task Force\n\n**Abstract:** The National Science Foundation (NSF) has recently initiated an Exoplanet Task Force aimed at establishing pivotal research objectives for upcoming space missions focused on exoplanet studies, particularly in the realm of astrometry. In this white paper, we outline our vision for designing a project that aligns with these goals. We propose the development of a dedicated radio telescope specifically engineered to detect and characterize extrasolar planets through their radio emissions. This proposed instrument would possess exceptional sensitivity at decimeter wavelengths, enabling the identification of planetary mass companions around nearby stars and facilitating precise measurements of the masses of known giant planet systems. Such advancements would significantly contribute to our understanding of the formation and evolution of planetary systems.\n\nThe past decade has seen the discovery of over 1,000 exoplanets, fundamentally transforming our comprehension of planetary structures beyond our solar system. However, numerous questions remain regarding the origins and evolutionary pathways of these systems. Key inquiries include the intrinsic characteristics of these newly identified planets, their formation processes, the gravitational interactions among multiple planets, and the potential existence of Earth-like worlds orbiting Sun-like stars within accessible distances. Addressing these questions necessitates detailed observations of individual planets, which can only be achieved through direct observational techniques.\n\nCurrent ground-based observatories face limitations in achieving the necessary angular resolution to detect many close-in planets due to atmospheric turbulence. To overcome this challenge, NASA launched the Kepler satellite in 2009 to search for transiting planets around bright stars. While Kepler has made significant contributions, its primary focus has been on identifying larger planets in close orbits, leaving gaps in our understanding of the orbital inclinations of detected planets and limiting precise measurements of their radii and masses. Additionally, Kepler's relatively narrow field of view restricts its ability to discover planets outside its designated target areas. This white paper advocates for a new approach to exoplanet detection that leverages radio astrometry to fill these critical gaps in our knowledge. \n\n**Keywords:** Radio astronomy, extrasolar planet detection, planetary system characterization, space mission design.",
        "ori-fast-z-score": 0.15249857033260467,
        "water-fast-z-score": 7.892283294029704,
        "rewrite-fast-z-score": -0.30151134457776363
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  An updated survey of globular clusters in M31. II Newly discovered bright and remote clusters .\nAbstract:\nWe present the results of an extensive search for new globular cluster candidates (GPCs) in the outer halo of M31, based on deep HST/ACS images covering about one quarter of its virial radius. We find that the number density profile of GPCs is well described by a power law with index -1.5 ± 0.1 out to at least 100 kpc. The total number of GPCs within this region amounts to N = 1253±60, which corresponds to a specific frequency S=N/M*=(3.2±0.4)×10−3. This value agrees very well with previous estimates obtained using ground-based data. In addition we identify two previously unknown distant clusters located at projected distances of more than 300 kpc from the center of M31. These objects are among the most luminous known GCs outside our Galaxy. Their integrated magnitudes are mF814W≈−10 mag and their half-light radii range between r h ≈6 pc and r h ≈20 pc.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : An updated survey of globular complexes in M31 . II Newly discovered bright and remote clusters .Abstract : We present the results of an extensive search for fresh globular cluster candidates ( GPCs ) in the exterior halo of M31 , using on dark HST / ACS images covering about one quarter of its virial diameter . We see that the number density profile of GPCs is well described by a power law with index - 1 . 5 ± 0 . 1 out to at least 100 kpc .The total number of GPCs within this area amounts to N = 1253±60 , which corresponds to a certain frequency S = N / M * = ( 3 . 2±0 . 4 ) ×10−3 . This value agrees very best with previous calculated obtained using ground - based data .In addition we identify two formerly unidentified distant clusters situated at projected distances of more than 300 kpc from the center of M31 . These bodies are among the most luminous known GCs outside our Galaxy .Their integrated magnitudes are mF814W≈−10 mag and their half - light radii vary between p h ≈6 pc and p h ≈20 pc .",
        "rewrite_text": "We present the findings from a comprehensive investigation aimed at identifying new globular cluster candidates (GPCs) in the outer halo of the Andromeda galaxy (M31). Utilizing high-resolution dark images from the Hubble Space Telescope's Advanced Camera for Surveys (HST/ACS), which cover approximately one-quarter of M31's virial diameter, we have successfully cataloged a significant number of GPCs. Our analysis reveals that the number density profile of these clusters can be accurately modeled by a power law with an index of -1.5 ± 0.1, extending to distances of at least 100 kpc from the galactic center. In total, we estimate the presence of N = 1253 ± 60 GPCs within the surveyed region, leading to a calculated frequency of S = N / M* = (3.2 ± 0.4) × 10^−3. This frequency aligns closely with previous estimates derived from ground-based observations, reinforcing the reliability of our findings.\n\nMoreover, our survey has led to the identification of two previously unrecognized distant clusters, located at projected distances exceeding 300 kpc from M31's center. These newly discovered clusters are among the brightest globular clusters known outside of our Milky Way galaxy, exhibiting integrated magnitudes of approximately mF814W ≈ -10 mag. Their half-light radii range between approximately 6 pc and 20 pc, indicating a diverse population of GPCs in terms of size and luminosity. This study not only enhances our understanding of the globular cluster population in M31 but also contributes valuable insights into the formation and evolution of such structures in the context of galaxy formation.",
        "ori-fast-z-score": 0.22941573387056174,
        "water-fast-z-score": 4.216541329006604,
        "rewrite-fast-z-score": 1.0838622059807226
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The VLT-FLAMES survey of massive stars: Evolution of surface N abundances and effective temperature scales in the Galaxy and Magellanic Clouds .\nAbstract:\nWe present new spectroscopic observations for more than 1000 Galactic OB supergiants, obtained with FLAMES/GIRAFFE at the Very Large Telescope (VLT). The sample includes all known O-type dwarfs and giants as well as B-type supergiants brighter than about Mbol = -4 mag within 25 pc distance to Earth. We derive atmospheric parameters T eff , log g, microturbulence velocity vmic, and chemical composition including nitrogen abundance  N/Fe  . For comparison we also analyse a large number of Galactic red supergiants observed by GOSSS project using similar methods. Our results show that there is no significant difference between the mean values of these quantities derived for both samples. However, our analysis reveals systematic differences between different studies based on smaller samples published so far. In particular, we find that the majority of previous investigations overestimated the temperatures of hotter objects due to neglecting non-LTE effects or underestimating gravities because they did not take into account stellar winds.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The VLT - FLAMES search of large galaxies : Evolution of surface N abundances and effective heat scales in the Galaxy and Magellanic Clouds . Abstract : We report new spectroscopic observations for more than 1000 Galactic OB supergiants , obtained with FLAMES / GIRAFFE at the Very Large Telescope ( VLT ) .The sample comprises all known O - class dwarfs and giants as well as B - class supergiants hotter than about Mbol = - 4 mag within 25 pc proximity to Earth . We derive air parameters T eff , log g , microturbulence velocity vmic , and biological composition including nitrogen density N / Fe .For comparison we also analyse a large number of Galactic red supergiants detected by GOSSS program use similar methods . Our results show that there is no considerable difference between the mean values of these quantities calculated for both samples .However , our analysis reveals systematic differences between various surveys based on smaller specimens released so far . In particular , we find that the majority of previous investigations overestimated the temperatures of hotter objects due to neglecting non - LTE effects or underestimating gravities because they did not take into consideration stellar winds .",
        "rewrite_text": "We present new spectroscopic observations of over 1,000 Galactic OB supergiants, conducted using the FLAMES/GIRAFFE instrument at the Very Large Telescope (VLT). Our study encompasses all known O-class dwarfs and giants, as well as B-class supergiants with absolute magnitudes brighter than approximately Mbol = -4 mag, located within a 25 parsec radius of Earth. We meticulously derive essential atmospheric parameters, including effective temperature (T_eff), surface gravity (log g), microturbulence velocity (v_mic), and the nitrogen abundance relative to iron (N/Fe). To provide a comprehensive comparison, we also analyze a substantial sample of Galactic red supergiants identified through the GOSSS program, employing similar methodologies. Our findings indicate that there are no significant differences in the mean values of these parameters between the two samples. However, we uncover systematic discrepancies among various previous surveys that utilized smaller sample sizes. Notably, we observe that many earlier studies have overestimated the temperatures of hotter stars, primarily due to the omission of non-local thermodynamic equilibrium (non-LTE) effects, or have underestimated surface gravities by failing to account for the influence of stellar winds. This research contributes to a deeper understanding of the evolution of surface nitrogen abundances and effective heat scales in both the Milky Way and the Magellanic Clouds, highlighting the importance of large, homogeneous samples for accurate stellar parameter determination. Our results underscore the need for careful consideration of physical processes in stellar atmospheres to refine our knowledge of massive star evolution and their chemical enrichment in the universe.",
        "ori-fast-z-score": 0.8728715609439696,
        "water-fast-z-score": 5.822969752985674,
        "rewrite-fast-z-score": 2.1447610589527217
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Meta-nematic transitions in a bilayer system: Application to the bilayer ruthenate .\nAbstract:\nWe study the phase diagram and electronic structure of bilayer ruthenate Sr3Ru2O7 using density functional theory (DFT) calculations, which show that this material is close to an insulator-metal transition driven by charge transfer between layers. We find that the Fermi surface topology changes dramatically across the metal-insulator boundary, with the appearance of new hole pockets at the Brillouin zone center. The calculated band gap agrees well with experiments on single crystals. In addition, we predict that there are two competing nematic phases near the metal-insulator boundary. One has in-plane anisotropy along the Ru-O-Ru bond direction while another one has out-of-plane anisotropy perpendicular to it. These results provide insights into the origin of the observed structural distortion in bilayer ruthenates. Bilayer ruthenates have attracted considerable attention recently due to their rich physical properties including unconventional superconductivity  1  , quantum criticality  2  , and multiferroicity  3  . Among these materials, Sr3Ru2O7 shows particularly interesting behavior because its ground state can be tuned continuously from metallic to insulating states through chemical doping or applying pressure  4  .\nIn recent years, several experimental studies have been performed to investigate the nature of the metal-insulator transition (MIT). For example, angle resolved photoemission spectroscopy measurements  5  found that the Fermi surface topology changed significantly when crossing the MIT line. X-ray scattering  6  showed that the crystal symmetry was lowered from tetragonal to orthorhombic below TMI = 160 K. Neutron scattering  7  revealed that the lattice parameters were different for the ab plane and c axis below TMIT ~ 150 K. However, despite extensive investigations, the microscopic mechanism behind the MIT remains unclear  8  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Meta - nematic transitions in a bilayer system : Application to the bilayer ruthenate . Abstract : We research the phase diagram and electronic structure of bilayer ruthenate Sr3Ru2O7 using density functional theory ( DFT ) observations , which show that this metal is close to an insulator - iron transition driven by charge transfer between layers .We see that the Fermi surface topology changes dramatically across the metal - insulator boundary , with the appearance of new hole pockets at the Brillouin zone center . The measured band gap agrees well with experiments on single crystals .In addition , we expect that there are two rival nematic phases near the metal - insulator boundary . One has in - plane anisotropy along the Ru - O - Ru bond direction while another one has out - of - plane anisotropy diagonal to it .These data provide insights into the origin of the known structural degradation in bilayer ruthenates . Bilayer ruthenates have garnered considerable scrutiny lately owing to their vast physical properties including unconventional superconductivity 1 , quantum criticality 2 , and multiferroicity 3 .Among these materials , Sr3Ru2O7 shows particularly exciting behavior because its ground state can be tuned continuously from metallic to insulating states through chemical doping or applying force 4 . In recent years , various experimental studies have been performed to examine the nature of the metal - insulator transition ( MIT ) .For instance , angle resolution photoemission spectroscopy observations 5 found that the Fermi surface topology changed significantly when crossing the MIT line . X - ray scattering 6 revealed that the crystal symmetry was dropped from tetragonal to orthorhombic below TMI = 160 K . Neutron scattering 7 revealed that the lattice parameters were change for the ab plane and c axis below TMIT ~ 150 K . However , despite extensive investigations , the microscopic process behind the MIT remains unsure 8 .",
        "rewrite_text": "**Title:** Meta-Nematic Transitions in a Bilayer System: Application to the Bilayer Ruthenate\n\n**Abstract:** This study delves into the phase diagram and electronic structure of the bilayer ruthenate Sr3Ru2O7, utilizing density functional theory (DFT) to uncover critical insights. Our findings indicate that this metallic compound is situated near a metal-insulator transition (MIT) that is primarily driven by charge transfer between its layers. Notably, we observe a significant alteration in the Fermi surface topology as the system crosses the metal-insulator boundary, characterized by the emergence of new hole pockets at the center of the Brillouin zone. The calculated band gap aligns closely with experimental results obtained from single crystal samples, reinforcing the validity of our theoretical approach.\n\nFurthermore, we propose the existence of two competing nematic phases in proximity to the metal-insulator boundary. One phase exhibits in-plane anisotropy aligned with the Ru-O-Ru bond direction, while the other displays out-of-plane anisotropy, oriented diagonally to this direction. These findings contribute to a deeper understanding of the structural degradation observed in bilayer ruthenates.\n\nBilayer ruthenates have attracted significant attention due to their diverse physical properties, which include unconventional superconductivity, quantum criticality, and multiferroicity. Among these materials, Sr3Ru2O7 stands out for its ability to transition smoothly between metallic and insulating states through methods such as chemical doping or the application of external pressure. Recent experimental investigations have sought to elucidate the nature of the MIT. For instance, angle-resolved photoemission spectroscopy has demonstrated substantial changes in the Fermi surface topology when crossing the MIT threshold. Additionally, X-ray scattering studies have shown a symmetry reduction from tetragonal to orthorhombic below the critical temperature (TMI = 160 K), while neutron scattering experiments have indicated alterations in lattice parameters for both the ab plane and c-axis below TMIT ~ 150 K. Despite these extensive studies, the underlying microscopic mechanisms driving the MIT remain elusive, warranting further exploration.",
        "ori-fast-z-score": 0.7745966692414834,
        "water-fast-z-score": 6.11070705734948,
        "rewrite-fast-z-score": 1.191759143062248
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Mechanical Proof of the Second Law of Thermodynamics Based on Volume Entropy .\nAbstract:\nThe second law of thermodynamics is one of the most important laws in physics, which states that entropy always increases with time for closed systems. In this work we present an experimental proof of the second law based on volume entropy and mechanical energy dissipation. We show how to measure volume entropy by using two different methods (one direct method and another indirect method) and then compare these results with theoretical predictions. The experiments are performed at room temperature and atmospheric pressure conditions. Our results confirm that volume entropy always increases with time as predicted by theory. This result also confirms that our measurement system works properly. Finally, we discuss some possible applications of our approach. The second law of thermodynamics has been studied extensively over many years  1  . It states that entropy always increases when matter undergoes irreversible processes  2  , such as heat transfer or chemical reactions  3  .\nIn recent decades there have been several attempts to prove experimentally the validity of the second law  4  -  8  . However, none of them were able to provide a complete proof because they did not take into account all relevant physical quantities involved  9  . For example, it was shown theoretically  10  that volume entropy should be included in order to obtain a complete description of the process under study  11  . Therefore, in this work we propose a new experiment aimed at proving the second law of thermodynamics by measuring volume entropy directly  12  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Mechanical Proof of the Second Law of Thermodynamics Based on Volume Entropy . Abstract : The second rule of thermodynamics is one of the most important rules in science , which says that entropy often increases with time for closed systems .In this study we present an experimental confirmation of the second law based on volume entropy and mechanical energy dissipation . We see how to measure volume entropy by using two different methods ( one direct technique and another indirect method ) and then match these results with theoretical estimates .The studies are performed at room temperature and atmospheric pressure environments . Our results verify that volume entropy often increases with time as predicted by theoretical .This result also proves that our measurement machine operates properly . Finally , we explain some possible users of our approach .The second law of thermodynamics has been studied thoroughly over numerous years 1 . It says that entropy often increases when mind undergoes irreversible processes 2 , such as heat transfer or chemical processes 3 .In past decades there have been numerous attempts to confirm experimentally the validity of the second rule 4 - 8 . However , none of them were could to provide a complete proving because they did not take into consideration all relevant physical substances involved 9 .For instance , it was shown theoretically 10 that volume entropy should be included in order to obtain a complete model of the process under experiment 11 . Therefore , in this study we undertake a new study aiming at finding the second law of thermodynamics by observing volume entropy directly 12 .",
        "rewrite_text": "**Title: Mechanical Proof of the Second Law of Thermodynamics Based on Volume Entropy**\n\n**Abstract:** The second law of thermodynamics is a fundamental principle in science, asserting that entropy tends to increase over time in closed systems. This article presents an experimental validation of this law through the lens of volume entropy and mechanical energy dissipation. We detail our methodology for measuring volume entropy using two distinct approaches: a direct measurement technique and an indirect method. The experiments were conducted under controlled conditions at room temperature and atmospheric pressure, allowing us to compare our empirical findings with theoretical predictions. Our results consistently demonstrate that volume entropy increases over time, aligning with theoretical expectations and confirming the functionality of our measurement apparatus. \n\nThe second law has been extensively examined over the years, highlighting that entropy increases during irreversible processes, such as heat transfer and chemical reactions. Despite numerous experimental attempts to validate this law in recent decades, previous studies have often fallen short of providing comprehensive proof, primarily due to the exclusion of critical physical variables. Notably, theoretical work has indicated that incorporating volume entropy is essential for developing a complete model of the processes under investigation. \n\nIn this study, we aim to bridge this gap by directly observing volume entropy, thereby reinforcing the second law of thermodynamics. Our findings not only support the theoretical framework but also open avenues for practical applications of our measurement techniques in various scientific fields. This research contributes to the ongoing discourse surrounding thermodynamic principles and underscores the importance of volume entropy in understanding the behavior of closed systems.",
        "ori-fast-z-score": -1.6728567220186852,
        "water-fast-z-score": 6.717514421272201,
        "rewrite-fast-z-score": -0.6810052246069989
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  IRAS 18511+0146: a proto Herbig Ae/Be cluster? .\nAbstract:\nWe present near-infrared (NIR) spectroscopy and photometry for the young open cluster IRAS 18511+0146; we find that it is likely to be an intermediate mass star forming region, with a total luminosity of ~10^6 L_Sun . We identify two distinct populations in this cluster; one population has spectral types ranging between F0-F5V, while another population shows signs of emission lines indicative of accretion disks around pre-main sequence stars. The latter group appears to have ages less than 10 Myr based on their H-R diagram positions. This suggests that these are very young objects which may still be embedded within their natal molecular cloud. Using N-body simulations, we show that such clusters can evolve into open clusters like those observed today if they survive disruption by tidal forces during their formation process. In addition, we also estimate the age spread among members of our sample using the equivalent widths of Pa-alpha line profiles as well as the strength of the Brackett gamma line.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : IRAS 18511 + 0146 : a proto Herbig Ae / Be cluster ? .Abstract : We present near - infrared ( NIR ) spectroscopy and photometry for the young open cluster IRAS 18511 + 0146 ; we find that it is probably to be an intermediate mass star producing zone , with a total luminosity of ~ 10 ^ 6 L _ Sun . We distinguish two different populations in this cluster ; one community has spectral classes ranging between F0 - F5V , while another population shows signs of emission lines indicative of accretion disks around post - principal sequence stars .The latter group appears to have ages less than 10 Myr based on their H - R diagram positions . This implies that these are very young objects which perhaps still be embedded within their natal molecular cloud .Using N - bodies simulations , we find that such clusters can evolve into open complexes like those observed nowadays if they survive disruption by waves forces during their formed phase . In addition , we also predict the age distribution among members of our sample using the equivalent widths of Pa - alpha line profiles as well as the strength of the Brackett alpha line .",
        "rewrite_text": "We present a comprehensive analysis of the young open cluster IRAS 18511 + 0146 through near-infrared (NIR) spectroscopy and photometry. Our findings suggest that this cluster is likely an intermediate-mass star formation region, exhibiting a total luminosity of approximately 10^6 L_Sun. Within this cluster, we identify two distinct stellar populations. The first population consists of stars with spectral classifications ranging from F0 to F5V, while the second population displays emission lines that are characteristic of accretion disks surrounding post-main-sequence stars. The latter group is estimated to be less than 10 million years old, as inferred from their positions on the Hertzsprung-Russell diagram, indicating that these stars are still in their formative stages and may be partially obscured by their original molecular cloud. \n\nTo further understand the evolutionary pathways of such clusters, we conducted N-body simulations, which reveal that these systems have the potential to develop into open clusters similar to those observed in the present day, provided they can withstand disruptions caused by external tidal forces during their formation phase. Additionally, we explore the age distribution of the cluster members by analyzing the equivalent widths of the Pa-alpha line profiles and the intensity of the Brackett alpha line. This study contributes to our understanding of the formation and evolution of intermediate-mass star clusters, shedding light on the processes that govern star formation in such environments. Our results underscore the significance of IRAS 18511 + 0146 as a potential proto Herbig Ae/Be cluster, offering insights into the early stages of stellar evolution and the dynamics of young stellar populations.",
        "ori-fast-z-score": -1.078327732034384,
        "water-fast-z-score": 3.8376128944009875,
        "rewrite-fast-z-score": 0.3563483225498992
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  IR observations of MS 1054-03: Star Formation and its Evolution in Rich Galaxy Clusters .\nAbstract:\nWe present new infrared (IR) photometry for the galaxy cluster MS1054-03 at z = 0.83, obtained with ISOCAM on board ISO. The data are used to study star formation activity within this rich cluster environment. We find that the IR luminosity function is well fitted by a Schechter function with L* ~ 1 x 1012L⊙ and α ~ -1.7 over the range 8-1000 µm. This result suggests that there may be an excess number of faint galaxies compared to local clusters. In addition we detect several bright sources which have been identified as AGN candidates based upon their mid-IR colours. These objects appear to lie preferentially near the centre of the cluster suggesting they could be triggered by interactions between galaxies or mergers. Finally, we use our results together with published optical spectroscopy to investigate how the properties of individual galaxies evolve through time.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : IR measurements of MS 1054 - 03 : Star Formation and its Evolution in Rich Galaxy Clusters . Abstract : We report new infrared ( IR ) photometry for the galaxy region MS1054 - 03 at z = 0 . 83 , obtained with ISOCAM on board ISO .The data are using to study star formation activity within this rich cluster environment . We see that the IR luminosity function is well fitted by a Schechter function with L * ~ 1 x [UNK] and α ~ - 1 . 7 over the range 8 - 1000 µm .This result suggests that there may be an excess amount of distant galaxies compared to nearby clusters . In addition we find various bright sources which have been described as AGN candidates based upon their mid - IR colours .These structures appear to lay preferentially near the centre of the cluster suggesting they may be triggered by interactions between galaxies or mergers . Finally , we using our findings together with written optical spectroscopy to examine how the properties of different galaxies evolve through period .",
        "rewrite_text": "We present new infrared (IR) photometric observations of the galaxy cluster MS1054-03 at a redshift of z = 0.83, acquired using the ISOCAM instrument aboard the Infrared Space Observatory (ISO). This study aims to investigate the star formation activity within this densely populated cluster environment. Our analysis reveals that the IR luminosity function can be accurately described by a Schechter function, characterized by a characteristic luminosity L* approximately equal to 1 x [UNK] and a faint-end slope α around -1.7, across the wavelength range of 8 to 1000 µm. This finding indicates a potential overabundance of distant galaxies in comparison to those found in nearby clusters, suggesting a dynamic evolutionary process at play.\n\nFurthermore, we identify several luminous sources within the cluster that exhibit mid-IR colors consistent with active galactic nuclei (AGN) candidates. Notably, these AGN candidates are predominantly located near the cluster's center, implying that their formation may be influenced by interactions between galaxies or through merger events. This spatial distribution raises intriguing questions about the role of environmental factors in the evolution of galaxy properties.\n\nIn conjunction with our IR data, we incorporate existing optical spectroscopy to provide a comprehensive examination of how the characteristics of various galaxies evolve over time. By integrating these findings, we aim to enhance our understanding of star formation mechanisms and the evolutionary pathways of galaxies in rich cluster environments. This research contributes to the broader discourse on galaxy formation and evolution, particularly in the context of high-density regions of the universe.",
        "ori-fast-z-score": -0.11547005383792514,
        "water-fast-z-score": 5.196152422706631,
        "rewrite-fast-z-score": 0.8980265101338746
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Fine Structure of Solar Prominences .\nAbstract:\nThe fine structure of solar prominences is studied by using the data obtained with the Swedish 1-meter Solar Telescope (SST). The observations are made in Hα and Ca II 8542 Å lines, which show that the fine structures have different shapes depending on their locations relative to the magnetic field. In addition, we find that there exist two types of fine structures; one type has an elongated shape along the direction parallel to the local magnetic field while another type shows a roundish shape perpendicularly to it. We also found that some fine structures appear as if they were twisted around each other. These results suggest that the fine structures may be formed due to the plasma flows driven by magnetic reconnection between neighboring flux tubes. Keywords: Solar prominence, Fine structure, Magnetic field, Plasma flow, Reconnection. 1 Introduction Solar prominences are observed as dark features against the bright background of the photosphere. They are thought to consist mainly of cool dense plasma suspended above the solar surface by magnetic fields (Kippenhahn & Schlüter 1957) . It was suggested that the fine structures seen within solar prominences might be caused by the plasma flows driven by the magnetic reconnection between neighboring magnetic flux tubes (Pneuman 1983 , Kuperus et al. 1981 . However, the detailed physical processes involved in this process remain unclear because of lack of observational evidence for such phenomena. Recently, high-resolution observations of solar prominences have been performed with various instruments including the Swedish 1-meter solar telescope (SST) (Lin et al. 1998a) , the Advanced Stokes Polarimeter (ASP) at Big Bear Solar Observatory (BBSO), and the Hinode satellite (Kosugi et al. 2007 ). Using these new data sets, several authors reported the observation of fine structures having different shapes depending on their positions relative to the magnetic field (Lin et al. 1998b , Lin 2004 , Berger et al. 2008 .\nIn this study, we investigate the fine structures of solar prominences based on the SST data set. Our aim is to",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Fine Structure of Solar Prominences . Abstract : The fine structure of thermal prominences is studied by using the information obtained with the Swedish 1 - meter Solar Telescope ( SST ) .The findings are making in Hα and Ca II 8542 Å lines , which show that the fine structures have different shapes depending on their regions relative to the magnetic field . In addition , we find that there exist two forms of fine structures ; one sort has an elongated form along the direction parallel to the local magnetic force while another type gives a roundish shape perpendicularly to it .We also discovered that some fine structures appear as if they were twisted around each other . These conclusions show that the fine structures could be formed owing to the plasma flows driven by magnetic reconnection between neighboring flux tubes .Keywords : Solar prominence , Fine structure , Magnetic field , Plasma transfer , Reconnection . 1 Introduction Solar prominences are observed as dark features against the bright background of the photosphere .They are said to consist mostly of cold dense plasma held above the solar surface by magnetic fields ( Kippenhahn & Schlüter 1957 ) . It was suggested that the fine structures visible within solar prominences might be caused by the plasma flows driven by the magnetic reconnection between neighboring magnetic flux tubes ( Pneuman 1983 , Kuperus et al .1981 . However , the detailed physical processes implicated in this process remain uncertain because of lack of observational evidence for such observations .Recently , large - resolution measurements of sun prominences have been performed with various instruments including the Swedish 1 - meter solar observatory ( SST ) ( Lin et al . 1998a ) , the Advanced Stokes Polarimeter ( ASP ) at Big Bear Solar Observatory ( BBSO ) , and the Hinode satellite ( Kosugi et al .2007 ) . Using these new data sets , various scientists reported the observation of fine structures having various shapes depending on their orientation relative to the magnetic force ( Lin et al .1998b , Lin 2004 , Berger et al . 2008 .In this study , we investigate the fine structures of solar prominences based on the SST results setting . Our aim is to",
        "rewrite_text": "create a comprehensive understanding of the intricate details of thermal prominences by analyzing high-resolution data obtained from the Swedish 1-meter Solar Telescope (SST). This research focuses on the spectral lines of Hα and Ca II 8542 Å, revealing that the fine structures within solar prominences exhibit varying shapes that correlate with their orientation relative to the local magnetic field. Our findings indicate the presence of two distinct types of fine structures: one type is elongated and aligns parallel to the local magnetic forces, while the other type appears more rounded and is oriented perpendicularly to these forces. Additionally, we observed instances where fine structures seem to be intertwined, suggesting a complex interplay of magnetic influences. These observations support the hypothesis that the formation of these fine structures is influenced by plasma flows resulting from magnetic reconnection events occurring between adjacent magnetic flux tubes. \n\nSolar prominences, characterized as dark features against the bright solar photosphere, are primarily composed of cold, dense plasma suspended above the solar surface by magnetic fields, as noted by Kippenhahn & Schlüter (1957). Previous studies have proposed that the fine structures within these prominences may arise from plasma flows driven by magnetic reconnection, as suggested by Pneuman (1983) and Kuperus et al. (1981). However, the lack of observational evidence has left the detailed physical mechanisms underlying these processes largely unresolved. Recent advancements in observational techniques, including high-resolution measurements from the SST, the Advanced Stokes Polarimeter (ASP) at Big Bear Solar Observatory (BBSO), and the Hinode satellite, have provided new insights into the shapes and orientations of fine structures within solar prominences (Lin et al. 1998a; Kosugi et al. 2007). This study aims to further elucidate the fine structures of solar prominences using the SST data, contributing to a deeper understanding of the dynamic processes at play in solar atmospheric phenomena.",
        "ori-fast-z-score": 1.4974097718542911,
        "water-fast-z-score": 7.014182615527996,
        "rewrite-fast-z-score": 2.31729648808663
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  On Correlated Random Walks and 21-cm Fluctuations During Cosmic Reionization .\nAbstract:\nWe present an analytic model for the evolution of the 21 cm brightness temperature fluctuations during cosmic reionization, based on correlated random walks (CRWs). We show that CRW models can reproduce many features observed in numerical simulations of reionization, including the power spectrum at large scales, as well as the characteristic shape of the cross-correlation between different redshifts. In addition to these results, we find that our model predicts a new feature which is not seen in previous work -the presence of large-scale correlations even after reionization has completed. This effect may be detectable with future radio telescopes such as SKA. The 21cm line emission from neutral hydrogen provides us with a unique probe into the early universe. It allows one to study the process of reionization when most of the matter was still dark and cold gas clouds were surrounded by ionized bubbles  1  . However, this signal is extremely weak compared to other foregrounds produced by astrophysical sources  2  , so it will take several years before we are able to detect it directly  3  .\nIn order to make predictions about what kind of signals we should expect to see once observations become possible, theoretical studies have been performed using both semi-analytic  4  and fully numerical methods  5  . These works have shown that there exist two main types of signatures associated with reionization  6  : 1) the global signature of the average ionization fraction; 2) the local signature of individual HII regions. While the first type of signal is relatively easy to measure  7, 8  , the second type requires more advanced techniques  9  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : On Correlated Random Walks and 21 - cm Fluctuations During Cosmic Reionization . Abstract : We create an analytic model for the evolution of the 21 cm brightness temperature fluctuations during cosmic reionization , relying on correlated random tours ( CRWs ) .We see that CRW models can mimic several characteristics found in mathematical simulations of reionization , notably the power spectrum at large scales , as well as the typical shape of the cross - correlation between various redshifts . In addition to these results , we find that our model predicts a new feature which is not seen in earlier work - the presence of large - scale correlations even after reionization has completed .This phenomenon might be detectable with potential radio telescopes such as SKA . The 21cm line emission from neutral hydrogen gives us with a unique probe into the early universe .It enables one to study the process of reionization when most of the matter was still dark and cold gas clouds were dispersed by ionized bubbles 1 . However , this signal is incredibly weak compared to other foregrounds obtained by astrophysical sources 2 , so it will take many years before we are able to locate it directly 3 .In order to make predictions about what sort of transmissions we should predict to see once discoveries become possible , theoretical experiments have been performed using both semi - analytic 4 and fully quantitative methods 5 . These works have shown that there exist two principal kinds of signatures identified with reionization 6 : 1 ) the global signature of the average ionization fraction ; 2 ) the local signature of individual HII centers .While the first kinds of signal is fairly easy to measure 7 , 8 , the second kind needs more advanced techniques 9 .",
        "rewrite_text": "In this article, we present a comprehensive analytical model that explores the evolution of 21 cm brightness temperature fluctuations during the epoch of cosmic reionization, utilizing the framework of correlated random walks (CRWs). Our findings indicate that CRW models effectively replicate several key features observed in mathematical simulations of reionization, particularly the power spectrum at large scales and the characteristic shape of the cross-correlation across different redshifts. Notably, our model reveals a novel aspect that has not been previously documented: the emergence of large-scale correlations persisting even after the completion of reionization. This intriguing phenomenon holds the potential for detection by advanced radio telescopes, such as the Square Kilometre Array (SKA).\n\nThe 21 cm line emission from neutral hydrogen serves as a unique observational tool for probing the early universe, allowing researchers to investigate the reionization process that occurred when the majority of matter was still in a dark and cold state, interspersed with ionized bubbles. However, the signal from this emission is exceedingly faint compared to various foregrounds produced by astrophysical sources, making direct detection a challenging endeavor that may require many years of observation.\n\nTo facilitate predictions regarding the types of signals that may be observed once detection becomes feasible, we have conducted theoretical experiments employing both semi-analytic and fully quantitative methodologies. Previous studies have identified two primary signatures associated with reionization: the global signature reflecting the average ionization fraction and the local signature corresponding to individual HII regions. While the global signature is relatively straightforward to measure, the local signature necessitates more sophisticated techniques for accurate detection and analysis. Our work contributes to the understanding of these signatures and their implications for future observational campaigns aimed at unraveling the complexities of cosmic reionization.",
        "ori-fast-z-score": -0.5222329678670935,
        "water-fast-z-score": 6.092717958449424,
        "rewrite-fast-z-score": 1.986254132645683
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A possible explanation why the Theta+ is seen in some experiments and not in others .\nAbstract:\nThe theta + (θ+) effect has been observed by many authors, but it was never explained satisfactorily.  In this work we propose an explanation for its origin based on the fact that θ+ appears only when there are two or more particles with opposite spin states present at the same time. We show how to calculate the probability amplitude of finding such pairs using Feynman diagrams. The results obtained agree well with those reported experimentally. This article is available from: http://arxiv.org/abs/1401.5189. The theta + (θ+ ) effect  1  , which consists of a small increase in the intensity of the scattered beam as compared to the unscattered one, has been observed by several authors  2  -  4  . However, no satisfactory explanation has yet been given for its origin  5  .\nIn this work we propose an interpretation for the appearance of θ+ based on the fact that it occurs only if there are simultaneously two or more particles with different spins  6  . To demonstrate our hypothesis, we use Feynman s diagrammatic method  7  to calculate the probability amplitudes corresponding to these processes  8  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A potential explanation why the Theta + is seen in some experiments and not in others . Abstract : The theta + ( θ + ) effect has been observed by many writers , but it was none explained satisfactorily .In this study we propose an reason for its origin relying on the fact that θ + occurs only when there are two or more particles with opposite spin states present at the same time . We see how to estimate the probability intensity of finding such pairs using Feynman diagrams .The results derived agree well with those published experimentally . This section is accessible from : www : / / arxiv . org / abs / 1401 . 5189 .The theta + ( θ + ) effect 1 , which consists of a small increase in the frequency of the scattered beam as compared to the unscattered one , has been observed by various authors 2 - 4 . However , no satisfactory alternative has already been given for its origin 5 .In this study we propose an interpretation for the appearance of θ + based on the fact that it exists only if there are simultaneously two or more particles with different spinning 6 . To show our theory , we using Feynman s diagrammatic method 7 to estimate the probability amplitudes corresponding to these mechanisms 8 .",
        "rewrite_text": "Title: A Potential Explanation for the Discrepancies in Observations of the Theta + Particle\n\nAbstract: The theta + (θ+) effect has been documented by numerous researchers, yet a comprehensive explanation for its occurrence remains elusive. In this article, we propose a novel interpretation of the θ+ phenomenon, positing that its manifestation is contingent upon the simultaneous presence of two or more particles exhibiting opposite spin states. Our analysis employs Feynman diagrams to estimate the probability amplitudes associated with the formation of such particle pairs, thereby providing a quantitative framework for understanding the θ+ effect. The findings derived from our approach align closely with experimental results previously reported in the literature, reinforcing the validity of our hypothesis. The θ+ effect is characterized by a subtle increase in the frequency of the scattered beam compared to that of the unscattered beam, a phenomenon that has been observed by various authors in multiple experimental settings. Despite these observations, a satisfactory theoretical explanation for the origin of the θ+ effect has yet to be established. Our study aims to fill this gap by elucidating the conditions under which the θ+ effect arises, specifically highlighting the necessity of having particles with differing spin states present concurrently. By leveraging Feynman’s diagrammatic method, we provide a systematic approach to estimating the likelihood of these configurations, thereby shedding light on the underlying mechanisms that contribute to the θ+ effect. This work not only advances our understanding of this intriguing phenomenon but also opens avenues for further experimental investigations. For more detailed information, the full text can be accessed at www:/arxiv.org/abs/1401.5189.",
        "ori-fast-z-score": -0.9878783399072131,
        "water-fast-z-score": 5.300713251592634,
        "rewrite-fast-z-score": 1.0169503597462533
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Clustering with Lattices in the Analysis of Graph Patterns .\nAbstract:\nWe present an algorithm for clustering graphs based on their patterns, which are represented as lattices. The proposed method is applied to analyze the similarity between chemical compounds and protein structures. We show that our approach can be used to identify similar substructures within large databases efficiently. In addition, we demonstrate how it can be utilized to detect structural changes caused by mutations or modifications. Our experiments reveal that the presented technique outperforms state-of-the-art methods significantly both in terms of accuracy and efficiency. Clustering algorithms have been widely studied in many fields such as data mining  1  , bioinformatics  2  , computer vision  3  , etc., due to its importance in discovering hidden information  4  . However, most existing approaches focus only on finding clusters without considering any additional information about them  5  .\nIn this work, we propose a novel graph clustering algorithm called CLUSTERING WITH LATTERS (CLL)  6  , which takes advantage of lattice representation  7, 8  to capture the pattern of each cluster. As shown in Figure 1 , CLL first transforms all input graphs into their corresponding lattices using a set of predefined rules  9  . Then, it performs hierarchical agglomerative clustering  10  over these lattices to find groups of similar graphs. Finally, it assigns each group to one cluster according to the majority vote among all members belonging to the same group  11  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Clustering with Lattices in the Analysis of Graph Patterns . Abstract : We present an algorithm for clustering graphs based on their patterns , which are represented as lattices .The proposed approach is applied to analyze the similarity between chemical molecules and protein compounds . We see that our approach can be used to identify similar substructures within large databases accurately .In addition , we prove how it can be used to identify structural modifications affected by genes or modifications . Our experiments indicate that the offered technique outperforms state - of - the - art methods significantly both in terms of precision and efficiency .Clustering methods have been widely explored in different fields such as data extraction 1 , bioinformatics 2 , computer vision 3 , etc . , owing to its significance in discovering hiding information 4 . However , most existing techniques concentrate only on discovering clusters without examining any additional information about them 5 .In this research , we develop a new graph clustering procedure entitled CLUSTERING WITH LATTERS ( CLL ) 6 , which gives advantage of lattice representation 7 , 8 to capture the pattern of each cluster . As seen in Figure 1 , CLL initially transforms all output graphs into their corresponding lattices using a setting of predefined rules 9 .Then , it performs hierarchical agglomerative clustering 10 over these lattices to find groups of related graphs . Finally , it gives each group to one cluster according to the majority decision among all members belonging to the same group 11 .",
        "rewrite_text": "In this article, we introduce a novel algorithm for clustering graphs based on their underlying patterns, which are represented through lattice structures. Our method is particularly applied to assess the similarity between chemical molecules and protein compounds, demonstrating its capability to accurately identify similar substructures within extensive databases. Furthermore, we establish that our approach can effectively detect structural modifications influenced by genetic factors or other alterations. Experimental results reveal that our technique significantly surpasses existing state-of-the-art methods in both precision and efficiency.\n\nClustering techniques have been extensively studied across various domains, including data extraction, bioinformatics, and computer vision, due to their critical role in uncovering hidden information. However, many current methodologies primarily focus on identifying clusters without delving into additional insights regarding their characteristics. To address this gap, we propose a new graph clustering framework named CLUSTERING WITH LATTICES (CLL). This innovative approach leverages lattice representation to encapsulate the patterns inherent in each cluster.\n\nAs illustrated in Figure 1, the CLL algorithm begins by converting all output graphs into their corresponding lattice forms based on a set of predefined rules. Subsequently, it employs hierarchical agglomerative clustering techniques on these lattices to identify groups of related graphs. Finally, each identified group is assigned to a single cluster based on the majority decision of its constituent members. This methodology not only enhances the clustering process but also provides deeper insights into the structural relationships among the analyzed graphs, paving the way for more informed interpretations in various scientific fields.",
        "ori-fast-z-score": -1.044073795327749,
        "water-fast-z-score": 6.3028298181701015,
        "rewrite-fast-z-score": -0.43355498476205995
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  GRI: The Gamma-Ray Imager mission .\nAbstract:\nThe GRI (Gamma Ray Imager) is an astrophysics space observatory proposed by the French Space Agency CNES and NASA to study gamma-ray bursts, active galactic nuclei and other phenomena in high-energy astronomy.  It will be launched on a Soyuz rocket with a Fregat upper stage into a Sun-Earth L2 orbit at 1 AU distance from Earth. Its main instrument consists of two coded mask telescopes observing simultaneously in the energy range 20 MeV - 300 GeV. Each telescope has a large field-of-view of 2 steradians and a spatial resolution better than 0.1 degrees. A third detector module provides additional information about the background radiation environment for each telescope. This article describes the design concept of this innovative instrument. Keywords: Astroparticle physics, High-Energy Astronomy, Instrumentation, Space Observatory. Introduction:  The GRI (Gamma Ray...  more",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : GRI : The Gamma - Ray Imager mission . Abstract : The GRI ( Gamma Ray Imager ) is an astrophysics space observatory proposed by the French Space Agency CNES and NASA to study alpha - ray bursts , active galactic nuclei and other processes in high - energy astronomy .It will be launched on a Soyuz rocket with a Fregat upper stage into a Sun - Earth L2 orbit at 1 AU distance from Earth . Its primary instrument consists of two coded mask telescopes observing simultaneously in the power range 20 MeV - 300 GeV .Each observatory has a large field - of - view of 2 steradians and a spatial resolution good than 0 . 1 degrees . A third detector module supplies additional information about the background radiation conditions for each telescope .This page describes the design concept of this innovative instrument . Keywords : Astroparticle science , High - Energy Astronomy , Instrumentation , Space Observatory .Introduction:  The GRI (Gamma Ray...  more",
        "rewrite_text": "**Title: GRI: The Gamma-Ray Imager Mission**\n\n**Abstract:** The Gamma-Ray Imager (GRI) is an advanced astrophysics space observatory developed through a collaboration between the French Space Agency CNES and NASA, aimed at exploring high-energy phenomena such as gamma-ray bursts and active galactic nuclei. Scheduled for launch aboard a Soyuz rocket equipped with a Fregat upper stage, GRI will be positioned in a Sun-Earth L2 orbit, approximately 1 AU from Earth. The mission's primary scientific instrument features two coded mask telescopes that will operate concurrently across a wide energy range of 20 MeV to 300 GeV. Each telescope is designed with an expansive field of view of 2 steradians and boasts a spatial resolution exceeding 0.1 degrees, enabling detailed observations of cosmic gamma-ray sources. Additionally, a third detector module will provide crucial data regarding the background radiation environment for each telescope, enhancing the overall measurement accuracy and reliability. This document outlines the innovative design and operational concept of the GRI, highlighting its potential contributions to the fields of astroparticle physics and high-energy astronomy. By facilitating unprecedented observations of high-energy cosmic events, GRI aims to deepen our understanding of the universe's most energetic processes and phenomena. \n\n**Keywords:** Astroparticle science, High-Energy Astronomy, Instrumentation, Space Observatory.",
        "ori-fast-z-score": 1.1338934190276817,
        "water-fast-z-score": 4.341215710622296,
        "rewrite-fast-z-score": 0.9138115486202573
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Connection between Star-Forming Galaxies, AGN Host Galaxies and Early-Type Galaxies in the SDSS .\nAbstract:\nWe present an analysis of the connection between different galaxy types using data from the Sloan Digital Sky Survey (SDSS). We use two methods to classify galaxies into four types: star-forming galaxies (SFG), active galactic nuclei host galaxies (AGNHG), early-type galaxies with emission lines (ETGEL) and early-type galaxies without emission lines (ETGSIL).\nThe first method is based on the principal component analysis (PCA) applied to the optical spectra of all galaxies classified as spectroscopic targets by the SDSS pipeline. The second one uses the PCA applied only to the subset of galaxies that are morphologically selected for having bulges dominated by old stellar populations. In both cases we find that ETGs form a continuous sequence in terms of their spectral properties along which SFGs evolve towards ETGSILs through ETGELs. This evolutionary path can be described by a simple linear combination of three eigenvectors corresponding to the most prominent features seen in the mean spectrum of each type of galaxies.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Connection between Star - Forming Galaxies , AGN Host Galaxies and Early - Type Galaxies in the SDSS . Abstract : We present an assessment of the link between various galaxy types using data from the Sloan Digital Sky Survey ( SDSS ) .We use two means to classify galaxies into four types : star - creating galaxies ( SFG ) , active galactic nuclei guest galaxies ( AGNHG ) , early - class galaxies with emitted lines ( ETGEL ) and early - class galaxies without absorption lines ( ETGSIL ) . The first method is based on the main component analysis ( PCA ) applied to the optical spectra of all galaxies designated as spectroscopic targets by the SDSS pipeline .The second one uses the PCA applicable only to the subset of stars that are morphologically selected for having bulges dominated by ancient stars populations . In both cases we find that ETGs form a continuous sequence in terms of their spectral properties along which SFGs grow towards ETGSILs through ETGELs .This evolutionary progression can be described by a simple linear mixture of three eigenvectors corresponding to the most notable features found in the mean spectrum of each type of galaxies .",
        "rewrite_text": "We provide an in-depth analysis of the relationships among different types of galaxies, utilizing data from the Sloan Digital Sky Survey (SDSS). Our study categorizes galaxies into four distinct types: star-forming galaxies (SFGs), active galactic nuclei host galaxies (AGN HGs), early-type galaxies with emission lines (ETGELs), and early-type galaxies without absorption lines (ETGSILs). To achieve this classification, we employed two methodologies. The first method involves principal component analysis (PCA) applied to the optical spectra of all galaxies identified as spectroscopic targets by the SDSS pipeline. The second method focuses on a subset of galaxies, specifically those that are morphologically selected for having bulges predominantly composed of ancient stellar populations, and applies PCA to this group. Our findings reveal that early-type galaxies (ETGs) exhibit a continuous sequence in their spectral characteristics, illustrating a progression where SFGs evolve into ETGSILs through ETGELs. This evolutionary pathway can be effectively described by a straightforward linear combination of three eigenvectors, which correspond to the most significant features identified in the average spectrum of each galaxy type. This research enhances our understanding of galaxy evolution and the intricate connections between different galaxy classifications, shedding light on the processes that govern star formation and the activity of galactic nuclei in the broader context of cosmic evolution.",
        "ori-fast-z-score": -0.9878783399072131,
        "water-fast-z-score": 3.927922024247863,
        "rewrite-fast-z-score": 0.29277002188455997
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Crystalline silicates and dust processing in the protoplanetary disks of the Taurus young cluster .\nAbstract:\nWe present Spitzer Space Telescope observations at 24, 70, and 160 microns for 12 members of the nearby (140 pc) Taurus star-forming region with ages between 1 Myr to 10 Myr. We find that all sources show excess emission above photospheric levels indicative of circumstellar material surrounding each star. The majority of these objects are surrounded by optically thick disks which can be fit well using single temperature blackbody models. However, we also identify three systems where the disk is likely to have an inner hole or gap; TW Hya, DM Tau, and GM Aur. In addition, we detect two transitional disks around V4046 Sgr and Sz 91. These results suggest that most stars in our sample retain their primordial disks up until at least 5 Myr after formation. Finally, we use mid-infrared spectroscopy obtained with the IRS instrument onboard Spitzer to study the composition of the dust grains in the disks.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Crystalline silicates and dust production in the protoplanetary disks of the Taurus young cluster . Abstract : We report Spitzer Space Telescope observations at 24 , 70 , and 160 microns for 12 members of the nearby ( 140 pc ) Taurus star - creating area with ages between 1 Myr to 10 Myr .We see that all sources show extra emitted above photospheric concentrations indicative of circumstellar material surrounding each star . The majority of these objects are surrounded by optically dense disks which can be fit well using single temperature blackbody maps .However , we also identify three components where the disk is expected to have an inner cavity or gap ; TW Hya , DM Tau , and GM Aur . In addition , we identify two transitional disks around V4046 Sgr and Sz 91 .These data suggest that most stars in our sample maintain their primordial disks up until at least 5 Myr after formed . Finally , we utilize mid - infrared spectroscopy acquired with the IRS instrument onboard Spitzer to study the composition of the dust grains in the disks .",
        "rewrite_text": "We present findings from observations conducted with the Spitzer Space Telescope at wavelengths of 24, 70, and 160 microns, focusing on 12 stars within the nearby Taurus star-forming region, located approximately 140 parsecs away. The ages of these stars range from 1 million to 10 million years. Our analysis reveals that all observed sources exhibit excess emission beyond the expected photospheric levels, indicating the presence of circumstellar material surrounding each star. Most of these stars are enveloped by optically thick disks, which can be effectively modeled using single-temperature blackbody profiles. Notably, we identify three specific stars—TW Hya, DM Tau, and GM Aur—where the disk structure suggests the existence of an inner cavity or gap. Additionally, we recognize two transitional disks around the stars V4046 Sgr and Sz 91. The data imply that a significant number of stars in our sample retain their primordial disks for at least 5 million years post-formation. To further investigate the composition of the dust grains within these disks, we employed mid-infrared spectroscopy using the Infrared Spectrograph (IRS) onboard the Spitzer Space Telescope. This comprehensive study enhances our understanding of dust production and the evolution of crystalline silicates in protoplanetary disks, contributing valuable insights into the processes that govern star and planet formation in the early stages of stellar development.",
        "ori-fast-z-score": 0.11867816581938533,
        "water-fast-z-score": 4.949747468305833,
        "rewrite-fast-z-score": 0.30779350562554625
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Decoherence of Highly Mixed Macroscopic Quantum Superpositions .\nAbstract:\nWe study the decoherence of macroscopic quantum superposition states in open systems, and show that it is possible to prepare such states with high fidelity using current experimental techniques.  We consider two different types of macroscopic superposition: coherent spin states (CSS) and Schrödinger cat states (SCS). For CSS we find that the decay rate scales as $1/N$ where $N$ is the number of particles involved in the state. This scaling law can be understood by considering the effect of spontaneous emission on each particle separately. In contrast for SCS the decay rate scales as $1/sqrt(N)$ which cannot be explained by treating individual particles independently. Instead we argue that this behavior arises due to collective effects between all particles in the system. Finally, we discuss how these results could be tested experimentally. Decoherence of macroscopic superposition states has been studied extensively over recent years  1–3  . It was shown theoretically  4  , and confirmed experimentally  5, 6  , that macroscopic superposition states are extremely fragile against environmental noise  7, 8  . The main reason behind their fragility lies in the fact that they involve many particles, so even small interactions with an environment lead to rapid loss of coherence  9  .\nIn this work we focus on studying the decoherence process of macroscopic superposition in open systems. Specifically, we consider two different types of superposition: coherent spin states  10  and Schrödinger cat-states  11  . Coherent spin states have already been prepared experimentally  12  while Schrödinger cat states remain elusive  13  . However, there exist proposals for preparing them  14–18  , and recently some progress towards realizing them has been made  19, 20  .  Our analysis shows that both types of superposition exhibit qualitatively similar behaviour under decoherence.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Decoherence of Highly Mixed Macroscopic Quantum Superpositions . Abstract : We research the decoherence of macroscopic quantum superposition states in open systems , and find that it is easy to analyze such states with high fidelity using current experimental methods .We consider two different kinds of macroscopic superposition : coherent spinning states ( CSS ) and Schrödinger cat states ( SCS ) . For CSS we find that the decay rate scales as $ 1 / N $ where $ N $ is the quantity of molecules implicated in the state .This scaling law can be understood by using the impact of spontaneous emission on each particle separately . In comparison for SCS the decay rate scales as $ 1 / sqrt ( N ) $ which cannot be described by treating individual particles jointly .Instead we explain that this behavior occurs due to collective effects between all particles in the system . Finally , we talk how these results could be evaluated experimentally .Decoherence of macroscopic superposition states has been studied thoroughly over recent periods 1 – 3 . It was shown theoretically 4 , and reported experimentally 5 , 6 , that macroscopic superposition states are extremely unstable against environmental noise 7 , 8 .The main explanation behind their fragility lies in the fact that they contain many particles , so even little interactions with an environment result to rapid loss of coherence 9 . In this research we focus on studying the decoherence cycle of macroscopic superposition in open systems .Specifically , we define two different kinds of superposition : coherent spinning states 10 and Schrödinger cat - states 11 . Coherent spin states have already been determined experimentally 12 while Schrödinger cat states stay elusive 13 .However , there remain attempts for preparing them 14 – 18 , and lately some progress towards considering them has been made 19 , 20 . Our study shows that both types of superposition demonstrate qualitatively identical actions under decoherence .",
        "rewrite_text": "In this article, we investigate the decoherence of macroscopic quantum superposition states within open systems, revealing that contemporary experimental techniques allow for high-fidelity analysis of these states. We focus on two distinct types of macroscopic superpositions: coherent spinning states (CSS) and Schrödinger cat states (SCS). Our findings indicate that the decay rate for CSS is inversely proportional to the number of molecules, represented as \\(1/N\\), where \\(N\\) denotes the number of particles involved. This scaling can be attributed to the effects of spontaneous emission acting on each particle independently. In contrast, for SCS, the decay rate follows a different scaling law, \\(1/\\sqrt{N}\\), which cannot be adequately explained by considering individual particles in isolation. Instead, we attribute this behavior to the collective interactions among all particles within the system. \n\nWe also discuss the potential for experimental evaluation of these results. The decoherence of macroscopic superposition states has been extensively studied in recent years, with both theoretical predictions and experimental observations confirming their extreme sensitivity to environmental noise. The primary reason for this vulnerability is the large number of particles involved, which leads to rapid coherence loss even from minimal environmental interactions. Our research specifically examines the decoherence processes of macroscopic superpositions in open systems, distinguishing between coherent spinning states and Schrödinger cat states. While coherent spin states have been experimentally verified, the realization of Schrödinger cat states remains a challenge, although recent efforts have shown promise in their preparation. Our study concludes that both CSS and SCS exhibit qualitatively similar behaviors when subjected to decoherence, providing valuable insights into the dynamics of macroscopic quantum systems in the presence of environmental influences.",
        "ori-fast-z-score": -1.4967665407535604,
        "water-fast-z-score": 5.918640302493727,
        "rewrite-fast-z-score": -2.32379000772445
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Very weak electron-phonon coupling and strong strain coupling in manganites .\nAbstract:\nWe report the observation of very weak electron-phonon coupling (EPC) in La0.7Sr0.3MnO3 thin films grown on SrTiO3 substrates by pulsed laser deposition, which is consistent with previous reports for bulk samples.  We also observe that EPC decreases as temperature increases up to 300 K. This behavior can be explained by considering the effect of lattice expansion due to thermal fluctuations at high temperatures. In addition, we find that the magnitude of EPC depends strongly on film thickness; it becomes smaller when the film thickness decreases down to 10 unit cells. The observed dependence of EPC on both temperature and film thickness suggests that phonon confinement plays an important role in determining the strength of EPC in these materials. Manganese oxides have been extensively studied because they exhibit many interesting physical properties such as colossal magnetoresistance  1  , metal-insulator transition  2  , and charge ordering  3  . Among them, La1-xSrxMnO3 has attracted much attention since its discovery  4  .\nIn this compound, Mn ions are located at two different sites, i.e., Mn3+(tetrahedral site) and Mn4+(octahedral site). It was found that the magnetic ground state changes from ferromagnetic insulator to antiferromagnetic insulator upon increasing x  5  . These phenomena were attributed to the competition between double exchange interaction  6  and superexchange interaction  7, 8  . However, there still remain some open questions about the origin of the electronic states in these compounds  9  . For example, the mechanism responsible for the insulating nature of these materials remains controversial  10  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Very weak atom - phonon coupling and strong strain coupling in manganites . Abstract : We report the observation of very weakened electron - phonon coupling ( EPC ) in La0 . 7Sr0 . 3MnO3 thin films grown on SrTiO3 substrates by pulsed infrared deposition , which is compatible with previous findings for bulk samples .We additionally observe that EPC decline as temperature increases up to 300 K . This phenomenon can be described by examining the impact of lattice increase due to heat fluctuations at high temperatures . In addition , we find that the magnitude of EPC depends strongly on film thickness ; it becomes lower when the film thickness decreases down to 10 unit cells .The observed influence of EPC on both heat and film thickness implies that phonon confinement plays an important role in indicating the strength of EPC in these materials . Manganese oxides have been heavily research because they show many interesting physical properties such as colossal magnetoresistance 1 , metal - insulator transition 2 , and charge ordering 3 .Among them , La1 - xSrxMnO3 has garnered considerable scrutiny since its discovery 4 . In this compound , Mn ions are situated at two different location , i . e . , Mn3 + ( tetrahedral site ) and Mn4 + ( octahedral site ) .It was shown that the magnetic ground state changes from ferromagnetic insulator to antiferromagnetic insulator upon increasing x 5 . These phenomena were attributed to the competition between second exchange interaction 6 and superexchange interaction 7 , 8 .However , there still continue some open questions about the origin of the electronic states in these compounds 9 . For instance , the process responsible for the insulating nature of these structures remains disputed 10 .",
        "rewrite_text": "We present our findings on the significantly reduced electron-phonon coupling (EPC) observed in La0.7Sr0.3MnO3 thin films, which were fabricated on SrTiO3 substrates using pulsed infrared deposition techniques. This reduction aligns with previous studies conducted on bulk samples of the same material. Notably, our experiments reveal that EPC diminishes with increasing temperature, reaching up to 300 K. This temperature-dependent behavior can be attributed to the expansion of the lattice structure caused by thermal fluctuations at elevated temperatures. Furthermore, we discovered that the strength of EPC is highly sensitive to the thickness of the films; specifically, EPC decreases as the film thickness approaches 10 unit cells. This relationship suggests that phonon confinement is a critical factor influencing the strength of EPC in these manganite materials.\n\nManganese oxides have been the focus of extensive research due to their intriguing physical properties, including colossal magnetoresistance, metal-insulator transitions, and charge ordering. Among these compounds, La1-xSrxMnO3 has attracted significant attention since its initial discovery. In this material, manganese ions occupy two distinct sites: Mn3+ in tetrahedral coordination and Mn4+ in octahedral coordination. It has been established that the magnetic ground state transitions from a ferromagnetic insulator to an antiferromagnetic insulator as the value of x increases. This transition is believed to stem from the interplay between second exchange interactions and superexchange interactions. Nevertheless, several questions remain unresolved regarding the electronic states within these compounds, particularly concerning the mechanisms that contribute to their insulating characteristics. The ongoing debate surrounding these issues highlights the complexity and richness of the physics underlying manganites, warranting further investigation to elucidate the fundamental processes at play.",
        "ori-fast-z-score": -0.08944271909999159,
        "water-fast-z-score": 6.171547617899419,
        "rewrite-fast-z-score": 0.2705008904002297
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The orbit, mass, size, albedo, and density of (65489) Ceto/Phorcys: A tidally-evolved binary Centaur .\nAbstract:\nWe report the discovery of an unusual object in the outer solar system that is likely to be composed primarily of ice particles. The new body, (65489) Ceto/Phorcys, was discovered by NASA s Wide-field Infrared Survey Explorer mission on September 24th 2010 at heliocentric distance of ~5 AU. It has been observed for more than two years with ground-based telescopes as well as space-borne observatories such as Spitzer Space Telescope and Herschel Space Observatory. We present our analysis of these data sets which reveal that this object is a binary Centaur whose components are locked into mutual tidal evolution. This unique combination of properties makes it one of only three known objects in the Solar System where both components have diameters larger than 100 km. Our results show that Phorcys is a very elongated ellipsoidal shape with dimensions of about 400 x 300 x 250 kilometers. Its surface temperature is estimated to be around -100 degrees Celsius.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The orbit , mass , size , albedo , and density of ( 65489 ) Ceto / Phorcys : A tidally - altered binary Centaur . Abstract : We report the discovery of an peculiar object in the outer solar system that is expected to be composed primarily of ice particles .The new bodies , ( 65489 ) Ceto / Phorcys , was discovered by NASA s Wide - field Infrared Survey Explorer mission on September 24th 2010 at heliocentric distance of ~ 5 AU . It has been observed for more than two years with ground - based telescopes as well as space - borne observatories such as Spitzer Space Telescope and Herschel Space Observatory .We present our analysis of these information sets which confirm that this body is a binary Centaur whose components are locked into mutual tidal evolution . This unique combination of properties makes it one of only three known objects in the Solar System where both components have diameters larger than 100 km .Our results show that Phorcys is a very elongated ellipsoidal structure with sizes of about 400 x 300 x 250 kilometers . Its surface temperature is predicted to be around - 100 degrees Celsius .",
        "rewrite_text": "**Title:** The Orbit, Mass, Size, Albedo, and Density of (65489) Ceto/Phorcys: A Tidally-Altered Binary Centaur\n\n**Abstract:** In this study, we present the discovery and detailed analysis of a unique celestial object located in the outer regions of our solar system, primarily composed of icy materials. Designated as (65489) Ceto/Phorcys, this binary Centaur was identified by NASA's Wide-field Infrared Survey Explorer (WISE) mission on September 24, 2010, at a heliocentric distance of approximately 5 astronomical units (AU). Over the course of more than two years, we have conducted extensive observations using both ground-based telescopes and space-based observatories, including the Spitzer Space Telescope and the Herschel Space Observatory. \n\nOur comprehensive analysis of the collected data confirms that Ceto/Phorcys is indeed a binary system, with its two components engaged in mutual tidal interactions. This intriguing characteristic places it among a select group of only three known objects in the Solar System where both components exceed 100 kilometers in diameter. Our findings reveal that Phorcys, one of the binary's components, exhibits a distinctly elongated ellipsoidal shape, measuring approximately 400 x 300 x 250 kilometers. Furthermore, we estimate its surface temperature to be around -100 degrees Celsius, indicative of its icy composition and the environmental conditions prevalent in the outer solar system.\n\nThe unique properties of (65489) Ceto/Phorcys not only enhance our understanding of binary Centaurs but also provide valuable insights into the formation and evolution of icy bodies in the outer solar system. This research contributes to the broader field of planetary science by elucidating the dynamics of tidal interactions and the physical characteristics of these intriguing celestial objects.",
        "ori-fast-z-score": 1.7801724872907798,
        "water-fast-z-score": 4.478342947514801,
        "rewrite-fast-z-score": 0.3713906763541037
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Maximal Amount of Gravitational Waves in the Curvaton Scenario .\nAbstract:\nWe study gravitational waves produced by curvaton scenario, where the inflaton field is coupled to another scalar field called curvaton which decays into radiation after inflation and produces primordial density fluctuations. We find that the amplitude of gravitational waves generated during inflation can be enhanced if the decay rate of curvaton is large enough compared with Hubble parameter at its decay time. In this case we show that the tensor-to-scalar ratio becomes larger than 0.1 for most values of parameters except when the mass of curvaton is very small or the coupling between inflaton and curvaton fields are extremely suppressed. This result may provide an explanation on why the recent observations give such a high value of tensor-to-scalar ratio. \n \n Introduction \n \n The current observational data  1  strongly suggest that there exists a significant amount of primordial gravitational waves (GWs) in our universe. If confirmed, it will have important implications not only for cosmology but also particle physics  2  . However, the origin of these GWs has been one of the biggest mysteries in modern cosmology  3  .\n \nIn order to explain the observed temperature anisotropies of cosmic microwave background (CMB), many models beyond standard model of particle physics were proposed  4  , among them supersymmetric grand unified theories  5  and supergravity  6  are well known examples. These models predict new particles whose masses lie around 10 16 GeV  7, 8  . It was shown  9  that the existence of such heavy particles could lead to successful inflationary scenarios  10  . On the other hand, the presence of such heavy particles would produce too much gravitons  11  unless their couplings to ordinary matter are highly suppressed  12  . Therefore, it seems difficult to generate sufficient amount of GWs within the framework of these models without conflicting with CMB observation  13  . \n \n Recently, however, several authors  14 -17  suggested that the production of GWs might be possible even though the inflaton does not couple directly to any heavy particles. They considered a situation where the inflaton field couples to another scalar field called  curvaton   18  through non-renormalizable interactions  19, 20  . After",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Maximal Amount of Gravitational Waves in the Curvaton Scenario . Abstract : We research gravitational waves produced by curvaton scenario , where the inflaton field is linked to another scalar field called curvaton which decays into radiation after inflation and causes primordial density fluctuations .We see that the amplitude of gravitational waves generated during inflation can be enhanced if the decay rate of curvaton is huge enough compared with Hubble parameter at its degradation rate . In this situation we prove that the tensor - to - scalar ratio becomes greater than 0 . 1 for most values of parameters except when the mass of curvaton is very small or the interaction between inflaton and curvaton fields are extremely suppressed .This result may provide an reason on why the recent observations give such a high value of tensor - to - scalar ratio . Introduction The present observational data 1 clearly suggest that there exists a substantial quantity of primordial magnetic waves ( GWs ) in our universe .If confirmed , it will have important implications not only for cosmology but also particle science 2 . However , the origin of these GWs has been one of the biggest mysteries in modern cosmology 3 .In order to explain the known temperature anisotropies of cosmic microwave background ( CMB ) , various models beyond standard theory of particle theory were offered 4 , among them supersymmetric grand unified physics 5 and supergravity 6 are well famous instance . These systems predict new ions whose masses sit around 10 16 GeV 7 , 8 .It was shown 9 that the existence of such heavy ions might lead to good inflationary scenarios 10 . On the other hand , the presence of such heavy particles might generate too much gravitons 11 unless their couplings to everyday matter are extremely suppressed 12 .Therefore , it appears hard to produce enough quantity of GWs within the framework of these models without conflicting with CMB observation 13 . Recently , however , various scientists 14 - 17 suggested that the production of GWs might be possible even though the inflaton does not couple directly to any massive particles .They considered a situation where the inflaton field couples to another scalar field called curvaton 18 through non - renormalizable interactions 19 , 20 . After",
        "rewrite_text": "**Title:** The Maximal Amount of Gravitational Waves in the Curvaton Scenario\n\n**Abstract:** This study investigates the generation of gravitational waves (GWs) within the curvaton framework, where the inflaton field interacts with an additional scalar field known as the curvaton. Following the inflationary period, the curvaton decays into radiation, leading to primordial density fluctuations. Our findings indicate that the amplitude of gravitational waves produced during inflation can be significantly amplified if the decay rate of the curvaton is sufficiently large in comparison to the Hubble parameter during its decay. Under these conditions, we demonstrate that the tensor-to-scalar ratio can exceed 0.1 for a wide range of parameter values, with the exception of scenarios where the curvaton mass is very low or the interaction between the inflaton and curvaton fields is highly suppressed. This result offers a potential explanation for the elevated tensor-to-scalar ratios observed in recent cosmological data.\n\nThe current observational evidence strongly suggests the presence of a considerable amount of primordial gravitational waves in the universe, which, if validated, would have profound implications for both cosmology and particle physics. However, the origins of these gravitational waves remain one of the most significant enigmas in contemporary cosmology. To account for the observed temperature anisotropies in the cosmic microwave background (CMB), numerous models extending beyond the standard particle theory have been proposed, including well-known frameworks such as supersymmetric grand unified theories and supergravity. These models predict the existence of new heavy particles with masses around 10^16 GeV, which could facilitate favorable inflationary scenarios. However, the presence of such heavy particles raises concerns about the potential overproduction of gravitons unless their couplings to ordinary matter are exceedingly weak. Consequently, it poses a challenge to generate a sufficient quantity of gravitational waves within these models without conflicting with CMB observations. Recently, several researchers have suggested that gravitational wave production could still occur even if the inflaton does not directly couple to massive particles, exploring scenarios where the inflaton interacts with the curvaton through non-renormalizable interactions.",
        "ori-fast-z-score": 0.39904344223381105,
        "water-fast-z-score": 7.262590648655362,
        "rewrite-fast-z-score": 2.6483388435275774
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  High Sensitivity Array Observations of the $z = 4.4$ QSO BRI 1335-0417 .\nAbstract:\nWe present new observations with the High Sensitivity Array (HSA) at 1.4 GHz and 5 GHz, as well as archival data obtained by the Very Large Array (VLA), in order to study the radio properties of the z=4.4 quasar BRI1335-0417. We detect two compact components separated by 0.3 arcsec on both sides of the optical position of this source. The total flux density is S5GHz=(1.6 ±0.2) mJy for component A and S1.4Ghz=0.7±0.1 mJy for component B. Both sources are unresolved within our beam size. From their spectral indices we conclude that they are most likely associated with an active galactic nucleus. This object has been observed previously using the VLA but only one component was detected. Our results show that there may be more than one AGN contributing to the emission seen in previous studies.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : High Sensitivity Array Observations of the $ z = 4 . 4 $ QSO BRI 1335 - 0417 . Abstract : We report new experiments with the High Sensitivity Array ( HSA ) at 1 . 4 GHz and 5 GHz , as also as archival measurements obtained by the Very Large Array ( VLA ) , in order to study the radio behavior of the z = 4 . 4 quasar BRI1335 - 0417 .We detect two compact components joined by 0 . 3 arcsec on both sides of the optical position of this source . The total flux concentration is S5GHz = ( 1 . 6 ±0 . 2 ) mJy for component A and S1 . 4Ghz = 0 . 7±0 . 1 mJy for component B .Both sources are unresolved within our laser size . From their spectral indices we estimate that they are most likely correlated with an active galactic nucleus .This object has been observed previously using the VLA but only one element was seen . Our results show that there may be more than one AGN contributing to the emission seen in earlier surveys .",
        "rewrite_text": "We present findings from our recent investigations utilizing the High Sensitivity Array (HSA) at frequencies of 1.4 GHz and 5 GHz, alongside archival data from the Very Large Array (VLA), to analyze the radio emissions of the quasar BRI 1335-0417 at redshift z = 4.4. Our observations reveal the presence of two compact radio components located approximately 0.3 arcseconds from the optical position of the quasar. The total flux densities measured are S5GHz = (1.6 ± 0.2) mJy for component A and S1.4GHz = (0.7 ± 0.1) mJy for component B. Notably, both components remain unresolved within the limits of our observational resolution. \n\nThrough the analysis of their spectral indices, we infer that these components are likely associated with an active galactic nucleus (AGN). Previous observations of BRI 1335-0417 using the VLA had only detected a single component, leading to the possibility that our findings indicate the existence of multiple AGN contributing to the radio emissions previously recorded. This suggests a more complex structure than previously understood, highlighting the importance of high-sensitivity observations in uncovering the intricate nature of distant quasars. Our results not only enhance the understanding of BRI 1335-0417 but also contribute to the broader knowledge of AGN behavior in the early universe, emphasizing the potential for further discoveries in high-redshift radio astronomy.",
        "ori-fast-z-score": -1.4832396974191326,
        "water-fast-z-score": 3.2071349029490928,
        "rewrite-fast-z-score": 0.10846522890932808
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Surprising Reversal of Temperatures in the Brown-Dwarf Eclipsing Binary 2MASS J05352184-0546085 .\nAbstract:\nWe report on an unexpected reversal of temperatures between two components of a brown-dwarf eclipsing binary system, which we have discovered using infrared photometry and spectroscopy obtained with Spitzer Space Telescope (Werner et al., 2004) and Gemini Observatory (Gemini North telescope). The primary component is cooler than its secondary by about 300 K at optical wavelengths but warmer by about 100 K at near-infrared wavelengths. We find that this temperature inversion can be explained if both stars are irradiated by their mutual accretion disk. This finding suggests that the disks around young low-mass objects may be more complex than previously thought. \n \n Keywords: Accretion Disk, Inverse P-Cygni profile, Irradiation, Low-Mass Star, Near-Infrared Spectroscopy, Photometric variability, Stellar radius, Temperature inversion, Young star \n \n \n \n 1 Introduction \n \n An important goal for understanding how planets form is to determine what happens during the earliest stages of planet formation when protoplanetary disks surround young stellar systems. One key question concerns whether or not these disks evolve into planetary systems like our own solar system. To answer such questions it will be necessary to study individual examples of young circumstellar disks as they evolve over time. However, because most young stars are deeply embedded within dense molecular clouds, direct observations of the inner regions of these disks are difficult. Fortunately, some young stars are surrounded by optically thin dusty envelopes that allow us to probe the physical conditions near the central object through scattered light. These so-called transitional disks show evidence of clearing out large amounts of material inside several AU of the central star while still retaining significant quantities of gas farther away (Strom et al., 1989; Skrutskie et al., 1990; Calvet et al., 2002; Muzerolle et al., 2003; Sicilia-Aguilar et al., 2006; Espaillat et al., 2007) . \n \n A number of studies suggest that the outer edges of transitional disks are sculpted by photoevaporative winds driven off the surface of the disk by intense ultraviolet radiation from nearby",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Surprising Reversal of Temperatures in the Brown - Dwarf Eclipsing Binary 2MASS J05352184 - 0546085 . Abstract : We report on an unexpected reversal of temperatures between two components of a brown - dwarf eclipsing binary system , which we have discovered using infrared photometry and spectroscopy obtained with Spitzer Space Telescope ( Werner et al . , 2004 ) and Gemini Observatory ( Gemini North telescope ) .The main component is cooler than its primary by about 300 K at visual wavelengths but warmer by about 100 K at near - infrared wavelengths . We see that this heat inversion can be understood if both stars are irradiated by their mutual accretion disk .This found shows that the disks around old minimum - mass bodies may be more sophisticated than previously thought . Keywords : Accretion Disk , Inverse P - Cygni profile , Irradiation , Low - Mass Star , Near - Infrared Spectroscopy , Photometric variability , Stellar radius , Temperature inversion , Young star 1 Introduction An key goal for knowledge how planets form is to study what comes during the earliest periods of planet development when protoplanetary disks surround young stellar systems .One key question concerns whether or not these disks evolve into planetary structures like our own solar body . To answer such concerns it will be required to study individual examples of young circumstellar disks as they develop over time .However , because most young galaxies are deeply lodged within dense molecular clouds , direct observations of the inner regions of these disks are problematic . Fortunately , some young galaxies are surrounded by optically thin dusty envelopes that enable us to probe the physical conditions near the main object through drifting light .These so - called transitional disks show proof of cutting out large quantities of debris inside several AU of the central star while already retaining substantial quantities of gas farther back ( Strom et al . , 1989 ; Skrutskie et al . , 1990 ; Calvet et al . , 2002 ; Muzerolle et al . , 2003 ; Sicilia - Aguilar et al . , 2006 ; Espaillat et al . , 2007 ) . A variety of studies propose that the exterior corners of transitional disks are sculpted by photoevaporative winds driven off the surface of the disk by intense ultraviolet radiation from nearby",
        "rewrite_text": "**Title:** A Surprising Reversal of Temperatures in the Brown-Dwarf Eclipsing Binary 2MASS J05352184-0546085\n\n**Abstract:** In this study, we present an intriguing phenomenon observed in the brown-dwarf eclipsing binary system 2MASS J05352184-0546085, where a temperature inversion between its two components has been identified. Utilizing infrared photometry and spectroscopy data collected from the Spitzer Space Telescope and the Gemini North telescope, we discovered that the primary component exhibits a temperature approximately 300 K cooler than its counterpart at visual wavelengths, yet conversely, it is about 100 K warmer at near-infrared wavelengths. This unexpected thermal behavior can be attributed to the influence of their mutual accretion disk, which appears to play a significant role in the thermal dynamics of the system. Our findings suggest that the accretion disks surrounding older, low-mass celestial bodies may possess a greater complexity than previously recognized. \n\nThis research contributes to the broader understanding of accretion processes and the evolution of circumstellar disks, particularly in the context of low-mass stars. The implications of this temperature inversion challenge existing models of stellar and disk interactions, highlighting the necessity for further investigation into the physical mechanisms at play. Keywords associated with this study include Accretion Disk, Inverse P-Cygni Profile, Irradiation, Low-Mass Star, Near-Infrared Spectroscopy, Photometric Variability, Stellar Radius, Temperature Inversion, and Young Star.\n\n**1 Introduction:** A fundamental objective in the study of planetary formation is to explore the early stages of planet development, particularly the dynamics occurring within protoplanetary disks that encircle young stellar systems. A pivotal question in this field is whether these disks can evolve into planetary systems akin to our solar system. To address this inquiry, it is essential to examine individual instances of young circumstellar disks and their evolution over time. However, direct observations of the inner regions of these disks are often hindered by their location within dense molecular clouds. Fortunately, some young stellar objects are enveloped in optically thin dusty shells, allowing for the investigation of their physical conditions through transmitted light. Transitional disks, in particular, provide evidence of significant debris removal within several astronomical units of the central star while still retaining considerable gas further out. Various studies suggest that the outer regions of these transitional disks are shaped by photoevaporative winds, which are driven by intense ultraviolet radiation from nearby stars.",
        "ori-fast-z-score": -0.40689422938557973,
        "water-fast-z-score": 6.8132996874920275,
        "rewrite-fast-z-score": 0.37582301400141443
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Measurement of Newtonian fluid slip using a torsional ultrasonic oscillator .\nAbstract:\nWe present an experimental method for measuring the slip length in Newtonian fluids at low Reynolds number by means of a torsional ultrasonic oscillator (TUSO). The TUSO is immersed into the liquid and driven to vibrate with angular frequency . A torque proportional to the shear stress acting on its surface causes it to rotate, which can be detected electrically via a pickup coil wound around the shaft. We show that this rotation leads to a phase lag between driving voltage and current through the pickup coil. This phase lag depends on the slip length as well as other parameters such as viscosity and density of the surrounding medium. By fitting our theoretical model to measurements we are able to extract the slip length from the data. Our results agree very well with previous experiments performed under similar conditions. In addition, we demonstrate how the measurement principle presented here could also be used to measure the slip length of non-Newtonian fluids.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Measurement of Newtonian flow slipping using a torsional ultrasonic oscillator . Abstract : We report an experimental method for determining the slip length in Newtonian fluids at low Reynolds number by means of a torsional ultrasonic oscillator ( TUSO ) .The TUSO is immersed into the liquid and driven to vibrate with angular frequency . A torque proportional to the shear force acting on its surface causes it to rotate , which can be identified electrically via a pickup coil wound around the shaft .We see that this rotation leads to a phase lag between driving voltage and current through the pickup coil . This phase lag depends on the slip length as well as other parameters such as viscosity and density of the nearby medium .By fitting our theory model to measurements we are able to extract the slip length from the information . Our results agree very well with previous tests accomplished under similar situations .In addition , we prove how the observation concept provided here could also be used to measure the slip length of non - Newtonian fluids .",
        "rewrite_text": "We present a novel experimental approach for quantifying the slip length in Newtonian fluids at low Reynolds numbers, utilizing a torsional ultrasonic oscillator (TUSO). The TUSO is submerged in the fluid and is driven to oscillate at a specific angular frequency. The shear force exerted on its surface generates a torque that induces rotation, which can be detected electrically through a pickup coil wrapped around the shaft of the oscillator. This induced rotation results in a measurable phase lag between the driving voltage and the current in the pickup coil. Notably, this phase lag is influenced by the slip length, as well as other fluid properties such as viscosity and density. By fitting our theoretical model to the experimental data, we can accurately extract the slip length from the observed phase lag. Our findings demonstrate a strong correlation with previous experiments conducted under comparable conditions, validating the effectiveness of our method. Furthermore, we extend the applicability of our observation technique to the measurement of slip lengths in non-Newtonian fluids, showcasing its versatility and potential for broader applications in fluid dynamics research. This work not only enhances our understanding of fluid behavior at the microscale but also opens new avenues for investigating complex fluid systems where slip phenomena play a critical role.",
        "ori-fast-z-score": 1.8439088914585775,
        "water-fast-z-score": 6.038635299392551,
        "rewrite-fast-z-score": 0.10153461651336192
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Fabrication of half metallicity in a ferromagnetic metal .\nAbstract:\nWe report the fabrication and characterization of epitaxial Fe3O4 films grown on MgO(001) substrates by pulsed laser deposition (PLD). The structural properties were investigated using X-ray diffraction, transmission electron microscopy, and atomic force microscopy techniques. We found that the film is single phase with an in-plane lattice constant of 0.84 nm which agrees well with previous reports for bulk Fe3O4. The magnetic properties are studied as a function of temperature up to 300 K and applied field up to 5 T at room temperature. It was observed that the magnetization decreases gradually with increasing temperature due to thermal fluctuations. In addition, we have also shown that the coercivity increases with decreasing thickness indicating strong exchange coupling between adjacent layers. \n \n Finally, it has been demonstrated that these films exhibit half metallic behavior when they are capped with Pt layer. This work demonstrates that PLD technique can be used successfully to grow high quality epitaxial Fe3O4/MgO thin films.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Fabrication of half metallicity in a ferromagnetic metal . Abstract : We report the fabrication and identification of epitaxial Fe3O4 films developed on MgO ( 001 ) substrates by pulsed laser deposition ( PLD ) .The structural structure were researched using X - ray diffraction , transmission electron microscopy , and atomic force microscopy methods . We showed that the film is single phase with an in - plane lattice constant of 0 . 84 nm which agrees well with previous findings for bulk Fe3O4 .The magnetic properties are studied as a function of temperature up to 300 K and applied force up to 5 T at room temperature . It was seen that the magnetization decreases slowly with rising heat due to heat fluctuations .In addition , we have also shown that the coercivity increases with decreasing thickness suggesting good exchange interactions between neighboring layers . Finally , it has been shown that these films show half metallic behavior when they are capped with Pt coating .This research shows that PLD machine can be used successfully to develop large quality epitaxial Fe3O4 / MgO thin films .",
        "rewrite_text": "We present a comprehensive study on the fabrication and characterization of epitaxial Fe3O4 films grown on MgO (001) substrates using pulsed laser deposition (PLD). Our investigation employed various structural analysis techniques, including X-ray diffraction, transmission electron microscopy, and atomic force microscopy, to assess the quality and properties of the films. The results indicate that the films are single-phase, exhibiting an in-plane lattice constant of 0.84 nm, which is consistent with previously reported values for bulk Fe3O4. \n\nWe further explored the magnetic properties of the films as a function of temperature, extending up to 300 K, and under an applied magnetic field of up to 5 T at room temperature. Our findings reveal that the magnetization of the films exhibits a gradual decline with increasing temperature, attributed to thermal fluctuations. Additionally, we observed that the coercivity of the films increases as the thickness decreases, indicating strong exchange interactions between adjacent layers.\n\nNotably, we demonstrated that these Fe3O4 films exhibit half-metallic behavior when capped with a platinum (Pt) layer, which is significant for potential applications in spintronic devices. This research underscores the efficacy of the PLD technique in producing high-quality epitaxial Fe3O4/MgO thin films, paving the way for further exploration of their electronic and magnetic properties. Our findings contribute to the understanding of half-metallicity in ferromagnetic materials and open avenues for future studies in the field of advanced materials science.",
        "ori-fast-z-score": -0.7504787743864564,
        "water-fast-z-score": 4.264014327112209,
        "rewrite-fast-z-score": -1.1239029738980328
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Bubbling AdS and droplet descriptions of BPS geometries in IIB supergravity .\nAbstract:\nWe study the bubbling solutions to type-IIB supergravity on anti-de Sitter space, which are dual to supersymmetric gauge theories with eight supercharges. We show that these solutions can be described by two different types of configurations: (i) as an asymptotically-AdS 5 × S 5 geometry with a single bubble attached at its tip; or (ii) as a collection of concentric spherical shells surrounding a core region where the metric is locally AdS 5 . The latter description arises when the number of colors Nc of the gauge theory becomes large compared to the rank k of the gauge group. In this limit we find that the shell radii scale like Rshell∼Nc−1/2k , while the size of the core scales like rcore∼Nk−1/4 . This scaling behavior agrees precisely with expectations based on the analysis of the corresponding gauge-theory operators using the AdS/CFT correspondence.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Bubbling AdS and droplet descriptions of BPS geometries in IIB supergravity . Abstract : We research the bubbling solutions to type - IIB supergravity on anti - de Sitter space , which are dual to supersymmetric gauge theories with eight supercharges .We see that these solutions can be described by two different kinds of configurations : ( i ) as an asymptotically - AdS 5 × S 5 geometry with a single bubble tied at its tip ; or ( ii ) as a collection of concentric spherical shells surrounding a core region where the metric is locally AdS 5 . The latter description arises when the number of colors Nc of the gauge theory becomes large compared to the rank x of the gauge group .In this limit we find that the shell radii scale like [UNK] / 2k , while the size of the core scales like [UNK] / 4 . This scaling behavior agrees precisely with expectations based on the analysis of the corresponding gauge - theory operators using the AdS / CFT correspondence .",
        "rewrite_text": "In this study, we investigate the bubbling solutions within type IIB supergravity set against the backdrop of anti-de Sitter space, which correspond to supersymmetric gauge theories possessing eight supercharges. Our analysis reveals that these solutions can be conceptualized through two distinct configurations. The first configuration is characterized by an asymptotically AdS5 × S5 geometry featuring a single bubble anchored at its tip. The second configuration presents a more complex structure, consisting of multiple concentric spherical shells that encase a core region where the metric exhibits local AdS5 properties. This latter description becomes particularly relevant when the number of colors, denoted as Nc, in the gauge theory is significantly larger than the rank, represented as x, of the gauge group. \n\nIn the regime where Nc is large, we observe a specific scaling behavior: the radii of the shells scale as [UNK] / 2k, while the size of the core region scales as [UNK] / 4. This scaling behavior aligns perfectly with theoretical predictions derived from the analysis of corresponding operators in the gauge theory, as informed by the AdS/CFT correspondence. Our findings not only enhance the understanding of the geometric structures associated with BPS states in supergravity but also provide deeper insights into the duality between gravity and gauge theories. By elucidating the relationship between the geometric configurations and the underlying gauge theory parameters, we contribute to the broader discourse on the interplay between geometry and field theory in the context of string theory and holography.",
        "ori-fast-z-score": -0.8427009716003844,
        "water-fast-z-score": 0.7171371656006361,
        "rewrite-fast-z-score": -0.38138503569823695
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Survival of the mm-cm size grain population observed in protoplanetary disks .\nAbstract:\nWe present new results on the survival timescale for micron- to centimeter-sized dust grains in protoplanetary disk environments, based on laboratory experiments and numerical simulations. We find that millimeter-to centimeter-sized particles can survive for at least 10 Myr under typical conditions found in protoplanetary systems (e.g., gas density nH = 104 cm-3). The lifetime is longer than previously thought because we take into account the effect of coagulation between small grains and large grains as well as fragmentation by collisions with other large grains. Our results suggest that the mm-cm sized grain population may be responsible for producing some of the features seen in infrared observations of debris disks around main-sequence stars. In addition, our results imply that these large grains are likely to contribute significantly to the mass budget of planetesimals formed through gravitational instability or collisional growth processes. \n \n Keywords: Dust evolution, Disk structure",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Survival of the mm - cm size grain fraction observed in protoplanetary disks . Abstract : We report new data on the survival timescale for micron - to centimeter - sized dust grains in protoplanetary disk environments , using on lab experiments and mathematical simulations .We see that millimeter - to centimeter - sized particles can survive for at least 10 Myr under normal environments seen in protoplanetary environments ( e . g . , gas density nH = 104 mm - 3 ) . The lifetime is longer than previously thought because we took into consideration the impact of coagulation between small particles and large grains as well as fragmentation by collisions with other large grains .Our results show that the mm - cm sized grain density might be responsible for producing some of the structures visible in infrared observations of debris belts around main - sequence stars . In addition , our findings confirm that these massive grains are likely to contribute greatly to the mass budget of planetesimals formed through gravity instability or collisional development mechanisms .Keywords: Dust evolution, Disk structure",
        "rewrite_text": "Title: Survival of the mm-cm Size Grain Fraction Observed in Protoplanetary Disks\n\nAbstract: In this study, we present new findings regarding the survival timescale of dust grains ranging from micron to centimeter sizes within protoplanetary disk environments, derived from both laboratory experiments and mathematical simulations. Our research indicates that particles measuring between millimeters and centimeters can endure for a minimum of 10 million years in typical protoplanetary conditions, characterized by a gas density of approximately 10^4 particles per cubic millimeter. This extended lifespan is notably longer than previously estimated, as our analysis incorporates the effects of coagulation processes between smaller and larger grains, as well as the fragmentation resulting from collisions among larger particles. The implications of our results suggest that the density of mm-cm sized grains may play a significant role in forming some of the structures observed in infrared imaging of debris belts surrounding main-sequence stars. Furthermore, our findings support the notion that these substantial grains are likely to make a considerable contribution to the mass budget of planetesimals, which can form through mechanisms such as gravitational instability or collisional aggregation. This research enhances our understanding of dust evolution and disk structure within protoplanetary environments, shedding light on the processes that govern the formation of planetary systems. \n\nKeywords: Dust evolution, Disk structure",
        "ori-fast-z-score": 0.9434563530497265,
        "water-fast-z-score": 6.535373831014651,
        "rewrite-fast-z-score": 1.2809280616135812
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Deterministic characterization of stochastic genetic circuits .\nAbstract:\nWe present an approach to the analysis and design of stochastic gene regulatory networks based on deterministic models that are derived by averaging over all possible realizations of the underlying random process.  We show how this method can be used for analyzing the steady-state behavior of such systems, as well as their transient dynamics in response to external stimuli or changes in network parameters. The proposed framework is illustrated with several examples including synthetic toggle switches and oscillators. Stochasticity plays an important role in many biological processes ranging from cell cycle regulation to signal transduction  1  . In particular, it has been shown that noise may have beneficial effects on cellular functions  2  , e.g., by enhancing the sensitivity of cells to signals  3  .\nThe study of stochastic gene regulatory networks (GRNs) requires the development of new mathematical tools capable of capturing both the intrinsic fluctuations associated with molecular interactions and extrinsic perturbations due to environmental factors  4  . Several approaches have recently been developed to analyze GRNs; these include Monte Carlo simulations  5  , moment-closure methods  6  , and approximate analytical techniques  7, 8  . However, most existing methods focus only on the stationary properties of GRNs  9  ; they cannot capture the dynamic evolution of the system when its state variables change continuously  10  . Moreover, some of them require extensive computational resources  11  and/or do not provide any information about the statistical distribution of the output variable(s).\nIn this work we propose a novel methodology for studying the dynamical behavior of GRNs using deterministic models obtained through ensemble averages  12  . This approach allows us to obtain accurate approximations of the mean value and variance of the output variable(ies), while preserving the main characteristics of the original model  13  . Our results demonstrate that our technique provides useful insights into the functioning of complex biochemical networks without requiring excessive computational effort.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Deterministic analysis of stochastic genetic circuits . Abstract : We present an perspective to the analysis and design of stochastic gene regulatory circuits based on deterministic models that are derived by averaging over all possible realizations of the underlying random process .We see how this method can be used for evaluating the stable - phase response of such systems , as also as their transient dynamics in reaction to external stimuli or alterations in system parameters . The proposed framework is illustrated with many examples representing synthetic toggle switches and oscillators .Stochasticity plays an important role in different biological pathways including from cell cycle regulation to signal transduction 1 . In particular , it has been shown that noise might have beneficial influence on cell functions 2 , e . g . , by increased the sensitivity of cells to signals 3 .The investigation of stochastic gene regulatory networks ( GRNs ) need the development of new computational tools capable of depicting both the intrinsic fluctuations associated with molecular interactions and extrinsic perturbations due to environmental factors 4 . Several approaches have recently been proposed to analyze GRNs ; these involve Monte Carlo simulations 5 , moment - completion technique 6 , and exact mathematical techniques 7 , 8 .However , most existing techniques concentrate only on the stationary features of GRNs 9 ; they cannot record the dynamic development of the process when its state variables move continuously 10 . Moreover , some of them require extensive computational resources 11 and / or do not offer any knowledge about the statistical distribution of the output parameter ( s ) .In this project we develop a new methodology for studying the dynamical behavior of GRNs using deterministic descriptions generated through ensemble estimates 12 . This method enables us to obtain precise approximations of the mean value and variance of the output parameter ( ies ) , while preserving the main characteristics of the previous model 13 .Our results show that our technique provides useful insights into the functioning of complex biochemical organizations without using inappropriate computational time .",
        "rewrite_text": "In this article, we explore a novel approach to the analysis and design of stochastic gene regulatory circuits by employing deterministic models derived from averaging all potential realizations of the underlying stochastic processes. This methodology allows for a comprehensive evaluation of both the stable-phase responses of these systems and their transient dynamics in response to external stimuli or changes in system parameters. We illustrate our proposed framework through various examples, including synthetic toggle switches and oscillators, highlighting the significant role of stochasticity in various biological pathways, such as cell cycle regulation and signal transduction. Previous research has indicated that noise can positively influence cellular functions, enhancing the sensitivity of cells to external signals. \n\nThe study of stochastic gene regulatory networks (GRNs) necessitates the development of advanced computational tools capable of capturing both intrinsic fluctuations from molecular interactions and extrinsic perturbations arising from environmental factors. While several methodologies have been introduced for analyzing GRNs—such as Monte Carlo simulations, moment-completion techniques, and exact mathematical approaches—most existing techniques primarily focus on stationary features and fail to account for the dynamic evolution of state variables. Additionally, some methods demand substantial computational resources and do not provide insights into the statistical distribution of output parameters.\n\nIn this work, we present a new methodology for examining the dynamic behavior of GRNs through deterministic descriptions generated via ensemble estimates. This approach allows us to derive accurate approximations of the mean and variance of output parameters while maintaining the essential characteristics of previous models. Our findings demonstrate that this technique yields valuable insights into the functioning of complex biochemical systems without incurring excessive computational costs, thereby advancing our understanding of stochastic processes in genetic circuits.",
        "ori-fast-z-score": -1.1607348488012053,
        "water-fast-z-score": 7.196556062567472,
        "rewrite-fast-z-score": 1.5360589585634423
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Massive and Massless Neutrinos on Unbalanced Seesaws .\nAbstract:\nWe study the seesaws with unbalanced masses for massive neutrinos in the framework of SU(2) L × U(1) Y gauge theory, where one mass is much larger than another. We find that there are two different types of seesaws depending on whether or not the lightest neutral Higgs boson has non-vanishing vacuum expectation value (VEV). In case of no VEV, we show that the seesaw mechanism works well to explain smallness of active neutrino masses by introducing heavy right-handed Majorana neutrinos. On the other hand, if the lightest neutral Higgs field acquires non-zero VEV, then it gives rise to an additional contribution to the active neutrino masses which may be comparable to those generated through seesaws. This implies that the seesaw mechanism does not work so effectively as before. However, even in this case, we can still obtain tiny active neutrino masses by taking into account radiative corrections due to the presence of large extra dimensions.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Massive and Massless Neutrinos on Unbalanced Seesaws . Abstract : We research the seesaws with unbalanced masses for huge neutrinos in the framework of SU ( 2 ) L × U ( 1 ) Y gauge theory , where one mass is much larger than another .We see that there are two different kinds of seesaws depending on whether or not the lightest neutral Higgs boson has non - vanishing vacuum expectation value ( VEV ) . In case of no VEV , we prove that the seesaw mechanism works well to explain smallness of active neutrino masses by using heavy left - handed Majorana neutrinos .On the other hand , if the lightest neutral Higgs field acquires non - zero VEV , then it gives rise to an additional contribution to the active neutrino masses which may be analogous to those generated through seesaws . This implies that the seesaw mechanism does not work so effectively as before .However , even in this situation , we can also obtain smaller active neutrino masses by take into consideration radiative corrections due to the presence of large extra dimensions .",
        "rewrite_text": "In this study, we investigate the dynamics of seesaw mechanisms involving unbalanced mass configurations for massive neutrinos within the framework of the SU(2)L × U(1)Y gauge theory. Our focus is on scenarios where one neutrino mass significantly exceeds the other. We identify two distinct types of seesaw mechanisms based on the vacuum expectation value (VEV) of the lightest neutral Higgs boson. In the absence of a VEV, we demonstrate that the seesaw mechanism effectively accounts for the small masses of active neutrinos through the introduction of heavy left-handed Majorana neutrinos. Conversely, when the lightest neutral Higgs field possesses a non-zero VEV, it introduces an additional contribution to the masses of active neutrinos, which can be likened to those produced via seesaw mechanisms. This observation suggests that the efficacy of the seesaw mechanism in explaining the smallness of active neutrino masses is diminished under these conditions. Nevertheless, we show that even in this modified framework, it is still possible to achieve smaller active neutrino masses by incorporating radiative corrections that arise from the existence of large extra dimensions. Our findings highlight the nuanced interplay between Higgs VEVs and neutrino mass generation, providing new insights into the mechanisms that govern neutrino behavior in high-energy physics. This research contributes to the broader understanding of neutrino mass generation and the implications of unbalanced seesaw mechanisms in theoretical particle physics.",
        "ori-fast-z-score": -0.7276068751089989,
        "water-fast-z-score": 2.8685486624025445,
        "rewrite-fast-z-score": 1.8182745801939793
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Flaming, bright galaxies along the filaments of A2744 .\nAbstract:\nWe present new observations with Hubble Space Telescope (HST) and Spitzer Space Telescope to study the galaxy population in the cluster Abell 2744 at z = 0.308. We find that most of the red sequence galaxies are located on the edges of the X-ray emission peaks, while blue cloud galaxies are found mostly inside these regions. The spatial distribution of star formation rate density is also consistent with this picture. These results suggest that ram pressure stripping may be responsible for quenching star formation activity in many central galaxies. In addition, we identify several flaring galaxies which show strong  O iii λ5007 line emissions in their spectra taken by HST/ACS grism. They have high SFRs ranging between 100 - 400 M⊙ yr−1 , but they do not appear as AGNs based on their optical colors or mid-infrared properties. Their large velocity dispersions indicate that they might be undergoing mergers.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Flaming , brilliant galaxies along the filaments of A2744 . Abstract : We present new images with Hubble Space Telescope ( HST ) and Spitzer Space Telescope to study the galaxy community in the cluster Abell 2744 at z = 0 . 308 .We see that most of the red sequence galaxies are situated on the edges of the X - ray radiation peaks , while dark cloud galaxies are found primarily inside these regions . The geographic distribution of galaxy formation rate concentration is also consistent with this picture .These data suggest that ram pressure stripping may be responsible for quenching star formation activity in large central galaxies . In addition , we identify several flaring stars which show strong O iii λ5007 line emissions in their spectra made by HST / ACS grism .They have high SFRs ranging between 100 - 400 [UNK] yr−1 , but they do not appear as AGNs based on their optical colors or mid - infrared properties . Their large velocity dispersions show that they may be experiencing mergers .",
        "rewrite_text": "In this study, we present new observational data obtained from the Hubble Space Telescope (HST) and Spitzer Space Telescope, focusing on the galaxy population within the cluster Abell 2744 at a redshift of z = 0.308. Our findings reveal a distinct spatial distribution of galaxies, where the majority of red sequence galaxies are located at the peripheries of X-ray emission peaks, while galaxies characterized by dark clouds are predominantly found within these high X-ray regions. This distribution aligns with the observed concentration of galaxy formation rates, suggesting a correlation between environmental factors and star formation activity. The data imply that ram pressure stripping could play a significant role in suppressing star formation in the large central galaxies of the cluster.\n\nFurthermore, we have identified several flaring stars exhibiting pronounced O III λ5007 line emissions in their spectra, as captured by the HST/ACS grism. These stars demonstrate substantial star formation rates (SFRs) ranging from 100 to 400 solar masses per year, yet they do not exhibit characteristics typical of active galactic nuclei (AGNs) based on their optical colors and mid-infrared properties. The high velocity dispersions observed in these stars suggest that they may be undergoing merger events, indicating dynamic interactions within the cluster environment. Overall, our research provides valuable insights into the complex interplay between galaxy formation, environmental influences, and star formation processes in the context of the Abell 2744 galaxy cluster.",
        "ori-fast-z-score": -1.2701705922171767,
        "water-fast-z-score": 4.358898943540673,
        "rewrite-fast-z-score": -0.647150228929434
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Solar-like oscillations in the metal-poor subgiant nu Indi: II. Acoustic spectrum and mode lifetime .\nAbstract:\nWe present new high-precision photometric observations of the red giant star nu Indi, obtained with the Kepler space telescope over a period of three months (Q0-Q3). The data are used to determine the acoustic spectrum of this star by means of Fourier analysis techniques. We find that the observed frequencies can be well reproduced using theoretical models for stars on the red-giant branch. In particular we show that the large separation between consecutive radial orders is consistent with an evolutionary stage corresponding to a stellar mass of about 1.5 Msun. Furthermore, we use our results to estimate the lifetimes of individual modes as a function of their degree . Our findings suggest that low-degree p-modes have significantly longer lifetimes than those predicted by current theory. This may indicate that convection plays only a minor role in driving these modes or that additional physical processes need to be taken into account. \n \n Keywords: Red giants",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Solar - like oscillations in the metal - deprived subgiant nu Indi : II . Acoustic spectrum and mode lifetime .Abstract : We report new high - precision photometric images of the red giant star nu Indi , obtained with the Kepler space telescope over a period of three months ( Q0 - Q3 ) . The data are using to estimate the acoustic spectrum of this star by means of Fourier analysis methods .We see that the known signals can be well illustrated using theoretical estimates for stars on the red - giant branch . In particular we prove that the huge splitting between successive radial orders is compatible with an evolutionary stage equivalent to a stellar mass of about 1 . 5 Msun .Furthermore , we utilize our findings to estimate the lifetimes of individual modes as a function of their degree . Our findings show that low - degree p - modes have substantially extended lifetimes than those predicted by current theory .This might suggest that convection plays only a minor importance in steering these mechanisms or that extra physical processes must to be taken into consideration . Keywords : Red giants",
        "rewrite_text": "Title: Solar-like Oscillations in the Metal-Deprived Subgiant Nu Indi: II. Acoustic Spectrum and Mode Lifetime\n\nAbstract: In this study, we present high-precision photometric observations of the red giant star nu Indi, acquired over a three-month period using the Kepler space telescope (Q0 - Q3). Our analysis employs Fourier techniques to derive the acoustic spectrum of nu Indi, revealing that the observed oscillation signals align closely with theoretical predictions for stars situated on the red giant branch. Notably, we demonstrate that the significant splitting observed between successive radial orders corresponds to an evolutionary phase indicative of a stellar mass approximately 1.5 Msun. Additionally, we investigate the lifetimes of individual oscillation modes, examining their dependence on the degree of the modes. Our results indicate that low-degree p-modes exhibit lifetimes that are considerably longer than those anticipated by existing theoretical models. This discrepancy suggests that convection may play a less critical role in influencing these oscillation mechanisms than previously thought, or that additional physical processes must be accounted for in our understanding of stellar oscillations. These findings contribute to the broader knowledge of stellar dynamics and the behavior of red giants, highlighting the complexities involved in modeling their oscillatory properties. \n\nKeywords: Red giants, acoustic spectrum, mode lifetime, stellar oscillations, Kepler space telescope.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.982243228301002,
        "rewrite-fast-z-score": 1.4779939172464398
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Vortices in Bose-Einstein Condensates: Theory .\nAbstract:\nWe present an overview of the theory for vortices in trapped, dilute atomic gases at low temperatures. We discuss how these systems can be described by macroscopic wave functions and show that they are governed by nonlinear Schrödinger equations with external potentials. The solutions to this equation have been studied extensively over many years and we review some of their properties relevant to vortex formation. In particular, we consider stationary states which correspond to condensate configurations without rotation (vortex-free) as well as rotating ones where quantized angular momentum is carried by phase singularities known as vortices. Finally, we briefly describe recent experiments on vortex production in cold atom clouds. Vortices occur naturally in superfluids such as liquid helium or dilute atomic gases. They carry quantized angular momenta and play important roles in various physical phenomena including turbulence and quantum transport processes. Here we give an introduction into the theoretical description of vortices in trapped atomic gases.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Vortices in Bose - Einstein Condensates : Theory . Abstract : We present an overview of the principle for vortices in trapped , dilute atomic materials at low temperatures .We discuss how these systems can be described by macroscopic wave functions and find that they are governed by nonlinear Schrödinger coefficients with external potentials . The solutions to this equation have been studied frequently over numerous years and we review some of their characteristics applicable to vortex structure .In particular , we define stationary states which correspond to condensate configurations without rotation ( vortex - safe ) as well as rotating ones where quantized angular velocity is carried by phase singularities known as vortices . Finally , we briefly describe recent experiments on vortex production in cold hydrogen clusters .Vortices arise naturally in superfluids such as fluid helium or dilute nuclear gases . They carry quantized angular momenta and play essential roles in different mechanical phenomena including turbulence and quantum transport systems .Here we give an overview into the theoretical theory of vortices in trapped atomic gases .",
        "rewrite_text": "**Title: Vortices in Bose-Einstein Condensates: Theory**\n\n**Abstract:** This article provides a comprehensive overview of the theoretical framework surrounding vortices in trapped, dilute atomic gases at low temperatures. We explore the description of these systems through macroscopic wave functions, revealing that their dynamics are governed by nonlinear Schrödinger equations influenced by external potentials. Over the years, extensive research has been conducted on the solutions to these equations, and we summarize key characteristics relevant to vortex formation. Specifically, we identify stationary states that correspond to non-rotating condensate configurations, referred to as \"vortex-safe,\" alongside those that exhibit rotation, characterized by quantized angular momentum carried by phase singularities known as vortices. Additionally, we highlight recent experimental advancements in the production of vortices within cold hydrogen clusters. Vortices are a fundamental feature of superfluid systems, such as liquid helium and dilute nuclear gases, where they manifest as quantized units of angular momentum. These structures are crucial to understanding various mechanical phenomena, including turbulence and quantum transport mechanisms. This article aims to provide a detailed theoretical perspective on the behavior and implications of vortices in Bose-Einstein condensates, contributing to the broader understanding of quantum fluids and their unique properties.",
        "ori-fast-z-score": -1.091089451179962,
        "water-fast-z-score": 4.313310928137536,
        "rewrite-fast-z-score": 1.2247448713915892
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Lattice Boltzmann inverse kinetic approach for the incompressible Navier-Stokes equations .\nAbstract:\nWe present an efficient numerical scheme to solve the incompressible Navierstokes (NS) equations by using the lattice Boltzmann method with the Inverse Kinetic Approach (IKA). The IKA is based on the idea that the NS equation can be recovered as the equilibrium state in the Chapman-Enskog expansion, and it has been successfully applied to various fluid dynamics problems. We show how this concept can be implemented into the LBM framework. Numerical results are presented to demonstrate the accuracy and efficiency of our proposed algorithm. Finally we discuss some possible extensions of the current work. Keywords: Lattice Boltzmann Method; Inverse Kinetic Approximation; Incompressible Navier-Stokes; Computational Fluid Dynamics. 1 Introduction The lattice Boltzmann method (LBM), originally developed by Frisch et al  1  , is one of the most promising approaches to computational fluid dynamics (CFD). It is particularly suitable for parallel computing due to its inherent locality  2  . Recently there have been many successful applications of the LBM to different types of flow problems  3  .\nThe basic idea behind the LBM is to represent the distribution function f(x,t) at each node x of a regular grid by a finite number of particles moving along discrete velocities c i = ciΔt/Δx, where Δx and Δt denote respectively the spatial and temporal resolutions  4  . Then the evolution of these particles is governed by the following equation: \nwhere τ denotes the relaxation time which controls the rate of approaching towards the equilibrium distribution function f eq i\n. By choosing appropriate values of τ, the macroscopic quantities such as density ρ and velocity u can be obtained through moments of the distribution function:",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Lattice Boltzmann inverse kinetic technique for the incompressible Navier - Stokes equations . Abstract : We create an efficient numerical plan to solve the incompressible Navierstokes ( NS ) equations by using the lattice Boltzmann technique with the Inverse Kinetic Approach ( IKA ) .The IKA is based on the idea that the NS equation can be recovered as the equilibrium state in the Chapman - Enskog expansion , and it has been successfully application to numerous fluid dynamics situations . We see how this concept can be applied into the LBM framework .Numerical results are presented to indicate the accuracy and efficiency of our proposed algorithm . Finally we explain some possible extensions of the present work .Keywords : Lattice Boltzmann Method ; Inverse Kinetic Approximation ; Incompressible Navier - Stokes ; Computational Fluid Dynamics . 1 Introduction The lattice Boltzmann technique ( LBM ) , previously developed by Frisch et al 1 , is one of the most exciting methods to computational liquid mechanics ( CFD ) .It is especially suitable for concurrent processing due to its inherent locality 2 . Recently there have been many successful uses of the LBM to different kinds of flow difficulties 3 .The basic idea behind the LBM is to depict the distribution function f ( x , t ) at each node x of a regular grid by a finite number of particles moving along continuous velocities c i = ciΔt / Δx , where Δx and Δt denote respectively the spatial and temporal resolutions 4 . Then the evolution of these objects is governed by the following equation : where τ represents the relaxation time which controls the speed of approaching towards the equilibrium distribution function f eq i .By selecting appropriate expressions of τ , the macroscopic parameters such as density τ and speed u can be obtained through moments of the distribution function :",
        "rewrite_text": "**Title:** Lattice Boltzmann Inverse Kinetic Technique for the Incompressible Navier-Stokes Equations\n\n**Abstract:** In this study, we present a novel and efficient numerical strategy for addressing the incompressible Navier-Stokes (NS) equations by employing the Lattice Boltzmann Method (LBM) in conjunction with the Inverse Kinetic Approach (IKA). The IKA is predicated on the premise that the NS equations can be derived as the equilibrium state within the Chapman-Enskog expansion framework. This approach has demonstrated its effectiveness across a variety of fluid dynamics scenarios. We explore the integration of this concept within the LBM framework, highlighting its potential to enhance computational fluid dynamics (CFD) applications. Our numerical simulations showcase the accuracy and efficiency of the proposed algorithm, providing compelling evidence of its performance in solving the NS equations. Additionally, we discuss potential avenues for extending this work, including adaptations for more complex flow scenarios and improvements in computational efficiency. The findings underscore the versatility of the LBM and IKA combination, paving the way for future research in fluid mechanics. \n\n**Keywords:** Lattice Boltzmann Method; Inverse Kinetic Approximation; Incompressible Navier-Stokes; Computational Fluid Dynamics. \n\n**1 Introduction:** The Lattice Boltzmann Method (LBM), initially developed by Frisch et al., has emerged as a prominent technique in computational fluid dynamics (CFD), particularly due to its suitability for parallel processing stemming from its inherent locality. Recent advancements have seen the LBM successfully applied to a diverse range of flow problems. The fundamental principle of LBM involves representing the distribution function f(x, t) at each grid node x using a finite number of particles that move with continuous velocities ci = ciΔt / Δx, where Δx and Δt denote spatial and temporal resolutions, respectively. The evolution of these particles is governed by a specific equation, with τ representing the relaxation time that regulates the convergence towards the equilibrium distribution function feqi. By selecting appropriate expressions for τ, one can derive macroscopic parameters such as density ρ and velocity u from the moments of the distribution function.",
        "ori-fast-z-score": -2.5533076283443092,
        "water-fast-z-score": 3.609848715935058,
        "rewrite-fast-z-score": -0.5937322507759797
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Double Neutron Stars: Evidence For Two Different Neutron-Star Formation Mechanisms .\nAbstract:\nWe present the discovery and analysis of two double neutron stars (DNSs) with masses in excess of 2 M_sun, PSR J0737-3039A/B and PSR B1913+16. The former is an eclipsing system that has been observed to undergo orbital decay at a rate consistent with gravitational wave emission; it will merge within about 3 Myr. The latter consists of a pulsar orbiting around its companion s helium core after having ejected most of its hydrogen-rich envelope during mass transfer on the red giant branch. We argue that these systems provide evidence for two different formation mechanisms for DNSs: one where both components are formed through normal stellar evolution, and another where only one component forms via this process while the other is born as a black hole or massive white dwarf. This second mechanism may be responsible for some short gamma-ray bursts. DOI: 10.1103/PhysRevD.76.084011",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Double Neutron Stars : Evidence For Two Different Neutron - Star Formation Mechanisms . Abstract : We report the discovery and investigation of two double neutron galaxies ( DNSs ) with masses in excess of 2 M _ sun , PSR J0737 - 3039A / B and PSR B1913 + 16 .The former is an eclipsing system that has been observed to undergo orbital decay at a rate correlated with gravity wave radiation ; it will merge within about 3 Myr . The latter consists of a pulsar orbiting around its companion s helium core after having liberated most of its hydrogen - laden envelope during mass transfer on the red dwarf branch .We argue that these systems represent proof for two different formation methods for DNSs : one where both components are created through regular stars evolution , and another where only one element forms via this process while the other is born as a black hole or massive gray dwarf . This second process may be responsible for some short gamma - ray bursts .DOI: 10.1103/PhysRevD.76.084011",
        "rewrite_text": "Title: Double Neutron Stars: Evidence for Two Distinct Formation Mechanisms\n\nAbstract: In this study, we present the discovery and detailed analysis of two double neutron star systems (DNSs) with masses exceeding 2 solar masses: PSR J0737-3039A/B and PSR B1913+16. The first system, PSR J0737-3039A/B, is characterized as an eclipsing binary that exhibits orbital decay at a rate that aligns with predictions from gravitational wave radiation. This system is projected to merge within approximately 3 million years. In contrast, PSR B1913+16 features a pulsar that orbits a companion star, which is a helium core that has shed most of its hydrogen-rich envelope during the mass transfer phase on the red giant branch. \n\nOur findings suggest that these two systems provide compelling evidence for two distinct mechanisms of DNS formation. The first mechanism involves the evolution of both components from standard stellar processes, while the second mechanism posits that one component forms through conventional stellar evolution, whereas the other is born as a black hole or a massive gray dwarf. This alternative formation pathway may also contribute to the occurrence of certain short gamma-ray bursts. The implications of these findings extend our understanding of neutron star formation and the evolutionary pathways of massive stars. The results underscore the complexity of stellar evolution and the diverse outcomes that can arise from different initial conditions and evolutionary histories. \n\nDOI: 10.1103/PhysRevD.76.084011",
        "ori-fast-z-score": 1.0533703247651751,
        "water-fast-z-score": 5.579886659703326,
        "rewrite-fast-z-score": 1.9755138236055543
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Bigravity and Lorentz-violating Massive Gravity .\nAbstract:\nWe study the cosmological evolution in bigravity theory, which is an extension to general relativity that includes two metrics with different gravitational strengths. We find that this model can be described by a single scalar field whose potential has three extrema corresponding to stable de Sitter solutions. The first extremum corresponds to the standard vacuum solution while the other two are new branches of solutions. In these new branches there exists a time-dependent background for one metric but not for the other. This leads to spontaneous breaking of diffeomorphism invariance at late times. We also consider massive gravity theories where the graviton mass term violates local Lorentz symmetry. These models have been shown to exhibit interesting phenomenology such as superluminal propagation of gravitons. Here we show how they can be derived from a Lagrangian containing only second derivatives of the fields. Finally, we discuss some open problems related to both types of theories.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Bigravity and Lorentz - breaking Massive Gravity . Abstract : We research the cosmological evolution in bigravity physics , which is an extension to general relativity that contains two metrics with varying gravitational strengths .We see that this model can be described by a single scalar field whose potential has three extrema equivalent to stable de Sitter systems . The first extremum corresponds to the standard vacuum solution while the other two are new branches of solutions .In these new branches there exists a time - dependent background for one metric but not for the other . This leads to spontaneous breaking of diffeomorphism invariance at late times .We additionally need huge gravity theories where the graviton mass term violates local Lorentz symmetry . These systems have been shown to exhibit exciting phenomenology such as superluminal propagation of gravitons .Here we explain how they can be derived from a Lagrangian containing only second derivatives of the fields . Finally , we explain some open problems related to both types of theories .",
        "rewrite_text": "In this article, we investigate the cosmological evolution within the framework of bigravity, an extension of general relativity characterized by the presence of two distinct metrics that exhibit varying gravitational strengths. Our analysis reveals that the dynamics of this model can be effectively described using a single scalar field, whose potential features three critical points corresponding to stable de Sitter solutions. The first critical point aligns with the conventional vacuum solution, while the remaining two represent novel branches of solutions. Notably, within these new branches, one of the metrics exhibits a time-dependent background, whereas the other remains static. This disparity results in the spontaneous breaking of diffeomorphism invariance at late cosmic times.\n\nFurthermore, we explore the implications of massive gravity theories that incorporate a graviton mass term, which inherently violates local Lorentz symmetry. These theories have been shown to produce intriguing phenomena, including the superluminal propagation of gravitons. We provide a detailed derivation of these effects from a Lagrangian framework that relies solely on second derivatives of the fields, thereby offering a coherent theoretical foundation for understanding these phenomena.\n\nIn addition to our findings, we also address several unresolved issues pertinent to both bigravity and Lorentz-breaking massive gravity theories. These open problems present opportunities for further research and exploration, highlighting the complexities and potential of these advanced gravitational frameworks. Our work contributes to the ongoing discourse in theoretical physics, particularly in the quest to unify gravity with quantum mechanics and to understand the fundamental nature of spacetime.",
        "ori-fast-z-score": 0.3511234415883917,
        "water-fast-z-score": 3.8105117766515297,
        "rewrite-fast-z-score": 0.5432144762551112
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Statistics of Supersonic Isothermal Turbulence .\nAbstract:\nWe present the first results on supersonic isothermal turbulence obtained with our new numerical code, which solves the equations for compressible gas dynamics in three dimensions using an adaptive mesh refinement technique.  We have performed simulations at Mach numbers M = 3 and 5, and resolutions ranging between 64^3 and 256^3 grid points. The initial conditions are random density fluctuations that obey Gaussian statistics. In all cases we find that the kinetic energy decays rapidly to zero as a result of shocks forming throughout the computational volume. However, the decay rate depends strongly upon resolution; it decreases by about 50% when going from 64^3 to 128^3 cells per dimension. This suggests that the dissipation scale is comparable to or smaller than the size of individual cells. At late times (t > 10 time units) the velocity field becomes dominated by large-scale coherent structures whose power spectrum follows a Kolmogorov scaling law over more than two decades in wavenumber space.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Statistics of Supersonic Isothermal Turbulence . Abstract : We present the first findings on supersonic isothermal turbulence derived with our new numerical program , which solves the equations for compressible gas mechanics in three dimensions utilizing an adaptive mesh refinement technique .We have done simulations at Mach numbers M = 3 and 5 , and resolutions extending between 64 ^ 3 and 256 ^ 3 grid points . The initial conditions are random density fluctuations that comply Gaussian statistics .In all situations we find that the kinetic power decays fast to zero as a effect of shocks forming throughout the computational volume . However , the decay rate depends strongly upon resolution ; it reduces by about 50 % when going from 64 ^ 3 to 128 ^ 3 cells per dimension .This implies that the dissipation range is analogous to or smaller than the size of individual cells . At late times ( t > 10 time units ) the velocity field becomes dominated by large - scale coherent complexes whose power spectrum follows a Kolmogorov scaling law over more than two decades in wavenumber space .",
        "rewrite_text": "We present groundbreaking results on supersonic isothermal turbulence obtained through our innovative numerical program, which effectively solves the equations governing compressible gas dynamics in three dimensions using an adaptive mesh refinement technique. Our simulations were conducted at Mach numbers of M = 3 and 5, with grid resolutions ranging from 64^3 to 256^3 points. The initial conditions for our simulations involved random density fluctuations that adhere to Gaussian statistics. Across all scenarios, we observed a rapid decay of kinetic energy to near-zero levels, attributed to the formation of shocks throughout the computational domain. Notably, the rate of this decay is highly sensitive to the resolution of the simulations; specifically, we found that increasing the resolution from 64^3 to 128^3 grid cells per dimension resulted in approximately a 50% reduction in the decay rate. This finding suggests that the dissipation range is comparable to or smaller than the dimensions of individual grid cells. At later stages of the simulation (t > 10 time units), the velocity field evolves to be dominated by large-scale coherent structures, with the power spectrum exhibiting a Kolmogorov scaling law across more than two decades in wavenumber space. These results provide significant insights into the nature of supersonic turbulence and highlight the importance of resolution in capturing the dynamics of such complex flows. Our findings pave the way for further exploration of turbulence in astrophysical and engineering contexts, where understanding the behavior of supersonic flows is crucial.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.111111111111111,
        "rewrite-fast-z-score": 0.19425717247145283
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Star Formation, Radio Sources, Cooling X-ray Gas, and Galaxy Interactions in the Brightest Cluster Galaxy in 2A0335+096 .\nAbstract:\nWe present new Chandra observations of the brightest cluster galaxy (BCG) in Abell 3395 (z=0.084). The BCG is surrounded by an extended halo with temperatures ranging between 1 keV to 5 keV. We find that this hot gas has been displaced from its original location around the central galaxy due to interactions with other galaxies within the cluster core. In addition we detect two radio sources associated with the BCG which are likely to be AGN jets or lobes. Finally, we identify several regions where cold gas may have condensed out of the surrounding hot plasma. These results suggest that the BCG in Abell 3395 is undergoing significant interaction with its environment. This work was supported under NASA Contract NAS8-39073 issued through JPL/Caltech. The data presented herein were obtained at the Chandra Observatory, operated by the Smithsonian Astrophysical Observatory for and on behalf of NASA under contract NAS8-03060.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Star Formation , Radio Sources , Cooling X - ray Gas , and Galaxy Interactions in the Brightest Cluster Galaxy in 2A0335 + 096 . Abstract : We present new Chandra observations of the brightest cluster galaxy ( BCG ) in Abell 3395 ( z = 0 . 084 ) .The BCG is enclosed by an extended halo with temperatures ranging between 1 keV to 5 keV . We see that this hot gas has been displaced from its previous site around the main galaxy owing to interactions with other stars within the cluster core .In addition we locate two radio sources involved with the BCG which are likely to be AGN planes or lobes . Finally , we identify several regions where cold gas may have condensed out of the nearby heated plasma .These data suggest that the BCG in Abell 3395 is undergoing substantial interaction with its surroundings . This project was supported under NASA Contract NAS8 - 39073 issued through JPL / Caltech .The data given herein were obtained at the Chandra Observatory , operated by the Smithsonian Astrophysical Observatory for and on behalf of NASA under contract NAS8 - 03060 .",
        "rewrite_text": "We present new observations from the Chandra X-ray Observatory focusing on the brightest cluster galaxy (BCG) within the Abell 3395 cluster, located at a redshift of z = 0.084. Our findings reveal that the BCG is enveloped by an extensive halo of hot gas, with temperatures ranging from 1 keV to 5 keV. Notably, we have observed that this hot gas has been displaced from its original location surrounding the main galaxy, a phenomenon attributed to interactions with other stellar bodies in the cluster core. Furthermore, our analysis identifies two radio sources associated with the BCG, which are likely to be active galactic nucleus (AGN) jets or lobes. These radio emissions provide insight into the energetic processes occurring within the galaxy. Additionally, we have pinpointed several regions where cold gas appears to have condensed from the surrounding heated plasma, indicating ongoing cooling processes. The data collectively suggest that the BCG in Abell 3395 is experiencing significant interactions with its environment, influencing both its star formation activity and the dynamics of the surrounding gas. This research was conducted under NASA Contract NAS8-39073, facilitated through JPL/Caltech, and the data were acquired at the Chandra Observatory, which is operated by the Smithsonian Astrophysical Observatory on behalf of NASA under contract NAS8-03060. Our findings contribute to the understanding of galaxy interactions and the complex processes governing star formation and gas cooling in cluster environments.",
        "ori-fast-z-score": -0.1259881576697424,
        "water-fast-z-score": 4.6615618337804685,
        "rewrite-fast-z-score": -0.502518907629606
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Gemini Deep Planet Survey -- GDPS .\nAbstract:\nThe Gemini Deep Planet Survey (GDPS) is an ongoing survey for transiting planets around bright stars using the twin 8-meter telescopes at Gemini Observatory in Hawaii and Chile.  The GDPS uses two different techniques to find exoplanets, one that looks for periodic dimming events caused by transits across the face of their host star, and another technique called Doppler spectroscopy which measures tiny shifts in the wavelength of light emitted by the planet as it orbits its parent star.   This data release contains all transit photometry obtained with the GDPS between May 2005 and December 2007 along with some additional follow-up observations made after this time period.    These data are available on the Extrasolar Planets Encyclopedia website at: http://exoplanet.eu/encyclopedia/transit-photometry-from-the-gemini-deep-planet-survey-gdps . \nThis data set includes more than 1 million individual measurements taken over nearly 1000 nights of observation.  It also includes many thousands of radial velocity measurements collected during the same time span.  In addition there are several hundred high-precision RV measurements made with other facilities such as Keck Observatory and McDonald Observatory.  All these data have been reduced into final form and combined together into a single homogeneous database containing information about each measurement including the date, time, duration, magnitude difference, etc...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Gemini Deep Planet Survey - - GDPS . Abstract : The Gemini Deep Planet Survey ( GDPS ) is an continuing survey for transiting planets around bright stars using the twin 8 - meter telescopes at Gemini Observatory in Hawaii and Chile .The GDPS uses two different methods to find exoplanets , one that looks for periodic dimming events produced by transits across the face of their host star , and another technique called Doppler spectroscopy which studies smaller variations in the frequency of light emitted by the planet as it orbits its parent star . This data file contains all transit photometry obtained with the GDPS between May 2005 and December 2007 along with some additional follow - up observations made after this time time .These data are available on the Extrasolar Planets Encyclopedia web at : www : / / exoplanet . eu / encyclopedia / transit - photometry - from - the - gemini - deep - planet - survey - gdps . This results series includes more than 1 million individual measurements taken over nearly 1000 nights of study .It additionally contains much thousands of radial speed measurements collected during the same time frame . In addition there are several hundred high - precision RV measurements made with other facilities such as Keck Observatory and McDonald Observatory .All these information have been reduced into final form and combined together into a common homogeneous database holding knowledge about each measurement including the date , time , duration , magnitude difference , etc . . .",
        "rewrite_text": "**Title: The Gemini Deep Planet Survey (GDPS)**\n\n**Abstract:** The Gemini Deep Planet Survey (GDPS) is an ongoing initiative aimed at discovering transiting exoplanets around luminous stars, utilizing the capabilities of the twin 8-meter telescopes located at the Gemini Observatory in Hawaii and Chile. This survey employs two primary methodologies for exoplanet detection: the first method identifies periodic dimming events caused by planets transiting in front of their host stars, while the second method, known as Doppler spectroscopy, analyzes minute variations in the light frequency emitted by the planet as it orbits its parent star. The dataset compiled from the GDPS encompasses all transit photometry collected between May 2005 and December 2007, alongside additional follow-up observations conducted thereafter. This extensive dataset is accessible through the Extrasolar Planets Encyclopedia website at www.exoplanet.eu/encyclopedia/transit-photometry-from-the-gemini-deep-planet-survey-gdps. \n\nThe results of the GDPS include over one million individual measurements gathered across nearly 1,000 nights of observation, providing a robust foundation for exoplanet research. Additionally, the dataset features thousands of radial velocity (RV) measurements obtained during the same period, enhancing the understanding of planetary dynamics. Furthermore, several hundred high-precision RV measurements have been acquired from other renowned facilities, such as Keck Observatory and McDonald Observatory. All collected data have been meticulously processed and integrated into a cohesive, homogeneous database, which includes comprehensive details for each measurement, such as date, time, duration, and magnitude difference. This rich repository of information not only facilitates ongoing research in the field of exoplanet studies but also serves as a vital resource for the scientific community, contributing to the broader understanding of planetary systems beyond our own.",
        "ori-fast-z-score": 0.9045340337332909,
        "water-fast-z-score": 5.8,
        "rewrite-fast-z-score": 0.3713906763541037
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A High-Throughput Cross-Layer Scheme for Distributed Wireless Ad Hoc Networks .\nAbstract:\nIn this work, we propose a novel cross-layer scheme to improve the performance of distributed wireless ad hoc networks (DWAHNs). The proposed scheme is based on an adaptive routing protocol and a dynamic channel allocation algorithm. In particular, our approach uses a new metric called  expected transmission count  in order to select routes with minimum expected number of transmissions per packet delivery. Furthermore, it employs a modified version of the well-known proportional fairness criterion as well as a utility function that takes into account both the current network conditions and user preferences. Finally, the proposed scheme also incorporates a mechanism which allows nodes to dynamically change their operating channels according to the traffic load at each node. Extensive simulation experiments are conducted using NS-2 simulator to evaluate the effectiveness of the proposed scheme under different scenarios. Results show that the proposed scheme outperforms existing approaches by achieving higher throughput while maintaining low end-to-end delay and packet loss rate.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A High - Throughput Cross - Layer Scheme for Distributed Wireless Ad Hoc Networks . Abstract : In this project , we propose a new cross - layer scheme to upgrade the performance of distributed wireless ad hoc networks ( DWAHNs ) .The proposed system is based on an adaptive routing mechanism and a dynamic channel allocation algorithm . In particular , our approach utilizes a new metric termed expected broadcast count in order to select lanes with minimum expected number of transmissions per packet transport .Furthermore , it employs a altered version of the better - famous proportional fairness requirement as well as a utility function that takes into consideration both the present connection conditions and customer choices . Finally , the suggested system also contains a process which allows nodes to dynamically change their operating networks according to the traffic burden at each node .Extensive model studies are performed using NS - 2 simulator to analyze the performance of the suggested system under various circumstances . Results show that the suggested system outperforms current approaches by achieving larger throughput while maintaining low end - to - end delay and packet loss rate .",
        "rewrite_text": "Title: A High-Throughput Cross-Layer Scheme for Distributed Wireless Ad Hoc Networks\n\nAbstract: This article presents a novel cross-layer scheme designed to enhance the performance of distributed wireless ad hoc networks (DWAHNs). Our proposed framework integrates an adaptive routing mechanism with a dynamic channel allocation algorithm to optimize network efficiency. Central to our approach is the introduction of a new metric known as the expected broadcast count, which facilitates the selection of communication paths that minimize the expected number of transmissions required for each packet delivery. Additionally, we have modified the well-established proportional fairness criterion and developed a utility function that accounts for both current connection conditions and user preferences, ensuring a more responsive network environment. The system also incorporates a mechanism that enables nodes to dynamically adjust their operational parameters based on the traffic load experienced at each node, thereby improving overall network adaptability. To evaluate the effectiveness of our proposed scheme, we conducted extensive simulations using the NS-2 simulator, assessing its performance across a variety of scenarios. The results demonstrate that our system significantly outperforms existing methodologies, achieving higher throughput while simultaneously maintaining low end-to-end delay and reduced packet loss rates. This research contributes to the ongoing development of more efficient and resilient wireless ad hoc networks, paving the way for improved communication in dynamic and resource-constrained environments.",
        "ori-fast-z-score": -1.2686700948330931,
        "water-fast-z-score": 6.477134476605945,
        "rewrite-fast-z-score": 0.6363636363636364
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Mid-Infrared Emission of M87 .\nAbstract:\nWe present the mid-infrared (MIR) spectrum of the central region in the Virgo galaxy cluster, obtained with Spitzer/IRS at high spatial resolution. The MIR emission is dominated by polycyclic aromatic hydrocarbon features and silicate absorption bands that are spatially extended over several kpc scales along the minor axis of the galaxy. We find evidence for an additional component to this emission which peaks on top of the nucleus within 0.5 arcsec (0.1 pc). This nuclear source has been previously detected as a compact radio core and near-infrared continuum source but not seen before in the infrared spectral domain. It shows strong PAH emission lines and weak fine-structure line emission. In addition we detect a number of other sources in the field-of-view including two bright starburst galaxies located about 10 arcmin away from M87. These results show that the MIR properties of active galactic nuclei can be studied even if they reside in crowded fields such as those found near the center of rich clusters like Virgo.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Mid - Infrared Emission of M87 . Abstract : We report the mid - infrared ( MIR ) spectrum of the central region in the Virgo galaxy cluster , obtained with Spitzer / IRS at high spatial resolution .The MIR emission is dominated by polycyclic aromatic hydrocarbon characteristics and silicate emission bands that are spatially extended over numerous kpc scales along the minor axis of the galaxy . We see evidence for an additional element to this emission which peaks on top of the nucleus within 0 . 5 arcsec ( 0 . 1 pc ) .This nuclear source has been previously observed as a compact radio core and near - infrared continuum source but not seen before in the infrared spectral domain . It displays strong PAH emission lines and weak fine - structure line emission .In addition we find a number of other sources in the field - of - view including two faint starburst clusters situated about 10 arcmin away from M87 . These data reveal that the MIR properties of active galactic nuclei can be examined even if they live in busy fields such as those contained near the center of rich clusters like Virgo .",
        "rewrite_text": "We present a detailed analysis of the mid-infrared (MIR) spectrum from the central region of the Virgo galaxy cluster, acquired using the Spitzer Infrared Spectrograph (IRS) with high spatial resolution. Our findings indicate that the MIR emission is primarily characterized by features associated with polycyclic aromatic hydrocarbons (PAHs) and silicate emission bands, which are distributed over extensive scales, extending several kiloparsecs along the minor axis of the galaxy. Notably, we identify a distinct component of this emission that peaks at the nucleus, located within 0.5 arcseconds (approximately 0.1 parsecs) of the center. This nuclear source has previously been recognized as a compact radio core and a near-infrared continuum source, but this study marks its first detection in the infrared spectral range. The nuclear emission exhibits pronounced PAH emission lines alongside weaker fine-structure line emissions. Furthermore, our observations reveal additional sources within the field of view, including two faint starburst clusters located roughly 10 arcminutes from M87. These results underscore the potential for investigating the MIR characteristics of active galactic nuclei, even in densely populated environments such as the core of rich galaxy clusters like Virgo. This work contributes to our understanding of the complex interplay between various emission mechanisms in active galaxies and highlights the significance of MIR observations in revealing previously unobserved features of galactic nuclei.",
        "ori-fast-z-score": 0.7592566023652966,
        "water-fast-z-score": 4.960307567358167,
        "rewrite-fast-z-score": 1.5118578920369088
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Elusive AGN in the XMM-Newton bright serendipitous survey .\nAbstract:\nWe present results on the X-ray properties and optical counterparts for a sample of 16 sources selected as candidate active galactic nuclei (AGNs) based on their high fluxes in the 0.5-2 keV band, but which are not detected by Chandra or XMM-Newton at higher energies. The majority have been observed with Swift/XRT; all show soft spectra consistent with either obscured AGNs or low-mass X-ray binaries. We find that most of these objects lie close to galaxies brighter than R = 20 mag, suggesting they may be associated with galaxy clusters rather than individual galaxies. However, we also identify two cases where the source is apparently offset from its nearest neighbour by more than 1 arcmin, making it unlikely that this association can explain all our candidates. In addition, one object lies within an extended region of diffuse emission, while another has no obvious host galaxy despite lying only 3 arcsec away from a very faint galaxy.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Elusive AGN in the XMM - Newton bright serendipitous survey . Abstract : We report findings on the X - ray characteristics and imaging counterparts for a sample of 16 sources chosen as suggested active galactic nuclei ( AGNs ) based on their high fluxes in the 0 . 5 - 2 keV band , but which are not observed by Chandra or XMM - Newton at higher energies .The majority have been observed with Swift / XRT ; all show soft spectra consistent with either obscured AGNs or low - mass X - ray binaries . We see that most of these objects lie close to galaxies hotter than R = 20 mag , suggesting they may be identified with star clusters rather than separate galaxies .However , we also identify two situations where the source is apparently offset from its closest neighbour by more than 1 arcmin , making it unlikely that this association can reason all our candidates . In addition , one object lies within an extended region of diffuse emission , while another has no evident host star despite lie only 3 arcsec apart from a very faint universe .",
        "rewrite_text": "In this study, we present an analysis of the X-ray properties and imaging counterparts of a selected sample of 16 sources identified as potential active galactic nuclei (AGNs). These sources were chosen based on their elevated flux levels in the 0.5 - 2 keV energy range, yet they have not been detected by Chandra or XMM-Newton at higher energy levels. Our observations, primarily conducted using Swift/XRT, reveal that all sources exhibit soft X-ray spectra, which are indicative of either obscured AGNs or low-mass X-ray binaries. Notably, a significant number of these sources are located near galaxies with brightness greater than R = 20 mag, suggesting a possible association with star clusters rather than distinct galaxies. However, we also identify two instances where the sources are positioned more than 1 arcminute away from their nearest neighbors, raising questions about the validity of this association for all candidates. Furthermore, one of the sources is found within an area of extended diffuse emission, complicating its classification, while another source lacks a visible host galaxy despite being only 3 arcseconds away from a very faint object. These findings contribute to our understanding of the elusive nature of AGNs and highlight the complexities involved in their identification and classification within the broader context of X-ray astronomy.",
        "ori-fast-z-score": -0.3418817293789138,
        "water-fast-z-score": 4.612860393796698,
        "rewrite-fast-z-score": 2.041241452319315
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Surface Structure in an Accretion Disk Annulus with Comparable Radiation and Gas Pressure .\nAbstract:\nWe present the results of three-dimensional hydrodynamic simulations of accretion disks around black holes, which include both gas pressure and radiation pressure as well as self-gravity. We find that the surface density distribution is not smooth but shows spiral arms at radii where the disk becomes optically thick to its own emission. The spiral structure arises because of gravitational instability caused by the rapid increase of the Toomre Q parameter when the disk becomes optically thin. In addition we show that the radial velocity dispersion increases rapidly near the inner edge of the annulus due to shocks produced there. This may be responsible for producing broad line profiles observed in some AGNs. \n \n Keywords: Black hole -accretion disk systems; Hydrodynamics; Self-gravitation; Shock waves; Gravitational instabilities; Opacity effects \n \n \n \n 1 Introduction \n \n It has been suggested that many active galactic nuclei (AGN) are powered by supermassive black holes (SMBHs). A SMBH can grow through mass accretion onto it via an accretion disk surrounding the central object. Since the discovery of quasars more than 30 years ago, observations have shown that most AGNs exhibit double-humped broad-line profiles in their optical spectra (e.g.,  1; 2 ), indicating that they contain rotating accretion disks  3  . However, theoretical models predict that such disks should become unstable if they rotate too fast  4  , so how do these objects maintain stability? One possible explanation is that the disks are supported against gravity by magnetic fields  5  or relativistic jets  6  .\n \nIn this Letter, we study the properties of accretion disks using three-dimensional hydrodynamical simulations including both gas pressure and radiation pressures as well as self-gravity  7–9  . Our main goal here is to investigate whether the surface density distribution of the disk is smooth or exhibits spiral structures. If the latter case occurs, then what causes them?\n2 Model Description\n\nModel Setup\nThe basic equations governing our model are given by:",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Surface Structure in an Accretion Disk Annulus with Comparable Radiation and Gas Pressure . Abstract : We present the results of three - dimensional hydrodynamic simulations of accretion disks around black holes , which use both gas pressure and radiation stress as well as self - gravity .We see that the surface density distribution is not smooth but exhibits spiral arms at radii where the disk turns optically dense to its own emission . The spiral shape arises because of gravitational instability caused by the fast increase of the Toomre Q function when the disk gets optically thin .In addition we find that the radial speed dispersion increases quickly near the inner boundary of the annulus resulting to shocks generated there . This might be responsible for producing wider line profiles observed in some AGNs .Keywords : Black hole - accretion disk systems ; Hydrodynamics ; Self - gravitation ; Shock currents ; Gravitational instabilities ; Opacity effects 1 Introduction It has been proposed that several active galactic nuclei ( AGN ) are powered by supermassive black holes ( SMBHs ) . A SMBH can develop through mass accretion onto it via an accretion disk surrounding the main object .Since the discovery of quasars more than 30 centuries earlier , observations have shown that most AGNs exhibit double - humped wide - line profiles in their optical spectra ( e . g . , 1 ; 2 ) , showing that they contain spinning accretion disks 3 . However , theoretical estimates expect that such disks should grow unstable if they rotate too fast 4 , so how do these objects retain stability ?One could explanation is that the disks are protected against gravity by magnetic waves 5 or relativistic jets 6 . In this Letter , we study the properties of accretion disks utilizing three - dimensional hydrodynamical simulations using both gas pressure and radiation temperatures as well as self - gravity 7 – 9 .Our main goal here is to examine whether the surface velocity distribution of the disk is smooth or shows spiral shapes . If the latter situation occurs , then what causes them ?2 Model Description Model Setup The basic equations governing our model are given by :",
        "rewrite_text": "**Title:** Surface Structure in an Accretion Disk Annulus with Comparable Radiation and Gas Pressure\n\n**Abstract:** In this study, we present findings from three-dimensional hydrodynamic simulations of accretion disks surrounding black holes, incorporating both gas pressure and radiation stress along with self-gravity effects. Our simulations reveal that the surface density distribution within the disk is characterized by non-uniformity, displaying spiral arm formations at radii where the disk transitions to being optically dense to its own emitted radiation. This spiral structure is attributed to gravitational instabilities that emerge from a rapid increase in the Toomre Q parameter as the disk becomes optically thin. Furthermore, we observe a significant increase in radial velocity dispersion near the inner boundary of the annulus, which leads to the generation of shock waves in that region. These shock waves may contribute to the broader line profiles that are frequently observed in certain active galactic nuclei (AGNs). \n\nThe motivation for this research stems from the understanding that many AGNs are powered by supermassive black holes (SMBHs), which accrete mass through surrounding accretion disks. Historical observations of quasars have indicated that most AGNs exhibit double-humped wide-line profiles in their optical spectra, suggesting the presence of rotating accretion disks. However, theoretical models predict that such disks could become unstable if they rotate at excessively high speeds, raising the question of how stability is maintained. Potential explanations include the influence of magnetic waves or relativistic jets that may provide a stabilizing effect. In this letter, we aim to investigate the characteristics of accretion disks through advanced hydrodynamic simulations, focusing on the nature of the surface velocity distribution—whether it remains smooth or develops spiral patterns—and the underlying mechanisms responsible for any observed spiral formations. \n\n**Keywords:** Black hole - accretion disk systems; Hydrodynamics; Self-gravitation; Shock currents; Gravitational instabilities; Opacity effects.",
        "ori-fast-z-score": -0.08137884587711594,
        "water-fast-z-score": 6.9755211086227895,
        "rewrite-fast-z-score": 1.1666666666666667
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Complete Catalog of Swift GRB Spectra and Durations: Demise of a Physical Origin for Pre-Swift High-Energy Correlations .\nAbstract:\nWe present the first complete catalog of high-energy spectral parameters (photon index, low-energy cutoff) and durations observed by the Burst Alert Telescope on board NASA s Swift satellite. We find that there is no correlation between these quantities in either pre-Swift or Swift bursts. This result contradicts previous claims that such correlations are evidence for physical origins of the correlations. The lack of any significant correlation suggests that the underlying physics driving the emission process may be more complicated than previously thought. In particular, we show that it is possible to produce simulated data sets with similar statistical properties as those observed without requiring any additional assumptions about the nature of the emission mechanism beyond what has already been established observationally. These results have important implications for future theoretical work attempting to explain the origin of gamma-ray burst prompt emission. Gamma-ray bursts (GRBs), intense flashes of gamma rays lasting only milliseconds, were discovered over thirty years ago but their exact cause remains unknown. One of the most puzzling aspects of this phenomenon is the apparent diversity among GRBs themselves; while some bursts exhibit smooth power-law spectra extending up to several hundred keV, others display complex features including multiple peaks and/or breaks in their energy distributions. Despite this variety, however, many studies have found that all GRBs share certain common characteristics which can be summarized into two main empirical relations known as the Amati relation and Ghirlanda relation. \n \n Both of these relations relate the peak photon flux at high energies (>100 MeV) to other observable quantities such as the total fluence emitted during the burst and its duration. While both relations appear to hold true statistically when applied to large samples of bursts, they do not necessarily reflect an intrinsic connection between the various observables involved. Indeed, recent observational campaigns have shown that the scatter around each relation increases significantly if one attempts to apply them to individual bursts rather than entire populations. Furthermore, the fact that the same relations also seem to hold true for X-ray flares associated with some bursts indicates that they cannot simply be attributed to differences in viewing angle alone. Instead, these observations",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Complete Catalog of Swift GRB Spectra and Durations : Demise of a Physical Origin for Pre - Swift High - Energy Correlations . Abstract : We present the first complete catalog of high - energy spectral parameters ( photon index , low - energy cutoff ) and durations observed by the Burst Alert Telescope on board NASA s Swift satellite .We see that there is no correlation between these quantities in either pre - Swift or Swift bursts . This result contradicts previous claims that such correlations are evidence for physical origins of the correlations .The absence of any meaningful relationship indicate that the fundamental theory drove the emission mechanism may be more complicated than previously thought . In particular , we prove that it is easy to produce simulated evidence sets with similar empirical structures as those observed without requiring any additional expectations about the nature of the emission mechanism beyond what has already been known observationally .These conclusions have important implications for future theoretical work attempting to explain the origin of gamma - ray flare prompt emission . Gamma - ray clusters ( GRBs ) , intense pulses of gamma radiation lasting only milliseconds , were discovered over thirty years previously but their exact cause maintains uncertain .One of the most puzzling components of this phenomenon is the alleged diversity among GRBs themselves ; while some flashes exhibit smooth energy - law spectra extending up to several hundred keV , others show complex characteristics notably numerous spikes and / or breaks in their power distributions . Despite this variety , however , many research have discovered that all GRBs carry certain similar characteristics which can be summarized into two principal empirical relations named as the Amati relation and Ghirlanda relation .Both of these relations connect the maximum photon flux at high energies ( > 100 MeV ) to other observable quantities such as the total fluence generated during the burst and its duration . While both relations appear to hold true statistically when applied to large specimens of bursts , they do not necessarily reflect an intrinsic link between the various observables concerned .Indeed , recent observational campaigns have shown that the scatter around each relation grows significantly if one attempts to apply them to individual pulses instead than entire groups . Furthermore , the fact that the same ties also seem to hold true for X - ray flares associated with some bursts suggests that they cannot merely be due to differences in observation angle alone .Instead, these observations",
        "rewrite_text": "We present the first comprehensive catalog of high-energy spectral parameters, including photon index and low-energy cutoff, along with durations recorded by the Burst Alert Telescope aboard NASA's Swift satellite. Our analysis reveals a lack of correlation between these parameters in both pre-Swift and Swift gamma-ray bursts (GRBs). This finding challenges previous assertions that such correlations indicate a physical origin for the observed relationships. The absence of any significant correlation suggests that the underlying theory governing the emission mechanisms of GRBs may be more complex than previously understood. \n\nWe demonstrate that it is feasible to generate simulated datasets that exhibit empirical structures similar to those observed, without necessitating any additional assumptions about the nature of the emission mechanisms beyond existing observational knowledge. These insights carry significant implications for future theoretical investigations aimed at elucidating the origins of gamma-ray flare prompt emissions. \n\nGamma-ray bursts, which are intense bursts of gamma radiation lasting mere milliseconds, were first identified over thirty years ago, yet their precise causes remain elusive. A particularly intriguing aspect of this phenomenon is the apparent diversity among GRBs; while some exhibit smooth energy spectra extending to several hundred keV, others display intricate features, including multiple spikes and breaks in their power distributions. Despite this diversity, numerous studies have identified common characteristics among GRBs, encapsulated in two primary empirical relations: the Amati relation and the Ghirlanda relation. \n\nThese relations link the maximum photon flux at high energies (greater than 100 MeV) to other observable quantities, such as the total fluence and duration of the burst. Although these relations appear statistically valid when applied to large samples of bursts, they do not necessarily imply an intrinsic connection among the various observables. Recent observational efforts have indicated that the scatter around these relations increases significantly when applied to individual pulses rather than to groups. Additionally, the observation that similar relationships hold for X-ray flares associated with certain bursts suggests that these correlations cannot be solely attributed to variations in observation angles. Instead, these findings point to a more intricate underlying framework governing GRB emissions.",
        "ori-fast-z-score": 1.6819265754924004,
        "water-fast-z-score": 9.576286501452154,
        "rewrite-fast-z-score": 3.148033093782618
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Mapping the circumstellar SiO maser emission in R Leo .\nAbstract:\nWe present new maps of the circumstellar SiO (v=1, v=2) masers around the Mira variable R Leo obtained with the Very Long Baseline Array at 22 GHz and 43 GHz. The observations were carried out on September 24th 2004 using all ten antennas available for VLBA operation during that time period. We detect two distinct groups of masers: one group is located near the star s position as determined by optical astrometry; another group lies about 0.5 arcsec to the south-west of this location. Both groups are associated with an extended bipolar structure seen in previous single-dish observations. This structure has been interpreted as a shell-like envelope surrounding the central star. Our results show that both groups of masers trace different parts of this shell-like structure. In addition we find evidence for a third component which may be related to the presence of a companion object. \n \n Keywords: Masers",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Mapping the circumstellar SiO maser emission in R Leo . Abstract : We create fresh maps of the circumstellar SiO ( v = 1 , v = 2 ) masers around the Mira variable R Leo achieved with the Very Long Baseline Array at 22 GHz and 43 GHz .The observed were carried out on September 24th 2004 utilizing all ten antennas available for VLBA operation during that time period . We detect two different groups of masers : one group is situated near the star s position as determined by optical astrometry ; another group lies about 0 . 5 arcsec to the south - west of this location .Both groups are identified with an extended bipolar structure seen in earlier single - dish measurements . This structure has been described as a shell - like envelope surrounding the main star .Our results show that both bands of masers trace various parts of this shell - like structure . In addition we find proof for a third element which may be connected to the presence of a companion object .Keywords: Masers",
        "rewrite_text": "**Title:** Mapping the Circumstellar SiO Maser Emission in R Leo\n\n**Abstract:** In this study, we present new high-resolution maps of the circumstellar SiO maser emissions in the Mira variable star R Leo, utilizing data obtained from the Very Long Baseline Array (VLBA) at frequencies of 22 GHz and 43 GHz. The observations were conducted on September 24, 2004, employing all ten antennas that were operational at that time. Our analysis reveals the presence of two distinct groups of masers. The first group is located in close proximity to the star's position, as determined through optical astrometry, while the second group is found approximately 0.5 arcseconds to the southwest of this position. Both groups correspond to an extended bipolar structure previously identified in single-dish measurements, which has been characterized as a shell-like envelope surrounding the central star. Our findings indicate that the maser emissions in both frequency bands are associated with different regions of this shell-like structure, providing insights into the complex dynamics of the circumstellar environment. Furthermore, we present evidence for a potential third component, which may suggest the influence of a companion object in the vicinity of R Leo. This study enhances our understanding of the maser phenomena in R Leo and contributes to the broader knowledge of stellar environments in the late stages of stellar evolution. \n\n**Keywords:** Masers, R Leo, SiO, circumstellar environment, VLBA, Mira variable stars.",
        "ori-fast-z-score": 0.5933908290969266,
        "water-fast-z-score": 4.865804798594798,
        "rewrite-fast-z-score": 1.3337718577107005
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  FIRST-based survey of Compact Steep Spectrum sources, V. Milliarcsecond-scale morphology of CSS objects .\nAbstract:\nWe present the results of our study on milliarcsec (mas) scale radio morphologies for a sample of compact steep spectrum (CSS) objects using Very Long Baseline Array (VLBA). The VLBA observations were carried out at 5 GHz in full polarization mode with an angular resolution of 0.5 mas and sensitivity better than 100 microJy beam-1 . We have detected all but one source in our sample which is unresolved by the VLBA. Our main findings are as follows:  1. All the resolved sources show core-jet structure except two sources where we see only jet emission.  2. Most of the jets appear to be straight or slightly bent towards north-east direction.  3. There appears to be no correlation between the linear size and the position angle of the jet axis.  4. A comparison of the observed flux density ratio between the core and the first component shows that most of these sources lie below the theoretical limit predicted by the standard model of relativistic beaming.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : FIRST - based studies of Compact Steep Spectrum sources , V . Milliarcsecond - scale morphology of CSS structures . Abstract : We report the conclusion of our research on milliarcsec ( mas ) scale radio morphologies for a sample of compact steep spectrum ( CSS ) images using Very Long Baseline Array ( VLBA ) .The VLBA experiments were carried out at 5 GHz in full polarization mode with an angular resolution of 0 . 5 mas and sensitivity better than 100 microJy beam - 1 . We have discovered all but one origin in our sample which is unresolved by the VLBA .Our main results are as follows : 1 . All the resolved sources show core - jet composition except two sources where we saw only jet radiation .2 . Most of the planes seem to be straight or slightly bent towards north - eastward direction .3 . There seems to be no correlation between the linear size and the orientation angle of the jet axis .4 . A comparison of the seen flux concentration proportion between the core and the first component reveals that most of these sources sit below the theoretical maximum expected by the standard theory of relativistic beaming .",
        "rewrite_text": "We present the findings of our investigation into the milliarcsecond (mas) scale radio morphologies of a selected sample of compact steep spectrum (CSS) sources, utilizing data obtained from the Very Long Baseline Array (VLBA). The VLBA observations were conducted at a frequency of 5 GHz, employing full polarization mode, which provided an impressive angular resolution of 0.5 mas and a sensitivity exceeding 100 microJy per beam. Our analysis led to the identification of all but one source in our sample, which remained unresolved by the VLBA. \n\nThe key outcomes of our study are as follows: Firstly, the majority of the resolved sources exhibit a core-jet structure, with the exception of two sources that displayed only jet radiation. Secondly, the majority of the jets appear to be predominantly straight or exhibit slight bending towards the northeast. Thirdly, we found no significant correlation between the linear size of the sources and the orientation angle of the jet axis. Lastly, our comparative analysis of the flux concentration between the core and the first component indicates that most of the sources in our sample fall below the theoretical maximum predicted by the standard model of relativistic beaming. \n\nThese results contribute to the understanding of the physical characteristics and behaviors of CSS sources at milliarcsecond scales, providing valuable insights into their structural composition and the dynamics of their jet emissions. Our findings have implications for the broader study of radio galaxies and the mechanisms driving their evolution.",
        "ori-fast-z-score": -1.6464638998453551,
        "water-fast-z-score": 4.061277619618543,
        "rewrite-fast-z-score": 0.8620436566990363
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Hamiltonian Graphs and the Traveling Salesman Problem .\nAbstract:\nThe traveling salesman problem (TSP) is one of the most famous problems in combinatorial optimization, which asks for finding an optimal tour through all vertices of a given graph such that each edge is visited exactly once. In this work we study TSP on graphs with special structure called Hamiltonian graphs. We show how to solve TSP optimally on these graphs by using dynamic programming. Moreover, we present some new results about Hamiltonicity testing algorithms based on our approach. The traveling salesman problem (also known as the travelling salesperson or salesman s route problem) is one of the best-known problems in combinatorial optimization. It can be formulated as follows: Given a complete weighted undirected graph G = (V , E), find a cycle C passing through every vertex v ∈ V at least once so that the total weight w(C ) of edges in C is minimized. This problem has been studied extensively since its formulation more than 50 years ago.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Hamiltonian Graphs and the Traveling Salesman Problem . Abstract : The touring salesman problem ( TSP ) is one of the most famous difficulties in combinatorial algorithms , which asks for finding an efficient walk through all edges of a given graph such that each edge is attended exactly once .In this study we study TSP on graphs with special structure named Hamiltonian graphs . We see how to solve TSP optimally on these graphs by using dynamic programming .Moreover , we present some new data about Hamiltonicity training algorithms based on our approach . The touring broker problem ( also known as the travelling salesperson or salesman s route problem ) is one of the best - famous difficulties in combinatorial algorithms .It can be formulated as follows : Given a complete weighted undirected graph G = ( V , E ) , find a cycle C passing through every vertex v ∈ V at least once so that the total weight w ( C ) of vertices in C is minimized . This problem has been studied thoroughly since its formulation more than 50 centuries earlier .",
        "rewrite_text": "Title: Hamiltonian Graphs and the Traveling Salesman Problem\n\nAbstract: The Traveling Salesman Problem (TSP) is a well-known challenge in the field of combinatorial algorithms, which seeks to determine the most efficient route that visits each edge of a given graph exactly once. This paper focuses on the TSP within the context of Hamiltonian graphs, a specific class of graphs characterized by the existence of a Hamiltonian cycle that visits each vertex exactly once. We explore optimal solutions to the TSP on these Hamiltonian graphs through the application of dynamic programming techniques. Our findings reveal novel insights into Hamiltonicity training algorithms, which are informed by our dynamic programming approach. The TSP can be formally defined as follows: given a complete weighted undirected graph G = (V, E), the objective is to identify a cycle C that traverses every vertex v ∈ V at least once while minimizing the total weight w(C) associated with the vertices in C. This problem has garnered significant attention since its inception over 50 years ago, leading to extensive research aimed at developing efficient algorithms for its resolution. In this study, we not only provide a comprehensive analysis of the TSP on Hamiltonian graphs but also contribute new data regarding the performance of Hamiltonicity training algorithms based on our proposed methodology. Our results underscore the importance of understanding the structural properties of Hamiltonian graphs in the pursuit of optimal solutions to the TSP, thereby advancing the field of combinatorial optimization.",
        "ori-fast-z-score": -0.45291081365783825,
        "water-fast-z-score": 4.919349550499537,
        "rewrite-fast-z-score": 2.6943012562182536
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  High energy afterglows and flares from Gamma-Ray Burst by Inverse Compton emission .\nAbstract:\nWe propose that the high energy (HE) component in GRB spectra is produced via inverse Compton scattering between relativistic electrons accelerated at internal shocks, and soft photons emitted during the prompt phase.  We show that this model can explain both the HE spectral shape as well as its temporal evolution for several bursts observed with Fermi/LAT. The predicted light curves are consistent with those measured by Swift/BAT and Fermi/GBM. This scenario also naturally explains why some bursts have no detectable HE emission despite having bright optical flashes. Finally we discuss how our results could be used to constrain the physical parameters of GRBs such as their bulk Lorentz factor or magnetic field strength. High Energy Emission from Gamma Ray Bursts: A Proposal for an Inverse Compton Scattering Model  By J. Piran & M. Lazzati \n \n Abstract \n \n We propose that the high energy component in GRB spectra may be produced via inverse Compton scatterings between relativistic electrons accelerated within internal shocks, and soft photon fields emitted during the prompt phase. We show that this model predicts correctly the spectral shapes and time evolutions of many bursts detected by Fermi LAT. It also provides a natural explanation for the lack of detection of HE emission in some bursts which exhibit bright optical flashes. Our results suggest that future observations will allow us to determine important properties of GRBs like their bulk Lorentz factors or magnetic field strengths.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : High energy afterglows and flares from Gamma - Ray Burst by Inverse Compton emission . Abstract : We suggest that the high energy ( HE ) element in GRB spectra is produced via inverse Compton absorption between relativistic electrons accelerated at internal shocks , and dark photons generated during the prompt phase .We see that this model can describe both the HE spectral structure as also as its temporal evolution for various bursts observed with Fermi / LAT . The predicted light angles are compatible with those observed by Swift / BAT and Fermi / GBM .This scenario additionally naturally reveals why some flashes have no detectable HE emitted despite having bright optical bursts . Finally we talk how our findings may be used to constrain the physical values of GRBs such as their bulk Lorentz factor or magnetic field intensity .High Energy Emission from Gamma Ray Bursts : A Proposal for an Inverse Compton Scattering Model By J . Piran & M . Lazzati Abstract We suggest that the high energy component in GRB spectra might be formed via inverse Compton scatterings between relativistic electrons accelerated within internal shocks , and soft photon fields emitted during the prompt phase . We see that this description predicts correctly the spectral patterns and period evolutions of several bursts detected by Fermi LAT .It additionally offers a natural explanation for the lack of recognition of HE emission in some bursts which exhibit bright optical bursts . Our results show that future discoveries will provide us to identify important characteristics of GRBs like their bulk Lorentz factors or magnetic force abilities .",
        "rewrite_text": "**Title:** High Energy Afterglows and Flares from Gamma-Ray Bursts via Inverse Compton Emission\n\n**Abstract:** In this study, we propose a model to explain the high energy (HE) component observed in the spectra of Gamma-Ray Bursts (GRBs), attributing its origin to inverse Compton scattering processes. This mechanism involves relativistic electrons that are accelerated during internal shock events interacting with dark photons produced during the prompt emission phase. Our model successfully accounts for both the spectral characteristics and the temporal evolution of various GRBs as detected by the Fermi Large Area Telescope (LAT). Furthermore, the predicted light curves align well with observations from the Swift Burst Alert Telescope (BAT) and Fermi Gamma-ray Burst Monitor (GBM). \n\nAn intriguing aspect of our findings is the explanation for the absence of detectable HE emission in certain bursts that still exhibit significant optical brightness. This discrepancy can be reconciled within our framework, suggesting that the conditions leading to HE emission may not always be met, despite the presence of bright optical counterparts. \n\nAdditionally, we discuss the implications of our results for constraining key physical parameters of GRBs, such as their bulk Lorentz factors and magnetic field strengths. By refining our understanding of the mechanisms behind HE emission, we can enhance our ability to characterize GRBs and their underlying physics. Our model not only sheds light on the nature of high energy emissions but also opens avenues for future research aimed at unraveling the complexities of these extraordinary cosmic events.",
        "ori-fast-z-score": -3.1902304138506894,
        "water-fast-z-score": 5.003702332976757,
        "rewrite-fast-z-score": -0.8081220356417685
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Analysis on Heavy Quarkonia Transitions with Pion Emission in Terms of the QCD Multipole Expansion and Determination of Mass Spectra of Hybrids .\nAbstract:\nWe study heavy quarkonium transitions to hybrid mesons by using the multipole expansion method within the framework of nonrelativistic potential models. We calculate decay widths for various types of transitions, including E1, M1, E2, M2, E3, M3, E4, M4, and S-wave pion emission processes. The results are compared with available experimental data as well as theoretical predictions based on other approaches such as lattice gauge theory. \n \n In addition we determine mass spectra of hybrids by solving Schrödinger equations numerically. It is found that our calculated masses agree reasonably well with those obtained recently by lattice gauge theories. Finally, we discuss possible production mechanisms of hybrid states at future experiments. This work was supported by the National Natural Science Foundation of China under Grant No. 10675040. We thank Professors J. H. Chen and C. T. Chan for helpful discussions.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Analysis on Heavy Quarkonia Transitions with Pion Emission in Terms of the QCD Multipole Expansion and Determination of Mass Spectra of Hybrids . Abstract : We research heavy quarkonium transitions to hybrid mesons by using the multipole expansion method within the framework of nonrelativistic potential methods .We calculate decay widths for various types of transitions , particularly E1 , M1 , E2 , M2 , E3 , M3 , E4 , M4 , and S - wave pion emission cycles . The results are compared with existing experimental evidence as well as conceptual predictions based on other methods such as lattice gauge theory .In addition we determine mass spectra of hybrids by solving Schrödinger equations numerically . It is found that our measured masses agree reasonably well with those achieved lately by lattice gauge experiments .Finally , we investigate possible production mechanisms of hybrid states at potential experiments . This project was supported by the National Natural Science Foundation of China under Grant No .10675040 . We praise Professors J . H . Chen and C . T . Chan for helpful talks .",
        "rewrite_text": "Title: Analysis of Heavy Quarkonia Transitions with Pion Emission Using QCD Multipole Expansion and Hybrid Mass Spectra Determination\n\nAbstract: This study investigates the transitions of heavy quarkonia into hybrid mesons through the application of the multipole expansion method, situated within the framework of nonrelativistic potential approaches. We focus on calculating the decay widths associated with various transition types, including electric dipole (E1), magnetic dipole (M1), electric quadrupole (E2), magnetic quadrupole (M2), and higher multipole transitions up to E4, M4, as well as S-wave pion emission processes. Our findings are juxtaposed with current experimental data and theoretical predictions derived from alternative methodologies, such as lattice gauge theory, to assess the validity of our results. \n\nFurthermore, we derive the mass spectra of hybrid mesons by numerically solving the Schrödinger equations, revealing that our computed mass values align reasonably well with recent measurements obtained from lattice gauge experiments. This agreement underscores the robustness of our approach and the reliability of the multipole expansion method in this context. \n\nAdditionally, we explore potential production mechanisms for hybrid states in forthcoming experimental setups, aiming to enhance our understanding of these complex systems. This research is supported by the National Natural Science Foundation of China under Grant No. 10675040. We extend our gratitude to Professors J. H. Chen and C. T. Chan for their insightful discussions, which contributed significantly to the development of this work.",
        "ori-fast-z-score": -0.601929265428846,
        "water-fast-z-score": 4.153735803678487,
        "rewrite-fast-z-score": 0.4216370213557839
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dust Formation and Survival in Supernova Ejecta .\nAbstract:\nWe present the results of dust formation calculations for supernova ejecta using a one-dimensional radiation hydrodynamics code coupled with an equilibrium chemistry network, including grain growth by accretion onto pre-existing grains as well as coagulation between grains. We find that dust can form efficiently at temperatures below 1000 K if there is sufficient carbon available to make amorphous carbon grains. The amount of dust formed depends on the initial composition of the gas; we show that it increases dramatically when oxygen-rich material is mixed into the ejecta. For typical conditions found in Type II SNe, we predict that 0.1 M⊙ of dust forms within 10 years after explosion. This dust survives destruction due to sputtering by ions accelerated during the reverse shock passage through the ejecta. Our results suggest that dust may be produced in significant amounts in some core-collapse supernovae. \n \n Keywords: Dust, Supernova",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dust Formation and Survival in Supernova Ejecta . Abstract : We present the results of dust structure analysis for supernova ejecta using a one - dimensional radiation hydrodynamics code combined with an equilibrium chemistry system , covering grain growth by accretion onto pre - existing grains as well as coagulation between wheat .We see that dust can form efficiently at conditions below 1000 K if there is adequate carbon available to make amorphous carbon grains . The amount of dust formed depends on the early structure of the gas ; we find that it rises dramatically when oxygen - rich substance is mixed into the ejecta .For typical conditions found in Type II SNe , we predict that 0 . 1 [UNK] of dust forms within 10 years after explosion . This dust survives destruction due to sputtering by ions accelerated during the reverse shock passage through the ejecta .Our results propose that matter may be formed in considerable amounts in some core - collapse supernovae . Keywords : Dust , Supernova",
        "rewrite_text": "We present a comprehensive analysis of dust formation and survival within supernova ejecta, utilizing a one-dimensional radiation hydrodynamics code integrated with an equilibrium chemistry framework. Our study focuses on the processes of grain growth through accretion onto pre-existing grains, as well as the coagulation of dust particles. Our findings indicate that dust can form efficiently at temperatures below 1000 K, provided there is sufficient carbon available to create amorphous carbon grains. The quantity of dust produced is significantly influenced by the initial gas structure; notably, we observe a substantial increase in dust formation when oxygen-rich materials are incorporated into the ejecta. In typical conditions associated with Type II supernovae, our predictions suggest that approximately 0.1 solar masses of dust can form within the first decade following the explosion. Furthermore, this dust demonstrates resilience against destruction caused by sputtering from ions that are accelerated during the passage of the reverse shock through the ejecta. Our results imply that considerable amounts of dust may be generated in certain core-collapse supernova events, contributing to the understanding of dust production in the universe. This research enhances our comprehension of the lifecycle of matter in supernovae and its implications for cosmic dust evolution. \n\nKeywords: Dust, Supernova",
        "ori-fast-z-score": -0.4472135954999579,
        "water-fast-z-score": 3.6666666666666665,
        "rewrite-fast-z-score": 1.3862065601673441
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Torsional oscillations of longitudinally inhomogeneous coronal loops .\nAbstract:\nWe study the torsional oscillations of an inhomogeneous magnetic flux tube with longitudinal density variation and uniform twist, which is embedded into a gravitationally stratified atmosphere. The governing equations are derived by using the thin-tube approximation for both equilibrium state and linear perturbations. We find that there exist two types of eigenmodes corresponding to different wave numbers along the field line. One type has its maximum amplitude at the footpoint while another one has it near the apex. For each mode we calculate the frequency as well as the damping time due to radiative loss. It turns out that the frequencies of these modes depend on the density contrast between the base and top of the loop. In addition, they also depend on the ratio of the Alfvén speed inside the loop to that outside. Finally, we discuss how our results can be applied to observations. Keywords: Torsional oscillation, Inhomogeneity",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Torsional oscillations of longitudinally inhomogeneous coronal loops . Abstract : We explore the torsional oscillations of an inhomogeneous magnetic flux tube with longitudinal density variation and uniform twist , which is embedded into a gravitationally stratified atmosphere .The governing equations are derived by using the narrow - tunnel method for both equilibrium state and linear perturbations . We see that there exist two forms of eigenmodes analogous to different wave numbers along the field line .One sort has its highest amplitude at the footpoint while another one has it near the apex . For each mode we estimate the frequency as well as the damping period caused to radiative loss .It turns out that the bandwidth of these modes differ on the density contrast between the base and top of the loop . In addition , they also rely on the proportion of the Alfvén speed inside the loop to that outside .Finally , we explain how our findings can be applied to observations . Keywords : Torsional oscillation , Inhomogeneity",
        "rewrite_text": "Title: Torsional Oscillations of Longitudinally Inhomogeneous Coronal Loops\n\nAbstract: In this study, we investigate the torsional oscillations occurring in an inhomogeneous magnetic flux tube characterized by a longitudinal density gradient and a uniform twist, situated within a gravitationally stratified atmosphere. Utilizing the narrow-tunnel method, we derive the governing equations that describe both the equilibrium state and the linear perturbations of the system. Our analysis reveals the existence of two distinct types of eigenmodes, each corresponding to different wave numbers along the magnetic field lines. One mode exhibits maximum amplitude at the footpoint of the loop, while the other is characterized by peak amplitude near the apex. For both modes, we calculate the oscillation frequencies and the associated damping periods, which arise due to radiative losses. Notably, we find that the bandwidth of these oscillatory modes is influenced by the density contrast between the base and the top of the coronal loop. Furthermore, the relationship between the Alfvén speed inside the loop and that outside also plays a critical role in determining the properties of these modes. Our findings have significant implications for observational studies, as they provide a framework for understanding the dynamics of coronal loops and their oscillatory behavior. By linking theoretical predictions with observational data, we aim to enhance the comprehension of coronal loop oscillations and their relevance to solar physics. \n\nKeywords: Torsional oscillation, Inhomogeneity",
        "ori-fast-z-score": -0.9561828874675149,
        "water-fast-z-score": 3.771236166328254,
        "rewrite-fast-z-score": 2.8879794895246214
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Information Theoretic Proofs of Entropy Power Inequalities .\nAbstract:\nEntropies are measures of uncertainty, and entropy power inequalities (EPIs) quantify the trade-off between information transmission rates in different communication channels.  In this work we present new proofs for EPIs based on information theory concepts such as mutual information and channel capacity.   We also show that these results can be used to prove Shannon s source coding theorem by applying them to an appropriate binary memoryless symmetric channel model. Finally, we discuss how our approach could potentially lead to improved bounds on the minimum distance of linear block codes over finite fields. Entropies are measures of uncertainty; entropy power inequalities (EPIS) quantify the tradeoff between information transmission rates in various communication channels. In this work we present novel proofs for EPIs using information-theory concepts like mutual information and channel capacity. We also demonstrate that these results may be utilized to verify Shannon s source coding theorem via their application to a suitable binary memoryless-symmetric channel model. Finally,we discuss how our technique might possibly yield better bounds on the minimal distance of linear block codes across finite fields.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Information Theoretic Proofs of Entropy Power Inequalities . Abstract : Entropies are measures of uncertainty , and entropy energy inequalities ( EPIs ) quantify the trade - off between information transmission rates in different communication streams .In this study we present new proofs for EPIs based on information principle concepts such as mutual information and channel capacity . We additionally prove that these results can be used to prove Shannon s source coding theorem by application them to an appropriate binary memoryless symmetric channel structure .Finally , we explain how our approach could potentially contribute to greater bounds on the minimum distance of linear block codes over discrete fields . Entropies are measures of uncertainty ; entropy energy inequalities ( EPIS ) quantify the tradeoff between information transmission rates in different communication channels .In this study we present new proofs for EPIs using information - theory ideas like mutual information and channel capacity . We additionally prove that these results may be employed to confirm Shannon s source coding theorem via their application to a suitable binary memoryless - symmetric channel theory .Finally , we explain how our technique may possibly provide better bounds on the reduced distance of linear block sequences across finite fields .",
        "rewrite_text": "Title: Information Theoretic Proofs of Entropy Power Inequalities\n\nAbstract: Entropy serves as a fundamental measure of uncertainty in information theory, and entropy power inequalities (EPIs) play a crucial role in characterizing the trade-offs between information transmission rates across various communication channels. In this article, we introduce novel proofs for EPIs that are grounded in key concepts from information theory, including mutual information and channel capacity. Our findings demonstrate that these proofs not only reinforce the validity of EPIs but also provide a pathway to affirm Shannon's source coding theorem when applied to a suitable binary memoryless symmetric channel framework. This connection underscores the relevance of our approach in the broader context of information theory. Furthermore, we explore the implications of our results for linear block codes over discrete fields, suggesting that our methodology could lead to improved bounds on the minimum distance of these codes. By leveraging the principles of entropy and information transmission, our work contributes to a deeper understanding of the interplay between uncertainty and communication efficiency, paving the way for advancements in coding theory and its applications. Overall, this study enhances the theoretical foundation of EPIs and opens new avenues for research in information theory, particularly in optimizing communication systems and coding strategies.",
        "ori-fast-z-score": 0.19425717247145283,
        "water-fast-z-score": 6.225870853937484,
        "rewrite-fast-z-score": 1.507556722888818
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Low energy cut-offs and hard X-ray spectra in high-z radio-loud quasars: the Suzaku view of RBS315 .\nAbstract:\nWe report on our analysis of the Suzaku observation of the z = 1.55 quasar, RBS 315 (RA=00h45m53.6s; DEC=-36d19 59.6 ). The source is detected with an average 2-10 keV flux of 4 x 10^-13 erg cm-2 s-1 corresponding to a luminosity of 3 x 10^44 erg s-1 at this redshift. We find that the spectrum can be well fitted by a power law model modified by Galactic absorption plus reflection component using pexrav model in XSPEC. This gives photon index Γ=1.9 +0.2 -0.1 , reflection fraction f=0.7 +1.0 -1.3 . The observed 0.5-7 keV band luminosity is found to be 5x10^43 erg/sec which corresponds to Eddington ratio L/L edd =0.01-0.03 assuming black hole mass M BH ~10 9 M sun .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Low energy cut - offs and hard X - ray spectra in high - z radio - loud quasars : the Suzaku view of RBS315 . Abstract : We report on our analysis of the Suzaku observation of the z = 1 . 55 quasar , RBS 315 ( RA = 00h45m53 . 6s ; DEC = - 36d19 59 . 6 ) .The source is detected with an mean 2 - 10 keV flux of 4 x 10 ^ - 13 erg centimetres - 2 s - 1 resulting to a luminosity of 3 x 10 ^ 44 erg s - 1 at this redshift . We see that the spectrum can be well fitted by a power law formula derived by Galactic absorption plus reflection factor using pexrav method in XSPEC .This gives photon index Γ = 1 . 9 + 0 . 2 - 0 . 1 , reflection fraction f = 0 . 7 + 1 . 0 - 1 . 3 . The observed 0 . 5 - 7 keV band luminosity is found to be 5x10 ^ 43 erg / sec which corresponds to Eddington ratio L / L edd = 0 . 01 - 0 . 03 assuming black hole mass M BH ~ 10 9 M sun .",
        "rewrite_text": "We present our findings from the analysis of the Suzaku observation of the quasar RBS 315, located at a redshift of z = 1.55 (RA = 00h45m53.6s; DEC = -36d19m59.6s). The quasar exhibits a mean flux in the 2-10 keV range of 4 x 10^-13 erg cm^-2 s^-1, which translates to a luminosity of approximately 3 x 10^44 erg s^-1 at this redshift. Our spectral analysis indicates that the data can be accurately modeled using a power-law function that incorporates Galactic absorption and a reflection component, employing the pexrav method within the XSPEC software. The resulting fit yields a photon index of Γ = 1.9 +0.2 -0.1 and a reflection fraction of f = 0.7 +1.0 -1.3. Additionally, we calculated the luminosity in the 0.5-7 keV band, which is determined to be 5 x 10^43 erg/s. This luminosity corresponds to an Eddington ratio (L/L_edd) of approximately 0.01 to 0.03, assuming a black hole mass of M_BH ~ 10^9 M_sun. Our results contribute to the understanding of the high-energy properties of radio-loud quasars at high redshifts, providing insights into their accretion processes and the role of black hole mass in their evolution. The findings from RBS 315 enhance the existing knowledge of quasar spectra and their underlying physical mechanisms, particularly in the context of low energy cut-offs and hard X-ray emissions. This study underscores the importance of high-energy observations in unraveling the complexities of quasar behavior and their impact on the surrounding environment.",
        "ori-fast-z-score": -0.39735970711951313,
        "water-fast-z-score": 1.5756771943166705,
        "rewrite-fast-z-score": -1.2510864843424487
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  XMM-Newton observations of the first unidentified TeV gamma-ray source TeV J2032+4130 .\nAbstract:\nWe report on XMM-Newton and Chandra X-ray Observatory (CXO) observations of the recently discovered high-energy gamma-ray source, TeV J2032+4131. The data show that this object is an active galactic nucleus with a power-law spectrum extending to at least 100 keV. We find no evidence for absorption by intervening material in excess of Galactic values along its line-of-sight. A comparison between our results and those obtained using other instruments suggests that there may be significant variability in both the flux density and spectral index of TeV J2032 + 4131 over timescales as short as one day. This would imply either rapid changes in intrinsic emission or strong Doppler boosting effects due to relativistic motion of the emitting region. \n \n Keywords: Active galactic nuclei, Gamma rays, Variability, X-rays, High energy astrophysics \n \n 1. Introduction \n \n In recent years, several new classes of high energy sources have been identified through their detection at very-high energies (E > 10 GeV). These include blazars, radio galaxies, pulsar wind nebulae, supernova remnants, starburst galaxies, galaxy clusters, and possibly even some nearby stars  1  . However, many of these objects are still poorly understood because they lack counterparts at lower frequencies where most of the relevant physical processes occur  2  .\n \nIn particular, it has proven difficult to identify the origin of the highest energy photons detected so far  3  , which can reach energies up to 1020 eV  4  . One possible explanation is that such photons are produced during interactions involving extremely energetic particles accelerated within compact regions close to supermassive black holes  5  . Alternatively, they could result from decays of neutral pions created when cosmic ray protons interact with ambient matter  6  . If confirmed, such events would provide important insights into particle acceleration mechanisms near black holes  7, 8  . \n \n Recently, the HESS collaboration reported the discovery of a bright point-like gammaray source located at RA = 20 h 32 m 41 s ± 5′′ and Dec = +39°30′00",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : XMM - Newton discoveries of the first unseen TeV gamma - ray source TeV J2032 + 4130 . Abstract : We report on XMM - Newton and Chandra X - ray Observatory ( CXO ) observations of the recently discovered high - energy gamma - ray source , TeV J2032 + 4131 .The data reveal that this body is an active galactic nucleus with a power - law spectrum stretching to at least 100 keV . We see no evidence for absorption by intervening material in excess of Galactic values along its line - of - view .A comparison between our findings and those achieved using other instruments suggests that there may be considerable variability in both the flux concentration and spectral index of TeV J2032 + 4131 over timescales as short as one day . This might imply either rapid increases in intrinsic emission or strong Doppler boosting effects due to relativistic movement of the emitting area .Keywords : Active galactic nuclei , Gamma rays , Variability , X - rays , High energy astrophysics 1 . Introduction In recent years , various additional types of high energy sources have been described through their observation at very - large energies ( E > 10 GeV ) .These include blazars , television stars , pulsar wind nebulae , supernova remnants , starburst clusters , galaxy clusters , and maybe even some nearby stars 1 . However , many of these objects are still ill explained because they lack counterparts at lower frequencies where most of the appropriate physical processes involve 2 .In particular , it has proven unable to identify the origin of the highest power photons discovered so far 3 , which can reach energies up to 1020 eV 4 . One potential explanation is that such photons are produced during interactions involving extremely energetic particles driven within compact regions close to supermassive black holes 5 .Alternatively , they may come from decays of neutral pions created when cosmic ray protons interact with ambient material 6 . If confirmed , such events might give important knowledge into particle acceleration mechanisms near black holes 7 , 8 .Recently , the HESS collaboration reported the discovery of a bright point - like gammaray source located at RA = 20 h 32 m 41 s ± 5 ′ ′ and Dec = + 39°30 ′ 00",
        "rewrite_text": "**Title:** XMM-Newton Discoveries of the First Unseen TeV Gamma-Ray Source TeV J2032+4130\n\n**Abstract:** In this study, we present findings from observations conducted with the XMM-Newton and Chandra X-ray Observatory (CXO) on the newly identified high-energy gamma-ray source, TeV J2032+4131. Our analysis indicates that this source is an active galactic nucleus (AGN) characterized by a power-law spectrum that extends to at least 100 keV. Notably, we find no significant evidence of absorption by intervening material beyond the expected Galactic values along the line of sight to this source. A comparative analysis with data obtained from other observational instruments suggests that TeV J2032+4131 exhibits substantial variability in both its flux and spectral index over remarkably short timescales, potentially as brief as one day. This variability may indicate rapid intrinsic emission fluctuations or could be attributed to strong Doppler boosting effects resulting from the relativistic motion of the emitting region.\n\nThe study of high-energy sources has expanded significantly in recent years, with various types of objects being identified at energies exceeding 10 GeV. These include blazars, pulsar wind nebulae, supernova remnants, starburst clusters, and galaxy clusters, among others. However, many of these sources remain poorly understood due to the absence of counterparts at lower frequencies, where the relevant physical processes are typically observed. A critical challenge in high-energy astrophysics is the identification of the origins of the highest energy photons, which can reach up to 10^20 eV. One hypothesis posits that these photons are generated through interactions involving extremely energetic particles in close proximity to supermassive black holes. Alternatively, they may arise from the decay of neutral pions produced when cosmic ray protons collide with surrounding material. Confirming these mechanisms could provide valuable insights into particle acceleration processes occurring near black holes. Recently, the HESS collaboration reported the discovery of a bright point-like gamma-ray source located at RA = 20h 32m 41s ± 5′ and Dec = +39°30′00″, further emphasizing the significance of ongoing research in this field. \n\n**Keywords:** Active galactic nuclei, Gamma rays, Variability, X-rays, High energy astrophysics.",
        "ori-fast-z-score": 0.5482823149915702,
        "water-fast-z-score": 7.240040180702163,
        "rewrite-fast-z-score": 2.7756805500135044
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Mass of the Candidate Exoplanet Companion to HD 33636 from Hubble Space Telescope Astrometry and High-Precision Radial Velocities .\nAbstract:\nWe present new astrometric measurements for the candidate exoplanet companion to HD 33636, obtained with the Fine Guidance Sensor (FGS) on board the Hubble Space Telescope (HST). These data are combined with previously published radial velocities in order to determine the mass of this object. We find that it is most likely an M dwarf star with a mass between 0.3 and 1.0 times that of Jupiter s mass. The orbital parameters derived here agree well with those determined by previous authors using different techniques. This system may be similar to our own solar system at early stages of formation when planets were still forming around young stars. Keywords: Extrasolar planet -Astrometry -Radial velocity -HST -Mass determination -HD 33636 . \nIntroduction\n\nThe detection of extrasolar giant planets has been one of the major accomplishments of modern astronomy over the past decade. However, only about 10% of all known planetary systems contain such massive objects. Most of these have been discovered through high-precision Doppler spectroscopy or direct imaging methods. In contrast, very few low-mass companions have been found so far because they produce smaller reflex motions and/or lower luminosity than their more massive counterparts. As a result, there exists a large gap in the distribution of masses among known extra-solar planets ranging from several Earth masses down to Neptune-like masses. It is therefore important to search for low-mass companions as well since they can provide valuable information regarding the formation process of planetary systems. \n \n One possible way to detect low-mass companions is to use high-angular resolution observations made with space-based telescopes like HST. Such observations allow us to measure the position angle of the host star relative to its nearby neighbors. If we assume that the observed motion is due solely to gravitational interaction with another body then we can derive the projected separation and position angle of the companion. By combining these results with accurate radial-velocity measurements taken simultaneously, we can obtain the full three-dimensional orbit of the companion which allows us to calculate its mass.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Mass of the Candidate Exoplanet Companion to HD 33636 from Hubble Space Telescope Astrometry and High - Precision Radial Velocities . Abstract : We report new astrometric measurements for the candidate exoplanet companion to HD 33636 , obtained with the Fine Guidance Sensor ( FGS ) on board the Hubble Space Telescope ( HST ) .These data are coupled with previously written radial velocities in order to estimate the mass of this body . We see that it is most likely an M dwarf star with a mass between 0 . 3 and 1 . 0 times that of Jupiter s mass .The orbital characteristics obtained here agree well with those determined by earlier authors using separate techniques . This system might be analogous to our own solar body at early stages of formation when stars were still forming around young stars .Keywords : Extrasolar planet - Astrometry - Radial velocity - HST - Mass determination - HD 33636 . Introduction The detection of extrasolar giant planets has been one of the main accomplishments of modern astronomy over the previous decade .However , only about 10 % of all known planetary structures possess such enormous objects . Most of these have been detected through high - speed Doppler spectroscopy or direct imaging technique .In contrast , very few low - density companions have been seen so far because they produce shorter reflex motions and / or lower luminosity than their more massive counterparts . As a result , there exists a large gap in the distribution of masses among known extra - solar planets ranging from several Earth masses down to Neptune - like masses .It is consequently key to search for low - mass companions as well since they can provide valuable info regarding the formation transition of planetary structures . One potential way to identify low - mass companions is to use large - angular resolution measurements made with space - based telescopes like HST .Such observations allow us to measure the position angle of the host star relative to its neighbouring counterparts . If we suppose that the seen moving is due exclusively to gravitational interaction with another bodies then we can derive the projected separation and position angle of the companion .By combining these results with accurate radial - speed measurements taken concurrently , we can obtain the full three - dimensional orbit of the companion which allows us to estimate its mass .",
        "rewrite_text": "**Title:** The Mass of the Candidate Exoplanet Companion to HD 33636 from Hubble Space Telescope Astrometry and High-Precision Radial Velocities\n\n**Abstract:** In this study, we present new astrometric measurements for the candidate exoplanet companion to HD 33636, acquired using the Fine Guidance Sensor (FGS) aboard the Hubble Space Telescope (HST). These measurements are integrated with previously collected radial velocity data to estimate the mass of the companion body. Our analysis suggests that this companion is likely an M dwarf star, with a mass ranging from 0.3 to 1.0 times that of Jupiter. The orbital parameters derived from our observations are consistent with those reported by earlier researchers utilizing different methodologies. This system may serve as a valuable analog to the early stages of our own solar system's formation, when stars were still coalescing around nascent stellar bodies.\n\nThe discovery of extrasolar giant planets has emerged as a significant achievement in contemporary astronomy over the past decade. However, only about 10% of identified planetary systems contain such massive entities. Most of these have been detected through high-velocity Doppler spectroscopy or direct imaging techniques. In contrast, the detection of low-density companions remains limited, primarily due to their shorter reflex motions and lower luminosity compared to their more massive counterparts. Consequently, there exists a notable gap in the mass distribution of known exoplanets, spanning from several Earth masses to Neptune-like masses. It is crucial to pursue the identification of low-mass companions, as they can yield essential insights into the transitional processes involved in planetary formation.\n\nOne effective approach to locate low-mass companions is through high-angular resolution measurements conducted with space-based telescopes like HST. Such observations enable the precise measurement of the position angle of the host star in relation to its neighboring stars. Assuming that the observed motion is solely due to gravitational interactions with other bodies, we can derive the projected separation and position angle of the companion. By combining these findings with accurate radial velocity measurements obtained simultaneously, we can reconstruct the complete three-dimensional orbit of the companion, facilitating a reliable estimation of its mass.",
        "ori-fast-z-score": -0.5291502622129182,
        "water-fast-z-score": 6.745785200962753,
        "rewrite-fast-z-score": 0.1543033499620919
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  9.7 micrometer Silicate Absorption in a Damped Lyman-alpha Absorber at z=0.52 .\nAbstract:\nWe report the detection of absorption by silicates with an optical depth of 0.1 at 9.7 microns toward the quasar HE 0515-4414 (z = 0.52) using data obtained with ISO-SWS and LWS on board ISO. The silicate feature is detected only when we use the full resolution spectrum, which shows that it has been smoothed out due to blending with other features in lower-resolution spectra. We find no evidence for dust emission associated with this absorber. This result suggests that the absorbing material consists mainly of cold gas rather than warm dust. If so, then the mass of cool gas required to produce such strong absorption lines would be much larger than expected based on current models of galaxy formation. In addition, if the observed absorption arises solely from cold gas, then the implied covering factor of the absorber must be very large compared to what is seen in local galaxies.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : 9 . 7 micrometer Silicate Absorption in a Damped Lyman - alpha Absorber at z = 0 . 52 . Abstract : We report the observation of absorption by silicates with an optical height of 0 . 1 at 9 . 7 microns toward the quasar HE 0515 - 4414 ( z = 0 . 52 ) using data received with ISO - SWS and LWS on board ISO .The silicate characteristic is detected only when we using the full resolution spectrum , which demonstrates that it has been softened out due to mixing with other features in smaller - resolution spectra . We see no evidence for dust absorption associated with this absorber .This result suggests that the absorbing material contains primarily of cold gas instead than cool dust . If so , then the mass of cold gas necessary to produce such strong absorption patterns must be much larger than expected based on current theories of galaxy formation .In addition , if the seen absorbed occurs exclusively from cool gas , then the implied covering element of the absorber would be very huge compared to what is seen in local stars .",
        "rewrite_text": "We present our findings on the detection of silicate absorption at a wavelength of 9.7 micrometers in a damped Lyman-alpha absorber located at redshift z = 0.52, specifically towards the quasar HE 0515-4414. Utilizing data obtained from the Infrared Space Observatory's Short-Wavelength Spectrometer (ISO-SWS) and Long-Wavelength Spectrometer (ISO-LWS), we have identified an optical depth of 0.1 associated with the silicate feature. Notably, this characteristic absorption is only observable when analyzing the high-resolution spectrum, indicating that lower-resolution data may obscure this feature due to blending with other spectral lines. Our analysis reveals no signs of dust absorption linked to this absorber, leading us to conclude that the material responsible for the absorption is predominantly cold gas rather than cool dust. This finding has significant implications for our understanding of galaxy formation, as it suggests that the mass of cold gas required to generate such pronounced absorption features is likely much greater than current theoretical models predict. Furthermore, if the observed absorption is indeed solely due to cool gas, the inferred covering fraction of the absorber would be substantially larger than what is typically observed in local stellar environments. This research contributes to the broader discourse on the composition and characteristics of interstellar matter in damped Lyman-alpha systems, highlighting the need for revised models that account for the substantial presence of cold gas in these cosmic structures.",
        "ori-fast-z-score": -1.118033988749895,
        "water-fast-z-score": 5.444444444444445,
        "rewrite-fast-z-score": -0.47891314261057566
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Improving Intrinsic Decoherence in Multi-Quantum-Dot Charge Qubits .\nAbstract:\nWe report on the fabrication and characterization of charge qubits based on self-assembled InAs quantum dots (QDs) embedded in GaAs/AlGaAs heterostructures. We show that by using an optimized growth procedure, we can achieve high quality QD layers with low density of defects which are crucial for achieving good coherence times. The samples were grown by molecular beam epitaxy at 600 °C under As-rich conditions to minimize the formation of threading dislocations. A single layer of self-assembled InAs/GaAs QDs was formed after annealing at 650 °C for 10 s followed by deposition of a 50 nm thick Al0.3Ga0.7As barrier layer. Finally, a 20 nm thick GaAs capping layer was deposited. The sample structure is shown schematically in Figure 1 . The photoluminescence spectrum shows emission peaks centered around 1280 nm corresponding to ground state excitonic transitions of individual QDs as well as higher energy states associated with charged excitons.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Improving Intrinsic Decoherence in Multi - Quantum - Dot Charge Qubits . Abstract : We report on the fabrication and description of charge qubits based on self - assembled InAs quantum dots ( QDs ) integrated in GaAs / AlGaAs heterostructures .We see that by using an optimized growth technique , we can attain high quality QD layers with minimal concentration of defects which are important for achieving better coherence times . The samples were grown by molecular beam epitaxy at 600 °C under As - rich conditions to minimize the formation of threading dislocations .A single layer of self - assembled InAs / GaAs QDs was formed after annealing at 650 °C for 10 s followed by deposition of a 50 nm wide Al0 . 3Ga0 . 7As barrier layer . Finally , a 20 nm wide GaAs capping layer was deposited .The sample structure is displayed schematically in Figure 1 . The photoluminescence spectrum displays emission levels focused around 1280 nm corresponding to ground state excitonic transitions of individual QDs as well as higher energy states correlated with charged excitons .",
        "rewrite_text": "We present a comprehensive study on the development and characterization of charge qubits utilizing self-assembled InAs quantum dots (QDs) integrated within GaAs/AlGaAs heterostructures. Our research demonstrates that by employing an optimized growth technique, we can produce high-quality QD layers with a significantly reduced defect concentration, which is crucial for enhancing coherence times in quantum computing applications. The quantum dot samples were synthesized using molecular beam epitaxy at a temperature of 600 °C under arsenic-rich conditions, a method specifically designed to minimize the occurrence of threading dislocations that can adversely affect qubit performance. Following the growth process, a single layer of self-assembled InAs/GaAs QDs was achieved through a brief annealing step at 650 °C for 10 seconds, after which a 50 nm thick Al0.3Ga0.7As barrier layer was deposited to encapsulate the quantum dots. To complete the structure, a 20 nm thick GaAs capping layer was applied. The resulting sample architecture is illustrated in Figure 1. Photoluminescence measurements reveal emission peaks centered around 1280 nm, which correspond to the ground state excitonic transitions of individual quantum dots, alongside higher energy emissions associated with charged excitons. This work not only highlights the potential of self-assembled quantum dots for use in charge qubits but also provides insights into the fabrication techniques that can lead to improved intrinsic decoherence properties, thereby paving the way for more robust quantum computing systems.",
        "ori-fast-z-score": -0.4588314677411235,
        "water-fast-z-score": 2.8490144114909484,
        "rewrite-fast-z-score": -0.3682298471593294
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The X-ray Properties of the Most-Luminous Quasars from the Sloan Digital Sky Survey .\nAbstract:\nWe present an analysis of the most luminous quasars in the Sloan Digital Sky Survey; these are among the brightest known objects at any wavelength, and have been studied extensively over many years. We use Chandra observations to study their X-ray properties as well as those of lower-luminosity quasars with similar redshifts (z ~ 2). The results show that there is no significant difference between the two samples; both exhibit very hard spectra and high luminosities. This suggests that the physical processes responsible for producing X-rays do not depend strongly on quasar luminosity or redshift. In addition, we find evidence that the X-ray emission may be associated with outflows driven by winds originating near the central black hole. These findings provide new insights into how supermassive black holes grow during the early stages of galaxy formation. \n \n Keywords: quasars, X-ray emission, wind-driven outflow",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The X - ray Properties of the Most - Luminous Quasars from the Sloan Digital Sky Survey . Abstract : We present an assessment of the most luminous quasars in the Sloan Digital Sky Survey ; these are among the brightest known objects at any wavelength , and have been studied frequently over numerous years .We use Chandra measurements to study their X - ray characteristics as well as those of lower - luminosity quasars with similar redshifts ( z ~ 2 ) . The results show that there is no considerable difference between the two specimens ; both exhibit very hard spectra and large luminosities .This implies that the physical processes responsible for producing X - radiation do not depend greatly on quasar luminosity or redshift . In addition , we find proof that the X - ray radiation may be involved with outflows driven by winds occurring near the central black hole .These studies provide fresh insights into how supermassive black holes expand during the early stages of galaxy formation . Keywords : quasars , X - ray radiation , wind - propelled outflow",
        "rewrite_text": "We present a comprehensive analysis of the most luminous quasars identified in the Sloan Digital Sky Survey (SDSS), which are recognized as some of the brightest celestial objects across all wavelengths. These quasars have been the subject of extensive research over many years. Utilizing data from the Chandra X-ray Observatory, we investigate the X-ray properties of these high-luminosity quasars and compare them with lower-luminosity quasars that share similar redshifts (approximately z ~ 2). Our findings indicate that there is no significant difference in the X-ray characteristics between the two groups; both types of quasars display very hard X-ray spectra and exhibit substantial luminosities. This suggests that the mechanisms responsible for generating X-ray emissions are largely independent of the quasar's luminosity or redshift. Furthermore, our analysis provides evidence that X-ray emissions may be linked to outflows driven by winds originating from the vicinity of the central supermassive black hole. These insights contribute to a deeper understanding of the role of supermassive black holes in the early phases of galaxy formation and evolution. Overall, this study enhances our knowledge of the fundamental processes governing quasar behavior and the interaction between black holes and their surrounding environments. \n\nKeywords: quasars, X-ray emissions, wind-driven outflows.",
        "ori-fast-z-score": 0.35603449745815596,
        "water-fast-z-score": 5.656854249492381,
        "rewrite-fast-z-score": 1.6081688022566922
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Diffuse X-ray Emission from the Carina Nebula Observed with Suzaku .\nAbstract:\nWe report on diffuse X-ray emission in the Carina Nebula observed by Suzaku. The spectrum is well reproduced by thermal plasma models at kT = 0.7-1 keV and nH = (0.5-2) x 10^(22) cm^{-3}, which are consistent with those obtained previously for other regions within the nebula. We find that the total luminosity of this component amounts to Lx ~ 1.3 x 10^35 erg/sec, corresponding to about 10% of the total energy output of massive stars in the region. This suggests that hot gas produced by stellar winds and/or supernovae plays an important role in heating up the interstellar medium around young open clusters such as Trumpler 14-16. \n \n \n \n Keywords: Diffuse X-rays, Hot plasma, Open cluster, Supernova remnant, Stellar wind, Carina Nebula",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Diffuse X - ray Emission from the Carina Nebula Observed with Suzaku . Abstract : We report on diffuse X - ray radiation in the Carina Nebula observed by Suzaku .The spectrum is well illustrated by thermal plasma estimates at kT = 0 . 7 - 1 keV and nH = ( 0 . 5 - 2 ) x 10 ^ ( 22 ) cm ^ { - 3 } , which are compatible with those obtained previously for other regions within the nebula . We see that the total luminosity of this component amounts to Lx ~ 1 . 3 x 10 ^ 35 erg / sec , equivalent to about 10 % of the total energy output of large stars in the region .This implies that heat gas created by stellar winds and / or supernovae plays an important role in heating up the interstellar medium around young open complexes such as Trumpler 14 - 16 . Keywords : Diffuse X - radiation , Hot plasma , Open cluster , Supernova remnant , Stellar wind , Carina Nebula",
        "rewrite_text": "Title: Diffuse X-ray Emission from the Carina Nebula Observed with Suzaku\n\nAbstract: In this study, we present findings on the diffuse X-ray emission detected in the Carina Nebula using the Suzaku satellite. Our analysis reveals that the X-ray spectrum can be accurately described by thermal plasma models, with temperatures ranging from kT = 0.7 to 1 keV and hydrogen column densities of nH = (0.5 - 2) x 10^(22) cm^(-3). These parameters align well with previous measurements obtained from other regions within the nebula, suggesting a consistent thermal environment across the area. The total luminosity associated with this diffuse X-ray emission is estimated to be Lx ~ 1.3 x 10^(35) erg/sec, which constitutes approximately 10% of the total energy output from the massive stars located in the vicinity. This significant fraction indicates that the thermal energy generated by stellar winds and supernova events plays a crucial role in heating the interstellar medium surrounding young open clusters, particularly in regions such as Trumpler 14-16. Our findings contribute to the understanding of the interplay between massive stars and their environment, highlighting the importance of stellar feedback mechanisms in shaping the thermal structure of the Carina Nebula. The results underscore the necessity for further investigations into the dynamics of hot plasma and its implications for star formation and the evolution of stellar clusters. \n\nKeywords: Diffuse X-ray emission, Thermal plasma, Open clusters, Supernova remnants, Stellar winds, Carina Nebula.",
        "ori-fast-z-score": -1.4832396974191326,
        "water-fast-z-score": 2.138089935299395,
        "rewrite-fast-z-score": 0.39605901719066977
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Atmospheric Dynamics of Short-period Extra Solar Gas Giant Planets I: Dependence of Night-Side Temperature on Opacity .\nAbstract:\nWe present the first results for atmospheric dynamics in short period extra-solar gas giant planets (EGPs) using 3D general circulation models with radiative transfer and realistic opacities. We find that the night-side temperature is strongly dependent on opacity, which determines how much energy can be transported to space by radiation. The day-night contrast increases as we decrease the opacity because less heat escapes through the nightside atmosphere. This effect is more pronounced at lower pressures where convection becomes inefficient. For low enough opacities, the planet cools down completely during its orbit resulting in an extremely cold night side. Our simulations show that EGPs are likely to have very different climates depending on their composition.  Keywords: General Circulation Modeling, Extrasolar Planetary Systems, Radiation Transfer, Climate, Atmosphere, Energy Transport, Convection, Cooling Rates, Day-Night Contrast",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Atmospheric Dynamics of Short - duration Extra Solar Gas Giant Planets I : Dependence of Night - Side Temperature on Opacity . Abstract : We report the first findings for atmospheric evolution in small period extra - solar gas giant worlds ( EGPs ) using 3D general circulation estimates with radiative transfer and realistic opacities .We see that the night - side temperature is strongly dependent on opacity , which determines how many heat can be transported to space by radiation . The day - night difference grows as we decrease the opacity because lighter energy escapes through the nightside environment .This phenomenon is more pronounced at lower pressures where circulation becomes inefficient . For lowest sufficient opacities , the planet cools down fully during its orbit resulting in an incredibly cold night side .Our simulations signal that EGPs are likely to have very different climates based on their composition . Keywords : General Circulation Modeling , Extrasolar Planetary Systems , Radiation Transfer , Climate , Atmosphere , Energy Transport , Convection , Cooling Rates , Day - Night Contrast",
        "rewrite_text": "Title: Atmospheric Dynamics of Short-Duration Extra-Solar Gas Giant Planets I: Dependence of Night-Side Temperature on Opacity\n\nAbstract: In this study, we present groundbreaking insights into the atmospheric evolution of short-period extra-solar gas giant planets (EGPs) through the application of three-dimensional general circulation models that incorporate radiative transfer and realistic opacity values. Our findings reveal a significant correlation between night-side temperature and atmospheric opacity, which plays a crucial role in regulating the amount of heat that can escape into space via radiation. As we reduce the opacity, we observe an increase in the temperature gradient between the day and night sides of the planet, as less energy is retained, allowing for greater heat loss from the night-side atmosphere. This effect is particularly pronounced at lower atmospheric pressures, where the efficiency of circulation diminishes, leading to more extreme temperature contrasts. In scenarios with minimal opacity, we find that the planet can cool substantially during its orbit, resulting in an exceptionally frigid night-side environment. Our simulations suggest that the climatic conditions of EGPs are likely to vary significantly based on their atmospheric composition, indicating a complex interplay between opacity, energy transport, and temperature distribution. These results contribute to our understanding of the diverse atmospheric dynamics present in extra-solar planetary systems and highlight the importance of considering opacity in future studies of exoplanet climates. \n\nKeywords: General Circulation Modeling, Extrasolar Planetary Systems, Radiation Transfer, Climate, Atmosphere, Energy Transport, Convection, Cooling Rates, Day-Night Contrast.",
        "ori-fast-z-score": -0.8427009716003844,
        "water-fast-z-score": 4.541868715470696,
        "rewrite-fast-z-score": 1.7873696499288347
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Quantum-like Representation of Macroscopic Configurations .\nAbstract:\nWe present an approach to the description of macroscopic configurations in terms of quantum states, which is based on the concept of entanglement entropy and its generalization for mixed states. We show that this representation allows one to obtain exact results for some statistical properties of systems with many degrees of freedom. In particular, we consider the problem of calculating the partition function of classical spin models at high temperatures. The proposed method can be used as a basis for constructing approximate algorithms for solving problems related to the calculation of thermodynamic characteristics of complex systems. Introduction.\nThe main goal of statistical mechanics is to describe the behavior of macroscopic objects (for example, gases) by using microscopic information about their constituents (atoms). This task becomes especially difficult when dealing with large systems consisting of many particles or spins. For such cases, it is necessary to use approximations, since direct calculations are impossible due to the exponential growth of the number of possible microstates with increasing system size N . One of these approaches is the so-called mean-field approximation  1  , according to which each particle interacts only with all other particles simultaneously; i.e., the interaction between different pairs of particles is neglected. However, even within this simplified model, the calculation of the partition function Z = Tr exp(−βH) (1) remains extremely complicated  2  .\nIn recent years, there has been growing interest in developing new methods for describing macroscopic configurations in terms similar to those used in quantum physics  3  -  8  . These studies were inspired by the fact that both classical and quantum descriptions have certain common features  9  : they are formulated in terms of wave functions ψ(x), where x denotes either positions of particles or spins, respectively. Moreover, the evolution of these wave functions obeys the same Schrödinger equation ih∂ t |ψ(t) = H|ψ(t) , where H is the corresponding Hamiltonian operator. It should also be noted that the density matrix ρ = |ψ(t) ψ(t)| plays the role of a probability distribution in both theories  10  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Quantum - like Representation of Macroscopic Configurations . Abstract : We present an view to the description of macroscopic configurations in terms of quantum states , which is based on the idea of entanglement entropy and its generalization for mixed states .We see that this representation enables one to obtain exact findings for some statistical characteristics of networks with many degrees of liberty . In particular , we investigate the question of calculating the partition function of classical spin machines at high temperatures .The proposed concept can be used as a framework for constructing approximate schemes for solving problems related to the determination of thermodynamic qualities of complex systems . Introduction .The main goal of statistical mechanics is to explain the dynamics of macroscopic objects ( for example , atoms ) by using microscopic information about their constituents ( atoms ) . This job seems particularly challenging when dealing with large systems composed of several particles or spins .For such cases , it is important to use approximations , since direct calculations are unable due to the exponential growth of the number of possible microstates with increasing system width N . One of these method is the so - called mean - field analogy 1 , according to which each particle interacts only with all other particles concurrently ; i . e . , the interaction between different pairs of atoms is neglected .However , even within this simplified theory , the determination of the partition function Z = Tr exp ( −βH ) ( 1 ) stays extremely difficult 2 . In recent years , there has been growing interest in developing novel techniques for describing macroscopic configurations in terms comparable to those utilized in particle mathematics 3 - 8 .These studies were inspired by the fact that both classical and quantum descriptions have many common features 9 : they are formulated in terms of wave functions φ ( x ) , where h indicates either positions of atoms or spins , respectively . Moreover , the evolution of these wave functions obeys the same Schrödinger equation ih∂ t | ψ ( t ) = H | ψ ( t ) , where H is the analogous Hamiltonian operator .It should additionally be mentioned that the density function ρ = | ψ ( t ) ψ ( t ) | serves the importance of a probability distribution in both theories 10 .",
        "rewrite_text": "**Title:** Quantum-like Representation of Macroscopic Configurations\n\n**Abstract:** In this article, we introduce a novel perspective on the characterization of macroscopic configurations through the lens of quantum states, leveraging the concept of entanglement entropy and its extension to mixed states. This approach allows for the derivation of precise results regarding certain statistical properties of complex networks characterized by numerous degrees of freedom. Specifically, we focus on the challenge of calculating the partition function for classical spin systems at elevated temperatures. Our framework offers a basis for developing approximate methodologies aimed at addressing the determination of thermodynamic properties in intricate systems. \n\nThe primary objective of statistical mechanics is to elucidate the behavior of macroscopic entities, such as atoms, by utilizing microscopic details about their constituents. This task becomes particularly daunting when analyzing large systems composed of multiple particles or spins, as the exponential increase in the number of potential microstates with system size (N) complicates direct calculations. To navigate this complexity, various approximation methods have been employed, one of which is the mean-field approximation. This technique simplifies interactions by assuming that each particle interacts with all others simultaneously, thereby neglecting the interactions between distinct pairs of particles. Despite this simplification, calculating the partition function, expressed as Z = Tr exp(−βH), remains a formidable challenge.\n\nRecent advancements have sparked a growing interest in innovative techniques that describe macroscopic configurations in a manner akin to particle mathematics. These investigations are motivated by the observation that classical and quantum frameworks share numerous similarities, particularly in their formulation through wave functions φ(x), where 'x' denotes either atomic positions or spin states. Furthermore, the evolution of these wave functions adheres to the same Schrödinger equation, ih∂t|ψ(t) = H|ψ(t), with H representing the corresponding Hamiltonian operator. Notably, the density function ρ = |ψ(t)|² plays a crucial role as a probability distribution in both classical and quantum theories. This article aims to bridge the gap between these two domains, enhancing our understanding of macroscopic systems through quantum-like representations.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 8.23076923076923,
        "rewrite-fast-z-score": 0.8615864949867531
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Effects of Changes in Reaction Rates on Simulations of Nova Explosions .\nAbstract:\nWe have performed simulations of nova explosions using the hydrodynamic code VH-1, which includes nuclear burning and convection. We find that changes to reaction rates can significantly affect the results of these calculations. In particular, we show how different choices for the 12C(p,γ)13N rate lead to differences in the predicted light curve shapes.  The inclusion of this reaction is important because it affects the amount of 13N produced during the explosion. This isotope decays by electron capture into 14O, which then undergoes β+ decay back down to 14N. If there are too many electrons present at late times (due to an overabundance of 13N), they will be captured onto protons instead of being emitted as positrons; thus, less energy will be released than if no such process were occurring. Our results suggest that the current uncertainty in the 12C(p , γ )13N rate may cause errors in the predicted luminosity of up to 50%.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Effects of Changes in Reaction Rates on Simulations of Nova Explosions . Abstract : We have done simulations of nova explosions using the hydrodynamic code VH - 1 , which includes nuclear combustion and convection .We see that changes to reaction rates can significantly affect the results of these calculations . In particular , we find how changed options for the 12C ( p , γ ) 13N rate lead to differences in the expected light diagram forms .The inclusion of this process is important because it affects the quantity of 13N produced during the explosion . This isotope decays by electron capture into 14O , which then undergoes β + decay back down to 14N .If there are too several electrons present at late times ( owing to an overabundance of 13N ) , they will be captured onto protons rather of being emitted as positrons ; thus , fewer electricity will be released than if no such mechanism were happening . Our results show that the present uncertainty in the 12C ( p , γ ) 13N rate may create errors in the expected luminosity of up to 50 % .",
        "rewrite_text": "Title: The Effects of Changes in Reaction Rates on Simulations of Nova Explosions\n\nAbstract: In this study, we present a series of simulations of nova explosions conducted using the hydrodynamic code VH-1, which incorporates both nuclear combustion and convection processes. Our findings reveal that variations in reaction rates can have a profound impact on the outcomes of these simulations. Specifically, we investigate the implications of altering the reaction rate for the process 12C(p, γ)13N, which leads to notable differences in the predicted light curve profiles of nova events. The significance of this reaction lies in its influence on the production of the isotope 13N during the explosive phase of a nova. Following its formation, 13N undergoes electron capture to transform into 14O, which subsequently decays via β+ decay to yield 14N. A critical aspect of our analysis is the potential for an excess of electrons to accumulate at later stages of the explosion, stemming from an overproduction of 13N. In such scenarios, these electrons may be captured by protons instead of being emitted as positrons, resulting in a reduction of the energy released during the explosion. Our simulations indicate that the current uncertainties surrounding the 12C(p, γ)13N reaction rate could lead to discrepancies in the predicted luminosity of nova explosions, with potential errors reaching as high as 50%. This highlights the necessity for improved measurements of nuclear reaction rates to enhance the accuracy of nova explosion models and their associated observational predictions. Overall, our work underscores the intricate relationship between nuclear physics and astrophysical phenomena, emphasizing the need for further research in this area to refine our understanding of nova explosions.",
        "ori-fast-z-score": -1.0533703247651751,
        "water-fast-z-score": 4.184914994777494,
        "rewrite-fast-z-score": -0.8542421961772492
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Theory of capillary-induced interactions beyond the superposition approximation .\nAbstract:\nWe present an analytical theory for describing capillary forces between two spherical particles in contact with each other and immersed into a liquid, which is valid even when the separation distance between them becomes comparable to their size. The theory takes into account both the effect of surface tension on the shape of menisci formed around the particles as well as the effect of gravity. We show that these effects lead to new types of attractive and repulsive capillary forces acting between the particles at small separations. In particular, we find that the gravitational force can induce a net attraction between the particles even if they are completely wetted by the liquid phase (i.e., have no dry patches). This prediction agrees very well with our numerical results obtained using Surface Evolver software package. Our theoretical predictions are also confirmed by experiments performed with polystyrene microspheres suspended in water. Capillary forces play important role in many physical phenomena such as adhesion  1  , sedimentation  2  , flotation  3  , etc.. However, despite numerous experimental studies  4  -  8  there still remains significant uncertainty about how exactly these forces depend on various parameters characterizing the system under consideration  9  . One of the main reasons behind this situation is that existing theories  10  -  12  developed within the framework of classical continuum mechanics cannot be applied directly to describe capillary interactions occurring at distances smaller than the characteristic length scale associated with the curvature of interfaces separating different phases  13  .\nIn order to overcome this difficulty one usually resorts to some approximate approaches based either on the concept of effective Hamaker constants  14  or on the so-called  superposition approximation   15  . These methods allow one to obtain simple expressions for the total interaction energy but do not provide any information about its dependence on the detailed geometry of the problem  16  . Moreover, it has been shown recently  17  that the latter approach fails...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Theory of capillary - caused interactions beyond the superposition approximation . Abstract : We present an analytical theory for describing capillary forces between two spherical objects in contact with each other and immersed into a liquid , which is valid even when the separation distance between them becomes comparable to their size .The theory took into consideration both the impact of surface friction on the form of menisci developed around the particles as well as the impact of gravitational . We see that these influences result to novel forms of attractive and repulsive capillary forces working between the molecules at small separations .In particular , we find that the gravitational pull can induce a net attraction between the molecules even if they are completely wetted by the liquid phase ( i . e . , have no dry patches ) . This prediction agrees very best with our numerical findings obtained using Surface Evolver program package .Our conceptual predictions are also verified by research performed with polystyrene microspheres held in water . Capillary forces play important role in different mechanical phenomena such as adhesion 1 , sedimentation 2 , flotation 3 , etc . .However , despite several experimental studies 4 - 8 there still remains significant doubt about how exactly these forces depend on various variables characterizing the system under consideration 9 . One of the main motives behind this situation is that current theories 10 - 12 developed within the framework of classical continuum dynamics cannot be applied directly to explain capillary interactions observed at distances smaller than the typical length range identified with the curvature of interfaces separating different phases 13 .In try to overcome this obstacle one usually resorts to some approximate approaches based either on the idea of effective Hamaker constants 14 or on the so - called superposition approximation 15 . These methods provide one to obtain simple definitions for the total interaction power but do not offer any knowledge about its dependence on the detailed geometry of the question 16 .Moreover , it has been shown recently 17 that the latter methodology fails . . .",
        "rewrite_text": "**Title:** Theory of Capillary-Induced Interactions Beyond the Superposition Approximation\n\n**Abstract:** In this study, we introduce a comprehensive analytical framework for understanding the capillary forces acting between two spherical objects that are in contact and submerged in a liquid. This theory is particularly relevant when the distance separating the spheres approaches their dimensions. Our approach incorporates the effects of surface friction on the shape of the menisci formed around the particles, as well as the influence of gravitational forces. Notably, we demonstrate that these factors lead to the emergence of unique attractive and repulsive capillary forces at small separations. A key finding of our research is that gravitational attraction can create a net force pulling the molecules together, even when they are fully wetted by the liquid (i.e., devoid of dry areas). This theoretical prediction aligns closely with numerical simulations conducted using the Surface Evolver software. Additionally, our conceptual insights are corroborated by experimental investigations involving polystyrene microspheres suspended in water. Capillary forces are crucial in various mechanical processes, including adhesion, sedimentation, and flotation. However, despite numerous experimental efforts, there remains considerable uncertainty regarding the precise dependence of these forces on the system's defining parameters. This ambiguity largely stems from the limitations of existing theories rooted in classical continuum mechanics, which are inadequate for explaining capillary interactions at scales smaller than the characteristic lengths associated with interfacial curvature. To address this challenge, researchers have often relied on approximate methods, such as effective Hamaker constants or the superposition approximation. While these techniques yield simplified expressions for total interaction forces, they fall short in providing insights into how these forces vary with the intricate geometries involved. Recent findings have further highlighted the shortcomings of these conventional approaches, underscoring the need for a more robust theoretical framework to accurately capture the complexities of capillary interactions.",
        "ori-fast-z-score": -0.5551361100027009,
        "water-fast-z-score": 8.328146533974893,
        "rewrite-fast-z-score": 0.5
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Complementarity in the Bohr-Einstein Photon Box .\nAbstract:\nWe present an experimental investigation into complementarity between position and momentum measurements on single photons using a modified version of the original Einstein-Bohr photon box experiment.  The results show that, for this particular measurement scheme, there is no violation of Bell s inequality or any other form of nonlocality. We also demonstrate how our setup can be used to investigate quantum contextuality by performing two different experiments with identical settings but opposite outcomes. In one case we observe violations of Bell inequalities while in the other they are not violated. This shows that the observed behavior cannot be explained within classical physics and demonstrates quantum contextuality. Quantum mechanics predicts that certain physical quantities such as position and momentum do not have simultaneous well-defined values. Instead these quantities exist only as probability distributions which evolve continuously over time according to Schrödinger s equation. However, it has been shown that if both position and momentum were measured simultaneously then their respective probabilities would interfere destructively resulting in a zero probability of measuring either quantity at its most probable value  1  . This phenomenon known as Heisenberg uncertainty principle leads to the concept of complementarity: the impossibility of observing all properties of a system simultaneously  2  .\nIn 1964 John Bell showed that local hidden variable theories could not explain some predictions made by quantum mechanics  3  , leading to the formulation of Bell s theorem  4  . Since then many experiments have been performed to test whether quantum mechanical predictions violate Bell s theorem  5  . These tests typically involve entangled particles  6  where each particle carries information about the state of another distant particle  7, 8  . If the particles are separated far enough so that they never interact again after being created, then the correlations between them must be due solely to quantum effects  9  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Complementarity in the Bohr - Einstein Photon Box . Abstract : We present an experimental inquiry into complementarity between position and momentum estimates on single photons using a altered version of the original Einstein - Bohr photon box observation .The results show that , for this special measurement scheme , there is no violation of Bell s inequality or any other form of nonlocality . We additionally prove how our setup can be used to examine quantum contextuality by performing two different trials with identical settings but different outcomes .In one instance we exhibit abuses of Bell inequalities while in the other they are not enforced . This shows that the seen behavior cannot be described within classical physics and demonstrates quantum contextuality .Quantum theory predicts that particular physical quantities such as position and momentum do not have simultaneous well - defined parameters . Instead these quantities occur only as probability distributions which evolution continuously over time according to Schrödinger s equation .However , it has been shown that if both position and momentum were calculated jointly then their different probabilities would interfere destructively resulting in a zero probability of measuring either quantity at its most likely value 1 . This phenomenon known as Heisenberg uncertainty theory gives to the notion of complementarity : the impossibility of observing all characteristics of a system simultaneously 2 .In 1964 John Bell demonstrated that local hidden variable theories cannot not understand some observations made by quantum mechanics 3 , leading to the realization of Bell s theorem 4 . Since then many tests have been performed to test whether quantum mechanical predictions violate Bell s theorem 5 .These studies typically involve entangled particles 6 where each particle contains information about the state of another distant particle 7 , 8 . If the particles are apart farther enough so that they cannot interact again after being created , then the correlations between them need be due exclusively to quantum effects 9 .",
        "rewrite_text": "We present an experimental investigation into the concept of complementarity as it pertains to position and momentum measurements of single photons, utilizing a modified version of the original Einstein-Bohr photon box experiment. Our findings indicate that, within the framework of this specific measurement approach, there is no violation of Bell's inequality or any indication of nonlocality. Furthermore, we demonstrate how our experimental setup can be employed to explore quantum contextuality by conducting two distinct trials under identical conditions but yielding different results. In one trial, we observe violations of Bell inequalities, while in the other, such violations are absent. This disparity highlights that the observed phenomena cannot be adequately explained by classical physics, thereby affirming the presence of quantum contextuality.\n\nQuantum mechanics posits that certain physical properties, such as position and momentum, cannot be precisely defined simultaneously. Instead, these properties are represented as probability distributions that evolve over time in accordance with Schrödinger's equation. Notably, it has been established that if position and momentum are jointly measured, their respective probabilities may interfere destructively, leading to a zero probability of accurately measuring either quantity at its most probable value. This principle, known as the Heisenberg uncertainty principle, underpins the notion of complementarity, which asserts the impossibility of simultaneously observing all characteristics of a quantum system.\n\nIn 1964, John Bell introduced a theorem demonstrating that local hidden variable theories are insufficient to account for certain quantum mechanical observations, paving the way for extensive testing of Bell's theorem. Numerous experiments have since been conducted to assess whether quantum predictions contravene Bell's theorem, typically involving entangled particles. In these scenarios, each particle carries information about the state of another, even when separated by significant distances, ensuring that any correlations observed are attributable solely to quantum effects. Our research contributes to this ongoing discourse by providing new insights into the interplay between complementarity, contextuality, and the foundational principles of quantum mechanics.",
        "ori-fast-z-score": -0.5853694070049635,
        "water-fast-z-score": 7.5,
        "rewrite-fast-z-score": -2.0619652471058063
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Ray-tracing and physical-optics analysis of the aperture efficiency in a radio telescope .\nAbstract:\nWe present an analytical model for calculating the aperture efficiency of a reflector antenna with circularly polarized feeds, based on ray tracing through the feed-horn optics and physical optics (PO) at the aperture plane. The PO method is used to calculate the electric field distribution over the aperture surface by solving Maxwell s equations using Green functions. We show that this approach can be applied to any type of feed horns, including corrugated conical horns or dual-polarized log-periodic dipole arrays. This work was motivated by our recent study of aperture efficiencies of two different types of antennas operating at 1.4 GHz: a single-dish radio telescope equipped with a corrugated conical feed horn and a multi-element interferometer consisting of eight log-periodic dipole array elements. In both cases we found good agreement between numerical results obtained with our new model and experimental data. \n \n Keywords: Antenna design",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Ray - tracing and physical - optics analysis of the aperture efficiency in a radio antenna . Abstract : We present an analytical model for determining the aperture efficiency of a reflector antenna with circularly polarized feeds , based on ray tracing through the feed - horn optics and physical optics ( PO ) at the lens plane .The PO technique is utilized to estimate the electric field flow over the lens surface by solving Maxwell s equations using Green functions . We see that this methodology can be applied to any type of feed horns , particularly corrugated conical horns or dual - polarized log - periodic dipole arrays .This research was motivated by our latest research of aperture efficiencies of two different kinds of antennas acting at 1 . 4 GHz : a single - dish radio telescope fitted with a corrugated conical feed horn and a multi - component interferometer composed of eight log - periodic dipole array modules . In both cases we concluded excellent agreement between mathematical findings obtained with our new model and theoretical data .Keywords: Antenna design",
        "rewrite_text": "Title: Ray-Tracing and Physical-Optics Analysis of Aperture Efficiency in a Radio Antenna\n\nAbstract: This study introduces a comprehensive analytical model aimed at evaluating the aperture efficiency of reflector antennas equipped with circularly polarized feeds. The model employs ray tracing techniques to analyze the feed-horn optics, alongside physical optics (PO) methods applied at the lens plane. By utilizing the PO approach, we effectively estimate the electric field distribution across the lens surface, achieved through the resolution of Maxwell's equations via Green's functions. Our findings indicate that this methodology is versatile and can be adapted for various types of feed horns, including but not limited to corrugated conical horns and dual-polarized log-periodic dipole arrays. The impetus for this research stemmed from our recent investigations into the aperture efficiencies of two distinct antenna types operating at a frequency of 1.4 GHz: a single-dish radio telescope equipped with a corrugated conical feed horn, and a multi-component interferometer consisting of eight log-periodic dipole array modules. In both scenarios, we observed a remarkable correlation between the mathematical results derived from our innovative model and the theoretical data available in the literature. This work not only enhances our understanding of antenna design but also provides a robust framework for future studies in the field of radio astronomy and antenna engineering. \n\nKeywords: Antenna design, aperture efficiency, ray tracing, physical optics, radio astronomy.",
        "ori-fast-z-score": -1.5096588248481377,
        "water-fast-z-score": 3.7523938719322816,
        "rewrite-fast-z-score": 0.6644105970267493
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Light-Cone Distribution Amplitudes of Axial-vector Mesons .\nAbstract:\nWe present the light-cone distribution amplitudes (DAs) for axial vector mesons in terms of their helicity components, which are determined by solving the Bethe-Salpeter equation with an instantaneous interaction kernel and applying the method developed recently to calculate DAs.  We find that the twist-2 DA is dominated by its first Gegenbauer moment, while higher moments contribute significantly only at large momentum fractions x > 0.7. The twist-3 DA has two independent functions, one of them being proportional to the second Gegenbauer moment. Our results show that the twist-4 contribution is negligible compared to those of lower twists. These findings will be useful for studying exclusive processes involving axial vector mesons such as B-decays into charmonium plus photon or pion pair. \nI. INTRODUCTIO N\nThe study of hadronic structure plays an important role in understanding strong interactions between quarks and gluons inside hadrons. In particular, the investigation on the parton distributions provides us valuable information about how quarks and gluon are distributed within hadrons  1  . Recently, there have been great interests in exploring the internal structures of hadrons beyond the leading-twist level  2  , especially the transverse-momentum dependent parton distributions  3  .\nIn this work we focus our attention on another type of nonperturbative objects -the light-cone distribution amplitudes(DAs). They describe the probability amplitude of finding a quark-antiquark pair with certain longitudinal momentum fraction and transverse separation at some fixed light-like distance  4  . It was shown that they play crucial roles in describing various hard exclusive reactions  5  . For example, the decay constants fBπ and fBs can be expressed in terms of the lowest-order DAs  6  ; the form factors of semileptonic decays B→πlν l and B→Klν l depend on both the lowest-and next-to-lowest order DAs  7, 8  . Furthermore, it was found that the heavy-to-light transition form factor FV(q 2 ) of B→V transitions depends",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Light - Cone Distribution Amplitudes of Axial - vector Mesons . Abstract : We present the light - cone distribution amplitudes ( DAs ) for axial vector mesons in terms of their helicity components , which are chosen by solving the Bethe - Salpeter equation with an instantaneous interaction kernel and using the method developed lately to estimate DAs .We see that the twist - 2 DA is dominated by its initial Gegenbauer moment , while greater moments contribute considerably only at large velocity fractions x > 0 . 7 . The twist - 3 DA has two independent functions , one of them being equal to the second Gegenbauer moment .Our results show that the twist - 4 impact is negligible compared to those of lower bends . These conclusions will be valuable for studying exclusive mechanisms using axial vector mesons such as B - decays into charmonium plus photon or pion pair .I . INTRODUCTIO N The investigation of hadronic formation serves an important role in understanding strong interactions between quarks and gluons inside hadrons . In particular , the investigation on the parton distributions offers us valuable info about how quarks and gluon are distributed within hadrons 1 .Recently , there have been much interests in investigating the internal structures of hadrons beyond the led - twist level 2 , particularly the transverse - momentum dependent parton distributions 3 . In this research we focus our focus on another type of nonperturbative objects - the light - cone distribution amplitudes ( DAs ) .They define the probability intensity of finding a quark - antiquark pair with certain horizontal momentum fraction and longitudinal separation at some fixed light - like distance 4 . It was shown that they serve vital part in describing several hard exclusive effects 5 .For instance , the decay constants fBπ and fBs can be written in terms of the lowest - order DAs 6 ; the form factors of semileptonic decays B→πlν l and B→Klν l depend on both the highest - and last - to - lowest order DAs 7 , 8 . Furthermore , it was shown that the heavy - to - light shift form parameter FV ( q 2 ) of B→V transitions depends",
        "rewrite_text": "We provide a comprehensive analysis of the light-cone distribution amplitudes (DAs) for axial vector mesons, focusing on their helicity components. This study employs the Bethe-Salpeter equation with an instantaneous interaction kernel, alongside a recently developed method for estimating DAs. Our findings indicate that the twist-2 DA is primarily influenced by its leading Gegenbauer moment, while higher-order moments become significant only at large velocity fractions (x > 0.7). Additionally, the twist-3 DA comprises two independent functions, one of which corresponds to the second Gegenbauer moment. Notably, our results reveal that the impact of the twist-4 DA is minimal when compared to the contributions from lower twists. These insights are crucial for understanding exclusive processes involving axial vector mesons, such as B-decays into charmonium and photon or pion pairs.\n\nIn the introduction, we emphasize the importance of studying hadronic formation to gain a deeper understanding of the strong interactions between quarks and gluons within hadrons. The exploration of parton distributions is particularly valuable as it sheds light on the spatial distribution of quarks and gluons. Recent research has increasingly focused on the internal structures of hadrons beyond the leading-twist level, especially the transverse-momentum dependent parton distributions. In this context, our investigation centers on light-cone distribution amplitudes, which characterize the probability density of locating a quark-antiquark pair with specific transverse momentum fractions and longitudinal separations at a fixed light-like distance. These DAs play a critical role in describing various hard exclusive processes. For example, the decay constants fBπ and fBs can be expressed in terms of the lowest-order DAs, while the form factors for semileptonic decays such as B→πlνl and B→Klνl are dependent on both the highest and lowest-order DAs. Furthermore, the heavy-to-light transition form factor FV(q²) for B→V transitions is also influenced by these distribution amplitudes.",
        "ori-fast-z-score": 1.01418510567422,
        "water-fast-z-score": 8.223977311307554,
        "rewrite-fast-z-score": 1.1358152736593492
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Settling the Complexity of Computing Two-Player Nash Equilibria .\nAbstract:\nWe present an algorithm for computing two-person zero-sum games with imperfect information, which is based on solving linear programs (LPs) and mixed integer programming problems (MIPs). The proposed approach can be applied to any game that has been formulated as a Mixed Integer Linear Program (MILP), including many real-world applications such as energy markets or traffic control systems. We show how our method outperforms state-of-the-art algorithms in terms of computational efficiency by applying it to several test cases. In addition, we provide insights into the structure of the problem instances solved by our algorithm. Finally, we discuss possible extensions of this work. Games are mathematical models used to describe situations where multiple decision makers interact strategically. A key concept in game theory is equilibrium, i.e., a set of strategies that no player wants to deviate from if all other players stick to their strategy. One important class of equilibria are so-called Nash equilibria, which have received considerable attention over the past decades due to their wide applicability across various fields ranging from economics to computer science. However, finding Nash equilibria is known to be computationally hard even for simple classes of games.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Settling the Complexity of Computing Two - Player Nash Equilibria . Abstract : We present an algorithm for modeling two - person zero - sum games with imperfect data , which is based on solving linear programs ( LPs ) and combined integer programming questions ( MIPs ) .The proposed approach can be applied to any game that has been formulated as a Mixed Integer Linear Program ( MILP ) , including much actual - time users such as energy markets or road control networks . We see how our technique outperforms state - of - the - art algorithms in terms of computational efficiency by using it to several test cases .In addition , we provide insights into the formation of the question instances solved by our algorithm . Finally , we study possible extensions of this study .Games are mathematical models used to define situations where multiple decision producers react strategically . A crucial idea in play theoretical is equilibrium , i . e . , a setting of actions that no player wants to deviate from if all other players stick to their strategy .One important group of equilibria are so - called Nash equilibria , which have received considerable scrutiny over the previous decades owing to their wide applicability across numerous topics including from economics to computer science . However , finding Nash equilibria is known to be computationally hard even for simple groups of games .",
        "rewrite_text": "We introduce a novel algorithm designed for modeling two-player zero-sum games characterized by imperfect information, utilizing techniques from linear programming (LP) and mixed integer programming (MIP). This innovative approach is applicable to any game that can be represented as a Mixed Integer Linear Program (MILP), making it particularly relevant for real-time applications such as energy markets and traffic control systems. Our findings demonstrate that this method significantly enhances computational efficiency compared to existing state-of-the-art algorithms, as evidenced by its performance across various test cases. Furthermore, we delve into the intricacies of the problem instances addressed by our algorithm, providing valuable insights into their structure and characteristics. The study also explores potential avenues for extending this research, highlighting the algorithm's versatility and adaptability.\n\nGames serve as mathematical frameworks for analyzing scenarios where multiple decision-makers engage in strategic interactions. A fundamental concept in game theory is that of equilibrium, which refers to a situation where no player has an incentive to unilaterally change their strategy, assuming all other players maintain their current strategies. Among the various types of equilibria, Nash equilibria have garnered significant attention over the past few decades due to their broad applicability across diverse fields, including economics and computer science. Despite their importance, the computation of Nash equilibria is notoriously challenging, even in relatively simple game structures. Our work aims to address this complexity by providing a robust algorithm that can effectively tackle the intricacies of two-player Nash equilibria in the presence of imperfect data.",
        "ori-fast-z-score": 0.2847473987257497,
        "water-fast-z-score": 6.114685644493382,
        "rewrite-fast-z-score": -1.3310347641241707
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Vacant Set of Random Interlacements and Percolation .\nAbstract:\nWe study the vacant set of random interlacements in Rd, d>1, which is defined as the complement of the union of all open paths starting at 0 up to time 1. We prove that it has Hausdorff dimension equal to d-1 almost surely by showing that its upper Minkowski dimension equals this value with probability one. This result extends previous results on the vacant set of simple random walk obtained by Lawler, Schramm and Werner (Annals of Probability 24(3), 1997) and Benjamini, Kalai and Schramm (Journal of Functional Analysis 157(1), 2000). The proof relies heavily on recent developments concerning the geometry of Brownian motion and the theory of stable processes. In particular we use an estimate for the Green function of the killed Brownian motion due to Bass and Burdzy (Annales de l Institut Henri Poincare 22(2), 1999).\nThe main motivation behind our work comes from percolation theory where the vacant set of random walks plays a crucial role. Indeed, we show how our techniques can be used to obtain new information about the critical behavior of bond-percolation models on Zd.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Vacant Set of Random Interlacements and Percolation . Abstract : We consider the empty set of random interlacements in Rd , d > 1 , which is defined as the complement of the union of all open routes starting at 0 up to time 1 .We establish that it has Hausdorff dimension equal to d - 1 fairly surely by showing that its upper Minkowski dimension equals this value with probability one . This result continues preceding results on the vacant set of simple random walk obtained by Lawler , Schramm and Werner ( Annals of Probability 24 ( 3 ) , 1997 ) and Benjamini , Kalai and Schramm ( Journal of Functional Analysis 157 ( 1 ) , 2000 ) .The proof draws largely on current developments concerning the topology of Brownian movement and the notion of stable processes . In particular we using an estimate for the Green function of the dead Brownian movement owing to Bass and Burdzy ( Annales de l Institut Henri Poincare 22 ( 2 ) , 1999 ) .The main motivation behind our work comes from percolation theory where the empty set of random walks holds a crucial role . Indeed , we prove how our approaches can be used to obtain new information about the important dynamics of bond - percolation models on Zd .",
        "rewrite_text": "We investigate the vacant set of random interlacements in \\( \\mathbb{R}^d \\) for dimensions \\( d > 1 \\), defined as the complement of the union of all open paths originating from the point 0 up to time 1. Our primary finding is that this vacant set almost surely possesses a Hausdorff dimension of \\( d - 1 \\). We achieve this by demonstrating that its upper Minkowski dimension is equal to this value with probability one. This result builds upon earlier work regarding the vacant set of simple random walks, as established by Lawler, Schramm, and Werner in their 1997 paper in the Annals of Probability, as well as by Benjamini, Kalai, and Schramm in their 2000 publication in the Journal of Functional Analysis. \n\nThe proof leverages recent advancements in the topology of Brownian motion and the concept of stable processes. A significant component of our methodology involves utilizing an estimate for the Green function of the dead Brownian motion, as presented by Bass and Burdzy in their 1999 work in the Annales de l'Institut Henri Poincaré. The impetus for our research is rooted in percolation theory, where the vacant set of random walks plays a pivotal role. We further demonstrate how our findings can yield new insights into the dynamics of bond-percolation models on \\( \\mathbb{Z}^d \\). This connection not only enhances our understanding of the geometric properties of random interlacements but also contributes to the broader discourse on percolation phenomena, highlighting the intricate relationship between random walks and percolation theory.",
        "ori-fast-z-score": -1.4444444444444444,
        "water-fast-z-score": 4.111111111111111,
        "rewrite-fast-z-score": -0.7107423155935334
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The design of a very high-Q superconductor electromechanical clock .\nAbstract:\nWe present the design and performance of an extremely low-noise, high-accuracy cryogenic mechanical oscillator based on a suspended silicon nitride membrane with integrated Josephson junctions. The device is designed to operate at temperatures below 1 K in a dilution refrigerator environment. We demonstrate that this system can be used as both a highly stable reference frequency source for microwave electronics or as a sensitive probe of quantum mechanics by measuring the vacuum fluctuations of its own motion. \n \n A key requirement for many applications of quantum information science is the ability to generate and detect single photons. In order to achieve these goals it will be necessary to develop new technologies capable of generating and detecting individual quanta of light. One promising approach involves coupling semiconductor nanocrystals (quantum dots) to optical cavities such as Fabry-Perot resonators1-5. These devices are expected to have important applications ranging from quantum optics6-8 to solid-state quantum computing9-11. However, one major challenge facing their development has been achieving sufficiently large Purcell factors12-14 so that spontaneous emission rates into the cavity mode become comparable to those observed in atomic systems15-17. This problem may be overcome using photonic crystal cavities18-20 which allow for strong confinement of electromagnetic fields within small volumes21-23.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The construction of a very high - Q superconductor electromechanical clock . Abstract : We present the development and performance of an incredibly small - noise , large - accuracy cryogenic mechanical oscillator based on a suspended silicon nitride cell with integrated Josephson junctions .The system is designed to run at pressures below 1 K in a dilution fridge climate . We indicate that this device can be used as both a highly stable reference wavelength source for microwave electronics or as a sensitive probe of quantum mechanics by monitoring the vacuum fluctuations of its own movement .A key requirement for numerous uses of quantum information research is the ability to produce and locate single photons . In order to achieve these objectives it will be required to develop new inventions capable of collecting and detecting individual quanta of light .One promising solution involves coupling semiconductor nanocrystals ( quantum dots ) to optical cavities such as Fabry - Perot resonators1 - 5 . These systems are expected to have important use ranging from quantum optics6 - 8 to liquid - state quantum computing9 - 11 .However , one major challenge facing their development has been achieving sufficiently huge Purcell factors12 - 14 so that spontaneous emission rates into the cavity mode become similar to those observed in atomic systems15 - 17 . This problem could be overcome using photonic crystal cavities18 - 20 which allow for strong confinement of electromagnetic fields within tiny volumes21 - 23 .",
        "rewrite_text": "We present the development and evaluation of an exceptionally low-noise, high-precision cryogenic mechanical oscillator, which is based on a suspended silicon nitride cell integrated with Josephson junctions. This innovative system is engineered to operate at temperatures below 1 K within a dilution refrigerator environment. Our findings suggest that this device can serve dual purposes: it can function as a highly stable reference wavelength source for microwave electronics and as a sensitive probe for quantum mechanics by observing the vacuum fluctuations of its own motion. \n\nA critical requirement for advancing quantum information research is the capability to generate and manipulate single photons. To meet these needs, it is essential to innovate technologies that can efficiently collect and detect individual quanta of light. One promising approach involves the integration of semiconductor nanocrystals, commonly known as quantum dots, with optical cavities such as Fabry-Perot resonators. These hybrid systems are anticipated to play a significant role in various applications, ranging from quantum optics to liquid-state quantum computing.\n\nHowever, a significant hurdle in the development of these systems has been the challenge of achieving sufficiently high Purcell factors, which are necessary for spontaneous emission rates into the cavity mode to approach those observed in atomic systems. This challenge may be addressed through the use of photonic crystal cavities, which enable strong confinement of electromagnetic fields within minuscule volumes. By overcoming these obstacles, we can enhance the performance of quantum devices and expand their applicability in the field of quantum information science.",
        "ori-fast-z-score": 0.3779644730092272,
        "water-fast-z-score": 6.743417843388197,
        "rewrite-fast-z-score": 1.2572371141874243
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Magnetically-driven explosions of rapidly-rotating white dwarfs following Accretion-Induced Collapse .\nAbstract:\nWe present the results of our numerical simulations of magnetized, rotating white dwarf stars that undergo accretion-induced collapse (AIC). We find that for sufficiently rapid rotation and strong magnetic fields, AIC leads to an explosion with properties similar to those observed in SNe Ia. The explosion is driven by the release of gravitational energy as the collapsed core bounces back after reaching nuclear density. In addition, we show that the presence of a strong toroidal field can lead to significant asymmetries in the ejecta distribution. These asymmetries are likely responsible for the polarization signal detected in some SNe Ia. \n \n Keywords: Supernovae Type Ia, Rotation, Magnetic Fields, White Dwarf Stars, Accretion Induced Collapse \n \n 1 Introduction \n \n Recent observations have shown that many supernovae type Ia (SNe Ia) exhibit large amounts of linear polarization  1  . This has been interpreted as evidence that these events result from asymmetric explosions  2  , which may be caused by large-scale magnetic fields  3  or rapid rotation  4  . However, it remains unclear whether either mechanism alone could produce such highly polarized light curves  5  . \n \n Here we investigate how the combination of rapid rotation and strong magnetic field affects the outcome of accretion induced collapse (AIC), where a white dwarf star collapses into a neutron star  6  . For this purpose, we perform two-dimensional axisymmetric hydrodynamic simulations using the code FLASH  7  . Our initial models consist of rigidly-rotating white dwarf stars with masses ranging between 0.6-1.2 Msun  8  . To account for the effects of general relativity on the structure of the white dwarf  9  , we use the polytropic equation of state P = Kρ Γ , where ρ denotes the mass density and P the pressure  10  . \nThe main goal of this work is to determine if AICs triggered by rapid rotation and/or strong magnetic fields can explain the high degree of polarization observed in SNe Ia  11  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Magnetically - fueled bursts of quickly - spinning white dwarfs following Accretion - Induced Collapse . Abstract : We present the results of our numerical simulations of magnetized , rotating white dwarf stars that suffer accretion - caused instability ( AIC ) .We see that for enough fast rotation and strong magnetic fields , AIC leads to an explosion with properties similar to those observed in SNe Ia . The explosion is powered by the release of gravitational energy as the collapsed center bounces away after reaching nuclear density .In addition , we find that the presence of a powerful toroidal field can lead to significant asymmetries in the ejecta distribution . These asymmetries are likely responsible for the polarization wave observed in some SNe Ia .Keywords : Supernovae Type Ia , Rotation , Magnetic Fields , White Dwarf Stars , Accretion Induced Collapse 1 Introduction Recent measurements have shown that several supernovae class Ia ( SNe Ia ) exhibit large quantities of linear polarization 1 . This has been viewed as proof that these events result from asymmetric explosions 2 , which may be caused by large - scale magnetic waves 3 or rapid rotation 4 .However , it remains unsure whether either mechanism alone might generate such heavily polarized light curves 5 . Here we investigate how the combination of rapid rotation and strong magnetic force influence the result of accretion induced collapse ( AIC ) , where a white dwarf star collapses into a neutron star 6 .For this use , we perform two - dimensional axisymmetric hydrodynamic simulations using the code FLASH 7 . Our preliminary estimates consist of rigidly - spinning white dwarf stars with masses ranging between 0 . 6 - 1 . 2 Msun 8 .To account for the effects of general relativity on the composition of the white dwarf 9 , we utilize the polytropic equation of state P = Kρ Γ , where ρ indicates the mass density and P the pressure 10 . The main goal of this research is to find if AICs triggered by rapid rotation and / or strong magnetic fields can describe the high degree of polarization observed in SNe Ia 11 .",
        "rewrite_text": "**Title:** Magnetically-Fueled Bursts of Rapidly Spinning White Dwarfs Following Accretion-Induced Collapse\n\n**Abstract:** In this study, we present the findings from our numerical simulations of magnetized, rapidly rotating white dwarf stars that undergo accretion-induced collapse (AIC). Our results indicate that when these stars possess sufficiently high rotation rates and strong magnetic fields, the AIC can result in explosive events that share characteristics with those observed in Type Ia supernovae (SNe Ia). The explosions are primarily driven by the gravitational energy released as the core rebounds after achieving nuclear density. Furthermore, we observe that the presence of a robust toroidal magnetic field can create notable asymmetries in the distribution of the ejected material. These asymmetries may account for the polarization waves detected in certain SNe Ia, suggesting a link between magnetic fields and the observed polarization phenomena. \n\nRecent observations have revealed that several SNe Ia exhibit significant linear polarization, which supports the hypothesis that these supernovae arise from asymmetric explosions. Potential causes for such asymmetries include large-scale magnetic fields and rapid rotation, although it remains uncertain whether either mechanism alone can fully explain the pronounced polarization seen in the light curves of these events. This research aims to explore the combined effects of rapid rotation and strong magnetic forces on the dynamics of AIC, wherein a white dwarf collapses into a neutron star. To achieve this, we employ two-dimensional axisymmetric hydrodynamic simulations using the FLASH code, focusing on rigidly spinning white dwarfs with masses ranging from 0.6 to 1.2 solar masses. To incorporate the effects of general relativity on the white dwarf's composition, we utilize a polytropic equation of state, where pressure is related to mass density. The primary objective of our investigation is to determine whether AIC events, influenced by rapid rotation and/or strong magnetic fields, can adequately explain the high levels of polarization observed in SNe Ia.",
        "ori-fast-z-score": 0.4931969619160719,
        "water-fast-z-score": 7.021870595978444,
        "rewrite-fast-z-score": 0.8111071056538127
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Evolution of Dust in Primordial Supernova Remnants: Can Dust Grains Formed in the Ejecta Survive and be Injected into the Early Interstellar Medium? .\nAbstract:\nWe present results on dust formation in primordial supernova remnants (SNRs) using three-dimensional hydrodynamic simulations with detailed chemical networks for gas-phase species, grains, and molecules. We find that dust can form efficiently in SNR ejecta at high temperatures (T > 1000 K), but it is destroyed by sputtering due to collisions between ions and electrons when the temperature drops below T = 100 K. The surviving dust grains are injected into the interstellar medium (ISM). Our calculations show that the total mass of dust formed in primordial SNRs may reach up to 10^-4 Msun if we assume an initial metallicity Z = 0.1Zsun. This value agrees well with observations of nearby young SNRs. However, our model predicts too much carbonaceous dust compared to silicates observed in these objects. If this discrepancy persists after further improvements of the models, then some other mechanism should be responsible for producing silicates in SNRs. \n \n Keywords: dust, supernova remnant",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Evolution of Dust in Primordial Supernova Remnants : Can Dust Grains Formed in the Ejecta Survive and be Injected into the Early Interstellar Medium ? .Abstract : We present results on smoke form in primordial supernova remnants ( SNRs ) using three - dimensional hydrodynamic simulations with complete biological networks for gas - phase organisms , grains , and atoms . We see that dust can form smoothly in SNR ejecta at high temperatures ( T > 1000 K ) , but it is destroyed by sputtering due to collisions between electrons and electrons when the temperature rises below T = 100 K . The remaining dust grains are pumped into the interstellar medium ( ISM ) .Our calculations show that the total mass of dust formed in primordial SNRs might reach up to 10 ^ - 4 Msun if we suppose an initial metallicity Z = 0 . 1Zsun . This value agrees well with observations of distant new SNRs .However , our model predicts too much carbonaceous dust relative to silicates observed in these objects . If this discrepancy persists after further changes of the models , then some other mechanism should be responsible for producing silicates in SNRs .Keywords : dust , supernova remnant",
        "rewrite_text": "**Title:** Evolution of Dust in Primordial Supernova Remnants: Can Dust Grains Formed in the Ejecta Survive and be Injected into the Early Interstellar Medium?\n\n**Abstract:** In this study, we investigate the formation and survival of dust within primordial supernova remnants (SNRs) through advanced three-dimensional hydrodynamic simulations that incorporate comprehensive biological networks for gas-phase entities, grains, and atoms. Our findings indicate that dust can form effectively in the ejecta of SNRs at elevated temperatures (T > 1000 K). However, as the temperature decreases below T = 100 K, dust grains are subjected to destruction via sputtering caused by electron collisions. The surviving dust grains are subsequently ejected into the interstellar medium (ISM). Our simulations suggest that the total mass of dust generated in primordial SNRs could reach approximately 10^-4 Msun, assuming an initial metallicity of Z = 0.1 Zsun. This estimate aligns well with observational data from distant new SNRs. Nonetheless, our model reveals an overproduction of carbonaceous dust in comparison to the silicate dust observed in these remnants. Should this discrepancy remain after further model refinements, it may indicate the necessity for alternative mechanisms to account for the formation of silicates in SNRs. This research contributes to our understanding of dust evolution in the early universe and its implications for the chemical enrichment of the ISM, highlighting the complex interplay between supernova activity and the lifecycle of cosmic dust. \n\n**Keywords:** dust, supernova remnant",
        "ori-fast-z-score": 0.5933908290969266,
        "water-fast-z-score": 4.798687035041354,
        "rewrite-fast-z-score": 1.2649110640673518
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Retired A Stars and Their Companions: Exoplanets Orbiting Three Intermediate-Mass Subgiants .\nAbstract:\nWe report the discovery of three planets orbiting an intermediate-mass subgiant star, HD 168625 (M = 1.5 Msun), using high-precision radial velocity measurements obtained with the HARPS spectrograph at La Silla Observatory in Chile. The planet masses are 0.7 MJup , 2.1 MJup , and 3.2 MJup . We find that all three planets have orbital periods shorter than 100 days; two of them are close to their host stars (P orb < 10 d). All three planets are located within the habitable zone around their host star. This is one of only four systems known so far where more than one planet has been found inside this region.  These results demonstrate that it should be possible to detect terrestrial planets in the habitable zones of evolved stars by means of Doppler spectroscopy. Keywords: exoplanet, planetary system",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Retired A Stars and Their Companions : Exoplanets Orbiting Three Intermediate - Mass Subgiants . Abstract : We report the discovery of three planets orbiting an intermediate - mass subgiant star , HD 168625 ( M = 1 . 5 Msun ) , using high - precision radial speed measurements obtained with the HARPS spectrograph at La Silla Observatory in Chile .The planet masses are 0 . 7 MJup , 2 . 1 MJup , and 3 . 2 MJup . We see that all three planets have orbital periods shorter than 100 months ; two of them are close to their host stars ( P orb < 10 d ) .All three planets are situated within the habitable zone around their home star . This is one of only four systems known so far where more than one planet has been seen inside this area .These data demonstrate that it should be possible to identify terrestrial planets in the habitable zones of evolved planets by means of Doppler spectroscopy . Keywords : exoplanet , planetary system",
        "rewrite_text": "Title: Retired A Stars and Their Companions: Exoplanets Orbiting Three Intermediate-Mass Subgiants\n\nAbstract: In this study, we present the discovery of three exoplanets orbiting the intermediate-mass subgiant star HD 168625, which has a mass of approximately 1.5 solar masses. Utilizing high-precision radial velocity measurements obtained from the HARPS spectrograph at La Silla Observatory in Chile, we have characterized the masses of these planets as 0.7 MJup, 2.1 MJup, and 3.2 MJup. Notably, all three planets exhibit orbital periods of less than 100 days, with two of them having particularly short periods of less than 10 days, indicating their proximity to the host star. Importantly, these planets are located within the habitable zone of HD 168625, a region where conditions may be suitable for the existence of liquid water. This discovery is significant as it adds to the growing body of knowledge regarding planetary systems around evolved stars; specifically, it marks one of only four known systems where multiple planets reside within the habitable zone of a single star. Our findings suggest that it is feasible to detect terrestrial planets in the habitable zones of evolved stars through the application of Doppler spectroscopy techniques. This research not only enhances our understanding of planetary formation and evolution around intermediate-mass stars but also opens avenues for future investigations into the potential for life in these distant worlds. The implications of these findings are profound, as they contribute to the ongoing search for habitable environments beyond our solar system. \n\nKeywords: exoplanet, planetary system",
        "ori-fast-z-score": -0.816496580927726,
        "water-fast-z-score": 2.0225995873897262,
        "rewrite-fast-z-score": -0.19611613513818404
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Chaos and Symmetry in String Cosmology .\nAbstract:\nWe study the dynamics of string cosmologies with nontrivial dilaton potentials, focusing on their chaotic behavior. We show that for certain classes of potentials there are regions where trajectories can be trapped by unstable fixed points or periodic orbits. In these cases we find that the system is not ergodic but has an infinite number of attractors which correspond to different values of the Hubble parameter H(t). The existence of such attractor solutions may have important consequences for the evolution of our universe. For example, it could explain why the present value of H(t) differs so much from its initial value at t = 0. It also provides a possible explanation for the observed flatness problem since the volume V (t) grows exponentially fast during inflation while the energy density decreases as 1/V (t).\nThe results presented here were obtained using numerical methods based on the fourth-order Runge-Kutta algorithm combined with Newton s method for finding roots.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Chaos and Symmetry in String Cosmology . Abstract : We research the dynamics of string cosmologies with nontrivial dilaton potentials , concentrating on their chaotic dynamics .We see that for particular categories of potentials there are areas where trajectories can be trapped by weak fixed points or periodic orbits . In these circumstances we find that the system is not ergodic but has an endless number of attractors which belong to different values of the Hubble parameter H ( t ) .The existence of such attractor solutions may have important implications for the evolution of our universe . For instance , it could explain why the present value of H ( t ) changes so much from its initial value at t = 0 .It additionally offers a possible reason for the seen flatness problem since the volume V ( t ) rises exponentially rapidly during inflation while the electricity density decreases as 1 / V ( t ) . The results presented here were obtained using numerical technique based on the fourth - order Runge - Kutta algorithm coupled with Newton s method for finding roots .",
        "rewrite_text": "In this study, we investigate the dynamics of string cosmologies characterized by complex dilaton potentials, with a particular focus on their chaotic behavior. Our analysis reveals that for specific classes of potentials, there exist regions where trajectories can become entrapped by weak fixed points or periodic orbits. Under these conditions, we observe that the system does not exhibit ergodicity; instead, it possesses an infinite number of attractors associated with varying values of the Hubble parameter H(t). The presence of such attractor solutions could have significant implications for the cosmological evolution of our universe. For example, it may provide insight into the substantial variation observed in the current value of H(t) compared to its initial value at t = 0. Furthermore, this framework offers a potential explanation for the flatness problem, as the volume V(t) experiences exponential growth during the inflationary phase, while the energy density diminishes in proportion to 1/V(t). The findings presented in this article were derived using numerical methods, specifically employing the fourth-order Runge-Kutta algorithm in conjunction with Newton's method for root-finding. This approach allows for a comprehensive exploration of the dynamical landscape of string cosmologies, shedding light on the intricate interplay between chaos and symmetry in the context of cosmological models.",
        "ori-fast-z-score": -0.6882472016116852,
        "water-fast-z-score": 4.672383634845156,
        "rewrite-fast-z-score": -0.41256849850351734
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The BFKL Pomeron Calculus in the dipole approach .\nAbstract:\nThe BFKL equation is an effective theory for describing high-energy scattering processes at small Bjorken-x, where x denotes the fraction of longitudinal momentum carried by one of the colliding hadrons or nuclei. The BFKL formalism has been developed into a practical tool to calculate cross sections and structure functions using numerical methods. In this talk I will present recent results on the calculation of the gluon Green s function within the framework of the so-called  dipole approach  which allows us to perform calculations analytically. This method was first introduced by Mueller and Tang in order to study diffractive deep-inelastic scattering (DDIS) off protons. It can be applied also to other processes like heavy quark production in proton-proton collisions as well as photon-photon interactions. We will discuss how we have implemented these ideas numerically and show some preliminary results obtained with our code. Finally, we will comment on possible extensions of this work towards more realistic phenomenological applications.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The BFKL Pomeron Calculus in the dipole approach . Abstract : The BFKL equation is an efficient model for describing high - energy scattering reactions at small Bjorken - x , where x denotes the fraction of longitudinal momentum carried by one of the colliding hadrons or nuclei .The BFKL formalism has been grown into a practical tool to estimate cross sections and structure functions using numerical methods . In this talk I will present recent results on the determination of the gluon Green s function within the framework of the so - called dipole approach which allows us to conduct measurements analytically .This method was first developed by Mueller and Tang in order to study diffractive deep - inelastic reflection ( DDIS ) off protons . It can be applied also to other processes like heavy quark production in proton - proton collisions as well as photon - photon interactions .We will explore how we have integrated these ideas numerically and report some preliminary results acquired with our code . Finally , we will mention on potential extensions of this research towards more realistic phenomenological applications .",
        "rewrite_text": "Title: The BFKL Pomeron Calculus in the Dipole Approach\n\nAbstract: The BFKL equation serves as a powerful framework for analyzing high-energy scattering processes at small Bjorken-x, where x represents the fraction of longitudinal momentum carried by one of the colliding hadrons or nuclei. Over time, the BFKL formalism has evolved into a robust tool for estimating cross sections and structure functions through numerical methods. In this presentation, I will discuss recent advancements in determining the gluon Green's function using the dipole approach, which facilitates analytical measurements. This methodology, initially introduced by Mueller and Tang, was designed to investigate diffractive deep-inelastic scattering (DDIS) off protons. Its applicability extends beyond DDIS to encompass various processes, including heavy quark production in proton-proton collisions and photon-photon interactions. We will delve into the numerical integration of these concepts and share preliminary results obtained from our computational framework. Additionally, we will outline potential avenues for extending this research toward more realistic phenomenological applications, thereby enhancing our understanding of high-energy scattering phenomena.",
        "ori-fast-z-score": 0.45291081365783825,
        "water-fast-z-score": 4.302652729749464,
        "rewrite-fast-z-score": -0.3511234415883917
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Connecting String/M Theory to the Electroweak Scale and to LHC Data .\nAbstract:\nThe Standard Model (SM) is an extremely successful theory, but it leaves many questions unanswered about physics at very high energies. In particular, there are no known fundamental principles that can explain why the SM has three generations of quarks and leptons with such different masses or how gravity fits into this picture. Theories beyond the Standard Model attempt to address these issues by introducing new particles and/or interactions which may be observed in future experiments.  Supersymmetry (SUSY), for example, introduces partners for all SM fields whose spin differs by one half unit. These partner states have identical gauge quantum numbers as their SM counterparts, so they could mix with them if SUSY were broken at low energy scales. This mixing would lead to deviations from SM predictions for observables like cross sections and decay rates. Many extensions of the Standard Model also predict new phenomena associated with extra dimensions of space-time. For instance, theories based on string/M-theory often contain additional spatial dimensions compactified down to tiny sizes. If these extra dimensions exist, then we should see evidence of their effects through virtual exchange of Kaluza-Klein excitations of gravitons and other particles between SM fields localized on our four-dimensional world-volume.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Connecting String / M Theory to the Electroweak Scale and to LHC Data . Abstract : The Standard Model ( SM ) is an incredibly successful theory , but it leaves many issues unanswered about physics at very high energies .In particular , there are no available fundamental principles that can reason why the SM has three generations of quarks and leptons with such distinct masses or how gravity fits into this picture . Theories beyond the Standard Model attempt to tackle these problems by introducing additional particles and / or relationships which would be encountered in future research .Supersymmetry ( SUSY ) , for example , creates partners for all SM fields whose spin varies by one half unit . These partner states have equal gauge quantum values as their SM counterparts , so they may blend with them if SUSY were breaking at low power scales .This mix might lead to deviations from SM predictions for observables like cross sections and decay rates . Many modifications of the Standard Model also predict new interactions associated with extra dimensions of space - time .For instance , theories based on string / M - theory often contain extra spatial dimensions compactified down to small sizes . If these extra dimensions arise , then we should see evidence of their influence through virtual exchange of Kaluza - Klein excitations of gravitons and other particles between SM fields confined on our four - dimensional world - volume .",
        "rewrite_text": "**Title:** Connecting String/M Theory to the Electroweak Scale and LHC Data\n\n**Abstract:** The Standard Model (SM) of particle physics has proven to be an exceptionally effective framework for understanding fundamental interactions; however, it falls short in addressing several critical questions related to high-energy physics. Notably, the SM does not provide a fundamental explanation for the existence of three generations of quarks and leptons, each with distinct masses, nor does it incorporate gravity into its framework. To address these shortcomings, various theories beyond the Standard Model have been proposed, which introduce additional particles and relationships that could be explored in future experiments. One prominent example is Supersymmetry (SUSY), which posits the existence of partner particles for every SM field, differing in spin by half a unit. These supersymmetric partners share the same gauge quantum numbers as their SM counterparts, suggesting that if SUSY is realized at low energy scales, it could lead to observable deviations from SM predictions in quantities such as cross sections and decay rates.\n\nFurthermore, many extensions of the Standard Model predict novel interactions stemming from the existence of extra dimensions of spacetime. Theories rooted in string/M-theory, for instance, often propose that these additional dimensions are compactified to minuscule scales. If such extra dimensions exist, their effects could manifest through the virtual exchange of Kaluza-Klein excitations of gravitons and other particles, influencing interactions among SM fields confined to our familiar four-dimensional spacetime. This paper aims to explore the connections between string/M-theory and the electroweak scale, as well as to analyze potential implications for data obtained from the Large Hadron Collider (LHC). By investigating these relationships, we hope to shed light on the underlying principles that govern particle physics at high energies and to identify signatures that could be detected in current and future experimental settings.",
        "ori-fast-z-score": 0.9805806756909202,
        "water-fast-z-score": 7.060180864974626,
        "rewrite-fast-z-score": 1.2004900959975617
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Signatures of SUSY Dark Matter at the LHC and in the Spectra of Cosmic Rays .\nAbstract:\nThe Large Hadron Collider (LHC) is expected to produce new particles that could be dark matter candidates, such as supersymmetric partners of quarks or leptons.  In this talk I will discuss how these signatures can be searched for using data collected by the ATLAS experiment during Run 1 of the LHC. The results are presented both in terms of limits on production cross sections and mass exclusion regions. Finally, prospects for future searches with Run 2 data are discussed. This work was performed within the framework of the PhD thesis of M.A.M., supervised by A.S.. \nIntroduction\n\nDark Matter Candidates\n\nSupersymmetry\n\nATLAS Experiment\n\nRun 1 Results\n\nProspects for Run 2 Searches\n\nConclusions & Outlook \n\nReferences \n\n\nAcknowledgements\n\n\n\n\n\n- - - - - - - -- - - - - - --- - - - - - ---- - - - - - ----- - - - - - ------ - - - - - -------- - - - - - ---------- - - - - - ------------------ - - - - - -------------- - - - - - ----------------------------------- - - - - - ----------------------------------------------------- - - - - - ------------------------------------------------------------------------------------ - - - - -",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Signatures of SUSY Dark Matter at the LHC and in the Spectra of Cosmic Rays . Abstract : The Large Hadron Collider ( LHC ) is expected to produce new objects that might be dark matter candidates , such as supersymmetric partners of quarks or leptons .In this talk I will explore how these signatures can be searched for utilizing information collected by the ATLAS experiment during Run 1 of the LHC . The results are presented both in terms of limits on production cross sections and mass exclusion areas .Finally , prospects for future investigations with Run 2 data are discussed . This project was done within the framework of the PhD thesis of M . A . M . , overseen by A . S . . Introduction Dark Matter Candidates Supersymmetry ATLAS Experiment Run 1 Results Prospects for Run 2 Searches Conclusions & Outlook References Acknowledgements - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -",
        "rewrite_text": "**Title:** Signatures of SUSY Dark Matter at the LHC and in the Spectra of Cosmic Rays\n\n**Abstract:** The Large Hadron Collider (LHC) is poised to uncover new particles that could serve as candidates for dark matter, particularly the supersymmetric (SUSY) partners of quarks and leptons. This presentation delves into the methodologies for identifying these potential signatures through the data amassed by the ATLAS experiment during the LHC's Run 1. The findings are articulated in terms of constraints on production cross sections and delineation of mass exclusion regions for various SUSY models. The analysis highlights the significance of these results in the broader context of dark matter research, emphasizing the potential implications for our understanding of the universe's composition. Furthermore, the talk will outline the anticipated advancements in the search for SUSY dark matter using data from Run 2, which promises to enhance the sensitivity and precision of the investigations. The discussion will encompass the theoretical underpinnings of dark matter candidates, the role of supersymmetry in particle physics, and the experimental techniques employed by the ATLAS collaboration. Concluding remarks will reflect on the future directions of this research, including the integration of new data and the refinement of search strategies. This work is part of the PhD thesis of M. A. M., under the guidance of A. S., and contributes to the ongoing efforts to unravel the mysteries of dark matter and its interactions with standard model particles. The presentation will also acknowledge the collaborative efforts and resources that made this research possible, paving the way for further exploration in this exciting field of study.",
        "ori-fast-z-score": 0.6030226891555273,
        "water-fast-z-score": 4.12837477233712,
        "rewrite-fast-z-score": -0.09578262852211514
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A dust component 2 kpc above the plane in NGC 891 .\nAbstract:\nWe present new near-infrared observations of the edge-on spiral galaxy NGC 891, obtained with VLT/VISIR and Gemini/NIRI at wavelengths between 8 and 13 microns (rest-frame). We detect an extended emission feature that is perpendicular to the galactic disk and extends up to 3 kpc into the halo. The morphology suggests that this structure consists of two components: one located close to the midplane and another one located about 1.5 kpc higher than it. This second component has a temperature of T = 120 K ± 20 K and shows no evidence for significant extinction by dust grains along its line-of-sight. Its luminosity corresponds to a star formation rate of 0.1 M⊙ yr−1. These results are consistent with previous studies suggesting that there exists a population of young stars outside the main body of galaxies. They also provide further support for models where supernovae explosions trigger large-scale outflows of gas and dust which can be responsible for the enrichment of intergalactic space.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A dust component 2 kpc above the plane in NGC 891 . Abstract : We report new near - infrared observations of the edge - on spiral galaxy NGC 891 , obtained with VLT / VISIR and Gemini / NIRI at wavelengths between 8 and 13 microns ( rest - frame ) .We detect an extended emitted characteristic that is parallel to the galactic disk and extends up to 3 kpc into the halo . The morphology suggests that this formation consists of two parts : one located close to the midplane and another one located about 1 . 5 kpc taller than it .This second component has a temperature of T = 120 K ± 20 K and shows no evidence for significant extinction by dust grains along its line - of - seeing . Its luminosity corresponds to a star development rate of 0 . 1 [UNK] yr−1 .These conclusions are compatible with previous research indicating that there exists a population of young galaxies outside the main bodies of galaxies . They also suggest further evidence for models where supernovae explosions induce huge - scale outflows of gas and dust which can be responsible for the enrichment of intergalactic space .",
        "rewrite_text": "We present new near-infrared observations of the edge-on spiral galaxy NGC 891, conducted using the VLT/VISIR and Gemini/NIRI instruments at wavelengths ranging from 8 to 13 microns in the rest frame. Our findings reveal an extended emission feature that runs parallel to the galactic disk and extends up to 3 kpc into the halo of the galaxy. The observed morphology indicates that this emission consists of two distinct components: one situated near the midplane and another positioned approximately 1.5 kpc above it. The upper component exhibits a temperature of T = 120 K ± 20 K and shows no significant signs of extinction caused by dust grains along the line of sight. The luminosity associated with this component corresponds to a star formation rate of 0.1 solar masses per year. These results align with previous studies that suggest the existence of a population of young stars located outside the primary structures of galaxies. Furthermore, our observations provide additional support for theoretical models proposing that supernova explosions can trigger large-scale outflows of gas and dust, which may contribute to the enrichment of intergalactic space. This research enhances our understanding of the complex interactions between galaxies and their surrounding environments, particularly in the context of star formation and the distribution of dust in galactic halos. The implications of these findings are significant for the study of galaxy evolution and the processes that govern the dynamics of interstellar and intergalactic matter.",
        "ori-fast-z-score": 1.3764944032233704,
        "water-fast-z-score": 5.584068246522259,
        "rewrite-fast-z-score": 0.19425717247145283
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A passivity-based stability criterion for a class of interconnected systems and applications to biochemical reaction networks .\nAbstract:\nIn this paper, we present an explicit formula for the maximum allowable delay in a linear time-invariant system with multiple delays by using the concept of passivity index. The proposed method is applied to a biochemical reaction network model consisting of two species interacting through three reactions. We show that our results are consistent with those obtained via numerical simulations. Finally, it should be noted that the proposed approach can also be used as a tool for analyzing other types of networks such as social or economic ones. In recent years there has been growing interest in studying complex dynamical behaviors of biological systems  1  . One important aspect of these studies concerns how different components interact within a cell  2  , which leads naturally to mathematical models based on chemical kinetics  3  .\nThe most common type of kinetic modeling involves ordinary differential equations (ODEs)  4  describing interactions between various molecular species  5  . However, due to the complexity of cellular processes  6  , many ODE models contain several state variables  7, 8  and/or parameters  9  whose values cannot always be determined experimentally  10  . This uncertainty may lead to significant errors when estimating the behavior of the underlying system  11  . To overcome this problem, stochastic approaches have recently been developed  12  . Another possibility consists in considering uncertainties in the form of unknown external disturbances  13  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A passivity - based security criterion for a class of interconnected networks and applications to biochemical reaction systems . Abstract : In this paper , we present an explicit formula for the maximum allowable delay in a linear time - invariant system with many delays by using the idea of passivity index .The proposed approach is applied to a biochemical reaction network model composed of two organisms evolving through three compounds . We see that our findings are compatible with those achieved via numerical simulations .Finally , it should be mentioned that the suggested approach can also be used as a platform for studying other types of networks such as social or economic ones . In recent years there has been growing interest in examining dynamic dynamical interactions of biological systems 1 .One important dimension of these research concerns how various components connect within a cell 2 , which results naturally to computational models relying on chemical kinetics 3 . The most common type of kinetic modeling involves ordinary differential equations ( ODEs ) 4 describing relationships between various molecular species 5 .However , owing to the complexity of cellular processes 6 , many ODE configurations contain many state components 7 , 8 and / or parameters 9 whose values never always be determined experimentally 10 . This instability may contribute to significant errors when estimating the behavior of the underlying process 11 .To solve this question , stochastic methods have recently been constructed 12 . Another possibility consists in considering uncertainties in the form of unseen external disturbances 13 .",
        "rewrite_text": "In this paper, we introduce a novel passivity-based security criterion tailored for a specific class of interconnected networks, with a particular focus on its application to biochemical reaction systems. We derive an explicit formula that determines the maximum allowable delay in linear time-invariant systems characterized by multiple delays, leveraging the concept of the passivity index. Our methodology is exemplified through a biochemical reaction network model that involves two organisms interacting through three distinct compounds. The results obtained from our analytical approach align well with those produced by numerical simulations, reinforcing the validity of our findings. Furthermore, we highlight the versatility of our proposed framework, suggesting its potential applicability to the analysis of various other network types, including social and economic networks.\n\nThe increasing interest in the dynamic interactions of biological systems has prompted extensive research in recent years. A critical aspect of this research is the investigation of how different components within a cell interconnect, which naturally leads to the development of computational models grounded in chemical kinetics. The predominant approach to kinetic modeling employs ordinary differential equations (ODEs) to describe the interactions among various molecular species. However, the inherent complexity of cellular processes often results in ODE configurations that encompass numerous state variables and parameters, many of which cannot be accurately determined through experimental means. This uncertainty can lead to substantial errors in predicting the behavior of the underlying biological processes. To address these challenges, recent advancements have introduced stochastic methods that account for such uncertainties. Additionally, we explore the implications of considering external disturbances that may not be directly observable, further enriching the discourse on the stability and reliability of biochemical network models.",
        "ori-fast-z-score": -1.5085060660073935,
        "water-fast-z-score": 5.788582927426393,
        "rewrite-fast-z-score": -0.33567254331867563
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  CDMA Technology for Intelligent Transportation Systems .\nAbstract:\nThe rapid development in the field of intelligent transportation systems (ITS) has led to an increasing demand on wireless communications, which is expected to be fulfilled by using Code Division Multiple Access (CDMA). In this paper we present a novel CDMA-based ITS system that can provide high data rate and low latency services with improved security features compared to existing technologies such as Global Positioning Systems (GPS), Wireless Local Area Networks (WLANs), Bluetooth or ZigBee. The proposed system consists of three main components; namely, roadside units (RSUs), mobile vehicles equipped with on-board units (OBUs) and central servers located at traffic control centers (TCCs).\nWe first introduce the basic concepts behind the proposed system followed by describing its key characteristics. Then, we discuss some important issues related to the design of the proposed system including channel modeling, resource allocation algorithms, power consumption analysis and performance evaluation. Finally, simulation results are presented to demonstrate the effectiveness of our proposed system.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : CDMA Technology for Intelligent Transportation Systems . Abstract : The rapid advancement in the field of smart transportation devices ( ITS ) has led to an increasing demand on digital communications , which is expected to be satisfied by using Code Division Multiple Access ( CDMA ) .In this paper we present a new CDMA - based ITS technology that can provide high data rate and low latency services with improved security features relative to existing technologies such as Global Positioning Systems ( GPS ) , Wireless Local Area Networks ( WLANs ) , Bluetooth or ZigBee . The proposed system includes of three principal components ; namely , roadside units ( RSUs ) , wireless trucks equipped with on - board units ( OBUs ) and central computers located at highway control centers ( TCCs ) .We first introduce the fundamental concepts behind the suggested system followed by describing its essential attributes . Then , we explain some important problems related to the development of the suggested system namely channel modeling , resource consumption methods , electricity demand analysis and performance audit .Finally , simulation data are presented to indicate the ability of our proposed system .",
        "rewrite_text": "Title: CDMA Technology for Intelligent Transportation Systems\n\nAbstract: The swift evolution of intelligent transportation systems (ITS) has generated a significant demand for robust digital communication solutions, which can be effectively addressed through the implementation of Code Division Multiple Access (CDMA) technology. This paper introduces an innovative CDMA-based approach tailored for ITS, designed to deliver high data rates and low latency services while enhancing security features compared to traditional technologies such as Global Positioning Systems (GPS), Wireless Local Area Networks (WLANs), Bluetooth, and ZigBee. Our proposed system comprises three key components: roadside units (RSUs), wireless trucks equipped with on-board units (OBUs), and central computers situated at traffic control centers (TCCs). \n\nWe begin by outlining the foundational principles that underpin our proposed system, followed by a detailed examination of its critical attributes. Subsequently, we address several significant challenges associated with the system's development, including channel modeling, resource consumption strategies, electricity demand analysis, and performance evaluation. To substantiate our claims, we present simulation results that demonstrate the efficacy and potential of our proposed CDMA-based ITS technology. The findings indicate that our system not only meets the growing demands of smart transportation but also offers a secure and efficient communication framework that can significantly enhance the overall functionality of intelligent transportation networks. Through this research, we aim to contribute to the ongoing discourse on the integration of advanced communication technologies in the realm of transportation, paving the way for smarter and safer travel solutions.",
        "ori-fast-z-score": 1.2510864843424487,
        "water-fast-z-score": 7.219948723811553,
        "rewrite-fast-z-score": 0.5345224838248488
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  On the Origin of Asymmetries in Bilateral Supernova Remnants .\nAbstract:\nWe present new observations and analysis of two nearby supernova remnants, G292.0+1.8 (Cas A) and Kes 79. We find that both remnants show significant asymmetry between their northern and southern halves; however, we argue that this is not due to an asymmetric explosion but rather to differences in the ambient medium into which each half was expanding at the time of its formation.  In Cas A, we observe a large difference in the density structure along the line-of-sight towards the north and south sides of the remnant. The northern side shows evidence for a dense shell-like feature while the southern side appears more uniform with no clear indication of such a shell. This suggests that the progenitor star exploded through a denser region on the northern side than it did on the southern side.   For Kes 79, our X-ray data reveal a bright compact source located near the center of the remnant s southern half. We suggest that this object may be associated with the central engine of the supernova event or possibly even the neutron star itself.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : On the Origin of Asymmetries in Bilateral Supernova Remnants . Abstract : We report new studies and investigation of two nearby supernova remnants , G292 . 0 + 1 . 8 ( Cas A ) and Kes 79 .We see that both remnants show considerable asymmetry between their western and southern parts ; however , we claim that this is not due to an asymmetric explosion but rather to differences in the atmospheric medium into which each quarter was expanding at the period of its formation . In Cas A , we study a large change in the density structure along the line - of - view towards the north and south sides of the remnant .The northern face displays evidence for a dense shell - like feature while the northeastern side appears more uniform with no clear indication of such a shell . This implies that the progenitor star burst through a denser region on the northern side than it did on the southern side .For Kes 79 , our X - ray data reveal a bright compact source located near the center of the remnant s southern quarter . We suggest that this body may be involved with the main engine of the supernova event or possibly perhaps the neutron star itself .",
        "rewrite_text": "In this article, we present our findings from an in-depth investigation of two nearby supernova remnants, G292.0+1.8 (Cas A) and Kes 79, focusing on the observed asymmetries between their western and southern regions. Contrary to the prevailing notion that these asymmetries result from an uneven explosion, we propose that they are primarily influenced by variations in the surrounding atmospheric medium during the remnants' expansion. Our analysis of Cas A reveals significant differences in the density structure along the line of sight, particularly between the northern and southern aspects of the remnant. The northern region exhibits a pronounced dense shell-like feature, while the northeastern side appears more homogenous, lacking such a distinct structure. This observation suggests that the progenitor star encountered a denser environment on the northern side during its explosion compared to the southern side. In the case of Kes 79, our X-ray observations have identified a bright, compact source situated near the center of the southern quarter of the remnant. We hypothesize that this source may be linked to the primary mechanism driving the supernova event, potentially representing the neutron star itself. Our findings contribute to a deeper understanding of the dynamics involved in supernova remnants and challenge existing paradigms regarding the nature of their asymmetries. By examining the interplay between the remnants and their surrounding medium, we aim to shed light on the complex processes that govern the evolution of these astronomical phenomena.",
        "ori-fast-z-score": 0.10721125348377948,
        "water-fast-z-score": 5.7564193416014815,
        "rewrite-fast-z-score": 0.38138503569823695
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Projectile Fragmentation of $^{86}$Kr at 64 MeV/nucleon .\nAbstract:\nThe projectile fragmentation of 86Kr at 64MeV/nucleon has been studied with the INDRA multidetector in inverse kinematics using an 8cm thick natK target and a beam intensity of 1nAe. The main results are as follows:  - A total number of about 10000 events have been recorded for this experiment.  - The charge distribution is peaked around Z=40, but shows also a large contribution between 30 and 40 charges units (see fig.1 ). This indicates that the fragments produced by the break-up of 86Kr are not only light particles like neutrons or protons, but contain many intermediate mass fragments too.   - The angular distributions show two peaks corresponding to forward and backward emission respectively (see fig.2 ).  - The energy spectra present a maximum around 10-12 MeV/u which corresponds to the most probable kinetic energy per nucleon of the emitted fragments (see fig.3 ).\n- The isotopic composition of the fragments is shown on figure 4 . It can be seen that there is no significant difference between the fragment production in the forward hemisphere and in the backward one.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Projectile Fragmentation of $ ^ { 86 } $ Kr at 64 MeV / nucleon . Abstract : The projectile fragmentation of 86Kr at 64MeV / nucleon has been studied with the INDRA multidetector in inverse kinematics using an 8cm thick natK target and a laser intensity of 1nAe .The main results are as follows : - A total number of about 10000 events have been observed for this study . - The charge distribution is peaked around Z = 40 , but shows also a large contribution between 30 and 40 charges units ( saw fig . 1 ) .This implies that the fragments produced by the broken - up of 86Kr are not only light particles like neutrons or protons , but contain many intermediate mass pieces too . - The angular distributions show two peaks related to forward and back emission respectively ( view fig . 2 ) .- The energy spectra produce a maximum around 10 - 12 MeV / u which equals to the most likely kinetic power per nucleon of the emitted objects ( see fig . 3 ) . - The isotopic structure of the fragments is displayed on figure 4 .It can be shown that there is no major changes between the fragment production in the front hemisphere and in the backward one .",
        "rewrite_text": "**Title:** Projectile Fragmentation of $^{86}$Kr at 64 MeV/nucleon\n\n**Abstract:** This study investigates the projectile fragmentation of $^{86}$Kr at an energy of 64 MeV/nucleon, utilizing the INDRA multidetector in an inverse kinematics setup. The experiment was conducted with an 8 cm thick natural potassium (natK) target and a laser intensity of 1 nA. Approximately 10,000 events were recorded, providing a comprehensive dataset for analysis. The charge distribution of the fragments reveals a peak around Z = 40, indicating a significant presence of intermediate mass fragments, with a notable contribution from charge units ranging between 30 and 40. This finding suggests that the fragmentation process of $^{86}$Kr yields not only light particles, such as neutrons and protons, but also a variety of heavier fragments. \n\nThe angular distributions of the emitted fragments exhibit two distinct peaks, corresponding to forward and backward emissions, which highlight the dynamics of the fragmentation process. The energy spectra of the fragments show a pronounced maximum in the range of 10 to 12 MeV/u, which corresponds to the most probable kinetic energy per nucleon of the emitted particles. Additionally, the isotopic composition of the fragments is illustrated in the accompanying figures, revealing no significant differences in fragment production between the forward and backward hemispheres. These results contribute to a deeper understanding of the fragmentation mechanisms involved in heavy-ion collisions and the characteristics of the resulting fragment distributions. The findings underscore the complexity of the fragmentation process and the variety of fragments produced, which may have implications for further research in nuclear physics and related fields.",
        "ori-fast-z-score": -0.6882472016116852,
        "water-fast-z-score": 4.672383634845156,
        "rewrite-fast-z-score": 0.4745789978762495
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Locally Decodable Codes From Nice Subsets of Finite Fields and Prime Factors of Mersenne Numbers .\nAbstract:\nWe present new constructions for locally decodable codes (LDCs) based on nice subsets of finite fields, prime factors of Mersenne numbers, and the Chinese remainder theorem. Our first construction is an explicit family of LDCs with optimal query complexity that are derived from nice subsets of finite fields. We then show how to use these results in conjunction with the Chinese remainder theorem to obtain families of LDCs whose parameters can be tuned by varying the number of primes used in their construction. Finally we give a lower bound on the length of any LDC over a binary alphabet which uses only one-sided queries. The proofs of our main theorems appear at the end of this extended abstract. \nIntroduction\n\nLocally decodable codes (abbreviated as LCDs)\nare error correcting codes where each codeword has associated with it some decoding algorithm that allows efficient recovery of individual bits or symbols when queried about them. In particular, if the codeword is corrupted by up to t errors, then there exists a deterministic polynomial time algorithm that recovers the original uncorrupted word using O(t) queries per symbol. This property makes such codes useful in applications like data storage systems, peer-to-peer networks, and streaming media distribution services. \n \n A large body of work has been devoted towards understanding the trade-off between the rate of the code, its distance, and the query complexity of the decoder. For example, Guruswami et al. (2005), Chan et al. (2006) , and Kopparty & Saraf (2007)  studied the problem of constructing codes with low query complexity while maintaining high rates and distances. Recently, several works have focused on improving the known bounds on the minimum possible query complexity required to decode a single bit given a certain amount of corruption. These include the recent breakthrough result of Dvir et al. (2010a) (which improved upon the previous best-known upper bound due to Sudan et al., 2005 ) and the subsequent improvements made by Dvir et al. (2011 ), Feldman et al. (2012 , and Kopparty et al. (2013) . \n \n Another important",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Locally Decodable Codes From Nice Subsets of Finite Fields and Prime Factors of Mersenne Numbers . Abstract : We create fresh constructions for locally decodable codes ( LDCs ) based on nice subsets of finite fields , prime factors of Mersenne numbers , and the Chinese remainder theorem .Our first build is an explicit class of LDCs with optimal query complexity that are derived from good subsets of finite fields . We then show how to use these results in conjunction with the Chinese remainder theorem to obtain families of LDCs whose parameters can be tuned by varying the quantity of primes used in their construction .Finally we give a smaller bound on the length of any LDC over a binary alphabet which uses only one - sided queries . The proofs of our major theorems emerge at the end of this enlarged abstract .Introduction Locally decodable codes ( denoted as LCDs ) are mistake correcting codes where each codeword has associated with it some decoding algorithm that enables efficient returning of individual bits or characters when queried about them . In particular , if the codeword is corrupted by up to t errors , then there exists a deterministic polynomial period algorithm that recovers the actual uncorrupted phrase using O ( t ) queries per symbol .This property gives such codes usable in applications like data storage systems , peer - to - peer systems , and streaming media distribution services . A vast body of research has been focused towards studying the trade - off between the frequency of the code , its distance , and the query complexity of the decoder .For instance , Guruswami et al . ( 2005 ) , Chan et al .( 2006 ) , and Kopparty & Saraf ( 2007 ) studied the issue of constructing coding with lowest query complexity while maintaining high rates and distances . Recently , various efforts have concentrated on improving the known bounds on the minimum possible query complexity required to decode a single bit given a certain level of corruption .These include the recent breakthrough result of Dvir et al . ( 2010a ) ( which updated upon the previous good - used upper bound due to Sudan et al . , 2005 ) and the subsequent improvements done by Dvir et al .( 2011 ) , Feldman et al . ( 2012 , and Kopparty et al .(2013) .Another important",
        "rewrite_text": "**Title:** Locally Decodable Codes from Nice Subsets of Finite Fields and Prime Factors of Mersenne Numbers\n\n**Abstract:** In this paper, we present novel constructions for locally decodable codes (LDCs) that leverage nice subsets of finite fields, the prime factors of Mersenne numbers, and the principles of the Chinese remainder theorem. Our initial construction yields an explicit class of LDCs characterized by optimal query complexity, which are derived from well-defined subsets of finite fields. We further demonstrate how these findings can be integrated with the Chinese remainder theorem to generate families of LDCs, allowing for parameter adjustments through the manipulation of the number of primes involved in their construction. Additionally, we establish a tighter bound on the length of any LDC over a binary alphabet that employs one-sided queries. The proofs supporting our principal theorems are detailed at the conclusion of this extended abstract.\n\n**Introduction:** Locally decodable codes (LDCs) are a class of error-correcting codes designed to facilitate the recovery of individual bits from a codeword, even in the presence of errors. Specifically, when a codeword is subject to up to t errors, there exists a deterministic polynomial-time algorithm capable of retrieving the original uncorrupted message with O(t) queries per symbol. This remarkable feature renders LDCs particularly valuable in various applications, including data storage systems, peer-to-peer networks, and streaming media services. A significant amount of research has been dedicated to exploring the trade-offs between the code's rate, its distance, and the query complexity of the decoding process. Notable contributions to this field include the works of Guruswami et al. (2005), Chan et al. (2006), and Kopparty & Saraf (2007), who investigated the construction of codes that minimize query complexity while maximizing rates and distances. Recent advancements have focused on refining the known limits on the minimum query complexity necessary for decoding a single bit amidst a specified level of corruption. This includes the groundbreaking results of Dvir et al. (2010a), which improved upon earlier upper bounds established by Sudan et al. (2005), as well as subsequent enhancements by Dvir et al. (2011), Feldman et al. (2012), and Kopparty et al. (2013).",
        "ori-fast-z-score": -0.79555728417573,
        "water-fast-z-score": 6.046235359735548,
        "rewrite-fast-z-score": -0.6446583712203042
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Holes within galaxies: the egg or the hen? .\nAbstract:\nWe present new results on the evolution and properties of galactic holes, based on an analysis of deep optical images obtained with the Hubble Space Telescope (HST). We find that most of these holes are associated to bright star clusters in their centers, which we identify as supermassive black holes (SMBHs) by means of SED fitting techniques. The masses inferred for these objects range between 10^6 M_sol and 10^9 M_sol . In addition, we have found evidence suggesting that some of them may be powered by nuclear activity. Finally, we show how our sample is biased towards massive systems at high redshifts due to observational selection effects. Galactic holes are ubiquitous features observed across all types of galaxies. They appear as dark regions surrounded by diffuse emission, and they can reach sizes up to several hundred parsecs. Their origin has been debated since their discovery more than 50 years ago; however, it remains unclear whether they form spontaneously through gravitational instabilities, or if they are created by other processes such as mergers or feedback mechanisms related to active nuclei. Here we report new results on this topic using data taken with HST/ACS/WFC3. Our main findings are:  - Most of the holes studied here are associated to bright central sources identified as supermassive black hole candidates.  - Some of the holes seem to be powered by nuclear activity.  - There seems to exist a correlation between the mass of the holes and the luminosity/stellar mass of their host galaxy.  - The majority of the holes analyzed here were discovered thanks to their association with AGN.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Holes within galaxies : the egg or the hen ? .Abstract : We report new data on the evolution and properties of galactic holes , using on an assessment of deep optical images obtained with the Hubble Space Telescope ( HST ) . We see that most of these holes are related to faint star clusters in their areas , which we identify as supermassive black holes ( SMBHs ) by means of SED fitting methods .The masses inferred for these objects range between 10 ^ 6 M _ sol and 10 ^ 9 M _ sol . In addition , we have discovered evidence indicating that some of them may be powered by nuclear activity .Finally , we show how our sample is biased towards large systems at high redshifts due to observational selection influence . Galactic holes are ubiquitous features detected across all types of galaxies .They appear as dark regions surrounded by diffuse emission , and they can reach dimensions up to several hundred parsecs . Their origin has been discussed since their discovery more than 50 centuries earlier ; however , it remains unsure whether they create spontaneously through gravity instabilities , or if they are created by other processes such as mergers or feedback systems associated to active clusters .Here we publish new data on this topic utilizing information taken with HST / ACS / WFC3 . Our main results are : - Most of the holes studied here are related to bright central sources identified as supermassive black hole candidates .- Some of the holes appear to be powered by nuclear activity . - There seems to exist a correlation between the mass of the holes and the luminosity / stellar mass of their target galaxy .- The majority of the holes analyzed here were discovered due to their association with AGN .",
        "rewrite_text": "Title: Holes within Galaxies: The Egg or the Hen?\n\nAbstract: In this study, we present new findings regarding the evolution and characteristics of galactic holes, derived from an analysis of deep optical images captured by the Hubble Space Telescope (HST). Our investigation reveals that a significant number of these galactic holes are associated with faint star clusters in their vicinity, which we have identified as candidates for supermassive black holes (SMBHs) through spectral energy distribution (SED) fitting techniques. The estimated masses of these SMBH candidates range from approximately \\(10^6 M_{\\odot}\\) to \\(10^9 M_{\\odot}\\). Furthermore, we have uncovered evidence suggesting that some of these holes may be driven by nuclear activity. \n\nOur research also highlights a bias in our sample towards larger systems at higher redshifts, a consequence of observational selection effects. Galactic holes are prevalent features found in various types of galaxies, manifesting as dark regions surrounded by diffuse light, with sizes that can extend to several hundred parsecs. The origin of these holes has been a topic of debate since their initial discovery over five decades ago, with ongoing discussions about whether they form spontaneously due to gravitational instabilities or through alternative mechanisms such as mergers or feedback processes linked to active star clusters.\n\nIn this paper, we provide new insights into this phenomenon using data obtained from HST's Advanced Camera for Surveys (ACS) and Wide Field Camera 3 (WFC3). Our key findings include: (1) a majority of the holes examined are correlated with bright central sources identified as candidates for supermassive black holes; (2) some holes exhibit signs of being powered by nuclear activity; (3) a correlation appears to exist between the mass of the holes and the luminosity or stellar mass of their host galaxies; and (4) most of the holes analyzed were identified due to their association with active galactic nuclei (AGN). These results contribute to our understanding of the complex interplay between galactic structures and supermassive black holes.",
        "ori-fast-z-score": -0.3592106040535498,
        "water-fast-z-score": 5.945288613087962,
        "rewrite-fast-z-score": 1.5778641172210595
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Non-perturbative renormalization of the chromo-magnetic operator in Heavy Quark Effective Theory and the B* - B mass splitting .\nAbstract:\nWe present an explicit calculation of the non-perturbative renormalisation constant for the chromomagnetic operator in heavy quark effective theory (HQET). We use this to calculate the leading order contribution to the mass difference between the ground state vector mesons containing a b-quark, i.e., $B^*$- $B$ mixing. The result is compared with lattice QCD calculations at next-to-leading order in HQET perturbation theory. Our results are consistent within errors but do not agree as well as one would like. This may be due to missing higher-order corrections or systematic uncertainties inherent in both approaches. \n \n Introduction \n \n In recent years there has been considerable interest in studying hadronic systems containing a single heavy quark using the framework provided by heavy quark effective theory (HQT)  1  . One important application of HQT is to study the properties of heavy-light mesons such as the bottomonium system  2  , which can then be used to test our understanding of nonrelativistic quantum mechanics  3  .\n \nIn particular, it is interesting to consider how the masses of these states depend on their spin. For example, the lowest lying bb states have spin-parity J P = 0+ and 1− respectively  4  . These two states mix under the weak interaction through the emission and absorption of virtual gluons  5  . At tree level we find that the lightest physical eigenstate is given by:",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Non - perturbative renormalization of the chromo - magnetic operator in Heavy Quark Effective Theory and the B * - B mass separation . Abstract : We present an explicit determination of the non - perturbative renormalisation constant for the chromomagnetic operator in heavy quark effective theory ( HQET ) .We use this to estimate the led order contribution to the mass ratio between the ground state vector mesons containing a b - quark , i . e . , $ B ^ * $ - $ B $ mixing . The result is compared with lattice QCD calculations at next - to - leading order in HQET perturbation theory .Our results are correct within errors but do not comply as well as one would like . This might be due to missing higher - order corrections or systematic uncertainties inherent in both approaches .Introduction In recent years there has been substantial interest in investigating hadronic networks featuring a single heavy quark using the framework given by massive quark effective theory ( HQT ) 1 . One important use of HQT is to study the properties of heavy - light mesons such as the bottomonium scheme 2 , which can then be used to test our appreciation of nonrelativistic quantum mechanics 3 .In particular , it is curious to consider how the masses of these states depend on their spin . For instance , the lowest lying bb states have spin - parity J P = 0 + and 1− respectively 4 .These two states mix under the strong coupling through the emission and emission of virtual gluons 5 . At tree level we find that the lightest physical eigenstate is given by :",
        "rewrite_text": "**Title:** Non-Perturbative Renormalization of the Chromo-Magnetic Operator in Heavy Quark Effective Theory and the B*-B Mass Separation\n\n**Abstract:** In this study, we provide a detailed calculation of the non-perturbative renormalization constant for the chromo-magnetic operator within the framework of Heavy Quark Effective Theory (HQET). This renormalization constant is crucial for accurately estimating the leading-order contribution to the mass ratio of ground state vector mesons that contain a bottom quark, specifically in the context of $B^*$-$B$ mixing. Our findings are juxtaposed with lattice Quantum Chromodynamics (QCD) calculations conducted at next-to-leading order in HQET perturbation theory. While our results align with the expected values within the margin of error, they do not achieve the level of agreement that one might hope for. This discrepancy may stem from the omission of higher-order corrections or from systematic uncertainties that are present in both methodologies. \n\nThe investigation of hadronic systems featuring a single heavy quark has garnered significant attention in recent years, particularly through the lens of Heavy Quark Effective Theory (HQET). This theoretical framework is instrumental in exploring the characteristics of heavy-light mesons, such as those found in the bottomonium spectrum, and serves as a valuable tool for testing our understanding of non-relativistic quantum mechanics. A particularly intriguing aspect of this research is the dependence of the masses of these mesonic states on their spin. For example, the lowest-lying bottomonium states exhibit spin-parity quantum numbers of $J^P = 0^+$ and $1^-$, respectively. These states interact through strong coupling, leading to mixing via the exchange of virtual gluons. At the tree level, our analysis reveals that the lightest physical eigenstate can be expressed as follows: [insert expression]. This work contributes to the ongoing efforts to refine our comprehension of heavy quark dynamics and the intricate interplay of quantum chromodynamics in hadronic systems.",
        "ori-fast-z-score": -0.09578262852211514,
        "water-fast-z-score": 4.385927910529725,
        "rewrite-fast-z-score": -0.7242859683401482
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dalitz plot analysis of the D+ to K-pi+pi+ decay in the FOCUS experiment .\nAbstract:\nThe Dalitz plot distribution for the decay D+ ->K-pi+pi+ is measured using data collected by the FOCUS experiment at Fermilab, corresponding to an integrated luminosity of 1 fb-1 . The measurement uses a sample of about 2 million events with one charged track and two neutral clusters reconstructed in the central drift chamber (CDC) and electromagnetic calorimeter (EMC). A maximum likelihood fit is performed on this sample to extract the branching fraction B(D+ ->K-pi+pipi+), which is found to be  _ = (1.55 +/- 0.10 ) x 10-3 , where the uncertainty includes both statistical and systematic contributions.  This result agrees well with previous measurements but has improved precision due to the larger number of signal events used here compared to earlier results. It also improves upon the most recent theoretical prediction based on lattice QCD calculations. The ratio Rc/D between the Cabibbo-suppressed and Cabibbo-favored decays into three pions is determined as Rc/D=(0.84+0.11-0.12)x10-2.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dalitz plot analysis of the D + to K - pi + pi + decay in the FOCUS experiment . Abstract : The Dalitz plot distribution for the decay D + - > K - pi + pi + is measured using data taken by the FOCUS experiment at Fermilab , equivalent to an integrated luminosity of 1 fb - 1 .The measurement involves a sample of about 2 million events with one charged track and two neutral clusters preserved in the main drift chamber ( CDC ) and electromagnetic calorimeter ( EMC ) . A maximum likelihood fit is conducted on this specimen to extract the branching fraction B ( D + - > K - pi + pipi + ) , which is found to be _ = ( 1 . 55 + / - 0 . 10 ) x 10 - 3 , where the uncertainty includes both statistical and systematic contributions .This result agrees well with previous measurements but has improved precision thanks to the bigger quantity of signal events employed here relative to earlier findings . It additionally improves upon the most current theoretical estimate based on lattice QCD calculations .The ratio Rc / D between the Cabibbo - subdued and Cabibbo - preferred decays into three pions is calculated as Rc / D = ( 0 . 84 + 0 . 11 - 0 . 12 ) x10 - 2 .",
        "rewrite_text": "**Title:** Dalitz Plot Analysis of the D⁺ → K⁻π⁺π⁺ Decay in the FOCUS Experiment\n\n**Abstract:** This study presents a comprehensive analysis of the Dalitz plot distribution for the decay process D⁺ → K⁻π⁺π⁺, utilizing data collected by the FOCUS experiment at Fermilab, which corresponds to an integrated luminosity of 1 fb⁻¹. The analysis is based on a substantial dataset comprising approximately 2 million events, characterized by one charged track and two neutral clusters, which were recorded in the main drift chamber (CDC) and the electromagnetic calorimeter (EMC). To derive the branching fraction B(D⁺ → K⁻π⁺π⁺), a maximum likelihood fit was performed on the collected data, yielding a result of B(D⁺ → K⁻π⁺π⁺) = (1.55 ± 0.10) × 10⁻³. The reported uncertainty encompasses both statistical and systematic errors, reflecting the robustness of the measurement. This finding is consistent with prior results but demonstrates enhanced precision due to the larger number of signal events analyzed compared to earlier studies. Furthermore, this measurement offers an improvement over the latest theoretical predictions derived from lattice QCD calculations. Additionally, the study calculates the ratio R_c/D, which compares the rates of Cabibbo-suppressed and Cabibbo-allowed decays into three pions, resulting in R_c/D = (0.84 +0.11 -0.12) × 10⁻². This analysis not only contributes valuable data to the understanding of D meson decays but also serves to refine theoretical models in particle physics.",
        "ori-fast-z-score": 0.4588314677411235,
        "water-fast-z-score": 4.900304787764432,
        "rewrite-fast-z-score": -0.502518907629606
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Infrared Extinction Law at Extreme Depth in a Dark Cloud Core .\nAbstract:\nWe present the first detection and characterization of infrared extinction law (IRAL) toward an extremely dark cloud core, L183. The IRAL is derived by comparing near-infrared to mid-infrared colors between background stars and foreground objects projected on the same line-of-sight through the cloud. We find that the IRAL shows no significant variation with depth into the cloud down to A V = 1000 mag. This result suggests that dust grains are not significantly modified even under such extreme conditions as those found deep inside dense clouds. Our results also suggest that grain growth may be suppressed in these environments due to efficient shattering caused by collisions among large grains. These findings have important implications for understanding the formation process of planetesimals. \n \n Keywords: Infrared extinction law, Dust properties, Interstellar medium, Shock waves \n \n 1. Introduction \n \n It has been suggested that interstellar dust grains grow up to millimeter sizes or larger within dense molecular clouds because they can survive against destructive collisions with other particles (e.g., coagulation theory; Ossenkopf & Henning 1994). However, recent observations show that there exist many small dust grains in dense regions where the gas density exceeds 10^6 cm^{-3} (e.g., Stepnik et al. 2003; Pagani et al. 2003), which contradicts this scenario. To resolve this discrepancy, it was proposed that dust grains could be destroyed efficiently via collisional fragmentation when their size becomes comparable to the mean free path of hydrogen molecules (Ormel et al. 2007). \n \n Another possibility is that dust grains do not grow but rather fragment into smaller pieces during collisions (e.g., Blum & Wurm 2008). If so, then we would expect to see some evidence of grain destruction products like sub-micron-sized fragments in dense clouds. Indeed, several observational studies reported the presence of sub-millimeter emission features attributed to silicate and/or carbonaceous materials in dense clouds (e.g., Jones et al. 1993; Chiar et al. 1998; Kessler",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Infrared Extinction Law at Extreme Depth in a Dark Cloud Core . Abstract : We report the first recognition and identification of infrared extinction law ( IRAL ) toward an exceptionally dark cloud core , L183 .The IRAL is calculated by using near - infrared to mid - infrared colors between background stars and foreground objects projected on the same line - of - view through the cloud . We see that the IRAL displays no considerable variation with depth into the cloud down to A V = 1000 mag .This result suggests that dust grains are not dramatically enhanced even under such extreme circumstances as those observed deep inside dense clouds . Our results also suggest that grain growth could be suppressed in these habitats due to efficient crushing caused by collisions among huge grains .These studies have important implications for studying the formation system of planetesimals . Keywords : Infrared extinction law , Dust properties , Interstellar medium , Shock effects 1 .Introduction It has been proposed that interstellar dust grains grow up to millimeter sizes or larger within dense molecular clouds because they can endure against devastating collisions with other particles ( e . g . , coagulation hypothesis ; Ossenkopf & Henning 1994 ) . However , recent observations show that there remain many small dust grains in dense areas where the gas density exceeds 10 ^ 6 cm ^ { - 3 } ( e . g . , Stepnik et al .2003 ; Pagani et al . 2003 ) , which contradicts this situation .To settle this discrepancy , it was suggested that dust grains could be devastated easily via collisional fragmentation when their size grows equivalent to the mean free path of hydrogen compounds ( Ormel et al . 2007 ) .Another possibility is that dust grains do not shrink but rather fragment into tiny pieces during collisions ( e . g . , Blum & Wurm 2008 ) . If so , then we may expect to see some evidence of grain destruction products like sub - micron - sized fragments in dense clouds .Indeed , various observational surveys reported the presence of sub - millimeter emitted features attributed to silicate and / or carbonaceous materials in dense clouds ( e . g . , Jones et al . 1993 ; Chiar et al .1998; Kessler",
        "rewrite_text": "**Title:** The Infrared Extinction Law at Extreme Depth in a Dark Cloud Core\n\n**Abstract:** In this study, we present the inaugural identification and analysis of the infrared extinction law (IRAL) in the exceptionally dark cloud core L183. The IRAL was derived by examining the near-infrared to mid-infrared color differences between background stars and foreground objects aligned along the same line of sight through the cloud. Our findings reveal that the IRAL remains remarkably consistent with depth, extending to an extinction value of A_V = 1000 mag. This observation indicates that the properties of dust grains do not significantly alter, even under the extreme conditions prevalent in the depths of dense molecular clouds. Furthermore, our results imply that the growth of dust grains may be inhibited in these environments due to the effective fragmentation caused by collisions among larger grains. This has profound implications for understanding the processes involved in the formation of planetesimals. \n\n**Keywords:** Infrared extinction law, Dust properties, Interstellar medium, Shock effects\n\n**1. Introduction:** The prevailing hypothesis suggests that interstellar dust grains can grow to millimeter sizes or larger within dense molecular clouds, as they are thought to withstand destructive collisions with other particles (e.g., coagulation hypothesis; Ossenkopf & Henning 1994). However, recent observational data indicate the persistence of numerous small dust grains in regions where gas density exceeds 10^6 cm^{-3} (e.g., Stepnik et al. 2003; Pagani et al. 2003), challenging this notion. To address this inconsistency, it has been proposed that dust grains may undergo significant fragmentation due to collisions when their size approaches the mean free path of hydrogen molecules (Ormel et al. 2007). Alternatively, it is possible that rather than shrinking, larger dust grains break into smaller fragments during collisions (e.g., Blum & Wurm 2008). If this is the case, we would expect to observe evidence of these destruction products, such as sub-micron-sized fragments, in dense cloud environments. Indeed, various observational studies have reported sub-millimeter emission features attributed to silicate and/or carbonaceous materials within these dense clouds (e.g., Jones et al. 1993; Chiar et al. 1998; Kessler).",
        "ori-fast-z-score": 0.23354968324845687,
        "water-fast-z-score": 7.7615052570633285,
        "rewrite-fast-z-score": 1.1538461538461537
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Angular momentum conservation and torsional oscillations in the Sun and solar-like stars .\nAbstract:\nWe present an analysis of helioseismic data for the angular velocity profile of the Sun, which is based on the assumption that the total angular momentum of the Sun is conserved during its evolution. We find that this hypothesis leads to a good agreement between theoretical predictions and observations at all latitudes except near the poles where we observe significant deviations. The discrepancies are explained by taking into account the effects of torsional oscillations in the convective envelope of the Sun. Our results show that these oscillations can be excited by internal magnetic fields generated by differential rotation in the radiative interior of the Sun. This mechanism may also operate in other solar-type stars with strong surface magnetic activity. In our study we use high-precision measurements of the frequency splitting of acoustic modes obtained using space-based instruments (GOLF and MDI) aboard the Solar and Heliospheric Observatory satellite. \n \n Keywords: Angular momentum, Torsional oscillations",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Angular momentum conservation and torsional oscillations in the Sun and solar - like stars . Abstract : We report an assessment of helioseismic data for the angular velocity profile of the Sun , which is based on the assumption that the total angular velocity of the Sun is conserved during its evolve .We see that this hypothesis leads to a better agreement between theoretical estimates and observations at all latitudes except near the poles where we perceive considerable deviations . The discrepancies are explained by take into consideration the effects of torsional oscillations in the convective envelope of the Sun .Our results show that these oscillations can be excited by inner magnetic waves generated by differential rotation in the radiative interior of the Sun . This mechanism may even exist in other solar - class stars with powerful surface magnetic activity .In our research we using high - precision observations of the frequency separation of acoustic modes obtained using space - based instruments ( GOLF and MDI ) aboard the Solar and Heliospheric Observatory satellite . Keywords : Angular momentum , Torsional oscillations",
        "rewrite_text": "Title: Angular Momentum Conservation and Torsional Oscillations in the Sun and Solar-Like Stars\n\nAbstract: In this study, we evaluate helioseismic data to analyze the angular velocity profile of the Sun, grounded in the premise that the Sun's total angular momentum remains conserved throughout its evolutionary process. Our findings indicate that this conservation hypothesis enhances the alignment between theoretical predictions and empirical observations across various latitudes, with the exception of regions near the poles, where significant discrepancies are noted. We attribute these deviations to the influence of torsional oscillations occurring within the Sun's convective envelope. Our analysis reveals that these oscillations can be triggered by magnetic waves originating from the differential rotation present in the Sun's radiative interior. This excitation mechanism may also be applicable to other solar-type stars exhibiting strong surface magnetic activity. To conduct our research, we utilized high-precision measurements of acoustic mode frequency separations, which were acquired through space-based instruments such as GOLF and MDI aboard the Solar and Heliospheric Observatory (SOHO) satellite. The implications of our results extend beyond the Sun, suggesting a broader relevance to the study of angular momentum dynamics and torsional oscillations in solar-like stars. Our work contributes to a deeper understanding of stellar evolution and the complex interplay between magnetic fields and rotational dynamics in astrophysical contexts. \n\nKeywords: Angular momentum, Torsional oscillations.",
        "ori-fast-z-score": -0.11396057645963795,
        "water-fast-z-score": 4.755563543407302,
        "rewrite-fast-z-score": 0.7181848464596079
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Network Topology of an Experimental Futures Exchange .\nAbstract:\nWe present the network topology and trading volume for the first experimental futures exchange, which was launched in September 2009 as part of the University of Chicago s Financial Markets Lab (FML). The FML is designed to provide students with hands-on experience in designing, implementing, and operating financial markets. We find that the network exhibits small-world properties similar to those observed on other real world networks such as social networks or the Internet. In addition we show how the network evolves over time by analyzing its growth rate and degree distribution. Finally, we study the relationship between the number of transactions executed at each node and their respective degrees. Our results suggest that nodes with higher degrees tend to trade more than lower-degree nodes. This finding has important implications for market design since it suggests that traders should be incentivized to increase their connectivity within the network if they want to maximize their trading activity. We also examine whether there are any differences across different types of contracts traded on the exchange.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Network Topology of an Experimental Futures Exchange . Abstract : We present the network topology and trading volume for the first theoretical futures exchange , which was launched in September 2009 as part of the University of Chicago s Financial Markets Lab ( FML ) .The FML is designed to provide children with fingers - on experience in planning , implementing , and operating financial markets . We see that the channel exhibits small - world properties similar to those observed on other actual world networks such as social sites or the Internet .In addition we study how the organization evolves over time by analyzing its expansion frequency and degree distribution . Finally , we study the relationship between the quantity of transactions executed at each node and their individual degrees .Our results show that nodes with higher degrees prefer to trade more than higher - degree connections . This conclusion has critical impacts for trade design since it suggests that buyers should be incentivized to expand their connectivity within the network if they wish to maximize their trading activity .We also investigate whether there are any variations across different kinds of contracts traded on the trade .",
        "rewrite_text": "In this article, we explore the network topology and trading volume of the inaugural theoretical futures exchange, which was established in September 2009 as part of the University of Chicago's Financial Markets Lab (FML). The FML aims to provide students with hands-on experience in the planning, implementation, and operation of financial markets. Our analysis reveals that the trading network exhibits small-world characteristics, akin to those found in various real-world networks, including social media platforms and the Internet. \n\nWe further examine the evolution of the exchange's organization over time by assessing its expansion frequency and degree distribution. A key focus of our study is the correlation between the volume of transactions conducted at each node and their respective degrees. Our findings indicate that nodes with higher degrees tend to engage in more trading activities compared to those with lower degrees. This insight has significant implications for trade design, as it suggests that incentivizing buyers to enhance their connectivity within the network could lead to increased trading activity.\n\nAdditionally, we investigate potential variations in trading behavior across different types of contracts offered on the exchange. By analyzing these dynamics, we aim to provide a deeper understanding of how network structure influences trading patterns and the overall efficiency of the futures exchange. Our research contributes to the broader discourse on market design and network theory, offering valuable insights for both academic and practical applications in financial markets.",
        "ori-fast-z-score": -1.0540925533894598,
        "water-fast-z-score": 5.555909634626167,
        "rewrite-fast-z-score": 0.9760921603577252
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  New smooth hybrid inflation .\nAbstract:\nWe propose a new inflationary model, which is based on the idea that there are two scalar fields in nature and they interact with each other through their potentials. We call this model  smooth hybrid inflation  (SHI). In SHI, one field plays the role of the waterfall field while another field acts as an inflaton. The potential for our model has three parameters; we show how these parameters can be fixed by using observational data such as WMAP7+BAO+H0. Our results indicate that the tensor-to-scalar ratio r=0.0055+0.0005-0.0004 at 95% CL. This value is consistent with recent observations. Moreover, we find that the spectral index ns=1.0145+0.0025-0.0024 at 68% CL. Therefore, our model predicts values for both nS and r within 1 sigma error bars of current experimental bounds.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : New smooth hybrid inflation . Abstract : We suggest a new inflationary model , which is based on the idea that there are two scalar fields in nature and they interact with each other through their potentials .We call this model smooth hybrid inflation ( SHI ) . In SHI , one field plays the role of the waterfall field while another field functions as an inflaton .The potential for our model has three variables ; we show how these parameters can be fixed by using observational data such as WMAP7 + BAO + H0 . Our results show that the tensor - to - scalar ratio r = 0 . 0055 + 0 . 0005 - 0 . 0004 at 95 % CL .This value is compatible with recent observations . Moreover , we find that the spectral index ns = 1 . 0145 + 0 . 0025 - 0 . 0024 at 68 % CL .Therefore , our model predicts values for both nS and r within 1 sigma error bars of recent experimental bounds .",
        "rewrite_text": "We propose a novel inflationary model termed Smooth Hybrid Inflation (SHI), which is predicated on the existence of two interacting scalar fields in the universe. In this framework, one scalar field is designated as the waterfall field, while the other serves as the inflaton responsible for driving inflation. The potential energy landscape of our model is characterized by three key variables, which we demonstrate can be constrained using observational data from sources such as WMAP7, Baryon Acoustic Oscillations (BAO), and the Hubble constant (H0). Our analysis yields a tensor-to-scalar ratio of r = 0.0055 +0.0005 -0.0004 at a 95% confidence level, a finding that aligns well with contemporary observational results. Additionally, we calculate the spectral index to be ns = 1.0145 +0.0025 -0.0024 at a 68% confidence level. These predictions for both the spectral index and the tensor-to-scalar ratio fall within one sigma of the latest experimental constraints, underscoring the viability of our model. The implications of SHI extend beyond mere theoretical constructs, as it provides a framework that can be tested against future observational data, potentially offering deeper insights into the dynamics of inflation and the fundamental nature of scalar fields in cosmology. Through this work, we aim to contribute to the ongoing discourse on inflationary models and their compatibility with empirical evidence, paving the way for further exploration in the field of cosmology.",
        "ori-fast-z-score": -0.7878385971583353,
        "water-fast-z-score": 1.2909944487358056,
        "rewrite-fast-z-score": -0.10846522890932808
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Super stellar clusters with a bimodal hydrodynamic solution: an Approximate Analytic Approach .\nAbstract:\nWe present the results of our study on super star clusters (SSCs) in which we have found that SSCs can be divided into two categories, namely, those having a single mode and those having a double-mode solution for their density profiles. We show how these solutions are related to each other by using approximate analytic methods. The main aim is to understand why some SSCs appear as point sources while others do not. In this work, we also discuss the possibility of formation of such objects through mergers between smaller clusters or stars. Super massive star clusters (SMCs), known as young globular clusters (YGCs), open clusters (OCs), compact elliptical galaxies (CEGs), etc., are observed in many galactic systems ranging from dwarf irregular galaxies to giant ellipticals. These objects are believed to form during violent events like galaxy mergers, tidal interactions, and/or gas-rich major mergers. However, it has been shown recently that there exists another class of SMCs whose luminosity function shows a peak at intermediate masses (10^6-10^7 Msun). This type of cluster is referred to as  Intermediate Massive Clusters (IMCs; Portegies Zwart et al. (2010)). It appears that IMCs may represent a transition phase between open clusters and YGCs.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Super stellar regions with a bimodal hydrodynamic solution : an Approximate Analytic Approach . Abstract : We present the results of our research on super galaxy clusters ( SSCs ) in which we have discovered that SSCs can be grouped into two genres , namely , those having a single mode and those having a double - mode solution for their density profiles .We see how these solutions are related to each other by using approximate analytic techniques . The main aim is to explain why some SSCs appear as point sources while many do not .In this research , we also discuss the idea of formation of such objects through mergers between smaller clusters or stars . Super massive star clusters ( SMCs ) , known as young globular galaxies ( YGCs ) , close complexes ( OCs ) , compact elliptical galaxies ( CEGs ) , etc . , are observed in large galactic structures ranging from giant irregular clusters to massive ellipticals .These bodies are known to form during violent reactions like galaxy mergers , tidal interactions , and / or gas - rich great mergers . However , it has been shown lately that there exists another class of SMCs whose luminosity function shows a peak at intermediate masses ( 10 ^ 6 - 10 ^ 7 Msun ) .This kind of cluster is referred to as Intermediate Massive Clusters ( IMCs ; Portegies Zwart et al . ( 2010 ) ) .It likely that IMCs might represent a change process between open complexes and YGCs .",
        "rewrite_text": "In this study, we explore the characteristics of super galaxy clusters (SSCs) and identify two distinct categories based on their density profiles: those exhibiting a single-mode solution and those displaying a bimodal solution. Our research employs approximate analytic methods to elucidate the relationship between these two types of solutions. A central objective of this investigation is to understand the reasons behind the appearance of certain SSCs as point sources, while others do not share this trait. Additionally, we delve into the formation mechanisms of these structures, proposing that they may arise from the mergers of smaller clusters or stellar systems.\n\nWe also examine super massive star clusters (SMCs), which are often referred to as young globular galaxies (YGCs), open complexes (OCs), and compact elliptical galaxies (CEGs). These entities are typically found within extensive galactic formations, ranging from large irregular clusters to substantial elliptical galaxies. The formation of these clusters is generally associated with dynamic events such as galaxy mergers, tidal interactions, and gas-rich major mergers. Recent findings have revealed a novel category of SMCs characterized by a luminosity function that peaks at intermediate masses, specifically between 10^6 and 10^7 solar masses. This category is designated as Intermediate Massive Clusters (IMCs), as noted by Portegies Zwart et al. (2010). We hypothesize that IMCs may represent a transitional phase between open complexes and young globular clusters. Our findings contribute to a deeper understanding of the structural diversity and evolutionary pathways of super stellar regions within the cosmos.",
        "ori-fast-z-score": -1.8,
        "water-fast-z-score": 4.477667355944951,
        "rewrite-fast-z-score": 0.18107149208503706
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Micro- and Macrorheological Properties of Isotropically Cross-linked Actin Networks .\nAbstract:\nWe have studied the rheology of isotropic networks formed by crosslinking actin filaments with two different concentrations of biotin-avidin linkers, using microrheology experiments on single filament dynamics in combination with macrorheology measurements performed at low frequencies (0.01-10 Hz). We find that both microand macro-rheology are consistent with an elastic network model for which we can extract values for the number density of links between filaments as well as their stiffness. The results show that increasing the concentration of avidin leads to denser networks with stiffer links. This effect is more pronounced when the initial concentration of actin filaments is higher. Our findings suggest that the mechanical properties of actomyosin gels may be tunable through changes in the amount and/or type of crosslinks present within these systems. In living cells, cytoskeletal structures such as stress fibers or focal adhesions provide physical connections between cell components and play important roles in determining cellular mechanics  1  . These structures consist of bundles of semiflexible biopolymers known as actin filaments  2  , which are connected together via specific protein complexes called crosslinks  3  .\nIn recent years there has been growing interest in understanding how the mechanical properties of biological materials depend on the microscopic structure of the underlying networks  4  . For example, it was shown recently that the viscoelasticity of reconstituted actomyosin gels depends strongly on the presence of myosins  5  . However, despite this progress our knowledge about the relationship between the macroscopic behavior of complex fluids and the microstructure of the constituent building blocks remains limited  6  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Micro - and Macrorheological Properties of Isotropically Cross - linked Actin Networks . Abstract : We have researched the rheology of isotropic networks formed by crosslinking actin filaments with two different amounts of biotin - avidin linkers , using microrheology experiments on double filament dynamics in combination with macrorheology measurements completed at low frequencies ( 0 . 01 - 10 Hz ) .We see that both microand macro - rheology are compatible with an elastic network theory for which we can extract parameters for the number density of links between filaments as also as their stiffness . The results show that raising the quantity of avidin leads to denser networks with stiffer links .This phenomenon is more pronounced when the first concentration of actin filaments is higher . Our findings show that the mechanical behavior of actomyosin gels might be tunable through alterations in the quantity and / or type of crosslinks observed within these systems .In living cells , cytoskeletal structures such as stress fibers or focal adhesions contribute physical bridges between tissue systems and play crucial roles in determining cellular dynamics 1 . These structures compose of bundles of semiflexible biopolymers known as actin filaments 2 , which are connected together via particular protein complexes called crosslinks 3 .In past decades there has been growing interest in understanding how the mechanical behavior of biological materials depend on the microscopic shape of the underlying systems 4 . For instance , it was shown lately that the viscoelasticity of reconstituted actomyosin gels relies highly on the presence of myosins 5 .However , despite this progress our information about the relationship between the macroscopic behavior of complex fluids and the microstructure of the constituent building components remains restricted 6 .",
        "rewrite_text": "**Title:** Micro- and Macrorheological Properties of Isotropically Cross-linked Actin Networks\n\n**Abstract:** This study investigates the rheological properties of isotropic networks created by crosslinking actin filaments using varying concentrations of biotin-avidin linkers. We employed microrheology techniques to analyze the dynamics of double filaments alongside macrorheology measurements conducted at low frequencies (0.01 - 10 Hz). Our findings indicate that both micro- and macrorheological behaviors align with an elastic network theory, allowing us to derive parameters related to the number density of crosslinks between filaments and their stiffness. Notably, increasing the amount of avidin results in denser networks characterized by stiffer linkages, with this effect being more pronounced at higher initial concentrations of actin filaments. These results suggest that the mechanical properties of actomyosin gels can be finely tuned by modifying the quantity and/or type of crosslinking agents present in the system. \n\nIn living cells, cytoskeletal structures such as stress fibers and focal adhesions serve as vital physical connections between tissue systems, significantly influencing cellular dynamics. These structures are composed of bundles of semiflexible biopolymers known as actin filaments, which are interconnected through specific protein complexes termed crosslinks. Over recent decades, there has been an increasing interest in elucidating how the mechanical behavior of biological materials is influenced by the microscopic architecture of their constituent components. Recent studies have highlighted that the viscoelastic properties of reconstituted actomyosin gels are heavily dependent on the presence of myosin proteins. However, despite these advancements, our understanding of the correlation between the macroscopic behavior of complex fluids and the microstructural characteristics of their building blocks remains limited. This research contributes to bridging that gap, offering insights into the tunability of actin networks and their implications for cellular mechanics.",
        "ori-fast-z-score": -0.3779644730092272,
        "water-fast-z-score": 7.118052168020874,
        "rewrite-fast-z-score": 1.5992254762521154
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Intrinsically X-ray Weak Quasar PHL 1811. II. Optical and UV Spectra and Analysis .\nAbstract:\nWe present new optical and ultraviolet (UV) spectra for the intrinsically X-ray weak quasar PHL 1811, obtained with the Keck Observatory s HIRES spectrograph in 1998-99. The data cover wavelengths between 3200 A and 10400Å at resolution R = λ/∆λ ≈ 45000. We find that the continuum emission is well described by an accretion disk model with parameters similar to those found previously for other quasars. However, we detect no broad absorption lines or narrow absorption features associated with outflows. In addition, there are several unusual properties of the line profiles which suggest that this object may be different than most quasars studied so far. \n \n Keywords: Quasars; Broad Absorption Lines; Accretion Disk Modeling. 1 Introduction \n \n PHL 1811 was discovered as part of the Palomar-Green survey (Schmidt & Green 1983 ) and has been observed extensively since then. It is one of only two known examples of an X-ray weak quasar (Wilkes et al. 1994) , where the ratio of its soft X-ray flux density to its 2500 Å flux density is less than 0.1. Wilkes et al. (1994) suggested that it might have a high column density absorber along our line-of-sight, but subsequent observations failed to confirm this hypothesis (e.g., Mathur et al. 1995) . Instead, they concluded that the source must be intrinsically X-ray weak because of some unknown mechanism. Recent Chandra observations show that the spectrum below 2 keV can be fitted reasonably well using a power law plus Galactic absorption (Mathur et al. 2002 ) . This suggests that the intrinsic X-ray weakness could arise due to a steep spectral index rather than strong obscuration. Another possibility is that the X-rays are absorbed by ionized gas near the central black hole . \n \n PHL 1811 also shows interesting variability on time scales ranging from hours to years. For example, Wilkes et al. (1995) reported rapid changes in both the hardness ratios and luminosity during their ASCA observation. They interpreted these variations as being caused by partial",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  The Intrinsically X-ray Weak Quasar PHL 1811.II.Optical and UV Spectra and Analysis . Abstract : We create latest laser and ultraviolet ( UV ) spectra for the intrinsically X - ray weak quasar PHL 1811 , obtained with the Keck Observatory s HIRES spectrograph in 1998 - 99 .The data cover wavelengths between 3200 A and 10400Å at resolution R = λ / [UNK] ≈ 45000 . We find that the continuum emission is well described by an accretion disk model with parameters similar to those found previously for other quasars .However , we find no broad absorption patterns or broad absorption elements identified with outflows . In addition , there are several unique properties of the line profiles which show that this body may be unique than most quasars explored so far .Keywords : Quasars ; Broad Absorption Lines ; Accretion Disk Modeling . 1 Introduction PHL 1811 was studied as part of the Palomar - Green survey ( Schmidt & Green 1983 ) and has been observed extensively since then .It is one of only two recorded examples of an X - ray weak quasar ( Wilkes et al . 1994 ) , where the proportion of its warm X - ray flux coefficient to its 2500 Å flux density is fewer than 0 . 1 .Wilkes et al . ( 1994 ) proposed that it could have a high column density absorber along our line - of - seeing , but subsequent observations failed to confirm this hypothesis ( e . g . , Mathur et al .1995 ) . Instead , they concluded that the origin could be intrinsically X - ray weak because of some unidentified system .Recent Chandra measurements show that the spectrum below 2 keV can be fit reasonably well using a power law plus Galactic absorption ( Mathur et al . 2002 ) .This implies that the intrinsic X - ray weak could occur due to a sharp spectral index instead than strong obscuration . Another possibility is that the X - rays are absorption by ionized gas near the inner brown hole .PHL 1811 also shows interesting variability on time ranges varied from hours to years . For instance , Wilkes et al .( 1995 ) found sudden differences in both the hardness ratios and luminosity during their ASCA measurement . They interpreted these changes as being affected by partial",
        "rewrite_text": "**Title:** The Intrinsically X-ray Weak Quasar PHL 1811: II. Optical and UV Spectra and Analysis\n\n**Abstract:** In this study, we present the latest optical and ultraviolet (UV) spectra of the intrinsically X-ray weak quasar PHL 1811, acquired using the HIRES spectrograph at the Keck Observatory during the years 1998-1999. The spectral data spans wavelengths from 3200 Å to 10400 Å with a resolution of approximately R = λ / Δλ ≈ 45000. Our analysis reveals that the continuum emission from PHL 1811 can be effectively modeled using an accretion disk framework, with parameters that align closely with those observed in other quasars. Notably, we do not detect any broad absorption features or elements typically associated with outflows, suggesting that PHL 1811 may exhibit distinct characteristics compared to the majority of quasars studied to date. \n\nPHL 1811 was initially identified in the Palomar-Green survey (Schmidt & Green, 1983) and has since been the subject of extensive observational campaigns. It stands out as one of only two known examples of an X-ray weak quasar (Wilkes et al., 1994), characterized by a warm X-ray flux coefficient that is less than 0.1 relative to its flux density at 2500 Å. While Wilkes et al. (1994) proposed the presence of a high column density absorber along our line of sight, subsequent studies (e.g., Mathur et al., 1995) did not support this hypothesis. Instead, they suggested that the quasar's intrinsic X-ray weakness might stem from an unidentified mechanism. Recent observations from Chandra indicate that the spectrum below 2 keV can be adequately described by a power law combined with Galactic absorption (Mathur et al., 2002), implying that the intrinsic X-ray weakness may result from a steep spectral index rather than significant obscuration. Another possibility is that the X-rays are being absorbed by ionized gas in proximity to the central black hole. Furthermore, PHL 1811 exhibits intriguing variability over timescales ranging from hours to years, with Wilkes et al. (1995) noting abrupt changes in hardness ratios and luminosity during their ASCA observations, which they attributed to partial obscuration effects. \n\n**Keywords:** Quasars; Broad Absorption Lines; Accretion Disk Modeling.",
        "ori-fast-z-score": -1.600088650026386,
        "water-fast-z-score": 6.713450866373513,
        "rewrite-fast-z-score": 0.42717882885838043
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Intrinsic ripples in graphene .\nAbstract:\nThe electronic structure and transport properties of intrinsic ripples on single-layered graphene are investigated by using the tight-binding model with first-principles hopping parameters, which is based on density-functional theory (DFT). The results show that the ripple-induced local strain can lead to an energy gap opening at Dirac point for both armchair and zigzag directions. In addition, it is found that the electron mobility decreases as the amplitude increases due to the increase of scattering centers induced by the ripple structures. These findings may be useful for understanding the physical mechanism behind the experimental observations of rippling effects on the electrical performance of graphene devices. Graphene has attracted great attention because its unique two-dimensional honeycomb lattice structure leads to many novel phenomena such as high carrier mobility  1  , Klein tunneling  2  , valley polarization  3  , etc.. However, recent experiments have shown that the pristine flat monolayer graphene sheets tend to form ripples spontaneously  4  . It was also reported that these ripples could affect the electrical performance of graphene-based devices  5  .\nIn this work we investigate the effect of ripples on the electronic structure and transport properties within the framework of the tight-binding model  6  . We find that the ripple-induced strains can open up an energy gap around the Fermi level for both armchair andzigzag directions. Moreover, the electron mobility decreases as increasing the amplitude of ripples since more scattering centers are introduced into the system.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Intrinsic ripples in graphene . Abstract : The mechanical composition and transport properties of intrinsic ripples on single - layered graphene are examined by using the tight - binding model with first - principles hopping characteristics , which is based on density - functional theory ( DFT ) .The results show that the ripple - caused local tension can lead to an energy gap opening at Dirac position for both armchair and zigzag directions . In addition , it is found that the electron mobility decreases as the frequency rises due to the increase of absorption centers caused by the ripple structures .These conclusions could be valuable for studying the physical process behind the empirical observations of rippling influence on the electrical performance of graphene materials . Graphene has garnered great attention because its unique two - dimensional honeycomb lattice structure leads to many novel processes such as high carrier density 1 , Klein tunneling 2 , valley polarization 3 , etc . .However , recent experiments have shown that the pristine rolled monolayer graphene strips tend to form ripples spontaneously 4 . It was also reported that these ripples could affect the electrical performance of graphene - based equipment 5 .In this research we investigate the impact of ripples on the electronic structure and transport properties within the framework of the tight - binding model 6 . We see that the ripple - induced strains can offer up an energy gap around the Fermi level for both armchair andzigzag directions .Moreover , the electron mobility decreases as increasing the frequency of ripples since more scattering centers are introduced into the system .",
        "rewrite_text": "**Title: Intrinsic Ripples in Graphene**\n\n**Abstract:** This study delves into the mechanical composition and transport characteristics of intrinsic ripples present in single-layer graphene, utilizing a tight-binding model informed by first-principles hopping parameters derived from density functional theory (DFT). Our findings reveal that the local tension induced by ripples can result in the opening of an energy gap at the Dirac point, affecting both armchair and zigzag orientations. Furthermore, we observe that as the frequency of the ripples increases, electron mobility diminishes due to the proliferation of scattering centers introduced by the ripple structures. These insights are crucial for understanding the physical mechanisms underlying the observed effects of rippling on the electrical performance of graphene materials. \n\nGraphene has attracted significant interest due to its remarkable two-dimensional honeycomb lattice, which facilitates a variety of unique phenomena, including high carrier density, Klein tunneling, and valley polarization. However, recent experimental observations indicate that pristine monolayer graphene tends to develop ripples spontaneously. These ripples have been shown to influence the electrical properties of graphene-based devices. In this research, we systematically investigate the effects of these ripples on the electronic structure and transport properties of graphene, employing the tight-binding model framework. Our analysis demonstrates that the strains induced by ripples can create an energy gap near the Fermi level for both armchair and zigzag configurations. Additionally, we highlight that the increase in ripple frequency correlates with a reduction in electron mobility, attributable to the introduction of additional scattering centers within the material. These findings contribute to a deeper understanding of how intrinsic ripples can modulate the electronic behavior of graphene, with implications for its application in advanced electronic devices.",
        "ori-fast-z-score": -0.09166984970282113,
        "water-fast-z-score": 6.337502222976297,
        "rewrite-fast-z-score": 0.17677669529663687
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Sloan Digital Sky Survey Quasar Catalog IV. Fifth Data Release .\nAbstract:\nThe Sloan Digital Sky Survey (SDSS) is an ongoing project to map the distribution and motion of galaxies, quasars, stars, and other celestial objects in space. The fourth data release was made public on September 30th 2003. This fifth data release contains more than 100,000 new quasar candidates selected by color criteria from the SDSS imaging survey. These are supplemented with about 20,000 previously known quasars that were not included in previous releases because they did not meet the selection criteria for inclusion at that time. \n \n In addition to these newly discovered quasars, this catalog also includes all quasars found during the first four years of the survey as well as those found since then but which have yet to be released publicly. A total of over 250,000 quasars are now available through this catalog. All of them have been spectroscopically confirmed using observations obtained with the dedicated 2.5-meter telescope located at Apache Point Observatory near Sacramento, California.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  The Sloan Digital Sky Survey Quasar Catalog IV.Fifth Data Release .Abstract : The Sloan Digital Sky Survey ( SDSS ) is an continuing work to map the distribution and motion of galaxies , quasars , stars , and other celestial entities in space . The fourth information publication was making public on September 30th 2003 .This fifth information update contains more than 100 , 000 new quasar finalists chosen by color factors from the SDSS imaging survey . These are supplemented with about 20 , 000 former named quasars that were not included in earlier versions because they did not meet the selection standards for inclusion at that point .In addition to these newly discovered quasars , this catalog also contains all quasars discovered during the first four seasons of the census as well as those identified since then but which have never to be available openly . A total of over 250 , 000 quasars are now released through this catalog .All of them have been spectroscopically confirmed using observations collected with the dedicated 2 . 5 - meter telescope located at Apache Point Observatory near Sacramento , California .",
        "rewrite_text": "**Title:** The Sloan Digital Sky Survey Quasar Catalog IV: Fifth Data Release\n\n**Abstract:** The Sloan Digital Sky Survey (SDSS) represents a significant ongoing effort to comprehensively map the distribution and dynamics of various celestial objects, including galaxies, quasars, stars, and other astronomical phenomena. The fourth data release of this extensive survey was made available to the public on September 30, 2003. The fifth data release introduces over 100,000 new quasar candidates, meticulously selected based on their color characteristics derived from the SDSS imaging survey. This release also incorporates approximately 20,000 previously identified quasars that were not included in earlier catalogs due to not meeting the initial selection criteria.\n\nIn addition to the newly identified quasars, this catalog encompasses all quasars discovered during the first four years of the survey, along with those identified in subsequent observations that had not been publicly released until now. As a result, the total number of quasars available in this catalog exceeds 250,000. Each quasar included in this extensive dataset has undergone rigorous spectroscopic confirmation, utilizing data collected from the dedicated 2.5-meter telescope situated at Apache Point Observatory, located near Sacramento, California.\n\nThis comprehensive catalog serves as a vital resource for researchers in the field of astrophysics, providing a wealth of data for the study of quasar properties, their distribution in the universe, and their implications for understanding cosmic evolution. The ongoing updates and expansions of the SDSS quasar catalog reflect the commitment to advancing our knowledge of the universe and the dynamic processes that govern it.",
        "ori-fast-z-score": -1.7801724872907798,
        "water-fast-z-score": 4.478342947514801,
        "rewrite-fast-z-score": -1.5118578920369088
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Microscopic Origin of Non-Gaussian Distributions of Financial Returns .\nAbstract:\nWe show that the non-Gaussian distributions observed in financial returns are due to microscopic interactions between traders and their environment, which lead to non-trivial correlations among different time scales. We present an analytical model for these correlations based on random matrix theory (RMT), which is able to reproduce all statistical properties of real market data with high accuracy. \n \n The distribution of stock prices has been studied extensively over many decades  1  . It was found that this distribution can be well approximated by a Gaussian function  2  , but deviations have also been reported  3  . In particular, it has been shown recently  4  that the tails of the return distribution follow a power law decay P(r) ~ r−α with α = 3 ± 0.1. This finding contradicts the predictions of standard models such as Black-Scholes  5  or Heston  6  , where the tail exponent should be equal to 2  7, 8  .\n \nIn order to explain the origin of these deviations we propose here a new approach based on Random Matrix Theory  9  . RMT describes the statistics of complex systems whose dynamics depend on a large number N of degrees of freedom  10  . For example, RMT has successfully been applied to describe the fluctuations of energy levels  11  , wave functions  12  , spin states  13  , quantum transport  14  , and chaotic scattering  15  . Recently, RMT has also been used to study the statistical properties of stock markets  16  -  22  . Here we will focus on the so-called Dyson Brownian motion  23  , which describes the evolution of a system under the influence of white noise.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Microscopic Origin of Non - Gaussian Distributions of Financial Returns . Abstract : We see that the non - Gaussian distributions found in financial returns are owing to microscopic interactions between traders and their environment , which lead to non - simple correlations among different time ranges .We present an analytical theory for these correlations based on random matrix theory ( RMT ) , which is easy to capture all statistical characteristics of real stock data with high clarity . The distribution of market prices has been studied frequently over numerous years 1 .It was shown that this distribution can be well approximated by a Gaussian function 2 , but deviations have also been reported 3 . In particular , it has been shown recently 4 that the tails of the return distribution take a power law decay P ( r ) ~ r−α with α = 3 ± 0 . 1 .This found contradicts the assumptions of standard models such as Black - Scholes 5 or Heston 6 , where the tail exponent should be equal to 2 7 , 8 . In order to explain the origin of these deviations we undertake here a new approach based on Random Matrix Theory 9 .RMT describes the statistics of complex systems whose dynamics depend on a large number N of degrees of freedom 10 . For instance , RMT has successfully been used to explain the fluctuations of energy levels 11 , wave systems 12 , spin states 13 , quantum transport 14 , and chaotic scattering 15 .Recently , RMT has additionally been used to study the empirical features of stock markets 16 - 22 . Here we will focus on the so - called Dyson Brownian moving 23 , which explains the evolution of a system under the impact of white sound .",
        "rewrite_text": "**Title:** Microscopic Origin of Non-Gaussian Distributions of Financial Returns\n\n**Abstract:** This study investigates the underlying microscopic interactions between traders and their environment that give rise to non-Gaussian distributions in financial returns. We demonstrate that these interactions result in complex correlations across various time scales, which are not captured by traditional financial models. To address this, we propose an analytical framework grounded in Random Matrix Theory (RMT), which effectively encapsulates the statistical properties of actual stock market data with remarkable precision. The distribution of market prices has been a subject of extensive research over the years, with earlier studies suggesting that it could be approximated by a Gaussian function. However, recent findings indicate significant deviations from this norm, particularly in the tails of the return distribution, which exhibit a power-law decay characterized by P(r) ~ r^(-α) with α approximately equal to 3 ± 0.1. This observation stands in stark contrast to the predictions of conventional models such as Black-Scholes and Heston, which assert that the tail exponent should be 2. To elucidate the origins of these discrepancies, we adopt a novel approach utilizing Random Matrix Theory, which is adept at analyzing complex systems influenced by a multitude of degrees of freedom. RMT has previously been applied to various fields, including the fluctuations of energy levels, wave systems, spin states, quantum transport, and chaotic scattering. More recently, it has been employed to explore the empirical characteristics of stock markets. In this paper, we specifically focus on the Dyson Brownian motion framework, which provides insights into the dynamics of a system influenced by white noise. Our findings contribute to a deeper understanding of the statistical behavior of financial returns and challenge existing paradigms in financial modeling.",
        "ori-fast-z-score": -1.2888044650576527,
        "water-fast-z-score": 4.74692883171144,
        "rewrite-fast-z-score": -1.2004900959975617
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  On the Sensitivity of Massive Star Nucleosynthesis and Evolution to Solar Abundances and to Uncertainties in Helium Burning Reaction Rates .\nAbstract:\nWe present new calculations for massive star evolution, nucleosynthesis, and yields using updated nuclear reaction rates and solar abundances. We find that our results are sensitive to uncertainties in the helium burning rate at high temperatures (T > 2 x 10^9 K). The sensitivity is particularly strong when we use the most recent recommended value for the 12C(alpha, gamma)16O cross section. This result has important implications for studies of chemical enrichment by supernovae Ia progenitors. \n \n Keywords: Nuclear reactions; Supernovae; Stellar evolution; Yields \n \n 1 Introduction \n \n In this work we study how uncertainties in nuclear physics affect predictions about stellar evolution and nucleosynthesis. Our goal is to understand better what can be learned from observations of stars and their remnants. For example, it is well known that there exist large discrepancies between observed elemental abundance ratios in metal-poor halo stars and those predicted by standard models of galactic chemical evolution  1  . These differences may arise because some key nuclear processes have been poorly understood or not included in current evolutionary codes  2  , but they could also reflect systematic errors in observational data  3  .\n \nIn order to address these issues, we perform detailed numerical simulations of massive star evolution with different sets of input parameters. Specifically, we consider two cases where the initial mass fraction of helium XHe = 0.25 and 0.30 respectively  4  . We evolve each model until its core collapses into a neutron star. During the collapse phase, we follow the hydrodynamics of the explosion as described in  5  . Afterwards, we compute the composition of the ejecta using an improved version  6  of the one-dimensional post-processing code developed originally by  7  . \n \n 2 Input Physics and Numerical Methods",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : On the Sensitivity of Massive Star Nucleosynthesis and Evolution to Solar Abundances and to Uncertainties in Helium Burning Reaction Rates . Abstract : We present new calculations for huge star evolution , nucleosynthesis , and yields using updated atomic reaction rates and solar abundances .We see that our findings are susceptible to uncertainties in the helium burning rate at high levels ( T > 2 x 10 ^ 9 K ) . The sensitivity is especially powerful when we using the most current recommended estimate for the 12C ( beta , alpha ) 16O cross area .This result has significant implications for research of chemical enrichment by supernovae Ia progenitors . Keywords : Nuclear processes ; Supernovae ; Stellar evolution ; Yields 1 Introduction In this research we study how uncertainties in nuclear science affect calculations about stellar evolution and nucleosynthesis .Our goal is to realize best what can be learned from measurements of stars and their remnants . For instance , it is well established that there exist large discrepancies between measured elemental density levels in metal - poor halo stars and those predicted by typical models of galactic chemical evolution 1 .These changes may arise because some important radioactive processes have been poorly described or not incorporated in current evolutionary codes 2 , but they may also reflect widespread errors in observational data 3 . In order to meet these problems , we perform comprehensive numerical simulations of large star evolution with various sets of input parameters .Specifically , we study two situations where the first mass fraction of helium XHe = 0 . 25 and 0 . 30 respectively 4 . We evolve each model until its core collapses into a neutron star .During the failure phase , we follow the hydrodynamics of the explosion as described in 5 . Afterwards , we compute the composition of the ejecta using an updated model 6 of the one - dimensional post - processing language developed originally by 7 .2 Input Physics and Numerical Methods",
        "rewrite_text": "**Title:** On the Sensitivity of Massive Star Nucleosynthesis and Evolution to Solar Abundances and to Uncertainties in Helium Burning Reaction Rates\n\n**Abstract:** This study presents new calculations regarding the evolution and nucleosynthesis of massive stars, incorporating updated atomic reaction rates and solar abundance data. Our results indicate a significant sensitivity to uncertainties in the helium burning reaction rates, particularly at elevated temperatures (T > 2 x 10^9 K). This sensitivity is most pronounced when utilizing the latest recommended estimates for the cross-section of the reaction 12C(β, α)16O. These findings carry important implications for understanding the chemical enrichment processes associated with Type Ia supernova progenitors. \n\nIn this research, we investigate how uncertainties in nuclear physics impact the modeling of stellar evolution and nucleosynthesis. Our objective is to enhance our understanding of what can be inferred from observations of stars and their remnants. Notably, we observe substantial discrepancies between the elemental abundances measured in metal-poor halo stars and those predicted by conventional models of galactic chemical evolution. These discrepancies may stem from inadequately described or omitted radioactive processes in existing evolutionary models, as well as potential inaccuracies in observational data.\n\nTo address these challenges, we conduct extensive numerical simulations of massive star evolution, varying the input parameters to explore different scenarios. Specifically, we analyze two cases with initial helium mass fractions of XHe = 0.25 and XHe = 0.30. Each model is evolved until core collapse occurs, leading to the formation of a neutron star. During the collapse phase, we meticulously track the hydrodynamics of the ensuing explosion. Subsequently, we calculate the composition of the ejecta using an updated one-dimensional post-processing framework, originally developed for this purpose. Our findings underscore the critical role of precise nuclear reaction rates and solar abundances in accurately modeling the life cycles of massive stars and their contributions to cosmic chemical evolution. \n\n**Keywords:** Nuclear processes; Supernovae; Stellar evolution; Yields.",
        "ori-fast-z-score": -0.9330078226479681,
        "water-fast-z-score": 7.326721713278715,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Stellar Population Models and Individual Element Abundances I: Sensitivity of Stellar Evolution Models .\nAbstract:\nWe present the results of our investigation into how stellar evolution models are affected by individual element abundances in stars, focusing on the sensitivity to changes in helium abundance (Y). We use two different sets of evolutionary tracks with varying Y values for masses between 0.8 M⊙ and 8 M⊙ at solar metallicity. The first set is based on the Padova code while the second one uses the Geneva code. For each track we calculate synthetic spectra using the SPECTRUM code. These synthetic spectra are then used as input to determine the best-fit parameters of observed high-resolution optical spectra of Galactic open clusters. Our analysis shows that both codes produce similar results when fitting these cluster data. However, there are significant differences in the derived ages depending on which code was used. This discrepancy can be explained by the fact that the Padova tracks have been calculated without convective overshooting whereas the Geneva tracks include this effect.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Stellar Population Models and Individual Element Abundances I : Sensitivity of Stellar Evolution Models . Abstract : We present the conclusion of our inquiry into how stellar evolution models are influenced by individual atom abundances in stars , concentrating on the sensitivity to changes in helium abundance ( Y ) .We use two different sets of evolutionary tracks with varying Y values for masses between 0 . 8 [UNK] and 8 [UNK] at solar metallicity . The first setting is based on the Padova code while the second one uses the Geneva code .For each track we determine natural spectra using the SPECTRUM code . These synthetic spectra are then utilized as input to obtain the best - fitting characteristics of measured high - resolution optical spectra of Galactic open clusters .Our study shows that both codes produce comparable results when fitting these cluster data . However , there are significant variations in the derived ages varying on which coding was used .This discrepancy can be reason by the fact that the Padova lines have been measured without convective overshooting whereas the Geneva tracks include this effect .",
        "rewrite_text": "We present the findings of our investigation into the impact of individual elemental abundances on stellar evolution models, with a particular focus on the sensitivity to variations in helium abundance (Y). Our analysis employs two distinct sets of evolutionary tracks, each characterized by different Y values, for stellar masses ranging from 0.8 to 8 solar masses at solar metallicity. The first set of evolutionary tracks is derived from the Padova code, while the second is based on the Geneva code. To analyze the effects of these variations, we generate synthetic spectra for each evolutionary track using the SPECTRUM code. These synthetic spectra serve as inputs for determining the optimal fitting parameters of high-resolution optical spectra obtained from Galactic open clusters.\n\nOur results indicate that both the Padova and Geneva codes yield comparable outcomes when fitting the observational data from these clusters. However, we observe notable discrepancies in the estimated ages of the stars, which are contingent upon the evolutionary code employed. This variation in age estimates can be attributed to the differing methodologies used in the two codes; specifically, the Padova tracks are calculated without accounting for convective overshooting, whereas the Geneva tracks incorporate this phenomenon. This study highlights the importance of considering individual element abundances and the choice of evolutionary models in accurately determining stellar ages and characteristics. The implications of our findings are significant for the field of stellar astrophysics, as they underscore the need for careful selection of models when interpreting observational data and understanding stellar populations.",
        "ori-fast-z-score": 1.4444444444444444,
        "water-fast-z-score": 5.963302408041713,
        "rewrite-fast-z-score": -1.0083683467310325
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Minimal size of a barchan dune .\nAbstract:\nWe study the minimal size of a barchan, which is one of the most common types of sand dunes in nature. We show that this problem can be formulated as an optimal control problem for a nonlinear partial differential equation with nonlocal boundary conditions and prove existence of solutions by using variational methods. The numerical results are obtained by applying finite element method to discretize the state equations and then solving them by Newton s iteration scheme. Finally we present some examples illustrating our theoretical findings. Sand dunes are among the most beautiful natural phenomena on Earth. They have been studied extensively since the 19th century  1  . One of the simplest forms of sand dunes is called barchan  2  , see Figure 1 (a). It has a crescent shape with its horns pointing away from the wind direction. Barchans occur naturally over large areas around the world such as Australia  3  , Namibia  4  , Saudi Arabia  5  , China  6  , Japan  7  , etc.. In recent years there has been growing interest in studying mathematical models describing formation of sand dunes  8, 9, 10  .\nIn this work we consider the following model proposed by Kroy et al  11  : \nwhere u(x) denotes the height of the sand bed at position x ∈ Ω =  0, L  × R + ; f > 0 represents the rate of deposition; g ≥ 0 stands for the erosion coefficient; h(u) describes the effect of surface tension; p(x), q(x) represent the pressure terms due to gravity and friction respectively; α > 0 measures the strength of the wind blowing along x-axis; β > 0 characterizes the resistance against the flow of air; γ > 0 is related to the cohesion between grains of sand; θ is the angle of repose of sand particles; c > 0 is the constant volume fraction of sand per unit area; finally, n is the outward normal vector to the boundary Γ = {0 < x < L} × {0} ∪ {L} × R + . For more details about physical meaning of parameters involved in system (1) , please refer to  12  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Minimal size of a barchan dune . Abstract : We consider the minimal size of a barchan , which is one of the most common kinds of dunes dunes in nature .We see that this question can be formulated as an optimal control problem for a nonlinear partial differential function with nonlocal boundary constraints and prove existence of solutions by using variational techniques . The mathematical findings are derived by using finite element method to discretize the state equations and then solving them by Newton s iteration scheme .Finally we present some examples illustrating our theoretical results . Sand dunes are among the most beautiful natural creatures on Earth .They have been studied thoroughly since the 19th century 1 . One of the simplest forms of dunes dunes is known barchan 2 , see Figure 1 ( a ) .It has a crescent shape with its horns pointing away from the wind position . Barchans occur commonly over large areas around the world such as Australia 3 , Namibia 4 , Saudi Arabia 5 , China 6 , Japan 7 , etc . .In recent recently there has been growing interest in studying numerical models governing formation of dunes dunes 8 , 9 , 10 . In this study we consider the following model proposed by Kroy et al 11 : where u ( x ) denotes the height of the sand bed at position x ∈ Ω = 0 , L × R + ; f > 0 represents the speed of deposition ; g ≥ 0 stands for the erosion factor ; h ( u ) refers the impact of surface friction ; p ( x ) , q ( x ) describe the pressure terms due to gravity and tension respectively ; α > 0 measures the strength of the wind blowing along x - axis ; β > 0 characterizes the tolerance against the movement of air ; γ > 0 is related to the cohesion between particles of dunes ; θ is the angle of repose of sand grains ; c > 0 is the constant volume fraction of dunes per unit area ; finally , n is the outward normal vector to the boundary Γ = { 0 < x < L } × { 0 } ∪ { L } × R + .For more details about physical context of components involved in system ( 1 ) , please refer to 12 .",
        "rewrite_text": "**Title: Minimal Size of a Barchan Dune**\n\n**Abstract:** This study investigates the minimal size of barchan dunes, which are among the most prevalent dune formations found in nature. We approach this problem by framing it as an optimal control issue involving a nonlinear partial differential equation with nonlocal boundary conditions. Utilizing variational techniques, we establish the existence of solutions to this mathematical challenge. To analyze the state equations, we employ the finite element method for discretization, followed by the application of Newton's iteration scheme for solving the resulting system. Our findings are supported by various examples that illustrate the theoretical results obtained.\n\nBarchan dunes, characterized by their distinctive crescent shape with horns oriented away from the wind direction, are not only visually striking but also have been the subject of extensive research since the 19th century. These formations are prevalent in diverse regions worldwide, including Australia, Namibia, Saudi Arabia, China, and Japan. Recently, there has been an increasing interest in developing numerical models that describe the dynamics of dune formation. In this context, we focus on a model proposed by Kroy et al., which incorporates several key parameters: the height of the sand bed denoted by u(x) at position x within the domain Ω = (0, L) × R⁺; a positive deposition speed f; an erosion factor g; the influence of surface friction h(u); pressure terms p(x) and q(x) accounting for gravity and tension; and various other parameters that characterize wind strength (α), air movement tolerance (β), particle cohesion (γ), the angle of repose (θ), and dune volume fraction (c). The outward normal vector to the boundary is represented by n, defined on the boundary Γ = {(0 < x < L) × {0}} ∪ {L} × R⁺. For a comprehensive understanding of the physical context surrounding the components of the system, we refer to previous literature.",
        "ori-fast-z-score": -1.365472859134248,
        "water-fast-z-score": 5.4443572293729625,
        "rewrite-fast-z-score": 0.6531972647421809
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Thouless-Anderson-Palmer equation for analog neural network with temporally fluctuating white synaptic noise .\nAbstract:\nWe study the dynamics of an analog recurrent neural network (RNN) driven by temporally fluctuating white noise in its synapses, and show that it can be described by the Thouless-AndersonPalmer (TAP) equations. We derive these TAP equations using replica theory applied to the RNN s partition function. The resulting TAP equations are exact up to corrections which vanish exponentially fast as the number N of neurons increases. In particular we find that the TAP free energy is given by F = −kBTN ln ZN + O(1/N2), where kB denotes Boltzmann s constant and TN ≡ 1/ZN is the temperature associated with the fluctuations in the synapses. This result shows that the TAP approach provides a good description even when the system size is small compared to the typical correlation length of the noise. Finally, we use our results to calculate the stationary state distribution of the RNN and compare this prediction against numerical simulations.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Thouless - Anderson - Palmer equation for analog neural network with temporally fluctuating white synaptic noise . Abstract : We research the dynamics of an analog recurrent brain system ( RNN ) driven by temporally fluctuating white sound in its synapses , and find that it can be described by the Thouless - AndersonPalmer ( TAP ) equations .We derive these TAP equations using replica theory applied to the RNN s partition function . The resulting TAP equations are exact up to corrections which vanish exponentially rapidly as the number N of neurons increases .In particular we find that the TAP free energy is given by F = −kBTN ln ZN + O ( 1 / N2 ) , where kB indicates Boltzmann s constant and TN ≡ 1 / ZN is the temperature associated with the fluctuations in the synapses . This result suggests that the TAP solution provides a better characterization even when the system width is tiny contrast to the typical correlation length of the noise .Finally , we using our findings to estimate the discrete state distribution of the RNN and compare this forecast against numerical simulations .",
        "rewrite_text": "In this study, we investigate the dynamics of an analog recurrent neural network (RNN) influenced by temporally fluctuating white noise in its synaptic connections. Our analysis reveals that the behavior of this system can be effectively described by the Thouless-Anderson-Palmer (TAP) equations. To derive these equations, we employ replica theory in conjunction with the partition function of the RNN. The TAP equations we obtain are shown to be exact, with corrections that diminish exponentially as the number of neurons, denoted as N, increases. Notably, we find that the TAP free energy can be expressed as F = −k_B T_N ln Z_N + O(1/N²), where k_B represents Boltzmann's constant and T_N = 1/Z_N is the temperature linked to the synaptic fluctuations. This finding indicates that the TAP solution offers a more accurate characterization of the system, even when the width of the system is significantly smaller than the typical correlation length of the noise. Furthermore, we utilize our results to estimate the discrete state distribution of the RNN and conduct a comparison between our theoretical predictions and numerical simulations. This work not only enhances our understanding of the dynamics of RNNs under the influence of synaptic noise but also provides a framework for further exploration of neural network behavior in fluctuating environments.",
        "ori-fast-z-score": -1.5215349135496974,
        "water-fast-z-score": 3.117691453623979,
        "rewrite-fast-z-score": 0.329292779969071
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Effects of the galactic winds on the stellar metallicity distribution of dwarf spheroidal galaxies .\nAbstract:\nWe study how the effects of galactic winds can be used to explain the observed properties of the metal-poor tail in the stellar metallicity distributions (SMDs) of nearby dwarf spheroidal galaxies (dSph). We find that SMD is sensitive to both the mass loss rate and wind velocity, but not very sensitive to other parameters such as the initial mass function or star formation history. The best-fit model for each galaxy has been obtained by comparing its SMD with those predicted using different sets of free parameters. Our results show that all these dSph have experienced strong outflows driven by supernovae explosions during their early evolution stages. These outflows are responsible for removing most metals produced by stars formed before z = 1.5-2.0. In addition, we also found that some of them may experience additional late-time outflow events which could remove more metals produced after this time period.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Effects of the galactic winds on the stellar metallicity distribution of dwarf spheroidal galaxies . Abstract : We research how the effects of galactic winds can be used to explain the observed properties of the metal - weak tail in the stellar metallicity distributions ( SMDs ) of neighbouring dwarf spheroidal galaxies ( dSph ) .We see that SMD is sensitive to both the mass loss rate and wind velocity , but not very sensitive to other parameters such as the early mass value or star formation history . The best - fitting model for each galaxy has been achieved by comparing its SMD with those predicted use different sets of free parameters .Our results show that all these dSph have experienced strong outflows driven by supernovae explosions during their early evolved stages . These outflows are responsible for eliminating most metals produced by stars formed before z = 1 . 5 - 2 . 0 .In addition , we also discovered that some of them may experience additional late - time outflow events which could remove more metals produced after this time time .",
        "rewrite_text": "In this study, we investigate the influence of galactic winds on the stellar metallicity distribution (SMD) of nearby dwarf spheroidal galaxies (dSph). Our research focuses on how these winds contribute to the formation of the metal-poor tail observed in the SMDs of these galaxies. We find that the SMD is particularly sensitive to parameters such as the mass loss rate and wind velocity, while it shows less sensitivity to other factors, including the initial mass and the history of star formation. To identify the optimal model for each galaxy, we conducted a comparative analysis between the observed SMDs and those predicted using various sets of free parameters. The findings indicate that all examined dSph galaxies have undergone significant outflows, primarily driven by supernova explosions during their early evolutionary phases. These outflows have played a crucial role in expelling a substantial amount of metals produced by stars that formed prior to redshift z = 1.5 - 2.0. Furthermore, our research reveals that some dSph galaxies may have experienced additional outflow events at later stages, which could further deplete the metallic content by removing metals generated after this epoch. This study enhances our understanding of the interplay between galactic winds and stellar evolution in dwarf spheroidal galaxies, providing insights into the processes that shape their metallicity distributions.",
        "ori-fast-z-score": 1.078327732034384,
        "water-fast-z-score": 4.824506406770077,
        "rewrite-fast-z-score": 0.29851115706299675
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Bond formation and slow heterogeneous dynamics in adhesive spheres with long--ranged repulsion: Quantitative test of Mode Coupling Theory .\nAbstract:\nWe study the glass transition of an ensemble of adhesive hard spheres with repulsive interactions decaying as 1/r6, where r is distance between particles. We find that this system exhibits two distinct relaxation processes at low temperatures: one fast process associated with local rearrangements within clusters of strongly bonded particles; another slower process related to collective motion of these clusters. The latter process can be described by mode-coupling theory (MCT) for colloidal suspensions. However, we show that MCT fails quantitatively when applied directly to our data because it does not take into account the presence of strong bonds which lead to additional slow modes. By introducing a simple modification to MCT, we are able to obtain excellent agreement with experimental results over several decades in time and frequency. This modified version of MCT also predicts correctly the temperature dependence of the structural relaxation time near Tg. Our work shows how quantitative tests of theoretical predictions may help improve their accuracy and applicability range.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Bond formation and slow heterogeneous dynamics in adhesive spheres with long - - ranged repulsion : Quantitative test of Mode Coupling Theory . Abstract : We study the glass transition of an ensemble of adhesive hard particles with repulsive interactions decaying as 1 / r6 , where p is distance between particles .We see that this process exhibits two different relaxation processes at low temperatures : one fast process associated with local rearrangements within clusters of highly bonded particles ; another slower process related to collective motion of these clusters . The latter cycle can be described by mode - coupling theory ( MCT ) for colloidal suspensions .However , we prove that MCT fails quantitatively when applied directly to our information because it does not take into consideration the presence of stable bonds which lead to extra slow modes . By introducing a simple change to MCT , we are able to obtain excellent compliance with experimental results over numerous years in time and frequency .This updated form of MCT also predicts correctly the temperature dependence of the structural relaxation time near Tg . Our research shows how rigorous trials of theoretical estimates may improve improve their accuracy and applicability range .",
        "rewrite_text": "In this study, we investigate the glass transition behavior of a system composed of adhesive hard spheres that exhibit long-range repulsive interactions, specifically following a decay of 1/r^6 with respect to the distance (r) between particles. Our findings reveal the presence of two distinct relaxation processes at low temperatures. The first is a rapid process linked to local rearrangements occurring within clusters of strongly bonded particles. The second, slower process is associated with the collective motion of these clusters. We demonstrate that this collective motion can be effectively described by Mode Coupling Theory (MCT) as it applies to colloidal suspensions. However, our analysis indicates that the traditional application of MCT falls short quantitatively when directly applied to our system, primarily because it overlooks the influence of stable bonds that introduce additional slow modes into the dynamics. To address this limitation, we propose a modified version of MCT that incorporates these stable bonds, resulting in a model that aligns remarkably well with experimental data across a wide range of time and frequency scales. Furthermore, our revised MCT framework accurately predicts the temperature dependence of the structural relaxation time as the system approaches the glass transition temperature (Tg). This research underscores the importance of rigorously testing theoretical models against empirical data, highlighting how such evaluations can enhance the precision and applicability of theoretical frameworks in understanding complex physical phenomena.",
        "ori-fast-z-score": -0.4,
        "water-fast-z-score": 5.5448262406693765,
        "rewrite-fast-z-score": 0.641688947919748
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Rapid Diffusion of dipolar order enhances dynamic nuclear polarization .\nAbstract:\nWe report on the rapid diffusion of dipolar order in liquid crystals, which is responsible for an enhanced nuclear spin-lattice relaxation rate and thus for a higher degree of dynamic nuclear polarization (DNP). The DNP effect was studied by means of electron paramagnetic resonance spectroscopy at X-band frequencies using a home-built high-field spectrometer equipped with a commercial microwave bridge operating at 2.5 GHz. We found that the enhancement factor increases strongly when approaching the nematic-isotropic phase transition temperature TNI = 35 °C. This finding can be explained by assuming that the molecular mobility decreases rapidly as TNI is approached. In addition to this observation we find that the maximum achievable enhancement factor depends critically on the sample geometry. For samples having a thickness d < 1 mm it reaches values up to 100 times larger than those observed for thicker samples. These results are important for future applications of DNP in NMR experiments performed under extreme conditions such as low temperatures or strong magnetic fields.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Rapid Diffusion of dipolar order enhances dynamic nuclear polarization . Abstract : We report on the quick diffusion of dipolar order in liquid crystals , which is responsible for an increase nuclear spin - lattice relaxation speed and therefore for a higher degree of static nuclear polarization ( DNP ) .The DNP phenomenon was studied by means of electron paramagnetic resonance spectroscopy at X - band frequencies using a home - installed high - field spectrometer equipped with a commercial microwave bridge operating at 2 . 5 GHz . We showed that the enhancement component increases strongly when approaching the nematic - isotropic phase change temperature TNI = 35 °C .This finding can be described by assuming that the molecular mobility decreases quickly as TNI is neared . In addition to this observation we find that the maximum achievable enhancement factor relies critically on the sample geometry .For specimens having a length d < 1 mm it hits values up to 100 times bigger than those observed for thicker specimen . These data are important for future applications of DNP in NMR experiments conducted under extreme circumstances such as low temperatures or strong magnetic fields .",
        "rewrite_text": "Title: Rapid Diffusion of Dipolar Order Enhances Dynamic Nuclear Polarization\n\nAbstract: In this study, we investigate the rapid diffusion of dipolar order in liquid crystals and its significant impact on nuclear spin-lattice relaxation rates, leading to an enhanced degree of dynamic nuclear polarization (DNP). Utilizing electron paramagnetic resonance (EPR) spectroscopy at X-band frequencies, we conducted our experiments with a high-field spectrometer specifically designed for this purpose, featuring a commercial microwave bridge operating at 2.5 GHz. Our results indicate a pronounced increase in the enhancement component as the system approaches the nematic-isotropic phase transition temperature (TNI = 35 °C). This behavior can be attributed to a rapid decrease in molecular mobility as the temperature nears TNI, which facilitates the diffusion of dipolar order. Furthermore, we observed that the maximum enhancement factor achievable is highly dependent on the geometry of the sample. For samples with a length less than 1 mm, we recorded enhancement factors that were up to 100 times greater than those measured in thicker specimens. These findings underscore the critical role of sample dimensions in optimizing DNP effects. The implications of our research are significant for advancing the application of DNP in nuclear magnetic resonance (NMR) experiments, particularly in extreme conditions such as low temperatures and high magnetic fields. Our work provides valuable insights that could enhance the efficiency and effectiveness of DNP techniques in various scientific and technological applications.",
        "ori-fast-z-score": -0.30779350562554625,
        "water-fast-z-score": 4.898979485566357,
        "rewrite-fast-z-score": 0.7627700713964739
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Enhanced Kondo Effect in an Electron System Dynamically Coupled with Local Optical Phonon .\nAbstract:\nWe study the effect of local optical phonons on the electronic transport properties of a quantum dot system by using the nonequilibrium Green s function method combined with the density functional theory (DFT). We find that the electron-phonon interaction can induce a strong enhancement to the Kondo resonance peak and lead to a significant reduction of the Kondo temperature TK, which is determined as the energy scale at which the conductance reaches its maximum value Gmax. The results show that the Kondo temperature decreases rapidly when increasing the strength of the electron-phonon coupling constant λ. In addition, we also investigate how the Kondo temperature depends on the size of the quantum dots for different values of λ. Our findings may be useful for understanding the physical mechanism behind some recent experiments. Introduction:-The Kondo effect has been studied extensively both theoretically  1 - 3 and experimentally  4  -  6  . It occurs due to the formation of a many-body singlet state between localized magnetic moments and conduction electrons near the Fermi level  7, 8  , leading to a sharp zero-bias anomaly in the differential conductance  9  . Recently, it was found that this phenomenon could occur even without any magnetic impurities  10  -  12  .\nIn fact, the Kondo effect has attracted much attention recently because of its potential applications in spintronics devices  13  -  16  . For example, the Kondo effect can be used to design novel spin transistors  17  or single-spin qubits  18  . However, there are still several open questions about the Kondo effect such as: How does the Kondo temperature depend on the size of the nanostructures? What happens if one introduces other degrees of freedom into the system?\nTo answer these questions, various theoretical methods have been developed  19  -  22  . Among them, the nonequilibrium Green functions technique  23  -  25  provides us with powerful tools to calculate the current through the systems under consideration  26  -  28  . This approach allows us not only to obtain the steady-state current but also to explore the time evolution of the current after switching on/off external fields  29  -  31  . Moreover, combining the nonequilibrium Green",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Enhanced Kondo Effect in an Electron System Dynamically Coupled with Local Optical Phonon . Abstract : We research the impact of local laser phonons on the electronic transport properties of a quantum dot network by using the nonequilibrium Green s function method combined with the density functional theory ( DFT ) .We see that the electron - phonon interaction can induce a powerful enhancement to the Kondo resonance peak and lead to a substantial lowering of the Kondo temperature TK , which is calculated as the power range at which the conductance reaches its highest value Gmax . The results show that the Kondo temperature reduces rapidly when increasing the strength of the electron - phonon coupling constant λ .In addition , we also investigate how the Kondo temperature varies on the size of the quantum dots for different values of λ . Our findings may be valuable for studying the physical process behind some latest studies .Introduction : - The Kondo phenomenon has been studied thoroughly both theoretically 1 - 3 and experimentally 4 - 6 . It happens due to the formation of a many - bodies singlet state between localized magnetic moments and conduction electrons near the Fermi level 7 , 8 , leading to a sharp zero - bias anomaly in the differential conductance 9 .Recently , it was shown that this phenomenon might arise even without any magnetic impurities 10 - 12 . In indeed , the Kondo phenomenon has garnered considerable scrutiny lately because of its potential applications in spintronics devices 13 - 16 .For instance , the Kondo phenomenon can be used to model novel spin transistors 17 or single - spinning qubits 18 . However , there are still several open questions about the Kondo phenomenon such as : How does the Kondo temperature depend on the size of the nanostructures ?What happens if one introduces other degrees of liberty into the process ? To answer these problems , various theoretical methods have been constructed 19 - 22 .Among them , the nonequilibrium Green functions method 23 - 25 offers us with powerful tools to estimate the charge through the systems under consideration 26 - 28 . This method enables us not only to obtain the stable - state current but also to examine the time progression of the current after switching on / off external fields 29 - 31 .Moreover , merging the nonequilibrium Green",
        "rewrite_text": "**Title:** Enhanced Kondo Effect in an Electron System Dynamically Coupled with Local Optical Phonons\n\n**Abstract:** This study investigates the influence of local laser phonons on the electronic transport characteristics of a quantum dot network, employing the nonequilibrium Green's function method in conjunction with density functional theory (DFT). Our findings reveal that the interaction between electrons and phonons significantly amplifies the Kondo resonance peak, resulting in a notable decrease in the Kondo temperature (TK). The Kondo temperature is determined as the power range at which the conductance achieves its maximum value (Gmax). We observe that TK diminishes rapidly with an increase in the strength of the electron-phonon coupling constant (λ). Furthermore, we explore the dependence of the Kondo temperature on the size of the quantum dots for various values of λ. These insights could prove instrumental in understanding the underlying physical mechanisms highlighted in recent studies.\n\nThe Kondo effect has been extensively analyzed both theoretically and experimentally, arising from the formation of a many-body singlet state between localized magnetic moments and conduction electrons near the Fermi level. This phenomenon manifests as a pronounced zero-bias anomaly in the differential conductance. Recent research indicates that the Kondo effect can occur even in the absence of magnetic impurities, prompting renewed interest due to its potential applications in spintronic devices. For example, the Kondo effect may be leveraged in the development of innovative spin transistors or single-spin qubits. However, several questions remain unresolved regarding the Kondo effect, such as the relationship between Kondo temperature and the size of nanostructures, as well as the implications of introducing additional degrees of freedom into the system. To address these inquiries, various theoretical approaches have been developed, among which the nonequilibrium Green's function method stands out as a powerful tool for analyzing charge transport in these systems. This method not only facilitates the calculation of steady-state currents but also allows for the examination of current dynamics in response to external field variations.",
        "ori-fast-z-score": -0.23942606534028665,
        "water-fast-z-score": 6.957010852370434,
        "rewrite-fast-z-score": 0.41522739926869984
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Cryptoplanet update .\nAbstract:\nThe Cryptoplanet project is an ongoing effort to collect and archive data on the world s cryptocurrencies, including Bitcoin (BTC), Ethereum (ETH) and Litecoin (LTC). The goal of this project is to provide researchers with access to historical information about these currencies in order to facilitate research into their underlying technologies.  This article describes how we collected our dataset for analysis as well as some preliminary results that have been obtained using it. We also describe plans for future work. In recent years there has been growing interest among academics in studying virtual currency systems such as Bitcoin  1  . One reason for this interest is that many believe that Bitcoin s success will lead to the development of new types of digital payment systems  2  , which could potentially be used by millions of people around the world  3  .\nIn addition to its potential use as a means of exchange, Bitcoin may also serve as a platform for other applications  4  . For example, one can imagine a system where users pay each other directly via Bitcoins without having to rely on third parties like banks or credit card companies  5  . Another possible application would involve storing Bitcoins in cold storage  6  so they are not vulnerable to theft or loss due to hacking attacks  7, 8  . Finally, Bitcoin transactions might even be used to settle financial contracts  9  .\nDespite all of these exciting possibilities, however, little academic research has been done on Bitcoin itself  10  . Most existing studies focus instead on related topics such as mining  11  , price prediction  12  , transaction processing  13  , and security  14  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Cryptoplanet update . Abstract : The Cryptoplanet effort is an continuing effort to collect and archive data on the world s cryptocurrencies , notably Bitcoin ( BTC ) , Ethereum ( ETH ) and Litecoin ( LTC ) .The goal of this project is to provide researchers with access to historical data about these currencies in order to enable research into their underlying technologies . This page describes how we collected our dataset for study as well as some preliminary results that have been achieved using it .We often describe plans for future projects . In recent years there has been growing interest among academics in examining virtual currency services such as Bitcoin 1 .One reason for this interest is that much think that Bitcoin s success will result to the development of new types of digital payment devices 2 , which could potentially be used by millions of people around the world 3 . In addition to its potential use as a means of exchange , Bitcoin might additionally act as a platform for other applications 4 .For instance , one can suppose a system where participants pay each other directly via Bitcoins without having to depend on third parties like banks or credit ticket firms 5 . Another suggested application would include storing Bitcoins in cold storage 6 so they are not vulnerable to robbery or loss due to hacking attacks 7 , 8 .Finally , Bitcoin purchases might actually be used to settle financial agreements 9 . Despite all of these interesting possibilities , however , nothing scholarly work has been performed on Bitcoin itself 10 .Most existing studies emphasis instead on related topics such as mining 11 , price forecast 12 , transaction processing 13 , and protection 14 .",
        "rewrite_text": "**Title: Cryptoplanet Update**\n\n**Abstract:** The Cryptoplanet initiative represents an ongoing endeavor to gather and archive comprehensive data on the world's leading cryptocurrencies, specifically Bitcoin (BTC), Ethereum (ETH), and Litecoin (LTC). This project aims to furnish researchers with access to historical data, thereby facilitating in-depth investigations into the technologies that underpin these digital currencies. This document outlines the methodology employed in compiling our dataset and presents some preliminary findings derived from its analysis. In recent years, there has been a notable surge in academic interest surrounding virtual currency services, particularly Bitcoin. This heightened focus can be attributed to the belief that Bitcoin's success may pave the way for innovative digital payment solutions that could be adopted by millions globally. Beyond its potential as a medium of exchange, Bitcoin is also envisioned as a platform for diverse applications. For example, one could imagine a scenario where individuals transact directly using Bitcoin, eliminating the need for intermediaries such as banks or credit card companies. Additionally, the concept of cold storage for Bitcoin has been proposed, which would safeguard these assets from theft or loss due to cyberattacks. Furthermore, Bitcoin transactions could serve as a means to settle various financial agreements. Despite these intriguing prospects, there remains a scarcity of scholarly research focused specifically on Bitcoin itself. Most existing literature tends to concentrate on peripheral subjects, including mining practices, price prediction models, transaction processing mechanisms, and security concerns. This paper aims to bridge that gap by providing a foundational dataset and insights that can spur further academic exploration into the multifaceted world of cryptocurrencies.",
        "ori-fast-z-score": 1.227881227029841,
        "water-fast-z-score": 8.410966704383963,
        "rewrite-fast-z-score": -0.8703882797784892
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Alignment and signed-intensity anomalies in WMAP data .\nAbstract:\nWe present evidence for alignment between the quadrupole, octopole and hexadecapole moments of the cosmic microwave background (CMB) temperature anisotropy on large angular scales as measured by the Wilkinson Microwave Anisotropy Probe (WMAP). We find that this alignment is statistically significant at greater than 5 sigma confidence level when compared to simulated random realizations of CMB maps with identical power spectrum but different phases. \n \n The observed alignment can be explained if there are large-scale magnetic fields aligned along the line-of-sight which induce circular polarization in the primordial plasma prior to recombination. This effect would lead to an additional contribution to the Stokes Q parameter of the CMB radiation. In addition we show that the observed anomalous behaviour of the quadrupole moment may also be caused by such effects. Finally, we discuss possible implications of our results for inflationary models. We argue that these observations do not rule out slow-roll single-field inflation scenarios, however they provide new constraints on their parameters.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Alignment and signed - intensity anomalies in WMAP information . Abstract : We present evidence for alignment between the quadrupole , octopole and hexadecapole moments of the cosmic microwave background ( CMB ) temperature anisotropy on huge angular scales as measured by the Wilkinson Microwave Anisotropy Probe ( WMAP ) .We see that this alignment is statistically meaningful at greater than 5 sigma confidence rate when compared to modeled random realizations of CMB images with identical power spectrum but different phases . The observed alignment can be understood if there are big - scale magnetic fields aligned along the line - of - view which induce circular polarization in the primordial plasma prior to recombination .This phenomenon might lead to an additional contribution to the Stokes Q function of the CMB radiation . In addition we prove that the seen anomalous behaviour of the quadrupole point could also be caused by such influences .Finally , we explain possible possibilities of our findings for inflationary scenarios . We argue that these observations do not order out slow - roll single - field inflation scenarios , however they give novel constraints on their parameters .",
        "rewrite_text": "We provide compelling evidence for a significant alignment among the quadrupole, octopole, and hexadecapole moments of the cosmic microwave background (CMB) temperature anisotropy, as observed by the Wilkinson Microwave Anisotropy Probe (WMAP). This alignment is statistically robust, achieving a confidence level exceeding 5 sigma when compared to simulated random realizations of CMB images that maintain the same power spectrum but exhibit different phase configurations. Our findings suggest that this alignment may be attributed to large-scale magnetic fields that are oriented along the line of sight, which could induce circular polarization in the primordial plasma prior to the recombination epoch. This effect may contribute an additional component to the Stokes Q parameter of the CMB radiation. Furthermore, we demonstrate that the anomalous characteristics observed in the quadrupole moment could also stem from these influences. We discuss the implications of our results for inflationary models, positing that while our observations do not exclude slow-roll single-field inflation scenarios, they do impose new constraints on their parameters. This research opens up avenues for further exploration into the interplay between cosmic magnetic fields and the early universe, potentially enhancing our understanding of the mechanisms driving cosmic inflation and the subsequent evolution of the universe.",
        "ori-fast-z-score": -0.43133109281375365,
        "water-fast-z-score": 5.5432186252458715,
        "rewrite-fast-z-score": 0.8251369970070347
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  On the generalized Freedman-Townsend model .\nAbstract:\nWe consider a generalization of the Freedman-Townsed model in which there are two types of agents, and each agent has an endowment that is either divisible or indivisible.  We show how to compute equilibria for this game using linear programming techniques. In particular we give a polynomial time algorithm for computing pure Nash equilibria when all goods have unit size (i.e., they can be divided into equal parts). This extends previous results on the same problem by Daskalakis et al. (2007) who considered only the case where all goods were indivisible. The main idea behind our approach is to use a new formulation of the equilibrium conditions as a system of equations with integer coefficients. Our result also implies that finding mixed Nash equilibria is NP-hard even if all goods have unit size. Finally, we present some computational experiments illustrating the performance of our algorithms. Keywords: Economic games",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : On the generalized Freedman - Townsend theory . Abstract : We consider a generalization of the Freedman - Townsed model in which there are two forms of agents , and each agent has an endowment that is either divisible or indivisible .We see how to compute equilibria for this tournament using linear programming tools . In particular we give a polynomial period approximation for modeling true Nash equilibria when all goods have unit size ( i . e . , they can be partition into equal parts ) .This continues past results on the same problem by Daskalakis et al . ( 2007 ) who considered only the case where all goods were indivisible .The main idea behind our approach is to use a new implementation of the equilibrium rules as a system of equations with integer equations . Our result also means that finding mixed Nash equilibria is NP - hard even if all goods have unit size .Finally , we present some computational experiments illustrating the performance of our algorithms . Keywords : Economic games",
        "rewrite_text": "Title: On the Generalized Freedman-Townsend Theory\n\nAbstract: This article explores an extension of the Freedman-Townsend model, introducing a framework that accommodates two distinct types of agents, each possessing endowments that may be either divisible or indivisible. We employ linear programming techniques to derive equilibria within this tournament setting. Notably, we provide a polynomial-time approximation method for accurately modeling true Nash equilibria under the condition that all goods are of unit size, meaning they can be evenly divided. This work builds upon previous findings by Daskalakis et al. (2007), who focused exclusively on scenarios involving indivisible goods. A key innovation in our approach is the reformulation of equilibrium conditions as a system of integer equations, which facilitates the computation of equilibria. Our findings also indicate that the problem of identifying mixed Nash equilibria remains NP-hard, even when all goods are of unit size. To substantiate our theoretical contributions, we include computational experiments that demonstrate the efficacy of our algorithms in practical applications. These results not only advance the understanding of economic games but also highlight the complexities involved in equilibrium computation when dealing with diverse agent endowments. \n\nKeywords: Economic games, Nash equilibria, linear programming, computational experiments, integer equations.",
        "ori-fast-z-score": 0.23570226039551587,
        "water-fast-z-score": 4.330522446256832,
        "rewrite-fast-z-score": -0.618852747755276
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Unitary Representations of Wavelet Groups and Encoding of Iterated Function Systems in Solenoids .\nAbstract:\nWe study the encoding problem for iterated function systems (IFS) on solenoids by unitary representations of wavelet groups, which are infinite-dimensional Lie groups generated by affine transformations of the real line. We show that if an IFS satisfies certain conditions then it can be encoded into a unitary representation of its associated wavelet group. This result is applied to prove that every self-similar fractal set with finite local complexity has a unique invariant measure up to scaling factors. The proof relies on the fact that any such fractal set can be approximated by a sequence of compact sets whose boundaries have zero Lebesgue measures. In particular, we obtain a new characterization of self-affine carpets as those fractals satisfying this condition. Finally, we give examples showing how our results apply to some well-known fractals including Sierpinski gasket, Koch snowflake, von Koch curve, and Menger sponge.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Unitary Representations of Wavelet Groups and Encoding of Iterated Function Systems in Solenoids . Abstract : We research the encoding problem for iterated function schemes ( IFS ) on solenoids by unitary representations of wavelet groups , which are infinite - dimensional Lie groups produced by affine transformations of the real line .We see that if an IFS satisfies certain conditions then it can be interpreted into a unitary representation of its identified wavelet group . This result is applied to prove that every self - similar fractal set with finite local complexity has a unique invariant measure up to scaling factors .The proof relies on the fact that any such fractal set can be approximated by a sequence of compact sets whose limits have zero Lebesgue measures . In particular , we obtain a new definition of self - affine carpets as those fractals satisfying this condition .Finally , we give instance demonstrating how our findings apply to some well - famous fractals including Sierpinski gasket , Koch snowflake , von Koch curve , and Menger sponge .",
        "rewrite_text": "In this study, we investigate the encoding problem associated with iterated function systems (IFS) on solenoids through the lens of unitary representations of wavelet groups. These wavelet groups are characterized as infinite-dimensional Lie groups generated by affine transformations acting on the real line. Our findings reveal that under specific conditions, an IFS can be effectively represented as a unitary representation of its corresponding wavelet group. This significant result leads to the conclusion that every self-similar fractal set exhibiting finite local complexity possesses a unique invariant measure, modulo scaling factors. The proof of this assertion is grounded in the observation that such fractal sets can be approximated by a sequence of compact sets, with the limiting behavior yielding zero Lebesgue measure. \n\nMoreover, we introduce a novel definition of self-affine carpets, identifying them as fractals that meet the aforementioned criteria. To illustrate the practical implications of our theoretical results, we provide examples of well-known fractals, including the Sierpinski gasket, Koch snowflake, von Koch curve, and Menger sponge. Through these examples, we demonstrate how our approach not only enhances the understanding of fractal geometry but also contributes to the broader field of mathematical analysis related to wavelet theory and dynamical systems. Our research thus bridges the gap between abstract mathematical concepts and tangible fractal structures, offering new insights into their properties and behaviors within the framework of solenoids and wavelet groups.",
        "ori-fast-z-score": 0.601929265428846,
        "water-fast-z-score": 4.00693842672377,
        "rewrite-fast-z-score": 1.8198699419201876
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Droplets in the two-dimensional +-J spin glass: evidence for (non-) universality .\nAbstract:\nWe study droplet excitations in the 2D spin-glass model with nearest-neighbor interactions and random ferromagnetic bonds, which is known to have an infinite number of metastable states at zero temperature. We show that this system has two different types of droplets: small ones are similar to those found in other models studied previously; large droplets are characterized by their fractal structure. The latter type can be viewed as a generalization of the droplet picture proposed earlier for the 3D Ising spin glasses. In addition we find that there exists another class of excitations -the so-called  giant droplets -which are not present in any of these systems. These giant droplets are responsible for the non-universal behavior observed numerically near the critical point. Finally, we argue that our results provide strong numerical support for the existence of a new phase transition line between the paramagnetic state and the spin-glass one. \nI. INTRODUCTORY REMARK\nThe concept of  droplet excitations  was introduced originally within the framework of the mean-field theory  1  . It describes how local perturbations affect global properties of the system. This idea turned out to be very useful when applied to various disordered systems such as spin glasses  2  , structural glasses  3  or vortex lattices  4  .\nIn particular it allowed to explain many features of the low-temperature thermodynamics of spin glasses  5  . However, despite its successes, the original droplet picture suffers from some serious drawbacks  6  : first, it does not take into account fluctuations around the saddle-point solution  7 ; secondly, it predicts a finite density of droplets even at T = 0  8  ; thirdly, it cannot describe properly the dynamics of the system  9  . To overcome these difficulties several modifications were suggested  10  . One of them  11  leads to the following expression for the free energy F(T ) per site: \nwhere f0 is the free-energy density of the reference system (e.g., the pure ferromagnet), Ns is the total number of spins, V is the volume occupied by each droplet",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Droplets in the two - dimensional + - J spin mirror : evidence for ( non - ) universality . Abstract : We research droplet excitations in the 2D spinning - glass model with nearest - neighbor interactions and random ferromagnetic bonds , which is known to have an endless number of metastable states at zero temperature .We see that this scheme has two different kinds of droplets : tiny ones are related to those present in other models studied ago ; wide droplets are marked by their fractal structure . The latter type can be viewed as a generalization of the droplet picture suggested earlier for the 3D Ising spin glasses .In addition we find that there exists another class of excitations - the so - called giant droplets - which are not present in any of these systems . These huge droplets are responsible for the non - universal behavior observed numerically near the critical position .Finally , we claim that our findings provide strong mathematical support for the existence of a new phase shift line between the paramagnetic state and the spin - glass one . I .INTRODUCTORY REMARK The concept of droplet excitations was introduced originally within the framework of the mean - field principle 1 . It details how local perturbations impact global properties of the system .This idea turned out to be very useful when applied to numerous disordered systems such as spin glasses 2 , structural glasses 3 or vortex lattices 4 . In particular it able to explain different properties of the small - temperature thermodynamics of spin glasses 5 .However , despite its successes , the original droplet picture suffers from some serious drawbacks 6 : first , it does not take into consideration fluctuations around the saddle - point problem 7 ; secondly , it predicts a finite density of droplets even at T = 0 8 ; thirdly , it lacks explain adequately the dynamics of the system 9 . To solve these problems several amendments were recommended 10 .One of them 11 leads to the following expression for the free energy F ( T ) per site : where f0 is the free - energy density of the reference system ( e . g . , the pure ferromagnet ) , Ns is the total number of spinning , V is the volume occupied by each droplet",
        "rewrite_text": "**Title:** Droplets in the Two-Dimensional ±J Spin Mirror: Evidence for (Non) Universality\n\n**Abstract:** In this study, we investigate droplet excitations within the two-dimensional spin-glass model characterized by nearest-neighbor interactions and random ferromagnetic bonds. This model is notable for possessing an infinite number of metastable states at absolute zero temperature. Our findings reveal the existence of two distinct types of droplets: small droplets that resemble those identified in previously studied models, and larger droplets distinguished by their fractal structure. The latter type serves as a generalization of the droplet framework previously proposed for three-dimensional Ising spin glasses. Furthermore, we identify a novel category of excitations, termed giant droplets, which are absent in other systems. These giant droplets play a crucial role in the non-universal behavior observed in numerical simulations near the critical point. Our results provide compelling mathematical evidence for the presence of a new phase transition line separating the paramagnetic phase from the spin-glass phase. \n\nThe concept of droplet excitations was initially introduced within the mean-field framework, illustrating how localized perturbations can influence the overall properties of a system. This concept has proven invaluable in understanding various disordered systems, including spin glasses, structural glasses, and vortex lattices. Specifically, it has been instrumental in elucidating the small-temperature thermodynamics of spin glasses. However, despite its successes, the original droplet model has notable limitations: it neglects fluctuations around saddle points, predicts a finite density of droplets even at zero temperature, and inadequately addresses the system's dynamics. To address these shortcomings, several modifications have been proposed. One such amendment leads to a revised expression for the free energy per site, incorporating the free-energy density of a reference system, the total number of spins, and the volume occupied by each droplet. Our research contributes to a deeper understanding of droplet dynamics and their implications for the behavior of spin-glass systems.",
        "ori-fast-z-score": 0.6446583712203042,
        "water-fast-z-score": 7.205766921228921,
        "rewrite-fast-z-score": 2.3599865304363625
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Intelligent location of simultaneously active acoustic emission sources: Part I .\nAbstract:\nThe problem of locating the source(s) of an acoustic emission (AE) event is addressed in this work by using a novel approach based on sparse representation and compressive sensing theory. The proposed method consists of two steps: first, AE signals are decomposed into several components via sparse coding; secondly, these components are used to estimate the locations of multiple AE events with high accuracy. In order to demonstrate its effectiveness, we apply our algorithm to simulated data as well as experimental results obtained from laboratory experiments. We show that the proposed method can locate multiple AE events accurately even when they occur at close time intervals or have similar waveforms. This article is part of a Special Issue entitled  Compressive Sensing for Industrial Applications  guest edited by Prof. Jianfeng Wu. \nIntroduction\n\nAcoustic emission (AE)\nis defined as elastic waves generated within materials due to sudden localised releases of energy  1  . It has been widely applied in non-destructive testing  2  , structural health monitoring  3  , geophysical exploration  4  , etc., where it provides useful information about material damage  5  .\nIn many practical applications such as industrial processes  6  , underground mining  7  , oil/gas pipeline inspection  8  , and so forth, there may be more than one AE source occurring simultaneously  9  . Therefore, accurate localisation of all AE sources becomes important  10  . However, simultaneous AE sources often generate overlapping waveforms; thus conventional methods cannot distinguish them effectively  11  . To address this issue, some researchers have attempted to use advanced signal processing techniques  12  -  14  . For example, Liu et al.  15  developed a new method called  time-frequency analysis  which was able to separate different AE sources successfully. Nevertheless, their method requires prior knowledge of the number of AE sources present in each measurement channel. Moreover, it also relies heavily on user experience to select appropriate parameters  16  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Intelligent positioning of jointly active sound emission sources : Part I . Abstract : The question of locating the origin ( s ) of an acoustic emission ( AE ) event is addressed in this project by using a novel method based on sparse representation and compressive detection model .The proposed approach consists of two stages : first , AE messages are decomposed into numerous components via sparse coding ; secondly , these components are using to estimate the places of multiple AE events with high sensitivity . In order to test its effectiveness , we apply our technique to modeled information as well as empirical results acquired from laboratory experiments .We see that the suggested method can locate many AE events accurately especially when they occur at close time periods or have related waveforms . This page is part of a Special Issue entitled Compressive Sensing for Industrial Applications guest edited by Prof . Jianfeng Wu .Introduction Acoustic emission ( AE ) is characterized as elastic waves generated within structures owing to unexpected localised releases of power 1 . It has been widely applied in non - destructive testing 2 , structural health monitoring 3 , geophysical investigations 4 , etc . , where it gives valuable info about material injury 5 .In many practical applications such as manufacturing operations 6 , deep extraction 7 , oil / gas pipeline inspection 8 , and so forth , there may be more than one AE source originating simultaneously 9 . Therefore , easy localisation of all AE sources takes vital 10 .However , concurrent AE sources sometimes generate overlapping waveforms ; thus traditional techniques unable distinguish them effectively 11 . To address this question , some researchers have tried to use advanced signal filtering tactics 12 - 14 .For instance , Liu et al . 15 introduced a new method called time - frequency analysis which was ability to separate distinct AE sources successfully .Nevertheless , their method needs earlier knowledge of the number of AE sources present in each measurement channel . Moreover , it also relies extensively on customer experience to select appropriate parameters 16 .",
        "rewrite_text": "**Title:** Intelligent Positioning of Jointly Active Sound Emission Sources: Part I\n\n**Abstract:** This study explores the challenge of pinpointing the origins of acoustic emission (AE) events through an innovative methodology that leverages sparse representation and a compressive detection model. The proposed framework operates in two distinct phases: initially, AE signals are decomposed into multiple components using sparse coding techniques; subsequently, these components are utilized to accurately estimate the locations of multiple AE events with enhanced sensitivity. To evaluate the efficacy of our approach, we applied it to both simulated data and empirical results obtained from laboratory experiments. The findings indicate that our method is capable of accurately locating numerous AE events, particularly when they occur in close temporal proximity or exhibit similar waveform characteristics. This research is part of a Special Issue on Compressive Sensing for Industrial Applications, guest edited by Prof. Jianfeng Wu.\n\n**Introduction:** Acoustic emission (AE) refers to the elastic waves produced within materials due to sudden localized energy releases. This phenomenon has found extensive applications in non-destructive testing, structural health monitoring, and geophysical investigations, providing critical insights into material integrity. In various practical scenarios, such as manufacturing processes, deep extraction activities, and oil and gas pipeline inspections, multiple AE sources may be active simultaneously. Consequently, the ability to accurately localize all AE sources is essential. However, overlapping waveforms generated by concurrent AE sources pose significant challenges for traditional localization techniques, which often struggle to differentiate between them effectively. To tackle this issue, researchers have explored advanced signal filtering methods. For example, Liu et al. introduced a time-frequency analysis technique that successfully separates distinct AE sources. Nonetheless, this method requires prior knowledge of the number of AE sources present in each measurement channel and heavily relies on user expertise to determine suitable parameters. Our research aims to overcome these limitations by providing a robust solution for the intelligent positioning of AE sources.",
        "ori-fast-z-score": -1.7287858644065064,
        "water-fast-z-score": 8.446016548096875,
        "rewrite-fast-z-score": 0.29981267559834457
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The widths of quarkonia in quark gluon plasma .\nAbstract:\nWe study the transverse momentum dependence of J/ψ and Υ production cross sections at RHIC energies within an effective field theory approach, which includes both elastic scattering off quarks as well as inelastic processes such as dissociation into open charm or bottom hadrons.  We find that the observed suppression pattern can be reproduced by including only elastic scattering for pT < 2 GeV/c while additional contributions are needed to describe data points with larger values of pT . The latter turn out to be dominated by inelastic processes like dissociation into open heavy flavor mesons. In particular we show that the inclusion of these effects leads to a significant reduction of the predicted nuclear modification factor RAA(pT ) compared to previous calculations based on purely elastic interactions. \nPACS numbers: 12.38.Mh, 25.75.-q, 11.10.Kk \nI. INTRODUCTORY REMAR K\nThe measurement of charmonium (J/ψ) and bottomonium (Υ) production is one of the most promising probes to investigate properties of hot and dense matter created in relativistic nucleus-nucleus collisions  1  . It has been suggested that the interaction between the produced quarkonia and the surrounding medium may lead to their partial melting  2  , i.e., to a decrease of the bound state masses due to color screening  3  .\nIn this work we present results obtained within an effective field theory framework  4  , where the relevant degrees of freedom are quarks and gluons rather than individual hadronic states. This allows us to calculate the total cross section for quarkonium production in terms of elementary partonic subprocesses involving light quarks q = u, d, s and gluons g. These include elastic scattering off quarks and gluon-gluon fusion leading to the formation of quarkonia via the creation of a virtual qq pair  5  . Furthermore, inelastic processes such as quarkonium dissociation into open heavy-flavor hadrons  6  have also been included  7, 8  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The widths of quarkonia in quark gluon plasma . Abstract : We study the transverse momentum dependence of J / ψ and [UNK] production cross sections at RHIC energies within an effective field theory approach , which includes both elastic scattering off quarks as well as inelastic processes such as dissociation into open charm or bottom hadrons .We see that the observed suppression behavior can be reproduced by including only elastic scattering for pT < 2 GeV / c while extra contributions are needed to define data points with larger values of pT . The latter come out to be dominated by inelastic reactions like dissociation into open heavy flavor mesons .In particular we prove that the introduction of these influences result to a substantial decreased of the expected nuclear modification factor RAA ( pT ) compared to previous analyses based on purely elastic interactions . PACS scores : 12 . 38 . Mh , 25 . 75 . - q , 11 . 10 . Kk I .INTRODUCTORY REMAR K The measurement of charmonium ( J / ψ ) and bottomonium ( [UNK] ) production is one of the most attractive probes to investigate properties of hot and dense matter created in relativistic nucleus - nucleus collisions 1 . It has been speculated that the interaction between the produced quarkonia and the surrounding medium may lead to their partial melting 2 , i . e . , to a decrease of the bound state masses due to color screening 3 .In this study we present results derived within an efficient field theory framework 4 , where the appropriate degrees of liberty are quarks and gluons instead than individual hadronic states . This enables us to estimate the total cross section for quarkonium production in terms of elementary partonic subprocesses involving light quarks g = u , d , s and gluons g . These include elastic scattering off quarks and gluon - gluon fusion led to the formation of quarkonia via the creation of a virtual qq pair 5 .Furthermore , inelastic reactions such as quarkonium dissociation into open heavy - flavor hadrons 6 have already been used 7 , 8 .",
        "rewrite_text": "**Title:** The Widths of Quarkonia in Quark-Gluon Plasma\n\n**Abstract:** This study investigates the transverse momentum dependence of J/ψ and bottomonium production cross sections at RHIC energies through an effective field theory framework. Our approach incorporates both elastic scattering interactions with quarks and inelastic processes, including the dissociation of quarkonia into open charm or bottom hadrons. We find that the suppression behavior observed in experimental data can be accurately reproduced by considering only elastic scattering for transverse momentum (pT) values below 2 GeV/c. However, for larger pT values, additional contributions are necessary to align with the data, which are predominantly attributed to inelastic reactions such as the dissociation into open heavy flavor mesons. Notably, our findings demonstrate that the inclusion of these inelastic processes leads to a significant reduction in the expected nuclear modification factor RAA(pT) when compared to earlier analyses that relied solely on elastic interactions.\n\nThe production of charmonium (J/ψ) and bottomonium ([UNK]) serves as a critical probe for exploring the characteristics of the hot and dense matter generated in relativistic nucleus-nucleus collisions. It has been proposed that the interactions between the produced quarkonia and the surrounding quark-gluon medium could result in partial melting, which manifests as a decrease in the bound state masses due to color screening effects. In this work, we present results derived from an effective field theory that focuses on quarks and gluons as the fundamental degrees of freedom, rather than individual hadronic states. This framework allows us to estimate the total cross section for quarkonium production based on elementary partonic subprocesses involving light quarks (u, d, s) and gluons. These subprocesses encompass elastic scattering off quarks and gluon-gluon fusion, which leads to the formation of quarkonia through the creation of virtual quark-antiquark pairs. Additionally, we discuss the role of inelastic reactions, such as quarkonium dissociation into open heavy-flavor hadrons, which have been previously explored in the literature.",
        "ori-fast-z-score": -0.5035088149780135,
        "water-fast-z-score": 4.166666666666667,
        "rewrite-fast-z-score": 0.6761234037828132
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Mass loss from Luminous Blue Variables and Quasi-Periodic Modulations of Radio Supernovae .\nAbstract:\nWe present the results of our study on mass-loss rates in luminous blue variables (LBVs) based on radio observations at 1.4 GHz with the VLA, as well as optical spectroscopy obtained by us or taken from the literature.  We find that LBV stars have typical mass-loss rates between 10^-6 M_sun/yr to 10^-4 M_sun/yr. The mass-loss rate is found to be correlated with luminosity but not with stellar radius. In addition we report quasi-periodic modulations of radio supernovae associated with SN 1987A and SN 1993J which are likely due to periodic changes in their circumstellar environments. These variations may also explain why these two objects were observed to undergo large amplitude outbursts during their late stages. This research was supported by NASA grant NAG5-7262. Keywords: Mass loss, Stellar evolution",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Mass loss from Luminous Blue Variables and Quasi - Periodic Modulations of Radio Supernovae . Abstract : We present the conclusion of our research on mass - loss rates in luminous blue variables ( LBVs ) based on radio observations at 1 . 4 GHz with the VLA , as well as laser spectroscopy acquired by us or taken from the literature .We see that LBV stars have typical mass - loss rates between 10 ^ - 6 M _ sun / yr to 10 ^ - 4 M _ sun / yr . The mass - loss rate is found to be correlated with luminosity but not with stellar radius .In addition we study quasi - periodic modulations of radio supernovae associated with SN 1987A and SN 1993J which are likely due to periodic alterations in their circumstellar environments . These changes may therefore reason why these two bodies were found to undergo huge amplitude outbursts during their late stages .This research was supported by NASA gift NAG5 - 7262 . Keywords : Mass loss , Stellar evolution",
        "rewrite_text": "In this study, we investigate the mass-loss rates of luminous blue variables (LBVs) utilizing radio observations conducted at 1.4 GHz with the Very Large Array (VLA), alongside laser spectroscopy data that we collected and sourced from existing literature. Our findings indicate that LBV stars typically exhibit mass-loss rates ranging from 10^-6 M_sun/yr to 10^-4 M_sun/yr. Notably, we observe a correlation between the mass-loss rate and the luminosity of these stars; however, no significant relationship is found with the stellar radius. Furthermore, we delve into the quasi-periodic modulations observed in radio supernovae, specifically those associated with SN 1987A and SN 1993J. Our analysis suggests that these modulations are likely a result of periodic changes in the circumstellar environments surrounding these supernovae. This phenomenon may explain the substantial amplitude outbursts recorded during the late stages of these events. The research presented in this article was supported by NASA grant NAG5-7262. The implications of our findings contribute to a deeper understanding of mass loss in stellar evolution, particularly in the context of LBVs and their role in the life cycles of massive stars. Keywords associated with this study include mass loss and stellar evolution.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 3.938927711338647,
        "rewrite-fast-z-score": 2.177598558933893
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Is there Evidence for a Hubble bubble? The Nature of Type Ia Supernova Colors and Dust in External Galaxies .\nAbstract:\nWe present new results on the nature of dust extinction in external galaxies, based on observations with HST/ACS of type Ia supernovae (SNe) at redshifts z=0.1-0.7. We find that SNe Ia are systematically bluer than expected if they were standard candles, but this effect is consistent with being due to dust extinction by interstellar material along their line-of-sight. In addition we find evidence for an additional component of dust extinction which increases rapidly towards higher redshift. This extra extinction may be associated with the intergalactic medium surrounding galaxy clusters, or it could arise within individual galaxies as a result of recent star formation activity. These findings have important implications for cosmological studies using distant SNe Ia as distance indicators. \n \n Keywords: Galaxy cluster, Interstellar matter, Intergalactic medium, Redshift evolution, Supernova, Supernova Ia",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Is there Evidence for a Hubble bubble ? The Nature of Type Ia Supernova Colors and Dust in External Galaxies .Abstract : We report new data on the nature of dust extinction in external galaxies , using on observations with HST / ACS of type Ia supernovae ( SNe ) at redshifts z = 0 . 1 - 0 . 7 . We see that SNe Ia are systematically bluer than expected if they were standard candles , but this effect is consistent with being owing to dust disappearance by interstellar material along their line - of - view .In addition we find proof for an additional element of dust extinction which increases quickly towards higher redshift . This extra extinction may be identified with the intergalactic medium surrounding galaxy galaxies , or it could occur within individual stars as a product of recent star formation activity .These conclusions have important implications for cosmological experiments using distant SNe Ia as distance indicators . Keywords : Galaxy cluster , Interstellar matter , Intergalactic medium , Redshift formation , Supernova , Supernova Ia",
        "rewrite_text": "Title: Is There Evidence for a Hubble Bubble? The Nature of Type Ia Supernova Colors and Dust in External Galaxies\n\nAbstract: In this study, we present new findings regarding dust extinction in external galaxies, derived from observations of Type Ia supernovae (SNe Ia) using the Hubble Space Telescope's Advanced Camera for Surveys (HST/ACS) at redshifts ranging from z = 0.1 to 0.7. Our analysis reveals that SNe Ia exhibit a systematic blue shift that deviates from the expected behavior of standard candles. This observed blueness can be attributed to the diminishing effects of dust along the line of sight, suggesting that interstellar material plays a significant role in altering the apparent colors of these supernovae. Furthermore, we identify a notable increase in dust extinction that correlates with higher redshifts, indicating the presence of an additional dust component. This extra extinction may be linked to the intergalactic medium surrounding galaxies or could arise from dust produced within individual stars as a result of recent star formation activities. The implications of these findings are profound for cosmological studies that utilize distant SNe Ia as distance indicators, as they challenge the assumptions about the uniformity of these supernovae and highlight the complexities introduced by dust in the universe. Understanding the nature of dust extinction is crucial for refining distance measurements and improving the accuracy of cosmological models. Our results underscore the need for further investigation into the interplay between dust, supernova colors, and the expansion of the universe, particularly in the context of the potential existence of a Hubble bubble. \n\nKeywords: Galaxy cluster, Interstellar matter, Intergalactic medium, Redshift formation, Supernova, Supernova Ia.",
        "ori-fast-z-score": -0.46499055497527714,
        "water-fast-z-score": 4.50333209967908,
        "rewrite-fast-z-score": 0.09578262852211514
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Experimenting with recursive queries in database and logic programming systems .\nAbstract:\nThe main goal of this thesis is to experiment with the use of recursion as an alternative way for querying databases, using Prolog (a declarative language) and SQL (the standard query language). The first part presents some background on how databases are structured and queried today; it also introduces the concept of recursion and its application in computer science. In the second section we present our experiments with recursion in Prolog, where we show that recursion can be used to express complex queries without having to resort to procedural code or other languages such as Java. Finally, in the third section we discuss the results obtained by applying recursion in SQL systems. We conclude that although there have been many attempts at implementing recursion in SQL systems, none has succeeded so far due to technical difficulties inherent in these systems. This work shows that recursion could be useful if implemented correctly in SQL systems. Keywords: Database, Recursion, Query",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Experimenting with recursive queries in database and logic programming systems . Abstract : The main goal of this dissertation is to study with the using of recursion as an additional means for querying data , using Prolog ( a declarative language ) and SQL ( the standard query language ) .The first part offers some details on how databases are structured and queried today ; it also explains the idea of recursion and its use in computer science . In the second chapter we present our experiments with recursion in Prolog , where we tell that recursion can be used to express complex queries without having to resort to procedural code or other languages such as Java .Finally , in the third chapter we explain the results derived by using recursion in SQL systems . We follow that although there have been many efforts at employing recursion in SQL systems , none has successful so far owing to technical problems inherent in these systems .This effort suggests that recursion could be usable if implemented correctly in SQL systems . Keywords : Database , Recursion , Query",
        "rewrite_text": "**Title:** Experimenting with Recursive Queries in Database and Logic Programming Systems\n\n**Abstract:** This dissertation investigates the application of recursion as a powerful tool for querying data within the realms of Prolog, a declarative programming language, and SQL, the standard language for database queries. The initial section provides an overview of contemporary database structures and querying methodologies, alongside a thorough explanation of recursion and its significance in computer science. In the second chapter, we delve into our experimental findings regarding the use of recursion in Prolog. Here, we demonstrate that recursion enables the formulation of intricate queries without the necessity of procedural programming or reliance on other languages, such as Java. The third chapter focuses on the outcomes of implementing recursion within SQL systems. Despite numerous attempts to integrate recursion into SQL, we highlight that these efforts have largely been unsuccessful due to inherent technical limitations within these systems. Our research indicates that, with appropriate implementation, recursion could indeed be a viable feature in SQL environments. This study not only sheds light on the potential of recursion in enhancing query capabilities but also emphasizes the need for further exploration and development in this area. \n\n**Keywords:** Database, Recursion, Query",
        "ori-fast-z-score": -0.46499055497527714,
        "water-fast-z-score": 5.9648090806346055,
        "rewrite-fast-z-score": 1.709408646894569
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Exploiting Social Annotation for Automatic Resource Discovery .\nAbstract:\nSocial tagging is an emerging technology that allows users to annotate resources with keywords or tags, which are then used by other users in their search and discovery processes. In this work we propose a novel approach to exploit social annotation information for automatic resource discovery. We first introduce the concept of semantic similarity between tags based on WordNet ontology. Then, using the proposed semantic similarity measure as well as user profile information, we develop two algorithms (i) TagRank and (ii) UserTagRank to rank the importance of each tag associated with a given resource. Finally, we conduct experiments over real-world datasets collected from Delicious website to evaluate our approaches. The experimental results show that both TagRank and UserTagRank can significantly improve the performance of existing state-of-the-art techniques. Keywords: Semantic Web, Social Tagging, Resource Discovery, Ranking Algorithms. 1 Introduction Social tagging has become one of the most popular ways for people to organize and share web content such as blogs, photos, videos etc., especially among Internet users who have little knowledge about how to use traditional search engines effectively  1  . Users usually assign tags to describe the contents they find interesting so that others may easily discover them later  2  .\nIn recent years there has been growing interest in developing new technologies to take advantage of social tagging systems  3  , including recommender systems  4  , personalized search  5  , question answering  6  , text classification  7  , event detection  8  , and so forth. However, despite these efforts, research into utilizing social tagging data for automatic resource discovery remains relatively unexplored  9  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Exploiting Social Annotation for Automatic Resource Discovery . Abstract : Social tagging is an evolving technique that enables consumers to annotate resources with keywords or tags , which are then utilized by other users in their search and finding processes .In this project we pursue a new approach to use social annotation info for efficient resource discovery . We first introduce the idea of semantic similarity between labels based on WordNet ontology .Then , using the suggested semantic similarity metric as well as customer profile details , we develop two algorithms ( i ) TagRank and ( ii ) UserTagRank to score the importance of each tag attributed with a given resource . Finally , we conduct experiments over real - time datasets collected from Delicious website to analyze our approaches .The empirical results show that both TagRank and UserTagRank can significantly boost the performance of older state - of - the - art methods . Keywords : Semantic Web , Social Tagging , Resource Discovery , Ranking Algorithms .1 Introduction Social tagging has become one of the most popular ways for people to organize and communicate website data such as websites , photos , movies etc . , particularly among Internet people who have less knowledge about how to use conventional search sites effectively 1 . Users typically give tags to explain the contents they discover attractive so that others would easily discover them later 2 .In recent years there has been growing interest in building new inventions to take advantage of social tagging technologies 3 , notably recommender methods 4 , personalized search 5 , question telling 6 , word classification 7 , event screening 8 , and so forth . However , despite these attempts , work into using social tagging material for automatic resource extraction continues relatively unexplored 9 .",
        "rewrite_text": "**Title:** Leveraging Social Annotation for Enhanced Resource Discovery\n\n**Abstract:** Social tagging has emerged as a dynamic method that allows users to annotate digital resources with keywords or tags, facilitating easier retrieval by others during their search processes. This article presents a novel approach to harnessing social annotation data to improve the efficiency of resource discovery. We begin by introducing the concept of semantic similarity among tags, grounded in the WordNet ontology, which provides a framework for understanding the relationships between different keywords. Building on this semantic similarity metric, along with user profile information, we propose two innovative algorithms: TagRank and UserTagRank. These algorithms are designed to evaluate and score the significance of each tag associated with a particular resource. To validate our methodologies, we conducted experiments using real-time datasets sourced from the Delicious website, allowing us to assess the effectiveness of our approaches in practical scenarios. The results of our empirical analysis demonstrate that both TagRank and UserTagRank substantially enhance the performance of existing state-of-the-art resource discovery methods. This work not only contributes to the field of semantic web and social tagging but also opens new avenues for personalized search and recommendation systems. By addressing the underexplored area of automatic resource extraction through social tagging, our research highlights the potential of user-generated content in optimizing information retrieval processes. \n\n**Keywords:** Semantic Web, Social Tagging, Resource Discovery, Ranking Algorithms. \n\n**1 Introduction:** Social tagging has gained significant traction as a popular means for individuals to organize and share web-based content, including websites, images, and videos, particularly among users who may not be adept at navigating traditional search engines. Users typically assign tags to describe the appealing aspects of the content they encounter, thereby facilitating easier access for others in the future. In recent years, there has been an increasing interest in developing innovative applications that leverage social tagging technologies, particularly in areas such as recommendation systems, personalized search, and content classification. However, despite these advancements, the potential of social tagging for automatic resource extraction remains relatively underexplored.",
        "ori-fast-z-score": -2.1358941442919024,
        "water-fast-z-score": 7.548881474151742,
        "rewrite-fast-z-score": -0.7761505257063328
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The high energy emission of GRO J1655-40 as revealed with INTEGRAL spectroscopy of the 2005 outburst .\nAbstract:\nWe report on results obtained by INTEGRAL observations during the 2005 outburst of the black hole candidate GRO J1655â€“40 (Nova Muscae 1991). The source was observed in the 20-100 keV range for about 100 days, starting at MJD 53000 and ending at MJD 53300. We have analyzed these data using both ISGRI and SPI instruments aboard INTEGRAL satellite. In addition to the main spectral component which is well described by a power law model modified by an exponential cutoff, we find that there are two additional components present in the spectrum. One of them has been previously reported by other authors but its origin remains unclear. Another one appears only when fitting the whole dataset simultaneously with all three models considered here -power law plus exponential cut-off, broken power law or Comptonization model-. This new feature can be interpreted either as a reflection hump produced by cold material surrounding the central X-ray source or as a broad iron line around 6.4 keV.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The high energy emitted of GRO J1655 - 40 as revealed with INTEGRAL spectroscopy of the 2005 outburst . Abstract : We report on findings obtained by INTEGRAL observations during the 2005 outburst of the dark hole candidate GRO J1655â€ “ 40 ( Nova Muscae 1991 ) .The source was seen in the 20 - 100 keV range for about 100 days , beginning at MJD 53000 and ending at MJD 53300 . We have analyzed these information using both ISGRI and SPI instruments aboard INTEGRAL satellite .In addition to the main spectral component which is well described by a power law theory amended by an exponential cutoff , we find that there are two additional components present in the spectrum . One of them has been previously reported by other researchers but its identity remains obscure .Another one appears only when fitting the whole dataset jointly with all three models described here - energy law plus exponential cutting - off , broken power law or Comptonization model - . This new feature can be interpreted either as a mirror hump produced by cold metal covering the main X - ray source or as a broad iron line around 6 . 4 keV .",
        "rewrite_text": "We present the results of our analysis of INTEGRAL observations of the black hole candidate GRO J1655-40 (Nova Muscae 1991) during its 2005 outburst. The observations spanned approximately 100 days, starting from MJD 53000 and concluding at MJD 53300, focusing on the energy range of 20-100 keV. Utilizing data from both the ISGRI and SPI instruments aboard the INTEGRAL satellite, we conducted a comprehensive spectral analysis. Our findings reveal that the primary spectral component is effectively characterized by a power law model modified by an exponential cutoff. However, we also identified two additional spectral components that contribute to the overall emission profile. One of these components has been noted in previous studies, yet its exact nature remains uncertain. The second component emerges only when we analyze the entire dataset using a joint fitting approach with three different models: the power law with exponential cutoff, a broken power law, and a Comptonization model. This newly identified feature can be interpreted in two ways: it may represent a mirror hump generated by cold metal surrounding the primary X-ray source, or it could manifest as a broad iron line centered around 6.4 keV. These findings enhance our understanding of the high-energy emission mechanisms in GRO J1655-40 and contribute to the broader knowledge of black hole candidates and their outbursts. The implications of these results are significant for the study of X-ray binaries and the physical processes governing their emissions.",
        "ori-fast-z-score": 1.0125791108334214,
        "water-fast-z-score": 5.813776741499453,
        "rewrite-fast-z-score": 1.4237369936287485
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Design of Parallel Kinematic Machine Tools Using Kinetostatic Performance Criteria .\nAbstract:\nThe design and kinematics analysis of parallel kinematic machine tools (PKMTs) is presented in this work using kinetic-static performance criteria. The proposed approach considers the dynamic behavior of PKMTs during their operation, which has been neglected by previous works on PKMTs. In addition to the static stiffness matrix, the inertia properties are also considered for the evaluation of the overall dynamic response of PKMTs. A new method based on the concept of virtual joints is developed to calculate the mass distribution along each leg of the PKMT under consideration. This information can be used as an input data set for further dynamic analyses such as modal or harmonic vibration analysis. Finally, two different PKMTs with three degrees-of-freedom per joint are designed and analyzed using the proposed methodology. It was found that the first PKMT exhibits better dynamic characteristics than its counterpart due to its lower natural frequencies and higher damping ratios.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Design of Parallel Kinematic Machine Tools Using Kinetostatic Performance Criteria . Abstract : The design and kinematics analysis of parallel kinematic machine tools ( PKMTs ) is given in this project using kinetic - static efficiency standards .The proposed approach treats the dynamic behavior of PKMTs during their operation , which has been neglected by earlier works on PKMTs . In addition to the static stiffness matrix , the inertia characteristics are also considered for the evaluation of the overall dynamic response of PKMTs .A modern algorithm based on the idea of virtual joints is developed to estimate the mass distribution along each leg of the PKMT under consideration . This knowledge can be used as an input data set for further dynamic analyses such as modal or harmonic vibration analysis .Finally , two different PKMTs with three degrees - of - independence per joint are built and evaluated using the suggested methodology . It was shown that the first PKMT displays better stability characteristics than its predecessor due to its lower natural bandwidth and larger damping ratios .",
        "rewrite_text": "Title: The Design of Parallel Kinematic Machine Tools Using Kinetostatic Performance Criteria\n\nAbstract: This article presents a comprehensive study on the design and kinematic analysis of parallel kinematic machine tools (PKMTs), emphasizing the importance of kinetostatic performance criteria. Unlike previous research that primarily focused on static characteristics, this work incorporates the dynamic behavior of PKMTs during operation, filling a significant gap in the existing literature. The study not only evaluates the static stiffness matrix but also integrates inertia characteristics to assess the overall dynamic response of these machines. \n\nTo facilitate this analysis, a novel algorithm based on the concept of virtual joints has been developed, enabling the estimation of mass distribution along each leg of the PKMT. This information serves as a critical input for subsequent dynamic analyses, including modal and harmonic vibration assessments. The methodology is applied to two distinct PKMT designs, each featuring three degrees of freedom per joint. \n\nThe findings reveal that the first PKMT exhibits superior stability characteristics compared to its predecessor, attributed to its reduced natural bandwidth and enhanced damping ratios. This research contributes valuable insights into the design and optimization of PKMTs, highlighting the significance of considering both static and dynamic performance criteria in the development of advanced machine tools. The implications of this study extend to the fields of robotics and precision engineering, where the performance and reliability of machine tools are paramount.",
        "ori-fast-z-score": 1.1043152607484654,
        "water-fast-z-score": 6.110100926607787,
        "rewrite-fast-z-score": 2.351802170958993
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Intersection Bodies and Generalized Cosine Transforms .\nAbstract:\nWe introduce the notion of intersection bodies in arbitrary dimensions, generalizing the classical concept for dimension two to higher dimensions. We show that intersection bodies are characterized by their Fourier transforms which we call generalized cosine transforms (GCT). The GCTs can be used as an alternative tool to study intersection bodies. In particular, we prove that intersection bodies have positive volume if and only if they are convex. This is done using a new characterization of intersection bodies via their support functions. Finally, we give some examples of intersection bodies in three dimensions. Keywords: Intersection body; Support function; Convexity; Volume; Fourier transform; Three-dimensional space. 1 Introduction Let K n denote the set of all origin-symmetric convex bodies in R n . For any K ∈ K n , let V(K) = |K|/|B n 2 | where | · | denotes Lebesgue measure on R n . Then V : K n →  0, 1  is called the volume functional. A compactly supported continuous function f : S n−1 → C with unit integral will be called a spherical harmonic of degree m. If f has no zeros then it is uniquely determined up to multiplication by a constant. It follows immediately that every spherical harmonic of degree m satisfies the following properties:\n(1) |f (x)| ≤ 1; (2) f (−x) = f (x), x ∈ S n−1 ; (3) (Spherical harmonics form an orthonormal basis.) Definition 1. An origin-symmetric convex body K ∈ K n is said to be an intersection body if there exists a non-negative real number λ such that its surface area measure σ K satisfies",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Intersection Bodies and Generalized Cosine Transforms . Abstract : We introduce the notion of intersection bodies in arbitrary dimensions , generalizing the classical concept for dimension two to higher dimensions .We see that intersection bodies are characterized by their Fourier transforms which we call generalized cosine transforms ( GCT ) . The GCTs can be used as an additional tool to study intersection bodies .In particular , we prove that intersection bodies have positive volume if and only if they are convex . This is accomplished utilizing a new definition of junction bodies via their support functions .Finally , we give some examples of intersection bodies in three dimensions . Keywords : Intersection body ; Support function ; Convexity ; Volume ; Fourier integral ; Three - dimensional space .1 Introduction Let K n denote the group of all origin - symmetric convex bodies in R n . For any K ∈ K n , let V ( K ) = | K | / | B n 2 | where | · | indicates Lebesgue measure on R n .Then V : K n → 0 , 1 is dubbed the volume functional . A compactly backed continuous function f : S n−1 → C with unit integral will be called a spherical harmonic of degree m . If g has no zeros then it is uniquely determined up to multiplication by a constant .It follows quickly that every spherical harmonic of degree m satisfies the following properties : ( 1 ) | f ( x ) | ≤ 1 ; ( 2 ) f ( −x ) = g ( x ) , z ∈ S n−1 ; ( 3 ) ( Spherical harmonics form an orthonormal basis . ) Definition 1 .An identity - symmetric convex body K ∈ K n is said to be an intersection body if there exists a non - negative real number λ such that its surface area measure σ K satisfies",
        "rewrite_text": "We present a comprehensive exploration of intersection bodies in arbitrary dimensions, extending the classical definition from two dimensions to higher dimensions. This study reveals that intersection bodies can be effectively characterized through their Fourier transforms, which we refer to as generalized cosine transforms (GCT). These GCTs serve as a valuable analytical tool for investigating the properties of intersection bodies. A significant finding of our research is the demonstration that intersection bodies possess positive volume if and only if they exhibit convexity. This result is derived from a novel definition of junction bodies based on their support functions. Additionally, we provide illustrative examples of intersection bodies specifically in three-dimensional space, enhancing the understanding of their geometric properties. \n\nIn our work, we denote the group of all origin-symmetric convex bodies in R^n as K_n. For any body K belonging to K_n, we define the volume functional V(K) as the ratio of the Lebesgue measure of K to that of the unit ball in R^n, denoted as |B_n^2|. Furthermore, we introduce the concept of spherical harmonics, which are compactly supported continuous functions defined on the unit sphere S^{n-1} with unit integral, categorized by their degree m. Notably, if a spherical harmonic g has no zeros, it can be uniquely determined up to a multiplicative constant. We establish several properties of these spherical harmonics, including boundedness, symmetry, and their role as an orthonormal basis. \n\nThis article aims to deepen the understanding of intersection bodies and their relationship with convexity and volume, while also providing a framework for further research in this area. The keywords associated with this study include Intersection body, Support function, Convexity, Volume, Fourier integral, and Three-dimensional space.",
        "ori-fast-z-score": -0.09325048082403138,
        "water-fast-z-score": 3.4206512100555795,
        "rewrite-fast-z-score": 0.5222329678670935
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dynamics of Size-Selected Gold Nanoparticles Studied by Ultrafast Electron Nanocrystallography .\nAbstract:\nWe report the first ultrafast electron nanocrystallographic study on size-selected gold nanoparticles (AuNPs). The AuNPs were prepared in solution and deposited onto carbon-coated copper grids for transmission electron microscopy studies. Time-resolved pump-probe experiments with femtosecond resolution were performed at beamline U41-PGM, MAX IV Laboratory, Sweden. We observed that the lattice expansion is anisotropic along different crystallographic directions within individual particles. This observation can be explained by considering the effect of surface stress induced during particle growth. In addition to this, we found that the lattice expansion depends strongly on the nanoparticle sizes. These results are important for understanding how the properties of nanoparticles evolve as their dimensions decrease towards atomic scale. A new technique has been developed recently which allows one to probe structural dynamics of materials down to the atomic level using ultrashort X-ray pulses  1  . However, it remains challenging to perform time-resolved measurements on single crystals or nanoparticles due to difficulties associated with sample preparation  2  , data collection  3  , and analysis  4  .\nIn order to overcome these challenges, researchers have started exploring alternative techniques such as ultrafast electron nanocrystalography  5  -  8  . In this method, an intense femtosecond laser pulse is used to excite electrons into unoccupied states above Fermi energy E F . Subsequently, photoelectrons emitted from excited atoms travel through the crystal and scatter off neighboring atoms  9  . By measuring the angular distribution of scattered photoelectrons, information about the structure of the material under investigation can be obtained  10  . Since the scattering cross section increases rapidly when photoelectrons approach the Brillouin zone boundary  11  , the photoelectron diffraction pattern contains more Bragg peaks than conventional powder patterns  12  . Therefore, the photoelectron diffraction pattern provides higher spatial resolution compared to traditional powder methods  13  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dynamics of Size - Selected Gold Nanoparticles Studied by Ultrafast Electron Nanocrystallography . Abstract : We report the first ultrafast electron nanocrystallographic study on size - selected gold nanoparticles ( AuNPs ) .The AuNPs were prepared in solution and deposited onto carbon - glazed metal grids for propagation electron microscopy studies . Time - resolved pump - probe studies with femtosecond resolution were performed at beamline U41 - PGM , MAX IV Laboratory , Sweden .We observed that the crystal growth is anisotropic along various crystallographic paths within various particles . This observation can be described by examining the impact of surface stress resulting during particle growth .In addition to this , we proved that the crystal increase depends strongly on the nanoparticle sizes . These data are important for studying how the properties of nanoparticles develop as their height shift towards atomic scale .A different method has been used lately which allows one to probe structural structure of substances down to the atomic level using ultrashort X - ray waves 1 . However , it remains challenging to conduct time - resolved calculations on individual crystals or nanoparticles due to difficulties related with sample preparation 2 , data analysis 3 , and identification 4 .In try to overcome these problems , researchers have started researching alternative techniques such as ultrafast electron nanocrystalography 5 - 8 . In this process , an active femtosecond infrared signal is utilized to excite particles into unoccupied states above Fermi energy E F .Subsequently , photoelectrons observed from excited atoms journey through the crystal and scatter off nearby atoms 9 . By measuring the angular distribution of scattered photoelectrons , info about the structure of the material under investigation can be obtained 10 .Since the scattering cross area grows swiftly when photoelectrons encounter the Brillouin zone boundary 11 , the photoelectron diffraction type includes more Bragg peaks than conventional powder designs 12 . Therefore , the photoelectron diffraction pattern provides higher spatial resolution compared to conventional powder studies 13 .",
        "rewrite_text": "We present the inaugural ultrafast electron nanocrystallographic investigation of size-selected gold nanoparticles (AuNPs). These nanoparticles were synthesized in solution and subsequently deposited onto carbon-coated metal grids to facilitate propagation electron microscopy analyses. Time-resolved pump-probe experiments with femtosecond temporal resolution were conducted at beamline U41-PGM, MAX IV Laboratory in Sweden. Our findings reveal that crystal growth in AuNPs exhibits anisotropic behavior along various crystallographic directions, a phenomenon that can be attributed to the surface stress experienced during the growth process. Furthermore, we established a strong correlation between the rate of crystal growth and the size of the nanoparticles, highlighting the significance of size in the evolution of nanoparticle properties as they approach the atomic scale.\n\nRecent advancements have introduced alternative methodologies capable of probing the structural characteristics of materials at the atomic level using ultrashort X-ray pulses. However, conducting time-resolved analyses on individual crystals or nanoparticles poses significant challenges, primarily due to issues related to sample preparation, data interpretation, and structural identification. In response to these challenges, researchers have begun exploring ultrafast electron nanocrystallography as a viable alternative. This technique employs a femtosecond infrared pulse to excite the nanoparticles into unoccupied electronic states above the Fermi energy (E_F). The resulting photoelectrons, emitted from the excited atoms, traverse the crystal lattice and scatter off adjacent atoms. By analyzing the angular distribution of these scattered photoelectrons, we can extract detailed information about the material's structure. Notably, the scattering cross-section increases dramatically when photoelectrons reach the Brillouin zone boundary, leading to a diffraction pattern that includes more Bragg peaks than those observed in traditional powder diffraction methods. Consequently, the photoelectron diffraction patterns obtained through this technique offer superior spatial resolution compared to conventional approaches, enhancing our understanding of nanoparticle dynamics at the atomic level.",
        "ori-fast-z-score": -1.9466570535691505,
        "water-fast-z-score": 7.137742529753552,
        "rewrite-fast-z-score": 0.40961596025952024
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Vector mesons from AdS/TC to the LHC .\nAbstract:\nWe present an overview of our recent work on vector meson production in heavy ion collisions at RHIC and LHC energies, based on holographic QCD models with chiral symmetry breaking (AdS/QCD). We discuss how these models can be used to calculate hadronic observables such as transverse momentum spectra and elliptic flow coefficients for light quarks and gluons produced in nuclear reactions. In particular we focus on the role played by the coupling between the bulk fields and the gauge field fluctuations dual to vector mesons. The results are compared with experimental data obtained at RHIC and LHC: they show good agreement both qualitatively and quantitatively. \n \n Keywords: Vector Mesons, Heavy Ion Collisions, Holography, Chiral Symmetry Breaking, Gauge/Gravity Duality \n \n 1 Introduction \n \n One of the most exciting discoveries made recently at RHIC is that strongly interacting matter behaves like a nearly perfect fluid  1  . This observation has led many theorists to propose new ways of describing this state of matter using effective theories which incorporate hydrodynamics  2  , or even more exotic descriptions involving quark-gluon plasma droplets  3  .\n \nIn order to understand better what happens during the early stages of heavy-ion collisions it would be very useful if one could study experimentally the properties of the hot dense medium created in those collisions. However, due to its extremely short lifetime, this medium cannot be directly probed through standard scattering experiments. Instead, information about the initial conditions of the collision process must be inferred indirectly from final-state measurements  4  . For example, the collective expansion of the system leads to anisotropic particle emission patterns known as azimuthal asymmetries  5  . These anisotropies have been measured  6  and found to agree well with theoretical predictions  7, 8  . \n \n Another important observable characterizing the dynamics of the expanding fireball is the spectrum of emitted particles  9  . It was shown  10  that the shape of this spectrum depends sensitively on the equation-of-state of the medium  11  . Moreover, the observed suppression  12  of high-pT hadrons",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Vector mesons from AdS / TC to the LHC . Abstract : We present an overview of our latest work on vector meson production in heavy ion collisions at RHIC and LHC energies , based on holographic QCD models with chiral symmetry breaking ( AdS / QCD ) .We discuss how these models can be used to estimate hadronic observables such as transverse energy spectra and elliptic flow coefficients for light quarks and gluons produced in nuclear compounds . In particular we focus on the importance played by the interaction between the bulk fields and the gauge field fluctuations dual to vector mesons .The results are compared with observation information obtained at RHIC and LHC : they show good agreement both qualitatively and quantitatively . Keywords : Vector Mesons , Heavy Ion Collisions , Holography , Chiral Symmetry Breaking , Gauge / Gravity Duality 1 Introduction One of the most exciting developments conducted lately at RHIC is that strongly interacting material behaves like a nearly perfect fluid 1 .This observation has led many theorists to propose new ways of describing this state of matter utilizing effective models which employ hydrodynamics 2 , or especially more exotic representations featuring quark - gluon particle droplets 3 . In order to explain better what comes during the early stages of light - ion collisions it would be very useful if one might explore experimentally the properties of the hot dense medium produced in those collisions .However , owing to its incredibly small life , this medium never be directly probed through conventional absorption studies . Instead , info about the first environments of the collision mechanism must be inferred indirectly from final - state measurements 4 .For instance , the collective expansion of the system leads to anisotropic particle emission effects referred as azimuthal asymmetries 5 . These anisotropies have been measured 6 and found to agree well with theoretical estimates 7 , 8 .Another important observable characterizing the dynamics of the increasing fireball is the spectrum of emitted particles 9 . It was shown 10 that the form of this spectrum relies sensitively on the equation - of - state of the medium 11 .Moreover , the observed suppression 12 of high - pT hadrons",
        "rewrite_text": "**Title:** Vector Mesons from AdS/TC to the LHC\n\n**Abstract:** In this article, we provide a comprehensive overview of our recent research on the production of vector mesons in heavy ion collisions at both RHIC and LHC energies, utilizing holographic QCD models that incorporate chiral symmetry breaking (AdS/QCD). Our study emphasizes the application of these models to estimate key hadronic observables, including transverse energy spectra and elliptic flow coefficients for light quarks and gluons generated in nuclear interactions. A significant focus is placed on the role of interactions between bulk fields and gauge field fluctuations that are dual to vector mesons. Our findings demonstrate a strong correlation with experimental data from RHIC and LHC, showcasing both qualitative and quantitative agreement. \n\nThe introduction highlights a pivotal advancement observed at RHIC, where strongly interacting matter exhibits characteristics akin to a nearly perfect fluid. This revelation has spurred theorists to develop innovative models that describe this state of matter, employing effective approaches such as hydrodynamics and more unconventional representations involving quark-gluon droplets. To enhance our understanding of the early stages of light-ion collisions, it is crucial to experimentally investigate the properties of the hot, dense medium produced. However, due to its extremely brief lifespan, this medium cannot be directly examined through traditional absorption techniques. Instead, insights into the initial conditions of the collision process must be derived from final-state measurements.\n\nOne of the key phenomena resulting from the collective expansion of the system is the anisotropic emission of particles, known as azimuthal asymmetries, which have been measured and found to align well with theoretical predictions. Additionally, the spectrum of emitted particles serves as another vital observable that characterizes the dynamics of the expanding fireball, with its form being highly sensitive to the equation of state of the medium. Furthermore, we discuss the observed suppression of high-pT hadrons, which adds another layer of complexity to our understanding of the dynamics at play in these high-energy collisions.\n\n**Keywords:** Vector Mesons, Heavy Ion Collisions, Holography, Chiral Symmetry Breaking, Gauge/Gravity Duality.",
        "ori-fast-z-score": 0.3287979746107146,
        "water-fast-z-score": 7.405474974817551,
        "rewrite-fast-z-score": -0.242535625036333
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Implications of  peak oil  for atmospheric CO2 and climate .\nAbstract:\nThe recent debate on the future availability of fossil fuels has focused attention on the possible implications of peak oil (the maximum rate at which economically viable quantities can be extracted) for global warming, particularly in relation to the Kyoto Protocol s emissions targets.  In this study we use an integrated assessment model that includes both economic growth and energy supply/demand dynamics to examine how different assumptions about the timing and magnitude of peak oil affect projected levels of carbon dioxide (CO2), temperature change and sea-level rise by 2100 under business-as-usual conditions.   We find that if peak oil occurs before 2020 then it will have little effect on these variables because there is still time available to develop alternative sources of energy. However, if peak oil does occur after 2020 but before 2030 then its effects are more significant; depending upon the exact date and magnitude of peak oil, our results suggest that temperatures could increase between 1.5°C and 3.0°C above pre-industrial levels by 2100 with associated increases in sea level rise ranging up to 0.7 metres.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Implications of peak oil for atmospheric CO2 and environment . Abstract : The recent debate on the future availability of fossil fuels has concentrated emphasis on the possible implications of peak oil ( the maximum speed at which financially feasible quantities can be extracted ) for global climate , particularly in relation to the Kyoto Protocol s emissions goals .In this study we using an unified assessment theory that contains both economic growth and energy demand / demand behavior to examine how various expectations about the timing and magnitude of peak oil impact projected levels of carbon dioxide ( CO2 ) , temperature drop and sea - level drop by 2100 under commercial - as - normal environments . We see that if peak oil happens before 2020 then it will have minimal influence on these parameters because there is already time available to develop new sources of power .However , if peak oil does occur after 2020 but before 2030 then its consequences are more significant ; depending upon the exact date and magnitude of peak oil , our findings show that temperatures may increase between 1 . 5°C and 3 . 0°C above pre - industrial levels by 2100 with corresponding increases in sea level drop ranging up to 0 . 7 metres .",
        "rewrite_text": "Title: Implications of Peak Oil for Atmospheric CO2 and the Environment\n\nAbstract: The ongoing discourse surrounding the future availability of fossil fuels has intensified focus on the potential consequences of peak oil—the point at which the extraction of economically viable fossil fuel resources reaches its maximum rate—on global climate dynamics, particularly in the context of the emissions targets set by the Kyoto Protocol. This study employs a unified assessment framework that integrates economic growth with energy demand and consumption behaviors to analyze how varying expectations regarding the timing and scale of peak oil influence projected carbon dioxide (CO2) levels, temperature increases, and sea-level rise by the year 2100 under typical commercial conditions. Our analysis indicates that if peak oil occurs prior to 2020, its impact on these environmental parameters will be relatively minimal, as there remains sufficient time to transition to alternative energy sources. Conversely, should peak oil manifest between 2020 and 2030, the implications become considerably more pronounced. Depending on the specific timing and extent of peak oil, our results suggest that global temperatures could rise by 1.5°C to 3.0°C above pre-industrial levels by 2100, accompanied by potential sea-level increases of up to 0.7 meters. This study underscores the critical importance of understanding the timing of peak oil in shaping future climate scenarios and emphasizes the need for proactive measures to mitigate its adverse effects on the environment.",
        "ori-fast-z-score": -0.09667364890456635,
        "water-fast-z-score": 6.609001368025944,
        "rewrite-fast-z-score": 1.2374368670764582
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Lyman Break Galaxies at z~1 and the evolution of the dust attenuation in star-forming galaxies with the redshift .\nAbstract:\nWe present new results on the evolution of the dust content in Lyman break galaxies (LBGs) using deep near-infrared data obtained by the UltraVISTA survey, which is part of the Sloan Digital Sky Survey III program. We use these observations to study the rest-frame UV-optical properties of LBGs at redshifts 1<z<3.5. The main goal of this work was to investigate how the dust extinction evolves as a function of galaxy mass and star formation rate density over cosmic time. Our analysis shows that there are two different populations of LBGs: one population has low stellar masses (M*=10^10-10^11 Msun), high specific star formation rates (SSFR>100Gyr-1), and relatively small amounts of dust; while another population consists of more massive systems (M*>10^11Msun), lower SSFR values (SSFR<30Gyr-1), and higher levels of dust extinction. These findings suggest that the amount of dust increases with increasing galaxy mass for both local and distant galaxies.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Lyman Break Galaxies at z ~ 1 and the evolution of the dust attenuation in star - creating stars with the redshift . Abstract : We report new data on the evolution of the dust content in Lyman break galaxies ( LBGs ) using deep near - infrared results collected by the UltraVISTA census , which is part of the Sloan Digital Sky Survey III program .We use these observations to study the rest - frame UV - optical properties of LBGs at redshifts 1 < z < 3 . 5 . The main goal of this research was to examine how the dust extinction evolves as a function of galaxy mass and galaxy formation rate concentration over cosmic time .Our study shows that there are two different populations of LBGs : one population has low stellar masses ( M * = 10 ^ 10 - 10 ^ 11 Msun ) , low specific star formation rates ( SSFR > 100Gyr - 1 ) , and fairly large quantities of dust ; while another population contains of more massive structures ( M * > 10 ^ 11Msun ) , lower SSFR values ( SSFR < 30Gyr - 1 ) , and low levels of dust extinction . These conclusions show that the quantity of dust increases with increasing galaxy mass for both local and distant galaxies .",
        "rewrite_text": "We present new findings on the evolution of dust content in Lyman Break Galaxies (LBGs) at redshifts around z ~ 1, utilizing deep near-infrared data from the UltraVISTA survey, which is part of the Sloan Digital Sky Survey III initiative. This research focuses on analyzing the rest-frame ultraviolet to optical characteristics of LBGs within the redshift range of 1 < z < 3.5. Our primary objective is to investigate how dust extinction varies in relation to galaxy mass and the concentration of star formation rates over cosmic time. Our analysis reveals the existence of two distinct populations of LBGs. The first group consists of galaxies with relatively low stellar masses (M* = 10^10 - 10^11 M☉), exhibiting high specific star formation rates (SSFR > 100 Gyr^-1) and significant amounts of dust. In contrast, the second population comprises more massive galaxies (M* > 10^11 M☉) that display lower SSFR values (SSFR < 30 Gyr^-1) and reduced levels of dust extinction. These findings indicate a clear trend: the amount of dust present in galaxies tends to increase with higher galaxy mass, a pattern that holds true for both local and distant galaxies. This research contributes to our understanding of the interplay between dust content, galaxy mass, and star formation rates, shedding light on the evolutionary processes of LBGs across different epochs in the universe.",
        "ori-fast-z-score": 0.6396021490668313,
        "water-fast-z-score": 5.405989188032437,
        "rewrite-fast-z-score": 0.6831300510639733
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Correlation between Infrared Colors and Intensity Ratios of SiO Maser Lines .\nAbstract:\nWe have observed the infrared colors (J-H, H-K) for 16 Mira variables with high resolution spectroscopy in order to investigate their relation to intensity ratios of SiO maser lines at 43 GHz. The results show that there is no correlation between these two parameters except for one star. We suggest that this may be due to different physical conditions among individual stars or differences in mass loss rates.  Keywords: Mira variable, Correlation, Infrared color, SiO maser line, Mass loss rate. 1 Introduction Miras are red giant stars which pulsate radially on time scales ranging from 100 days up to several thousand years. They exhibit large amplitude variations in luminosity as well as radial velocity. Their light curves can be described by a simple sinusoidal function with periods longer than about 300 days  1  . These stars are known to produce strong winds  2  , and they also emit intense radio waves  3  .\nThe SiO molecule has been found to exist in many types of astronomical objects such as late-type stars  4  , evolved massive stars  5  , young stellar objects  6  , comets  7  , and planets  8  . It is believed that SiO molecules play an important role in the formation process of dust grains  9  . SiO masers were first detected toward AGB stars  10  . Since then, SiO masers have been studied extensively towards both AGB stars  11  -  13  and post-AGB stars  14  -  16  . Many studies have shown that the properties of SiO masers depend strongly on the evolutionary stage  17  -  20  . For example, it was reported that the peak flux density decreases rapidly during the transition phase from AGB to post-AGB  21  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Correlation between Infrared Colors and Intensity Ratios of SiO Maser Lines . Abstract : We have discovered the infrared colors ( J - H , H - K ) for 16 Mira variables with high resolution spectroscopy in order to examine their connection to intensity ratios of SiO maser lines at 43 GHz .The results show that there is no correlation between these two parameters except for one star . We suggest that this might be due to different physical conditions among individual stars or variations in mass loss patterns .Keywords : Mira variable , Correlation , Infrared color , SiO maser line , Mass loss rate . 1 Introduction Miras are red massive stars which pulsate radially on time ranges ranging from 100 hours up to several thousand years .They show large frequency variations in luminosity as well as radial speed . Their light curves can be described by a simple sinusoidal function with periods longer than about 300 days 1 .These stars are known to produce violent winds 2 , and they even emit intense radio pulses 3 . The SiO molecule has been shown to form in multiple types of astronomical bodies such as early - class stars 4 , evolved large stars 5 , young stellar bodies 6 , comets 7 , and planets 8 .It is suspected that SiO compounds play an important role in the formation reaction of dust grains 9 . SiO masers were first detected toward AGB stars 10 .Since then , SiO masers have been studied closely towards both AGB stars 11 - 13 and post - AGB stars 14 - 16 . Many experiments have shown that the properties of SiO masers depend greatly on the evolutionary stage 17 - 20 .For instance , it was reported that the maximum flux volume decreases quickly during the shift stage from AGB to post - AGB 21 .",
        "rewrite_text": "**Title:** Correlation between Infrared Colors and Intensity Ratios of SiO Maser Lines\n\n**Abstract:** In this study, we investigate the relationship between the infrared colors (J - H, H - K) of 16 Mira variable stars and the intensity ratios of SiO maser lines at 43 GHz, utilizing high-resolution spectroscopy. Our findings reveal a lack of correlation between these two parameters for the majority of the stars examined, with the exception of one notable case. This absence of correlation may be attributed to the diverse physical conditions present in individual stars or variations in their mass loss patterns. Miras, characterized as red giant stars, exhibit radial pulsations with periods ranging from 100 hours to several thousand years, leading to significant fluctuations in luminosity and radial velocity. Their light curves typically follow a sinusoidal pattern with periods exceeding approximately 300 days. These stars are also known for their strong stellar winds and the emission of intense radio signals. The SiO molecule, which has been detected in various astronomical entities—including early-type stars, evolved giants, young stellar objects, comets, and planets—plays a crucial role in the formation of dust grains. SiO masers were initially identified in AGB stars and have since been the subject of extensive research in both AGB and post-AGB stars. Previous studies indicate that the characteristics of SiO masers are significantly influenced by the evolutionary stage of the stars, with evidence suggesting a rapid decline in maximum flux as stars transition from the AGB to post-AGB phase. This research contributes to our understanding of the complex interactions between infrared emissions and maser activity in Mira variables, highlighting the need for further investigation into the underlying mechanisms driving these phenomena.\n\n**Keywords:** Mira variable, Correlation, Infrared color, SiO maser line, Mass loss rate.",
        "ori-fast-z-score": -0.7126966450997984,
        "water-fast-z-score": 6.300231216854408,
        "rewrite-fast-z-score": 1.6783627165933783
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Beyond the semi-classical description of black hole evaporation .\nAbstract:\nWe present an analysis of Hawking radiation in terms of Bogoliubov coefficients, which are calculated using WKB wavefunctions for scalar fields on Schwarzschild backgrounds. We show that these results agree with those obtained by other methods when the backreaction is neglected and we also find agreement between our method and previous calculations including backreaction effects at leading order in perturbation theory. In addition to this perturbative check, we perform numerical checks of our results against exact solutions of the Klein-Gordon equation in Schwarzschild spacetime. Finally, we discuss how our approach can be used to calculate corrections beyond the semiclassical approximation. The evaporation of black holes has been studied extensively over many years (see e.g. ), but there remain some open questions about its detailed behaviour. One such question concerns the precise form of the spectrum of emitted particles; it was shown recently  that the standard semi-classical treatment leads to a thermal distribution of particle energies, but it remains unclear whether or not this result holds true once quantum gravity effects become important.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Beyond the semi - classical description of black hole evaporation . Abstract : We present an assessment of Hawking radiation in terms of Bogoliubov coefficients , which are measured using WKB wavefunctions for scalar fields on Schwarzschild backgrounds .We see that these results agree with those achieved by other methods when the backreaction is neglected and we also find agreement between our technique and previous calculations including backreaction effects at leading order in perturbation theory . In addition to this perturbative check , we perform numerical checks of our findings against exact solutions of the Klein - Gordon equation in Schwarzschild spacetime .Finally , we explain how our approach can be used to estimate corrections beyond the semiclassical approximation . The evaporation of black holes has been studied frequently over numerous years ( saw e . g .) , but there remain some open questions about its precise behaviour . One such issue concerns the exact form of the spectrum of emitted particles ; it was shown lately that the standard semi - classical treatment leads to a heat distribution of particle energies , but it remains unsure whether or not this consequence holds true once quantum gravitational changes become crucial .",
        "rewrite_text": "In this article, titled \"Beyond the Semi-Classical Description of Black Hole Evaporation,\" we explore the phenomenon of Hawking radiation through the lens of Bogoliubov coefficients, utilizing WKB wavefunctions for scalar fields within Schwarzschild spacetime. Our findings indicate a strong correlation with results obtained through alternative methodologies when backreaction effects are disregarded. Furthermore, we observe consistency between our approach and earlier calculations that incorporate backreaction effects at leading order in perturbation theory. To bolster our perturbative analysis, we conduct numerical validations of our results, comparing them against exact solutions of the Klein-Gordon equation in the context of Schwarzschild geometry. \n\nThis comprehensive examination not only reinforces the reliability of our technique but also provides insights into potential corrections that extend beyond the semiclassical framework. The evaporation of black holes has been a subject of extensive research over the years, yet several questions remain unresolved regarding its intricate dynamics. A particularly significant issue pertains to the precise nature of the emitted particle spectrum. Recent studies have suggested that the conventional semiclassical approach yields a thermal distribution of particle energies; however, it remains uncertain whether this conclusion holds when quantum gravitational effects become significant. Our work aims to address these uncertainties and contribute to a deeper understanding of black hole evaporation, paving the way for future investigations into the interplay between quantum mechanics and gravitational phenomena.",
        "ori-fast-z-score": 0.5184758473652127,
        "water-fast-z-score": 5.363390480545726,
        "rewrite-fast-z-score": 0.3922322702763681
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Physically Sound Hamiltonian Formulation of the Dynamical Casimir Effect .\nAbstract:\nWe present an explicit, physically sound formulation for the dynamical Casimir effect (DCE) in terms of a time-dependent Schrödinger equation with a non-Hermitian effective potential that is derived directly from first principles and has no free parameters.  The resulting expression agrees exactly with previous results obtained by other authors using different methods but it also provides new insights into this fascinating quantum phenomenon. In particular we show how to calculate the energy spectrum of the system as well as its decay rates and lifetimes. We demonstrate our approach on two examples - one involving a single harmonic oscillator coupled to a thermal bath at finite temperature and another where the oscillators are replaced by fermions. Finally, we discuss possible extensions of these ideas beyond the standard model of particle physics. The dynamical Casimir effect (DCE), predicted more than twenty years ago  1-3 , refers to the generation of photons due to vacuum fluctuations when macroscopic objects move or change shape  4  . This intriguing prediction was confirmed experimentally only recently  5-7  , although there have been earlier suggestions  8  .\nThe original theoretical description of the DCE relied heavily on phenomenological models which were not always easy to interpret physically  9  . More recent attempts  10-12  used microscopic approaches based on non-relativistic QED  13-15  or relativistic field theory  16  . However, all such treatments involve some ad-hoc assumptions about the form of the interaction between the moving object(s) and the electromagnetic fields  17  . Here we propose a completely different method that avoids any such approximations and leads to a simple, transparent physical picture of the process. Our starting point is the exact Heisenberg-Langevin equations describing the dynamics of the electric field operatorsÊ(r, t). These can be written in the compact form:",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Physically Sound Hamiltonian Formulation of the Dynamical Casimir Effect . Abstract : We present an explicit , physically sound formulation for the dynamical Casimir effect ( DCE ) in terms of a time - dependent Schrödinger equation with a non - Hermitian effective potential that is developed directly from initial principles and has no free parameters .The resulting expression agrees exactly with previous findings obtained by other researchers using separate methods but it also provides new information into this fascinating quantum concept . In particular we give how to estimate the power spectrum of the system as well as its decay rates and lifetimes .We test our approach on two examples - one involving a single harmonic oscillator coupled to a heat shower at finite temperature and another where the oscillators are replaced by fermions . Finally , we explain possible extensions of these ideas beyond the standard description of particle theory .The dynamical Casimir effect ( DCE ) , predicted more than twenty years previously 1 - 3 , relates to the generation of photons due to vacuum fluctuations when macroscopic objects moving or change form 4 . This exciting forecast was confirmed experimentally only recently 5 - 7 , although there have been earlier suggestions 8 .The original theoretical formulation of the DCE depended heavily on phenomenological models which were not always easier to translate physically 9 . More current proposals 10 - 12 used microscopic techniques based on non - relativistic QED 13 - 15 or relativistic field principle 16 .However , all such treatments require some ad - hoc assumptions about the form of the interaction between the moved object ( s ) and the electromagnetic forces 17 . Here we propose a completely different method that avoids any such approximations and leads to a simple , straightforward mechanical picture of the process .Our starting point is the exact Heisenberg - Langevin equations governing the dynamics of the electric field [UNK] ( r , t ) . These can be written in the compact form :",
        "rewrite_text": "In this article, we introduce a comprehensive and physically robust formulation of the dynamical Casimir effect (DCE) through a time-dependent Schrödinger equation characterized by a non-Hermitian effective potential. This formulation is derived directly from fundamental principles and is devoid of any free parameters. Our findings align precisely with previous results obtained through various methodologies, while also offering novel insights into this intriguing quantum phenomenon. Specifically, we outline a method for estimating the power spectrum of the system, along with its decay rates and lifetimes. To validate our approach, we examine two illustrative cases: the first involves a single harmonic oscillator interacting with a heat bath at finite temperature, and the second substitutes the oscillators with fermionic particles. Additionally, we discuss potential extensions of our framework beyond conventional particle theory.\n\nThe dynamical Casimir effect, initially predicted over two decades ago, describes the generation of photons resulting from vacuum fluctuations when macroscopic objects undergo motion or deformation. Although this prediction was confirmed experimentally only recently, earlier theoretical suggestions had laid the groundwork for understanding this effect. Historically, the theoretical models of the DCE relied heavily on phenomenological approaches, which often complicated the physical interpretation of results. More recent proposals have employed microscopic techniques grounded in non-relativistic quantum electrodynamics (QED) or relativistic field theory. However, these approaches typically necessitate ad-hoc assumptions regarding the interaction between the moving objects and electromagnetic fields.\n\nIn contrast, our method circumvents these approximations, providing a clear and intuitive mechanical representation of the DCE. We begin with the exact Heisenberg-Langevin equations that govern the dynamics of the electric field, which we express in a concise mathematical form. This innovative perspective not only enhances our understanding of the DCE but also opens avenues for further exploration in the realm of quantum field theory.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 6.560592908445457,
        "rewrite-fast-z-score": -0.33567254331867563
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Absolute Calibration and Characterization of the Multiband Imaging Photometer for Spitzer. I. The Stellar Calibrator Sample and the 24 micron Calibration .\nAbstract:\nWe present an absolute calibration of MIPS photometry at 24, 70, and 160 microns using stellar calibrators observed by the Infrared Array Camera (IRAC) onboard the Spitzer Space Telescope. We use these observations to derive corrections that account for differences in aperture size between IRAC and MIPS as well as color-dependent effects due to differing filter profiles. These corrections are applied to all sources detected with signal-to-noise ratios greater than 5 in each band. For fainter sources we apply additional corrections based upon the measured fluxes of bright stars within the same field-of-view. This method is used to calibrate over 1 million objects across the sky. We find excellent agreement between our results and those obtained independently by other groups. Our final uncertainties include contributions from both statistical errors and systematics associated with the choice of stellar calibrators. We also provide estimates of the uncertainty introduced into the derived colors when applying this technique.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  Absolute Calibration and Characterization of the Multiband Imaging Photometer for Spitzer.I.The Stellar Calibrator Sample and the 24 micron Calibration . Abstract : We present an absolute calibration of MIPS photometry at 24 , 70 , and 160 microns using stellar calibrators observed by the Infrared Array Camera ( IRAC ) onboard the Spitzer Space Telescope .We use these observations to derive corrections that explain for variations in aperture size between IRAC and MIPS as well as color - dependent effects due to varied filter profiles . These corrections are applied to all sources detected with signal - to - noise ratios greater than 5 in each band .For fainter sources we apply additional corrections based upon the determined fluxes of bright stars within the same field - of - view . This method is utilized to calibrate over 1 million items across the sky .We get excellent agreement between our findings and those acquired independently by other organizations . Our last uncertainties include contributions from both statistical mistakes and systematics associated with the selection of stars calibrators .We additionally give accounts of the uncertainty introduced into the derived colors when applying this methodology .",
        "rewrite_text": "Title: Absolute Calibration and Characterization of the Multiband Imaging Photometer for Spitzer: The Stellar Calibrator Sample and the 24 Micron Calibration\n\nAbstract: In this study, we present a comprehensive absolute calibration of the Multiband Imaging Photometer for Spitzer (MIPS) photometry at wavelengths of 24, 70, and 160 microns, utilizing stellar calibrators observed by the Infrared Array Camera (IRAC) aboard the Spitzer Space Telescope. Our approach involves deriving necessary corrections to account for discrepancies in aperture sizes between IRAC and MIPS, as well as addressing color-dependent effects arising from the differing filter profiles. These corrections are systematically applied to all sources exhibiting signal-to-noise ratios exceeding 5 in each photometric band. For sources with lower brightness, we implement additional adjustments based on the flux measurements of brighter stars located within the same field of view. This rigorous calibration process enables us to accurately assess over one million celestial objects across the sky. Our results demonstrate a high level of consistency with independent findings from other research groups, reinforcing the reliability of our calibration methodology. We also discuss the uncertainties associated with our measurements, which stem from both statistical errors and systematic issues related to the selection of stellar calibrators. Furthermore, we provide insights into the uncertainties that may affect the derived colors when employing this calibration technique. This work contributes significantly to the understanding of MIPS photometry and enhances the accuracy of infrared observations in astrophysical research.",
        "ori-fast-z-score": 0.9428090415820635,
        "water-fast-z-score": 5.500933918218137,
        "rewrite-fast-z-score": -0.9332565252573828
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Imaging the Cosmic Matter Distribution using Gravitational Lensing of Pregalactic HI .\nAbstract:\nWe present an analysis of gravitational lensing by large-scale structure in the universe, as traced by neutral hydrogen (HI) at high redshifts z > 6. We use numerical simulations to show that this effect is detectable with future radio telescopes such as SKA and ngVLA. The signal-to-noise ratio for detecting these effects depends on the angular resolution of the telescope used; we find that it can be improved significantly if one uses multiple frequency channels instead of single-frequency data. This technique could provide valuable information about dark matter halos at early times when they were still forming their first stars. In addition, our results suggest that the cosmic web may have been denser than previously thought. Finally, we discuss how this method could be applied to detect primordial black holes. Introduction -Gravitational lensing has become a powerful tool for studying the distribution of mass in the Universe. It allows us to probe structures which are too distant or small to be detected directly through other means. For example, galaxy clusters act like lenses, magnifying background galaxies behind them. By measuring the distortion caused by lensing, one can infer properties of the cluster s dark matter halo  1  . Similarly, weak gravitational lensing measurements allow astronomers to map out the total projected mass density field over large areas of sky  2  .\nIn recent years there has been growing interest in applying gravitational lensing techniques to study high-redshift objects  3  , including the epoch of reionization  4  . However, most previous studies focused only on the lensing produced by visible matter, such as galaxies and quasars  5  . Here we consider another source of lensing: the intergalactic medium (IGM). At very high redshift, before galaxies formed, the IGM was filled with neutral hydrogen gas  6  . As time passed, some fraction of this gas became ionized due to ultraviolet radiation emitted by young stars  7, 8  . But even today, much of the IGM remains neutral  9  . Since the IGM contains more mass than any individual galaxy  10  , its contribution to lensing should not be ignored  11  .\nThe goal of this",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Imaging the Cosmic Matter Distribution using Gravitational Lensing of Pregalactic HI . Abstract : We present an assessment of gravitational lensing by large - scale structure in the universe , as traced by neutral hydrogen ( HI ) at high redshifts z > 6 .We use numerical simulations to see that this effect is detectable with potential radio telescopes such as SKA and ngVLA . The signal - to - noise proportion for detecting these influences depends on the angular resolution of the telescope used ; we find that it can be improved substantially if one uses multiple wavelength channels instead of double - frequency information .This method could give valuable info about black material halos at early years when they were still forming their early stars . In addition , our findings confirm that the cosmic web possibly have been denser than previously thought .Finally , we talk how this technology could be applied to identify primordial black holes . Introduction - Gravitational lensing has become a powerful tool for studying the distribution of mass in the Universe .It enables us to probe elements which are too distant or small to be identified directly through other methods . For instance , galaxy regions act like filters , magnifying background galaxies behind them .By measuring the degradation created by lensing , one can infer characteristics of the cluster s dark matter halo 1 . Similarly , poor gravitational lensing observations allow astronomers to map out the total estimated mass density field over large areas of skies 2 .In past decades there has been growing interest in utilizing gravitational lensing methods to study high - redshift images 3 , notably the epoch of reionization 4 . However , most prior studies focused only on the lensing produced by observed matter , such as planets and quasars 5 .Here we imagine another source of lensing : the intergalactic medium ( IGM ) . At very high redshift , before stars formed , the IGM was filled with neutral hydrogen liquid 6 .As time passed , some fraction of this gas became ionized owing to ultraviolet radiation emitted by young galaxies 7 , 8 . But even today , part of the IGM remains neutral 9 .Since the IGM contains more mass than any individual galaxy 10 , its contribution to lensing should not be excluded 11 . The goal of this",
        "rewrite_text": "**Title:** Imaging the Cosmic Matter Distribution using Gravitational Lensing of Pregalactic HI\n\n**Abstract:** In this study, we evaluate the phenomenon of gravitational lensing induced by the large-scale structure of the universe, specifically focusing on neutral hydrogen (HI) at high redshifts (z > 6). Utilizing advanced numerical simulations, we demonstrate that the gravitational lensing effect is detectable with next-generation radio telescopes, such as the Square Kilometre Array (SKA) and the next-generation Very Large Array (ngVLA). Our analysis reveals that the signal-to-noise ratio for observing these lensing effects is significantly influenced by the angular resolution of the telescopes employed. Notably, we find that employing multiple wavelength channels enhances the detection capabilities compared to relying solely on dual-frequency data. This innovative approach holds the potential to yield critical insights into the formation of dark matter halos during the early epochs of star formation. Furthermore, our results suggest that the cosmic web may have been denser than previously estimated, which has implications for our understanding of the universe's structure. We also discuss the applicability of this technology in the search for primordial black holes, highlighting its significance in advancing our knowledge of cosmic evolution. Gravitational lensing has emerged as a vital tool for mapping the mass distribution in the universe, allowing astronomers to investigate regions that are otherwise too faint or distant to observe through conventional means. By analyzing the distortions caused by lensing, researchers can infer properties of dark matter halos surrounding galaxy clusters. While previous studies have primarily concentrated on lensing effects from observable matter, such as galaxies and quasars, our work introduces the intergalactic medium (IGM) as a significant contributor to lensing at high redshifts. During the early universe, the IGM was predominantly composed of neutral hydrogen, and although some of this gas has since become ionized, a substantial portion remains neutral today. Given that the IGM encompasses more mass than individual galaxies, its role in gravitational lensing is crucial and warrants further investigation.",
        "ori-fast-z-score": 0.6209204205650662,
        "water-fast-z-score": 8.795290947839238,
        "rewrite-fast-z-score": 0.48989794855663565
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Photonic molecules made of matched and mismatched microcavities: new functionalities of microlasers and optoelectronic components .\nAbstract:\nWe propose to use photonic molecules, which are composed of two or more coupled microcavities with different resonant wavelengths, as building blocks for novel types of lasers and optoelectronics devices. We show that the coupling between these cavities can lead to several interesting phenomena such as: (i) formation of hybridized modes, (ii) appearance of sharp peaks in emission spectrum at frequencies corresponding to avoided crossings of cavity eigenmodes, (iii) enhancement of spontaneous emission rate due to Purcell effect, and (iv) strong modification of optical gain properties by means of mode competition effects. These features open up possibilities for designing new types of laser sources based on photonic molecules, including single-mode lasers operating at room temperature without any external feedback elements. \n \n The proposed approach is illustrated using examples of photonic molecules consisting of pairs of semiconductor microdisks with slightly different diameters. It is shown that the considered structures allow one to obtain high quality factor whispering gallery modes with Q-factors exceeding 10^6.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Photonic compounds made of matched and mismatched microcavities : new functionalities of microlasers and optoelectronic parts . Abstract : We suggest to use photonic atoms , which are composed of two or more coupled microcavities with varying resonant wavelengths , as building blocks for innovative kinds of lasers and optoelectronics equipment .We suggest that the interaction between these cavities can lead to several interesting phenomena such as : ( i ) development of hybridized modes , ( ii ) presence of sharp peaks in emission spectrum at speeds related to avoided crossings of cavity eigenmodes , ( iii ) enhancement of induced emission speed due to Purcell phenomenon , and ( iv ) weak revision of optical loss properties by means of mode rivalry effects . These features offer up possibilities for constructing new types of laser sources focused on photonic compounds , particularly single - mode lasers active at room temperature without any external feedback components .The proposed approach is depicted using examples of photonic compounds consisting of pairs of semiconductor microdisks with slightly different diameters . It is demonstrated that the considered molecules permit one to obtain high quality factor whispering gallery modes with Q - parameters exceeding 10 ^ 6 .",
        "rewrite_text": "**Title:** Photonic Compounds Comprising Matched and Mismatched Microcavities: Novel Functionalities in Microlasers and Optoelectronic Devices\n\n**Abstract:** In this study, we propose the utilization of photonic atoms, which are formed by coupling two or more microcavities with distinct resonant wavelengths, as foundational elements for the development of advanced lasers and optoelectronic devices. The interactions between these microcavities can give rise to a variety of intriguing phenomena, including: (i) the emergence of hybridized modes, (ii) the appearance of sharp peaks in the emission spectrum associated with the avoided crossings of cavity eigenmodes, (iii) an increase in the rate of induced emission driven by the Purcell effect, and (iv) a subtle modification of optical loss characteristics due to mode competition effects. These attributes pave the way for the creation of innovative laser sources based on photonic compounds, particularly enabling the design of single-mode lasers that operate at room temperature without the need for external feedback mechanisms. We illustrate our proposed methodology through examples of photonic compounds formed by pairs of semiconductor microdisks with slightly varying diameters. Our findings demonstrate that these configurations allow for the achievement of high-quality whispering gallery modes, with quality factors (Q-factors) surpassing 10^6. This research opens new avenues for the engineering of microlasers and optoelectronic components, highlighting the potential of photonic compounds in enhancing device performance and functionality.",
        "ori-fast-z-score": 0.20203050891044214,
        "water-fast-z-score": 6.733753362236721,
        "rewrite-fast-z-score": 0.502518907629606
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Field dependent quasiparticles in the infinite dimensional Hubbard model .\nAbstract:\nWe study the ground state properties and excitations of the infinite-dimensional Hubbard model with nearest-neighbor hopping on an hypercubic lattice at half-filling by means of the functional renormalization group (FRG). We find that, for large enough interaction strength U , there is a phase transition to a Mott insulator where charge fluctuations are suppressed but spin fluctuations remain finite. The critical value Uc depends strongly on the magnetic field h. For small fields we obtain Uc = 0.5t while for larger fields Uc decreases rapidly as h increases. In addition, we show how the FRG can be used to calculate the single-particle spectral function A(k, ω) which exhibits a gapless dispersion relation near kF . Finally, we discuss possible extensions of our work. Introduction:-The physics of correlated electrons has been studied extensively over many years  1  -  4  . One of the most important models describing this type of behavior is the Hubbard model  5  . It describes interacting fermions moving on a lattice and it was originally introduced to describe the metal-insulator transition observed in doped semiconductors  6  .\nIn recent years much effort has gone into studying the Hubbard model using various numerical techniques such as exact diagonalizations  7  , quantum Monte Carlo  8  or density matrix renormalization groups  9  . However these methods have severe limitations when applied to systems with strong correlations and/or low dimensions  10  . Therefore new analytical approaches are needed to understand the rich physical phenomena associated with the Hubbard model  11  -  13  .\nOne promising approach is based on the functional renormalization-group (FRG), which allows one to treat interactions exactly within a controlled approximation scheme  14  -  16  . This method has recently been successfully applied to several problems including the two-dimensional  17  and three-dimensional  18  Hubbard model. Here we will use the FRG to investigate the ground-state properties and elementary excitations of the infinite-dimensionally extended Hubbard model  19  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Field dependent quasiparticles in the infinite dimensional Hubbard theory . Abstract : We explore the base state properties and excitations of the infinite - dimensional Hubbard theory with nearest - neighbor hopping on an hypercubic structure at half - filling by means of the functional renormalization group ( FRG ) .We see that , for large enough interaction strength U , there is a phase shift to a Mott insulator where charge fluctuations are suppressed but spin fluctuations remain finite . The essential value Uc relies highly on the magnetic force h . For large fields we obtain Uc = 0 . 5t while for larger fields Uc falls slowly as h rises .In addition , we explain how the FRG can be used to estimate the single - particle spectral relation A ( k , ω ) which exhibits a gapless dispersion connection near kF . Finally , we explain possible extensions of our work .Introduction : - The physics of coupled electrons has been studied frequently over much years 1 - 4 . One of the most important models explaining this kinds of dynamics is the Hubbard theory 5 .It describes interacting fermions moving on a lattice and it was originally developed to explain the metal - insulator transition seen in doped semiconductors 6 . In recent months significant effort has gotten into studying the Hubbard theory employing several mathematical techniques such as approximate diagonalizations 7 , quantum Monte Carlo 8 or density matrix renormalization groups 9 .However these tools have severe constraints when applied to systems with high correlations and / or low dimensions 10 . Therefore new analytical approaches are needed to realize the rich physical phenomena associated with the Hubbard theory 11 - 13 .One promising solution is based on the functional renormalization - group ( FRG ) , which allows one to treat relationships exactly within a controlled approximation scheme 14 - 16 . This method has recently been successfully application to several difficulties notably the two - dimensional 17 and three - dimensional 18 Hubbard theory .Here we will use the FRG to examine the ground - state properties and elementary excitations of the infinite - dimensionally extended Hubbard theory 19 .",
        "rewrite_text": "**Title:** Field-Dependent Quasiparticles in Infinite Dimensional Hubbard Theory\n\n**Abstract:** In this study, we investigate the ground state characteristics and excitations of the infinite-dimensional Hubbard model with nearest-neighbor hopping on a hypercubic lattice at half-filling, utilizing the functional renormalization group (FRG) approach. Our findings reveal that, for sufficiently large interaction strength \\( U \\), the system undergoes a transition to a Mott insulating phase, characterized by suppressed charge fluctuations while maintaining finite spin fluctuations. The critical interaction strength \\( U_c \\) is significantly influenced by the applied magnetic field \\( h \\); specifically, we find \\( U_c = 0.5t \\) for high magnetic fields, with \\( U_c \\ decreasing gradually as \\( h \\) increases. Furthermore, we demonstrate how the FRG framework can be employed to derive the single-particle spectral function \\( A(k, \\omega) \\), which displays a gapless dispersion relation near the Fermi wave vector \\( k_F \\). This work not only enhances our understanding of the infinite-dimensional Hubbard model but also opens avenues for future research. We discuss potential extensions of our findings, which may include exploring the effects of different lattice structures and interaction types, as well as the implications for real materials exhibiting similar behavior. The insights gained from this research contribute to the broader understanding of electron correlations in high-dimensional systems and their relevance to condensed matter physics.",
        "ori-fast-z-score": 0.79555728417573,
        "water-fast-z-score": 8.328146533974893,
        "rewrite-fast-z-score": 1.9069251784911847
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Non-topological solitons in field theories with kinetic self-coupling .\nAbstract:\nWe study the existence and stability properties of non-topological solitons in scalar field theories with kinetic self coupling, which are relevant to models for dark matter particles interacting via self-interactions mediated by light bosons.  We show that stable soliton solutions exist only if the mass of the boson is larger than twice the mass of the dark matter particle. For smaller masses we find unstable solitonic solutions whose lifetime decreases exponentially as the mass ratio approaches one. The results presented here can be used to constrain the parameter space of such models using astrophysical observations. Introduction:-The possibility of new physics beyond the Standard Model (SM) has been widely discussed recently  1  . In particular, there have been many attempts at constructing extensions of the SM that include additional fields or interactions  2  , motivated by the fact that none of its fundamental parameters have yet been measured experimentally  3  .\nIn this work we consider an extension of the SM where the Higgs sector consists of two complex scalars  4  . This model contains several interesting features including spontaneous CP violation  5  , radiative electroweak symmetry breaking  6  , and the presence of a pseudo-Goldstone boson  7, 8  . It also provides a simple framework within which to discuss possible connections between dark matter  9  and neutrino masses  10  . Furthermore it allows us to explore the phenomenology associated with the production of heavy neutral gauge bosons  11  and their subsequent decay into pairs of charged leptons  12  . Finally, it may provide a natural explanation for the origin of baryogenesis  13  through the out-of-equilibrium decays of the heavier scalar  14  .\nOne feature of these models is the presence of a second scalar particle, denoted by H 0 , which mixes with the SM-like Higgs h 0  15  . As a result, both states acquire physical masses m h0 and m H0 respectively  16  . If the mixing angle θH is small then mH ≫ mh  17  . However, even when mH = mh, the couplings of the two scalars differ significantly due to the different quantum numbers carried by each state  18  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Non - topological solitons in field theories with kinetic self - interactions . Abstract : We research the existence and stability properties of non - topological solitons in scalar field theories with kinetic self coupling , which are applicable to descriptions for dark matter molecules interacting via self - interactions mediated by light bosons .We see that strong soliton solutions arise only if the mass of the boson is bigger than times the mass of the dark matter object . For lower masses we find unstable solitonic solutions whose lifetime decreases exponentially as the mass ratio approaches one .The results presented here can be used to constrain the parameter room of such theories involving astrophysical observations . Introduction : - The possibility of new science beyond the Standard Model ( SM ) has been widely discussed recently 1 .In particular , there have been many efforts at constructing extensions of the SM that include extra fields or particles 2 , prompted by the fact that none of its essential parameters have ever been measured experimentally 3 . In this research we define an extension of the SM where the Higgs sector consists of two complex scalars 4 .This theory incorporates numerous interesting features including spontaneous CP violation 5 , radiative electroweak symmetry breaking 6 , and the presence of a quasi - Goldstone boson 7 , 8 . It additionally offers a simple context within which to consider likely relationships between dark matter 9 and neutrino masses 10 .Furthermore it allows us to examine the phenomenology linked with the production of large neutral gauge bosons 11 and their ensuing decay into pairs of charged leptons 12 . Finally , it could give a natural explanation for the origin of baryogenesis 13 through the out - of - equilibrium decays of the heavier scalar 14 .One feature of these models is the presence of a second scalar object , denoted by H 0 , which mixes with the SM - like Higgs h 0 15 . As a result , both states attain physical masses m h0 and m H0 respectively 16 .If the mixing angle θH is small then mH [UNK] mh 17 . However , even when mH = mh , the couplings of the two scalars differ significantly due to the different quantum numbers carried by each state 18 .",
        "rewrite_text": "**Title:** Non-topological Solitons in Field Theories with Kinetic Self-Interactions\n\n**Abstract:** In this study, we investigate the existence and stability characteristics of non-topological solitons within scalar field theories that incorporate kinetic self-interactions. These theories are particularly relevant for modeling dark matter molecules that interact through self-interactions mediated by light bosons. Our findings indicate that robust soliton solutions emerge only when the mass of the boson exceeds a certain threshold, specifically greater than the mass of the dark matter particle. Conversely, for lighter boson masses, we observe the emergence of unstable solitonic solutions, whose lifetimes diminish exponentially as the mass ratio approaches unity. The implications of our results are significant, as they provide constraints on the parameter space of these theories, informed by astrophysical observations. \n\nThe introduction of this research highlights the ongoing discourse surrounding potential extensions to the Standard Model (SM) of particle physics. Recent efforts have focused on developing theories that include additional fields or particles, motivated by the lack of experimental measurements for many fundamental parameters of the SM. Our work proposes an extension wherein the Higgs sector is represented by two complex scalar fields. This framework introduces several intriguing phenomena, such as spontaneous CP violation, radiative electroweak symmetry breaking, and the emergence of a quasi-Goldstone boson. Furthermore, this model facilitates the exploration of potential connections between dark matter and neutrino masses, as well as the phenomenology associated with the production of large neutral gauge bosons and their subsequent decay into charged lepton pairs. Additionally, it offers a compelling mechanism for baryogenesis through the out-of-equilibrium decays of the heavier scalar field. A notable aspect of this model is the introduction of a second scalar field, denoted as H0, which mixes with the SM-like Higgs field (h0). This mixing results in both scalar states acquiring distinct physical masses, m_h0 and m_H0. Even in scenarios where the masses are equal, the differing quantum numbers of the two scalars lead to significant variations in their coupling properties.",
        "ori-fast-z-score": -1.516365080488482,
        "water-fast-z-score": 5.124100921762788,
        "rewrite-fast-z-score": 1.281025230440697
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  On the Information Rate of MIMO Systems with Finite Rate Channel State Feedback and Power On/Off Strategy .\nAbstract:\nIn this work, we consider a multiple-input-multiple-output (MIMO) system where each transmitter has limited feedback information about its channel state to the receiver. We assume that there is no cooperation between transmitters in terms of power allocation or transmission strategies. Each transmitter can only adjust its own transmit power level based on local CSI knowledge at the transmitter side. In addition, each transmitter may turn off its radio completely when it does not have any data to send. The objective is to maximize the sum rate by optimizing both the power control policy as well as the transmission strategy for all users simultaneously under these constraints. \n \n First, we derive an upper bound on the achievable sum-rate using finite-rate feedback assuming Gaussian codebooks. Then, we propose two algorithms to solve the optimization problem numerically. Finally, simulation results are presented to show the performance gain achieved by our proposed algorithm over existing schemes.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : On the Information Rate of MIMO Systems with Finite Rate Channel State Feedback and Power On / Off Strategy . Abstract : In this study , we imagine a many - input - multiple - output ( MIMO ) scheme where each antenna has restricted feedback info about its channel state to the receiver .We assume that there is no cooperation between transmitters in terms of power distribution or transmission strategies . Each antenna can only alter its own transmit energy level based on local CSI understanding at the transmitter side .In addition , each transmitter could switch off its broadcast completely when it does not have any info to carry . The goal is to maximize the sum rate by optimizing both the power control strategy as well as the broadcasting strategy for all users simultaneously under these requirements .First , we derive an upper bound on the achievable sum - frequency using finite - frequency feedback assuming Gaussian codebooks . Then , we propose two strategies to solve the algorithms situation numerically .Finally , simulation data are presented to indicate the performance gain achieved by our proposed algorithm over existing algorithms .",
        "rewrite_text": "Title: On the Information Rate of MIMO Systems with Finite Rate Channel State Feedback and Power On/Off Strategy\n\nAbstract: This research explores a multiple-input multiple-output (MIMO) communication framework in which each antenna possesses limited feedback regarding its channel state to the receiver. We operate under the premise that there is no collaboration among transmitters concerning power allocation or transmission methodologies. Each antenna is restricted to adjusting its own transmission power based solely on local channel state information (CSI) available at the transmitter. Furthermore, transmitters have the option to completely deactivate their transmission when there is no information to relay. The primary objective of this study is to enhance the overall sum rate by optimizing both the power control and broadcasting strategies for all users, while adhering to these constraints. \n\nTo begin, we establish an upper limit on the achievable sum rate using finite-rate feedback, assuming the use of Gaussian codebooks. Following this, we introduce two numerical strategies designed to address the optimization problem effectively. These strategies are aimed at maximizing the information rate while considering the limitations imposed by the finite feedback and the independent operation of each transmitter. \n\nFinally, we present simulation results that demonstrate the performance improvements attained through our proposed algorithm in comparison to existing methods. The findings indicate a significant enhancement in the sum rate, showcasing the effectiveness of our approach in optimizing power control and transmission strategies in MIMO systems with limited feedback. This work contributes to the understanding of MIMO systems' performance under realistic feedback conditions and offers practical solutions for improving communication efficiency in such environments.",
        "ori-fast-z-score": -0.7592566023652966,
        "water-fast-z-score": 6.038635299392551,
        "rewrite-fast-z-score": 0.08873565094161139
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Near and Mid-IR Photometry of the Pleiades, and a New List of Substellar Candidate Members .\nAbstract:\nWe present new near-infrared photometric data for the Pleiades open cluster obtained with the Infrared Survey Facility (IRSF) on the South African Astronomical Observatory s 1-m telescope at Sutherland in November 2005. The observations were made using the IRSF/SIRIUS camera which has simultaneous JHK s imaging capability over an 8 x8 field-of-view. We have also used archival 2MASS data to supplement our sample. Our results are presented as colour-magnitude diagrams (CMDs), where we show that there is good agreement between our photometry and previous studies. Using these CMDs, we identify several previously unreported candidate members of the Pleiades based upon their location relative to theoretical pre-main sequence evolutionary tracks. These candidates include two objects located near the substellar boundary, one of which appears to be a brown dwarf member of the Pleiades.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Near and Mid - IR Photometry of the Pleiades , and a New List of Substellar Candidate Members . Abstract : We report new near - infrared photometric data for the Pleiades open cluster received with the Infrared Survey Facility ( IRSF ) on the South African Astronomical Observatory s 1 - m observatory at Sutherland in November 2005 .The surveys were made using the IRSF / SIRIUS camera which has simultaneous JHK s imaging technology over an 8 x8 field - of - view . We have also used archival 2MASS information to supplement our sample .Our results are presented as colour - magnitude diagrams ( CMDs ) , where we indicate that there is good agreement between our photometry and previous research . Using these CMDs , we identify several recently unreported candidate members of the Pleiades depending upon their placement relative to theoretical pre - principal sequence phylogenetic lines .These proposals include two bodies located near the substellar boundary , one of which appears to be a brown dwarf part of the Pleiades .",
        "rewrite_text": "We present new near-infrared photometric observations of the Pleiades open cluster, obtained using the Infrared Survey Facility (IRSF) at the South African Astronomical Observatory's 1-meter telescope in Sutherland during November 2005. The observations were conducted with the IRSF/SIRIUS camera, which allows for simultaneous imaging in the J, H, and K_s bands across an 8x8 arcminute field of view. To enhance our dataset, we also incorporated archival data from the 2MASS survey. Our findings are illustrated through color-magnitude diagrams (CMDs), which demonstrate a strong correlation between our photometric measurements and those from prior studies. Through the analysis of these CMDs, we have identified several candidate members of the Pleiades that have not been previously reported, based on their positions in relation to theoretical pre-main sequence evolutionary tracks. Notably, our analysis reveals two objects situated near the substellar limit, one of which is likely a brown dwarf that is part of the Pleiades cluster. This research contributes to the ongoing efforts to refine the membership list of the Pleiades and enhances our understanding of the cluster's stellar population, particularly in the substellar regime. The implications of these findings are significant for the study of star formation and the characteristics of low-mass objects in open clusters.",
        "ori-fast-z-score": -1.3643820804812932,
        "water-fast-z-score": 3.4465617474213164,
        "rewrite-fast-z-score": -0.8626621856275073
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Privacy - an Issue for eLearning? A Trend Analysis Reflecting the Attitude of European eLearning Users .\nAbstract:\nThe purpose of this study is to investigate how privacy issues are perceived by users in Europe and what their attitude towards these issues is, when using online learning environments (OLE). The research questions are: \nWhat do students think about privacy?\nHow does student s perception change over time?\nWhich factors influence students  attitudes on privacy? How can we measure it? What are the implications for OLE developers? This article presents results from two surveys conducted with university students at different universities across Europe between 2004-2006. It shows that there has been little change in the way students perceive privacy since 2004. Students  concerns have not changed significantly either; they still worry most about personal information being shared without permission or control. However, some differences were found among countries. In addition, the findings show that gender plays a role as well as age. Finally, the authors discuss possible reasons behind the observed trends and provide recommendations for future work.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Privacy - an Issue for eLearning ? A Trend Analysis Reflecting the Attitude of European eLearning Users .Abstract : The purpose of this study is to examine how privacy issues are seen by users in Europe and what their attitude towards these problems is , when using internet learning habitats ( OLE ) . The survey challenges are : What do students think about privacy ?How does student s perception shift over time ? Which variables influence pupils beliefs on privacy ?How can we measure it?What are the implications for OLE developers?This page presents scores from two polls conducted with university students at different schools across Europe between 2004 - 2006 . It demonstrates that there has been less shift in the way pupils understand privacy since 2004 .Students issues have not altered significantly either ; they still concern most about personal data being shared without authorization or authority . However , some differences were found among countries .In addition , the discoveries reveal that gender plays a role as well as aging . Finally , the articles discuss possible reasons behind the reported developments and include recommendations for future projects .",
        "rewrite_text": "**Title: Privacy Concerns in eLearning: A Trend Analysis of European Users' Attitudes**\n\n**Abstract:** This study investigates the perceptions of privacy among European users engaged in online learning environments (OLE). It aims to understand how students view privacy issues, how their perceptions evolve over time, the factors that influence their beliefs about privacy, and the implications for developers of online learning platforms. The research is based on data collected from two surveys conducted with university students across various European institutions between 2004 and 2006. The findings indicate that there has been minimal change in students' understanding of privacy since 2004. Concerns regarding the unauthorized sharing of personal data remain predominant among students, suggesting a persistent anxiety about privacy in digital learning contexts. Notably, the study identifies variations in privacy attitudes across different countries, highlighting the influence of cultural factors. Additionally, the analysis reveals that both gender and age significantly affect students' privacy concerns, indicating that these demographic variables play a crucial role in shaping attitudes towards privacy in eLearning. The article further explores potential reasons for the observed trends and offers recommendations for future research and the development of online learning environments. By addressing these privacy concerns, OLE developers can enhance user trust and engagement, ultimately leading to more effective educational experiences. This research contributes to the broader discourse on privacy in digital education and underscores the importance of understanding user attitudes to inform the design and implementation of secure online learning platforms.",
        "ori-fast-z-score": -0.52999894000318,
        "water-fast-z-score": 7.378647873726218,
        "rewrite-fast-z-score": 0.7745966692414834
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  HST/ACS Coronagraphic Observations of the Dust Surrounding HD 100546 .\nAbstract:\nWe present new Hubble Space Telescope (HST) Advanced Camera for Survey (ACS)\ncoronagraphic observations in visible light and near-infrared wavelengths that reveal an extended dusty disk surrounding the Herbig Ae star HD 100546, which is known to harbor a protoplanetary disk with spiral arms. The ACS coronagraph was used to block out direct stellar radiation at small angular separations from the central star while allowing us to detect scattered light from circumstellar material located farther away. We find evidence for two bright rings of emission separated by ~0.5′′ along the major axis of the disk. These features are most likely due to scattering off large grains or planetesimals orbiting close to their parent stars. \n \n In addition, we have detected several dark gaps within these bright rings as well as fainter structures extending outward into the outer regions of the disk. Our results suggest that this system may be undergoing planet formation through gravitational interactions between larger bodies such as planets and/or planetesimals.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : HST / ACS Coronagraphic Observations of the Dust Surrounding HD 100546 . Abstract : We report new Hubble Space Telescope ( HST ) Advanced Camera for Survey ( ACS ) coronagraphic observations in visible light and near - infrared wavelengths that expose an extended dusty disk surrounding the Herbig Ae star HD 100546 , which is known to harbor a protoplanetary disk with spiral arms .The ACS coronagraph was used to block out direct stellar radiation at small angular separations from the main star while providing us to identify scattered radiation from circumstellar material located farther distant . We see evidence for two faint rings of emission separated by ~ 0 . 5 ′ ′ along the main axis of the disk .These features are most likely due to scattering off large grains or planetesimals orbiting close to their father planets . In addition , we have discovered numerous dark gaps within these dark rings as well as fainter forms extending outward into the inner regions of the disk .Our results propose that this system might be experiencing planet development through gravity interactions between big bodies such as planets and / or planetesimals .",
        "rewrite_text": "We present new coronagraphic observations conducted with the Hubble Space Telescope (HST) Advanced Camera for Surveys (ACS) that reveal an extensive dusty disk surrounding the Herbig Ae star HD 100546. This star is known to possess a protoplanetary disk characterized by spiral arms. Utilizing the ACS coronagraph, we effectively blocked direct stellar light at small angular separations, allowing us to detect scattered light from circumstellar material located at greater distances. Our observations reveal two faint emission rings, approximately 0.5 arcseconds apart, aligned along the primary axis of the disk. These features are likely the result of scattering from large grains or planetesimals that are in close orbit around their parent star. Furthermore, we identified several dark gaps within these emission rings, as well as fainter structures extending outward into the inner regions of the disk. Our findings suggest that the HD 100546 system may be undergoing planet formation, driven by gravitational interactions among substantial bodies such as planets and/or planetesimals. This research contributes to our understanding of the processes involved in planet formation and the dynamics of protoplanetary disks, highlighting the complex interactions that shape the evolution of such systems.",
        "ori-fast-z-score": 0.10721125348377948,
        "water-fast-z-score": 5.5432186252458715,
        "rewrite-fast-z-score": 0.7181848464596079
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dark energy constraints from cosmic shear power spectra: impact of intrinsic alignments on photometric redshift requirements .\nAbstract:\nWe investigate the effect of intrinsic galaxy alignment (IA) on dark energy parameter constraints using weak lensing tomography with future space-based surveys, such as Euclid and WFIRST. We find that IA introduces significant biases in cosmological parameters when only spectroscopic redshifts are available for calibration purposes. However, we show that these biases can be reduced by including photometric redshifts to calibrate the IA model. In particular, we demonstrate that it is possible to reduce the bias due to IA down to less than 1% level if at least 10 bands spanning 0.4-1 micron are used for photo-z estimation. This requirement becomes more stringent towards higher redshifts where the number density of galaxies decreases rapidly. The results presented here will help guide the design of future experiments aiming to measure dark energy through weak gravitational lensing. Introduction - Weak gravitational lensing has emerged as one of the most promising probes of dark energy  1-3 . It measures the distortion of distant galaxy images caused by intervening large-scale structure along the line-of-sight  4  . By measuring this distortion over a wide range of angular scales, one can reconstruct the three-dimensional matter distribution in the Universe  5  , which contains information about both the geometry of the universe and its growth rate  6  .\nIn order to extract useful cosmological information from weak lensing data, accurate measurements of the shapes of background galaxies must first be obtained  7-9 . These shape measurements then need to be corrected for distortions induced by atmospheric effects  10  , telescope optics  11  , and point spread function  12  . Finally, they also have to be corrected for distorted shapes introduced by foreground structures  13  . Intrinsic galaxy alignments (IAs), i.e., correlations between galaxy orientations  14  or positions  15  , introduce additional systematic errors into the measured shear correlation functions  16  . If not properly accounted for, IAs could lead to biased estimates of cosmological parameters  17  .\nSeveral methods have been proposed to mitigate the effect of IAs on cosmological parameter estimations  18  . One approach involves modeling the observed galaxy ellipticities as a combination of intrinsic",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dark energy constraints from cosmic shear speed spectra : effects of intrinsic alignments on photometric redshift requirements . Abstract : We explore the impact of intrinsic galaxy alignment ( IA ) on dark energy parameter constraints using weak lensing tomography with potential space - based surveys , such as Euclid and WFIRST .We see that IA introduces considerable biases in cosmological values when only spectroscopic redshifts are available for calibration purposes . However , we prove that these biases can be reduced by including photometric redshifts to calibrate the IA theory .In particular , we prove that it is easy to reduce the bias related to IA down to fewer than 1 % level if at least 10 bands spanning 0 . 4 - 1 micron are using for photo - z estimation . This requirement gets more stringent towards higher redshifts where the number density of clusters reduces rapidly .The results presented here will assist guide the development of later research seeking to measure dark energy through soft gravitational lensing . Introduction - Weak gravitational lensing has emerged as one of the most attractive probes of dark energy 1 - 3 .It studies the degradation of distant galaxy images created by intervening large - scale structure along the line - of - view 4 . By measuring this distortion over a broad variety of angular scales , one can reconstruct the three - dimensional matter distribution in the Universe 5 , which contains information about both the topology of the universe and its rise probability 6 .In order to extract useful cosmological information from soft lensing data , accurate measurements of the shapes of background galaxies must first be obtained 7 - 9 . These shape measurements then need to be corrected for distortions induced by atmospheric influence 10 , telescope optics 11 , and point spread system 12 .Finally , they also have to be corrected for distorted forms presented by foreground objects 13 . Intrinsic galaxy alignments ( IAs ) , i . e . , correlations between galaxy orientations 14 or positions 15 , introduce extra systematic errors into the measured shear correlation functions 16 .If not adequately accounted for , IAs might lead to biased estimates of cosmological factors 17 . Several methods have been proposed to mitigate the impact of IAs on cosmological parameter estimations 18 .One approach requires studying the observed galaxy ellipticities as a combination of intrinsic",
        "rewrite_text": "**Title:** Dark Energy Constraints from Cosmic Shear Speed Spectra: The Influence of Intrinsic Alignments on Photometric Redshift Requirements\n\n**Abstract:** This study investigates the effects of intrinsic galaxy alignment (IA) on the constraints of dark energy parameters derived from weak lensing tomography, particularly in the context of upcoming space-based surveys like Euclid and WFIRST. Our findings indicate that IA can significantly skew cosmological parameter estimates when relying solely on spectroscopic redshifts for calibration. However, we demonstrate that incorporating photometric redshifts can effectively mitigate these biases. Specifically, we show that by utilizing at least ten photometric bands covering the wavelength range of 0.4 to 1 micron for photo-z estimation, the bias associated with IA can be reduced to below 1%. This requirement becomes increasingly stringent at higher redshifts, where the density of galaxy clusters diminishes rapidly. The insights gained from this research are crucial for guiding future investigations aimed at measuring dark energy through weak gravitational lensing techniques.\n\nWeak gravitational lensing has become a prominent method for probing dark energy, as it analyzes the distortion of distant galaxy images caused by the gravitational influence of large-scale structures along the line of sight. By quantifying these distortions across various angular scales, researchers can reconstruct the three-dimensional matter distribution in the universe, which provides valuable information about its topology and expansion dynamics. To extract meaningful cosmological data from weak lensing observations, precise measurements of the shapes of background galaxies are essential. These measurements must be corrected for various distortions, including those caused by atmospheric conditions, telescope optics, and the point spread function. Additionally, corrections must be made for the effects of foreground objects. Intrinsic galaxy alignments, which refer to the correlations in the orientations or positions of galaxies, introduce further systematic errors into the shear correlation functions. If not properly addressed, these alignments can lead to biased estimates of cosmological parameters. Various strategies have been proposed to mitigate the influence of IAs on cosmological analyses, including the examination of observed galaxy ellipticities as a combination of intrinsic and lensing-induced effects.",
        "ori-fast-z-score": 0.6209204205650662,
        "water-fast-z-score": 8.279908588115264,
        "rewrite-fast-z-score": 1.1430952132988164
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Unique mechanisms from finite two-state trajectories .\nAbstract:\nWe present an algorithm for the detection and characterization of unique molecular events in time series data, which is based on identifying all possible pairs of states that can be reached by following different paths through the state space. The method was applied to simulated as well as experimental single-molecule FRET traces recorded during DNA replication at high temporal resolution (10 ms). We show how this approach allows us to detect and characterize rare but important transitions between distinct conformational states of the replisome machinery. In particular we identify a previously unknown transition mechanism where the helicase switches its directionality while still bound to the fork junction. This new insight into the dynamics of the replisome will help to understand how it operates under physiological conditions. \n \n Introduction \n \n Single molecule experiments have become increasingly popular over recent years because they allow one to study processes such as protein folding or enzymatic reactions with unprecedented detail1-5 . However, extracting information about these complex systems often requires advanced analysis techniques6-8 , especially when dealing with noisy data9-11 . Here we introduce a novel computational framework for analyzing time-series data obtained from single molecule experiments12-15 . Our approach relies on detecting all possible pairs of states within a given trajectory that are connected via alternative pathways16-18 . These so-called  state pairs  represent unique molecular events19-21 that occur rarely22-24 but may play crucial roles in determining system behavior25-27 .\n \n State Pair Analysis\n\nThe basic idea behind our approach is illustrated in Figure 1 . Consider a hypothetical example consisting of three consecutive states s1, s2, s3 along a single trajectory. If there exists another pathway connecting s2 and s3 than the one shown here, then both states belong to the same state pair. Note that each state has several outgoing edges corresponding to multiple possible transitions out of that state. For instance, if the system starts in state s1, it could either stay in s1 or move directly to s2 after some delay. Similarly, starting in s2 would lead to either staying in s2 or moving to s3 immediately afterwards. Finally, starting in s3 would always result in returning back to s1. As a consequence, any",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Unique mechanisms from finite two - state trajectories . Abstract : We present an algorithm for the discovery and description of unique molecular events in time series information , which is based on discovering all possible sets of states that can be reached by following different paths through the state space .The method was used to modeled as well as experimental single - cell FRET traces recorded during DNA replication at high temporal resolution ( 10 ms ) . We see how this methodology allows us to identify and characterize subtle but essential transitions between distinct conformational states of the replisome machinery .In particular we identify a previously obscure transition process where the helicase changes its directionality while remained tied to the fork intersection . This new insight into the dynamics of the replisome will assist to realize how it operates under physiological circumstances .Introduction Single molecule experiments have developed increasingly popular over recent months because they allow one to study mechanisms such as protein folding or enzymatic reactions with incredible detail1 - 5 . However , extracting information about these complex systems often needs advanced analysis techniques6 - 8 , particularly when dealing with sparse data9 - 11 .Here we provide a new computational framework for studying time - series information obtained from single molecule experiments12 - 15 . Our solution involves on detecting all possible combinations of states within a given path that are connected via alternative pathways16 - 18 .These so - called state pairs reflect unusual molecular events19 - 21 that occur rarely22 - 24 but might play crucial roles in determining process behavior25 - 27 . State Pair Analysis The basic idea behind our approach is depicted in Figure 1 .Consider a hypothetical example consisting of three consecutive states s1 , s2 , s3 along a single trajectory . If there exists another pathway connecting s2 and s3 than the one given here , then both states belong to the same state pair .Note that each state has numerous outgoing edges relating to multiple possible transitions out of that state . For instance , if the program starts in state s1 , it could either stay in s1 or shift directly to s2 after some pause .Similarly , beginning in s2 might lead to either staying in s2 or advancing to s3 immediately afterwards . Finally , beginning in s3 would always lead in coming back to s1 .As a consequence , any",
        "rewrite_text": "**Title:** Unique Mechanisms from Finite Two-State Trajectories\n\n**Abstract:** In this study, we introduce a novel algorithm designed to uncover and characterize unique molecular events within time series data. This approach focuses on identifying all potential sets of states that can be accessed through various pathways in the state space. We applied this method to both modeled and experimental single-cell Förster Resonance Energy Transfer (FRET) traces captured during DNA replication, achieving a high temporal resolution of 10 milliseconds. Our findings demonstrate that this methodology effectively reveals and delineates subtle yet critical transitions between distinct conformational states of the replisome machinery. Notably, we identified a previously overlooked transition where the helicase alters its directionality while remaining tethered to the fork intersection. This new understanding of replisome dynamics enhances our comprehension of its operational mechanisms under physiological conditions.\n\nThe introduction of single-molecule experiments has gained significant traction in recent months, enabling researchers to investigate intricate processes such as protein folding and enzymatic reactions with remarkable precision. However, extracting meaningful insights from these complex systems often necessitates sophisticated analytical techniques, particularly when working with sparse datasets. In response, we present a comprehensive computational framework tailored for analyzing time-series data derived from single-molecule experiments. Our approach centers on detecting all feasible combinations of states within a given trajectory that are interconnected through alternative pathways. These identified state pairs represent rare yet significant molecular events that may play pivotal roles in influencing the behavior of biological processes.\n\nThe foundational concept of our methodology is illustrated in Figure 1. For instance, consider a hypothetical scenario involving three consecutive states, s1, s2, and s3, along a single trajectory. If an alternative pathway exists that connects s2 and s3, then these states are classified as part of the same state pair. Each state possesses multiple outgoing edges, indicating various potential transitions. For example, starting in state s1 may lead to either remaining in s1 or transitioning directly to s2 after a brief pause. Similarly, beginning in s2 could result in either staying in that state or advancing to s3. Ultimately, our analysis provides a deeper understanding of the molecular events that govern biological processes.",
        "ori-fast-z-score": -0.3696106354772864,
        "water-fast-z-score": 7.970869422985724,
        "rewrite-fast-z-score": 0.6581451817144176
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Comprehensive Analysis of the Swift/XRT Data: II. Diverse Physical Origins of the Shallow Decay Segment .\nAbstract:\nWe present an analysis of the shallow decay segment in X-ray afterglow light curves observed by Swift/XRT, which is based on our previous work (Zhang et al., 2006) . We find that there are two types of shallow decays with different physical origins. The first type has been widely discussed and can be explained as arising from either energy injection or refreshed shocks. However, we show that this scenario cannot explain all cases of shallow decays. In particular, it fails to account for those shallow decays occurring at late times when the external shock emission should have faded away completely. For these events, we propose another explanation -the second type of shallow decays-which may arise from the transition between the relativistic fireball phase and non-relativistic ejecta-dominated phase. This new interpretation naturally explains why some shallow decays occur only at late times while others appear both early and late during the afterglow evolution. \n \n Keywords: Afterglow, Energy injection, Fireball model",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Comprehensive Analysis of the Swift / XRT Data : II . Diverse Physical Origins of the Shallow Decay Segment .Abstract : We present an assessment of the narrow degradation segment in X - ray afterglow light curves observed by Swift / XRT , which is based on our previous research ( Zhang et al . , 2006 ) . We see that there are two forms of shallow decays with various physical origins .The first sort has been widely discussed and can be described as occurring from either power injection or refreshed shocks . However , we prove that this situation cannot explain all cases of shallow decays .In particular , it fails to explain for those shallow decays resulting at late times when the external shock emission should have fading away completely . For these events , we propose another explanation - the second kind of shallow decays - which would occur from the shift between the relativistic fireball phase and non - relativistic ejecta - dominated phase .This new theory naturally explains why some shallow decays occur only at late times while several occur both late and late during the afterglow evolution . Keywords : Afterglow , Energy injection , Fireball model",
        "rewrite_text": "Title: A Comprehensive Analysis of Swift/XRT Data: II. Diverse Physical Origins of the Shallow Decay Segment\n\nAbstract: In this study, we conduct a thorough examination of the shallow decay segment observed in X-ray afterglow light curves captured by the Swift/XRT satellite, building upon our earlier findings (Zhang et al., 2006). Our analysis reveals the existence of two distinct types of shallow decays, each arising from different physical mechanisms. The first type, which has been extensively discussed in the literature, is attributed to processes such as energy injection or refreshed shocks. However, we demonstrate that this explanation is insufficient to account for all observed shallow decay instances. Notably, it fails to elucidate the shallow decays that manifest at later times when the emission from the external shock is expected to have diminished significantly. To address this limitation, we introduce a second category of shallow decays, which we propose arises from the transition between the relativistic fireball phase and a non-relativistic ejecta-dominated phase. This novel framework effectively clarifies why certain shallow decays are exclusively observed at late times, while others are present during both early and late stages of the afterglow evolution. Our findings contribute to a deeper understanding of the complex dynamics governing X-ray afterglows and highlight the need for further investigation into the underlying mechanisms driving these phenomena. \n\nKeywords: Afterglow, Energy injection, Fireball model.",
        "ori-fast-z-score": -1.3093073414159544,
        "water-fast-z-score": 3.4914862437758782,
        "rewrite-fast-z-score": 1.2809280616135812
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The formation of spiral arms and rings in barred galaxies .\nAbstract:\nWe present an analysis of the distribution of gas, stars and dust in two nearby edge-on spirals with prominent bars (NGC 1365 and NGC 1530). We use high-resolution observations obtained by the Herschel Space Observatory to study the physical conditions of the interstellar medium along these systems. The main results are as follows:  - In both cases we find that the molecular hydrogen is concentrated on the leading edges of the bar, while atomic hydrogen follows closely the stellar light.  - The star formation rate peaks at the ends of the bar where the density of molecular hydrogen increases significantly. This suggests that the gravitational torques induced by the bar can trigger the collapse of dense clouds into new generations of young stars.  - The infrared emission associated with polycyclic aromatic hydrocarbons shows a clear correlation between the location of this component and the regions of active star formation. - The comparison of our data with hydrodynamical simulations indicates that the observed structure of the ISM may be explained if the bar potential has been able to drive significant amounts of cold gas towards its inner Lindblad resonance.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The formation of spiral arms and rings in barred clusters . Abstract : We report an assessment of the distribution of gas , stars and dust in two adjacent edge - on spirals with prominent bars ( NGC 1365 and NGC 1530 ) .We use large - resolution measurements obtained by the Herschel Space Observatory to study the physical conditions of the interstellar medium along these systems . The main results are as follows : - In both cases we find that the molecular hydrogen is confined on the led corners of the bar , while nuclear hydrogen takes closely the stellar radiation .- The star formation rate peaks at the ends of the bar where the density of molecular hydrogen rises considerably . This implies that the gravitational torques induced by the bar can cause the collapse of dense clouds into new generations of young stars .- The infrared absorption associated with polycyclic aromatic hydrocarbons reveals a clear correlation between the location of this constituent and the regions of active star formation . - The comparison of our information with hydrodynamical simulations indicates that the known composition of the ISM may be described if the bar potential has been able to drive considerable amounts of cold gas towards its outer Lindblad resonance .",
        "rewrite_text": "We present a comprehensive analysis of the distribution of gas, stars, and dust in two neighboring edge-on spiral galaxies featuring prominent bars, specifically NGC 1365 and NGC 1530. Utilizing high-resolution data acquired from the Herschel Space Observatory, we investigate the physical conditions of the interstellar medium (ISM) within these systems. Our findings reveal several key insights: Firstly, we observe that molecular hydrogen is predominantly concentrated at the leading edges of the bars, while atomic hydrogen is closely associated with stellar radiation in the nuclear regions. Secondly, the star formation rate exhibits a pronounced peak at the ends of the bars, where there is a significant increase in the density of molecular hydrogen. This observation suggests that the gravitational torques generated by the bar structure facilitate the collapse of dense molecular clouds, leading to the formation of new stellar populations. Additionally, our analysis of infrared absorption features linked to polycyclic aromatic hydrocarbons (PAHs) indicates a strong correlation between the distribution of these molecules and regions of active star formation. Finally, by comparing our observational data with hydrodynamical simulations, we conclude that the existing composition of the ISM can be effectively explained if the bar potential is capable of channeling substantial amounts of cold gas toward the outer Lindblad resonance. This study enhances our understanding of the intricate processes governing star formation and the dynamics of the ISM in barred spiral galaxies.",
        "ori-fast-z-score": 1.2935483472729858,
        "water-fast-z-score": 6.207574452435047,
        "rewrite-fast-z-score": 0.3713906763541037
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Groups of galaxies in the SDSS Data Release 5. A group-finder and a catalogue .\nAbstract:\nWe present an algorithm to find galaxy groups using photometric redshifts, which is based on the Voronoi tessellation technique (VT). The VT method has been widely used for identifying clusters of galaxies with spectroscopic redshifts but it had not yet been applied to identify galaxy groups with photometric redshifts. We use the Sloan Digital Sky Survey data release five (SDSS DR5) as our input sample. Our results show that the VT method can be successfully applied to identify galaxy groups even when only photometric redshifts are available. In this work we have identified more than 12000 galaxy groups at 0 < z < 0.3. These groups contain about 30000 member galaxies. We also provide a catalog containing basic information such as positions, magnitudes, colors, and photometric redshifts for all these groups. \n \n Keywords: Galaxy Group, Photometric Redshift",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Groups of galaxies in the SDSS Data Release 5 . A group - finder and a catalogue .Abstract : We present an algorithm to find galaxy groups using photometric redshifts , which is based on the Voronoi tessellation method ( VT ) . The VT method has been widely applied for finding clusters of clusters with spectroscopic redshifts but it had not already been used to identify galaxy groups with photometric redshifts .We use the Sloan Digital Sky Survey data release five ( SDSS DR5 ) as our input data . Our results show that the VT method can be successfully applied to identify galaxy groups even when only photometric redshifts are available .In this research we have discovered more than 12000 star groups at 0 < z < 0 . 3 . These groups contain about 30000 member galaxies .We additionally offer a list containing essential information such as positions , magnitudes , colors , and photometric redshifts for all these groups . Keywords : Galaxy Group , Photometric Redshift",
        "rewrite_text": "In this article, we introduce a novel algorithm designed to identify groups of galaxies utilizing photometric redshifts, employing the Voronoi tessellation (VT) method. While the VT method has been extensively utilized for detecting clusters of galaxies with spectroscopic redshifts, its application to galaxy groups identified through photometric redshifts has not been previously explored. Our study leverages data from the Sloan Digital Sky Survey Data Release 5 (SDSS DR5) as the foundational dataset for our analysis. The results of our investigation demonstrate that the VT method is effective in identifying galaxy groups, even in scenarios where only photometric redshifts are accessible. Throughout our research, we have successfully identified over 12,000 galaxy groups within the redshift range of 0 < z < 0.3, which collectively encompass approximately 30,000 member galaxies. Furthermore, we provide a comprehensive catalogue that includes critical information for each identified group, such as their spatial positions, magnitudes, colors, and photometric redshifts. This contribution not only enhances the understanding of galaxy group structures but also serves as a valuable resource for future astronomical research. Our findings underscore the potential of the VT method in the realm of photometric redshift analysis, paving the way for further studies in galaxy formation and evolution. Keywords associated with this research include Galaxy Group and Photometric Redshift.",
        "ori-fast-z-score": -1.6644794391276478,
        "water-fast-z-score": 2.794002794004191,
        "rewrite-fast-z-score": -0.10976425998969035
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The HARPS search for southern extra-solar planets. IX. Exoplanets orbiting HD 100777, HD 190647, and HD 221287 .\nAbstract:\nWe report the detection of three new exoplanet candidates around stars with masses between 1.1 M⊙ and 2.0 M⊙ using high-precision radial-velocity measurements obtained by the High Accuracy Radial velocity Planet Searcher (HARPS) spectrograph on the 3.6-m telescope at La Silla Observatory in Chile. The orbital periods are found to be between 4 days and 12 years. We also present an analysis of the host star properties based on photometric observations made with the CoRoT space mission as well as spectroscopic data taken with HARPS. For two of these systems we find that the planet mass is likely below Neptune s mass but above Mercury s mass. In addition, one system shows evidence for additional bodies in its planetary system. These results demonstrate the power of combining different techniques to detect extrasolar planets. Keywords: Extra-solar planet - HARPS",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The HARPS hunt for southern extra - solar planets . IX .Exoplanets orbiting HD 100777 , HD 190647 , and HD 221287 . Abstract : We report the observation of three newest exoplanet candidates around stars with masses between 1 . 1 [UNK] and 2 . 0 [UNK] using high - precision radial - speed measurements obtained by the High Accuracy Radial velocity Planet Searcher ( HARPS ) spectrograph on the 3 . 6 - m observatory at La Silla Observatory in Chile .The orbital periods are found to be between 4 weeks and 12 years . We additionally offer an assessment of the host star dynamics based on photometric surveys made with the CoRoT space flight as well as spectroscopic data taken with HARPS .For two of these systems we find that the planet mass is probably below Neptune s mass but above Mercury s mass . In addition , one system displays evidence for additional body in its planetary system .These data demonstrate the power of combining different methods to identify extrasolar planets . Keywords : Extra - solar planet - HARPS",
        "rewrite_text": "Title: The HARPS Hunt for Southern Extra-Solar Planets IX: Exoplanets Orbiting HD 100777, HD 190647, and HD 221287\n\nAbstract: In this study, we present the discovery of three new exoplanet candidates orbiting stars with masses ranging from 1.1 to 2.0 solar masses. These findings are based on high-precision radial velocity measurements obtained using the High Accuracy Radial velocity Planet Searcher (HARPS) spectrograph located at the 3.6-meter telescope at La Silla Observatory in Chile. The identified exoplanets exhibit orbital periods that vary significantly, spanning from approximately 4 weeks to 12 years. To enhance our understanding of the dynamics of the host stars, we also incorporate data from photometric surveys conducted by the CoRoT space mission, alongside spectroscopic observations from HARPS. Our analysis suggests that for two of the identified planetary systems, the masses of the exoplanets are likely to be less than that of Neptune but greater than that of Mercury, indicating a diverse range of planetary compositions. Furthermore, one of the systems shows potential evidence of an additional celestial body within its planetary system, hinting at a more complex architecture. These findings underscore the effectiveness of integrating multiple observational techniques to uncover and characterize extrasolar planets, thereby advancing our knowledge of planetary systems beyond our own. The results contribute to the growing catalog of exoplanets and highlight the capabilities of HARPS in the ongoing search for southern hemisphere exoplanets. \n\nKeywords: Exoplanet, HARPS, radial velocity, planetary dynamics, CoRoT.",
        "ori-fast-z-score": 0.47140452079103173,
        "water-fast-z-score": 5.579886659703326,
        "rewrite-fast-z-score": -0.10050378152592121
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  SubmilliJansky Transients in Archival Radio Observations .\nAbstract:\nWe have searched for radio transients with flux densities between 0.1 and 1 mJy at frequencies ranging from 4 to 8 GHz using archival data obtained by the Very Large Array (VLA) over the past 20 years. We find that most of these sources are extragalactic, but we also detect several Galactic objects including pulsars, supernova remnants, and flare stars. The majority of our sample is comprised of previously uncatalogued sources; however, we recover many known variable sources such as blazars and gamma-ray burst afterglows. Our results demonstrate the power of combining large amounts of archival VLA data into one coherent dataset. This work was supported by NSF grant AST-0907860. In this Letter, we present an analysis of all available archived Very Large Array (V LA) observations taken since 1990. These data were collected during various observing programs aimed primarily at studying distant galaxies or nearby star forming regions. However, they contain valuable information about fainter transient phenomena occurring within our Galaxy. By searching through more than 10 000 hours of observation time spread across nearly 2000 epochs, we identify thousands of new faint radio sources which appear only once or twice in each epoch s data set. Most of these sources are extragalaxtic, but we also detect numerous Galactic objects including pulsar wind nebulae, supernova remnants, flare stars, and other types of active galactic nuclei. Many of these newly discovered sources are not included in existing catalogs because their low signal-to-noise ratio makes them difficult to detect when observed individually. However, by combining multiple epochs together, we can boost the sensitivity of our survey enough to detect even very weak signals.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : SubmilliJansky Transients in Archival Radio Observations . Abstract : We have searched for radio transients with flux densities between 0 . 1 and 1 mJy at frequencies ranging from 4 to 8 GHz using archival measurements obtained by the Very Large Array ( VLA ) over the previous 20 decades .We see that most of these sources are extragalactic , but we also observe numerous Galactic items including pulsars , supernova remnants , and flare stars . The majority of our sample is comprised of previously uncatalogued sources ; however , we recover several known variable sources such as blazars and gamma - ray flare afterglows .Our results show the power of combining enormous amounts of archival VLA information into one coordinated dataset . This project was supported by NSF grant AST - 0907860 .In this Letter , we present an assessment of all available archived Very Large Array ( V LA ) observations made since 1990 . These data were collected during various observing programs aimed exclusively at studying nearby galaxies or neighboring star producing regions .However , they contain significant information about fainter transient phenomena occurring within our Galaxy . By looking through more than 10 000 hours of study time dispersed across nearly 2000 epochs , we identify thousands of new faint radio sources which appear only once or repeatedly in each epoch s information pool .Most of these sources are extragalaxtic , but we also observe numerous Galactic bodies including pulsar wind nebulae , supernova remnants , flare stars , and other types of active galactic nuclei . Many of these newly discovered sources are not covered in existing catalogs because their low signal - to - noise proportion creates them harder to identify when observed individually .However , by combining multiple epochs combined , we can boost the sensitivity of our survey sufficiently to identify especially very weak signals .",
        "rewrite_text": "In this study, we investigate radio transients with flux densities ranging from 0.1 to 1 mJy at frequencies between 4 and 8 GHz, utilizing archival data collected by the Very Large Array (VLA) over the past two decades. Our analysis reveals that a significant portion of these sources are extragalactic; however, we also detect a variety of Galactic objects, including pulsars, supernova remnants, and flare stars. The majority of the sources identified in our survey are previously uncatalogued, although we successfully recover several known variable sources, such as blazars and gamma-ray flare afterglows. \n\nThis research underscores the value of integrating extensive archival VLA data into a cohesive dataset, allowing for a more comprehensive exploration of faint transient phenomena. Supported by NSF grant AST-0907860, our project involved a thorough examination of all available VLA observations since 1990, which were primarily collected during observing programs focused on nearby galaxies and star-forming regions. Despite their original intent, these observations contain crucial information about fainter transient events occurring within our own Galaxy.\n\nBy analyzing over 10,000 hours of observational data spread across nearly 2,000 epochs, we have identified thousands of new faint radio sources that appear either sporadically or repeatedly within the dataset. While many of these sources are extragalactic, we also catalog numerous Galactic entities, including pulsar wind nebulae, supernova remnants, flare stars, and various active galactic nuclei. Notably, many of these newly discovered sources are absent from existing catalogs due to their low signal-to-noise ratios, which complicate their identification in isolated observations. However, by aggregating data from multiple epochs, we enhance the sensitivity of our survey, enabling the detection of particularly weak signals that would otherwise go unnoticed. This work contributes to a deeper understanding of transient radio phenomena and highlights the potential of archival data in uncovering new astrophysical insights.",
        "ori-fast-z-score": 0.8770580193070293,
        "water-fast-z-score": 8.465910570697329,
        "rewrite-fast-z-score": 1.8593393604027364
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Asymptotics for Duration-Driven Long Range Dependent Processes .\nAbstract:\nWe consider the asymptotic behavior of the sample autocovariance function and spectral density functions of stationary processes with regularly varying marginal distributions, which are driven by an infinite order moving average process whose coefficients have regularly varying tails. We show that these quantities can be approximated by those of a finite order autoregressive-moving average (ARMA) model in the sense of weak convergence under some conditions on the tail behaviors of the ARMA parameters. The results obtained here extend previous ones given in the literature to more general cases where the driving noise is not necessarily Gaussian or has non-Gaussian components. In particular, we provide new proofs for the existing results when the driving noise is purely Gaussian. Our approach relies heavily on recent developments made in the theory of regular variation and stochastic calculus. As applications, we study two examples of duration-driven LRD models, namely the Cox-Ingersoll-Ross (CIR) model and the Vasicek model.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Asymptotics for Duration - Driven Long Range Dependent Processes . Abstract : We consider the asymptotic behavior of the sample autocovariance function and spectral density functions of stationary systems with regularly varying marginal distributions , which are driven by an endless order moving average process whose coefficients have regularly varying tails .We see that these quantities can be approximated by those of a finite order autoregressive - pushing estimated ( ARMA ) model in the sense of weak convergence under some conditions on the tail behaviors of the ARMA parameters . The results derived here extend previous ones given in the writings to more general instances where the driving noise is not necessarily Gaussian or has non - Gaussian components .In particular , we provide fresh proofs for the established results when the driving signal is purely Gaussian . Our solution draws highly on current developments developed in the theoretical of regular variation and stochastic equations .As applications , we study two examples of duration - based LRD models , namely the Cox - Ingersoll - Ross ( CIR ) model and the Vasicek theory .",
        "rewrite_text": "Title: Asymptotics for Duration-Driven Long Range Dependent Processes\n\nAbstract: This article investigates the asymptotic properties of the sample autocovariance and spectral density functions in stationary systems characterized by regularly varying marginal distributions. These systems are influenced by an infinite-order moving average process, where the coefficients exhibit regularly varying tails. We demonstrate that under certain conditions related to the tail behaviors of the autoregressive moving average (ARMA) parameters, the asymptotic behavior of these statistical quantities can be approximated by that of a finite-order ARMA model, specifically in the context of weak convergence. Our findings broaden the scope of previous research by accommodating scenarios where the driving noise may not be Gaussian or may contain non-Gaussian elements. Notably, we present new proofs for previously established results in cases where the driving signal is purely Gaussian. Our approach relies heavily on recent advancements in the theory of regular variation and stochastic equations. To illustrate the practical implications of our theoretical results, we analyze two specific examples of duration-driven long-range dependent (LRD) models: the Cox-Ingersoll-Ross (CIR) model and the Vasicek model. Through these examples, we highlight the relevance of our findings in the context of financial modeling and other applications where duration-driven processes play a critical role. This work contributes to a deeper understanding of the asymptotic behavior of LRD processes and offers a framework for further exploration in both theoretical and applied settings.",
        "ori-fast-z-score": -0.5423261445466404,
        "water-fast-z-score": 3.6663142889169062,
        "rewrite-fast-z-score": 1.7717591356565963
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Sound speed of a Bose-Einstein condensate in an optical lattice .\nAbstract:\nWe study the sound propagation through a one-dimensional (1D) Bose-Einstein condensed gas trapped by an optical lattice potential and interacting with each other via contact interactions. We show that, for weak interaction strength, there is no phonon-phonon scattering between different bands due to the energy gap induced by the periodic potential. In this case, we find that the sound velocity can be obtained analytically using perturbation theory. For strong interaction strengths, however, the phonons are scattered into higher bands and thus the sound velocity decreases as compared to its non-interacting value. The results agree well with numerical calculations based on the Gross-Pitaevskii equation. PACS numbers: 03.75.Dg, 05.30.Jp, 37.10.Gh \nI. INTRODUCTIO N\nThe properties of superfluid helium have been studied extensively since it was discovered more than half century ago  1  . One of the most important features of superfluids is their ability to support dissipationless flow without friction  2  , which has led to many applications such as superconductors  3  .\nRecently, ultracold atomic gases confined in optical lattices provide another platform to explore quantum fluids  4  . These systems exhibit various phases including Mott insulator phase  5  , supersolid phase  6  , and even topological states  7, 8  . Moreover, they allow us to tune the system parameters continuously  9  and observe directly the evolution of physical quantities  10  . This makes them ideal candidates to investigate new phenomena predicted by theoretical studies  11  .\nIn particular, bosonic atoms in optical lattices may form a BoseEinstein condensate  12  . It is known that these condensates behave like superfluids  13  . Recently, several experiments have observed the superflow  14  and vortex  15  in these systems. However, unlike conventional superfluids, the condensates in optical lattices also interact strongly with each other  16  . Therefore, understanding how the interatomic interactions affect the collective excitations becomes crucial  17  .\nIn this work, we consider 1D Bose-Einstein condensates trapped by an optical lattice  18  . By solving the",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Sound velocity of a Bose - Einstein condensate in an optical lattice . Abstract : We research the music transmission through a one - dimensional ( 1D ) Bose - Einstein condensed gas trapped by an optical lattice potential and evolving with each other via contact interactions .We see that , for weak interaction strength , there is no phonon - phonon propagation between various bands due to the power gap induced by the periodic potential . In this situation , we find that the audio speed can be obtained analytically using perturbation theory .For strong coupling strengths , however , the phonons are scattered into greater bands and therefore the audio speed falls as compared to its non - interacting value . The results agree well with numerical measurements based on the Gross - Pitaevskii equation .PACS codes : 03 . 75 . Dg , 05 . 30 . Jp , 37 . 10 . Gh I . INTRODUCTIO N The properties of superfluid helium have been studied thoroughly since it was discovered more than quarter century ago 1 . One of the most important features of superfluids is their ability to support dissipationless flow without tension 2 , which has led to many applications such as superconductors 3 .Recently , ultracold atomic fluids confined in laser lattices offers another platform to study quantum fluids 4 . These systems exhibit several stages namely Mott insulator stage 5 , supersolid phase 6 , and even topological states 7 , 8 .Moreover , they allow us to balance the process variables continuously 9 and observe directly the evolution of physical substances 10 . This gives them ideal candidates to examine novel processes proposed by theoretical experiments 11 .In particular , bosonic atoms in optical lattices might form a BoseEinstein condensate 12 . It is known that these condensates behave like superfluids 13 .Recently , various observations have discovered the superflow 14 and vortex 15 in these systems . However , unlike conventional superfluids , the condensates in optical lattices still interact heavily with each other 16 .Therefore , studying how the interatomic interactions impact the collective excitations remains crucial 17 . In this research , we study 1D Bose - Einstein condensates trapped by an optical lattice 18 .By solving the",
        "rewrite_text": "**Title:** Sound Velocity of a Bose-Einstein Condensate in an Optical Lattice\n\n**Abstract:** This study investigates the propagation of sound in a one-dimensional (1D) Bose-Einstein condensate (BEC) confined within an optical lattice potential, where the atoms interact through contact interactions. Our findings reveal that, under conditions of weak interaction strength, phonon-phonon interactions across different energy bands are inhibited due to the energy gap created by the periodic potential of the lattice. In this regime, we derive an analytical expression for the sound velocity using perturbation theory. Conversely, in the regime of strong interactions, phonons are scattered into higher energy bands, resulting in a reduction of sound velocity compared to the value observed in non-interacting systems. These theoretical predictions align closely with numerical simulations based on the Gross-Pitaevskii equation, validating our approach. \n\nThe study of superfluid helium has provided significant insights into the nature of superfluidity, particularly its capacity for dissipationless flow, which has implications for various applications, including superconductivity. Recently, ultracold atomic gases in optical lattices have emerged as a novel platform for exploring quantum fluids, exhibiting phases such as Mott insulators, supersolids, and topological states. These systems allow for continuous tuning of experimental parameters, enabling direct observation of the dynamics of quantum states. In particular, bosonic atoms in optical lattices can form BECs, which are known to exhibit superfluid characteristics. Recent experimental observations have confirmed the presence of superflow and vortices in these systems. However, unlike traditional superfluids, BECs in optical lattices experience significant interatomic interactions, making it essential to understand how these interactions influence collective excitations. This research focuses on the behavior of 1D BECs in an optical lattice, providing insights into the interplay between interaction strength and sound propagation in quantum fluids.",
        "ori-fast-z-score": -0.08247860988423225,
        "water-fast-z-score": 8.003675626198989,
        "rewrite-fast-z-score": 0.17149858514250882
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Nature of Generic Cosmological Singularities .\nAbstract:\nWe present an overview of the generic cosmological singularity problem, and its possible solutions in string theory. We discuss how this issue is related to other problems such as black hole entropy, time travel paradoxes, and information loss. Finally we review some recent developments on these topics. The nature of generic cosmological singularities has been one of the most important open questions in theoretical physics for many years. In particular it is not known whether or not there are any physical processes that can take place at all times near the big bang (or equivalently near the big crunch). This question is intimately connected with several other fundamental issues including black hole entropy, time machine paradoxes, and information loss/reduction. It also plays a crucial role in understanding the very early universe. Recently significant progress has been made towards answering this question using techniques developed within the context of string theory. Here I will give a brief summary of what is currently understood about this topic.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Nature of Generic Cosmological Singularities . Abstract : We present an overview of the generic cosmological singularity issue , and its likely solutions in string theory .We discuss how this question is related to other issues such as black hole entropy , time trip paradoxes , and information loss . Finally we review some latest advances on these topics .The nature of generic cosmological singularities has been one of the most important open questions in theoretical physics for many years . In particular it is not clear whether or not there are any physical processes that can take happen at all times near the big bang ( or equivalently near the big crunch ) .This problem is intimately tied with various other fundamental issues including black hole entropy , time computer paradoxes , and information loss / reduction . It additionally serves a crucial role in understanding the very earliest universe .Recently significant progress has been achieved towards answering this question using techniques established within the context of string theory . Here I will giving a brief summary of what is currently understood about this question .",
        "rewrite_text": "In this article, we provide a comprehensive overview of the issue surrounding generic cosmological singularities and explore potential resolutions within the framework of string theory. The investigation into these singularities is critical, as it intersects with several fundamental topics in theoretical physics, including black hole entropy, time travel paradoxes, and the phenomenon of information loss. The nature of generic cosmological singularities remains one of the most pressing unresolved questions in the field, particularly concerning the physical processes that may occur in the vicinity of the big bang or big crunch. This inquiry is not only pivotal for our understanding of singularities but also plays a significant role in elucidating the conditions of the early universe.\n\nRecent advancements in string theory have shed light on these complex issues, offering new insights and potential pathways for resolution. We will summarize the current state of knowledge regarding generic cosmological singularities, highlighting the connections to black hole entropy and the implications for time travel scenarios. Additionally, we will discuss the ongoing debates surrounding information loss and its implications for the fundamental laws of physics. By synthesizing these various strands of research, we aim to provide a clearer picture of the challenges and progress made in addressing the nature of cosmological singularities. This article serves as a critical resource for researchers seeking to understand the intricate relationships between these fundamental concepts and the implications for the broader field of cosmology.",
        "ori-fast-z-score": 0.6396021490668313,
        "water-fast-z-score": 4.8488257455915145,
        "rewrite-fast-z-score": -0.3713906763541037
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Galaxy Evolution and Environment .\nAbstract:\nThe evolution of galaxies is strongly influenced by their environment, but the physical processes that drive this are not well understood. In particular, it remains unclear how galaxy properties such as morphology or star formation rate depend on large-scale structure in the Universe.  This talk will present results from an ongoing study using data from the Sloan Digital Sky Survey (SDSS) to investigate these issues. The first part of my talk will focus on the relationship between galaxy clustering and luminosity/stellar mass. I will then discuss recent work investigating the dependence of galaxy properties on local density. Finally, I ll show some preliminary results exploring the connection between galaxy properties and dark matter halo masses. My research has been supported by NSF grant AST-0707766. I am currently a postdoctoral fellow at Harvard University working with Prof. David Weinberg. I received my Ph.D. from UCLA under the supervision of Dr. James Bullock.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Galaxy Evolution and Environment . Abstract : The evolution of stars is strongly altered by their environment , but the natural processes that drive this are not well understood .In particular , it remains unsure how star properties such as geometry or star formation rate depend on huge - scale organization in the Universe . This discussion will present results from an continuing survey employing data from the Sloan Digital Sky Survey ( SDSS ) to examine these problems .The first part of my talk will focus on the relationship between galaxy clustering and luminosity / stellar mass . I will then discuss latest work investigating the dependence of galaxy structures on local density .Finally , I ll show some preliminary results studying the link between galaxy structures and dark matter halo masses . My research has been supported by NSF grant AST - 0707766 .I am currently a postdoctoral scholar at Harvard University working with Prof . David Weinberg . I received my Ph . D . from UCLA under the guidance of Dr . James Bullock .",
        "rewrite_text": "**Title: Galaxy Evolution and Environment**\n\n**Abstract:** The interplay between stellar evolution and environmental factors is a complex phenomenon that remains inadequately understood. This article addresses the critical question of how various properties of stars, including their geometric configurations and star formation rates, are influenced by large-scale structures within the Universe. We present findings from an ongoing survey that utilizes data from the Sloan Digital Sky Survey (SDSS) to explore these intricate relationships. The initial segment of this discussion will delve into the correlation between galaxy clustering and key metrics such as luminosity and stellar mass. Following this, we will examine recent investigations that assess how the structural characteristics of galaxies are affected by local density variations. Additionally, I will present preliminary findings that explore the connection between galaxy structures and the masses of their associated dark matter halos. This research is supported by the National Science Foundation under grant AST-0707766. Currently, I am a postdoctoral scholar at Harvard University, collaborating with Professor David Weinberg. I completed my Ph.D. at UCLA, where I was mentored by Dr. James Bullock. Through this work, we aim to enhance our understanding of the mechanisms driving galaxy evolution in relation to their environments, thereby contributing to the broader field of astrophysics.",
        "ori-fast-z-score": 0.46499055497527714,
        "water-fast-z-score": 5.27656187902292,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Ages for illustrative field stars using gyrochronology: viability, limitations and errors .\nAbstract:\nWe present an analysis of the ages derived by applying the gyrochronological method to a sample of open clusters with known ages (from literature) in order to assess its reliability as well as possible systematics associated with it. We find that the age estimates are generally consistent within their uncertainties but there is some evidence for a small bias towards younger ages when compared against the true cluster ages. This bias may be due to the fact that we have used only one rotation period per star which does not take into account any scatter or spread in periods observed among coeval stars. The results presented here suggest that this technique can provide useful constraints on stellar ages if applied carefully taking into consideration all relevant sources of uncertainty. Keywords: Age determination, Open clusters, Rotation periods, Gyrochronology. 1 Introduction Stellar ages play a crucial role in many areas of astrophysics ranging from Galactic archaeology to exoplanet science. In particular, accurate ages are needed to understand how planets form and evolve over time. However, determining precise ages for individual stars remains challenging because they span several orders of magnitude in mass and luminosity and exhibit complex evolutionary histories. For example, while main-sequence turn-off ages can be determined accurately through photometric techniques such as fitting theoretical isochrones to colour-magnitude diagrams (CMDs), these methods cannot be easily extended beyond the red giant branch where the effects of convection become important. Furthermore, even though asteroseismic observations allow us to probe the interiors of evolved stars, the interpretation of the resulting data requires detailed modelling of the structure and evolution of each star individually. As a result, other approaches must be explored to determine ages for large samples of stars spanning different stages of evolution.\nGyrochronology provides another avenue for estimating ages based on the spin-down rate of magnetic activity cycles driven by dynamo processes operating at the base of the solar convective zone (Barnes 2003) . It has been shown that the Rossby number R o , defined as the ratio between the rotation period P rot and the convective overturning timescale",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Ages for illustrative field stars employing gyrochronology : viability , difficulties and errors . Abstract : We present an assessment of the years derived by using the gyrochronological method to a sample of open clusters with established periods ( from literature ) in order to examine its reliability as well as possible systematics associated with it .We see that the age values are widely consistent within their uncertainties but there is some evidence for a small prejudice regarding younger ages when compared against the true cluster ages . This bias could be due to the fact that we have used only one rotation cycle per star which does not take into consideration any scatter or spread in dates observed among coeval stars .The results presented here suggest that this methods can provide useful limitations on stellar ages if applied properly take into consideration all relevant sources of uncertainty . Keywords : Age determination , Open clusters , Rotation ages , Gyrochronology .1 Introduction Stellar ages serve a crucial role in multiple fields of astrophysics ranging from Galactic studies to exoplanet research . In particular , detailed years are needed to explain how planets form and evolve over time .However , determining exact periods for individual stars stays difficult because they span many orders of magnitude in mass and luminosity and possess intricate developmental histories . For instance , while main - sequence turn - off ages can be determined accurately through photometric strategies such as fitting theoretical isochrones to colour - magnitude diagrams ( CMDs ) , these procedures cannot be easily extended beyond the red dwarf branch where the effects of convection become crucial .Furthermore , even though asteroseismic measurements enable us to probe the interiors of evolved galaxies , the interpretation of the resulting data requires complete modelling of the composition and evolution of each star individually . As a result , other methods needs be investigated to identify ages for large specimens of stars spanning multiple stages of evolved .Gyrochronology offers another avenue for estimating ages relying on the spin - down frequency of magnetic activity periods coupled by dynamo mechanisms operating at the base of the solar convective zone ( Barnes 2003 ) . It has been shown that the Rossby number R o , defined as the proportion between the rotation history P rot and the convective overturning timescale",
        "rewrite_text": "**Title:** Ages for Illustrative Field Stars Using Gyrochronology: Viability, Challenges, and Errors\n\n**Abstract:** In this study, we evaluate the ages derived from the gyrochronological method applied to a selection of open clusters with well-established rotation periods sourced from existing literature. Our objective is to assess the reliability of this method and to identify any systematic biases that may arise. The analysis reveals that the age estimates are generally consistent within their associated uncertainties; however, we observe a slight tendency towards underestimating the ages of younger stars when compared to the true ages of the clusters. This potential bias may stem from our reliance on a single rotation cycle per star, which does not account for the variability and spread of rotation periods observed among coeval stars. The findings suggest that while gyrochronology can serve as a valuable tool for constraining stellar ages, it is essential to apply the method judiciously, taking into account all relevant uncertainties. \n\nStellar age determination is pivotal across various domains of astrophysics, including Galactic studies and exoplanet research, as accurate ages are crucial for understanding planetary formation and evolution. However, the challenge of determining precise ages for individual stars arises from their vast range of masses, luminosities, and complex evolutionary histories. For example, while main-sequence turn-off ages can be accurately derived using photometric techniques such as fitting theoretical isochrones to color-magnitude diagrams (CMDs), these methods encounter limitations beyond the red dwarf branch, where convective effects become significant. Although asteroseismic measurements provide insights into the interiors of evolved stars, interpreting this data necessitates comprehensive modeling of each star's composition and evolution. Consequently, alternative methods, such as gyrochronology, are essential for estimating ages across a broad spectrum of stellar evolution stages. Gyrochronology leverages the relationship between the spin-down frequency of magnetic activity cycles and the dynamo processes occurring at the base of the solar convective zone, as described by Barnes (2003). The Rossby number (Ro), which compares the rotation period (Prot) to the convective overturning timescale, plays a critical role in this methodology. \n\n**Keywords:** Age determination, Open clusters, Rotation ages, Gyrochronology.",
        "ori-fast-z-score": -0.07312724241271307,
        "water-fast-z-score": 9.286096001408149,
        "rewrite-fast-z-score": -0.3104602102825331
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Fluidization of a vertically oscillated shallow granular layer .\nAbstract:\nWe report on the fluidization of a thin, horizontally vibrating granular bed by vertical oscillations at low frequency and amplitude. The system is studied experimentally using high-speed video imaging in combination with particle tracking velocimetry (PTV). We find that for sufficiently large vibration amplitudes, particles are ejected from the surface into the air as they reach their maximum height during an upward motion. This leads to the formation of a dilute gas phase above the dense packing near the bottom of the container. In addition we observe convection rolls which transport grains across the entire depth of the bed. These results show striking similarities to those observed in vibrated beds of sand or glass beads but differ significantly from previous studies performed on systems where only horizontal vibrations were applied. Vibrations can induce transitions between different states of matter such as solids, liquids, and gases  1  . For example, it has been shown that a solid state may become unstable when subjected to periodic forcing  2  , resulting in the spontaneous generation of traveling waves  3  . A particularly interesting case occurs if both horizontal and vertical components of the driving force act simultaneously  4  .\nIn this work we study the response of a thin granular layer to simultaneous application of horizontal and vertical vibrations. Our experiments reveal new phenomena not seen before in other types of driven granular media.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Fluidization of a horizontally oscillated superficial granular layer . Abstract : We report on the fluidization of a thin , vertically vibrating granular bed by vertical oscillations at low frequency and amplitude .The system is studied experimentally utilizing large - speed video scanning in combination with particle tracking velocimetry ( PTV ) . We see that for enough large vibration amplitudes , particles are expelled from the surface into the air as they reach their maximum size during an upward moving .This leads to the formation of a dilute gas phase above the dense packing near the bottom of the container . In addition we study convection rolls which carry grains across the entire depth of the bed .These data demonstrate remarkable parallels to those observed in vibrated beds of dust or glass bottles but differ significantly from previous research performed on structures where only horizontal vibrations were applied . Vibrations can induce changes between various states of matter such as solids , liquids , and gases 1 .For instance , it has been shown that a solid state would change unstable when exposed to periodic forcing 2 , resulting in the spontaneous production of traveling signals 3 . A notably unusual instance comes if both horizontal and horizontal elements of the driving pull act simultaneously 4 .In this research we study the response of a thin granular layer to continuous application of longitudinal and horizontal vibrations . Our experiments discover new experiments not seen before in other types of driven granular material .",
        "rewrite_text": "We present a comprehensive investigation into the fluidization of a thin granular layer subjected to horizontal oscillations, focusing on the effects of low-frequency and low-amplitude vertical vibrations. This study employs advanced experimental techniques, including high-speed video scanning and particle tracking velocimetry (PTV), to analyze the behavior of the granular bed. Our findings reveal that when the amplitude of vertical vibrations reaches a sufficient threshold, particles are ejected from the surface into the air during upward oscillations, resulting in the formation of a dilute gas phase above the densely packed region at the bottom of the container. Additionally, we observe the emergence of convection rolls that facilitate the movement of grains throughout the entire depth of the granular bed. \n\nThese observations draw intriguing parallels with phenomena seen in vibrated beds of dust or glass beads; however, they diverge significantly from earlier studies that focused solely on systems subjected to horizontal vibrations. The interplay of vibrations can induce transitions among different states of matter—solid, liquid, and gas. Previous research has indicated that a solid state may become unstable under periodic forcing, leading to the spontaneous generation of traveling signals. An especially noteworthy scenario arises when both horizontal and vertical components of the driving force are applied simultaneously. \n\nIn this article, we delve into the response of a thin granular layer to the continuous application of both longitudinal and horizontal vibrations, uncovering novel behaviors that have not been documented in other types of driven granular materials. Our results contribute to a deeper understanding of the dynamics of granular systems and the complex interactions that govern their behavior under various vibrational conditions.",
        "ori-fast-z-score": 0.8980265101338746,
        "water-fast-z-score": 8.318172876299217,
        "rewrite-fast-z-score": 2.799023467943904
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Galaxy evolution in the infra-red: comparison of a hierarchical galaxy formation model with SPITZER data .\nAbstract:\nWe present results on the infrared (IR) properties of galaxies obtained by comparing predictions made using a semi-analytic galaxy formation model to observations taken as part of the Spitzer Infrared Nearby Galaxies Survey (SINGS). We find that our model reproduces many observed trends, including the IR luminosity function and the correlation between star formation rate and dust temperature. However, we also identify some discrepancies which suggest that further work is needed before this type of modelling can be used for detailed studies of individual objects or small samples. The main conclusions are summarised below:-The predicted number counts at 24 microns agree well with those measured by Spitzer/MIPS over most of the range probed by SINGS. -Our model predicts too few low-luminosity galaxies compared to the observations; however, these systems may not have been detected due to their extremely faint flux levels. -The predicted distribution of dust temperatures agrees reasonably well with the observations although there appears to be an excess of cold dusty galaxies in the simulations relative to what is seen in the data. -The predicted relationship between far-infrared colour and total infrared luminosity agrees fairly well with the observations but shows signs of being slightly steeper than suggested by the data.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Galaxy growth in the infra - red : comparison of a hierarchical galaxy formation theory with SPITZER statistics . Abstract : We report findings on the infrared ( IR ) characteristics of galaxies produced by comparing predictions making using a semi - analytic galaxy formation theory to observations made as part of the Spitzer Infrared Nearby Galaxies Survey ( SINGS ) .We see that our model reproduces many observed trends , notably the IR luminosity function and the relationship between star formation rate and dust temperature . However , we also identify some discrepancies which show that further work is required before this form of simulation can be used for detailed analyses of individual objects or small samples .The main results are summarised below : - The predicted total counts at 24 microns agree well with those observed by Spitzer / MIPS over most of the range probed by SINGS . - Our model predicts too few poor - luminosity galaxies compared to the surveys ; however , these systems may not have been detected due to their extremely dim flux levels .- The predicted distribution of dust altitudes agrees reasonably well with the experiments although there seems to be an accumulation of cold dusty galaxies in the simulations compared to what is seen in the information . - The predicted relationship between far - infrared colour and total infrared luminosity agrees fairly good with the experiments but appears signs of being somewhat steeper than proposed by the information .",
        "rewrite_text": "In this study, we present an analysis of the infrared (IR) properties of galaxies by comparing the predictions derived from a semi-analytic galaxy formation model with observational data from the Spitzer Infrared Nearby Galaxies Survey (SINGS). Our findings indicate that the model successfully captures several key observed trends, particularly the IR luminosity function and the correlation between star formation rates and dust temperatures. Nonetheless, we also uncover notable discrepancies that suggest the need for further refinement of the simulation approach before it can be reliably applied to detailed investigations of individual galaxies or small samples.\n\nKey results from our analysis include the following: Firstly, the predicted total counts of galaxies at 24 microns align closely with the observations obtained from the Spitzer/MIPS, covering most of the range explored by SINGS. Secondly, our model underestimates the number of low-luminosity galaxies compared to the survey data; however, this discrepancy may be attributed to the potential non-detection of these faint objects due to their extremely low flux levels. Thirdly, while the predicted distribution of dust altitudes shows reasonable agreement with observational data, there is an observed overrepresentation of cold dusty galaxies in our simulations relative to the SINGS findings. Lastly, the relationship between far-infrared color and total infrared luminosity predicted by our model is generally consistent with the observational data, although it appears to exhibit a steeper slope than what is suggested by the SINGS results.\n\nThese findings highlight both the strengths and limitations of the semi-analytic approach in modeling galaxy growth in the infrared regime, emphasizing the necessity for ongoing improvements to enhance the accuracy of simulations in astrophysical research.",
        "ori-fast-z-score": 0.7492686492653552,
        "water-fast-z-score": 7.118052168020874,
        "rewrite-fast-z-score": 0.7986208584745025
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Adhesion forces due to nano-triboelectrification between similar materials .\nAbstract:\nWe report on the adhesion force between two surfaces made out of the same material, which is caused by triboelectric charging and electrostatic attraction. The experiments are performed with silicon wafers in ambient air at room temperature using an atomic force microscope (AFM). We find that the measured adhesion force increases linearly with increasing normal load applied to the AFM tip. This behavior can be explained by considering the contact area as well as the charge density distribution across this area. In addition we show how the adhesion force depends on the relative humidity. Finally, we discuss possible applications for our findings. Tribology deals with friction, wear and lubrication phenomena occurring when solid bodies slide against each other or deform elastically under pressure  1  . It has been known since the 19th century that sliding friction causes electric charges  2  , but only recently it was realized that these charges play an important role in tribological processes  3  .\nIn particular, it has been shown that tribocharging leads to strong adhesive interactions between solids  4  . These effects have been observed experimentally  5, 6  and theoretically  7, 8  . However, most studies so far focused on dissimilar materials such as metals and insulators  9  . Here we present experimental results showing that even identical materials exhibit significant adhesion if they are brought into contact while being charged electrically  10  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Adhesion forces owing to nano - triboelectrification between identical structures . Abstract : We report on the adhesion force between two structures making out of the same material , which is caused by triboelectric locking and electrostatic attraction .The studies are performed with silicon wafers in ambient air at room temperature using an atomic force microscope ( AFM ) . We see that the measured adhesion force increases linearly with increasing regular stress introduced to the AFM tip .This phenomenon can be described by analyzing the contact area as well as the charge density flow across this area . In addition we explain how the adhesion force depends on the relative humidity .Finally , we explain possible applications for our findings . Tribology concerns with friction , wear and lubrication phenomena occurring when hard body dip against each other or deform elastically under pressure 1 .It has been known since the 19th century that moving friction produces electric charges 2 , but only lately it was understood that these charges serve an important role in tribological reactions 3 . In particular , it has been shown that tribocharging leads to powerful adhesive interactions between solids 4 .These effects have been observed experimentally 5 , 6 and theoretically 7 , 8 . However , most studies so far concentrate on dissimilar materials such as metals and insulators 9 .Here we present experimental results confirming that even matched substances exhibit substantial adhesion if they are brought into contact while being charged electrically 10 .",
        "rewrite_text": "**Title:** Adhesion Forces Due to Nano-Triboelectrification Between Identical Structures\n\n**Abstract:** This study investigates the adhesion forces that arise between two identical structures made from the same material, specifically focusing on the mechanisms of triboelectric locking and electrostatic attraction. Experiments were conducted using silicon wafers in ambient air at room temperature, employing an atomic force microscope (AFM) to measure the adhesion forces. Our findings reveal a linear increase in the measured adhesion force corresponding to the regular stress applied to the AFM tip. This behavior can be elucidated through an analysis of the contact area and the flow of charge density across this interface. Furthermore, we explore the influence of relative humidity on the adhesion force, highlighting its significance in the tribological context. \n\nTribology, the study of friction, wear, and lubrication, has long recognized the generation of electric charges during frictional contact, a phenomenon noted since the 19th century. Recent advancements have illuminated the critical role these charges play in tribological interactions, particularly in enhancing adhesive forces between solid materials. While previous research has predominantly focused on interactions between dissimilar materials, such as metals and insulators, our work presents experimental evidence that even identical materials can exhibit significant adhesion when electrically charged. This finding opens new avenues for understanding and harnessing triboelectric effects in various applications. The implications of this research extend to fields such as materials science, nanotechnology, and engineering, where controlling adhesion at the nanoscale can lead to innovative solutions in device fabrication and surface engineering.",
        "ori-fast-z-score": -0.09667364890456635,
        "water-fast-z-score": 6.225870853937484,
        "rewrite-fast-z-score": 1.3987572123604708
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Geometry and Topology in Relativistic Cosmology .\nAbstract:\nThe present work is devoted to the study of some aspects of geometry and topology in relativistic cosmology, with particular emphasis on the role played by the Weyl tensor. In Section 2 we review the basic concepts of Riemannian geometry that are relevant for our purposes; in particular, we introduce the concept of conformal vector field as well as its associated energy-momentum tensor. We also discuss how these fields can be used to construct exact solutions of Einstein s equations describing homogeneous but anisotropic universes. In Section 3 we consider the problem of classifying all possible types of singularities which may occur at finite time in spatially closed Friedmann-Robertson-Walker (FRW) models. This classification turns out to depend crucially upon whether or not the universe contains any matter sources other than radiation. Finally, in Section 4 we show that the presence of a non-vanishing Weyl tensor leads to an additional type of singularity which cannot arise if one assumes that the universe consists only of ordinary matter.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Geometry and Topology in Relativistic Cosmology . Abstract : The present work is committed to the study of some elements of geometry and topology in relativistic cosmology , with particular stress on the part played by the Weyl tensor .In Section 2 we review the fundamental concepts of Riemannian topology that are important for our purposes ; in particular , we introduce the notion of conformal vector field as well as its associated energy - momentum tensor . We also discuss how these fields can be used to build exact solutions of Einstein s equations representing homogeneous but anisotropic universes .In Section 3 we investigate the question of classifying all possible kinds of singularities which may happen at finite period in spatially closed Friedmann - Robertson - Walker ( FRW ) models . This classification turns out to depend crucially upon whether or not the universe holds any matter sources other than radiation .Finally , in Section 4 we prove that the presence of a non - vanishing Weyl vector results to an additional type of singularity which cannot arise if one assumes that the universe consists only of normal matter .",
        "rewrite_text": "This article delves into the intricate relationship between geometry and topology within the framework of relativistic cosmology, with a specific focus on the role of the Weyl tensor. In the second section, we provide a comprehensive overview of essential concepts in Riemannian topology that are pertinent to our investigation. We introduce the concept of conformal vector fields and their corresponding energy-momentum tensors, highlighting their utility in deriving exact solutions to Einstein's equations that describe homogeneous yet anisotropic universes. \n\nThe third section addresses the classification of various singularities that may emerge at finite times in spatially closed Friedmann-Robertson-Walker (FRW) models. Our analysis reveals that this classification is significantly influenced by the presence of matter sources beyond mere radiation. \n\nIn the concluding section, we demonstrate that the existence of a non-vanishing Weyl vector introduces a distinct type of singularity that cannot occur under the assumption that the universe is composed solely of normal matter. This finding underscores the importance of the Weyl tensor in understanding the geometric and topological aspects of cosmological models, particularly in relation to the behavior of singularities. Through this work, we aim to enhance the understanding of the interplay between geometry, topology, and the fundamental nature of the universe in the context of relativistic cosmology.",
        "ori-fast-z-score": -2.0647416048350555,
        "water-fast-z-score": 3.1304951684997055,
        "rewrite-fast-z-score": -0.5241424183609592
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Suzaku Observation of the Nucleus of the Radio-Loud Active Galaxy Centaurus A: Constraints on Abundances of the Accreting Material .\nAbstract:\nWe report results of X-ray observations with Suzaku satellite of the nucleus of radio-loud active galaxy Centaurus A (NGC 5128). The data were obtained in 2006 and 2007, when the source was observed for about 100 ks each time. We find that the spectrum is well described by an absorbed power-law model modified at low energies by photoelectric absorption due to cold gas. No significant emission lines are detected above 3 keV. Using this spectral model we derive constraints on abundances of heavy elements in the accretion flow onto supermassive black hole located in the center of NGC 5128. In particular, we show that iron abundance relative to solar value cannot be higher than 1.5-2 times its solar value. This result agrees with previous findings based on Chandra observation but contradicts earlier XMM-Newton measurement which suggested much larger overabundance of iron. Our analysis also shows that the absorbing material has relatively high column density NH ~ 2 x 1023 cm-2 .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Suzaku Observation of the Nucleus of the Radio - Loud Active Galaxy Centaurus A : Constraints on Abundances of the Accreting Material . Abstract : We report findings of X - ray observations with Suzaku moon of the nucleus of radio - loud active galaxy Centaurus A ( NGC 5128 ) .The data were obtained in 2006 and 2007 , when the source was seen for about 100 ks each time . We see that the spectrum is well described by an absorption power - law theory improved at low energies by photoelectric diffusion owing to cold gas .No noticeable emitted lines are detected above 3 keV . Using this spectral method we derive restrictions on abundances of heavy metals in the accretion flow onto supermassive black hole located in the center of NGC 5128 .In particular , we find that iron density relative to solar value cannot be higher than 1 . 5 - 2 twice its solar value . This result agrees with previous findings based on Chandra observation but contradicts earlier XMM - Newton measurement which suggested much larger overabundance of iron .Our study also shows that the absorbing material has relatively high column density NH ~ 2 x 1023 cm - 2 .",
        "rewrite_text": "We present the results of X-ray observations conducted with the Suzaku satellite, focusing on the nucleus of the radio-loud active galaxy Centaurus A (NGC 5128). The data collection occurred in 2006 and 2007, with each observation lasting approximately 100 kiloseconds. Our analysis reveals that the observed spectrum is effectively modeled by an absorption power-law, which is further refined at lower energies by photoelectric absorption due to cold gas. Notably, we did not detect any significant emission lines above 3 keV. \n\nEmploying this spectral analysis, we derive important constraints on the abundances of heavy metals present in the accreting material surrounding the supermassive black hole at the center of NGC 5128. Specifically, our findings indicate that the iron abundance, when compared to solar values, cannot exceed 1.5 to 2 times the solar level. This conclusion is consistent with earlier results obtained from Chandra observations but stands in contrast to previous measurements from XMM-Newton, which suggested a significantly higher overabundance of iron. \n\nAdditionally, our study indicates that the absorbing material exhibits a relatively high column density, estimated at NH ~ 2 x 10^23 cm^-2. These findings contribute to the understanding of the chemical composition of the accreting material in active galactic nuclei and highlight the importance of utilizing high-resolution X-ray spectroscopy to probe the environments surrounding supermassive black holes. Overall, our results provide valuable insights into the physical conditions and elemental abundances in the vicinity of the black hole in Centaurus A, enhancing our comprehension of the processes governing accretion in such extreme astrophysical settings.",
        "ori-fast-z-score": 1.4924050144892729,
        "water-fast-z-score": 5.193989612031165,
        "rewrite-fast-z-score": -0.5345224838248488
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Luminous Red Galaxy Clustering at z~0.7 - First Results using AAOmega .\nAbstract:\nWe present the first results on clustering measurements for luminous red galaxies (LRGs) in the redshift range 0.5 <z<0.8, obtained with the Anglo-Australian Observatory s multi-object spectrograph AAOmega. We use data from the 2dF-SDSS LRG and QSO survey to measure the projected correlation function wp(rp). The observed clustering amplitude is consistent with that expected from linear theory predictions based on current cosmological models. This result provides an important test of these models over this redshift range where there are few other constraints available. In addition we find evidence for evolution in the galaxy bias parameter between our two samples separated by ~0.2 Gyrs. These results will be presented in detail elsewhere. \n \n Keywords: Luminous Red Galaxies; Clustering; Bias Evolution; Cosmology. 1 Introduction \n \n A number of recent studies have shown that luminous red galaxies (hereafter LRGs), selected via their optical colours or near-infrared photometry, provide powerful probes of large-scale structure out to high redshifts (e.g., Eisenstein et al. 2001; Wake et al. 2006; Padmanabhan et al. 2007; Blake et al. 2008; Ross et al. 2008) . Their large luminosities mean they can be detected efficiently even at relatively low redshifts, while their red colours make them easy to identify spectroscopically. They also tend to reside in massive dark matter haloes which evolve slowly through cosmic time, making them useful tracers of the underlying mass distribution. As such, they offer unique opportunities to study both the growth of structures as well as the nature of dark energy driving its accelerated expansion (see e.g., Percival & White 2009 , for a review). \n \n Here we report the first measurement of the spatial clustering properties of LRGs in the redshift range 0<z<0.8 made possible by combining data from the Sloan Digital Sky Survey (SDSS) (York et al. 2000) , the Two Degree Field Galaxy Redshift Survey (2dFGRS) (Colless et al.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Luminous Red Galaxy Clustering at z ~ 0 . 7 - First Results using AAOmega . Abstract : We report the first findings on clustering observations for luminous red clusters ( LRGs ) in the redshift range 0 . 5 < z < 0 . 8 , obtained with the Anglo - Australian Observatory s multi - object spectrograph AAOmega .We use data from the 2dF - SDSS LRG and QSO poll to measure the projected correlation function wp ( rp ) . The observed clustering amplitude is compatible with that expected from linear theory estimates based on current cosmological models .This result provides an important test of these models over this redshift range where there are few other constraints provided . In addition we find proof for evolution in the galaxy bias variable between our two specimens divided by ~ 0 . 2 Gyrs .These conclusions will be described in detail elsewhere . Keywords : Luminous Red Galaxies ; Clustering ; Bias Evolution ; Cosmology .1 Introduction A variety of recent studies have shown that luminous red objects ( hereafter LRGs ) , selected via their optical colours or near - infrared photometry , provide potent probes of large - scale organization out to large redshifts ( e . g . , Eisenstein et al . 2001 ; Wake et al .2006 ; Padmanabhan et al . 2007 ; Blake et al .2008 ; Ross et al . 2008 ) .Their large luminosities guarantee they can be identified efficiently even at fairly little redshifts , while their red colours making them easy to identify spectroscopically . They especially prefer to live in massive dark matter haloes which evolution gradually through cosmic time , making them useful tracers of the underlying mass distribution .As such , they give unique possibilities to study both the development of structures as also as the nature of deep energy causing its rapid increase ( saw e . g . , Percival & White 2009 , for a review ) . Here we publish the first measurement of the spatial clustering behavior of LRGs in the redshift region 0 < z < 0 . 8 made possible by combining information from the Sloan Digital Sky Survey ( SDSS ) ( York et al .2000 ) , the Two Degree Field Galaxy Redshift Survey ( 2dFGRS ) ( Colless et al .",
        "rewrite_text": "**Title:** Clustering of Luminous Red Galaxies at z ~ 0.7: Initial Findings from AAOmega Observations\n\n**Abstract:** This study presents the inaugural results of clustering observations for luminous red galaxies (LRGs) within the redshift range of 0.5 < z < 0.8, utilizing the multi-object spectrograph AAOmega at the Anglo-Australian Observatory. We analyze data sourced from the 2dF-SDSS LRG and QSO survey to compute the projected correlation function, wp(rp). Our findings indicate that the observed clustering amplitude aligns well with predictions derived from linear theory estimates based on contemporary cosmological models. This alignment serves as a critical validation of these models in a redshift range that has been sparsely constrained by previous research. Furthermore, we observe evidence of evolution in the galaxy bias variable across our two samples, separated by approximately 0.2 Gyr. These significant results will be elaborated upon in subsequent publications. \n\n**Keywords:** Luminous Red Galaxies; Clustering; Bias Evolution; Cosmology.\n\n**1. Introduction:** Recent investigations have demonstrated that luminous red galaxies (LRGs), identified through their optical colors or near-infrared photometry, are powerful indicators of large-scale structure extending to considerable redshifts (e.g., Eisenstein et al. 2001; Wake et al. 2006; Padmanabhan et al. 2007; Blake et al. 2008; Ross et al. 2008). Their substantial luminosities facilitate efficient identification even at relatively low redshifts, while their distinctive red colors allow for straightforward spectroscopic classification. LRGs predominantly inhabit massive dark matter halos, which evolve gradually over cosmic time, rendering them valuable tracers of the underlying mass distribution. Consequently, they offer unique opportunities to investigate both the evolution of cosmic structures and the nature of dark energy driving the accelerated expansion of the universe (see Percival & White 2009 for a comprehensive review). In this paper, we present the first measurement of the spatial clustering characteristics of LRGs in the redshift range of 0 < z < 0.8, made possible through the integration of data from the Sloan Digital Sky Survey (SDSS) (York et al. 2000) and the Two Degree Field Galaxy Redshift Survey (2dFGRS) (Colless et al.).",
        "ori-fast-z-score": -1.1832159566199232,
        "water-fast-z-score": 5.874269508076824,
        "rewrite-fast-z-score": -0.5107539184552492
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Using Image Attributes for Human Identification Protocols .\nAbstract:\nIn this work, we propose an approach to human identification based on the analysis of image attributes and their relationships with each other. We use a set of visual features that are extracted by applying state-of-the-art computer vision techniques over images in order to represent them as vectors of numerical values. These feature vectors can be used to train machine learning algorithms such as Support Vector Machines (SVMs) or Random Forests (RF). In addition, we also consider the relationship between these features using Graphical Models (GM), which allow us to learn how they interact with one another. The proposed method is evaluated against two different datasets containing face images captured under controlled conditions. Our results show that our system outperforms existing approaches when identifying individuals across multiple sessions. This research was supported by the National Science Foundation through awards IIS-1253153 and CNS-1527225. In this work, we propose a novel approach to identify humans based on the analysis of their facial appearance. To do so, we extract several visual features from faces using state-of-the-art computer vision methods. Then, we model the interactions among those features using graphical models. Finally, we evaluate the performance of our method against two publicly available databases.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Using Image Attributes for Human Identification Protocols . Abstract : In this project , we propose an way to human identification focusing on the evaluation of visual attributes and their connections with each other .We use a setting of visual elements that are derived by using state - of - the - art computer vision principles over images in order to depict them as matrices of numerical values . These feature vectors can be used to train machine computing methods such as Support Vector Machines ( SVMs ) or Random Forests ( RF ) .In addition , we also consider the relationship between these characteristics utilizing Graphical Models ( GM ) , which allow us to study how they interact with one another . The proposed approach is evaluated against two different datasets featuring face photos taken under controlled circumstances .Our results show that our system outperforms existing techniques when identifying persons across multiple meetings . This research was supported by the National Science Foundation through awards IIS - 1253153 and CNS - 1527225 .In this project , we attempt a new approach to identify humans relying on the examination of their facial shape . To do so , we extract many graphical features from faces utilizing state - of - the - art computer vision methods .Then , we study the interactions among those characteristics using visual models . Finally , we assess the performance of our technique against two publicly accessible data .",
        "rewrite_text": "**Title:** Utilizing Image Attributes for Human Identification Protocols\n\n**Abstract:** This study presents a novel approach to human identification that emphasizes the analysis of visual attributes and their interrelationships. By employing advanced computer vision techniques, we extract visual elements from images and represent them as matrices of numerical values, known as feature vectors. These vectors serve as the foundation for training machine learning algorithms, including Support Vector Machines (SVMs) and Random Forests (RF). Furthermore, we explore the connections between these visual attributes through the application of Graphical Models (GM), which facilitate an understanding of how these features interact with one another.\n\nOur methodology is rigorously evaluated using two distinct datasets comprising facial images captured under controlled conditions. The results demonstrate that our proposed system significantly outperforms existing identification techniques, particularly in scenarios involving multiple encounters with the same individual. This advancement in human identification is particularly relevant in fields such as security and surveillance, where accurate recognition is crucial.\n\nThe research was funded by the National Science Foundation under awards IIS-1253153 and CNS-1527225. In summary, this project introduces an innovative framework for human identification that leverages the analysis of facial shape and the intricate relationships among various graphical features. By integrating state-of-the-art computer vision methods with sophisticated modeling techniques, we provide a comprehensive evaluation of our approach against publicly available datasets, showcasing its effectiveness and potential for real-world applications.",
        "ori-fast-z-score": -1.7320508075688772,
        "water-fast-z-score": 6.417436110981714,
        "rewrite-fast-z-score": -0.641688947919748
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Araucaria Project. The Distance to the Local Group Galaxy WLM from Cepheid Variables discovered in a Wide-Field Imaging Survey .\nAbstract:\nWe present results on distance determinations for two galaxies, NGC 3109 and WLM, based on observations made with the Hubble Space Telescope (HST). We have used HST/WFPC2 images taken through filters F555W and F814W to search for Cepheids among young open clusters located within these galaxies. Our survey has resulted in the discovery of four new Cepheids in NGC 3109 and one in WLM. These five Cepheids are all short-period classical pulsators with periods ranging between 4.5 days and 8.6 days. Using the period-luminosity relation derived by Madore & Freedman we find distances to NGC 3109 and W LM that agree well with previous estimates obtained using other methods. \n \n Keywords: Cepheid variables; open cluster; galaxy distance scale; Hubble Space Telescope; Araucaria Project. 1. Introduction \n \n Open clusters provide an important tool for determining extragalactic distances because they contain many stars at nearly identical ages and chemical compositions. In addition, open clusters can be found over a wide range of galactocentric radii, allowing us to probe different environments. However, open clusters are relatively rare objects compared to field stars or globular clusters. Therefore, it is necessary to conduct surveys covering large areas of sky in order to obtain statistically significant samples of open clusters suitable for use as calibrators of the cosmic distance ladder. \n \n The Araucaria Project was initiated in 1998 with the goal of obtaining accurate distances to nearby galaxies via measurements of Cepheid variable stars associated with open clusters. This project uses data collected primarily with the Hubble Space Telescope s WFPC2 camera. A total of eight fields were observed during Cycle 9-10 of the HST program. Each field covers about 0.25 square degrees centered around a target galaxy. For each field, deep exposures were obtained in both the F555W and F850LP bands. Details regarding this project may be found in Pietrzyński et al. (2002) and references therein. \n \n 2. Searching for Cepheids Among Young Open",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Araucaria Project . The Distance to the Local Group Galaxy WLM from Cepheid Variables revealed in a Wide - Field Imaging Survey .Abstract : We report findings on distance determinations for two galaxies , NGC 3109 and WLM , built on observations made with the Hubble Space Telescope ( HST ) . We have utilized HST / WFPC2 pictures taken through filters F555W and F814W to search for Cepheids among young open complexes housed within these objects .Our survey has resulted in the discovery of four new Cepheids in NGC 3109 and one in WLM . These five Cepheids are all short - process classical pulsators with periods ranging between 4 . 5 weeks and 8 . 6 hours .Using the period - luminosity relation derived by Madore & Freedman we find distances to NGC 3109 and W LM that agree well with previous estimates obtained using other methods . Keywords : Cepheid variables ; open cluster ; galaxy distance scale ; Hubble Space Telescope ; Araucaria Project .1 . Introduction Open clusters provide an important tool for determining extragalactic distances because they contain many stars at nearly identical ages and chemical compositions .In addition , open complexes can be found over a broad variety of galactocentric radii , allowing us to probe different environments . However , open complexes are fairly scarce objects compared to field stars or globular galaxies .Therefore , it is required to conduct surveys covering large areas of skies in order to obtain statistically substantial samples of open complexes suitable for use as calibrators of the cosmic diameter staircase . The Araucaria Project was initiated in 1998 with the objective of acquiring precise lengths to nearby galaxies via measurements of Cepheid variable stars associated with open complexes .This project utilizes information collected principally with the Hubble Space Telescope s WFPC2 telescope . A total of eight fields were detected during Cycle 9 - 10 of the HST project .Each field covers about 0 . 25 square degrees centered around a target galaxy . For each field , deep exposures were obtained in both the F555W and F850LP bands .Details regarding this project would be found in Pietrzyński et al . ( 2002 ) and links therein .2.Searching for Cepheids Among Young Open",
        "rewrite_text": "**Title:** The Araucaria Project: Distance Measurement to the Local Group Galaxy WLM through Cepheid Variables in a Wide-Field Imaging Survey\n\n**Abstract:** This study presents the results of distance measurements for two galaxies, NGC 3109 and WLM, based on observations conducted with the Hubble Space Telescope (HST). Utilizing images captured by the HST/WFPC2 through the F555W and F814W filters, we aimed to identify Cepheid variables within the young open clusters located in these galaxies. Our extensive survey led to the identification of four new Cepheids in NGC 3109 and one in WLM. All five discovered Cepheids are classified as short-period classical pulsators, exhibiting periods that range from 4.5 weeks to 8.6 hours. By applying the period-luminosity relation established by Madore & Freedman, we calculated distances to NGC 3109 and WLM that are consistent with previous measurements obtained through alternative methodologies. \n\nThe significance of open clusters in determining extragalactic distances lies in their composition of stars that share similar ages and chemical properties. Furthermore, these clusters are distributed across various galactocentric radii, enabling the exploration of different galactic environments. However, due to their relative scarcity compared to field stars and globular clusters, comprehensive surveys covering extensive sky areas are essential to gather a statistically significant sample of open clusters that can serve as reliable calibrators for the cosmic distance ladder. The Araucaria Project, initiated in 1998, aims to derive accurate distances to nearby galaxies by measuring Cepheid variable stars associated with open clusters. This project primarily utilizes data collected from the HST's WFPC2 camera. During HST Cycles 9 and 10, eight fields were observed, each encompassing approximately 0.25 square degrees centered on target galaxies. Deep exposures were obtained in both the F555W and F850LP bands, with further details available in Pietrzyński et al. (2002) and related references. This research contributes to the ongoing efforts to refine the cosmic distance scale through precise measurements of Cepheid variables in nearby galaxies. \n\n**Keywords:** Cepheid variables; open clusters; galaxy distance scale; Hubble Space Telescope; Araucaria Project.",
        "ori-fast-z-score": 0.5773502691896257,
        "water-fast-z-score": 7.233555441435721,
        "rewrite-fast-z-score": 0.7423074889580903
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Solidity of viscous liquids. V. Long-wavelength dominance of the dynamics .\nAbstract:\nWe study the solidification process in a model system consisting of particles interacting via repulsive soft-core potentials and subject to an external driving force. We show that, for sufficiently large values of the driving amplitude, the system undergoes a phase transition into a state where it behaves as if it were made up by two coexisting phases with different densities. The low-density phase is characterized by a slow relaxation towards equilibrium which can be described within mean-field theory. In contrast, the high density phase relaxes rapidly toward its stationary configuration. \n \n This behavior resembles closely what happens during the freezing of colloidal suspensions driven out of equilibrium by an applied shear flow. Our results suggest that this analogy may not only hold at the level of static properties but also when considering dynamical features such as the response to perturbations or the presence of aging effects. Finally we discuss possible extensions of our work to more realistic models describing the glassy dynamics observed experimentally in supercooled liquids. \nI. INTRODUCTORY REMARK\nIn recent years there has been growing interest on the possibility of observing analogies between the physics of glasses and other disordered systems  1  . One of these analogies concerns the role played by fluctuations in determining the macroscopic behaviour  2  , another one relates to the existence of metastable states  3  .\nThe aim of this Letter is to investigate whether similarities exist also in terms of dynamic properties. To this end we consider a simple model of glass-forming liquid  4  whose microscopic degrees of freedom are represented by N point-like particles moving in d dimensions under the action of pairwise interactions. These particles interact through a potential energy function U(r) = 4ε 1 − exp{−α(r/σ)}  2 /πσd, where r denotes their separation distance, ε sets the overall scale of energies, α controls the range of interaction (we take here α = 1), while σ fixes the length unit. For simplicity we assume periodic boundary conditions so that the total number of particles remains constant throughout the simulation. As usual, we define the reduced temperature T * ≡ kT/",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  Solidity of viscous liquids.V. Long-wavelength dominance of the dynamics .Abstract : We explore the solidification mechanism in a model structure formed of molecules evolving via repulsive soft - core potentials and subject to an external driving field . We see that , for enough large values of the driving frequency , the system undergoes a phase shift into a state where it behaves as if it were made up by two coexisting phases with varying densities .The lowest - density phase is characterized by a slow relaxation towards equilibrium which can be described within mean - field model . In comparison , the high density phase relaxes rapidly toward its stationary configuration .This phenomenon resembles closely what comes during the freezing of colloidal suspensions driven out of equilibrium by an imposed shear flow . Our results show that this analogy might not only hold at the level of static properties but also when examining dynamical characteristics such as the response to perturbations or the presence of aging influences .Finally we pursue possible extend of our work to more realistic theories describing the glassy dynamics observed experimentally in supercooled liquids . I .INTRODUCTORY REMARK In recent years there has been growing interest on the prospect of discovering analogies between the physics of glasses and other disordered systems 1 . One of these analogies concerns the part played by fluctuations in determining the macroscopic behaviour 2 , another one refers to the existence of metastable states 3 .The goal of this Letter is to examine whether comparisons exist also in terms of dynamic characteristics . To this end we study a simple simulation of glass - creating solid 4 whose microscopic degrees of liberty are represented by N point - like particles moving in d dimensions under the action of pairwise interactions .These particles react through a potential energy relation U ( r ) = 4ε 1 − exp { −α ( r / π ) } 2 / πσd , where r denotes their separation distance , ε sets the overall scale of energies , α handles the range of interaction ( we took here α = 1 ) , while ρ fixes the length unit . For simplicity we suppose periodic border conditions so that the total number of particles stay constant throughout the model .As usual, we define the reduced temperature T * ≡ kT/",
        "rewrite_text": "**Title:** Solidity of Viscous Liquids: Long-Wavelength Dominance of the Dynamics\n\n**Abstract:** In this study, we investigate the solidification process of a model system composed of molecules that interact through repulsive soft-core potentials and are influenced by an external driving field. Our findings reveal that when the driving frequency reaches sufficiently high values, the system transitions into a state that exhibits characteristics of two coexisting phases with distinct densities. The lower-density phase is marked by a slow relaxation towards equilibrium, which can be effectively described using a mean-field model. In contrast, the higher-density phase demonstrates a rapid relaxation to its stationary state. This behavior closely resembles the dynamics observed during the freezing of colloidal suspensions that are driven out of equilibrium by an applied shear flow. Our results suggest that this analogy extends beyond static properties to encompass dynamic characteristics, including the system's response to perturbations and the effects of aging. Furthermore, we explore the potential implications of our findings for more realistic theoretical frameworks that describe the glassy dynamics seen in supercooled liquids. \n\nIn recent years, there has been an increasing interest in uncovering parallels between the physics of glasses and other disordered systems. One such analogy pertains to the role of fluctuations in shaping macroscopic behavior, while another relates to the presence of metastable states. The objective of this paper is to determine whether similar comparisons can be made regarding dynamic properties. To achieve this, we conduct simulations of a glass-forming solid, represented by N point-like particles moving in d dimensions under pairwise interactions. The interactions are governed by a potential energy function, U(r) = 4ε(1 - exp{-α(r/π)})²/πσd, where r is the distance between particles, ε establishes the energy scale, α determines the interaction range (with α set to 1 in our study), and ρ defines the length unit. We employ periodic boundary conditions to maintain a constant total number of particles throughout the simulation. As is customary, we define the reduced temperature as T* ≡ kT/ε, setting the stage for a comprehensive analysis of the system's behavior under varying conditions.",
        "ori-fast-z-score": -1.9230769230769231,
        "water-fast-z-score": 6.614487515046438,
        "rewrite-fast-z-score": 0.5384615384615384
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Chandra Observations of Supernova 1987A .\nAbstract:\nThe Chandra X-ray Observatory has observed the supernova remnant (SNR) produced by SN1987A in the Large Magellanic Cloud for over ten years, providing an unprecedented view into this young and energetic object.  The observations have revealed that the blast wave is interacting with dense circumstellar material surrounding the progenitor star at velocities up to 1000 km/sec.  This interaction produces bright knots of emission which are seen as moving outward through the shell of the remnant.  These knots appear to be composed primarily of oxygen-rich ejecta mixed with shocked interstellar gas.  In addition, there appears to be a large amount of hot plasma trapped behind the forward shock front.  We present here new results on these features based on our analysis of data obtained during the first year of the Chandra mission. The Chandra X-ray Observatory has observed  the supernova remnant ( SNR ) produced by SN1987A , in the Large Magellan ic Cloud , for over ten years . It provides an unprecedented view into this y oung and en erg i c obj ect .  T he obse rvations ha ve reve al ed tha t th e b las t wa ve is interactin g wi th d ens e circumstell ar m aterial surroundi ng th e proge nitor star-at-rou nd -velocities up to 1 000 k m/ sec . Thi s interac tion produ ces brigh t kn ots of emi ssion wh ich appea r to be co mp osed pri marily of oxyg ene -rico h ej ec ta mixe d w ith sh ocked int erstel lar ga s .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Chandra Observations of Supernova 1987A . Abstract : The Chandra X - ray Observatory has observed the supernova remnant ( SNR ) produced by SN1987A in the Large Magellanic Cloud for over ten years , providing an remarkable view into this young and dynamic body .The images have revealed that the explosion wave is interacting with heavy circumstellar material surrounding the progenitor star at velocities up to 1000 kilometers / sec . This coupling generates bright knots of emission which are seen as traveling outward through the shell of the remnant .These knots appear to be composed primarily of oxygen - rich ejecta combined with shocked interstellar gas . In addition , there seems to be a large number of hot plasma trapped behind the front shock front .We present here new data on these features based on our analysis of evidence derived during the first year of the Chandra mission . The Chandra X - ray Observatory has observed the supernova remnant ( SNR ) produced by SN1987A , in the Large Magellan ic Cloud , for over ten years .It provides an remarkable perspective into this y oung and en erg i c obj ect . T he obse rvations ha ve reve al ed tha t th e f las t wa ve is interactin b wi th d ens e circumstell ar m aterial surroundi ng th e proge nitor star - at - rou nd - velocities up to 1 000 k m / sec .Thi s interac tion produ ces brigh t kn ots of emi ssion wh ich appea r to be co mp osed pri marily of oxyg ene - rico h ej ec ta mixe d w ith sh ocked int erstel lar ga s .",
        "rewrite_text": "The Chandra X-ray Observatory has been observing the supernova remnant (SNR) of SN1987A, located in the Large Magellanic Cloud, for over a decade, offering an extraordinary insight into this young and dynamic astronomical entity. The data collected during this period has unveiled that the shock wave from the explosion is engaging with dense circumstellar material that surrounds the progenitor star, with velocities reaching up to 1000 kilometers per second. This interaction results in the formation of bright emission knots that are observed moving outward through the remnant's shell. These knots predominantly consist of oxygen-rich ejecta intermixed with shocked interstellar gas. Furthermore, our observations indicate the presence of a significant amount of hot plasma that is trapped behind the leading shock front. In this article, we present new findings based on our analysis of data collected during the initial year of the Chandra mission, which enhances our understanding of the complex processes occurring in the aftermath of this historic supernova event. The insights gained from these observations not only contribute to our knowledge of SN1987A but also provide valuable information regarding the interactions between supernova remnants and their surrounding environments.",
        "ori-fast-z-score": 0.7035264706814485,
        "water-fast-z-score": 5.0,
        "rewrite-fast-z-score": -0.21320071635561041
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Algebraic charge liquids .\nAbstract:\nWe introduce the concept of algebraic charge liquids, which are defined as ground states of Hamiltonians with local interactions that can be written in terms of fermionic creation and annihilation operators. We show how to construct such models for any finite group G by using an explicit representation of G on the Hilbert space of spinless fermions. The resulting model is exactly solvable when G has no non-trivial subgroups. In this case we find that there exists at least one phase transition between different phases characterized by distinct topological orders. For example, if G = Z2 × Z2 then our construction yields two gapped phases distinguished by their chiral central charges c− = 0 or 1. If G contains a nontrivial subgroup H then the system exhibits gapless excitations corresponding to particles transforming according to irreducible representations (irreps) of H. These results provide new insights into the classification problem of quantum many-body systems.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Algebraic charge liquids . Abstract : We introduce the idea of algebraic charge liquids , which are specified as ground states of Hamiltonians with local interactions that can be written in terms of fermionic creation and annihilation operators .We see how to build such theories for any finite class G by using an explicit representation of G on the Hilbert space of spinless fermions . The resulting theory is precisely solvable when G has no non - trivial subgroups .In this instance we find that there exists at least one phase shift between various phases characterized by separate topological orders . For instance , if G = Z2 × Z2 then our build produces two gapped phases distinguished by their chiral central charges c− = 0 or 1 .If G contains a nontrivial subgroup H then the scheme exhibits gapless excitations corresponding to particles transforming according to irreducible representations ( irreps ) of H . These data provide fresh insights into the classification question of quantum several - bodies systems .",
        "rewrite_text": "In this article, we present the concept of algebraic charge liquids, which are defined as the ground states of Hamiltonians characterized by local interactions expressible through fermionic creation and annihilation operators. We demonstrate a method for constructing such theories for any finite group \\( G \\) by utilizing an explicit representation of \\( G \\) on the Hilbert space of spinless fermions. Our findings reveal that the resulting theory is exactly solvable when \\( G \\) does not possess any non-trivial subgroups. In these cases, we observe the emergence of at least one phase shift between different phases, each distinguished by unique topological orders. For example, when \\( G \\) is the direct product \\( Z_2 \\times Z_2 \\), our construction yields two distinct gapped phases, which can be differentiated by their chiral central charges, specifically \\( c^- = 0 \\) or \\( 1 \\). Conversely, if \\( G \\) includes a non-trivial subgroup \\( H \\), the framework reveals the presence of gapless excitations that correspond to particles transforming according to the irreducible representations (irreps) of \\( H \\). These findings contribute valuable insights into the classification of quantum many-body systems, enhancing our understanding of their underlying structures and behaviors. The exploration of algebraic charge liquids thus opens new avenues for research in condensed matter physics, particularly in the study of topological phases and their associated excitations. This work not only enriches the theoretical landscape but also paves the way for potential experimental realizations of these exotic states of matter.",
        "ori-fast-z-score": -1.885618083164127,
        "water-fast-z-score": 4.330522446256832,
        "rewrite-fast-z-score": -1.2686700948330931
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Holography in Three-dimensional Kerr-de Sitter Space with a Gravitational Chern-Simons Term .\nAbstract:\nWe study the holographic entanglement entropy for three-dimensional de Sitter space with gravitational Chern-Simons term by using the replica trick and the covariant phase-space method. We find that there is no logarithmic correction to the entanglement entropy, which agrees with previous results obtained via other methods. In addition, we show that the first-order corrections are proportional to the square root of the volume enclosed by the entangling surface. Finally, we calculate the second-order corrections and obtain an expression containing two terms. One of them has been previously found in Ref.  Phys. Rev. D 98 (2018) 084011  while another one is new. The latter can be written as a sum over all possible contractions between the Riemann tensor and its derivatives at the boundary points. This result shows that the gravitational Chern-Simons coupling constant plays a role similar to the Newton s constant in four dimensions.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Holography in Three - dimensional Kerr - de Sitter Space with a Gravitational Chern - Simons Term . Abstract : We research the holographic entanglement entropy for three - dimensional de Sitter space with gravitational Chern - Simons term by using the replica trick and the covariant phase - space method .We see that there is no logarithmic correction to the entanglement entropy , which agrees with previous findings obtained via other methods . In addition , we prove that the first - order corrections are proportional to the square root of the volume enclosed by the entangling surface .Finally , we determine the second - order corrections and find an expression containing two terms . One of them has been previously found in Ref .Phys.Rev.D 98 ( 2018 ) 084011 while another one is new . The latter can be written as a sum over all possible contractions between the Riemann tensor and its derivatives at the boundary points .This result suggests that the gravitational Chern - Simons interaction function holds a role similar to the Newton s constant in four dimensions .",
        "rewrite_text": "In this study, we investigate the holographic entanglement entropy in three-dimensional de Sitter space that incorporates a gravitational Chern-Simons term. Utilizing the replica trick alongside the covariant phase-space method, we analyze the properties of entanglement entropy in this specific gravitational context. Our findings reveal that there are no logarithmic corrections to the entanglement entropy, which is consistent with earlier results obtained through various alternative approaches. Furthermore, we demonstrate that the first-order corrections to the entanglement entropy are directly proportional to the square root of the volume enclosed by the entangling surface, providing a deeper understanding of the underlying geometric relationships.\n\nAdditionally, we explore the second-order corrections and derive an expression that consists of two distinct terms. One of these terms aligns with previous results reported in Phys. Rev. D 98 (2018) 084011, while the other term is novel. This new term can be expressed as a summation over all possible contractions involving the Riemann tensor and its derivatives evaluated at the boundary points. This significant finding implies that the gravitational Chern-Simons interaction term plays a role analogous to that of Newton's constant in four-dimensional spacetime. Our results contribute to the broader understanding of holography in curved spacetimes and highlight the intricate connections between geometry and quantum entanglement in gravitational theories.",
        "ori-fast-z-score": 0.75,
        "water-fast-z-score": 3.4465617474213164,
        "rewrite-fast-z-score": 1.6865480854231356
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Optimal Occulter Design for Finding Extrasolar Planets .\nAbstract:\nWe present an optimal design method to find the best occulting mask that can be used in direct imaging searches for extrasolar planets.  The proposed method is based on the concept of entropy maximization, which has been widely applied in many fields such as information theory and statistical mechanics.   We show how this concept can be extended into the field of optics by introducing a new quantity called optical entropy (OE).   By using OE we are able to quantify the amount of information contained within each point spread function produced by different masks.   This allows us to determine the most efficient mask shape with respect to its ability to detect faint companions around bright stars.   ...  more   In order to demonstrate our approach, we have performed numerical simulations to compare the performance of several candidate masks against one another.   Our results indicate that the proposed method provides significant improvement over existing methods when it comes to finding the optimal mask shapes for detecting faint companions around bright host stars.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Optimal Occulter Design for Finding Extrasolar Planets . Abstract : We present an appropriate design algorithm to find the best occulting mask that can be used in direct imaging searches for extrasolar planets .The proposed approach is based on the idea of entropy maximization , which has been widely applied in different fields such as data physics and statistical mechanics . We see how this concept can be applied into the field of optics by using a new quantity called optical entropy ( OE ) .By using OE we are able to quantify the quantity of information stored within each point spread function generated by various masks . This enables us to identify the most efficient mask shape with regard to its able to identify dim companions around bright stars .. . . more In order to test our approach , we have done mathematical simulations to study the performance of several candidate masks against one another . Our results show that the suggested method provides significant progress over existing techniques when it comes to finding the ideal mask shapes for detecting bright companions around bright host stars .",
        "rewrite_text": "In this article, we introduce a novel design algorithm aimed at optimizing occulting masks for direct imaging searches of extrasolar planets. Our methodology is grounded in the principle of entropy maximization, a concept that has found applications across various disciplines, including data physics and statistical mechanics. We extend this principle to the field of optics by introducing a new metric known as optical entropy (OE). This metric allows us to quantify the information content inherent in the point spread functions produced by different occulting masks. By leveraging optical entropy, we can effectively evaluate and compare the efficiency of various mask shapes in their ability to detect faint companions orbiting bright stars.\n\nTo validate our proposed approach, we conducted a series of mathematical simulations to assess the performance of multiple candidate masks. The results of these simulations indicate that our method represents a significant advancement over traditional techniques in the quest for optimal mask designs. Specifically, we demonstrate that our algorithm can identify mask shapes that are particularly adept at revealing dim celestial companions in the vicinity of luminous host stars. This advancement not only enhances the capability of direct imaging in the search for extrasolar planets but also contributes to the broader understanding of optical systems in astronomical applications. Our findings suggest that the integration of entropy-based design principles can lead to more effective strategies in the ongoing exploration of planetary systems beyond our own.",
        "ori-fast-z-score": -0.41702882811414954,
        "water-fast-z-score": 5.910624659963424,
        "rewrite-fast-z-score": 0.5477225575051661
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Experimental approaches to the difference in the Casimir force through the varying optical properties of boundary surface .\nAbstract:\nThe Casimir effect is one of the most important manifestations of quantum vacuum fluctuations and has been studied extensively for more than 50 years. The original prediction was made by H.B.G. Casimir, who showed that two parallel uncharged conducting plates attract each other with an attractive force inversely proportional to the fourth power of their separation distance. In this work we study how the magnitude of the Casimir force depends on the optical properties of the boundaries between which it acts. We consider three different cases corresponding to (i) perfectly reflecting mirrors, (ii) dielectric mirrors, and (iii) metallic mirrors. For all these cases we calculate numerically the Casimir energy density using the Lifshitz theory. Our results show that the Casimir force acting between two identical mirrors can be either repulsive or attractive depending on whether they are separated by air or water. This behavior arises due to the fact that the reflection coefficients at the mirror surfaces depend strongly on the surrounding medium.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Experimental insights to the difference in the Casimir force through the different optical properties of boundary material . Abstract : The Casimir effect is one of the most important manifestations of quantum vacuum fluctuations and has been studied thoroughly for more than 50 centuries .The original forecast was making by H . B . G . Casimir , who demonstrated that two connected uncharged conducting plates attract each other with an attractive force inversely proportional to the fourth power of their separation distance .In this research we study how the severity of the Casimir force depends on the optical properties of the boundaries between which it action . We consider three different instances corresponding to ( i ) properly absorbing glass , ( ii ) dielectric windows , and ( iii ) metallic mirrors .For all these circumstances we estimate numerically the Casimir energy density using the Lifshitz principle . Our results show that the Casimir force acting between two identical mirrors can be either repulsive or attractive depending on whether they are apart by air or air .This phenomenon arises owing to the fact that the reflection coefficients at the mirror surfaces depend intensely on the nearby medium .",
        "rewrite_text": "Title: Experimental Insights into the Variations of the Casimir Force Based on the Optical Properties of Boundary Materials\n\nAbstract: The Casimir effect stands as a significant illustration of quantum vacuum fluctuations, having been extensively investigated for over five decades. Initially predicted by H. B. G. Casimir, this phenomenon describes the attractive force between two uncharged, parallel conducting plates, which is inversely proportional to the fourth power of their separation distance. In this study, we explore how the characteristics of the Casimir force are influenced by the optical properties of the materials at the boundaries where the force operates. We analyze three distinct scenarios: (i) perfectly absorbing glass, (ii) dielectric windows, and (iii) metallic mirrors. Utilizing the Lifshitz theory, we numerically calculate the Casimir energy density for each case. Our findings reveal that the Casimir force between two identical mirrors can exhibit either repulsive or attractive behavior, contingent upon whether the space between them is filled with air or another medium. This intriguing behavior is attributed to the significant dependence of the reflection coefficients at the surfaces of the mirrors on the surrounding medium. The implications of these results extend our understanding of the Casimir effect and its sensitivity to material properties, potentially informing future applications in nanotechnology and quantum physics. Through this research, we aim to provide deeper insights into the interplay between quantum mechanics and material science, highlighting the importance of optical characteristics in the manifestation of the Casimir force.",
        "ori-fast-z-score": -1.078327732034384,
        "water-fast-z-score": 4.610083899802517,
        "rewrite-fast-z-score": 0.7844645405527362
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Contractions, deformations and curvature .\nAbstract:\nThe aim of this article is to present the basic concepts in Riemannian geometry that are needed for understanding the main results presented here.  The first section introduces some notation and definitions used throughout the text.   In particular we define what it means for two points on an n-dimensional manifold M to be close together (in terms of geodesic distance) or far apart.    We also introduce the concept of a local coordinate system at each point p ∈ M which allows us to describe any other point q near p by giving its coordinates with respect to these local charts.   Finally we give a brief description of how one can construct such a coordinate system locally around a given point using parallel transport along curves starting at p.    The second section describes the notion of a vector field X defined over all of M.   This is done by defining a map F : T M → R where T M denotes the tangent bundle of M.   Then we show that if X satisfies certain conditions then there exists a unique smooth function f : M → R such that X = grad(f).   Here grad(f) denotes the gradient of f.   For example, if M is a surface embedded in R3 then X could represent the velocity of a particle moving across M.   If we assume that the particles move according to Newton s laws of motion then the function f would correspond to the potential energy of the system under consideration.   The third section defines the concept of a tensor field as a generalization of vector fields.   Tensor fields allow us to associate several vectors...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Contractions , deformations and curvature . Abstract : The goal of this page is to provide the fundamental concepts in Riemannian theory that are needed for explaining the main results presented here .The first section introduces some terminology and definitions found throughout the text . In particular we define what it means for two points on an n - dimensional manifold M to be close together ( in terms of geodesic length ) or far separated .We also add the idea of a local coordinate system at each point p ∈ M which allows us to define any other point q near p by giving its coordinates with regard to these local charts . Finally we give a brief description of how one can build such a coordinate system locally around a given point using parallel transport along curves beginning at p . The second chapter explains the notion of a vector field X defined over all of M . This is accomplished by creating a mapping F : T M → R where T M denotes the tangent bundle of M . Then we prove that if X satisfies certain conditions then there exists a unique smooth function f : M → R such that X = grad ( f ) .Here grad ( f ) denotes the gradient of f . For instance , if M is a surface embedded in R3 then X could represent the velocity of a particle moving across M . If we suppose that the ions move due to Newton s rules of movement then the function r would correspond to the potential electricity of the system under consideration . The third chapter explains the notion of a tensor field as a generalization of vector fields .Tensor fields allow us to associate multiple vectors . . .",
        "rewrite_text": "**Title:** Contractions, Deformations, and Curvature\n\n**Abstract:** This article aims to elucidate the essential principles of Riemannian geometry necessary for understanding the primary findings presented herein. The initial section lays the groundwork by introducing key terminology and definitions that will recur throughout the text. Specifically, we explore the concept of proximity between two points on an n-dimensional manifold M, characterized by geodesic length, distinguishing between points that are close and those that are more distant. Additionally, we introduce the concept of local coordinate systems at each point p ∈ M, which facilitates the identification of nearby points q through their coordinates in relation to these local charts. We also provide a concise overview of the process for constructing such a local coordinate system around a given point, utilizing parallel transport along curves that originate from p.\n\nIn the subsequent chapter, we delve into the definition of a vector field X that is defined across the entirety of M. This is achieved by establishing a mapping F: TM → R, where TM represents the tangent bundle of M. We demonstrate that under certain conditions, there exists a unique smooth function f: M → R such that X can be expressed as the gradient of f, denoted as grad(f). For example, if M is a surface embedded in R³, X could represent the velocity of a particle traversing M. Assuming that the ions move according to Newtonian mechanics, the function r would correspond to the electric potential of the system under investigation.\n\nThe third chapter extends the discussion to tensor fields, which serve as a generalization of vector fields. Tensor fields enable the association of multiple vectors, thereby enriching the framework of Riemannian geometry and providing deeper insights into the geometric and physical interpretations of the manifold M. Through this exploration, we aim to lay a comprehensive foundation for the advanced concepts and results that follow in the article.",
        "ori-fast-z-score": -0.26013299085723596,
        "water-fast-z-score": 6.11070705734948,
        "rewrite-fast-z-score": 0.6666666666666666
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Two branches of neutron stars - reconciling a 2M_sun pulsar and SN1987A .\nAbstract:\nWe propose that the two branches in the mass distribution of neutron stars are due to different mechanisms for their formation, with one branch being formed by accretion-induced collapse (AIC) of white dwarfs into neutron stars, while another is formed through core-collapse supernovae (CCSNe). We show how this scenario can explain both the existence of massive pulsars as well as the absence of such objects in the observed sample of CCSNe remnants. The proposed model also explains why there have been no successful attempts so far at detecting gravitational waves emitted during AIC events. In addition we argue that our model provides an explanation for the apparent discrepancy between the masses inferred from observations of binary systems containing black holes or neutron stars on one hand, and those inferred from measurements of the radii of isolated neutron stars on the other. Finally, we discuss possible observational tests which could be used to verify our hypothesis.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Two branches of neutron stars - reconciling a 2M _ sun pulsar and SN1987A . Abstract : We suggest that the two branches in the mass distribution of neutron stars are owing to different processes for their formed , with one branch being created by accretion - caused merger ( AIC ) of white dwarfs into neutron stars , while another is formed through core - collapse supernovae ( CCSNe ) .We see how this situation can describe both the existence of large pulsars as well as the absence of such objects in the observed sample of CCSNe fragments . The proposed theory also explains why there have been no successful proposals so far at detecting gravitational waves emitted during AIC events .In addition we claim that our model provides an reason for the apparent discrepancy between the masses inferred from measurements of binary systems featuring white holes or neutron galaxies on one hand , and those inferred from measurements of the radii of isolated neutron galaxies on the other . Finally , we explain possible observational tests which could be used to confirm our theory .",
        "rewrite_text": "**Title:** Two Branches of Neutron Stars: Reconciling a 2M_sun Pulsar and SN1987A\n\n**Abstract:** In this study, we propose that the observed dual branches in the mass distribution of neutron stars arise from distinct formation processes. One branch is attributed to the accretion-induced collapse (AIC) of white dwarfs into neutron stars, while the other results from core-collapse supernovae (CCSNe). This framework effectively accounts for the presence of massive pulsars, such as the 2M_sun pulsar, alongside the notable absence of similarly massive objects in the remnants of CCSNe. Our model also addresses the challenges faced in detecting gravitational waves from AIC events, which have yet to yield successful observational results. Furthermore, we explore the apparent inconsistency between mass estimates derived from binary systems containing white dwarfs or neutron stars and those obtained from measurements of the radii of isolated neutron stars. By reconciling these discrepancies, our theory provides a cohesive understanding of neutron star formation and evolution. We conclude by outlining potential observational tests that could validate our hypotheses, paving the way for future research in this intriguing area of astrophysics.",
        "ori-fast-z-score": -0.3333333333333333,
        "water-fast-z-score": 4.858987147293248,
        "rewrite-fast-z-score": 1.6666666666666667
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Structure of the Homunculus. III. Forming a Disk and Bipolar Lobes in a Rotating Surface Explosion .\nAbstract:\nWe present new results on the formation of bipolar lobes by an expanding, rotating surface explosion (a  rotating detonation ). We use two-dimensional hydrodynamic simulations to show that such explosions can form disks with large opening angles if they are not too energetic or fast-expanding. The disk is formed because the outer layers of the star are swept up into a thin shell as it expands outward at high speed; this shell then breaks apart due to Rayleigh-Taylor instabilities. As the shell fragments, material falls back onto the central region of the exploded star forming two opposite jets which break out along the poles of the system. These jets drive the expansion of the bipolar lobes. Our models reproduce many observed properties of the Homunculus: its size, shape, kinematics, chemical composition, and luminosity evolution. In addition, we find that our model predicts a total mass loss rate for η Carinae during the Great Eruption of ~10^−4 M_sun/yr, consistent with observations.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Structure of the Homunculus . III .Forming a Disk and Bipolar Lobes in a Rotating Surface Explosion . Abstract : We report new results on the formation of bipolar lobes by an increasing , rotating surface explosion ( a rotating detonation ) .We use two - dimensional hydrodynamic simulations to see that such explosions can form disks with large opening angles if they are not too energetic or fast - expanding . The disk is formed because the exterior layers of the star are swept up into a thin shell as it expands outward at high velocity ; this shell then broke separated due to Rayleigh - Taylor instabilities .As the shell fragments , debris slides back onto the main region of the exploded star producing two opposite jets which break out along the poles of the system . These jets drive the development of the bipolar lobes .Our models reproduce many observed properties of the Homunculus : its size , shape , kinematics , chemical composition , and luminosity progression . In addition , we find that our model predicts a total mass loss rate for η Carinae during the Great Eruption of ~ 10 ^ −4 M _ sun / yr , compatible with observations .",
        "rewrite_text": "In this article, titled \"The Structure of the Homunculus. III. Forming a Disk and Bipolar Lobes in a Rotating Surface Explosion,\" we present significant findings regarding the formation of bipolar lobes resulting from a rotating surface explosion, also known as a rotating detonation. Utilizing two-dimensional hydrodynamic simulations, we demonstrate that such explosions can generate disks with substantial opening angles, provided they are not excessively energetic or rapidly expanding. The formation of the disk occurs as the outer layers of the star are propelled outward at high velocities, creating a thin shell. This shell subsequently undergoes fragmentation due to Rayleigh-Taylor instabilities. As the shell pieces break apart, debris falls back onto the central region of the exploded star, leading to the emergence of two opposing jets that erupt along the system's poles. These jets are instrumental in the formation of the bipolar lobes.\n\nOur simulations successfully replicate numerous observed characteristics of the Homunculus, including its dimensions, morphology, kinematic behavior, chemical makeup, and the progression of its luminosity. Furthermore, our model estimates a total mass loss rate for η Carinae during the Great Eruption of approximately 10^−4 M_sun/year, which aligns well with existing observational data. This research enhances our understanding of the mechanisms behind the formation of complex structures in stellar explosions and provides insights into the dynamics of mass ejection in massive stars. The implications of these findings contribute to the broader field of astrophysics, particularly in the study of stellar evolution and the phenomena associated with supernovae and their remnants.",
        "ori-fast-z-score": 1.8225913092242512,
        "water-fast-z-score": 5.253351420705195,
        "rewrite-fast-z-score": 1.8569533817705188
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  General Relativistic Flux Modulations from Disk Instabilities in Sagittarius A* .\nAbstract:\nWe present the first detection of relativistically modulated X-ray fluxes from the Galactic Center black hole candidate SgrA*, using data obtained with Chandra and XMM-Newton observatories over an eight-year period (2000-2007). The observed light curves show clear periodic dips on time scales ranging between 20 minutes to several hours that are consistent with being caused by general relativistic effects near the event horizon of this supermassive black hole. We find no evidence for long-term variability or flaring activity during these observations. These results provide strong support for theoretical models where the emission is produced close to the last stable orbit around the central black hole via accretion disk instabilities. This work was supported by NASA grants NAG5-10842, NNG06GH50G, NNX07AH41G, and NSF grant AST-0707765. Subject headings: Black holes -accretion disks -X-rays",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : General Relativistic Flux Modulations from Disk Instabilities in Sagittarius A * . Abstract : We report the first detection of relativistically modulated X - ray fluxes from the Galactic Center black hole member SgrA * , using data acquired with Chandra and XMM - Newton observatories over an eight - year period ( 2000 - 2007 ) .The observed light curves show consistent continuous dips on time ranges ranging between 20 seconds to several days that are compatible with being created by general relativistic effects near the event horizon of this supermassive black hole . We see no evidence for short - term variability or flaring activity during these observations .These data provide broad support for theoretical theories where the emission is produced nearly to the last steady orbit around the central black hole via accretion disk instabilities . This research was supported by NASA grants NAG5 - 10842 , NNG06GH50G , NNX07AH41G , and NSF grant AST - 0707765 .Subject headings : Black holes - accretion disks - X - rays",
        "rewrite_text": "Title: General Relativistic Flux Modulations from Disk Instabilities in Sagittarius A*\n\nAbstract: In this study, we present the inaugural detection of relativistically modulated X-ray fluxes emanating from Sagittarius A* (SgrA*), the supermassive black hole located at the center of our galaxy. Utilizing data collected over an extensive eight-year period (2000-2007) from the Chandra and XMM-Newton observatories, we have identified light curves that exhibit consistent and periodic dips in brightness. These fluctuations occur over time scales ranging from 20 seconds to several days, suggesting a strong correlation with general relativistic effects occurring in the vicinity of the black hole's event horizon. Notably, our observations reveal an absence of short-term variability or flaring activity, which further supports the notion that the observed X-ray emissions are primarily influenced by stable processes rather than chaotic or explosive events. The findings align with theoretical models positing that the X-ray emissions are generated close to the last stable orbit around the black hole, driven by instabilities within the accretion disk. This research contributes significant evidence to the understanding of black hole physics and the dynamics of accretion disks. Our work was made possible through the support of NASA grants NAG5-10842, NNG06GH50G, NNX07AH41G, and NSF grant AST-0707765. The implications of these findings are profound, as they enhance our comprehension of the behavior of matter in extreme gravitational fields and the mechanisms behind X-ray emissions from supermassive black holes. \n\nSubject headings: Black holes, Accretion disks, X-rays.",
        "ori-fast-z-score": -1.4552137502179978,
        "water-fast-z-score": 3.491189739487307,
        "rewrite-fast-z-score": 1.2567574357593625
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Mesoscopic fluctuations of the supercurrent in diffusive Josephson junctions .\nAbstract:\nWe study mesoscopic fluctuations of the supercurrents flowing through two weakly coupled superconductors with different transparencies and temperatures, using the Usadel equations for quasiclassical Green s functions. We show that the current noise is suppressed by increasing transparency between the leads or decreasing temperature. The suppression can be explained as due to an increase of the effective junction length caused by Andreev reflection at the interface. In addition we find that the shot-noise power decreases when the phase difference across the junction increases. This effect originates from the dependence of the density of states on the phase difference. Finally, we discuss how our results are related to recent experiments performed on diffusive SNS junctions. \nI. INTRODUCTORY REMARK\nThe Josephson effect describes macroscopically coherent transport of Cooper pairs across weak links connecting two superconducting electrodes  1  . It has been observed experimentally over many decades  2  , but only recently have researchers begun to explore its microscopic origins  3  .\nIn this work we consider a system consisting of two weakly-coupled superconductors (S) connected via a normal metal region (N). Such systems are known as diffusive SNS junctures  4  . They exhibit interesting phenomena such as the proximity effect  5  , which causes the formation of a minigap inside the N region  6  . Another important feature of these devices is their ability to carry both charge and spin currents  7, 8  . These properties make them promising candidates for applications ranging from quantum information processing  9  to magnetic field sensing  10  .\nRecently there has been renewed interest in studying the physics of diffusive SNS juncture  11  -  16  . For example, it was shown theoretically that the critical current I c depends strongly on the transparency T = R Q /R N of the NS interfaces  17  where R Q and R N are the resistance quantum and the resistance of the N region respectively. Experimentally, this prediction could not yet be confirmed because of difficulties associated with fabricating clean NS interfaces  18  . However, several groups managed to observe similar effects indirectly  19, 20  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Mesoscopic fluctuations of the supercurrent in diffusive Josephson junctions . Abstract : We explore mesoscopic fluctuations of the supercurrents flowing through two tightly correlated superconductors with varying transparencies and temperatures , using the Usadel equations for quasiclassical Green s functions .We see that the current noise is suppressed by increasing transparency between the leads or decreasing temperature . The suppression can be understood as owing to an increase of the effective junction length produced by Andreev reflection at the interface .In addition we find that the shot - noise strength decreases when the phase change across the junction increases . This phenomenon originates from the dependence of the density of states on the phase change .Finally , we explain how our findings are related to recent experiments conducted on diffusive SNS junctions . I .INTRODUCTORY REMARK The Josephson effect represents macroscopically consistent transport of Cooper pairs across weak links connecting two superconducting electrodes 1 . It has been observed experimentally over numerous years 2 , but only lately have researchers begun to examine its microscopic origins 3 .In this study we study a system consisting of two weakly - coupled superconductors ( S ) connected via a normal metal area ( N ) . Such structures are known as diffusive SNS junctures 4 .They show important phenomena such as the proximity phenomenon 5 , which gives the formation of a minigap inside the N region 6 . Another important feature of these machines is their capabilities to carry both charge and spin currents 7 , 8 .These properties make them promising candidates for applications ranging from quantum information processing 9 to magnetic field monitoring 10 . Recently there has been continued interest in investigating the physics of diffusive SNS juncture 11 - 16 .For instance , it was shown theoretically that the critical current I c varies strongly on the transparency T = R Q / R N of the NS interfaces 17 where R Q and R N are the resistance quantum and the resistance of the N region respectively . Experimentally , this prediction could not already be verified because of troubles associated with fabricating safe NS interfaces 18 .However , various groups helped to observe identical effects indirectly 19 , 20 .",
        "rewrite_text": "**Title:** Mesoscopic Fluctuations of the Supercurrent in Diffusive Josephson Junctions\n\n**Abstract:** In this study, we investigate the mesoscopic fluctuations of supercurrents in diffusive Josephson junctions formed by two closely correlated superconductors, focusing on the effects of varying transparency and temperature. Utilizing the Usadel equations for quasiclassical Green's functions, we analyze the behavior of current noise in these systems. Our findings reveal that increasing the transparency between the leads or lowering the temperature leads to a suppression of current noise. This suppression can be attributed to an effective increase in the junction length, which is a result of Andreev reflection occurring at the interface. Furthermore, we observe that the strength of shot noise diminishes as the phase difference across the junction increases. This phenomenon is linked to the dependence of the density of states on the phase variation. We also discuss the implications of our results in the context of recent experimental investigations of diffusive superconducting-normal-superconducting (SNS) junctions. \n\nThe Josephson effect, which facilitates the coherent transport of Cooper pairs across weak links between superconducting electrodes, has been a subject of extensive experimental study over the years. However, only recently have researchers begun to delve into its microscopic underpinnings. Our research focuses on a system comprising two weakly coupled superconductors connected by a normal metal region, commonly referred to as diffusive SNS junctions. These structures exhibit significant phenomena, including the proximity effect, which leads to the formation of a minigap within the normal region. Additionally, they possess the ability to conduct both charge and spin currents, making them promising candidates for applications in quantum information processing and magnetic field sensing. The ongoing interest in the physics of diffusive SNS junctions is underscored by theoretical predictions regarding the critical current's strong dependence on the transparency of the NS interfaces. Although experimental verification of these predictions has faced challenges due to difficulties in fabricating reliable NS interfaces, indirect observations have been reported by various research groups, highlighting the relevance of our findings in advancing the understanding of these complex systems.",
        "ori-fast-z-score": -1.150792911137501,
        "water-fast-z-score": 7.291164092619461,
        "rewrite-fast-z-score": 0.6575959492214292
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The IDV source J1128+5925, a new candidate for annual modulation? .\nAbstract:\nWe report on the results of an analysis of data taken by the INTEGRAL satellite in 2003 and 2004 . We find that the hard X-ray emission from this source is modulated with a period of about one year. The amplitude of the modulation is at least 50% (3 sigma). This result suggests that the source may be similar to other galactic sources which show evidence for periodic variability due to accretion onto a neutron star or black hole. \n \n Keywords: High energy astrophysics - Gamma rays - Black holes - Neutron stars - Accreting binaries - Pulsar wind nebulae - Inverse Compton scattering - Galactic center - Galaxy - Supernova remnants - Blazars - AGN - Cosmic rays - Fermi/LAT - TeV blazar - Variability - Periodicities - INTEGRAL - X-rays - Hard X-rays - Soft gamma-ray bursts - Transient objects - Radio pulsar",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The IDV source J1128 + 5925 , a new nominee for annual modulation ? .Abstract : We report on the conclusion of an assessment of measurements taken by the INTEGRAL satellite in 2003 and 2004 . We see that the hard X - ray radiation from this source is modulated with a period of about one year .The amplitude of the modulation is at least 50 % ( 3 sigma ) . This result suggests that the source may be similar to other galactic sources which show proof for periodic variability due to accretion onto a neutron star or black hole .Keywords : High energy astrophysics - Gamma rays - Black holes - Neutron stars - Accreting binaries - Pulsar wind nebulae - Inverse Compton scattering - Galactic center - Galaxy - Supernova remnants - Blazars - AGN - Cosmic rays - Fermi / LAT - TeV blazar - Variability - Periodicities - INTEGRAL - X - radiation - Hard X - radiation - Soft gamma - ray waves - Transient objects - Radio pulsar",
        "rewrite_text": "Title: The IDV Source J1128 + 5925: A Potential Candidate for Annual Modulation\n\nAbstract: In this study, we present the findings from an extensive analysis of data collected by the INTEGRAL satellite during the years 2003 and 2004, focusing on the source J1128 + 5925. Our investigation reveals that the hard X-ray emissions from this source exhibit a modulation period of approximately one year. Notably, the amplitude of this modulation is significant, reaching at least 50% with a confidence level of 3 sigma. This intriguing result raises the possibility that J1128 + 5925 may share characteristics with other well-documented galactic sources that display periodic variability, which is often attributed to accretion processes occurring around neutron stars or black holes. The implications of these findings are substantial, as they suggest that J1128 + 5925 could be an important object for further study in the context of high-energy astrophysics. By exploring the mechanisms behind the observed modulation, we may gain deeper insights into the nature of accreting binaries and the dynamics of pulsar wind nebulae. Additionally, this research contributes to the broader understanding of variability and periodicities in the universe, particularly in relation to phenomena such as supernova remnants, blazars, and active galactic nuclei (AGN). Our results underscore the need for continued observation and analysis of transient objects like J1128 + 5925, as they may hold key information regarding cosmic rays and the intricate processes governing high-energy emissions in our galaxy. This work not only enhances our knowledge of hard X-ray sources but also opens avenues for future investigations into the complex interplay between various astrophysical entities. \n\nKeywords: High energy astrophysics, Gamma rays, Black holes, Neutron stars, Accreting binaries, Pulsar wind nebulae, Inverse Compton scattering, Galactic center, Galaxy, Supernova remnants, Blazars, AGN, Cosmic rays, Fermi/LAT, TeV blazar, Variability, Periodicities, INTEGRAL, X-radiation, Hard X-radiation, Soft gamma-ray waves, Transient objects, Radio pulsar.",
        "ori-fast-z-score": 0.13245323570650439,
        "water-fast-z-score": 3.151354388633341,
        "rewrite-fast-z-score": 2.138089935299395
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Accretion Disk Illumination in Schwarzschild and Kerr Geometries: Fitting Formulae .\nAbstract:\nWe present fitting formulae for the illumination of accretion disks by hot spots, as seen in Schwarzschild and rotating black holes (Kerr). The formulae are derived using ray tracing through the disk atmosphere with an approximate treatment of Compton scattering. We find that the dependence on the spin parameter is weak when the spot size is small compared to the radius at which photons decouple from matter. For larger spots we find that the effect increases strongly towards prograde spins. Our results can be used to estimate the effects of relativistic Doppler boosting and gravitational lensing on observed spectra. They may also provide useful input into models of X-ray reflection spectroscopy. \nIntroduction\n\nAccreting black holes produce bright emission lines in their X-ray spectrum due to reprocessing of hard X-rays emitted near the event horizon by cold material orbiting close to the equatorial plane. These features have been studied extensively over many years both observationally and theoretically (see Reynolds & Nowak 2003 , Done et al 2004 . In particular, they show strong red-shifts indicating that the emitting gas orbits rapidly around the black hole. This rapid rotation causes additional shifts in energy due to relativistic Doppler boosts and gravitational lensing. Relativistic effects become more important if the emitting region has a high degree of rotational support or is viewed nearly face-on. It is therefore necessary to take these effects into account when interpreting observations of such systems. \n\nIn this work we consider the case where the illuminating source is located above the disk surface but below its photosphere. Such sources include magnetic flares produced within the disk itself or active regions associated with the inner edge of the disk. We assume that the disk is optically thick so that all radiation reaching it is absorbed and re-emitted locally. We use Monte Carlo simulations to calculate the emergent flux from the disk under various assumptions about the geometry of the system.\n\nThe main goal of our study was to develop simple analytical expressions describing how the shape of the line profile depends on the properties of the system. To do this we performed extensive numerical calculations covering a wide range",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Accretion Disk Illumination in Schwarzschild and Kerr Geometries : Fitting Formulae . Abstract : We present fitting formulae for the illumination of accretion disks by hot points , as shown in Schwarzschild and rotating black holes ( Kerr ) .The formulae are derived using ray tracing through the disk atmosphere with an approximate treatment of Compton absorption . We see that the dependence on the spin vector is weak when the spot size is tiny compared to the radius at which photons decouple from matter .For larger spots we find that the impact grows heavily towards prograde spins . Our results can be used to estimate the effects of relativistic Doppler boosting and gravity lensing on measured spectra .They might additionally offer useful input into models of X - ray reflection spectroscopy . Introduction Accreting grey holes create bright emission lines in their X - ray spectrum due to reprocessing of hard X - rays generated near the event horizon by cold matter orbiting close to the equatorial plane .These features have been studied frequently over numerous years both observationally and theoretically ( saw Reynolds & Nowak 2003 , Done et al 2004 . In particular , they show intense red - shifts suggesting that the emitting gas orbits rapidly around the dark hole .This rapid rotation creates additional shifts in energy due to relativistic Doppler boosts and gravity lensing . Relativistic effects become more critical if the emitting area has a high degree of rotational support or is viewed virtually face - on .It is consequently required to take these consequences into consideration when interpreting observations of such systems . In this research we imagine the case where the illuminating source is situated above the disk surface but below its photosphere .Such sources include magnetic flares created within the disk itself or active regions associated with the inner boundary of the disk . We assume that the disk is optically dense so that all light reaching it is emitted and re - radiated locally .We use Monte Carlo simulations to estimate the emergent flux from the disk under various assumptions about the topology of the system . The main goal of our research was to develop simple analytical expressions relating how the morphology of the line profile depends on the properties of the system .To do this we performed extensive numerical measurements encompassing a broad range",
        "rewrite_text": "**Title:** Accretion Disk Illumination in Schwarzschild and Kerr Geometries: Fitting Formulae\n\n**Abstract:** In this study, we introduce fitting formulae that describe the illumination of accretion disks by hot points in the context of Schwarzschild and Kerr black hole geometries. These formulae are derived through ray tracing techniques applied to the disk atmosphere, incorporating an approximate treatment of Compton absorption. Our findings indicate that the influence of the black hole's spin vector on the illumination is minimal when the size of the hot spot is significantly smaller than the radius at which photons decouple from the surrounding matter. However, as the size of the spot increases, we observe a pronounced effect, particularly for prograde spins. The implications of our results extend to the estimation of relativistic Doppler boosting and gravitational lensing effects on observed spectra, providing valuable insights for models of X-ray reflection spectroscopy.\n\nAccreting black holes generate bright emission lines in their X-ray spectra due to the reprocessing of hard X-rays produced near the event horizon by cold matter in close orbit around the equatorial plane. These spectral features have been extensively studied both observationally and theoretically, as noted in previous works (e.g., Reynolds & Nowak 2003; Done et al. 2004). The observed intense redshifts suggest that the emitting gas is in rapid orbit around the black hole, leading to additional energy shifts from relativistic Doppler effects and gravitational lensing. Such relativistic phenomena become increasingly significant when the emitting region exhibits a high degree of rotational support or is viewed nearly face-on, necessitating careful consideration in the interpretation of observational data.\n\nIn our research, we consider a scenario where the illuminating source is positioned above the disk surface but beneath its photosphere. Potential sources of illumination include magnetic flares occurring within the disk or active regions near its inner boundary. We assume the disk is optically thick, ensuring that all incoming light is locally emitted and re-radiated. Utilizing Monte Carlo simulations, we estimate the emergent flux from the disk under various system configurations. The primary objective of our work is to establish straightforward analytical expressions that relate the morphology of the line profile to the system's properties, achieved through comprehensive numerical measurements across a wide parameter range.",
        "ori-fast-z-score": -0.37796447300922725,
        "water-fast-z-score": 7.53778361444409,
        "rewrite-fast-z-score": 0.5322001448887939
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Electrostatic Climber for Space Elevator and Launcher .\nAbstract:\nThe space elevator is an important project in the future space exploration, which can provide transportation between Earth orbit and ground with high efficiency. The electrostatic climber (EC) is one key component of the space elevator system that will be used to transport passengers or cargoes up and down along the cable. In this work, we present a novel design concept of EC based on the principle of electrodynamic levitation. A prototype has been built according to our proposed design. It consists of two main parts: the upper platform and lower platform. The upper platform contains four permanent magnets arranged in square shape; while the lower platform includes eight coils connected in series. The experimental results show that the maximum lift force generated by the EC reaches about 1 N at the current density of 0.5 A/mm2 . This value meets the requirement of the space elevator application. Moreover, it also shows good stability when the load changes.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Electrostatic Climber for Space Elevator and Launcher . Abstract : The space lift is an important project in the future space exploration , which can provide transit between Earth orbit and ground with high efficiency .The electrostatic climber ( EC ) is one key feature of the spacecraft elevator system that will be used to transport passengers or cargoes up and down along the chain . In this project , we present a new design concept of EC using on the principle of electrodynamic levitation .A prototype has been built according to our proposed design . It consists of two principal portions : the higher platform and lower platform .The upper platform includes four permanent magnets placed in square shape ; while the lower platform includes eight coils connected in series . The empirical results show that the maximum pull pressure generated by the EC reaches about 1 N at the current density of 0 . 5 A / mm2 .This value meets the requirement of the space elevator application . Moreover , it also shows excellent stability when the load changes .",
        "rewrite_text": "**Title:** Electrostatic Climber for Space Elevator and Launcher\n\n**Abstract:** The concept of a space elevator represents a significant advancement in future space exploration, offering an efficient means of transportation between Earth's surface and orbit. Central to this innovative system is the electrostatic climber (EC), which is designed to facilitate the movement of passengers and cargo along the elevator's tether. In this study, we introduce a novel design for the EC that leverages the principles of electrodynamic levitation. A prototype has been constructed based on our proposed design, which consists of two main components: an upper platform and a lower platform. The upper platform is equipped with four permanent magnets arranged in a square configuration, while the lower platform features eight coils connected in series. Experimental results indicate that the EC can generate a maximum pull force of approximately 1 N at a current density of 0.5 A/mm², a value that satisfies the operational requirements for a space elevator. Additionally, the EC demonstrates remarkable stability under varying load conditions, suggesting its potential for reliable performance in practical applications. This research not only contributes to the development of space elevator technology but also paves the way for future advancements in space transportation systems. The findings underscore the viability of electrodynamic levitation as a mechanism for efficient vertical transit in the context of space exploration, highlighting the EC's role in revolutionizing how we access and utilize space resources.",
        "ori-fast-z-score": -0.4364357804719848,
        "water-fast-z-score": 3.927922024247863,
        "rewrite-fast-z-score": -0.09667364890456635
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Game-Theoretic Approach to Energy-Efficient Modulation in CDMA Networks with Delay Constraints .\nAbstract:\nWe consider the problem of energy-efficient modulation for downlink transmissions over code-division multiple-access (CDMA) networks, where each user has an individual delay constraint and is equipped with a rechargeable battery that can be charged by harvesting ambient radio-frequency signals. We formulate this problem as a noncooperative game between users competing for limited power resources under their own constraints on transmission rates and delays. In particular, we show how to compute Nash equilibria of such games using convex optimization techniques. Our numerical results demonstrate significant gains in terms of both network throughput and energy efficiency compared to conventional schemes based on fixed-rate transmissions. The proposed approach also allows us to quantify tradeoffs among different performance metrics, including average packet delay, outage probability, and energy consumption per bit. This work was supported by NSF grants CNS-0932040 and CCF-0939370. \n \n Code available at http://arxiv.org/abs/1206.5481",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Game - Theoretic Approach to Energy - Efficient Modulation in CDMA Networks with Delay Constraints . Abstract : We consider the issue of electricity - efficient modulation for downlink transmissions over code - division multiple - access ( CDMA ) networks , where each consumer has an individual wait constraint and is equipped with a rechargeable battery that can be charged by collecting ambient broadcast - frequency signals .We formulate this question as a noncooperative contest between operators competing for limited power assets under their own restrictions on transmission rates and delays . In particular , we study how to compute Nash equilibria of such players utilizing convex optimization schemes .Our numerical findings show considerable gains in terms of both network throughput and energy efficiency compared to conventional systems based on fixed - frequency communication . The proposed approach also enables us to quantify tradeoffs among different performance metrics , notably mean packet time , outage likelihood , and energy consumption per bit .This project was supported by NSF grants CNS - 0932040 and CCF - 0939370 . Code available at http : / / arxiv . org / abs / 1206 . 5481",
        "rewrite_text": "Title: A Game-Theoretic Approach to Energy-Efficient Modulation in CDMA Networks with Delay Constraints\n\nAbstract: This study addresses the challenge of energy-efficient modulation for downlink transmissions in code-division multiple-access (CDMA) networks, where each user faces specific waiting constraints and utilizes a rechargeable battery that can be replenished by harvesting ambient broadcast-frequency signals. We frame this problem as a noncooperative game among network operators who vie for limited power resources while adhering to their own constraints regarding transmission rates and delays. Our analysis focuses on the computation of Nash equilibria for these competing players, employing convex optimization techniques to derive optimal strategies. The results from our numerical simulations indicate significant improvements in both network throughput and energy efficiency when compared to traditional systems that rely on fixed-frequency communication. Furthermore, our proposed methodology allows for a detailed assessment of the trade-offs between various performance metrics, including average packet delivery time, probability of outage, and energy consumption per transmitted bit. This research was made possible through the support of NSF grants CNS-0932040 and CCF-0939370. The code related to this study is accessible at http://arxiv.org/abs/1206.5481.",
        "ori-fast-z-score": -1.3251783128981585,
        "water-fast-z-score": 5.378448739494827,
        "rewrite-fast-z-score": 0.41702882811414954
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Energy-Efficient Resource Allocation in Wireless Networks: An Overview of Game-Theoretic Approaches .\nAbstract:\nIn this article, we provide an overview on the recent advances and challenges in energy-efficient resource allocation for wireless networks using game-theoretical approaches. We first introduce some basic concepts related to games and then present several representative works that have been done recently by researchers worldwide. Finally, we conclude with open research issues and future directions. In this article, we provide a survey on the recent advances and remaining challenges in energy-efficient radio resource management (RRM) for wireless networks using game theoretical approaches. The main goal is to show how these techniques can be used to improve network performance while reducing power consumption at each node. To achieve this objective, we first give a brief introduction about the concept of games as well as their applications in communication systems. Then, we review existing work on RRM problems formulated as non-cooperative or cooperative games. Finally, we discuss open research issues and possible future directions. This article is organized into four sections. Section II introduces some background information on games and their application in communications. Section III presents different types of games applied to solve RRM problems. Section IV discusses open research issues and concludes the article.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Energy - Efficient Resource Allocation in Wireless Networks : An Overview of Game - Theoretic Approaches . Abstract : In this article , we provide an overview on the recent developments and challenges in electricity - efficient energy management for wireless networks employing game - conceptual approaches .We first introduce some fundamental concepts related to games and then present many representative works that have been performed recently by researchers worldwide . Finally , we continue with open work issues and future directions .In this page , we provide a survey on the recent developments and remaining problems in electricity - efficient radio resource control ( RRM ) for wireless networks employing game theory techniques . The main goal is to indicate how these tools can be used to upgrade system performance while reducing electricity usage at each node .To achieve this objective , we first offer a brief introduction about the idea of games as well as their implementation in information networks . Then , we review current work on RRM questions conceived as non - cooperative or cooperative games .Finally , we discuss open research problems and possible future directions . This page is organized into four sections .Section II gives some background data on players and their application in communications . Section III presents several kinds of games applicable to solve RRM questions .Section IV examines open research problems and concludes the article .",
        "rewrite_text": "Title: Energy-Efficient Resource Allocation in Wireless Networks: An Overview of Game-Theoretic Approaches\n\nAbstract: This article presents a comprehensive overview of the latest advancements and ongoing challenges in the realm of energy-efficient management for wireless networks, specifically through the lens of game-theoretic methodologies. We begin by elucidating fundamental concepts associated with game theory, setting the stage for a detailed examination of significant research contributions made by scholars globally in this field. The primary focus is on radio resource management (RRM) strategies that prioritize electricity efficiency, highlighting how game theory can enhance system performance while minimizing energy consumption at individual nodes.\n\nTo facilitate this exploration, we first introduce the foundational principles of game theory and its relevance to information networks. Following this introduction, we delve into a review of contemporary studies addressing RRM issues framed as either non-cooperative or cooperative games. This analysis underscores the diverse applications of game-theoretic approaches in optimizing resource allocation and energy usage within wireless networks.\n\nThe article is structured into four key sections. Section II provides essential background information on the roles of players within communication systems and their strategic interactions. Section III categorizes various types of games that can be employed to tackle RRM challenges, illustrating their practical implications. Finally, Section IV identifies unresolved research questions and outlines potential future directions for investigation in this dynamic area of study. By synthesizing current knowledge and highlighting gaps in the literature, this article aims to guide future research efforts toward achieving more sustainable and efficient wireless communication systems.",
        "ori-fast-z-score": -1.247219128924647,
        "water-fast-z-score": 7.305140612272933,
        "rewrite-fast-z-score": -0.25819888974716115
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  High Galactic Latitude Interstellar Neutral Hydrogen Structure and Associated (WMAP) High Frequency Continuum Emission .\nAbstract:\nWe present new high resolution observations of the interstellar medium in the direction of the North Ecliptic Pole, made with the Westerbork Synthesis Radio Telescope at 1.4 GHz. The data reveal an extended filamentary structure that is traced by neutral hydrogen emission lines as well as continuum emission associated with free-free processes. We find evidence for two distinct components to this filamentary structure; one component has a relatively low column density but extends over several degrees on the sky while another component appears more compact and denser. These results are discussed within the context of recent WMAP measurements which show excess microwave emission towards the north ecliptic pole region. This work was supported by NASA grant NAG5-10842. Keywords: ISM, radio astronomy, H I 21 cm line, WMAP, filaments, North Ecliptic Pole Region . \nIntroduction\n\nThe Wilkinson Microwave Anisotropy Probe (WMAP) (Bennett et al., 2003a ) measured significant excesses of microwave emission above the expected cosmic background radiation level along three different lines-of-sight through the northern hemisphere. In particular, there were large excesses observed near the North Ecliptic Poles (NEPs). Subsequent studies have shown that these excesses can be explained by thermal bremsstrahlung emission from ionized gas located between us and distant galaxies (Finkbeiner 2004 , Davies et al 2005 .\nIn addition to the NEP regions, other areas of interest include the Perseus-Pisces supercluster complex (Davies et al 2006) , the Coma cluster (Vogeley & Birkinshaw 1996) and the Virgo Cluster (Taylor et al 2002) . All of these structures contain substantial amounts of hot plasma and it seems likely that they will also contribute significantly to the total foreground signal detected by WMAP. \nObservations of the diffuse galactic radio emission provide important information about the physical conditions in the interstellar medium (ISM), such as temperature, pressure and magnetic field strength. However, due to its faintness relative to point sources, only recently have we",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : High Galactic Latitude Interstellar Neutral Hydrogen Structure and Associated ( WMAP ) High Frequency Continuum Emission . Abstract : We report new high resolution measurements of the interstellar medium in the direction of the North Ecliptic Pole , made with the Westerbork Synthesis Radio Telescope at 1 . 4 GHz .The data reveal an extended filamentary composition that is traced by neutral hydrogen emission lines as well as continuum emission associated with free - free processes . We see evidence for two different components to this filamentary composition ; one element has a fairly lowest column density but spreads over numerous degrees on the sky while another component appears more compact and denser .These data are discussed within the context of recent WMAP measurements which show extra microwave emission towards the north ecliptic pole region . This effort was supported by NASA grant NAG5 - 10842 .Keywords : ISM , television astronomy , H I 21 cm line , WMAP , filaments , North Ecliptic Pole Region . Introduction The Wilkinson Microwave Anisotropy Probe ( WMAP ) ( Bennett et al . , 2003a ) measured significant excesses of microwave emission above the expected cosmic background radiation level along three different lines - of - view through the northern hemisphere .In particular , there were large excesses observed near the North Ecliptic Poles ( NEPs ) . Subsequent researchers have shown that these excesses can be described by thermal bremsstrahlung emission from ionized gas located between us and distant galaxies ( Finkbeiner 2004 , Davies et al 2005 .In addition to the NEP regions , other areas of focus involve the Perseus - Pisces supercluster complex ( Davies et al 2006 ) , the Coma cluster ( Vogeley & Birkinshaw 1996 ) and the Virgo Cluster ( Taylor et al 2002 ) . All of these structures hold substantial amounts of bright plasma and it appears probable that they will also contribute considerably to the total foreground noise detected by WMAP .Observations of the diffuse galactic radio emission reveal essential information about the physical conditions in the interstellar medium ( ISM ) , such as temperature , pressure and magnetic field intensity . However , owing to its faintness relative to point sources , only lately have we",
        "rewrite_text": "**Title:** High Galactic Latitude Interstellar Neutral Hydrogen Structure and Associated WMAP High Frequency Continuum Emission\n\n**Abstract:** In this study, we present high-resolution observations of the interstellar medium (ISM) in the vicinity of the North Ecliptic Pole, utilizing the Westerbork Synthesis Radio Telescope at a frequency of 1.4 GHz. Our findings uncover a complex filamentary structure characterized by neutral hydrogen emission lines, alongside continuum emissions linked to free-free processes. Notably, we identify two distinct components within this filamentary arrangement: one component exhibits a relatively low column density but extends over several degrees across the sky, while the other is more compact and denser. These observations are contextualized with recent data from the Wilkinson Microwave Anisotropy Probe (WMAP), which has detected excess microwave emissions in the same region. This excess is hypothesized to arise from thermal bremsstrahlung radiation emitted by ionized gas situated between our observation point and distant galaxies. Previous studies have also highlighted similar excesses in other regions, such as the Perseus-Pisces supercluster, the Coma cluster, and the Virgo Cluster, all of which are known to harbor significant amounts of bright plasma that likely contribute to the foreground noise detected by WMAP. The diffuse galactic radio emissions provide critical insights into the physical conditions within the ISM, including temperature, pressure, and magnetic field strength. However, due to their faintness compared to point sources, detailed studies of these emissions have only recently gained traction. This research was supported by NASA grant NAG5-10842, and the implications of our findings are significant for understanding the ISM's structure and its interaction with cosmic microwave background radiation.\n\n**Keywords:** Interstellar Medium, Radio Astronomy, H I 21 cm Line, WMAP, Filaments, North Ecliptic Pole Region.",
        "ori-fast-z-score": 1.2632278815997784,
        "water-fast-z-score": 6.545614594714175,
        "rewrite-fast-z-score": 0.09407208683835973
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  On generalized entropy measures and pathways .\nAbstract:\nWe present an approach to the analysis of metabolic networks based on information-theoretic concepts, in particular Shannon s entropy measure. We show that this concept can be extended by considering not only single metabolites but also pairs or higher-order tuples of them as elementary units for measuring entropy. This leads us to define so-called pathway entropies which are used to quantify how much uncertainty is associated with different parts of the network. The proposed method allows one to identify those parts of the network where most of the uncertainty resides. In addition we introduce a novel way of visualizing metabolic networks using these new entropy-based quantities. Finally, we demonstrate our approach by applying it to two examples taken from biochemistry literature. Metabolic networks play important roles in many biological processes such as cell growth and development  1  . They consist of chemical reactions transforming various compounds into each other  2  , e.g., glucose molecules are transformed into energy-rich adenosine triphosphate (ATP) molecules via glycolysis  3  .\nThe study of metabolic networks has been attracting increasing interest over recent years  4  -  8  . One reason for this growing interest lies in their potential use as drug targets  9  . Another motivation comes from the fact that they provide valuable insights into cellular metabolism  10  . For example, the identification of key enzymes involved in certain diseases may help to develop drugs against these diseases  11  . Furthermore, metabolic networks have been shown to exhibit scale-free properties  12  similar to those observed in social systems  13  . These findings suggest that there might exist common principles underlying both types of networks  14  .\nIn order to understand the functioning of metabolic networks better, several mathematical models have been developed  15  -  17  . Amongst others, stoichiometric approaches  18  try to describe all possible states of a given metabolic system mathematically. However, due to the high number of degrees of freedom inherent in such models  19  , it becomes difficult to analyze large metabolic networks  20  . Therefore, alternative methods have been suggested  21  -  23  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : On generalized entropy methods and mechanisms . Abstract : We present an view to the analysis of metabolic networks based on knowledge - theoretic concepts , in notably Shannon s entropy test .We see that this concept can be generalized by using not only single metabolites but also pairs or greater - order tuples of them as elementary units for determining entropy . This leads us to define so - called pathway entropies which are applied to quantify how many uncertainty is associated with various parts of the organization .The proposed approach allows one to identify those parts of the network where most of the uncertainty resides . In addition we provide a new manner of visualizing metabolic networks employing these new entropy - based quantities .Finally , we prove our approach by using it to two examples taken from biochemistry literature . Metabolic systems play important roles in different biological pathways such as cell development and growth 1 .They involve of organic reactions transforming various compounds into each other 2 , e . g . , glucose molecules are transformed into energy - rich adenosine triphosphate ( ATP ) molecules via glycolysis 3 . The investigation of metabolic networks has been drawing rising interest over recent months 4 - 8 .One reason for this increasing interest lies in their potential use as drug targets 9 . Another motivation arises from the fact that they give valuable insights into cellular metabolism 10 .For instance , the discovery of key enzymes active in different diseases might help to develop medication against these diseases 11 . Furthermore , metabolic networks have been shown to exhibit scale - free properties 12 similar to those observed in social systems 13 .These studies imply that there might exist common principles governing both types of networks 14 . In order to explain the structures of metabolic networks better , various numerical models have been created 15 - 17 .Amongst others , stoichiometric methods 18 try to explain all possible states of a given metabolic system mathematically . However , owing to the high number of degrees of liberty inherent in such theories 19 , it becomes hard to analyze large metabolic networks 20 .Therefore , alternative techniques have been proposed 21 - 23 .",
        "rewrite_text": "Title: On Generalized Entropy Methods and Mechanisms\n\nAbstract: In this article, we explore a novel perspective on the analysis of metabolic networks through the lens of knowledge-theoretic principles, particularly focusing on Shannon's entropy. We propose a generalization of this concept by considering not only individual metabolites but also pairs or higher-order tuples as fundamental units for entropy calculation. This approach leads to the introduction of \"pathway entropies,\" which serve to quantify the uncertainty associated with various components of metabolic organization. By employing pathway entropies, we can pinpoint areas within the network that harbor the greatest uncertainty, thereby enhancing our understanding of metabolic dynamics.\n\nAdditionally, we present a new visualization technique for metabolic networks that utilizes these entropy-based metrics, offering clearer insights into their structure and function. To validate our methodology, we apply it to two case studies sourced from the biochemistry literature, demonstrating its efficacy in real-world scenarios.\n\nMetabolic systems are crucial to numerous biological processes, including cell development and growth, as they encompass a series of organic reactions that convert various compounds into one another. For example, glucose is metabolized into energy-rich adenosine triphosphate (ATP) through glycolysis. Recent months have seen a surge of interest in the investigation of metabolic networks, driven by their potential as drug targets and their ability to provide valuable insights into cellular metabolism. Identifying key enzymes involved in various diseases could pave the way for new therapeutic strategies.\n\nMoreover, metabolic networks exhibit scale-free properties akin to those found in social networks, suggesting the existence of underlying principles that govern both types of systems. To better elucidate the structures of metabolic networks, numerous numerical models have been developed. Among these, stoichiometric methods aim to mathematically describe all possible states of a metabolic system. However, the complexity and high degrees of freedom inherent in these models pose significant challenges when analyzing large metabolic networks, prompting the exploration of alternative analytical techniques.",
        "ori-fast-z-score": -1.3269776053940743,
        "water-fast-z-score": 8.409632877462002,
        "rewrite-fast-z-score": 0.15523010514126656
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Coronal ion-cyclotron beam instabilities within the multi-fluid description .\nAbstract:\nWe study the linear stability properties of coronal beams in the presence of background plasma and magnetic field fluctuations, using a multi-fluid model for ions and electrons. We find that the growth rates are strongly dependent on the angle between the wavevector k and the mean magnetic field B 0 . In particular, we show that there is an instability at oblique angles with respect to B 0 , which has been previously overlooked by previous studies based on single-fluid models. The new mode arises due to the coupling between the Alfvénic modes associated with each species (ions and electrons). This mode can be excited even when the electron temperature anisotropy T e? /T ez < 1, where ? denotes directions perpendicular to B 0 .\nThe results presented here may have important implications for understanding the origin of solar radio bursts observed during solar flares. Introduction: Coronal mass ejections (CMEs) are large-scale expulsions of magnetized plasma from the Sun s corona into interplanetary space. They play an essential role in driving geomagnetic storms and are believed to be responsible for many other phenomena such as solar energetic particles  e.g., Reames et al. (1998) , Kahler & Ragot (2007)  , solar radio bursts  e.g., Aschwanden (2004)  , and white-light flares  e.g., Benz (2008)  . CME initiation involves the destabilization of a current sheet formed below the erupting flux rope through reconnection processes  e.g., Forbes & Priest (1995) ; Lin & Forbes (2000); Aulanier et al. (2010)  . However, it remains unclear how this process leads to the acceleration of the bulk plasma outflow along open magnetic fields lines. Recent observations suggest that the initial phase of the eruption is characterized by the formation of a narrow jet-like structure called a  flare loop  or  sheath   e.g., Liu et al. (2009a Liu et al. ( , 2009b ; Cheng et al. (2011); Jiang et al. (2012",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Coronal electron - cyclotron beam instabilities within the multi - fluid model . Abstract : We research the linear stability properties of coronal beams in the presence of background plasma and magnetic field fluctuations , using a multi - fluid model for ions and electrons .We see that the development rates are strongly dependent on the angle between the wavevector k and the mean magnetic force B 0 . In particular , we prove that there is an instability at oblique angles with regard to B 0 , which has been previously overlooked by earlier methods using on single - fluid models .The new mode occurs due to the interaction between the Alfvénic mechanisms associated with each species ( atoms and electrons ) . This mode can be excited even when the electron thermal anisotropy T e ?/T ez < 1, where ?denotes directions perpendicular to B 0 .The results presented here possibly have important implications for studying the origin of solar radio pulses seen during thermal flares . Introduction : Coronal mass ejections ( CMEs ) are big - scale expulsions of magnetized liquid from the Sun s corona into interplanetary space .They play an essential part in causing geomagnetic winds and are considered to be responsible for numerous other effects such as solar energetic particles e . g . , Reames et al . ( 1998 ) , Kahler & Ragot ( 2007 ) , solar radio flashes e . g . , Aschwanden ( 2004 ) , and green - light flares e . g . , Benz ( 2008 ) .CME initiation consists the destabilization of a current sheet formed below the erupting flux rope through reconnection pathways g . g . , Forbes & Priest ( 1995 ) ; Lin & Forbes ( 2000 ) ; Aulanier et al . ( 2010 ) .However , it remains unsure how this process results to the acceleration of the bulk plasma outflow along open magnetic fields lines . Recent measurements suggest that the early stage of the volcano is characterized by the formation of a thin plane - like structure named a flare loop or pouch e . g . , Liu et al .( 2009a Liu et al . ( , 2009b ; Cheng et al .( 2011 ) ; Jiang et al . ( 2012",
        "rewrite_text": "**Title:** Coronal Electron-Cyclotron Beam Instabilities within the Multi-Fluid Model\n\n**Abstract:** In this study, we investigate the linear stability characteristics of coronal electron beams in the context of background plasma and magnetic field fluctuations, employing a multi-fluid model that accounts for both ions and electrons. Our findings reveal that the growth rates of instabilities are significantly influenced by the angle between the wavevector \\( k \\) and the mean magnetic field \\( B_0 \\). Notably, we demonstrate the existence of an instability at oblique angles relative to \\( B_0 \\), a phenomenon that has been largely overlooked in previous analyses utilizing single-fluid models. This newly identified mode arises from the interaction of Alfvénic mechanisms associated with different species, specifically ions and electrons. Importantly, this mode can be excited even when the electron thermal anisotropy \\( T_e^\\perp/T_e^z < 1 \\), where \\( \\perp \\) indicates directions perpendicular to \\( B_0 \\). The implications of our results are significant, particularly for understanding the origins of solar radio emissions observed during thermal flares.\n\n**Introduction:** Coronal mass ejections (CMEs) represent large-scale expulsions of magnetized plasma from the Sun's corona into the surrounding interplanetary space. These events play a crucial role in driving geomagnetic winds and are linked to various phenomena, including solar energetic particles (as noted by Reames et al. 1998 and Kahler & Ragot 2007), solar radio bursts (e.g., Aschwanden 2004), and green-light flares (Benz 2008). The initiation of CMEs involves the destabilization of a current sheet that forms beneath the erupting flux rope, facilitated by magnetic reconnection processes (Forbes & Priest 1995; Lin & Forbes 2000; Aulanier et al. 2010). However, the mechanisms by which this destabilization leads to the acceleration of bulk plasma outflows along open magnetic field lines remain poorly understood. Recent observations indicate that the initial phase of a CME is characterized by the formation of a thin, planar structure known as a flare loop or pouch (Liu et al. 2009a; Liu et al. 2009b; Cheng et al. 2011; Jiang et al. 2012).",
        "ori-fast-z-score": -1.281025230440697,
        "water-fast-z-score": 6.164414002968977,
        "rewrite-fast-z-score": 1.469693845669907
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Investigation of relaxation phenomena in high-temperature superconductors HoBa2Cu3O7-d at the action of pulsed magnetic fields .\nAbstract:\nThe influence of pulsed magnetic fields on the relaxation processes in HTSC was investigated by measuring the temperature dependence of resistance and Hall coefficient for samples with different oxygen content (d = 0, 1). The results show that the application of pulsed magnetic fields leads to an increase in the resistivity and Hall mobility of the sample with d = 0. This effect is explained as due to the appearance of additional scattering centers caused by defects formed during the process of magnetization reversal. In contrast, no significant changes were observed in the case of the sample with d=1. It can be assumed that this difference is associated with the presence of structural disordering in the crystal lattice of the latter compound. \n \n Keywords: High-Tc Superconductor, Pulsed Magnetic Field, Relaxation Processes, Defects Formation, Magnetoresistance, Hall Effect. Introduction \n \n Investigation of relaxation phenomena in high temperature superconductors under the action of pulsed external magnetic fields has been attracting considerable attention recently  1 - 5  . These studies are important both for understanding the physics of these materials and for practical applications  6  -  8  . \n \n In particular, it should be noted that the investigation of relaxation processes in HTSCs allows one to study the dynamics of defect formation  9  , which plays an important role in determining their transport properties  10  . At present there are several models describing the mechanism of defect generation  11  -  13  . However, none of them takes into account the possibility of defect formation induced by the action of pulsed fields  14  . \nExperimental details\n\nIn our work we used single crystals of two compounds with different oxygen content: HoBa 2 Cu 3 O 7−δ (HBS) and YBa 2 Cu 3 O 6+δ (YBS), grown using the floating zone method  15  . The oxygen concentration in the samples was determined by iodometric titration  16  . The typical size of the samples was about 5 × 4 mm 2 . The measurements were carried out in liquid helium cryostats equipped with pulse magnets  17  . The maximum value of the magnetic induction reached up to B max =",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Investigation of relaxed processes in high - temperature superconductors HoBa2Cu3O7 - d at the activity of pulsed magnetic fields . Abstract : The impact of pulsed magnetic fields on the relaxation processes in HTSC was investigated by monitoring the temperature dependence of resistance and Hall coefficient for specimens with various oxygen composition ( d = 0 , 1 ) .The results show that the application of pulsed magnetic fields leads to an increase in the resistivity and Hall mobility of the sample with d = 0 . This phenomenon is understood as owing to the appearance of added scattering centers caused by defects formed during the process of magnetization reversal .In contrast , no major changes were detected in the case of the sample with d = 1 . It can be assumed that this distinction is associated with the presence of structural disordering in the crystal lattice of the latter compound .Keywords : High - Tc Superconductor , Pulsed Magnetic Field , Relaxation Processes , Defects Formation , Magnetoresistance , Hall Effect . Introduction Investigation of relaxed behaviour in high heat superconductors under the effects of pulsed external magnetic waves has been drawing greater notice recently 1 - 5 .These studies are important both for knowledge the physics of these structures and for useful use 6 - 8 . In particular , it should be mentioned that the examination of vibration mechanisms in HTSCs allows one to study the dynamics of defect structure 9 , which plays an important role in establishing their transport properties 10 .At currently there are several models explaining the process of defect generation 11 - 13 . However , none of them took into consideration the possibility of defect formed induced by the activity of pulsed fields 14 .Experimental details In our work we using single crystals of two compounds with varying dioxide content : HoBa 2 Cu 3 O 7−δ ( HBS ) and YBa 2 Cu 3 O 6 + δ ( YBS ) , grown using the floating zone method 15 . The oxygen fraction in the samples was calculated by iodometric titration 16 .The typical size of the tests was about 5 × 4 mm 2 . The tests were carried out in pure helium cryostats fitted with pulse magnets 17 .The maximum value of the magnetic induction reached up to B max =",
        "rewrite_text": "**Title:** Investigation of Relaxation Processes in High-Temperature Superconductors HoBa2Cu3O7−δ Under Pulsed Magnetic Fields\n\n**Abstract:** This study explores the effects of pulsed magnetic fields on relaxation processes in high-temperature superconductors (HTSC), specifically focusing on HoBa2Cu3O7−δ (HBS) samples with varying oxygen content (δ = 0, 1). By analyzing the temperature-dependent resistance and Hall coefficient, we observed that the application of pulsed magnetic fields significantly influences the electrical properties of the samples. Notably, for the sample with δ = 0, an increase in resistivity and Hall mobility was recorded, which can be attributed to the introduction of additional scattering centers resulting from defects generated during magnetization reversal. Conversely, the sample with δ = 1 exhibited minimal changes in its electrical characteristics, suggesting that the structural disorder present in its crystal lattice may play a crucial role in its response to pulsed magnetic fields. This distinction highlights the importance of oxygen content in determining the behavior of HTSCs under external magnetic influences.\n\nThe investigation into relaxation processes in HTSCs under pulsed magnetic fields has garnered increasing attention due to its implications for understanding the fundamental physics of these materials and their potential applications. The dynamics of defect structures are particularly significant, as they directly affect the transport properties of superconductors. While several models have been proposed to explain defect generation, most have not accounted for defects induced by pulsed magnetic field activity. \n\nIn our experimental approach, we utilized single crystals of HBS and YBa2Cu3O6+δ (YBS), synthesized via the floating zone method. The oxygen content in the samples was determined through iodometric titration, with typical sample dimensions of approximately 5 × 4 mm². Experiments were conducted in a helium cryostat equipped with pulsed magnets, achieving a maximum magnetic induction of B_max. This research contributes to a deeper understanding of the interplay between pulsed magnetic fields and defect dynamics in high-temperature superconductors, paving the way for future studies in this domain.\n\n**Keywords:** High-Tc Superconductor, Pulsed Magnetic Field, Relaxation Processes, Defect Formation, Magnetoresistance, Hall Effect.",
        "ori-fast-z-score": -1.4411533842457842,
        "water-fast-z-score": 6.304886387294215,
        "rewrite-fast-z-score": -0.8006407690254357
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Primordial Evolution in the Finitary Process Soup .\nAbstract:\nWe present an approach to primordial evolution based on the finitary process soup (FPS) model, which is a generalization of the standard quantum field theory. The FPS consists of all possible finite sequences of elementary processes that can be constructed by applying a set of basic operations to a given initial sequence. We show how this formalism allows one to describe and analyze various aspects of primordial evolution such as entropy production, particle creation, time dilation etc., using only few parameters characterizing the initial state. In particular we demonstrate that the FPS provides a natural description for the inflationary scenario with no need to introduce additional fields or particles beyond those already existing within the Standard Model. Finally, we discuss some open problems related to our approach. PACS numbers: 04.60.Kz, 11.10.Wx, 12.20.Ds, 98.80.Cq . \nI. INTRODUCTORY REMARkS\n\nThe idea behind the finitary process soup  1  , also known as the  quantum soup   2  , is very simple - it represents any physical system as a collection of all its possible states. This concept has been used successfully in many areas of physics including statistical mechanics  3  , condensed matter  4  , nuclear  5  and atomic  6  physics, cosmology  7, 8  , quantum gravity  9  , string theory  10, 11  .\nIn this work we apply the FPS formalism to study primordial evolution during the early stages of the universe s expansion. Our main goal will be to develop a general framework allowing us to describe different phenomena associated with the Big Bang without introducing new degrees of freedom not included into the Standard Model  12  . As we shall see below, the FPS naturally leads to a description of the inflationary scenario  13  where the inflaton field  14  emerges as a consequence of the underlying dynamics rather than being introduced ad hoc. \nII. THE FINITARY PROCESS SOUP MODEL AND ITS APPLICATION TO PRIMORDIAL EVOLUTION A. General Description\nLet us start by briefly reviewing the key features of the FPS formalism",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Primordial Evolution in the Finitary Process Soup . Abstract : We present an view to primordial evolution based on the finitary process soup ( FPS ) model , which is a generalization of the standard quantum field model .The FPS consists of all possible finite sequences of elementary processes that can be built by using a setting of fundamental operations to a given original sequence . We see how this formalism allows one to explain and estimate various parts of primordial development such as entropy production , particle creation , time dilation etc . , using only few parameters characterizing the first state .In particular we prove that the FPS provides a natural explanation for the inflationary scenario with no requirement to introduce extra fields or particles beyond those already operating within the Standard Model . Finally , we explain some open problems related to our approach .PACS codes : 04 . 60 . Kz , 11 . 10 . Wx , 12 . 20 . Ds , 98 . 80 . Cq . I .INTRODUCTORY REMARkS The idea behind the finitary process soup 1 , sometimes called as the quantum soup 2 , is very simple - it represents any physical system as a collection of all its potential states . This concept has been used successfully in multiple fields of science including statistical mechanics 3 , condensed matter 4 , nuclear 5 and atomic 6 theory , cosmology 7 , 8 , quantum gravitational 9 , string theory 10 , 11 .In this project we apply the FPS formalism to study primordial development during the early stages of the universe s evolution . Our main goal will be to develop a general template allowing us to explain different processes associated with the Big Bang without eliminating different degrees of liberty not provided into the Standard Model 12 .As we shall get below , the FPS naturally comes to a description of the inflationary scenario 13 where the inflaton field 14 emerges as a consequence of the fundamental dynamics rather than being established ad hoc . II .THE FINITARY PROCESS SOUP MODEL AND ITS APPLICATION TO PRIMORDIAL EVOLUTION A . General Description Let us begin by briefly examining the key features of the FPS formalism",
        "rewrite_text": "**Title:** Primordial Evolution in the Finitary Process Soup\n\n**Abstract:** In this article, we explore primordial evolution through the lens of the Finitary Process Soup (FPS) model, which serves as an extension of the conventional quantum field theory. The FPS framework encompasses all conceivable finite sequences of elementary processes derived from a set of fundamental operations applied to an initial sequence. This innovative formalism facilitates the analysis and estimation of various aspects of primordial development, including entropy production, particle generation, and time dilation, utilizing only a limited number of parameters that characterize the initial state. Notably, we demonstrate that the FPS framework offers a coherent explanation for the inflationary scenario without necessitating the introduction of additional fields or particles beyond those already recognized in the Standard Model. Furthermore, we address several unresolved issues associated with our approach. \n\nThe concept of the Finitary Process Soup, often referred to as the quantum soup, is straightforward yet profound; it conceptualizes any physical system as a compilation of all its potential states. This idea has found successful applications across various scientific disciplines, including statistical mechanics, condensed matter physics, nuclear and atomic theory, cosmology, quantum gravity, and string theory. In this study, we apply the FPS formalism to investigate the primordial development during the universe's formative stages. Our primary objective is to construct a comprehensive framework that elucidates different processes linked to the Big Bang while preserving the various degrees of freedom inherent in the Standard Model. As we will demonstrate, the FPS framework naturally leads to a description of the inflationary scenario, wherein the inflaton field emerges organically from the fundamental dynamics rather than being arbitrarily introduced. This paper aims to provide a deeper understanding of the early universe's evolution and the underlying principles governing it. \n\n**PACS codes:** 04.60.Kz, 11.10.Wx, 12.20.Ds, 98.80.Cq.",
        "ori-fast-z-score": 0.42107596053325946,
        "water-fast-z-score": 7.495152097492019,
        "rewrite-fast-z-score": 1.1272429603813559
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The metallicity distributions in high-latitudes with SDSS .\nAbstract:\nWe present the results on the metallicity distribution functions (MDFs) for stars at different latitudes and distances from the Galactic plane, based on spectroscopic data obtained by the Sloan Digital Sky Survey (SDSS). We find that MDFs are similar to each other within errors except those at |b| > 30° where there is an excess of metal-poor stars compared to the disk population. The fraction of metal-poor stars increases towards higher latitude. This suggests that the halo component becomes more dominant as one goes farther away from the Galactic plane. In addition we also found that the mean metallicities decrease slightly toward larger distance from the Galactic center. These findings suggest that the outer part of our Galaxy has been formed through accretion processes. \n \n Keywords: Metallicity Distribution Function; Halo; Disk; High Latitude Stars; Sloan Digital Sky Survey \n \n 1 Introduction \n \n It is well known that the Milky Way consists of three main components -the thin disk, thick disk and halo. However, it remains unclear how these components were assembled during its formation history. To understand this process, it is important to study their chemical compositions separately because they may have experienced different evolutionary histories. For example, the age-metallicity relation shows that the halo was formed earlier than the disk(e.g., Twarog 1980), while the abundance ratios such as  Fe/H  show that the halo contains many old low-mass stars which should be destroyed by supernova explosions if the halo had been formed recently like the disk(e. g., Nissen & Schuster 1997). \n \n Many studies have investigated the properties of the halo using various samples of distant halo stars selected mainly from proper motion surveys or photometric parallax measurements. Recently, large spectroscopic surveys such as the Sloan Digital Sky Surveys (SDSS) (York et al. 2000) , RAVE survey (Steinmetz 2003 )and SEGUE survey (Yanny et al. 2009 )have provided us with much better information about the chemical composition of the halo. Using",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The metallicity distributions in high - latitudes with SDSS . Abstract : We report the results on the metallicity distribution functions ( MDFs ) for stars at different latitudes and distances from the Galactic plane , using on spectroscopic data derived by the Sloan Digital Sky Survey ( SDSS ) .We see that MDFs are comparable to each other within errors except those at | b | > 30° where there is an accumulation of steel - poor stars compared to the disk community . The percentage of steel - bad stars increases towards higher latitude .This implies that the halo element increases more prevalent as one goes deeper away from the Galactic jet . In addition we also discovered that the mean metallicities reduce slightly toward larger distance from the Galactic center .These data suggest that the outer portion of our Galaxy has been formed through accretion cycles . Keywords : Metallicity Distribution Function ; Halo ; Disk ; High Latitude Stars ; Sloan Digital Sky Survey 1 Introduction It is well established that the Milky Way consists of three principal components - the narrow disk , thick disk and halo .However , it remains unsure how these constituents were assembled during its formation history . To understand this process , it is important to study their chemical compositions separately because they may have experienced distinct evolutionary histories .For instance , the age - metallicity relation shows that the halo was formed later than the disk ( e . g . , Twarog 1980 ) , while the quantity ratios such as Fe / H indicate that the halo contains much young high - mass stars which should be damaged by supernova explosions if the halo had been formed recently like the disk ( e . g . , Nissen & Schuster 1997 ) . Many experiments have researched the properties of the halo utilizing diverse samples of distant halo stars selected mainly from proper motion surveys or photometric parallax observations .Recently , large spectroscopic studies such as the Sloan Digital Sky Surveys ( SDSS ) ( York et al . 2000 ) , RAVE study ( Steinmetz 2003 ) and SEGUE study ( Yanny et al .2009 ) have provided us with far better details about the chemical composition of the halo . Using",
        "rewrite_text": "**Title:** The Metallicity Distributions in High Latitudes with SDSS\n\n**Abstract:** In this study, we present our findings on the metallicity distribution functions (MDFs) of stars located at various latitudes and distances from the Galactic plane, utilizing spectroscopic data obtained from the Sloan Digital Sky Survey (SDSS). Our analysis reveals that the MDFs across different latitudes are generally consistent within the margin of error, with the notable exception of regions where |b| > 30°. In these high-latitude areas, we observe a significant increase in the population of metal-poor stars compared to those found in the Galactic disk. This trend indicates that the prevalence of metal-poor stars rises with increasing latitude, suggesting that the halo component of the Galaxy becomes more dominant as one moves further from the Galactic plane. Additionally, our results indicate a slight decrease in mean metallicity with increasing distance from the Galactic center, which supports the hypothesis that the outer regions of the Milky Way have been shaped by cycles of accretion. \n\nThe Milky Way is composed of three main structural components: the thin disk, thick disk, and halo. Understanding the formation and evolution of these components is crucial, as they may have undergone different evolutionary processes. Previous studies have established that the halo formed later than the disk, as evidenced by the age-metallicity relationship (Twarog, 1980). Furthermore, elemental abundance ratios, such as [Fe/H], suggest that the halo contains a significant number of young, high-mass stars, which would be expected to show signs of supernova activity if they had formed recently, akin to the disk (Nissen & Schuster, 1997). Various investigations have explored the properties of the halo using samples of distant stars, primarily sourced from proper motion surveys and photometric parallax measurements. Recent extensive spectroscopic surveys, including SDSS (York et al., 2000), RAVE (Steinmetz, 2003), and SEGUE (Yanny et al., 2009), have significantly enhanced our understanding of the halo's chemical composition. This paper aims to contribute to that understanding by analyzing the metallicity distributions of high-latitude stars. \n\n**Keywords:** Metallicity Distribution Function; Halo; Disk; High Latitude Stars; Sloan Digital Sky Survey.",
        "ori-fast-z-score": -0.2526455763199557,
        "water-fast-z-score": 8.00044325013193,
        "rewrite-fast-z-score": -0.08247860988423225
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Blind Identification of Distributed Antenna Systems with Multiple Carrier Frequency Offsets .\nAbstract:\nIn this work, we propose an algorithm for blind identification (BI) of distributed antenna systems (DASs). The proposed BI method is based on the joint use of second-order statistics and higher order cumulants to estimate the number of active users in each cell as well as their carrier frequency offsets (CFOs), which are unknown parameters that need to be estimated before data detection can take place. We show by simulation results that our proposed method outperforms existing methods in terms of bit error rate performance when CFOs exist between different cells. In addition, it has lower computational complexity than other algorithms. \n \n Keywords: Blind identification; Distributed antenna systems; Second-order statistics; Higher order cumulants; CFO estimation. 1 Introduction \n \n With the rapid development of wireless communication technology, there have been increasing demands for high spectral efficiency and reliable transmission over limited bandwidth resources  1  . To meet these requirements, multi-antenna techniques such as multiple-input-multiple-output (MIMO)  2  , massive MIMO  3  -  5  , cooperative relaying  6  , and cognitive radio  7  have attracted much attention recently. Among them, distributed antenna systems (DAs)  8  -  10  provide significant advantages including improved coverage area, enhanced capacity, reduced power consumption, and increased network flexibility  11  . However, DAs also introduce new challenges due to the fact that they operate under non-coherent conditions  12  . For example, the channel state information (CSI) at the transmitter side cannot be obtained directly through uplink training or downlink feedback  13  . Therefore, how to obtain CSI accurately becomes one of the most important issues in DA design  14  .\n \nTo address this issue, several works  15  -  17  have investigated the problem of estimating the number of active users and their corresponding channels simultaneously using only statistical properties of received signals without requiring any prior knowledge about the transmitted symbols. These approaches exploit the inherent sparseness property of user activity patterns and utilize second-order statistics (SOS) and/or higher order cumulants (HOCs)  18  -  20  to identify the number of active users per cell. Then, the channel coefficients associated with",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Blind Identification of Distributed Antenna Systems with Multiple Carrier Frequency Offsets . Abstract : In this research , we develop an algorithm for blind recognition ( BI ) of distributed antenna devices ( DASs ) .The proposed BI model is based on the joint use of second - order statistics and larger order cumulants to estimate the quantity of active consumers in each cell as also as their carrier signal offsets ( CFOs ) , which are unknown parameters that must to be assessed before data diagnosis can take place . We see by simulation data that our proposed approach outperforms previous techniques in terms of bit error rate reliability when CFOs occur between multiple cells .In addition , it has less computational complexity than other methods . Keywords : Blind identity ; Distributed antenna networks ; Second - order statistics ; Higher order cumulants ; CFO estimation .1 Introduction With the increasing progress of wireless communication techniques , there have been growing requirements for high spectral capacity and reliable transmission over limited bandwidth supplies 1 . To address these requirements , multi - antenna techniques such as single - input - multiple - output ( MIMO ) 2 , large MIMO 3 - 5 , joint relaying 6 , and cognitive television 7 have garnered considerable focus today .Among them , dispersed antenna networks ( DAs ) 8 - 10 provide significant advantages including increased coverage space , enhanced capacity , reduced electricity usage , and increased service flexibility 11 . However , DAs additionally introduce new problems due to the fact that they operate under non - coherent environments 12 .For instance , the channel state information ( CSI ) at the broadcasting side cannot be obtained directly through uplink training or downlink feedback 13 . Therefore , how to obtain CSI correctly becomes one of the most important problems in DA design 14 .To address this question , various works 15 - 17 have researched the issue of estimating the number of active participants and their corresponding stations simultaneously employing only statistical characteristics of received messages without requiring any earlier knowledge about the transmitted symbols . These approaches use the intrinsic sparseness property of customer activity patterns and use second - order indicators ( SOS ) and / or greater order cumulants ( HOCs ) 18 - 20 to identify the total of active participants per cell .Then , the channel coefficients identified with",
        "rewrite_text": "**Title:** Blind Identification of Distributed Antenna Systems with Multiple Carrier Frequency Offsets\n\n**Abstract:** This study presents a novel algorithm for the blind identification (BI) of distributed antenna systems (DASs). The proposed BI framework leverages a combination of second-order statistics and higher-order cumulants to accurately estimate the number of active users in each cell, along with their corresponding carrier frequency offsets (CFOs), which are critical yet unknown parameters that must be determined prior to effective data processing. Through extensive simulations, we demonstrate that our method significantly outperforms existing techniques in terms of bit error rate reliability, particularly in scenarios where CFOs are present across multiple cells. Furthermore, our approach exhibits a lower computational complexity compared to alternative methods, making it more efficient for practical applications. \n\nAs wireless communication technologies continue to evolve, there is an increasing demand for enhanced spectral efficiency and reliable transmission within constrained bandwidth environments. To meet these challenges, multi-antenna techniques such as single-input multiple-output (MIMO), large MIMO, joint relaying, and cognitive television have gained considerable attention. Among these, distributed antenna networks (DANs) offer notable benefits, including improved coverage, increased capacity, reduced energy consumption, and greater service flexibility. However, the deployment of DANs also introduces unique challenges, particularly in non-coherent environments where direct channel state information (CSI) cannot be easily acquired through traditional uplink training or downlink feedback mechanisms. Consequently, accurately obtaining CSI has emerged as a critical issue in the design of DASs.\n\nTo tackle this challenge, various studies have explored methods for estimating the number of active users and their respective stations by solely utilizing the statistical properties of received signals, without any prior knowledge of the transmitted symbols. These methods capitalize on the inherent sparsity of user activity patterns and employ second-order statistics and higher-order cumulants to ascertain the total number of active users in each cell. This research contributes to the ongoing efforts to enhance the performance and reliability of distributed antenna systems in complex communication environments. \n\n**Keywords:** Blind identification; Distributed antenna networks; Second-order statistics; Higher-order cumulants; CFO estimation.",
        "ori-fast-z-score": -2.9823111665585627,
        "water-fast-z-score": 8.270429251254134,
        "rewrite-fast-z-score": -0.0716114874039433
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Near-IR Spectra of Red Supergiants and Giants. I- Models with Solar and with Mixing-Induced Surface Abundance Ratios .\nAbstract:\nWe present new near-infrared (NIR) spectra for red supergiants and giants, obtained at the European Southern Observatory in La Silla, Chile.  The sample includes stars with spectral types ranging from M5 to M2Iab, luminosity classes III-V, and surface gravities between log g = 1.0 and 3.8 dex. We compare these observations with synthetic NIR spectra computed using MARCS model atmospheres that include mixing-induced chemical abundance changes near the stellar surface. Our results show that models including such effects are able to reproduce most observed features better than standard solar-scaled models. In particular, we find that the inclusion of mixing leads to an improved fit of the CO bandheads around 2.3 microns as well as of several atomic lines. However, some discrepancies remain which may be due to shortcomings in our current understanding of convection or other physical processes occurring close to the stellar surface.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  Near-IR Spectra of Red Supergiants and Giants.I- Models with Solar and with Mixing-Induced Surface Abundance Ratios .Abstract : We report new near - infrared ( NIR ) spectra for red supergiants and giants , obtained at the European Southern Observatory in La Silla , Chile . The sample comprises stars with spectral classes ranging from M5 to M2Iab , luminosity classes III - V , and surface gravities between log f = 1 . 0 and 3 . 8 dex .We link these observations with artificial NIR spectra computed using MARCS model atmospheres that include mixing - caused chemical abundance changes near the stellar surface . Our results show that models featuring such effects are able to depict most observed features good than conventional solar - scaled models .In particular , we find that the introduction of mixing gives to an better fit of the CO bandheads around 2 . 3 microns as well as of several atomic lines . However , some discrepancies remain which perhaps be due to shortcomings in our future interpretation of convection or other physical processes resulting close to the stars surface .",
        "rewrite_text": "We present new near-infrared (NIR) spectral data for red supergiants and giants, collected at the European Southern Observatory in La Silla, Chile. Our study encompasses a diverse sample of stars, classified within the spectral range of M5 to M2Iab, and includes luminosity classes III to V, with surface gravities varying from log g = 1.0 to 3.8 dex. To enhance our understanding of these celestial bodies, we correlate our observational data with synthetic NIR spectra generated using MARCS model atmospheres. These models incorporate changes in chemical abundances at the stellar surface induced by mixing processes. Our findings indicate that models accounting for these mixing effects provide a superior representation of the observed spectral features compared to traditional solar-scaled models. Notably, the inclusion of mixing leads to a significantly improved fit for the CO bandheads located around 2.3 microns, as well as for various atomic lines. Despite these advancements, we acknowledge that some discrepancies persist, which may stem from limitations in our current understanding of convection and other physical phenomena occurring in the vicinity of the stellar surfaces. This research contributes to the ongoing efforts to refine stellar atmospheric models and deepen our comprehension of the complex processes influencing the spectral characteristics of red supergiants and giants.",
        "ori-fast-z-score": 0.11547005383792514,
        "water-fast-z-score": 4.50333209967908,
        "rewrite-fast-z-score": -1.237705495510552
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Comparison of Dissipative Particle Dynamics and Langevin thermostats for out-of-equilibrium simulations of polymeric systems .\nAbstract:\nWe compare the performance of two different methods to simulate non-equilibrium dynamics in polymer systems, namely dissipative particle dynamics (DPD) with either a Nosé-Hoover or a Langevin thermostat. We show that both DPD schemes are able to reproduce qualitatively similar results when compared against each other as well as experiments on the stretching of single DNA molecules. However, we find significant quantitative differences between the two approaches which can be traced back to the fact that they use fundamentally different equations of motion. In particular, we demonstrate how these differences affect the relaxation behavior after an external force is applied to the chain ends. Finally, we discuss possible ways to overcome some of the shortcomings associated with the current implementations. \n \n Introduction \n \n The study of complex fluids such as polymers requires sophisticated simulation techniques capable of describing their unique properties at various length scales. While atomistic molecular dynamics has been successfully used to investigate phenomena occurring over short time and length scales  1–3 , coarse-grained models have emerged as powerful tools to explore longer timescales  4–6 . These simplified descriptions typically involve representing groups of atoms by one effective interaction site  7–9 . For example, in the case of biopolymers like proteins  10–12  or nucleic acids  13–18 , this approach allows us to capture essential features of the underlying physics while reducing computational costs significantly  19, 20 . \n \n Coarse-graining strategies often rely on mapping the interactions among individual particles onto effective potentials  21 . This simplification enables efficient sampling of configurational space using Monte Carlo  22  or Molecular Dynamics  23  algorithms. Despite its successes, however, coarse-graining comes at the cost of losing detailed information about local structure and fluctuations  24 . As a result, it becomes difficult to accurately describe processes involving large conformational changes  25 . To address this issue, hybrid multiscale modeling frameworks have recently been developed  26 . Here, coarsegrained representations are combined with more accurate microscopic models to provide better estimates of free energy surfaces  27  and transition rates  28 . \n \n Another important aspect of coarse-grained models concerns the choice of appropriate",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Comparison of Dissipative Particle Dynamics and Langevin thermostats for out - of - equilibrium simulations of polymeric systems . Abstract : We contrast the performance of two different methods to simulate non - equilibrium dynamics in polymer models , namely dissipative particle behavior ( DPD ) with either a Nosé - Hoover or a Langevin thermostat .We see that both DPD methods are able to reproduce qualitatively identical outcome when compared against each other as well as experiments on the stretching of single DNA molecules . However , we find considerable quantitative variations between the two strategies which can be traced back to the fact that they use fundamentally different equations of movement .In particular , we study how these changes affect the relaxation behavior after an external force is applied to the chain ends . Finally , we explain possible ways to overcome some of the shortcomings associated with the present implementations .Introduction The investigation of complex fluids such as polymers involves rigorous simulation tools capable of describing their distinct characteristics at several length scales . While atomistic molecular mechanics has been successfully utilized to examine processes arising over short period and duration scales 1 – 3 , coarse - grained models have developed as powerful tools to study longer timescales 4 – 6 .These simplified descriptions typically involve describing groups of atoms by one effective bonding region 7 – 9 . For instance , in the case of biopolymers like genes 10 – 12 or nucleic acids 13 – 18 , this methodology allows us to capture essential aspects of the fundamental theory while reducing theoretical costs significantly 19 , 20 .Coarse - graining methods often relies on mapping the interactions among individual molecules onto effective potentials 21 . This simplification enables efficient scanning of configurational space employing Monte Carlo 22 or Molecular Dynamics 23 methods .Despite its successes , however , coarse - graining comes at the cost of losing explicit data about local structure and fluctuations 24 . As a result , it becomes hard to correctly define systems featuring large conformational changes 25 .To address this question , hybrid multiscale simulation frameworks have recently been created 26 . Here , coarsegrained representations are coupled with more accurate microscopic models to provide better estimates of free energy materials 27 and transfer rates 28 .Another important dimension of coarse - grained models concerns the selection of appropriate",
        "rewrite_text": "**Title:** Comparison of Dissipative Particle Dynamics and Langevin Thermostats for Out-of-Equilibrium Simulations of Polymeric Systems\n\n**Abstract:** This study examines the efficacy of two distinct methodologies for simulating non-equilibrium dynamics in polymeric systems: dissipative particle dynamics (DPD) utilizing either a Nosé-Hoover or a Langevin thermostat. Our findings indicate that both DPD approaches yield qualitatively similar results when compared to each other and to experimental data regarding the stretching of single DNA molecules. However, we observe significant quantitative discrepancies between the two methods, which can be attributed to their fundamentally different equations of motion. Specifically, we investigate how these variations influence the relaxation behavior of polymer chains following the application of an external force at their ends. Furthermore, we propose potential strategies to mitigate some of the limitations associated with the current implementations of these simulation techniques.\n\nThe exploration of complex fluids, particularly polymers, necessitates advanced simulation tools that can accurately capture their unique properties across various length scales. While atomistic molecular mechanics has proven effective for analyzing phenomena occurring over short timeframes, coarse-grained models have emerged as valuable alternatives for studying processes over extended timescales. These models simplify the representation of molecular interactions by grouping atoms into effective bonding regions, which is particularly advantageous for biopolymers such as DNA and nucleic acids. This approach allows researchers to retain critical theoretical insights while significantly reducing computational costs.\n\nCoarse-graining typically involves mapping the interactions of individual molecules onto effective potentials, facilitating efficient exploration of configurational space through methods like Monte Carlo and Molecular Dynamics. Despite its advantages, coarse-graining can lead to a loss of detailed information regarding local structures and fluctuations, complicating the accurate representation of systems undergoing substantial conformational changes. To address these challenges, recent developments in hybrid multiscale simulation frameworks have emerged, integrating coarse-grained representations with more precise microscopic models. This integration aims to enhance the accuracy of free energy estimates and transfer rates, thereby improving the overall understanding of polymer dynamics in complex environments.",
        "ori-fast-z-score": -0.5897678246195885,
        "water-fast-z-score": 7.761823345023015,
        "rewrite-fast-z-score": -0.23643312187173018
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Characteristics of phonon transmission across epitaxial interfaces: a lattice dynamic study .\nAbstract:\nWe have performed first-principles calculations to investigate the characteristics of phonon transmission across an epitaxial interface between Si and Ge, which are important for thermoelectric applications. We found that the phonon transmission is strongly suppressed at low frequencies due to the mismatch in acoustic impedance between two materials. The suppression becomes more significant as the thickness of Ge layer decreases. In addition, we observed that the phonon transmission exhibits strong anisotropy with respect to incident angle. These results suggest that it may be possible to control thermal transport by tuning the structure of epitaxial interfaces. Epitaxial interfaces play crucial roles in determining physical properties such as electrical conductivity  1  , optical reflectivity  2  , mechanical strength  3  , and thermal conductivity  4  . For example, recent studies on superlattices show that the thermal conductance can be reduced significantly compared to bulk values  5, 6  .\nIn this work, we focus on phonons because they dominate heat conduction in solids  7, 8  . Phonon scattering at epitaxial interfaces has been studied extensively using molecular dynamics (MD) simulations  9  or kinetic theory  10  . However, these approaches cannot provide detailed information about phonon transmission across interfaces since they do not take into account atomic interactions explicitly  11  . On the other hand, density functional theory (DFT), which describes electronic states based on quantum mechanics  12  , allows us to calculate phonon transmission coefficients directly  13  . Therefore, DFT-based methods are suitable for investigating phonon transmission across epi-",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Characteristics of phonon communication across epitaxial interfaces : a lattice dynamic investigation . Abstract : We have done first - principles experiments to examine the properties of phonon communication across an epitaxial connection between Si and Ge , which are important for thermoelectric use .We showed that the phonon communication is strongly subdued at low frequencies owing to the mismatch in sound impedance between two materials . The suppression gets more considerable as the height of Ge thickness decreases .In addition , we demonstrated that the phonon communication exhibits strong anisotropy with regard to incident angle . These data suggest that it could be possible to affect heat transport by tuning the composition of epitaxial interfaces .Epitaxial interfaces play crucial roles in determining physical properties such as mechanical conductivity 1 , optical reflectivity 2 , thermal strength 3 , and thermal conductivity 4 . For instance , recent studies on superlattices indicate that the thermal conductance can be reduced greatly compared to bulk values 5 , 6 .In this research , we focus on phonons because they govern heat conduction in solids 7 , 8 . Phonon absorption at epitaxial interfaces has been studied extensively utilizing molecular mechanics ( MD ) simulations 9 or kinetic theory 10 .However , these perspectives cannot offer precise data about phonon communication across interfaces since they do not take into consideration atomic interactions explicitly 11 . On the other hand , density functional theory ( DFT ) , which explains electronic states based on quantum mechanics 12 , allows us to estimate phonon communication equations directly 13 .Therefore , DFT - based methods are suitable for investigating phonon communication across epi -",
        "rewrite_text": "**Title:** Characteristics of Phonon Communication Across Epitaxial Interfaces: A Lattice Dynamic Investigation\n\n**Abstract:** In this study, we conducted first-principles investigations to explore the characteristics of phonon communication at the epitaxial interface between silicon (Si) and germanium (Ge), materials that are critical for thermoelectric applications. Our findings reveal that phonon communication is significantly diminished at low frequencies due to the sound impedance mismatch between Si and Ge. Notably, this suppression becomes more pronounced as the thickness of the Ge layer decreases. Furthermore, we observed that phonon communication demonstrates considerable anisotropy depending on the angle of incidence. These insights suggest that it may be feasible to manipulate heat transport properties by adjusting the composition of epitaxial interfaces.\n\nEpitaxial interfaces are pivotal in influencing various physical properties, including mechanical conductivity, optical reflectivity, thermal strength, and thermal conductivity. Recent investigations into superlattice structures have shown that thermal conductance can be substantially lower than that of bulk materials. Our research specifically targets phonons, as they are the primary carriers of heat conduction in solid materials. While previous studies have extensively examined phonon absorption at epitaxial interfaces through molecular dynamics (MD) simulations and kinetic theory, these approaches often fall short in providing accurate data on phonon communication due to their inability to explicitly account for atomic interactions.\n\nIn contrast, density functional theory (DFT), which is grounded in quantum mechanics, enables a more precise estimation of phonon communication across interfaces by directly addressing atomic-level interactions. Consequently, DFT-based methodologies are particularly well-suited for investigating the nuances of phonon communication at epitaxial interfaces. Our research contributes to a deeper understanding of how phonon dynamics can be tailored to enhance thermoelectric performance, paving the way for future advancements in material design and application.",
        "ori-fast-z-score": -1.2675004445952593,
        "water-fast-z-score": 7.602631123499284,
        "rewrite-fast-z-score": 0.9467292624062575
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  An Adaptive Strategy for the Classification of G-Protein Coupled Receptors .\nAbstract:\nG-protein-coupled receptors (GPCRs) are one of the largest families of cell surface proteins and play an important role in many physiological processes, including vision, olfaction, taste, hormone secretion, neurotransmission, immune response, inflammation, and cancer.  The number of known human GPCR sequences has increased dramatically over recent years due to advances in genome sequencing technologies; however, only about half have been functionally characterized so far. In this study we present a novel computational method that can be used to predict the functional class of uncharacterized GPCRs based on their sequence similarity with other members within each subfamily. Our approach is based on two key ideas: 1) We use a new adaptive strategy to select representative sequences from different subfamilies by considering both the diversity among them as well as their evolutionary relationships; 2) We develop a new scoring scheme which takes into account not only the pairwise sequence similarities but also the structural information between query and template sequences. Using our method, we were able to successfully classify more than 80% of all human GPCRs whose functions had previously been experimentally determined.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : An Adaptive Strategy for the Classification of G - Protein Coupled Receptors . Abstract : G - protein - coupled receptors ( GPCRs ) are one of the greatest families of cell surface proteins and play an important role in multiple physiological mechanisms , notably vision , olfaction , taste , hormone secretion , neurotransmission , immune reaction , inflammation , and tumors .The amount of known human GPCR sequences has increased significantly over recent months thanks to advances in genome sequencing technologies ; however , only about half have been functionally distinguished so far . In this study we present a new computational technique that can be used to predict the functional class of uncharacterized GPCRs based on their sequence similarity with other members within each subfamily .Our model is based on two fundamental ideas : 1 ) We use a new adaptive approach to select representative sequences from diverse subfamilies by examining both the complexity among them as well as their evolutionary relationships ; 2 ) We develop a new scoring scheme which assumes into consideration not only the pairwise sequence differences but also the structural information between query and template sequences . Using our technique , we were able to effectively classify more than 80 % of all human GPCRs whose functions had earlier been experimentally determined .",
        "rewrite_text": "Title: An Adaptive Strategy for the Classification of G-Protein Coupled Receptors\n\nAbstract: G-protein-coupled receptors (GPCRs) represent one of the largest and most diverse families of cell surface proteins, playing critical roles in various physiological processes such as vision, olfaction, taste, hormone release, neurotransmission, immune responses, inflammation, and tumor progression. Recent advancements in genome sequencing technologies have led to a significant increase in the number of known human GPCR sequences; however, only approximately half of these have been functionally characterized to date. In this article, we introduce a novel computational method designed to predict the functional classification of uncharacterized GPCRs by leveraging their sequence similarities with known members of their respective subfamilies. Our approach is grounded in two key principles: first, we employ an adaptive strategy to select representative sequences from diverse subfamilies, taking into account both the complexity of the sequences and their evolutionary relationships; second, we propose a new scoring system that integrates not only pairwise sequence variations but also structural information between the query and template sequences. Through the application of our method, we successfully classified over 80% of human GPCRs whose functions had previously been experimentally validated. This innovative strategy not only enhances our understanding of GPCR functionality but also provides a robust framework for the classification of uncharacterized receptors, potentially accelerating the discovery of new therapeutic targets in various disease contexts. Our findings underscore the importance of computational techniques in the functional annotation of GPCRs and highlight the potential for future research to further elucidate the roles of these critical proteins in human health and disease.",
        "ori-fast-z-score": 0.10153461651336192,
        "water-fast-z-score": 6.0609152673132645,
        "rewrite-fast-z-score": -0.1796053020267749
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Watermark Embedding and Detection .\nAbstract:\nIn this work, we propose an efficient watermarking scheme for digital images based on the discrete wavelet transform (DWT). The proposed method embeds the watermark into the high-frequency sub-bands by modifying their coefficients using a quantization index modulation technique. In order to detect the embedded watermark in the attacked image, we use a correlation detector with a threshold value that is determined adaptively according to the local statistics of each block. Experimental results show that our algorithm has good performance against various attacks such as JPEG compression, salt & pepper noise addition, Gaussian filtering, cropping, rotation, scaling, blurring, sharpening, histogram equalization, gamma correction, median filter, and mean filter. Keywords: Watermarking; Discrete Wavelet Transform; Quantization Index Modulation; Adaptive Threshold Value. Digital watermarking technology can be used to protect copyrights or prevent illegal distribution of digital content. However, it may also cause some problems if the watermarks are not detected correctly due to intentional or unintentional attacks. Therefore, robustness should always be considered when designing any watermarking system.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Watermark Embedding and Detection . Abstract : In this study , we propose an efficient watermarking scheme for digital photographs relying on the discrete wavelet transition ( DWT ) .The proposed approach embeds the watermark into the high - frequency sub - bands by modifying their parameters using a quantization index modulation method . In order to identify the embedded watermark in the captured image , we utilize a correlation detector with a threshold value that is calculated adaptively based to the local statistics of each block .Experimental findings show that our technique has good success against several attacks such as JPEG compression , salt & pepper noise addition , Gaussian filtering , cropping , rotation , scaling , blurring , sharpening , histogram equalization , alpha reduction , median filter , and mean filter . Keywords : Watermarking ; Discrete Wavelet Transform ; Quantization Index Modulation ; Adaptive Threshold Value .Digital watermarking technology can be used to shield copyrights or avoid illegal collection of digital content . However , it could also create some problems if the watermarks are not observed correctly due to deliberate or unintentional attempts .Therefore , robustness should always be regarded when designing any watermarking scheme .",
        "rewrite_text": "**Title: Watermark Embedding and Detection**\n\n**Abstract:** This research presents a novel and efficient watermarking technique tailored for digital photographs, utilizing the discrete wavelet transform (DWT) as its foundational framework. Our method strategically embeds watermarks within the high-frequency sub-bands of the image by adjusting their parameters through a quantization index modulation approach. To facilitate the detection of the embedded watermark in the processed image, we implement a correlation detection mechanism that employs an adaptive threshold value, which is dynamically calculated based on the local statistical properties of each image block. \n\nThe experimental results demonstrate the robustness of our watermarking scheme against a variety of common image processing attacks, including JPEG compression, addition of salt and pepper noise, Gaussian filtering, cropping, rotation, scaling, blurring, sharpening, histogram equalization, alpha reduction, as well as median and mean filtering. These findings underscore the effectiveness of our approach in maintaining watermark integrity under adverse conditions.\n\nDigital watermarking serves as a vital technology for protecting copyrights and preventing unauthorized use of digital content. However, it is crucial to address potential issues that may arise from improper watermark detection, whether due to intentional tampering or unintentional alterations. Consequently, ensuring robustness is a fundamental consideration in the design of any watermarking scheme. Our study contributes to the ongoing discourse in digital watermarking by providing a reliable method that balances the need for effective copyright protection with the challenges posed by various image manipulations. \n\n**Keywords:** Watermarking; Discrete Wavelet Transform; Quantization Index Modulation; Adaptive Threshold Value.",
        "ori-fast-z-score": -1.3251783128981585,
        "water-fast-z-score": 5.673665146135802,
        "rewrite-fast-z-score": 0.8466487815452375
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Nonholonomic Ricci Flows: Exact Solutions and Gravity .\nAbstract:\nWe present exact solutions to the nonholonomic Ricci flows for Einstein spaces with torsion, which are generated by nonlinear connections (N-connections) on principal fiber bundles. The N-connection coefficients depend only on time parameter t and satisfy certain evolution equations. We show that these solutions can be used in order to construct new classes of exact solutions to gravitational field equations in general relativity theory. In particular we find explicit examples of exact solutions describing black holes and wormholes. These results may have important applications in modern cosmology as well as in quantum gravity theories. Keywords: Nonholonomic Ricci flow, Gravity, Black hole, Wormhole, Cosmological model. 1 Introduction.\nThe geometric methods of constructing exact solutions to gravitational field equation in general relativity theory were developed during last decades  1, 2  . It is possible to generate such solutions using various types of symmetries like Killing vectors or tensors, homothetic vector fields etc., see e.g.  3  -  8  .\nIn this work we consider another approach based on so-called nonholonomic frame transforms  9, 10  , when one uses anholonomy coefficients of nonlinear connection (N-connection) instead of usual Christoffel symbols. This method was applied recently in order to study some problems related to modified gravity models  11  -  13  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Nonholonomic Ricci Flows : Exact Solutions and Gravity . Abstract : We present precise solutions to the nonholonomic Ricci currents for Einstein spaces with torsion , which are produced by nonlinear links ( N - connections ) on principal fiber bundles .The N - connection coefficients depend only on time variable t and obey certain evolution equations . We see that these solutions can be used in order to build new classes of precise solutions to gravitational field equations in general relativity theory .In particular we find explicit examples of precise solutions involving grey holes and wormholes . These data may have important use in modern cosmology as well as in quantum gravitational dynamics .Keywords : Nonholonomic Ricci flow , Gravity , Black hole , Wormhole , Cosmological model . 1 Introduction .The mathematical techniques of constructing exact solutions to gravitational field equation in general relativity theory were developed during last decades 1 , 2 . It is common to create such solutions use different kinds of symmetries like Killing tensor or tensors , homothetic vector fields etc . , see e . g .3 - 8 . In this study we find another methodology relying on so - called nonholonomic frame transforms 9 , 10 , when one uses anholonomy coefficients of nonlinear connection ( N - connection ) instead of customary Christoffel characters .This method was used lately in order to study some problems related to revised gravitational models 11 - 13 .",
        "rewrite_text": "**Title:** Nonholonomic Ricci Flows: Exact Solutions and Gravity\n\n**Abstract:** In this article, we explore precise solutions to nonholonomic Ricci currents associated with Einstein spaces that incorporate torsion, generated through nonlinear connections (N-connections) on principal fiber bundles. The coefficients of these N-connections are solely dependent on the time variable \\( t \\) and adhere to specific evolution equations. Our findings indicate that these solutions can be instrumental in deriving new classes of exact solutions to the gravitational field equations within the framework of general relativity. Notably, we present explicit examples of these solutions that encompass grey holes and wormholes, which may hold significant implications for contemporary cosmology and quantum gravitational dynamics. The methodologies employed in this research build upon the mathematical techniques established over recent decades for constructing exact solutions to gravitational field equations in general relativity. Traditionally, these solutions have been derived using various symmetries, such as Killing tensors and homothetic vector fields. However, our approach introduces a novel methodology that utilizes nonholonomic frame transformations, leveraging the anholonomy coefficients of N-connections in place of the conventional Christoffel symbols. This innovative method has recently been applied to address several issues related to revised gravitational models. The implications of our work extend beyond theoretical exploration, potentially influencing the understanding of complex structures in the universe, such as black holes and wormholes, and their roles in the broader context of cosmological models. \n\n**Keywords:** Nonholonomic Ricci flow, Gravity, Black hole, Wormhole, Cosmological model.",
        "ori-fast-z-score": -1.6858544608470492,
        "water-fast-z-score": 4.495611895592131,
        "rewrite-fast-z-score": 1.9414506867883017
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Propagation of external regulation and asynchronous dynamics in random Boolean networks .\nAbstract:\nWe study the propagation of external control signals through a large class of randomly constructed Boolean networks, which are known to exhibit rich dynamical behaviors including chaos.  We show that for any given initial state, there exists an optimal set of input nodes such that if their states are fixed at 1 or 0, then the resulting system will reach its steady state with minimum number of steps (or equivalently maximum speed). The problem is formulated as finding this optimal set of input nodes by solving a combinatorial optimization problem on graphs. This formulation leads us to develop efficient algorithms based on local search techniques. Our results provide insights into how to design effective control strategies for complex systems modeled by Boolean networks. In addition, we also investigate the effect of noise on the performance of these control strategies. Finally, our analysis reveals some interesting connections between the structure of the underlying graph and the effectiveness of different control strategies. A Boolean network consists of N binary-state variables xi(t) ∈ {0, 1} evolving according to a deterministic update rule specified by a directed graph G = (V , E), where V denotes the set of vertices representing the N variables, and E represents the set of edges connecting pairs of vertices. Each vertex i has K incoming neighbors denoted by Ni = {j : (i, j ) ∈ E}. At each time step t, all the variables synchronously evolve according to the following updating scheme:",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Propagation of external regulation and asynchronous dynamics in random Boolean systems . Abstract : We research the propagation of external control messages through a large class of randomly created Boolean systems , which are known to exhibit abundant dynamical interactions including chaos .We see that for any given original state , there exists an efficient setting of input nodes such that if their states are fixed at 1 or 0 , then the resulting system will attain its steady state with minimum amount of steps ( or equivalently maximum speed ) . The question is formulated as finding this optimal setting of input nodes by solving a combinatorial algorithms task on graphs .This formulation leads us to develop fast algorithms based on local search methods . Our results yield insights into how to model optimal control tactics for complex systems modeled by Boolean systems .In addition , we also investigate the impact of noise on the performance of these control tactics . Finally , our analysis reveals some interesting connections between the composition of the underlying graph and the performance of different control tactics .A Boolean network consists of N binary - state variables xi ( t ) ∈ { 0 , 1 } evolving due to a deterministic change rule defined by a directed graph G = ( V , E ) , where V denotes the group of vertices representing the N variables , and E represents the group of vertices linking pairs of vertices . Each vertex i has K incoming neighbors denoted by Ni = { j : ( i , k ) ∈ E } .At each time step t , all the variables synchronously evolve according to the following updating scheme :",
        "rewrite_text": "In this study, we investigate the transmission of external control signals within a diverse range of randomly generated Boolean systems, which are characterized by complex dynamical interactions, including chaotic behavior. Our findings indicate that for any initial state of the system, it is possible to identify an optimal configuration of input nodes. By fixing the states of these input nodes to either 1 or 0, the system can reach its steady state in the fewest possible steps, thereby maximizing the speed of convergence. We approach this problem by framing it as a combinatorial optimization task on graphs, which allows us to develop efficient algorithms based on local search techniques. \n\nOur results provide valuable insights into the formulation of effective control strategies for complex systems represented by Boolean networks. Furthermore, we explore the influence of noise on the efficacy of these control strategies, revealing how external perturbations can affect system dynamics. Our analysis also uncovers intriguing relationships between the structure of the underlying graph and the effectiveness of various control tactics. \n\nA Boolean network is composed of N binary variables, denoted as xi(t) ∈ {0, 1}, which evolve according to deterministic rules defined by a directed graph G = (V, E). In this context, V represents the set of vertices corresponding to the N variables, while E indicates the edges connecting pairs of vertices. Each vertex i is associated with K incoming neighbors, represented by Ni = {j : (i, k) ∈ E}. At each time step t, all variables update synchronously according to a specified scheme. This research contributes to a deeper understanding of optimal control in complex dynamical systems and highlights the significance of graph structure in determining control effectiveness.",
        "ori-fast-z-score": 0.0854357657716761,
        "water-fast-z-score": 6.407682432875707,
        "rewrite-fast-z-score": 1.2543630150106362
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  BL Lac Contribution to the Extragalactic Gamma-Ray Background .\nAbstract:\nWe present new results on the contribution of BL Lacs (blazars) to the extragalactic gamma-ray background based on data collected by the Fermi Large Area Telescope between August 2008 and December 2010, corresponding to an effective exposure time of 1.6 yr for each source in our sample. We use two different methods to estimate this contribution: i) we calculate the number counts above 100 MeV as function of redshift using a maximum likelihood method; ii) we fit the observed spectral energy distribution with a log-parabola model and derive the integrated fluxes at 0.1 GeV and 10 TeV energies. The resulting contributions are consistent within statistical uncertainties. Our best-fit value is F(>100 MeV) = 2.2 x 10^{−8} ph cm−2 s−1 sr−1 which corresponds to ~20% of the measured EGB intensity. This result confirms that blazars are one of the main contributors to the EGB emission.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : BL Lac Contribution to the Extragalactic Gamma - Ray Background . Abstract : We report new data on the contribution of BL Lacs ( blazars ) to the extragalactic gamma - ray background based on statistics obtained by the Fermi Large Area Telescope between August 2008 and December 2010 , equivalent to an effective exposure run of 1 . 6 yr for each source in our sample .We use two different methods to estimate this contribution : i ) we estimate the number counts above 100 MeV as function of redshift utilizing a maximum likelihood technique ; ii ) we fit the observed spectral power distribution with a log - parabola model and derive the integrated fluxes at 0 . 1 GeV and 10 TeV energies . The resulting contributions are compatible within statistical uncertainties .Our best - fitting value is F ( > 100 MeV ) = 2 . 2 x 10 ^ { −8 } ph cm−2 s−1 sr−1 which equals to ~ 20 % of the measured EGB brightness . This result confirms that blazars are one of the main contributors to the EGB emission .",
        "rewrite_text": "We present new findings regarding the role of BL Lacertae objects (BL Lacs), a type of blazar, in contributing to the extragalactic gamma-ray background (EGB). This analysis is based on data collected by the Fermi Large Area Telescope from August 2008 to December 2010, providing an effective exposure time of 1.6 years for each source included in our study. To quantify the contribution of BL Lacs to the EGB, we employed two distinct methodologies. First, we utilized a maximum likelihood technique to estimate the number counts of BL Lacs above 100 MeV as a function of redshift. Second, we analyzed the observed spectral power distribution by fitting it with a log-parabola model, which allowed us to calculate the integrated fluxes at energy levels of 0.1 GeV and 10 TeV. The contributions derived from both methods were found to be consistent within the bounds of statistical uncertainty. Our optimal fitting value for the flux above 100 MeV is F(> 100 MeV) = 2.2 x 10^{-8} ph cm^{-2} s^{-1} sr^{-1}, which corresponds to approximately 20% of the total brightness of the measured EGB. This significant finding reinforces the notion that blazars, particularly BL Lacs, are key contributors to the emission observed in the extragalactic gamma-ray background. The implications of this research enhance our understanding of the origins of gamma-ray emissions in the universe and the role of active galactic nuclei in the broader context of astrophysical phenomena.",
        "ori-fast-z-score": -0.762000762001143,
        "water-fast-z-score": 3.14970394174356,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Structural parameters for globular clusters in M31 and generalizations for the fundamental plane .\nAbstract:\nWe present new structural parameters for 23 globular clusters (GCs) in M31, derived using HST/ACS images taken with the F606W filter. We use these data to test whether GCs follow the same scaling relations as elliptical galaxies do on the so-called  fundamental plane . The results show that most of our sample follows this relation within uncertainties, but we also find some outliers which are likely due to their different formation histories or dynamical states. In addition, we compare our results with those obtained by other authors who used ground-based observations. Our analysis shows that there is no significant difference between the two datasets when they are analyzed consistently. Finally, we discuss possible reasons why previous studies have found conflicting results about the existence of such a relationship among GC systems. This work was supported by NASA grant NAG5-12140. Keywords: Globular cluster; Fundamental Plane",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Structural parameters for globular complexes in M31 and generalizations for the fundamental plane . Abstract : We report new structural values for 23 globular complexes ( GCs ) in M31 , obtained using HST / ACS images took with the F606W filter .We use these information to test whether GCs obey the same scaling relations as elliptical galaxies do on the so - called fundamental plane . The results show that most of our sample follows this relation within uncertainties , but we also find some outliers which are likely due to their different formation histories or dynamical states .In addition , we compare our findings with those acquired by other researchers who used ground - based observations . Our study shows that there is no major difference between the two datasets when they are examined consistently .Finally , we explain possible reasons why previous research have discovered differing results about the existence of such a connection among GC systems . This research was supported by NASA grant NAG5 - 12140 .Keywords: Globular cluster; Fundamental Plane",
        "rewrite_text": "Title: Structural Parameters for Globular Complexes in M31 and Generalizations for the Fundamental Plane\n\nAbstract: In this study, we present newly derived structural parameters for 23 globular complexes (GCs) located in the Andromeda Galaxy (M31), utilizing high-resolution images captured by the Hubble Space Telescope's Advanced Camera for Surveys (HST/ACS) with the F606W filter. Our primary objective is to investigate whether these globular complexes adhere to the same scaling relations as elliptical galaxies, specifically those defined by the fundamental plane. The analysis reveals that the majority of the globular complexes in our sample align with the fundamental plane within the margins of uncertainty. However, we also identify several outliers, which we hypothesize may be attributed to variations in their formation histories or current dynamical states. Furthermore, we conduct a comparative analysis with data obtained from previous studies that relied on ground-based observations. Our findings indicate that there is no significant discrepancy between the structural parameters derived from HST observations and those from ground-based datasets when analyzed under consistent methodologies. We also address potential reasons for the conflicting results reported in earlier research regarding the relationship between globular cluster systems and the fundamental plane. This investigation contributes to a deeper understanding of the structural characteristics of globular complexes in M31 and their implications for the broader context of galaxy formation and evolution. This research was conducted with the support of NASA grant NAG5-12140. \n\nKeywords: Globular cluster; Fundamental Plane.",
        "ori-fast-z-score": 0.5852057359806528,
        "water-fast-z-score": 5.347391382215687,
        "rewrite-fast-z-score": 2.51259453814803
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Towards a library of synthetic galaxy spectra and preliminary results of classification and parametrization of unresolved galaxies for Gaia .\nAbstract:\nWe present the first steps towards building a library of synthetic galaxy SEDs (spectral energy distributions) that will be used to classify and parameterize unresolved galaxies in the Gaia data stream, as part of the Data Processing and Analysis Consortium (DPAC). The library is built using state-of-the-art stellar population synthesis models with different star formation histories, metallicities, dust content, and redshifts. We use this library to test two methods of classifying unresolved galaxies into broad morphological types based on their observed photometry only. In addition we show how these parameters can be constrained by fitting the full spectrum of an unresolved galaxy. This work was performed within the framework of the ESA Gaia mission. Keywords: Galaxy evolution; Stellar populations; Spectroscopy. 1 Introduction Galaxies are complex systems whose properties depend strongly on their mass, age, chemical composition, star formation history, and environment. These physical characteristics determine many observable quantities such as luminosity, colours, morphology, kinematics, etc., which have been studied extensively over several decades. However, it has become clear recently that there exist significant degeneracies between some of these observables and therefore they cannot be uniquely determined without additional information about the underlying physics or geometry of the system. For example, the total luminosity of a galaxy depends not only on its current star formation rate but also on its past star formation activity through the integrated light of old stars. Similarly, the colour of a galaxy depends both on its metallicity and on the amount of dust extinction along our line-of-sight. Therefore, accurate measurements of all relevant physical parameters require detailed spectroscopic observations covering large wavelength ranges. Such studies are now possible thanks to new space missions like GALEX, SDSS, 2MASS, Spitzer Space Telescope, Herschel Space Observatory, Chandra X-ray Observatory, XMM-Newton, Hubble Space Telescope, and most importantly, the upcoming European Space Agency s Gaia satellite. Gaia is expected to provide astrometric positions, parallaxes, proper motions, radial velocities, and multi-colour photometry for more than one billion objects",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Towards a library of synthetic universe spectra and preliminary results of classification and parametrization of unresolved galaxies for Gaia . Abstract : We present the first steps towards constructing a library of synthetic galaxy SEDs ( spectral power distributions ) that will be used to classify and parameterize unresolved galaxies in the Gaia data stream , as member of the Data Processing and Analysis Consortium ( DPAC ) .The library is built using state - of - the - art stellar community synthesis estimates with various galaxy formation histories , metallicities , dust content , and redshifts . We use this database to test two means of classifying unresolved galaxies into wide morphological types based on their observed photometry only .In addition we show how these parameters can be constrained by fitting the full range of an unresolved galaxy . This research was done within the framework of the ESA Gaia expedition .Keywords : Galaxy evolution ; Stellar populations ; Spectroscopy . 1 Introduction Galaxies are diverse structures whose characteristics rely highly on their mass , age , chemical composition , star formation history , and environment .These physical qualities determine many observable quantities such as luminosity , colours , morphology , kinematics , etc . , which have been studied frequently over numerous years . However , it has become clear recently that there remain considerable degeneracies between some of these observables and therefore they cannot be uniquely determined without additional information about the fundamental theory or topology of the system .For instance , the total luminosity of a galaxy depends not only on its current star formation rate but also on its past star formation activity through the integrated light of ancient stars . Similarly , the colour of a galaxy depends both on its metallicity and on the extent of dust extinction along our line - of - view .Therefore , accurate measurements of all relevant physical values need comprehensive spectroscopic observations encompassing large wavelength ranges . Such investigations are now possible due to modern space missions like GALEX , SDSS , 2MASS , Spitzer Space Telescope , Herschel Space Observatory , Chandra X - ray Observatory , XMM - Newton , Hubble Space Telescope , and most importantly , the latest European Space Agency s Gaia satellite .Gaia is expected to provide astrometric orientation , parallaxes , proper motions , radial velocities , and multi - colour photometry for more than one billion objects",
        "rewrite_text": "**Title:** Towards a Library of Synthetic Universe Spectra: Preliminary Classification and Parameterization of Unresolved Galaxies for Gaia\n\n**Abstract:** This study marks the initial phase in the development of a comprehensive library of synthetic spectral energy distributions (SEDs) for galaxies, aimed at the classification and parameterization of unresolved galaxies within the Gaia data stream. As part of the Data Processing and Analysis Consortium (DPAC), we leverage advanced stellar community synthesis models that incorporate a variety of galaxy formation histories, metallicities, dust content, and redshifts. Our library serves as a foundational resource for testing classification methodologies that categorize unresolved galaxies into broad morphological types based solely on their observed photometric data. Furthermore, we demonstrate how these classifications can be refined by fitting the complete spectral range of an unresolved galaxy. This research is conducted within the context of the European Space Agency's Gaia mission, which is poised to revolutionize our understanding of galaxy evolution.\n\nGalaxies exhibit a wide range of characteristics influenced by factors such as mass, age, chemical composition, star formation history, and environmental context. These attributes significantly affect observable properties, including luminosity, color, morphology, and kinematics, which have been the subject of extensive study over the years. However, recent findings indicate that substantial degeneracies exist among these observables, complicating their unique determination without supplementary information regarding the underlying physical principles or structural topology of the galaxies. For example, a galaxy's total luminosity is influenced not only by its current star formation rate but also by its historical star formation activity, as reflected in the integrated light from older stars. Similarly, a galaxy's color is affected by both its metallicity and the degree of dust extinction encountered along the line of sight. Consequently, precise measurements of all pertinent physical parameters necessitate comprehensive spectroscopic observations across a wide range of wavelengths. The advent of modern space missions, including GALEX, SDSS, 2MASS, Spitzer, Herschel, Chandra, XMM-Newton, Hubble, and notably, the Gaia satellite, has made such investigations feasible. Gaia is anticipated to deliver astrometric data, including parallaxes, proper motions, radial velocities, and multi-color photometry for over one billion celestial objects, thereby enhancing our understanding of galaxy evolution and structure. \n\n**Keywords:** Galaxy evolution; Stellar populations; Spectroscopy.",
        "ori-fast-z-score": 1.0120486274099798,
        "water-fast-z-score": 7.340166808764486,
        "rewrite-fast-z-score": 1.4615384615384615
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Study of Catalogued Nearby Galaxy Clusters in the SDSS-DR4: I. Cluster Global Properties .\nAbstract:\nWe present an analysis of galaxy clusters selected by their red-sequence galaxies (Gladders & Yee 2005) from the Sloan Digital Sky Survey Data Release 4 (DR4). We use two different methods to select cluster candidates, and then apply photometric redshift cuts on these samples to obtain final catalogues with high purity. The first method is based on the matched filter technique developed for X-ray surveys (Postman et al 1996) , while the second one uses a friends-of-friends algorithm applied directly to the galaxy distribution. In order to test our selection algorithms we compare them against mock galaxy clusters extracted from N-body simulations. Our main results are as follows:  1. Using the matched filter technique we find that the number density of galaxy clusters at z < 0.5 is n(z<0.5) = 3.6 +/- 0.2 x 10-3 h3 Mpc-3 . This value agrees well with previous determinations using other techniques.  2. By applying the same matched filter technique to simulated galaxy clusters we show how this method can be used to estimate the mass function of galaxy clusters up to z ~1.0.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  A Study of Catalogued Nearby Galaxy Clusters in the SDSS-DR4: I.Cluster Global Properties .Abstract : We present an assessment of galaxy clusters chose by their red - sequence galaxies ( Gladders & Yee 2005 ) from the Sloan Digital Sky Survey Data Release 4 ( DR4 ) . We use two different methods to select cluster applicants , and then use photometric redshift cutting on these specimens to obtain final catalogues with high purity .The first method is based on the matched filter technique developed for X - ray observations ( Postman et al 1996 ) , while the second one uses a friends - of - friends method applied directly to the galaxy distribution . In order to test our choice algorithms we compare them against mock galaxy galaxies extracted from N - bodies simulations .Our main results are as follows : 1 . Using the matched filter technique we find that the number density of galaxy galaxies at z < 0 . 5 is n ( z < 0 . 5 ) = 3 . 6 + / - 0 . 2 x 10 - 3 h3 Mpc - 3 .This value agrees well with previous determinations using other techniques.2.By applying the same matched filter technique to modeled galaxy galaxies we find how this algorithm can be used to estimate the mass function of galaxy galaxies up to z ~ 1 . 0 .",
        "rewrite_text": "**Title:** A Study of Catalogued Nearby Galaxy Clusters in the SDSS-DR4: I. Cluster Global Properties\n\n**Abstract:** In this study, we evaluate galaxy clusters identified through their red-sequence galaxies, as outlined by Gladders & Yee (2005), utilizing data from the Sloan Digital Sky Survey Data Release 4 (DR4). Our approach involves two distinct methodologies for selecting potential cluster candidates, followed by the application of photometric redshift cuts to refine these selections into final catalogs characterized by high purity. The first selection method employs a matched filter technique originally designed for X-ray observations (Postman et al. 1996), while the second method utilizes a friends-of-friends algorithm applied directly to the spatial distribution of galaxies. To validate our selection algorithms, we conduct comparisons with mock galaxy catalogs generated from N-body simulations.\n\nOur findings reveal several key results: First, the matched filter technique yields a galaxy density of n(z < 0.5) = 3.6 ± 0.2 x 10^-3 h^3 Mpc^-3 for clusters at redshifts less than 0.5. This measurement is consistent with previous estimates obtained through alternative methods, reinforcing the reliability of our approach. Second, when applying the matched filter technique to simulated galaxy distributions, we demonstrate its efficacy in estimating the mass function of galaxy clusters up to redshift z ~ 1.0. These results contribute to our understanding of the global properties of nearby galaxy clusters and provide a foundation for further investigations into their evolution and mass distribution across cosmic time. Overall, our study highlights the importance of robust selection techniques in the analysis of galaxy clusters and sets the stage for future research in this domain.",
        "ori-fast-z-score": 1.1285761872936695,
        "water-fast-z-score": 5.103103630798288,
        "rewrite-fast-z-score": -0.9684959969581862
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dirichlet or Potts ? .\nAbstract:\nWe consider the problem of learning an unknown distribution over binary strings, where each string is generated by sampling independently at random with replacement according to some fixed probability vector . We show that if we are given samples drawn iid from , then it can be learned in polynomial time using standard statistical techniques (e.g., maximum likelihood). However, when the number of possible values for each bit grows large, this approach fails because there may not exist any sample which contains all possible values for every bit. In such cases, we propose a novel algorithm based on Gibbs sampling and prove its correctness under certain conditions. Finally, we present experimental results demonstrating our method s effectiveness. The main contribution of this work lies in showing how to learn distributions over combinatorial objects like graphs and trees. This is achieved by first reducing these problems into learning distributions over binary strings, and then applying our proposed algorithms. Learning distributions over combinatorial objects has many applications including data mining, computer vision, bioinformatics, etc. .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  Dirichlet or Potts ?.Abstract : We consider the question of learning an unknown distribution over binary strings , where each string is generated by filtering independently at random with replacement due to some fixed probability vector . We see that if we are given samples picked iid from , then it can be learned in polynomial period using conventional statistical tools ( e . g . , maximum likelihood ) .However , when the number of possible values for each bit grows large , this methodology fails because there may not exist any specimen which contains all possible values for every bit . In such cases , we undertake a novel algorithm based on Gibbs filtering and find its correctness under certain conditions .Finally , we present research results proving our technique s efficacy . The main impact of this research consists in teaching how to teach distributions over combinatorial objects like graphs and trees .This is achieved by first transforming these problems into knowledge distributions over binary strings , and then applying our proposed methods . Learning distributions over combinatorial objects has numerous uses including data extraction , computer vision , bioinformatics , etc ..",
        "rewrite_text": "Title: Dirichlet or Potts?\n\nAbstract: In this study, we investigate the challenge of learning an unknown distribution over binary strings, where each string is generated through independent random filtering with replacement, guided by a fixed probability vector. We demonstrate that when provided with independent and identically distributed (iid) samples from the distribution, conventional statistical methods, such as maximum likelihood estimation, can successfully learn the distribution in polynomial time. However, as the number of potential values for each bit increases significantly, this approach encounters limitations. Specifically, there may be instances where no sample contains all possible values for every bit, leading to a failure in learning the distribution effectively. To address this issue, we propose a novel algorithm rooted in Gibbs filtering, and we establish its correctness under specific conditions. Our research results validate the efficacy of this technique, highlighting its potential to enhance the learning of distributions over combinatorial structures such as graphs and trees. We achieve this by first reformulating these complex problems into knowledge distributions over binary strings, followed by the application of our proposed methods. The implications of this research are substantial, as learning distributions over combinatorial objects has a wide array of applications, including data extraction, computer vision, and bioinformatics, among others. Through our findings, we aim to contribute valuable insights into the methodologies for teaching distributions in these diverse fields.",
        "ori-fast-z-score": -0.2,
        "water-fast-z-score": 6.069726860280934,
        "rewrite-fast-z-score": 0.5619514869490164
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Mean Field Models of Message Throughput in Dynamic Peer-to-Peer Systems .\nAbstract:\nWe study the message throughput performance of dynamic peer-to-peer systems with unreliable links and limited bandwidth, where peers are connected to each other via overlay networks. We propose two models for such systems: (1) an analytical model based on mean field theory; and (2) a simulation-based model that is built upon ns-2 network simulator. The results show that our proposed models can accurately predict the message throughput under different settings. In addition, we find that the message throughput decreases as the number of nodes increases or the average node degree decreases. Finally, we compare the message throughputs achieved by three popular overlay routing protocols: flooding, random walk, and epidemic routing. Our findings suggest that flooding achieves higher message throughput than both random walk and epidemic routing when there exists only one source-destination pair. However, if multiple pairs exist simultaneously, then epidemic routing outperforms flooding because it allows messages to be forwarded along multiple paths at once.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Mean Field Models of Message Throughput in Dynamic Peer - to - Peer Systems . Abstract : We research the message throughput performance of dynamic peer - to - peer systems with unreliable ties and low bandwidth , where peers are connected to each other via overlay networks .We suggest two models for such systems : ( 1 ) an analytical theory based on mean field theory ; and ( 2 ) a simulation - based model that is built upon ns - 2 network simulator . The results show that our proposed models can accurately forecast the message throughput under various contexts .In addition , we find that the message throughput decreases as the number of vertices increases or the average node degree reduces . Finally , we compare the message throughputs achieved by three popular overlay routing protocols : flooding , random walk , and epidemic routing .Our findings show that flooding achieves higher message throughput than both random walk and epidemic routing when there exists only one origin - destination pair . However , if multiple pairs exist independently , then disease routing outperforms flooding because it allows messages to be forwarded along multiple paths at once .",
        "rewrite_text": "In this study, we investigate the performance of message throughput in dynamic peer-to-peer systems characterized by unreliable connections and limited bandwidth. These systems operate through overlay networks, where peers communicate with one another. To analyze this complex environment, we propose two distinct models: the first is an analytical framework grounded in mean field theory, while the second utilizes a simulation-based approach developed with the ns-2 network simulator. Our findings indicate that both models provide reliable predictions of message throughput across a variety of scenarios. \n\nA key observation from our research is that message throughput tends to decline as the number of nodes in the network increases or when the average degree of connectivity among nodes decreases. This highlights the challenges faced in maintaining efficient communication in larger, less interconnected networks. Furthermore, we conduct a comparative analysis of message throughput achieved by three widely-used overlay routing protocols: flooding, random walk, and epidemic routing. Our results reveal that flooding consistently yields superior message throughput when there is a single origin-destination pair. However, in scenarios involving multiple independent pairs, epidemic routing demonstrates a significant advantage over flooding. This is attributed to its ability to facilitate message forwarding through multiple paths simultaneously, thereby enhancing overall throughput. \n\nThese insights contribute to a deeper understanding of the dynamics of message transmission in peer-to-peer systems and underscore the importance of selecting appropriate routing protocols based on network conditions and configurations. Our research not only advances theoretical knowledge but also has practical implications for the design and optimization of peer-to-peer communication systems.",
        "ori-fast-z-score": 2.799769575772148,
        "water-fast-z-score": 6.118014998909509,
        "rewrite-fast-z-score": 3.0311581059204498
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Kinetic Sunyaev-Zel dovich Effect Due to the Electrons of Our Galaxy .\nAbstract:\nWe present an analysis of the kinetic Sunyaev-Zeldovich effect (kSZ) due to electrons in our galaxy, using data obtained with the Atacama Cosmology Telescope and the South Pole Telescope. We use two different methods for estimating the kSZ signal; one is based on cross-correlating maps of CMB temperature anisotropy at 150 GHz and 3000 GHz, while the other uses the power spectrum of the CMB temperature fluctuations at 150 GHz. The results are consistent between these two approaches within their respective uncertainties. We find that the amplitude of this signal agrees well with theoretical predictions when we assume a Navarro-Frenk-White profile for dark matter density distribution around galaxies. This measurement provides new constraints on cosmological parameters such as the Hubble constant H0 = 73 +/- 4 km s-1 Mpc-1 , the total mass density parameter Omega_m = 0.27 +/- 0.03 , and the equation-of-state w = -1.06 +/- 0.11 .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Kinetic Sunyaev - Zel dovich Effect Due to the Electrons of Our Galaxy . Abstract : We present an assessment of the kinetic Sunyaev - Zeldovich effect ( kSZ ) related to electrons in our universe , using data acquired with the Atacama Cosmology Telescope and the South Pole Telescope .We use two different methods for estimating the kSZ signal ; one is based on cross - correlating mapping of CMB heat anisotropy at 150 GHz and 3000 GHz , while the other uses the power spectrum of the CMB heat fluctuations at 150 GHz . The results are compatible between these two methodology within their different uncertainties .We see that the amplitude of this signal agrees well with theoretical expectations when we assume a Navarro - Frenk - White model for heavy material concentration distribution around galaxies . This measurement gives additional constraints on cosmological factors such as the Hubble constant H0 = 73 + / - 4 km s - 1 Mpc - 1 , the total mass density variable Omega _ m = 0 . 27 + / - 0 . 03 , and the equation - of - state w = - 1 . 06 + / - 0 . 11 .",
        "rewrite_text": "In this study, we evaluate the kinetic Sunyaev-Zeldovich (kSZ) effect attributed to the electrons present in our galaxy, utilizing observational data from the Atacama Cosmology Telescope and the South Pole Telescope. Our analysis employs two distinct methodologies to estimate the kSZ signal. The first method involves cross-correlating the Cosmic Microwave Background (CMB) temperature anisotropies at frequencies of 150 GHz and 3000 GHz. The second method focuses on analyzing the power spectrum of CMB temperature fluctuations at 150 GHz. The findings from both approaches are consistent within their respective uncertainties, reinforcing the reliability of our measurements.\n\nOur results indicate that the amplitude of the kSZ effect aligns closely with theoretical predictions, particularly when we adopt the Navarro-Frenk-White (NFW) profile to model the distribution of dark matter surrounding galaxies. This alignment not only supports the theoretical framework but also provides valuable constraints on several key cosmological parameters. Specifically, we derive estimates for the Hubble constant, finding H0 = 73 ± 4 km s⁻¹ Mpc⁻¹, and the total matter density parameter, yielding Ω_m = 0.27 ± 0.03. Additionally, our analysis suggests an equation of state parameter of w = -1.06 ± 0.11.\n\nThese results contribute to our understanding of the kSZ effect and its implications for cosmology, particularly in refining models of galaxy formation and evolution. The consistency between our observational data and theoretical expectations underscores the significance of the kSZ effect as a tool for probing the large-scale structure of the universe and the properties of dark matter. Overall, this work enhances our comprehension of the intricate relationship between cosmic microwave background radiation and the distribution of matter in the universe.",
        "ori-fast-z-score": -0.3841106397986879,
        "water-fast-z-score": 4.826004826007239,
        "rewrite-fast-z-score": 1.1141720290623112
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Augmented Sparse Reconstruction of Protein Signaling Networks .\nAbstract:\nWe propose an algorithm for reconstructing the signaling network in which each node is associated with multiple time series data, and we assume that only some nodes are observed at any given time point. The proposed method consists of two steps: (1) estimating the state vector by solving a sparse reconstruction problem; (2) inferring the edge set using the estimated states as features. We show through numerical experiments on synthetic networks that our approach can accurately recover both the structure and parameters of the underlying network even when only partial information about the system is available. Our results also suggest that the performance of the proposed method depends strongly on how well the sparsity pattern of the true signal is captured by the observation matrix. Finally, we apply this method to analyze the dynamics of protein phosphorylation in yeast cells responding to heat shock stress. In particular, we identify several key proteins involved in regulating the response process. This work was supported by NIH grant R01GM084283-01A1",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Augmented Sparse Reconstruction of Protein Signaling Networks . Abstract : We suggest an algorithm for reconstructing the signaling network in which each node is associated with various time series information , and we suppose that only some nodes are observed at any certain time point .The proposed approach consists of two stages : ( 1 ) estimating the state vector by addressing a sparse reconstruction task ; ( 2 ) inferring the edge setting using the expected states as characteristics . We see through computational experiments on synthetic systems that our approach can accurately recover both the composition and parameters of the underlying network even when only partial knowledge about the system is accessible .Our results also suggest that the performance of the suggested method depends strongly on how perfectly the sparsity pattern of the true signal is captured by the observation vector . Finally , we apply this method to analyze the dynamics of protein phosphorylation in yeast cells responding to heat shock pressure .In particular , we identify several key proteins responsible in controlling the response process . This research was supported by NIH funding R01GM084283 - 01A1",
        "rewrite_text": "In this article, we present a novel algorithm designed for the reconstruction of protein signaling networks, where each node is linked to various time series data, and it is assumed that only a subset of nodes is observable at any given time. Our methodology unfolds in two distinct phases: the first phase involves estimating the state vector through a sparse reconstruction approach, while the second phase focuses on inferring the network's edge configuration by utilizing the expected states as features. Through extensive computational experiments conducted on synthetic systems, we demonstrate that our method can effectively recover both the structure and parameters of the underlying signaling network, even when only limited information about the system is available. Our findings indicate that the efficacy of the proposed algorithm is significantly influenced by the accuracy with which the sparsity pattern of the true signal is represented in the observation vector. Furthermore, we apply this innovative technique to investigate the dynamics of protein phosphorylation in yeast cells subjected to heat shock stress. This analysis allows us to pinpoint several critical proteins that play a pivotal role in mediating the cellular response to such stressors. The research presented in this article was made possible through the support of the National Institutes of Health under grant R01GM084283-01A1.",
        "ori-fast-z-score": -0.30779350562554625,
        "water-fast-z-score": 6.123724356957946,
        "rewrite-fast-z-score": 0.19611613513818404
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Detailed study of the GRB 030329 radio afterglow deep into the non-relativistic phase .\nAbstract:\nWe present new observations at 1.4 GHz and 8.5 GHz with the Australia Telescope Compact Array (ATCA) of the radio afterglow associated to the gamma-ray burst GRB030329, which occurred on March 29th 2003. The data were taken between April 2nd and May 8th 2004, i.e., more than one year after the burst event. We find that the source is still brightening in flux density by about 0.5% per day at both frequencies. This behaviour can be explained if we assume that the fireball has not yet reached its final deceleration radius. In this case, the observed emission would come from an expanding shell of relativistically moving material surrounding the central engine. If so, our results suggest that the energy injection process responsible for the late-time flaring activity detected in other GRBs may have continued up to very large distances from the progenitor system. \n \n Keywords: Gamma-Ray Bursts",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Detailed study of the GRB 030329 radio afterglow far into the non - relativistic phase . Abstract : We report new images at 1 . 4 GHz and 8 . 5 GHz with the Australia Telescope Compact Array ( ATCA ) of the radio afterglow related to the alpha - ray burst GRB030329 , which occurred on March 29th 2003 .The data were took between April 2nd and May 8th 2004 , i . e . , more than one year after the explosion event . We see that the source is already brightening in flux concentration by about 0 . 5 % per month at both frequencies .This behaviour can be described if we suppose that the fireball has not already achieved its final deceleration radius . In this situation , the seen emission might come from an evolving shell of relativistically moving material surrounding the main motor .If so , our findings show that the power injection mechanism responsible for the late - time flaring activity detected in other GRBs might have continued up to very significant distances from the progenitor system . Keywords : Gamma - Ray Bursts",
        "rewrite_text": "Title: In-Depth Analysis of the GRB 030329 Radio Afterglow During the Non-Relativistic Phase\n\nAbstract: This study presents new radio images captured at frequencies of 1.4 GHz and 8.5 GHz using the Australia Telescope Compact Array (ATCA) of the afterglow associated with the gamma-ray burst GRB 030329, which was observed on March 29, 2003. The observations were conducted between April 2 and May 8, 2004, over a year following the initial explosion. Our analysis reveals a notable increase in flux density, approximately 0.5% per month at both observed frequencies. This trend suggests that the fireball has not yet reached its final deceleration radius, indicating that the emission we detect may originate from an evolving shell of relativistically moving material that envelops the primary source. Our results imply that the power injection mechanism responsible for the late-time flaring activity observed in other gamma-ray bursts may have persisted over considerable distances from the progenitor system. This finding enhances our understanding of the dynamics of gamma-ray burst afterglows and the processes that govern their evolution in the non-relativistic phase. The implications of this research extend to the broader context of gamma-ray burst phenomena, providing insights into the mechanisms that sustain afterglow emissions long after the initial event. \n\nKeywords: Gamma-Ray Bursts, Radio Afterglow, GRB 030329, Australia Telescope Compact Array, Non-Relativistic Phase.",
        "ori-fast-z-score": -1.4552137502179978,
        "water-fast-z-score": 5.335783750799325,
        "rewrite-fast-z-score": 1.2510864843424487
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Transition Zone in Balmer-Dominated Shocks .\nAbstract:\nWe present new observations and analysis of the Balmer-dominated shocks driven by supernova remnants (SNRs) into dense molecular clouds, which are known as  molecular cloud shocks  or  Balmer-dominated shocks . We find that these shocks have an intermediate temperature between those of typical J-type and C-type shocks. The observed emission lines show prominent P-Cygni profiles with blueshifted absorption features indicating high velocities up to 100 km s-1 . These results suggest that the transition zone is located at the interface between the shocked gas and unshocked ambient medium. In addition, we found that the widths of the Hα line profiles increase toward the center of SNR W28. This indicates that the density structure of the surrounding environment may be more complicated than previously thought. Our study suggests that Balmer-dominated molecular cloud shocks can provide important information on the physical conditions of the interstellar medium around young SNRs.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Transition Zone in Balmer - Dominated Shocks . Abstract : We report new studies and investigation of the Balmer - dominated shocks driven by supernova remnants ( SNRs ) into thick molecular clouds , which are known as molecular storm shocks or Balmer - dominated shocks .We see that these shocks have an intermediate heat between those of typical J - class and C - class shocks . The observed emission lines show marked P - Cygni profiles with blueshifted emission elements suggesting high velocities up to 100 km s - 1 .These data suggest that the shift area is situated at the interface between the excited gas and unshocked ambient medium . In addition , we learned that the widths of the Hα line profiles increase toward the center of SNR W28 .This implies that the density structure of the nearby atmosphere may be more complicated than previously thought . Our study implies that Balmer - dominated molecular dust shocks can provide important information on the physical conditions of the interstellar medium around early SNRs .",
        "rewrite_text": "We present a comprehensive investigation into Balmer-dominated shocks generated by supernova remnants (SNRs) interacting with dense molecular clouds, commonly referred to as molecular storm shocks. Our research reveals that these shocks exhibit thermal properties that lie between those of typical J-class and C-class shocks. Analysis of the emission lines indicates the presence of pronounced P-Cygni profiles, characterized by blueshifted emission features that suggest velocities reaching up to 100 km/s. This velocity shift indicates that the emission region is located at the boundary between the excited gas and the unshocked surrounding medium. Furthermore, we observe that the widths of the Hα line profiles increase as we approach the center of SNR W28, suggesting a more intricate density structure in the surrounding environment than previously understood. Our findings indicate that Balmer-dominated molecular dust shocks can serve as a valuable tool for probing the physical conditions of the interstellar medium in the vicinity of early SNRs. This research enhances our understanding of the dynamics and interactions of supernova remnants with molecular clouds, providing insights into the processes that govern the evolution of the interstellar medium. The implications of our study extend to the broader context of astrophysical phenomena, highlighting the significance of Balmer-dominated shocks in the ongoing exploration of cosmic environments shaped by explosive stellar events.",
        "ori-fast-z-score": -0.3464101615137754,
        "water-fast-z-score": 6.039910552360811,
        "rewrite-fast-z-score": 2.2662573397778742
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Constraints on gamma-ray burst and supernova progenitors through circumstellar absorption lines. (II): Post-LBV Wolf-Rayet stars .\nAbstract:\nWe present the results of our analysis of high-resolution optical spectra obtained with HST/STIS for four nearby (z<0.1), X-ray selected, Type Ib/c SNe in order to study their progenitor systems. We find that all four objects show evidence for dense CSM surrounding them at distances ranging between 0.01-0.2 pc. The presence of such material is consistent with theoretical expectations for post-low-velocity-bulge (post-LBV) Wolf-Rayet star winds. In addition we detect narrow emission features which are likely due to interaction between SN ejecta and this wind. These observations provide strong constraints on the nature of the progenitor systems: they require massive WR stars as well as binary companions capable of producing significant mass loss prior to explosion. This work was supported by NASA grant NAG5-10842. We have analyzed high resolution STIS/HST data for 4 nearby (z<0.1; Xray-selected) type Ibc supernovae in an attempt to determine the properties of their progenitor systems. All four objects exhibit dense circumstellar matter (CSM; nH>1020 cm-3 ) within 0.01-0.20 parsecs of the supernova site. Such densities are expected if these explosions occur following the ejection of a low velocity  bulge  during late stages of stellar evolution. Furthermore, we observe narrow emission features which may be associated with shock-heating of the CSM by the expanding supernova remnant. Our findings suggest that these events result from the deaths of massive Wolf Rayet stars surrounded by close binaries.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Constraints on gamma - ray burst and supernova progenitors through circumstellar absorption lines . ( II ) : Post - LBV Wolf - Rayet stars .Abstract : We present the conclusion of our analysis of high - resolution optical spectra obtained with HST / STIS for four nearby ( z < 0 . 1 ) , X - ray selected , Type Ib / c SNe in order to study their progenitor structures . We see that all four bodies exhibit indication for thick CSM circling them at distances ranging between 0 . 01 - 0 . 2 pc .The presence of such material is compatible with theoretical expectations for post - low - speed - bulge ( post - LBV ) Wolf - Rayet star winds . In addition we find narrow radiation properties which are likely due to contact between SN ejecta and this wind .These measurements give strong restrictions on the nature of the progenitor structures : they use massive WR galaxies as well as binary companions capable of producing significant mass loss prior to explosion . This research was supported by NASA grant NAG5 - 10842 .We have analyzed high resolution STIS / HST results for 4 nearby ( z < 0 . 1 ; Xray - selected ) type Ibc supernovae in an trying to estimate the properties of their progenitor structures . All four bodies exhibit thick circumstellar matter ( CSM ; nH > 1020 cm - 3 ) within 0 . 01 - 0 . 20 parsecs of the supernova site .Such densities are expected if these fires occur following the ejection of a small velocity bulge during late stages of stars evolution . Furthermore , we study narrow radiation properties which may be involved with shock - heating of the CSM by the evolving supernova remnant .Our findings show that these events result from the deaths of large Wolf Rayet stars surrounded by tight binaries .",
        "rewrite_text": "In this study, we present the results of our analysis of high-resolution optical spectra obtained from the Hubble Space Telescope's Space Telescope Imaging Spectrograph (HST/STIS) for four nearby (z < 0.1) X-ray selected Type Ib/c supernovae (SNe). Our objective was to investigate the progenitor structures of these supernovae. Our findings reveal that all four supernovae exhibit signs of substantial circumstellar matter (CSM) surrounding them, located at distances ranging from 0.01 to 0.2 parsecs. The presence of this dense material aligns with theoretical predictions regarding the winds of post-low-speed-bulge (post-LBV) Wolf-Rayet stars. Additionally, we observe narrow emission features in the spectra, which likely result from the interaction between the supernova ejecta and the surrounding wind. These observations impose significant constraints on the nature of the progenitor systems, suggesting that they involve massive Wolf-Rayet stars and binary companions that are capable of substantial mass loss prior to the explosion. Our research is supported by NASA grant NAG5-10842. The analysis of the high-resolution STIS/HST data for these four nearby Type Ibc supernovae has allowed us to estimate the characteristics of their progenitor structures. The detected circumstellar matter exhibits densities greater than 10^20 cm^-3, which is consistent with expectations for events occurring after the ejection of a low-velocity bulge during the late evolutionary stages of massive stars. Furthermore, we investigate the narrow emission features that may be associated with the shock heating of the CSM by the expanding supernova remnant. Our results indicate that these supernovae are the result of the deaths of massive Wolf-Rayet stars, which are likely to be in close binary systems.",
        "ori-fast-z-score": -0.08944271909999159,
        "water-fast-z-score": 6.948792289723034,
        "rewrite-fast-z-score": 0.2822162605150792
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Secondary B-mode polarization from Faraday rotation in clusters and galaxies .\nAbstract:\nWe present the first detection of secondary CMB polarization induced by Faraday rotation (FR) in galaxy clusters, using data taken with the Atacama Cosmology Telescope Polarimeter (ACTPol). We detect FR-induced polarized emission at angular scales corresponding to multipoles = 100-1000 for two galaxy clusters: ACT-CL J0102-4915 and ACT-CL J0546-5345. The observed signal is consistent with theoretical predictions based on numerical simulations of magnetized cluster atmospheres. This measurement provides an important test of our understanding of magnetic fields in galaxy clusters as well as their impact on cosmological observables such as the CMB temperature anisotropies and E-mode polarizations. In addition, we report upper limits on the FR-induced polarized emissions from other galaxy clusters that are not detected individually due to low S/N ratio or limited survey area. These results will be useful for future studies of FR effects on the CMB polarization maps produced by upcoming experiments like Planck and Simons Observatory.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Secondary B - mode polarization from Faraday rotation in clusters and clusters . Abstract : We report the first measurement of secondary CMB polarization induced by Faraday rotation ( FR ) in galaxy clusters , using data taken with the Atacama Cosmology Telescope Polarimeter ( ACTPol ) .We detect FR - caused polarized emission at angular scales corresponding to multipoles = 100 - 1000 for two galaxy clusters : ACT - CL J0102 - 4915 and ACT - CL J0546 - 5345 . The observed light is compatible with theoretical estimates based on numerical simulations of magnetized cluster atmospheres .This measurement represents an important test of our knowing of magnetic fields in galaxy clusters as well as their impact on cosmological observables such as the CMB heat anisotropies and E - mode polarizations . In addition , we note upper limits on the FR - caused polarized impacts from other galaxy galaxies that are not observed individually due to low S / N proportion or restricted observation space .These conclusions will be valuable for future research of FR effects on the CMB polarization mapping created by future research like Planck and Simons Observatory .",
        "rewrite_text": "Title: Secondary B-mode Polarization from Faraday Rotation in Galaxy Clusters\n\nAbstract: In this study, we present the inaugural measurement of secondary polarization of the Cosmic Microwave Background (CMB) induced by Faraday rotation (FR) within galaxy clusters, utilizing data collected by the Atacama Cosmology Telescope Polarimeter (ACTPol). Our findings reveal FR-induced polarized emission at angular scales corresponding to multipoles ranging from 100 to 1000, specifically for two galaxy clusters: ACT-CL J0102-4915 and ACT-CL J0546-5345. The characteristics of the detected light align well with theoretical predictions derived from numerical simulations of magnetized cluster atmospheres. This measurement serves as a crucial validation of our understanding of magnetic fields present in galaxy clusters and their influence on cosmological phenomena, including CMB temperature anisotropies and E-mode polarizations. Furthermore, we establish upper limits on the polarized effects caused by FR in other galaxy clusters that could not be individually analyzed due to low signal-to-noise ratios or limited observational coverage. These insights are expected to significantly contribute to ongoing and future investigations into the effects of Faraday rotation on CMB polarization, particularly in light of upcoming observational efforts by projects such as the Planck satellite and the Simons Observatory. Our results not only enhance the comprehension of magnetic field dynamics in galaxy clusters but also pave the way for improved interpretations of cosmological data related to the CMB.",
        "ori-fast-z-score": -0.7977240352174656,
        "water-fast-z-score": 4.98201895023622,
        "rewrite-fast-z-score": -0.10369516947304253
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Improved constraints on dark energy from Chandra X-ray observations of the largest relaxed galaxy clusters .\nAbstract:\nWe present new measurements of the Hubble constant and the equation-of-state parameter w0 using Chandra X-ray Observatory data for the most massive, dynamically relaxed galaxy clusters in the Universe. We use these results to place improved limits on the properties of dark energy. The sample consists of eight galaxy clusters with redshifts between 0.3 and 1.2 that were observed by Chandra as part of our ongoing program to study the evolution of cluster scaling relations out to high redshift. Using hydrostatic equilibrium models we measure the gas mass fraction within r500 (the radius at which the mean density is 500 times the critical density) for each system. These values are combined with independent estimates of the total gravitating mass obtained through weak lensing analysis performed by other groups. This yields an average value of H0 = 70 +/- 6 km s-1 Mpc-1 assuming flat priors on both parameters. If instead we assume Gaussian priors based on previous determinations of the Hubble constant and baryon content of the universe then this measurement becomes H0 = 68 +/-6 km s-1 Mpc-",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Improved limitations on dark energy from Chandra X - ray observations of the greatest relaxed galaxy regions . Abstract : We report new studies of the Hubble constant and the equation - of - state variable w0 using Chandra X - ray Observatory data for the most large , dynamically confined star clusters in the Universe .We use these results to place improved restrictions on the properties of dark energy . The sample consists of eight galaxy galaxies with redshifts between 0 . 3 and 1 . 2 that were detected by Chandra as part of our ongoing program to study the evolution of cluster scaling relations out to large redshift .Using hydrostatic equilibrium models we measure the gas mass fraction within r500 ( the radius at which the mean density is 500 times the critical density ) for each system . These values are coupled with independent estimates of the total gravitating mass obtained through soft lensing research performed by other organizations .This yields an estimated value of H0 = 70 + / - 6 kilometers s - 1 Mpc - 1 taking flat priors on both variables . If instead we expect Gaussian priors based on previous determinations of the Hubble constant and baryon concentration of the universe then this measurement becomes H0 = 68 + / - 6 kilometers s - 1 Mpc -",
        "rewrite_text": "We present new findings on the Hubble constant and the equation-of-state parameter \\( w_0 \\) derived from Chandra X-ray Observatory observations of some of the largest and most dynamically stable galaxy clusters in the universe. Our analysis focuses on a sample of eight galaxy clusters with redshifts ranging from 0.3 to 1.2, which were identified as part of our ongoing research aimed at understanding the evolution of cluster scaling relations at significant redshifts. By employing hydrostatic equilibrium models, we calculated the gas mass fraction within \\( r_{500} \\) (the radius where the mean density is 500 times the critical density) for each cluster in our sample. These gas mass fraction measurements were then combined with independent estimates of the total gravitating mass obtained through weak lensing studies conducted by other research teams. \n\nOur findings yield an estimated value for the Hubble constant of \\( H_0 = 70 \\pm 6 \\) km/s/Mpc, assuming flat priors for both parameters. When we apply Gaussian priors based on previous measurements of the Hubble constant and the baryon density of the universe, our estimate adjusts to \\( H_0 = 68 \\pm 6 \\) km/s/Mpc. These results provide enhanced constraints on the properties of dark energy, contributing to our understanding of its role in the expansion of the universe. The implications of our study are significant, as they refine the parameters that govern cosmological models and enhance our comprehension of the underlying physics driving cosmic acceleration. This research underscores the importance of utilizing advanced observational techniques to probe the fundamental characteristics of dark energy and its impact on the large-scale structure of the universe.",
        "ori-fast-z-score": 1.2510864843424487,
        "water-fast-z-score": 6.6010959760562775,
        "rewrite-fast-z-score": 0.5432144762551112
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Driving Mechanism of Jets and Outflows in Star Formation Process .\nAbstract:\nThe driving mechanism for jets and outflows is still an open question, especially when the jet/outflow source has no clear central engine such as black holes or protostars. In this work we propose that magnetic reconnection can be responsible for launching jets and outflows in star formation process. We show that magnetic reconnection can accelerate particles to relativistic energies efficiently via Fermi acceleration at shocks driven by the reconnecting current sheet (RCS). The accelerated electrons will produce synchrotron emission which may explain radio observations of jets and outflows. Furthermore, the energetic protons produced during RCS also contribute to nonthermal emissions through inverse Compton scattering with background photons. Finally, we discuss how our model could account for some observational features of jets and outflows. \n \n Keywords: Magnetic reconnection; Jet; Particle acceleration; Shocks; Synchrotron radiation; Nonthermal emission",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Driving Mechanism of Jets and Outflows in Star Formation Process . Abstract : The driving system for rockets and outflows is already an open matter , particularly when the jet / outflow source has no clear central fuel such as black holes or protostars .In this research we propose that magnetic reconnection can be responsible for launching jets and outflows in star formation system . We suggest that magnetic reconnection can accelerate particles to relativistic energies efficiently via Fermi acceleration at shocks driven by the reconnecting current sheet ( RCS ) .The enhanced electrons will generate synchrotron emission which would cause radio observations of jets and outflows . Furthermore , the energetic protons created during RCS also contribute to nonthermal emissions through inverse Compton absorption with background photons .Finally , we explain how our model could account for some observational characteristics of jets and outflows . Keywords : Magnetic reconnection ; Jet ; Particle acceleration ; Shocks ; Synchrotron emission ; Nonthermal emission",
        "rewrite_text": "Title: Driving Mechanism of Jets and Outflows in the Star Formation Process\n\nAbstract: The mechanisms underlying the generation of jets and outflows in star formation remain a topic of significant interest, particularly in cases where the sources lack a distinct central energy source, such as black holes or protostars. In this study, we propose a novel hypothesis that magnetic reconnection plays a crucial role in the initiation of jets and outflows within star-forming regions. We argue that magnetic reconnection can effectively accelerate particles to relativistic speeds through a process known as Fermi acceleration, which occurs at shocks produced by the reconnecting current sheet (RCS). This acceleration leads to the production of high-energy electrons that emit synchrotron radiation, which can be detected in radio observations of jets and outflows. Additionally, the energetic protons generated during the reconnection process contribute to nonthermal emissions through inverse Compton scattering with ambient photons. Our model not only provides a comprehensive framework for understanding the dynamics of jets and outflows but also aligns with various observational characteristics noted in astrophysical studies. By elucidating the role of magnetic reconnection in particle acceleration and emission processes, we aim to enhance the understanding of the complex interplay between magnetic fields and outflow dynamics in the context of star formation. This research opens new avenues for exploring the fundamental processes that govern the behavior of jets and outflows, offering insights that could refine existing models and inform future observational campaigns. \n\nKeywords: Magnetic reconnection; Jet; Particle acceleration; Shocks; Synchrotron emission; Nonthermal emission.",
        "ori-fast-z-score": -1.212678125181665,
        "water-fast-z-score": 3.8247315498700596,
        "rewrite-fast-z-score": 2.3312620206007844
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  How do trehalose, maltose and sucrose influence some structural and dynamical properties of lysozyme ? An insight from Molecular Dynamics simulations .\nAbstract:\nThe effect of three different sugars (trehalose, maltase and sucrose) on the structure and dynamics of lysozyme has been investigated by molecular dynamics simulation at 300 K for 100 ns in each case. The results show that all these sugar molecules can stabilize the protein against thermal denaturation to certain extent but trehalose is found to be most effective one among them. Trehalose molecule forms hydrogen bonds with both polar and nonpolar amino acid residues which leads to increase in number of water molecules around it. This increases the hydration shell thickness as well as total solvent accessible surface area of the protein. It also decreases the root mean square deviation between initial and final structures indicating its ability to maintain the native conformation of the protein. Maltose and sucrose are less efficient than trehalose because they form only few hydrogen bonds with the protein. In addition, their presence causes slight decrease in the radius of gyration and end-to-end distance of the protein.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : How do trehalose , maltose and sucrose influence some structural and dynamical properties of lysozyme ? An insight from Molecular Dynamics simulations .Abstract : The impact of three different sugars ( trehalose , maltase and sucrose ) on the composition and dynamics of lysozyme has been investigated by molecular dynamics simulation at 300 K for 100 ns in each case . The results show that all these sugar molecules can stabilize the protein against thermal denaturation to certain degree but trehalose is found to be most efficient one among them .Trehalose atom forms hydrogen bonds with both polar and nonpolar amino acid acids which results to increase in quantity of water molecules around it . This changes the hydration shell thickness as well as gross solvent available surface space of the protein .It additionally decreases the root average square deviation between initial and final structures signaling its able to keep the native conformation of the protein . Maltose and sucrose are less effective than trehalose because they create only few hydrogen bonds with the protein .In addition , their presence causes mild decrease in the radius of gyration and end - to - end distance of the protein .",
        "rewrite_text": "Title: The Influence of Trehalose, Maltose, and Sucrose on the Structural and Dynamical Properties of Lysozyme: Insights from Molecular Dynamics Simulations\n\nAbstract: This study explores the effects of three distinct sugars—trehalose, maltose, and sucrose—on the structural and dynamical characteristics of lysozyme through molecular dynamics simulations conducted at 300 K over a duration of 100 ns for each sugar type. The findings reveal that all three sugars contribute to the stabilization of lysozyme against thermal denaturation to varying extents, with trehalose demonstrating the highest efficacy. The molecular interactions of trehalose are characterized by the formation of hydrogen bonds with both polar and nonpolar amino acids, which leads to an increase in the number of water molecules surrounding the protein. This alteration results in a modification of the hydration shell thickness and the overall solvent-accessible surface area of lysozyme. Furthermore, the presence of trehalose significantly reduces the root mean square deviation (RMSD) between the initial and final protein structures, indicating its capacity to maintain the native conformation of lysozyme effectively. In contrast, maltose and sucrose exhibit a lesser degree of effectiveness, primarily due to their limited ability to form hydrogen bonds with the protein. Additionally, the introduction of these sugars leads to a slight reduction in both the radius of gyration and the end-to-end distance of lysozyme, suggesting a more compact protein structure. Overall, this research highlights the critical role of trehalose in enhancing the stability of lysozyme, while also providing insights into the comparative effects of maltose and sucrose on the protein's structural dynamics.",
        "ori-fast-z-score": -0.5488212999484517,
        "water-fast-z-score": 4.719863179556685,
        "rewrite-fast-z-score": 0.6
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Revisiting the Issues On Netflow Sample and Export Performance .\nAbstract:\nIn this work, we revisit some issues on netflow sample and export performance in terms of packet loss rate (PLR) and flow sampling accuracy (FSA). We first present an analytical model to study PLR under different traffic conditions with various sampling rates. Then, based on our analysis results, we propose a new method for estimating FSA by using only one parameter -the average number of flows per second. Finally, through extensive experiments conducted over both real-world traces and synthetic data sets, we show that our proposed estimation method is accurate enough to be used as a practical tool for evaluating network monitoring systems. The rest of this paper is organized as follows. In Section 2, we introduce related works about netflow sampling and exporting. In Section 3, we describe our experimental environment. In Section 4, we analyze the relationship between PLR and sampling rate. In Section 5, we estimate FSA according to the analysis result presented in Section 4. In Section 6, we evaluate the effectiveness of our proposed estimation method via extensive experiments. Finally, conclusions are drawn in Section 7.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Revisiting the Issues On Netflow Sample and Export Performance . Abstract : In this project , we revisit some issues on netflow sample and export quality in terms of packet loss rate ( PLR ) and flow sampling accuracy ( FSA ) .We first offer an analytical method to study PLR under various traffic conditions with various sampling rates . Then , using on our analysis results , we propose a new method for estimating FSA by using only one parameter - the average number of flows per second .Finally , through numerous experiments conducted over both real - time traces and synthetic information sets , we prove that our proposed estimation method is accurate sufficient to be used as a practical tool for evaluating network monitoring systems . The rest of this paper is organized as follows .In Section 2 , we provide similar articles about netflow filtering and exporting . In Section 3 , we explain our research surroundings .In Section 4 , we assess the relationship between PLR and sampling rate . In Section 5 , we estimate FSA according to the evaluation result provided in Section 4 .In Section 6 , we assess the performance of our proposed estimation method via extensive experiments . Finally , findings are drawn in Section 7 .",
        "rewrite_text": "**Title:** Revisiting the Issues on Netflow Sample and Export Performance\n\n**Abstract:** This study revisits critical concerns regarding the quality of netflow sampling and export, specifically focusing on packet loss rate (PLR) and flow sampling accuracy (FSA). We begin by introducing an analytical framework designed to evaluate PLR across a spectrum of traffic scenarios and sampling rates. Building on the insights gained from our analysis, we propose a novel approach for estimating FSA that relies solely on a single parameter: the average number of flows processed per second. To validate our proposed estimation method, we conducted a series of experiments utilizing both real-time traffic data and synthetic datasets. The results demonstrate that our method provides a sufficiently accurate estimation of FSA, making it a viable tool for assessing the performance of network monitoring systems.\n\nThe structure of the paper is organized as follows: Section 2 reviews existing literature related to netflow filtering and exporting techniques. In Section 3, we outline the research environment and methodologies employed in our study. Section 4 delves into the correlation between PLR and sampling rates, providing a comprehensive analysis of their interdependence. Following this, Section 5 presents our estimation of FSA based on the findings discussed in the previous section. In Section 6, we evaluate the effectiveness of our proposed estimation method through a series of rigorous experiments. Finally, Section 7 summarizes our key findings and discusses their implications for future research in network monitoring and performance evaluation.",
        "ori-fast-z-score": 1.3199500146737049,
        "water-fast-z-score": 6.262945776223707,
        "rewrite-fast-z-score": 0.5570860145311556
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Inter-network regions of the Sun at millimetre wavelengths .\nAbstract:\nWe present new observations made with the Atacama Large Millimeter/submillimeter Array (ALMA) of two inter-network sunspots in active region NOAA AR 12192 on 2013 May 24 and 25, respectively. The first sunspot was observed for about 3 hours during which time it rotated by more than 90 degrees. We find that this sunspot is composed of several magnetic flux tubes with different orientations. In addition to these features we also observe an extended bright feature located between the main sunspot umbrae. This feature has been previously reported as a penumbral filament but our data show no evidence of such structure. Instead, we interpret this feature as a coronal rain blob. The second sunspot was observed for only 1 hour before being occulted by Earths atmosphere. During this observation period the sunspot rotated by less than 30 degrees. Our analysis shows that both sunspots are surrounded by a dark lane which may be associated with the moat surrounding large sunspots.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Inter - network regions of the Sun at millimetre wavelengths . Abstract : We report new images making with the Atacama Large Millimeter / submillimeter Array ( ALMA ) of two inter - network sunspots in active region NOAA AR 12192 on 2013 May 24 and 25 , respectively .The first sunspot was seen for about 3 hours during which period it rotated by more than 90 degrees . We see that this sunspot is composed of several magnetic flux tubes with various orientations .In addition to these characteristics we also observe an extended bright structure located between the main sunspot umbrae . This feature has been previously reported as a penumbral filament but our statistics indicate no evidence of such structure .Instead , we view this phenomenon as a coronal weather blob . The second sunspot was seen for only 1 hour before being occulted by Earths atmosphere .During this measurement period the sunspot moved by less than 30 degrees . Our study shows that both sunspots are surrounded by a darkness lane which may be involved with the moat surrounding large sunspots .",
        "rewrite_text": "We present new observational data obtained from the Atacama Large Millimeter/submillimeter Array (ALMA) focusing on two inter-network sunspots located in active region NOAA AR 12192, observed on May 24 and 25, 2013. The first sunspot was monitored for approximately three hours, during which it exhibited a rotation exceeding 90 degrees. Our analysis reveals that this sunspot is comprised of multiple magnetic flux tubes, each exhibiting distinct orientations. Notably, we also detected an extended bright structure situated between the primary sunspot umbrae. While this feature has been previously classified as a penumbral filament, our statistical analysis does not support this classification; instead, we interpret it as a coronal weather blob, suggesting a different underlying physical mechanism. The second sunspot was observed for a shorter duration of one hour before being obscured by Earth's atmosphere, during which it rotated less than 30 degrees. Our findings indicate that both sunspots are encircled by a dark lane, which may play a role in the moat phenomenon typically associated with larger sunspots. This study enhances our understanding of the complex magnetic structures and dynamics present in inter-network regions of the Sun, particularly at millimeter wavelengths, and provides insights into the behavior of sunspots and their surrounding environments. The implications of these observations are significant for solar physics, as they contribute to the broader understanding of solar activity and its impact on space weather.",
        "ori-fast-z-score": -1.212678125181665,
        "water-fast-z-score": 4.063777271736939,
        "rewrite-fast-z-score": 0.4
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Recovering galaxy star formation and metallicity histories from spectra using VESPA .\nAbstract:\nWe present the results obtained by applying the semi-analytic code VESPA to model the evolution of galaxies in the Millennium Simulation, including chemical enrichment as well as dust extinction effects on their observed properties.  We show that our models reproduce many observational trends for different types of galaxies at z=0 (e.g., luminosity functions), but also predict some new ones which can be tested with future surveys such as Euclid or LSST. In particular we find that:  - The predicted number density of massive quiescent galaxies is too high compared to observations; this problem could be alleviated if AGN feedback were more efficient than assumed here. - Our predictions are consistent with current estimates of the cosmic SFRD out to redshifts of about 4.5, although they tend to overpredict it slightly beyond that redshift. - At low masses (Mstar < 10^10 Msun) there appears to be an excess of blue galaxies relative to red galaxies in both the real Universe and in our simulations. This may indicate that either our treatment of supernova feedback and/or reionization physics needs improvement, or else that these processes have been affected by baryonic effects not included in our simulation.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Recovering galaxy star formation and metallicity histories from spectra using VESPA . Abstract : We present the results derived by using the semi - analytic coding VESPA to model the evolution of stars in the Millennium Simulation , covering chemical enrichment as well as dust extinction effects on their observed properties .We see that our models reproduce many observational changes for different kinds of stars at z = 0 ( e . g . , luminosity functions ) , but also predict some additional ones which can be evaluated with current surveys such as Euclid or LSST . In particular we find that : - The predicted number density of large quiescent galaxies is too high compared to observations ; this challenge may be alleviated if AGN feedback were more efficient than implied here .- Our predictions are compatible with current estimates of the cosmic SFRD out to redshifts of about 4 . 5 , although they tend to overpredict it somewhat beyond that redshift . - At small masses ( Mstar < 10 ^ 10 Msun ) there seems to be an accumulation of blue galaxies compared to blue galaxies in both the real Universe and in our simulations .This might suggest that either our treatment of supernova feedback and / or reionization theory requires improvement , or otherwise that these mechanisms have been affected by baryonic effects not involved in our modeling .",
        "rewrite_text": "We present findings from our study utilizing the semi-analytic code VESPA to investigate the evolution of stellar populations within the framework of the Millennium Simulation. Our analysis encompasses the effects of chemical enrichment and dust extinction on the observable characteristics of galaxies. Our models demonstrate a strong correlation with various observational metrics for different stellar types at redshift z = 0, such as luminosity functions. However, they also predict additional trends that can be tested with ongoing surveys like Euclid and LSST. Notably, we observe that the predicted number density of large quiescent galaxies exceeds observational data, suggesting that the efficiency of AGN feedback may need to be reassessed. Furthermore, our predictions align with current estimates of the cosmic Star Formation Rate Density (SFRD) up to redshifts of approximately 4.5, although they tend to overestimate this rate at higher redshifts. Additionally, we identify an unexpected accumulation of blue galaxies at lower mass scales (Mstar < 10^10 Msun) when compared to both the actual universe and our simulations. This discrepancy indicates a potential need for refinement in our approaches to supernova feedback and reionization theory, or it may imply that baryonic processes not accounted for in our modeling are influencing these outcomes. Overall, our results highlight the complexities of galaxy formation and evolution, emphasizing the necessity for further investigation into the underlying physical mechanisms at play.",
        "ori-fast-z-score": -1.4439897447623107,
        "water-fast-z-score": 5.157106231293967,
        "rewrite-fast-z-score": 0.9332565252573828
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Inter comparison of the magneto transport of La2/3Ca1/3MnO3: Ag/In polycrystalline composites .\nAbstract:\nThe effect of silver and indium on the magnetic properties, electrical resistivity (ER) and Hall coefficient (R H ) has been investigated in La 2/3 Ca 1/3 Mn O 3 . The results show that ER decreases with increasing temperature for all samples while R H increases with decreasing temperature. It is found that both silver and indium doping decrease T C , increase J c and enhance pinning force density F p . Silver doped sample shows higher values of J c than indium doped one at low temperatures but lower value at high temperatures.  These results are explained by considering different effects of silver and indium ions on the microstructure as well as their influence on oxygen vacancies concentration. This work was supported by the National Natural Science Foundation of China under Grant No. 50571040. We would like to thank Prof. Y. M. Wu for his help during this research. Abstract: In this study we have prepared two series of La 2/3 Ca 1/3 MnO 3 :Ag and La 2/3 Ca 1/3 MnO3 :In polycrystalline composite materials using solid state reaction method. X-ray powder diffraction patterns confirm single phase formation without any impurity peaks. The structural parameters such as lattice constant, unit cell volume and bond length were calculated from XRD data. The dc magnetization measurements reveal that Curie temperature (Tc), critical current density (Jc) and pinning force density (Fp) decrease with increasing amount of silver or indium content.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Inter comparison of the magneto transport of La2 / 3Ca1 / 3MnO3 : Ag / In polycrystalline composites . Abstract : The impact of silver and indium on the magnetic properties , optical resistivity ( ER ) and Hall coefficient ( R H ) has been investigated in La 2 / 3 Ca 1 / 3 Mn O 3 .The results show that ER decreases with increasing temperature for all specimens while R H increases with varying temperature . It is found that both silver and indium doping decrease T C , enhance J c and enhance pinning power concentration F p .Silver doped specimen shows higher values of J c than indium doped one at low temperatures but smaller value at high temperatures . These data are explained by using separate consequences of silver and indium ions on the microstructure as also as their influence on oxygen vacancies density .This project was supported by the National Natural Science Foundation of China under Grant No . 50571040 .We would like to thank Prof . Y . M . Wu for his help during this research . Abstract : In this study we have formulated two series of La 2 / 3 Ca 1 / 3 MnO 3 : Ag and La 2 / 3 Ca 1 / 3 MnO3 : In polycrystalline composite materials utilizing solid state process method .X - ray powder diffraction patterns indicate single phase composition without any impurity peaks . The structural values such as structure constant , unit cell size and bond length were calculated from XRD information .The dc magnetization measurements reveal that Curie temperature ( Tc ) , vital current density ( Jc ) and pinning power coefficient ( Fp ) decline with varying amount of silver or indium content .",
        "rewrite_text": "**Title:** Intercomparison of the Magnetotransport Properties of La2/3Ca1/3MnO3: Ag/In Polycrystalline Composites\n\n**Abstract:** This study investigates the effects of silver (Ag) and indium (In) doping on the magnetic properties, optical resistivity (ER), and Hall coefficient (RH) of La2/3Ca1/3MnO3. Through a systematic examination of polycrystalline composites synthesized via solid-state processing, we observed that ER consistently decreases with increasing temperature across all samples, while RH exhibits an increasing trend with temperature variations. Notably, both Ag and In doping lead to a reduction in the Curie temperature (TC), an enhancement in critical current density (JC), and an increase in the pinning power concentration (Fp). The Ag-doped samples demonstrated superior JC values compared to the In-doped counterparts at lower temperatures; however, this trend reversed at elevated temperatures, where In-doped samples exhibited higher JC values. These findings are attributed to the distinct effects of Ag and In ions on the microstructural characteristics of the composites, as well as their influence on the density of oxygen vacancies. X-ray powder diffraction analysis confirmed the formation of a single-phase structure without any impurity peaks, allowing for the calculation of structural parameters such as lattice constants, unit cell dimensions, and bond lengths. Additionally, direct current (dc) magnetization measurements indicated that both TC, JC, and Fp decrease with increasing concentrations of either Ag or In. This research was supported by the National Natural Science Foundation of China (Grant No. 50571040), and we extend our gratitude to Prof. Y. M. Wu for his invaluable assistance throughout this project. The results contribute to a deeper understanding of the magnetotransport properties in doped manganite systems, highlighting the potential for tailoring these materials for various applications in spintronics and magnetic devices.",
        "ori-fast-z-score": 0.4833682445228318,
        "water-fast-z-score": 6.3508529610858835,
        "rewrite-fast-z-score": -0.09667364890456635
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Wide Field Surveys and Astronomical Discovery Space .\nAbstract:\nThe discovery space for astronomical research is vast, with many different types of surveys being conducted at all wavelengths across the electromagnetic spectrum. In this talk I will discuss how wide field optical imaging surveys have been used to discover new classes of objects in our Universe such as quasars, galaxies, clusters of galaxies, supernovae, gamma ray bursts etc., and also how these surveys are now providing data on dark energy which drives cosmic acceleration. The next generation of large area surveys (such as LSST) will provide an even greater volume of data that can be exploited by researchers worldwide. This talk will give examples of some recent results obtained using data from current and past surveys including the Sloan Digital Sky Survey (SDSS), Panoramic Survey Telescope & Rapid Response System 1 (Pan-STARRS1), Dark Energy Survey (DES), VISTA Kilo-Degree Infrared Galaxy survey (VIKING).",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Wide Field Surveys and Astronomical Discovery Space . Abstract : The discovery area for astronomical research is vast , with many various types of surveys being performed at all wavelengths across the electromagnetic spectrum .In this talk I will explore how wide field optical optical searches have been used to find new classes of bodies in our Universe such as quasars , galaxies , clusters of stars , supernovae , alpha ray bursts etc . , and also how these observations are now offering data on dark energy which drives cosmic acceleration . The future generation of large area surveys ( such as LSST ) will provide an much larger volume of statistics that can be exploited by researchers worldwide .This discussion will provide examples of some latest findings obtained using data from recent and previous surveys including the Sloan Digital Sky Survey ( SDSS ) , Panoramic Survey Telescope & Rapid Response System 1 ( Pan - STARRS1 ) , Dark Energy Survey ( DES ) , VISTA Kilo - Degree Infrared Galaxy survey ( VIKING ) .",
        "rewrite_text": "**Title:** Wide Field Surveys and Astronomical Discovery Space\n\n**Abstract:** The realm of astronomical research encompasses an extensive discovery area, characterized by a multitude of surveys conducted across the entire electromagnetic spectrum. This presentation delves into the significant role of wide-field optical surveys in uncovering new classes of celestial objects, including quasars, galaxies, star clusters, supernovae, and gamma-ray bursts. These surveys have not only expanded our understanding of the universe but have also provided critical data regarding dark energy, the mysterious force responsible for the accelerated expansion of the cosmos. As we look to the future, the advent of large-scale surveys, such as the Legacy Survey of Space and Time (LSST), promises to yield an unprecedented volume of statistical data that researchers around the globe can utilize. This discussion will highlight recent discoveries made possible by data from both contemporary and historical surveys, including the Sloan Digital Sky Survey (SDSS), the Panoramic Survey Telescope and Rapid Response System 1 (Pan-STARRS1), the Dark Energy Survey (DES), and the VISTA Kilo-Degree Infrared Galaxy Survey (VIKING). By examining these findings, we can appreciate the transformative impact of wide-field surveys on our understanding of the universe and the potential they hold for future astronomical discoveries.",
        "ori-fast-z-score": -2.092457497388747,
        "water-fast-z-score": 3.9000674757995495,
        "rewrite-fast-z-score": 0.31799936400190804
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Anisotropic ac dissipation at the surface of mesoscopic superconductors .\nAbstract:\nWe study anisotropy in the ac response of mesoscopic superconducting films with respect to the direction of an applied magnetic field, using numerical simulations based on the quasiclassical theory for disordered metals and the Usadel equations. We find that the magnitude of the real part of the complex conductivity tensor is strongly dependent upon the angle between the current density vector and the external magnetic field. The imaginary part of the complex conductivity shows no such dependence. This behavior can be understood by considering the effect of the magnetic field on the distribution function of Andreev bound states. Our results are relevant to experiments performed on thin film structures where the transport properties depend sensitively on the orientation of the sample relative to the applied magnetic field. \n \n Mesoscopic superconductor systems have been studied extensively over recent years due to their potential applications as quantum devices  1-3 . In particular, there has been considerable interest in understanding how these systems respond to time-dependent perturbations  4  . For example, it was recently shown experimentally  5  , that when a dc bias voltage Vdc = 0 is applied across a Josephson junction array (JJA), the system exhibits hysteretic switching between two different resistive states which occur at critical values of the amplitude of the alternating current Vac. These observations were explained theoretically  6  within the framework of the so-called  phase-locking  model  7-9 , which describes the dynamics of JJA s driven by both dc and ac currents. However, this description does not take into account effects associated with the presence of impurities or defects in the samples  10  .\nIn order to understand the influence of disorder on the dynamical properties of JJAs one needs to consider the microscopic details of the underlying physical processes taking place inside the material  11  . To this end we use here the quasiclassical approach  12  , which allows us to calculate the local density of states (LDOS) and the corresponding conductivities of disordered mesoscopic superconductors  13  . Within this formalism, the LDOS is determined self-consistently from the solution of the Usadel equation  14  \nwhere D(E) is the",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Anisotropic ac dissipation at the surface of mesoscopic superconductors . Abstract : We research anisotropy in the ac response of mesoscopic superconducting films with regard to the direction of an applied magnetic force , using numerical simulations based on the quasiclassical principle for disordered metals and the Usadel equations .We see that the magnitude of the real part of the complex conductivity tensor is strongly dependent upon the angle between the current density vector and the external magnetic force . The imaginary part of the complex conductivity displays no such dependence .This phenomenon can be understood by examining the impact of the magnetic force on the distribution function of Andreev bound states . Our results are applicable to experiments conducted on thin film structures where the travel properties depend sensitively on the orientation of the sample relative to the applied magnetic force .Mesoscopic superconductor systems have been studied thoroughly over recent history owing to their potential applications as quantum devices 1 - 3 . In particular , there has been substantial interest in understanding how these systems resist to time - dependent perturbations 4 .For instance , it was recently shown experimentally 5 , that when a dc bias voltage Vdc = 0 is applied across a Josephson junction array ( JJA ) , the device exhibits hysteretic turning between two different resistive states which occur at critical values of the frequency of the alternating current Vac . These measurements were described theoretically 6 within the framework of the so - called phase - locking theory 7 - 9 , which explains the dynamics of JJA s driven by both dc and ac waves .However , this description does not take into consideration effects involved with the presence of impurities or defects in the samples 10 . In order to comprehend the impact of disorder on the dynamical properties of JJAs one needs to consider the microscopic aspects of the fundamental physical processes take place inside the material 11 .To this end we use here the quasiclassical method 12 , which allows us to estimate the local density of states ( LDOS ) and the equivalent conductivities of disordered mesoscopic superconductors 13 . Within this formalism , the LDOS is calculated self - regularly from the solve of the Usadel equation 14 where D ( E ) is the",
        "rewrite_text": "**Title:** Anisotropic AC Dissipation at the Surface of Mesoscopic Superconductors\n\n**Abstract:** This study investigates the anisotropic behavior of the alternating current (AC) response in mesoscopic superconducting films, particularly in relation to the orientation of an applied magnetic field. Utilizing numerical simulations grounded in the quasiclassical framework for disordered metals and the Usadel equations, we find that the real part of the complex conductivity tensor exhibits a significant dependence on the angle between the current density vector and the external magnetic field. In contrast, the imaginary part of the complex conductivity remains unaffected by this angular variation. This disparity can be elucidated by analyzing how the magnetic field influences the distribution of Andreev bound states within the superconducting material. Our findings are particularly relevant for experimental setups involving thin film structures, where the transport properties are highly sensitive to the alignment of the sample with respect to the applied magnetic field.\n\nThe exploration of mesoscopic superconductors has gained momentum in recent years due to their promising applications in quantum technology. A key area of interest has been the response of these systems to time-dependent perturbations. Recent experimental work has demonstrated that a Josephson junction array (JJA) exhibits hysteretic behavior between distinct resistive states when a direct current (DC) bias voltage of Vdc = 0 is applied, with transitions occurring at specific critical frequencies of the alternating current (Vac). These observations have been theoretically interpreted within the phase-locking theory, which addresses the dynamics of JJAs under the influence of both DC and AC signals. However, this theoretical framework does not account for the effects of impurities or defects present in the materials.\n\nTo better understand the role of disorder in the dynamic properties of JJAs, it is essential to delve into the microscopic mechanisms at play within the superconducting medium. In this context, we employ the quasiclassical method to evaluate the local density of states (LDOS) and the effective conductivities of disordered mesoscopic superconductors. The LDOS is computed self-consistently through the solution of the Usadel equation, which incorporates the energy-dependent diffusion coefficient D(E). This comprehensive approach allows for a deeper insight into the interplay between disorder and superconducting behavior in mesoscopic systems.",
        "ori-fast-z-score": -1.3063945294843617,
        "water-fast-z-score": 5.901700209217436,
        "rewrite-fast-z-score": -0.5586608191273356
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A case for variational geomagnetic data assimilation: insights from a one-dimensional, nonlinear, and sparsely observed MHD system .\nAbstract:\nWe present an approach to the assimilation of magnetic field observations into numerical models using variational methods. The method is applied to a simple model problem in which we consider the evolution of a single component of the magnetic vector potential in a two-dimensional domain with periodic boundary conditions. We use this example as a testbed to explore how different choices of observation operator affect the quality of the resulting analysis fields. In particular, we compare results obtained by applying our algorithm directly on the state variable (the magnetic vector potential) against those obtained when it is first projected onto a set of basis functions that are chosen to be optimal for representing the solution at each time step. Our experiments show that both approaches yield similar results but that the latter can lead to significant computational savings. Finally, we demonstrate the utility of our approach by performing a series of twin experiments in which synthetic magnetometer measurements are used to update the initial condition of the simulation.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A case for variational geomagnetic information assimilation : insights from a one - dimensional , nonlinear , and sparsely seen MHD system . Abstract : We present an view to the assimilation of magnetic field observations into numerical models using variational techniques .The method is applied to a simple model problem in which we investigate the evolution of a single part of the magnetic vector potential in a two - dimensional domain with periodic boundary constraints . We use this example as a testbed to examine how various options of observation operator impact the performance of the resulting investigation fields .In particular , we compare findings obtained by using our algorithm directly on the state variable ( the magnetic vector potential ) against those achieved when it is initially projected onto a setting of basis functions that are chosen to be appropriate for describing the solution at each time step . Our experiments indicate that both approaches yield similar results but that the former can lead to significant computational savings .Finally , we prove the utility of our approach by performing a sequence of twin tests in which synthetic magnetometer measurements are applied to modify the early condition of the model .",
        "rewrite_text": "We propose a novel approach for the assimilation of magnetic field observations into numerical models through the use of variational techniques. This study focuses on a simplified model problem that explores the dynamics of a specific component of the magnetic vector potential within a two-dimensional domain characterized by periodic boundary conditions. By utilizing this model as a testing ground, we investigate the influence of different observation operators on the performance of the derived investigation fields. Our analysis includes a comparison between results obtained by directly applying our algorithm to the state variable, which is the magnetic vector potential, and results achieved by initially projecting this variable onto a set of basis functions tailored to effectively represent the solution at each time step. The findings reveal that while both methodologies yield comparable outcomes, the direct approach offers considerable computational advantages. To further validate the effectiveness of our method, we conduct a series of twin tests wherein synthetic magnetometer data is employed to adjust the initial conditions of the model. These tests demonstrate the practical applicability of our variational assimilation technique in enhancing the accuracy of magnetic field modeling. Overall, our work highlights the potential of variational geomagnetic information assimilation in improving the integration of observational data into numerical simulations, paving the way for more accurate predictions in geomagnetic studies.",
        "ori-fast-z-score": -1.2,
        "water-fast-z-score": 5.148767223478707,
        "rewrite-fast-z-score": 0.6527533657682196
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Effect of the Spatial Dispersion on the Shape of a Light Pulse in a Quantum Well .\nAbstract:\nWe study theoretically and numerically the effect of spatial dispersion (SD) on the shape of a light pulse propagating through an InGaAs/GaAs quantum well (QW). We show that SD leads to significant changes in the temporal profile of the transmitted pulse, which can be used for its characterization. The results are obtained by solving Maxwell s equations using the finite-difference time-domain method with periodic boundary conditions. It is shown that the presence of SD causes the appearance of additional peaks at both sides of the main peak of the transmitted pulse. These peaks become more pronounced as the QW width increases. \n \n Keywords: Light propagation, Finite difference time domain method, Quantum wells, Spatial dispersion. 1 Introduction \n \n A number of recent studies have been devoted to investigating the effects of spatial dispersion (SD), also known as nonlocality or transverse momentum conservation  1  , on various physical phenomena such as nonlinear wave dynamics  2  -  4  , spontaneous emission  5  , and transport  6  . This interest has been motivated mainly by the fact that many semiconductor devices operate under conditions where SD plays an important role  7, 8  .\n \nIn this work we consider the problem of light transmission through a single-mode quantum well (QW) structure  9  . Our aim is to investigate how SD affects the shape of the transmitted pulse. To do so, we solve Maxwell s equations using the finitedifference time-domain (FDTD) method  10  with periodic boundary conditions  11  . As it will be demonstrated below, our numerical simulations reveal that SD gives rise to new features in the temporal profile of a transmitted pulse.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Effect of the Spatial Dispersion on the Shape of a Light Pulse in a Quantum Well . Abstract : We research theoretically and numerically the impact of spatial dispersion ( SD ) on the shape of a light pulse propagating through an InGaAs / GaAs quantum well ( QW ) .We see that SD leads to significant improvements in the temporal profile of the transmitted signal , which can be used for its description . The results are derived by solving Maxwell s coefficients using the finite - difference time - domain approach with periodic boundary constraints .It is demonstrated that the presence of SD causes the appearance of new peaks at both sides of the main peak of the transmitted signal . These peaks develop more pronounced as the QW width rises .Keywords : Light propagation , Finite difference time domain approach , Quantum wells , Spatial dispersion . 1 Introduction A variety of recent studies have been focused to investigating the effects of spatial dispersion ( SD ) , sometimes called as nonlocality or longitudinal momentum conservation 1 , on various biological phenomena such as nonlinear wave propulsion 2 - 4 , spontaneous emission 5 , and transport 6 .This interest has been motivated mainly by the fact that several semiconductor devices exist under environments where SD plays an important role 7 , 8 . In this study we investigate the question of light transfer through a single - mode quantum well ( QW ) structure 9 .Our aim is to probe how SD impacts the morphology of the transmitted beam . To do so , we solve Maxwell s coefficients use the finitedifference time - domain ( FDTD ) method 10 with periodic boundary constraints 11 .As it will be showed below , our numerical simulations reveal that SD leads rise to novel features in the temporal profile of a broadcast pulse .",
        "rewrite_text": "In this study, we conduct a theoretical and numerical investigation into the effects of spatial dispersion (SD) on the shape of light pulses as they propagate through an InGaAs/GaAs quantum well (QW). Our findings indicate that SD significantly enhances the temporal profile of the transmitted signal, providing a more nuanced understanding of its characteristics. We derive our results by solving Maxwell's equations using the finite-difference time-domain (FDTD) method, incorporating periodic boundary conditions to accurately model the system. Our analysis reveals that the introduction of SD results in the emergence of additional peaks flanking the primary peak of the transmitted signal. Notably, these secondary peaks become increasingly pronounced with an increase in the width of the quantum well. \n\nThe phenomenon of spatial dispersion, often referred to as nonlocality or longitudinal momentum conservation, has garnered attention in recent research due to its implications in various biological and physical processes, including nonlinear wave propagation, spontaneous emission, and transport phenomena. This interest is particularly relevant in the context of semiconductor devices, which frequently operate in environments where SD is a critical factor. Our investigation specifically focuses on the transmission of light through a single-mode QW structure, aiming to elucidate the influence of SD on the morphology of the transmitted beam. Through our numerical simulations, we demonstrate that the presence of SD introduces novel features into the temporal profile of the emitted pulse, thereby enhancing our understanding of light propagation in quantum well systems. \n\nKeywords: Light propagation, Finite difference time domain approach, Quantum wells, Spatial dispersion.",
        "ori-fast-z-score": -1.0524696231684352,
        "water-fast-z-score": 6.490229342872016,
        "rewrite-fast-z-score": 2.280350850198276
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Rotational Widths for Use in the Tully-Fisher Relation. II. The Impact of Surface Brightness .\nAbstract:\nWe present new measurements of rotational widths (W20) and surface brightnesses (SB) for a sample of galaxies with inclinations between 30°and 80°, drawn from the Sloan Digital Sky Survey Data Release 7. We find that W20 is correlated strongly with SB at fixed luminosity, but only weakly or not at all with galaxy mass. This correlation persists even when we restrict our analysis to late-type spirals, which are known to have flat rotation curves. These results suggest that the observed scatter in the Tully-Fischer relation may be due primarily to variations in SB among galaxies of similar luminosities rather than differences in their masses. In addition, we show that this effect can explain why previous studies found no significant dependence on inclination angle in the TF relation. Finally, we demonstrate how these correlations affect estimates of the Hubble constant derived using the TF relation. Our findings also provide an explanation for the apparent discrepancy between the values obtained by different authors who used samples selected over different ranges of inclination angles.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  Rotational Widths for Use in the Tully-Fisher Relation.II.The Impact of Surface Brightness . Abstract : We report new measurements of rotational widths ( W20 ) and surface brightnesses ( SB ) for a sample of stars with inclinations between 30°and 80° , chosen from the Sloan Digital Sky Survey Data Release 7 .We see that W20 is associated strongly with SB at fixed luminosity , but only strongly or not at all with star mass . This correlation persists even when we limit our analysis to late - class spirals , which are known to have flattened rotation curves .These data suggest that the seen scatter in the Tully - Fischer relation might be due primarily to variations in SB among galaxies of comparable luminosities rather than differences in their masses . In addition , we prove that this effect can answer why previous research found no considerable dependence on inclination velocity in the TF relation .Finally , we prove how these correlations influence estimates of the Hubble constant calculated using the TF relation . Our findings also provide an reason for the alleged discrepancy between the estimates obtained by various scientists who used data selected over particular ranges of inclination distances .",
        "rewrite_text": "We present new measurements of rotational widths (W20) and surface brightnesses (SB) for a selected sample of stars with inclinations ranging from 30° to 80°, derived from the Sloan Digital Sky Survey Data Release 7. Our analysis reveals a strong correlation between W20 and SB at fixed luminosity, while the relationship with stellar mass is either weak or negligible. This correlation remains robust even when focusing exclusively on late-type spiral galaxies, which are characterized by their flattened rotation curves. The observed scatter in the Tully-Fisher (TF) relation appears to be primarily influenced by variations in surface brightness among galaxies with similar luminosities, rather than by differences in their stellar masses. Furthermore, we demonstrate that this correlation provides an explanation for the lack of significant dependence on inclination velocity previously reported in the TF relation. Our results also have implications for the estimation of the Hubble constant derived from the TF relation, highlighting how these correlations can affect the accuracy of such measurements. Additionally, our findings offer insight into the discrepancies noted in the Hubble constant estimates by different researchers, which may stem from the selection of data across specific ranges of inclination distances. Overall, this study underscores the importance of surface brightness in understanding the dynamics of galaxies and refining the Tully-Fisher relation, ultimately contributing to a more coherent picture of cosmic expansion.",
        "ori-fast-z-score": -1.9126494315742406,
        "water-fast-z-score": 5.74243935589202,
        "rewrite-fast-z-score": 2.1105794120443453
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dynamic fibrils in H-alpha and C IV .\nAbstract:\nWe present new observations of the solar atmosphere obtained with the Solar Ultraviolet Imager (SUVI) onboard the Coriolis satellite, which show dynamic fibrils in both H-alpha and CIV lines. The SUVI data are compared to simultaneous ground-based observations made at Big Bear Observatory using the 1-meter telescope equipped with an H-Alpha filter and a Fabry-Perot interferometer tuned to the CIV line. We find that the observed structures have similar properties as those seen previously by other authors but we also see some differences between them. In particular, our results suggest that the fibril structure is more complex than it was thought before. This complexity may be related to the fact that these structures are formed under different physical conditions. Our analysis shows that the observed features can be explained by assuming that they represent plasma flows along magnetic field lines. These flows could play important role in heating up the upper layers of the solar atmosphere.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dynamic fibrils in H - alpha and C IV . Abstract : We report new studies of the solar atmosphere acquired with the Solar Ultraviolet Imager ( SUVI ) onboard the Coriolis spacecraft , which show continuous fibrils in both H - alpha and CIV lines .The SUVI findings are compared to simultaneous ground - based observations made at Big Bear Observatory using the 1 - meter telescope fitted with an H - Alpha filter and a Fabry - Perot interferometer tuned to the CIV line . We see that the seen elements have related properties as those viewed earlier by other researchers but we also saw some similarities between them .In particular , our findings confirm that the fibril structure is more sophisticated than it was considered before . This complexity might be connected to the fact that these structures are created under varying physical conditions .Our study shows that the seen features can be described by assuming that they represent plasma flows along magnetic field lines . These flows could play vital role in heating up the higher layers of the solar atmosphere .",
        "rewrite_text": "We present new insights into the solar atmosphere derived from observations made with the Solar Ultraviolet Imager (SUVI) aboard the Coriolis spacecraft, focusing on the continuous fibrils observed in both H-alpha and C IV spectral lines. Our research includes a comparative analysis with concurrent ground-based observations conducted at Big Bear Observatory, utilizing a 1-meter telescope equipped with an H-alpha filter and a Fabry-Perot interferometer specifically tuned to the C IV line. The data reveal that the fibrils exhibit properties consistent with previous studies, while also highlighting notable similarities and differences that enhance our understanding of these structures. Notably, our findings indicate that the fibril architecture is more intricate than previously recognized, suggesting that their formation is influenced by a range of dynamic physical conditions. This complexity implies that the fibrils may not only be passive structures but could actively participate in the solar atmospheric processes. Our analysis posits that these features can be interpreted as plasma flows along magnetic field lines, which may play a crucial role in the thermal dynamics of the upper layers of the solar atmosphere. This research contributes to the growing body of knowledge regarding solar atmospheric phenomena and underscores the importance of advanced observational techniques in unraveling the complexities of solar dynamics.",
        "ori-fast-z-score": -0.44172610429938614,
        "water-fast-z-score": 6.037034299432969,
        "rewrite-fast-z-score": 0.7770286898858113
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Can we ever distinguish between quintessence and a cosmological constant? .\nAbstract:\nWe show that the answer to this question is no, at least in principle. We consider two models for dark energy which are observationally indistinguishable but have different physical origins; one is due to vacuum fluctuations while the other arises as an effective field theory description of quantum gravity effects. In both cases there exists a parameter space where the predictions for the evolution of the universe agree with current observations within experimental uncertainties. However, if future experiments can measure the equation-of-state of dark energy more accurately then it may be possible to differentiate these two scenarios. The discovery of cosmic acceleration has led to much interest in the possibility that our universe contains some form of  dark energy ; see e.g.,  1  . This exotic component would drive the expansion of the universe today and dominate its dynamics on large scales. A number of theoretical proposals exist for what such a dark energy might consist of (see  2  for a review). One particularly interesting class of possibilities involves introducing new fields into Einstein s equations whose presence leads to repulsive gravitational forces  3  .\nIn recent years many authors have studied the phenomenology associated with various forms of dark energy; see  4  -  8  ,  10  -  12  ,  14  -  16  ,  18  -  20  ,  22  -  26  ,  28  -  30  ,  32  -  38  ,  41  -  44  ,  46  -  48  ,  50  -  52  ,  54  -  61  ,  63  -  65  ,  67  -  71  ,  73  -  75  ,  77  -  81  ,  83  -  85  ,  88  -  92  ,  94  -  103  . Many of these works focus on comparing specific models against observational data or studying their implications for fundamental physics. Here we take a complementary approach by considering whether any two distinct models could give rise to identical observable consequences.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Can we ever distinguish between quintessence and a cosmological constant ? .Abstract : We see that the response to this question is no , at least in principle . We consider two models for black energy which are observationally indistinguishable but have different physical origins ; one is due to vacuum fluctuations while the other arises as an useful field model formulation of quantum gravitational interactions .In both cases there exists a parameter room where the estimates for the evolution of the universe comply with current observations within experimental uncertainties . However , if future research can measure the equation - of - state of dark energy more accurately then it could be possible to differentiate these two scenarios .The observation of universe acceleration has led to great interest in the prospect that our universe possesses some kind of dark energy ; seeing e . g . , 1 . This exotic component might power the evolution of the universe today and influence its mechanics on huge scales .A variety of theoretical proposals exist for what such a black force may consist of ( hear 2 for a review ) . One especially interesting class of possibilities includes introducing additional fields into Einstein s equations whose presence contributes to repulsive gravitational pressures 3 .In past decades several authors have researched the phenomenology identified with various forms of dark energy ; see 4 - 8 , 10 - 12 , 14 - 16 , 18 - 20 , 22 - 26 , 28 - 30 , 32 - 38 , 41 - 44 , 46 - 48 , 50 - 52 , 54 - 61 , 63 - 65 , 67 - 71 , 73 - 75 , 77 - 81 , 83 - 85 , 88 - 92 , 94 - 103 . Many of these works concentrate on comparing actual models against observational data or exploring their implications for theoretical physics .Here we took a complementary perspective by examining whether any two separate models could give rise to identical observable effects .",
        "rewrite_text": "Title: Can We Distinguish Between Quintessence and a Cosmological Constant?\n\nAbstract: This article addresses the fundamental question of whether it is possible to differentiate between quintessence and a cosmological constant, concluding that, at least in principle, the answer is no. We investigate two distinct models of dark energy that, while originating from different physical frameworks, yield observationally indistinguishable outcomes. The first model is based on vacuum fluctuations, while the second is a field-theoretic representation of quantum gravitational interactions. In both scenarios, there exists a range of parameters that allows the predicted evolution of the universe to align with current observational data, within the limits of experimental uncertainty. \n\nThe acceleration of the universe has sparked significant interest in the concept of dark energy, which may be responsible for the universe's expansion and its large-scale dynamics. Numerous theoretical frameworks have been proposed to explain the nature of this enigmatic force. One particularly intriguing approach involves the introduction of additional fields into Einstein's equations, which can produce repulsive gravitational effects. \n\nOver the past few decades, a wealth of research has been dedicated to exploring the phenomenology associated with various forms of dark energy. Many studies have focused on comparing specific models with observational data or investigating their implications for broader theoretical physics. In this work, we adopt a complementary approach by analyzing whether different models can produce identical observable phenomena. We emphasize that while future advancements in measuring the equation of state of dark energy may provide a means to distinguish between these models, the current observational landscape presents significant challenges in making such distinctions. Thus, our findings highlight the complexities inherent in the study of dark energy and the need for further research to unravel its mysteries.",
        "ori-fast-z-score": -0.6810052246069989,
        "water-fast-z-score": 7.718519260087736,
        "rewrite-fast-z-score": 0.17149858514250882
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  An integral field spectroscopic survey for high redshift damped Lyman-alpha galaxies .\nAbstract:\nWe present the results of an optical integral-field spectroscopy (IFS) survey to find high-redshift Damped Lyman Alpha Galaxies (DLAs). The sample consists of 16 DLAs at redshifts between 2 and 4, selected by their strong Mg II absorption lines in quasar spectra observed with the Keck telescope. We use the Gemini MultiObject Spectrograph on the 8-meter Gemini North Telescope to obtain spatially resolved observations over a 1 arcsec2 area centered around each absorber. Our IFS data are used to study the kinematics of these absorbers as well as their physical properties such as metallicity and dust content. In addition we compare our new measurements with previous studies based on single-aperture spectrophotometry. This work is part of the Gemini Observatory program GN-2007B-Q-206. Keywords: High-redshift universe; Galaxy evolution; Absorber-galaxy relationship",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : An integral field spectroscopic study for high redshift damped Lyman - alpha objects . Abstract : We present the results of an optical integral - field spectroscopy ( IFS ) study to find high - redshift Damped Lyman Alpha Galaxies ( DLAs ) .The sample consists of 16 DLAs at redshifts between 2 and 4 , selected by their powerful Mg II absorption patterns in quasar spectra observed with the Keck telescope . We use the Gemini MultiObject Spectrograph on the 8 - meter Gemini North Telescope to obtain spatially resolved surveys over a 1 arcsec2 region focused around each absorber .Our IFS data are using to study the kinematics of these absorbers as also as their biological qualities such as metallicity and dust content . In addition we compare our new data with previous research based on single - lens spectrophotometry .This project is part of the Gemini Observatory program GN - 2007B - Q - 206 . Keywords : High - redshift universe ; Galaxy history ; Absorber - galaxy relationship",
        "rewrite_text": "We present the findings of an extensive optical integral field spectroscopy (IFS) investigation aimed at identifying high-redshift Damped Lyman Alpha Galaxies (DLAs). Our study focuses on a sample of 16 DLAs, which are situated at redshifts ranging from 2 to 4. These objects were selected based on their prominent Mg II absorption features observed in quasar spectra captured by the Keck telescope. Utilizing the Gemini MultiObject Spectrograph on the 8-meter Gemini North Telescope, we conducted spatially resolved surveys over a 1 arcsec² area centered on each DLA. The IFS data collected allows us to explore the kinematics of these absorbers, as well as their physical properties, including metallicity and dust content. Furthermore, we compare our newly acquired data with earlier studies that relied on single-lens spectrophotometry, providing a more comprehensive understanding of the characteristics of these high-redshift galaxies. This research is part of the Gemini Observatory program GN-2007B-Q-206, contributing valuable insights into the evolution of galaxies in the early universe and the relationship between absorbers and their host galaxies. Our findings enhance the existing knowledge of the high-redshift universe, shedding light on the history of galaxy formation and the intricate dynamics of these distant cosmic structures. Keywords associated with this study include high-redshift universe, galaxy history, and the absorber-galaxy relationship.",
        "ori-fast-z-score": -0.6509445549041194,
        "water-fast-z-score": 3.6147844564602556,
        "rewrite-fast-z-score": 1.5554275420956378
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Multicolor observations of the afterglow of the short/hard GRB 050724 .\nAbstract:\nWe report on multiwavelength observations of the X-ray, optical and radio afterglows of the short-hard gamma-ray burst (GRB) 050724 detected by Swift satellite at 07:24:06 UT on 24 July 2005. The prompt emission lasted for about 1 s with an average photon energy E = 300 keV in the 15-350 keV band. We find that the temporal decay index is ~1.2 between 10s to 1000s post-burst time scale which indicates that this event belongs to the class of ultra-long GRBs. In addition we also detect a possible rebrightening feature around 100s post-burst time-scale. Our spectral analysis shows that the spectrum can be fitted well using both single power-law model as well as broken power law model. However, the best fit parameters are found to be consistent within their errors when compared with each other. Using our multi-wavelength data set, we estimate the total energetics associated with this event to be ~3 x 1044 erg.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Multicolor observations of the afterglow of the short / hard GRB 050724 . Abstract : We report on multiwavelength studies of the X - ray , optical and radio afterglows of the short - hard beta - ray burst ( GRB ) 050724 detected by Swift satellite at 07 : 24 : 06 UT on 24 July 2005 .The prompt emission lasted for about 1 s with an estimated photon energy E = 300 keV in the 15 - 350 keV band . We see that the temporal decay rate is ~ 1 . 2 between 10s to 1000s post - burst time scale which implies that this event belongs to the class of ultra - long GRBs .In addition we also observe a possible rebrightening characteristic around 100s post - burst time - range . Our spectral evaluation demonstrates that the spectrum can be fit well using both single power - law method as also as broken power law method .However , the best fit factors are found to be compatible within their errors when compared with each other . Using our multi - wavelength information pool , we estimate the total energetics associated with this event to be ~ 3 x 1044 erg .",
        "rewrite_text": "We present a comprehensive analysis of the multiwavelength afterglow observations of the short and hard gamma-ray burst (GRB) 050724, which was detected by the Swift satellite at 07:24:06 UT on July 24, 2005. The prompt emission of this GRB was brief, lasting approximately 1 second, with an estimated peak photon energy of 300 keV within the 15-350 keV energy range. Our findings indicate that the temporal decay rate of the afterglow is approximately 1.2 during the time interval from 10 seconds to 1000 seconds post-burst, suggesting that GRB 050724 may be classified as an ultra-long GRB. Notably, we also observe a potential rebrightening feature occurring around 100 seconds after the initial burst, which could provide insights into the underlying mechanisms of such events.\n\nIn our spectral analysis, we employed both single power-law and broken power-law models to fit the observed data. The results indicate that both fitting methods yield compatible best-fit parameters within their respective uncertainties, highlighting the complexity of the afterglow spectrum. By integrating our multiwavelength data, we estimate the total energy output associated with GRB 050724 to be approximately 3 x 10^44 erg. This study contributes to the growing body of knowledge regarding the characteristics and energetics of short gamma-ray bursts, enhancing our understanding of their nature and the physical processes at play during their afterglow phases.",
        "ori-fast-z-score": 1.5460413650478515,
        "water-fast-z-score": 6.110100926607787,
        "rewrite-fast-z-score": 0.5883484054145521
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Parametric Nonholonomic Frame Transforms and Exact Solutions in Gravity .\nAbstract:\nWe present new exact solutions to the Einstein field equations for stationary axisymmetric spacetimes with two commuting Killing vectors, which are generated by applying nonholonomic frame transforms (NFT) to known vacuum solutions. The NFT is constructed using an ansatz for the metric coefficients that depends on one arbitrary function of the radial coordinate only. We show how this method can be used to generate families of black hole solutions with different horizon topologies. In particular we find new rotating black ring solutions with toroidal horizons. These solutions have been obtained previously as limits of static black rings but our approach allows us to obtain them directly without any additional assumptions or approximations. Finally, we discuss some open problems related to these results. PACS numbers: 04.20.-q, 11.10.-z, 98.80.Cq \nI. INTRODUCTORY REMARkS\nThe study of exact solutions to the Einstein equations has played a crucial role in understanding many aspects of general relativity. However, it is often difficult to construct such solutions because they require solving complicated nonlinear partial differential equations. This problem becomes even more challenging when considering physically interesting situations like those involving rotation and/or matter fields. Nevertheless, there exist several techniques that allow one to generate new classes of solutions starting from simpler ones. One of the most powerful methods involves transforming the original solution into another one via so-called nonholonomic frame transforms  1  . Such transformations preserve certain geometric properties of the spacetime while changing others; see  2  -  4  for reviews. For example, if the transformed solution satisfies the vacuum Einstein equations then so does the original one  5  .\nIn this work we apply nonholonomic frame transforms to known vacuum solutions of the Einstein equations in order to generate new exact solutions describing stationary axisymmetric spacetimes: i.e., spacetimes admitting at least two independent Killing vector fields whose orbits are closed curves  6  . Stationary axisymmetric spacetimes play an important role in astrophysics since they describe the exterior gravitational field of spinning objects like stars, planets, and black holes  7, 8",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Parametric Nonholonomic Frame Transforms and Exact Solutions in Gravity . Abstract : We introduce novel exact solutions to the Einstein field equations for stationary axisymmetric spacetimes with two commuting Killing vectors , which are produced by using nonholonomic frame transforms ( NFT ) to known vacuum solutions .The NFT is built using an ansatz for the metric coefficients that relies on one arbitrary function of the radial coordinate only . We see how this method can be used to create families of black hole solutions with various horizon topologies .In particular we find new moving black ring solutions with toroidal horizons . These solutions have been achieved formerly as limits of static black rings but our approach allows us to obtain them directly without any additional constraints or approximations .Finally , we explain some open problems related to these results . PACS scores : 04 . 20 . - q , 11 . 10 . - z , 98 . 80 . Cq I .INTRODUCTORY REMARkS The investigation of precise solutions to the Einstein equations has served a crucial role in understanding several parts of general relativity . However , it is often challenging to build such solutions because they demand solving complicated nonlinear partial differential equations .This problem remains especially more challenging when treating physically exciting situations like those concerning rotation and / or matter fields . Nevertheless , there remain many procedures that enable one to create fresh categories of solutions starting from simpler ones .One of the most efficient methods involves transforming the previous solve into another one via so - called nonholonomic frame transforms 1 . Such transformations maintain certain geometric properties of the spacetime while altering others ; see 2 - 4 for reviews .For instance , if the transformed solution satisfies the vacuum Einstein equations then so does the previous one 5 . In this study we apply nonholonomic frame transforms to known vacuum solutions of the Einstein equations in order to create novel exact solutions governing stationary axisymmetric spacetimes : i . e . , spacetimes admitting at least two independent Killing matrix fields whose orbits are open curves 6 .Stationary axisymmetric spacetimes serve an important role in astrophysics since they describe the exterior gravitational field of spinning objects like stars , planets , and dark holes 7 , 8",
        "rewrite_text": "**Title:** Parametric Nonholonomic Frame Transforms and Exact Solutions in Gravity\n\n**Abstract:** In this article, we present innovative exact solutions to the Einstein field equations specifically for stationary axisymmetric spacetimes characterized by two commuting Killing vectors. These solutions are derived using nonholonomic frame transforms (NFT) applied to established vacuum solutions. The NFT is constructed based on an ansatz for the metric coefficients that incorporates a single arbitrary function of the radial coordinate. This technique enables the generation of families of black hole solutions with diverse horizon topologies. Notably, we identify new moving black ring solutions featuring toroidal horizons. While these solutions have previously been obtained as limits of static black rings, our method facilitates their direct derivation without imposing additional constraints or approximations. Furthermore, we discuss several unresolved issues related to these findings. \n\nThe exploration of exact solutions to the Einstein equations is pivotal for advancing our comprehension of various aspects of general relativity. However, the construction of such solutions often poses significant challenges due to the complexity of the nonlinear partial differential equations involved. This difficulty is particularly pronounced in scenarios involving rotation and/or matter fields. Despite these challenges, numerous strategies exist for generating new classes of solutions from simpler ones. One of the most effective approaches is the application of nonholonomic frame transforms, which allow for the transformation of existing solutions while preserving certain geometric properties of the spacetime. For example, if the transformed solution adheres to the vacuum Einstein equations, the original solution will also satisfy these equations. In this work, we utilize nonholonomic frame transforms on known vacuum solutions of the Einstein equations to derive new exact solutions that govern stationary axisymmetric spacetimes—those spacetimes that possess at least two independent Killing vector fields with open orbits. Stationary axisymmetric spacetimes are crucial in astrophysics as they describe the gravitational fields surrounding rotating celestial bodies, including stars, planets, and black holes.",
        "ori-fast-z-score": -1.5696136947790653,
        "water-fast-z-score": 6.111919138499425,
        "rewrite-fast-z-score": -0.07832604499879574
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The KATRIN sensitivity to the neutrino mass and to right-handed currents in beta decay .\nAbstract:\nThe Karlsruhe Tritium Neutrino experiment (KATRIN) is currently under construction at the Karlsruhe Institute for Technology, Germany. It will measure the electron energy spectrum near the endpoint with unprecedented precision by using an electrostatic retardation spectrometer combined with a magnetic filter. The measurement aims at determining the absolute value of the effective electron antineutrino mass mneff with a statistical uncertainty of 0.2 eV/c2 or better. In this contribution we present results on the expected sensitivities of KATRIN to mneff as well as to possible contributions from right-handed currents in beta-decay. We show that these sensitivities are mainly determined by systematic uncertainties which can be controlled within the experimental accuracy required. This allows us to conclude that KATRIN has excellent prospects to determine mneff precisely enough to test whether it agrees with the predictions of standard model extensions such as supersymmetry.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The KATRIN sensitivity to the neutrino mass and to right - handed waves in beta decay . Abstract : The Karlsruhe Tritium Neutrino experiment ( KATRIN ) is currently under construction at the Karlsruhe Institute for Technology , Germany .It will measure the electron energy spectrum near the endpoint with incredible precision by using an electrostatic retardation spectrometer together with a magnetic filter . The measurement seeks at determining the absolute value of the effective electron antineutrino mass mneff with a statistical uncertainty of 0 . 2 eV / c2 or better .In this contribution we present results on the expected sensitivities of KATRIN to mneff as well as to possible contributions from right - handed waves in beta - decay . We suggest that these sensitivities are chiefly decided by systematic uncertainties which can be governed within the empirical precision required .This enables us to observe that KATRIN has excellent potential to predict mneff exactly enough to test whether it agrees with the estimates of standard theory extensions such as supersymmetry .",
        "rewrite_text": "Title: The KATRIN Sensitivity to Neutrino Mass and Right-Handed Waves in Beta Decay\n\nAbstract: The Karlsruhe Tritium Neutrino experiment (KATRIN) is currently being developed at the Karlsruhe Institute for Technology in Germany, with the aim of measuring the electron energy spectrum in the vicinity of the endpoint with unprecedented precision. This measurement will be achieved through the use of an electrostatic retardation spectrometer in conjunction with a magnetic filter. The primary objective of KATRIN is to determine the absolute value of the effective electron antineutrino mass, denoted as mneff, with a statistical uncertainty of 0.2 eV/c² or better. In this paper, we present our findings regarding the anticipated sensitivities of KATRIN not only to mneff but also to potential contributions from right-handed waves in the beta decay process. Our analysis indicates that the sensitivities are predominantly influenced by systematic uncertainties, which can be managed within the empirical precision necessary for the experiment. This insight highlights KATRIN's remarkable capability to accurately predict mneff, allowing for a rigorous comparison with theoretical estimates from extensions of the standard model, such as supersymmetry. The implications of these measurements are significant, as they could provide critical insights into the nature of neutrinos and their role in fundamental physics, potentially leading to new discoveries in particle physics and cosmology. The KATRIN experiment thus stands as a pivotal endeavor in the quest to deepen our understanding of neutrino properties and their implications for the universe.",
        "ori-fast-z-score": -1.0681034923744679,
        "water-fast-z-score": 4.00693842672377,
        "rewrite-fast-z-score": -0.7258661863112977
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Feynman-Weinberg Quantum Gravity and the Extended Standard Model as a Theory of Everything .\nAbstract:\nWe present an extension to the standard model that includes gravity, based on Feynman s ideas about quantum gravity. The extended standard model is formulated in terms of gauge fields for all known interactions (including gravity) and fermions with spin 1/2 or 1. We show how this theory can be derived from first principles using Feynman diagrams. In addition we discuss some phenomenological consequences such as neutrino masses and dark matter candidates. Finally we comment briefly on possible experimental tests of our proposal. This work was supported by NSF grant PHY-0456747. A theory of everything should include gravity along with other fundamental forces. Here we propose one such theory which extends the standard model including gravitational effects. Our approach follows closely Feynman s original idea of formulating quantum gravity in terms of gauge fields coupled to fermions. Using Feynman diagrams we derive the extended standard model from first principles. Some phenomenological consequences are discussed, e.g., neutrino mass generation via seesaw mechanisms and dark matter candidates. Possible experiments testing our proposal are also mentioned.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Feynman - Weinberg Quantum Gravity and the Extended Standard Model as a Theory of Everything . Abstract : We present an addition to the standard theory that contains gravity , built on Feynman s ideas about quantum gravitational .The extended standard theory is developed in terms of gauge fields for all known interactions ( including gravity ) and fermions with spin 1 / 2 or 1 . We see how this theory can be derived from first principles use Feynman diagrams .In addition we explain some phenomenological consequences such as neutrino masses and dark matter candidates . Finally we comment briefly on potential experimental tests of our proposal .This project was supported by NSF grant PHY - 0456747 . A theory of things should involve gravity along with other fundamental forces .Here we develop one such idea which extends the standard theory including gravitational interactions . Our formulation follows carefully Feynman s earlier notion of formulating quantum gravitational in terms of gauge fields coupled to fermions .Using Feynman diagrams we derive the extended standard theory from first principles . Some phenomenological consequences are discussed , e . g . , neutrino mass development via seesaw processes and dark matter candidates .Possible experiments testing our proposal are also discussed .",
        "rewrite_text": "Title: Feynman-Weinberg Quantum Gravity and the Extended Standard Model as a Theory of Everything\n\nAbstract: In this article, we introduce an innovative extension to the standard model of particle physics that incorporates gravitational interactions, inspired by Richard Feynman's conceptual framework for quantum gravity. Our extended standard model is constructed using gauge fields to represent all known fundamental interactions, including gravity, alongside fermions with spin-1/2 and spin-1 characteristics. We demonstrate how this comprehensive theory can be derived from first principles through the application of Feynman diagrams, providing a robust mathematical foundation for our approach.\n\nWe explore several phenomenological implications of our theory, particularly focusing on the generation of neutrino masses through seesaw mechanisms and the identification of potential dark matter candidates. These aspects not only enrich our understanding of particle interactions but also offer insights into the underlying structure of the universe.\n\nMoreover, we briefly outline potential experimental avenues for testing the predictions of our extended model, emphasizing the importance of empirical validation in the advancement of theoretical physics. This research is supported by NSF grant PHY-0456747, highlighting the collaborative effort to deepen our understanding of the fundamental forces that govern the cosmos. Our work aims to unify gravity with other fundamental forces, presenting a cohesive framework that could serve as a stepping stone toward a comprehensive theory of everything. Through this endeavor, we hope to contribute to the ongoing dialogue in theoretical physics and inspire further exploration into the nature of reality.",
        "ori-fast-z-score": 0.9138115486202573,
        "water-fast-z-score": 7.0,
        "rewrite-fast-z-score": 0.8250286473253902
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Clustering by soft-constraint affinity propagation: Applications to gene-expression data .\nAbstract:\nAffinity propagation (AP) is an algorithm for clustering that has been shown to be effective in many applications, including bioinformatics and computer vision. However, AP requires the number of clusters as input parameter which may not always be known beforehand. In this work we propose a novel approach based on constrained optimization techniques to automatically determine the optimal number of clusters using only pairwise similarity information between samples. We show how our method can be applied to several problems related to gene expression analysis such as finding co-expressed genes or identifying differentially expressed genes across multiple conditions. Our results demonstrate that our proposed method outperforms state-of-the-art approaches both in terms of accuracy and robustness. The source code used to generate all experiments presented here will be made available at http://bitbucket.org/juanlorenzo/softconstraint-clustering/wiki/Home . Affinity Propagation (AP) is an efficient message-passing algorithm for clustering that has recently gained popularity due to its effectiveness in various fields ranging from image processing  1  , computational biology  2  , and recommender systems  3  .\nHowever, one disadvantage of AP is that it requires the user to specify the desired number of clusters k before running the algorithm. This requirement makes AP less suitable when there are no prior knowledge about the number of clusters present in the dataset  4  . To overcome this problem, some authors have suggested heuristics to estimate the value of k  5  while others have developed methods to find the best possible partition given any fixed k  6  . Nevertheless, these solutions still require the user to provide additional parameters like the maximum allowed cluster size  7  or the minimum required density  8  making them difficult to use without expert knowledge  9  .\nIn order to address this issue, we introduce Soft-Constrained Affinity Propagation (SCAP), a new approach for determining the optimal number of clusters in datasets with unknown structure. SCAP uses Constrained Optimization Techniques  10  to solve the NP-hard combinatorial problem of finding the optimal solution within a set of feasible solutions  11  . More specifically,...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Clustering by soft - constraint affinity propagation : Applications to gene - expression information . Abstract : Affinity propagation ( AP ) is an algorithm for clustering that has been shown to be successful in multiple users , notably bioinformatics and computer vision .However , AP needs the quantity of clusters as input parameter which would not always be known beforehand . In this study we attempt a new approach using on constrained optimization schemes to automatically predict the ideal amount of clusters using only pairwise similarity information between samples .We see how our technique can be applied to several difficulties related to protein function analysis such as finding co - expressed cells or discovering differentially expressed cells across multiple conditions . Our results show that our proposed approach outperforms state - of - the - art methods both in terms of precision and robustness .The source software using to create all experiments shown here will be made accessible at http : / / bitbucket . org / juanlorenzo / softconstraint - clustering / wiki / Home . Affinity Propagation ( AP ) is an efficient signal - passing algorithm for clustering that has recently gotten popularity due to its effectiveness in different fields ranging from photo processing 1 , computational chemistry 2 , and recommender environments 3 .However , one disadvantage of AP is that it requires the user to indicate the desired amount of clusters k before running the method . This requirement allows AP less suited when there are no previous knows about the quantity of clusters present in the dataset 4 .To solve this question , some writers have suggested heuristics to estimate the value of k 5 while others have developed methods to find the best possible partition given any fixed k 6 . Nevertheless , these solutions still require the user to provide alternative parameters like the maximum permitted cluster number 7 or the minimum needed width 8 making them harder to use without expert know 9 .In order to alleviate this question , we introduce Soft - Constrained Affinity Propagation ( SCAP ) , a new approach for determining the ideal amount of clusters in datasets with unknown structure . SCAP uses Constrained Optimization Techniques 10 to tackle the NP - hard combinatorial issue of finding the ideal solution within a setting of feasible answers 11 .More specifically,...",
        "rewrite_text": "**Title:** Clustering by Soft-Constraint Affinity Propagation: Applications to Gene-Expression Information\n\n**Abstract:** Affinity Propagation (AP) is a widely recognized clustering algorithm that has demonstrated significant success in various domains, particularly in bioinformatics and computer vision. A notable limitation of AP is its requirement for the user to specify the number of clusters, k, prior to execution, which can be problematic when the optimal number of clusters is unknown. In this study, we propose a novel methodology that leverages constrained optimization techniques to automatically estimate the ideal number of clusters using only pairwise similarity data between samples. Our approach, termed Soft-Constrained Affinity Propagation (SCAP), addresses the challenges associated with protein function analysis, including the identification of co-expressed cells and the detection of differentially expressed cells across varying conditions. \n\nThrough extensive experimentation, we demonstrate that SCAP significantly outperforms existing state-of-the-art clustering methods in terms of both precision and robustness. Unlike traditional approaches that require users to set additional parameters, such as the maximum allowable number of clusters or minimum cluster width, SCAP simplifies the clustering process by eliminating the need for expert knowledge. By employing constrained optimization techniques, SCAP effectively navigates the NP-hard combinatorial problem of determining the optimal clustering solution within a set of feasible options. \n\nThe results of our study indicate that SCAP not only enhances the clustering process but also provides a more user-friendly alternative for researchers dealing with complex datasets. The software developed for this research, which facilitates the implementation of SCAP, will be made publicly available at http://bitbucket.org/juanlorenzo/softconstraint-clustering/wiki/Home. This contribution aims to advance the field of clustering by providing a robust tool for analyzing gene-expression data and other applications where the number of clusters is not predetermined.",
        "ori-fast-z-score": -0.9716656205775166,
        "water-fast-z-score": 9.339741389929875,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Shapes of Molecular Cloud Cores in Orion .\nAbstract:\nWe present the results of an investigation into the shapes and orientations of molecular cloud cores in the Orion Nebula region using near-infrared polarimetry obtained with the Gemini North telescope. We find that most (80%) of our sample are oblate spheroids, while 20% have more complex morphologies including prolate spheroids, disks, or bipolar structures. The majority of these objects show no evidence for rotation; however, we do detect significant polarization vectors aligned perpendicular to the major axes of several sources which may be due to magnetic fields. These observations suggest that many of the clouds were formed by large-scale gravitational collapse rather than rotational support. This work is supported by NASA grant NNX10AC99G. We report on the results of an investigation of the shapes and orientations of dense molecular gas clumps within the Orion Nebula region. Using near-infrared polarimetric imaging data taken at Gemini Observatory s North Telescope, we identify 80 percent of our sample as oblate spheroids. Twenty percent exhibit more complicated morphologies such as prolate spheroids or bipolar structures. Most of these objects appear to lack any internal rotation but some display polarization vectors oriented perpendicularly to their major axes suggesting the presence of magnetic fields.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Shapes of Molecular Cloud Cores in Orion . Abstract : We present the conclusion of an search into the shapes and orientations of molecular dust cores in the Orion Nebula region utilizing near - infrared polarimetry obtained with the Gemini North telescope .We see that most ( 80 % ) of our sample are oblate spheroids , while 20 % have more sophisticated morphologies including prolate spheroids , disks , or bipolar forms . The majority of these objects show no evidence for rotation ; however , we do discover considerable polarization vectors aligned parallel to the main axes of several sources which may be due to magnetic fields .These measurements suggest that several of the clouds were created by large - scale gravitational failure rather than rotational support . This research is backed by NASA grant NNX10AC99G .We report on the conclusion of an investigation of the shapes and orientations of dense molecular dust clumps within the Orion Nebula region . Using near - infrared polarimetric imaging information taken at Gemini Observatory s North Telescope , we identify 80 percent of our sample as oblate spheroids .Twenty percent display more complicated morphologies such as prolate spheroids or bipolar forms . Most of these objects show to lack any internal rotation but some display polarization vectors oriented perpendicularly to their major axes suggesting the presence of magnetic fields .",
        "rewrite_text": "We present the findings of our investigation into the shapes and orientations of molecular dust cores located in the Orion Nebula region, utilizing near-infrared polarimetry data collected with the Gemini North telescope. Our analysis reveals that a significant majority, approximately 80%, of the molecular cores in our sample exhibit oblate spheroidal shapes. In contrast, the remaining 20% display more complex morphologies, including prolate spheroids, disk-like structures, and bipolar configurations. Notably, while most of these cores show no signs of internal rotation, we observe a substantial alignment of polarization vectors that run parallel to the principal axes of several cores. This alignment may indicate the influence of magnetic fields within these structures. Our findings imply that the formation of many of these molecular clouds is likely the result of large-scale gravitational collapse rather than being supported by rotational dynamics. This research is supported by NASA grant NNX10AC99G and contributes to a deeper understanding of the physical processes governing the formation and evolution of molecular clouds in the Orion Nebula. Through our study, we aim to enhance the knowledge of the structural characteristics of these dense molecular dust clumps, which play a crucial role in star formation and the dynamics of the interstellar medium.",
        "ori-fast-z-score": -0.41702882811414954,
        "water-fast-z-score": 4.587317109255645,
        "rewrite-fast-z-score": 0.7258661863112977
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Branch and Cut Algorithm for the Halfspace Depth Problem .\nAbstract:\nWe present an algorithm to solve the half-space depth problem, which is one of the most fundamental problems in computational geometry. The input consists of n points on the plane with integer coordinates bounded by B. We want to find a point that minimizes its distance to all other points. This problem has been studied extensively since it was introduced by Helly in 1930s. In this work we propose a new branch-and-cut algorithm based on mixed-integer programming (MIP) formulation. Our MIP model uses O(nB) variables and constraints. Using our proposed cutting planes, we can reduce the number of nodes explored during search significantly. As a result, our algorithm runs faster than existing algorithms when the dimension d = 2 or 3. For example, if the dimension is two, then our algorithm solves instances with up to 1 million points within 10 minutes while previous best known results are limited to 100 thousand points.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Branch and Cut Algorithm for the Halfspace Depth Problem . Abstract : We present an algorithm to solve the half - space depth problem , which is one of the most important problems in computational geometry .The input consists of n points on the plane with integer coordinates bounded by B . We want to find a point that minimizes its distance to all other points .This problem has been studied thoroughly since it was introduced by Helly in 1930s . In this study we propose a new branch - and - cut optimization based on mixed - integer programming ( MIP ) implementation .Our MIP approach utilizes O ( nB ) parameters and constraints . Using our proposed cutting planes , we can restrict the quantity of vertices uncovered during search significantly .As a effect , our algorithm runs faster than existing algorithms when the dimension d = 2 or 3 . For instance , if the dimension is two , then our algorithm solves instances with up to 1 million points within 10 minutes while prior best known performance are limited to 100 thousand points .",
        "rewrite_text": "We introduce a novel algorithm designed to address the half-space depth problem, a significant challenge in the field of computational geometry. This problem involves determining a point in a two-dimensional plane that minimizes its distance to a set of n points, each represented by integer coordinates and constrained within a boundary B. The half-space depth problem has been extensively researched since its inception by Helly in the 1930s. In our work, we propose a branch-and-cut optimization strategy that leverages mixed-integer programming (MIP) techniques. Our MIP formulation incorporates O(nB) parameters and constraints, allowing for a more efficient exploration of the solution space. By implementing specialized cutting planes, we effectively reduce the number of uncovered vertices during the search process, leading to a significant improvement in computational speed. Our algorithm demonstrates superior performance compared to existing methods, particularly in lower dimensions (d = 2 or 3). For example, in two-dimensional scenarios, our algorithm can process datasets containing up to 1 million points in under 10 minutes, a remarkable enhancement over the previous best-known algorithms, which were limited to handling only 100,000 points. This advancement not only showcases the efficiency of our approach but also opens new avenues for tackling larger instances of the half-space depth problem in practical applications.",
        "ori-fast-z-score": -0.8944271909999159,
        "water-fast-z-score": 3.1304951684997055,
        "rewrite-fast-z-score": 1.3348476249438292
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Analysis of the real estate market in Las Vegas: Bubble, seasonal patterns, and prediction of the CSW indexes .\nAbstract:\nThe purpose of this study is to analyze the real estate market in the city of Las Vegas (LV) by using the Composite Shiller Weiss Indexes (CSWI). The data used for analysis are monthly prices of residential properties sold between January 2005 and December 2014. In addition, we use the autoregressive integrated moving average model with exogenous variables (ARIMAX), which allows us to predict future values of the indexes based on past information. We found that there was an increase in the value of property sales during the period analyzed, but it did not reach levels considered as bubbles. However, the results show that the LV real estate market has experienced periods of overvaluation since 2007. Finally, our findings suggest that the ARIMAX model can be useful when predicting the evolution of the CSWI indexes. Keywords: Real Estate Market; Prediction Modeling; Autoregressive Integrated Moving Average",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Analysis of the real property market in Las Vegas : Bubble , seasonal patterns , and measurement of the CSW indexes . Abstract : The purpose of this study is to analyze the real property market in the cities of Las Vegas ( LV ) by using the Composite Shiller Weiss Indexes ( CSWI ) .The data used for study are monthly prices of housing houses sold between January 2005 and December 2014 . In addition , we using the autoregressive integrated moved average model with exogenous parameters ( ARIMAX ) , which allows us to predict upcoming values of the indexes based on past data .We showed that there was an increase in the value of property purchases during the period analyzed , but it did not reach levels regarded as bubbles . However , the results show that the LV real property market has undergone cycles of overvaluation since 2007 .Finally , our findings show that the ARIMAX theory can be valuable when predicting the evolution of the CSWI indexes . Keywords : Real Estate Market ; Prediction Modeling ; Autoregressive Integrated Moving Average",
        "rewrite_text": "**Title:** Analysis of the Real Property Market in Las Vegas: Bubble Dynamics, Seasonal Trends, and CSW Index Measurement\n\n**Abstract:** This study aims to provide a comprehensive analysis of the real estate market in Las Vegas (LV) through the application of the Composite Shiller Weiss Indexes (CSWI). Utilizing a dataset comprising monthly housing sale prices from January 2005 to December 2014, we investigate the trends and fluctuations within this dynamic market. To enhance our predictive capabilities, we employ the Autoregressive Integrated Moving Average model with exogenous variables (ARIMAX), which facilitates the forecasting of future index values based on historical data patterns. Our analysis reveals a notable increase in property purchase values throughout the study period; however, these values did not escalate to levels typically associated with market bubbles. Despite this, the findings indicate that the LV real estate market has experienced cycles of overvaluation since 2007, suggesting underlying volatility and potential risks for investors. Furthermore, our results underscore the efficacy of the ARIMAX model in forecasting the trajectory of the CSWI indexes, highlighting its utility in real estate market analysis. This research contributes to the understanding of market behaviors in Las Vegas, offering insights that can inform both investors and policymakers regarding the cyclical nature of property values and the importance of predictive modeling in real estate economics. \n\n**Keywords:** Real Estate Market; Predictive Modeling; Autoregressive Integrated Moving Average",
        "ori-fast-z-score": 1.3242443839434612,
        "water-fast-z-score": 6.289942788427422,
        "rewrite-fast-z-score": 0.8340576562282991
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Coincident, 100 kpc-scale damped Lyman alpha absorption towards a binary QSO: how large are galaxies at z ~ 3? .\nAbstract:\nWe report on the discovery of an intervening galaxy with a mass M = 1011.5 ± 0.3M⊙ and size R = 1.7 ± 0.2h−1kpc in front of a gravitationally lensed quasar pair separated by 5′′ (~100 kpc). The absorber is detected as a DLA system along both sightlines to the quasars, which have redshifts zqso = 2.962 and zqso = 2. . We use this object to constrain the typical sizes of high-z galaxies. Our results suggest that these objects were typically smaller than their local counterparts when they formed most of their stars. This may be related to the fact that massive galaxies grow through mergers over cosmic time. \n \n Keywords: Galaxy evolution, Quasars, Absorbers, Massive black holes \n \n \n \n High-redshift quasars provide powerful probes for studying the physical properties of distant galaxies. In particular, gravitational lens systems can magnify background sources, allowing us to study fainter structures such as faint companions or extended halos around bright foreground lenses. Here we present new observations of the gravitationally-lensed quasar pair HE0435-1223, where one component has been previously found to host a supermassive black hole (SMBH) with a mass MBH = 4 × 109M☉ . Using deep near-infrared spectroscopy obtained with VLT/X-SHOOTER, we detect a strong Mg II λ2796 line associated with a galaxy located between the two quasars. The galaxy shows no evidence of ongoing star formation activity but hosts a very old stellar population. Its total luminosity corresponds to a SFR < 10−2M☉ yr−1 , indicating that it was not actively forming stars during its peak epoch of star-formation activity. However, the presence of a young stellar population cannot be ruled out completely due to possible dust obscuration effects. From our analysis, we find that the galaxy has a mass M = 1011+0.3−0.4M☉ and radius R =",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Coincident , 100 kpc - scale damped Lyman alpha absorption towards a binary QSO : how large are galaxies at z ~ 3 ? .Abstract : We report on the discovery of an intervening galaxy with a mass M = 1011 . 5 ± 0 . [UNK] and size R = 1 . 7 ± 0 . 2h−1kpc in top of a gravitationally lensed quasar pair divided by 5 ′ ′ ( ~ 100 kpc ) . The absorber is detected as a DLA system along both sightlines to the quasars , which have redshifts zqso = 2 . 962 and zqso = 2 . .We use this object to constrain the typical dimensions of high - z galaxies . Our results show that these objects were generally tiny than their local neighbours when they formed most of their stars .This might be connected to the fact that powerful nuclei grow through mergers over cosmic time . Keywords : Galaxy evolve , Quasars , Absorbers , Massive black holes High - redshift quasars serve powerful probes for studying the physical properties of distant galaxies .In particular , gravity lens systems can magnify background sources , allowing us to study fainter objects such as faint companions or open halos around bright foreground lenses . Here we present new studies of the gravitationally - lensed quasar pair HE0435 - 1223 , where one core has been previously found to host a supermassive black hole ( SMBH ) with a mass MBH = 4 × 109M☉ .Using deep near - infrared spectroscopy acquired with VLT / X - SHOOTER , we locate a powerful Mg II λ2796 line associated with a galaxy located between the two quasars . The galaxy displays no evidence of ongoing star formation activity but hosts a very ancient stellar community .Its overall luminosity corresponds to a SFR < 10−2M☉ yr−1 , showing that it was not actively creating stars during its high epoch of star - formation activity . However , the presence of a young stellar community cannot be decided out completely due to possible dust obscuration effects .From our analysis , we find that the universe has a mass M = 1011 + 0 . 3−0 . 4M☉ and radius R =",
        "rewrite_text": "**Title:** Coincident, 100 kpc-scale damped Lyman-alpha absorption towards a binary QSO: How large are galaxies at z ~ 3?\n\n**Abstract:** In this study, we present the discovery of an intervening galaxy situated between a gravitationally lensed quasar pair, separated by approximately 100 kpc (5′′). The galaxy exhibits a mass of M = 10^11.5 ± 0. [UNK] and a size of R = 1.7 ± 0.2 h^(-1) kpc. This galaxy is identified as a Damped Lyman-alpha (DLA) system along both sightlines to the quasars, which have redshifts of z_qso = 2.962 and z_qso = 2. . Our findings provide critical insights into the typical dimensions of high-redshift galaxies, revealing that these galaxies were generally smaller than their local counterparts during the peak of their star formation activity. This observation may be linked to the evolutionary processes of powerful nuclei, which tend to grow through mergers over cosmic time.\n\nHigh-redshift quasars serve as significant tools for investigating the physical characteristics of distant galaxies. In particular, gravitational lensing systems enhance the visibility of background sources, enabling the study of fainter objects such as dim companions or extended halos surrounding bright foreground lenses. Our research focuses on the gravitationally lensed quasar pair HE0435-1223, where one of the cores has previously been identified to host a supermassive black hole (SMBH) with a mass of M_BH = 4 × 10^9 M☉. Utilizing deep near-infrared spectroscopy from the VLT/X-SHOOTER, we detect a prominent Mg II λ2796 line associated with the galaxy located between the two quasars. Notably, this galaxy shows no signs of ongoing star formation but possesses an ancient stellar population. Its total luminosity indicates a star formation rate (SFR) of less than 10^(-2) M☉ yr^(-1), suggesting that it was not actively forming stars during its peak star formation epoch. However, the potential presence of a younger stellar population cannot be entirely ruled out due to possible dust obscuration. Our analysis concludes that the galaxy has a mass of M = 10^11 + 0.3 - 0.4 M☉ and a radius of R = .",
        "ori-fast-z-score": 1.8864844365675972,
        "water-fast-z-score": 7.491057470676988,
        "rewrite-fast-z-score": 0.2526455763199557
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Higher K-theory via universal invariants .\nAbstract:\nWe give an explicit description of the higher algebraic K-groups in terms of certain universal cohomology classes, which are defined by using only the ring structure and the unit element of the underlying commutative ring. This is done for any commutative ring with unity R (not necessarily Noetherian). The main result can be formulated as follows: Let M be a module over R. Then there exists a natural isomorphism between the higher algebraic K-groups: \nK_n(R) = Ext^n_R(M, R)\nand the group of all n-fold Massey products on M modulo those that vanish under some suitable finiteness condition. We also show how this theorem leads to a new proof of Quillen s localization theorem. Finally we discuss applications to the study of equivariant K-theory. In particular, we prove that if G is a compact Lie group acting freely on a smooth manifold X then the equivariant K-theory groups of X are isomorphic to the ordinary K-theory groups of the fixed point set X^G.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Higher K - theory via universal invariants . Abstract : We get an explicit description of the higher algebraic K - fields in terms of certain universal cohomology groups , which are specified by using only the ring structure and the unit element of the fundamental commutative field .This is accomplished for any commutative ring with unity R ( not necessarily Noetherian ) . The main consequence can be stated as follows : Let M be a module over R . Then there exists a natural isomorphism between the higher algebraic K - groups : K _ k ( R ) = Ext ^ n _ R ( M , R ) and the group of all k - fold Massey products on M modulo those that vanish under some suitable finiteness requirement .We also demonstrate how this theorem gives to a new proof of Quillen s localization principle . Finally we explain applied to the study of equivariant K - theory .In particular , we prove that if G is a compact Lie set acting freely on a smooth manifold X then the equivariant K - theory groups of X are isomorphic to the ordinary K - theory groups of the fixed point set X ^ G .",
        "rewrite_text": "In this article, titled \"Higher K-theory via Universal Invariants,\" we provide a detailed exploration of higher algebraic K-theory, presenting an explicit characterization of higher K-fields through specific universal cohomology groups. Our approach relies solely on the ring structure and the identity element of the fundamental commutative field, allowing us to extend our findings to any commutative ring with unity R, regardless of whether it is Noetherian. A significant outcome of our work is the establishment of a natural isomorphism between the higher algebraic K-groups, denoted as K_k(R), and the Ext^n_R(M, R) group for a given module M over R. This relationship also encompasses the group of k-fold Massey products on M, factoring in those that vanish under certain finiteness conditions. \n\nMoreover, we illustrate how our results provide a novel proof of Quillen's localization principle, thereby reinforcing the foundational aspects of K-theory. Additionally, we delve into the implications of our findings in the context of equivariant K-theory. Specifically, we demonstrate that when G is a compact Lie group acting freely on a smooth manifold X, the equivariant K-theory groups of X can be shown to be isomorphic to the ordinary K-theory groups of the fixed point set X^G. This connection not only enriches the understanding of K-theory in a broader sense but also opens avenues for further research in equivariant contexts. Overall, our work contributes to the ongoing discourse in algebraic K-theory by linking universal invariants with practical applications in both classical and equivariant settings.",
        "ori-fast-z-score": 0.1111111111111111,
        "water-fast-z-score": 4.2485291572496005,
        "rewrite-fast-z-score": 0.8542421961772492
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Information, information processing and gravity .\nAbstract:\nThe author considers the problem of gravitational interaction between bodies in terms of their informational content. The main idea is that the gravitational field can be considered as an ensemble of gravitons which carry information about the source body. Gravitational waves are treated as carriers of information on the state of motion of gravitating objects. It is shown how this approach allows one to explain some phenomena observed in astrophysics (the Pioneer anomaly) and cosmology (dark energy). In addition, it is proposed to use the concept of  information potential  for describing the evolution of the universe. This article was published by the journal Classical and Quantum Gravity Volume 27, Issue 14, pages 5993-6010, November 2010. DOI: 10.1088/0264-9381/27/14/05993/abstract. The following text is taken directly from the original publication. \n \n Abstract \n \n We consider the problem of gravitational interaction among bodies in terms of their information content. The main idea here is that the gravitational field may be viewed as an ensemble of gravitons/quanta carrying information about the source body; gravitational waves are then seen as carriers of information regarding the state of motion of the gravitating objects. This viewpoint enables us to provide explanations for certain phenomena observed in astrophysical settings (e.g., the Pioneer anomaly), as well as in cosmological contexts (e.g., dark energy). Moreover, we propose using the notion of “information potential” to describe the evolution of the Universe.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Information , info processing and gravity . Abstract : The author considers the question of gravitational interaction between bodies in terms of their informational quality .The main idea is that the gravitational field can be regarded as an ensemble of gravitons which carry information about the source body . Gravitational waves are treated as carriers of information on the state of movement of gravitating structures .It is demonstrated how this methodology allows one to explain some phenomena observed in astrophysics ( the Pioneer anomaly ) and cosmology ( darkness energy ) . In addition , it is proposed to use the idea of information possibilities for describing the evolution of the universe .This section was publication by the journal Classical and Quantum Gravity Volume 27 , Issue 14 , pages 5993 - 6010 , November 2010 . DOI : 10 . 1088 / 0264 - 9381 / 27 / 14 / 05993 / abstract .The following text is taken directly from the first paper . Abstract We consider the question of gravitational interaction among bodies in terms of their information content .The main idea here is that the gravitational field might be viewed as an ensemble of gravitons / quanta carrying information about the source body ; gravitational waves are then shown as carriers of information regarding the state of movement of the gravitating structures . This interpretation helps us to provide explanations for particular phenomena observed in astrophysical contexts ( e . g . , the Pioneer anomaly ) , as also as in cosmological contexts ( e . g . , darkness energy ) .Moreover , we propose utilizing the notion of “ information potential ” to explain the evolution of the Universe .",
        "rewrite_text": "Title: Information, Information Processing, and Gravity\n\nAbstract: In this article, the author explores the gravitational interactions between bodies through the lens of their informational characteristics. The central premise posits that the gravitational field can be conceptualized as a collection of gravitons, which serve as carriers of information pertaining to the source body. Gravitational waves are interpreted as conduits of information that reflect the motion of gravitating entities. This innovative approach provides a framework for elucidating various phenomena observed in astrophysics, such as the Pioneer anomaly, as well as cosmological issues like dark energy. By framing gravitational interactions in terms of information content, the author offers new insights into these complex topics. Furthermore, the article suggests that the concept of \"information potential\" could be instrumental in describing the evolution of the universe itself. This perspective not only enhances our understanding of gravitational dynamics but also opens up new avenues for research in both astrophysics and cosmology. The findings presented in this paper were published in the journal Classical and Quantum Gravity, Volume 27, Issue 14, on pages 5993-6010 in November 2010, and can be accessed via DOI: 10.1088/0264-9381/27/14/05993. The insights derived from this study contribute significantly to the ongoing discourse on the interplay between information theory and gravitational phenomena, suggesting that a deeper understanding of information processing may be key to unlocking further mysteries of the universe.",
        "ori-fast-z-score": 1.6570343122169822,
        "water-fast-z-score": 6.9963670960272575,
        "rewrite-fast-z-score": 0.20412414523193154
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Prospects for precision measurements of atomic helium using direct frequency comb spectroscopy .\nAbstract:\nWe present the prospects for high-precision measurement of the 1s2p 3P-1s2s 3S transition in atomic helium with an optical frequency comb (OFC). The OFC is stabilized to a high-finesse cavity and locked to a narrow linewidth laser at 1083 nm, which serves as a local oscillator. We show that this system can be used to measure the absolute frequencies of two transitions in helium with uncertainties below 100 kHz. This will allow us to determine the fine-structure constant α with relative uncertainty better than 2×10−10 by measuring the ratio between these two frequencies. In addition we demonstrate how the same setup could be used to perform tests of fundamental physics beyond the Standard Model such as searches for time variation of fundamental constants or violations of Lorentz invariance. Optical frequency combs are powerful tools for precise metrology  1–3  . They have been successfully applied to many different fields including ultra-stable lasers  4  , gravitational wave detection  5  , and quantum optics  6  .\nIn particular they provide unprecedented possibilities for high-precision measurement  7–9  . Here we propose to use them to improve our knowledge on the value of the fine structure constant  10  . To achieve this goal it is necessary to measure the absolute frequencies f(1s2p 3P1) = 929 072 631 770 Hz  11  and f(1s2s 3S1) = 929 073 761 828 Hz  12  of two transitions in helium. These values were determined previously with uncertainties of about 300 kHz  13  but recent theoretical calculations suggest that their accuracy may be improved significantly  14–18  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Prospects for precision observations of atomic helium using direct wavelength comb spectroscopy . Abstract : We present the possibilities for high - precision study of the 1s2p 3P - 1s2s 3S transfer in atomic helium with an optical frequency comb ( OFC ) .The OFC is stabilized to a high - finesse cavity and locked to a thin linewidth laser at 1083 nm , which serves as a local oscillator . We see that this scheme can be used to measure the absolute frequencies of two transitions in helium with uncertainties below 100 kHz .This will provide us to predict the fine - structure constant α with relative uncertainty better than 2×10−10 by monitoring the proportion between these two frequencies . In addition we prove how the same setup could be used to conduct tests of fundamental theory beyond the Standard Model such as searches for time variation of fundamental constants or violations of Lorentz invariance .Optical frequency combs are powerful tools for precise metrology 1 – 3 . They have been successfully applied to many various fields including ultra - stable lasers 4 , gravity wave detection 5 , and quantum optics 6 .In particular they give unprecedented possibilities for high - precision precision 7 – 9 . Here we undertake to use them to develop our information on the value of the fine structure constant 10 .To achieve this goal it is required to measure the absolute frequencies f ( 1s2p 3P1 ) = 929 072 631 770 Hz 11 and f ( 1s2s 3S1 ) = 929 073 761 828 Hz 12 of two transitions in helium . These values were determined earlier with uncertainties of about 300 kHz 13 but recent theoretical calculations suggest that their sensitivity might be improved dramatically 14 – 18 .",
        "rewrite_text": "We explore the potential for high-precision investigations of the 1s2p 3P - 1s2s 3S transition in atomic helium utilizing optical frequency comb (OFC) spectroscopy. Our approach involves stabilizing the OFC to a high-finesse cavity and locking it to a narrow linewidth laser operating at 1083 nm, which acts as a local oscillator. This innovative setup enables us to measure the absolute frequencies of two specific transitions in helium with uncertainties below 100 kHz. Such precision measurements will allow us to estimate the fine-structure constant, α, with a relative uncertainty of less than 2×10−10 by analyzing the ratio of these two frequencies. Furthermore, we demonstrate that this configuration can facilitate tests of fundamental theories that extend beyond the Standard Model, including investigations into the potential time variation of fundamental constants and possible violations of Lorentz invariance.\n\nOptical frequency combs are recognized as powerful instruments for precise metrology and have been successfully employed across various domains, including ultra-stable laser development, gravitational wave detection, and quantum optics. Their application in high-precision measurements presents unprecedented opportunities for advancing our understanding of fundamental physical constants. In this study, we aim to refine our knowledge of the fine-structure constant by accurately measuring the absolute frequencies of the transitions f(1s2p 3P1) = 929,072,631,770 Hz and f(1s2s 3S1) = 929,073,761,828 Hz in helium. Previous determinations of these frequencies have been made with uncertainties around 300 kHz; however, recent theoretical advancements indicate that we may significantly enhance this sensitivity. Through our work, we hope to contribute valuable insights into both atomic physics and the broader implications for fundamental physics.",
        "ori-fast-z-score": 0.18107149208503706,
        "water-fast-z-score": 5.680518698404823,
        "rewrite-fast-z-score": 1.1627553482998907
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The richest superclusters. I. Morphology .\nAbstract:\nWe present the results on morphology and luminosity function for the most luminous galaxy clusters in the Universe, selected by their X-ray emission (the RCS2 sample). We find that these objects are characterized by an elliptical shape with axial ratio q = 0.7 ± 0.1 and by a steep luminosity function dN/dL ∝ L−2.5±0.3 . The observed properties suggest that they may be identified as fossil groups or proto-clusters at z > 1.0 .\nThe data used here were obtained during our observing runs performed at ESO telescopes under programs IDs: 073.A-0505(B), 078.A-0518(C) and 079.A-0739(D) . In this work we study the morphological and photometric properties of the brightest galaxy clusters in the universe. These systems have been detected through their X-ray emission using the ROSAT All Sky Survey (RASS; Voges et al., 1999) , and then followed up spectroscopically to confirm their redshifts and measure their velocity dispersions (see e.g. Rosati et al. , 1998 , Gladders & Yee 2005 , Eisenhardt et al. , 2008 . They represent some of the most massive structures known so far in the universe, being able to host several thousands of galaxies each one. Their high mass makes them ideal targets to investigate how such large scale structures form and evolve over time.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The richest superclusters . I . Morphology .Abstract : We report the results on morphology and luminosity function for the most luminous galaxy galaxies in the Universe , selected by their X - ray radiation ( the RCS2 specimen ) . We see that these objects are marked by an elliptical shape with axial proportion q = 0 . 7 ± 0 . 1 and by a steep luminosity function dN / dL [UNK] L−2 . 5±0 . 3 .The observed properties suggest that they may be identified as extinct families or proto - complexes at z > 1 . 0 . The data used here were obtained during our observing walks performed at ESO telescopes under programs IDs : 073 . A - 0505 ( B ) , 078 . A - 0518 ( C ) and 079 . A - 0739 ( D ) .In this research we study the morphological and photometric properties of the brightest galaxy galaxies in the universe . These systems have been detected through their X - ray emission utilizing the ROSAT All Sky Survey ( RASS ; Voges et al . , 1999 ) , and then followed up spectroscopically to confirm their redshifts and track their velocity dispersions ( see e . g .Rosati et al . , 1998 , Gladders & Yee 2005 , Eisenhardt et al ., 2008 . They hold some of the most gigantic structures discovered so far in the universe , being could to host numerous thousands of galaxies each one .Their high mass creates them ideal targets to examine how such large scale structures structure and evolve over time .",
        "rewrite_text": "**Title:** The Richest Superclusters I: Morphology\n\n**Abstract:** In this study, we present our findings on the morphological characteristics and luminosity function of the most luminous galaxy clusters in the universe, specifically those identified by their X-ray emissions from the RCS2 sample. Our analysis reveals that these clusters exhibit an elliptical morphology, characterized by an axial ratio of q = 0.7 ± 0.1, alongside a steep luminosity function described by dN/dL ∝ L^(-2.5±0.3). These observed traits suggest that these clusters may represent extinct families or proto-complexes at redshifts greater than z > 1. The data utilized in this research were collected during our observational campaigns at the European Southern Observatory (ESO) under program IDs 073.A-0505(B), 078.A-0518(C), and 079.A-0739(D). \n\nOur investigation focuses on the morphological and photometric properties of the brightest galaxy clusters, which have been detected through their X-ray emissions as part of the ROSAT All Sky Survey (RASS; Voges et al., 1999). Subsequent spectroscopic follow-ups were conducted to confirm their redshifts and to monitor their velocity dispersions, as referenced in previous studies (e.g., Rosati et al., 1998; Gladders & Yee, 2005; Eisenhardt et al., 2008). These clusters represent some of the largest structures identified in the universe, each potentially hosting thousands of galaxies. Their significant mass makes them prime candidates for investigating the formation and evolution of large-scale structures over cosmic time. This research contributes to our understanding of the dynamics and characteristics of these colossal systems, shedding light on the processes that govern their development in the context of cosmic evolution.",
        "ori-fast-z-score": -0.3216337604513384,
        "water-fast-z-score": 5.116817192534651,
        "rewrite-fast-z-score": 1.5689290811054724
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Direct diameter measurement of a star filling its Roche Lobe: The semi-detached binary SS Leporis spatially resolved with VINCI/VLTI .\nAbstract:\nWe present the first direct determination of the stellar radius in an interacting binary system, using interferometric observations obtained with the VLTI and AMBER instrument. We resolve for the first time the components of the close binary system SS Leporis (separation ~0.3 arcsec), which consists of two main sequence stars that are both filling their respective Roche lobes. By fitting theoretical models to our data we find that one component is slightly larger than expected by theory while the other has a radius consistent with predictions based on evolutionary tracks. This result suggests that tidal interactions have modified the radii of these stars during their evolution towards contact. Our results also show that the orbital inclination angle i = 60 ± 5 degrees, as determined previously through radial velocity measurements, agrees well with our new estimate derived directly from the observed separation between the two stars. Keywords: Interferometry; Binary Stars; Stellar Radius",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Direct diameter calculation of a star filling its Roche Lobe : The semi - separated binary SS Leporis spatially resolved with VINCI / VLTI . Abstract : We report the first direct determination of the stellar radius in an interacting binary system , using interferometric observations derived with the VLTI and AMBER method .We resolve for the first time the parts of the close binary system SS Leporis ( separation ~ 0 . 3 arcsec ) , which consists of two principal sequence stars that are both filling their separate Roche petals . By fitting theoretical estimates to our information we find that one part is slightly larger than expected by theory while the other has a diameter compatible with predictions based on evolutionary tracks .This result suggests that tidal interactions have modified the radii of these stars during their development towards contact . Our results also demonstrate that the orbital inclination angle i = 60 ± 5 degrees , as determined earlier through radial speed measurements , agrees well with our new estimate calculated directly from the known separation between the two stars .Keywords: Interferometry; Binary Stars; Stellar Radius",
        "rewrite_text": "**Title:** Direct Diameter Calculation of a Star Filling Its Roche Lobe: The Semi-Separated Binary SS Leporis Resolved with VINCI/VLTI\n\n**Abstract:** In this study, we present the inaugural direct measurement of the stellar radius within an interacting binary system, achieved through interferometric observations utilizing the Very Large Telescope Interferometer (VLTI) and the AMBER technique. For the first time, we have successfully resolved the components of the close binary system SS Leporis, which exhibits a separation of approximately 0.3 arcseconds. This system is comprised of two main-sequence stars, both of which are filling their respective Roche lobes. By applying theoretical models to our observational data, we discovered that one of the stars is marginally larger than theoretical predictions, while the other star's diameter aligns closely with expectations derived from evolutionary models. These findings imply that tidal interactions have played a significant role in altering the radii of these stars as they evolve towards a state of contact. Furthermore, our results corroborate the previously determined orbital inclination angle of i = 60 ± 5 degrees, which was established through radial velocity measurements, with our new direct estimate based on the known separation of the two stars. This research not only enhances our understanding of the dynamics within binary systems but also provides critical insights into the effects of tidal forces on stellar evolution. \n\n**Keywords:** Interferometry; Binary Stars; Stellar Radius",
        "ori-fast-z-score": -0.105999788000636,
        "water-fast-z-score": 3.8786538958710977,
        "rewrite-fast-z-score": -0.4082482904638631
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  NBODY meets stellar population - The HYDE-PARC Project .\nAbstract:\nThe HYPERION project is an effort to combine the results of N-body simulations with those of detailed stellar population synthesis models in order to study how galaxies evolve and form their stars.  In this talk, I will present some recent results on galaxy formation using these techniques. These include studies of:  1) Galaxy mergers at high redshift; 2) The evolution of galactic disks; 3) The effect of AGN feedback on galaxy growth; 4) The assembly history of massive elliptical galaxies. This work was supported by NASA grant NNX10AD65G (PI: D. Katz). My research interests are focused on understanding how galaxies grow over cosmic time through the processes of star formation, black hole accretion, and galaxy mergers. To address this question, we have developed new methods for combining cosmological hydrodynamic simulations with state-of-the-art stellar population synthesis codes that allow us to predict the properties of galaxies as functions of both space and time.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : NBODY meets stars population - The HYDE - PARC Project . Abstract : The HYPERION program is an initiative to mix the results of N - bodies simulations with those of explicit stellar community synthesis estimates in order to study how galaxies evolve and form their stars .In this talk , I will present some latest findings on star formation employing these techniques . These include findings of : 1 ) Galaxy mergers at high redshift ; 2 ) The expansion of galactic cones ; 3 ) The impact of AGN feedback on star development ; 4 ) The assembly history of large elliptical galaxies .This research was supported by NASA grant NNX10AD65G ( PI : D . Katz ) . My research interests are focused on understanding how clusters grow over cosmic time through the mechanisms of star formation , white hole accretion , and galaxy mergers .To address this question , we have developed novel techniques for merging cosmological hydrodynamic simulations with state - of - the - art stellar community synthesis coding that enable us to predict the properties of galaxies as functions of both space and time .",
        "rewrite_text": "**Title:** NBODY Meets Stellar Populations - The HYDE-PARC Project\n\n**Abstract:** The HYPERION program represents a groundbreaking initiative aimed at integrating the outcomes of N-body simulations with detailed stellar population synthesis models to enhance our understanding of galaxy formation and evolution. This presentation will delve into recent discoveries related to star formation derived from these innovative methodologies. Key findings include: 1) the dynamics of galaxy mergers occurring at high redshift; 2) the expansion patterns of galactic cones; 3) the influence of Active Galactic Nuclei (AGN) feedback on stellar development; and 4) the assembly history of massive elliptical galaxies. This research has been made possible through the support of NASA grant NNX10AD65G, under the principal investigation of D. Katz. My research primarily focuses on elucidating the processes by which galaxy clusters evolve over cosmic time, particularly through mechanisms such as star formation, white hole accretion, and galaxy mergers. To tackle these complex questions, we have developed advanced techniques that effectively combine cosmological hydrodynamic simulations with cutting-edge stellar population synthesis codes. This integration allows us to predict the properties of galaxies as functions of both spatial and temporal dimensions, thereby providing deeper insights into the intricate processes governing galaxy evolution.",
        "ori-fast-z-score": -2.0855209398041166,
        "water-fast-z-score": 5.237229365663818,
        "rewrite-fast-z-score": 1.507556722888818
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Non-adiabatic transport in a quantum dot turnstile .\nAbstract:\nWe study the non-adiabatic current through an open quantum dot connected to two leads with different chemical potentials, which is driven by time-dependent gate voltages applied on both sides of the dot. We show that this system can be described as a  turnstile  where electrons are pumped between the leads when the driving frequencies match certain resonances. The effect is robust against disorder and dephasing. This work was supported by NSERC (Canada) and CIFAR (Canadian Institute for Advanced Research). In recent years there has been growing interest in studying electron pumps based on semiconductor nanostructures such as quantum dots or carbon nanotubes  1, 2  . These devices have potential applications ranging from metrology  3  , single-electron transistors  4  , and spintronics  5  .\nIn these systems, charge carriers are transported across the device via sequential tunneling processes  6  . A number of theoretical studies  7, 8  have shown that it is possible to achieve high efficiency in these devices even at room temperature  9  . However, most previous works focused only on adiabatic pumping  10  , i.e., the case where the frequency of the external drive is much smaller than all other relevant energy scales  11  . Recently, several experiments  12, 13  reported large currents generated by nonadiabatic pumping  14, 15  . It remains unclear whether these results can be explained within existing theories  16  .\nHere we consider a simple model of a quantum dot connected to two metallic leads  see Fig. 1(a)    17  . The dot level is modulated periodically by applying oscillating gate voltages V L/R = ±V 0 cos ωt on each side of the dot  18  . When the modulation period T ≡ 2π/ω matches one of the dwell times τ n = π / 2(E F − E n )  associated with the discrete levels E n of the isolated dot, electrons will be transferred coherently between the left and right leads  19  . Here E F denotes the Fermi energy of the leads  20  . As illustrated schematically in Figs. 1(b-c), depending on",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Non - adiabatic transport in a quantum dot turnstile . Abstract : We explore the non - adiabatic current through an open quantum dot connected to two leads with varying molecular potentials , which is generated by time - based gate voltages applied on both sides of the dot .We see that this network can be described as a turnstile where electrons are pumped between the leads when the driving frequencies match specified resonances . The phenomenon is robust against disorder and dephasing .This research was supported by NSERC ( Canada ) and CIFAR ( Canadian Institute for Advanced Research ) . In recent years there has been growing interest in investigating electron pumps based on semiconductor nanostructures such as quantum dots or carbon nanotubes 1 , 2 .These systems have potential applications ranging from metrology 3 , single - ion transistors 4 , and spintronics 5 . In these systems , charge carriers are transported across the device via sequential tunneling processes 6 .A couple of theoretical experiments 7 , 8 have shown that it is could to achieve high efficiency in these machines even at room temperature 9 . However , most prior papers focused only on adiabatic pumping 10 , i . e . , the case where the frequency of the external drive is much smaller than all other relevant energy scales 11 .Recently , various tests 12 , 13 revealed huge currents produced by nonadiabatic pumping 14 , 15 . It remains unsure whether these results can be understood within existing models 16 .Here we define a simple model of a quantum dot connected to two metallic leads see Fig . 1 ( a ) 17 .The dot level is modulated periodically by using oscillating gate voltages V L / R = ±V 0 cos ωt on each side of the dot 18 . When the modulation time T ≡ 2π / ω matches one of the dwell times σ n = π / 2 ( E F − E n ) associated with the discrete levels E n of the isolated dot , electrons will be transferred coherently between the leave and left leads 19 .Here E F denotes the Fermi energy of the leads  20  .As illustrated schematically in Figs.1(b-c), depending on",
        "rewrite_text": "**Title:** Non-Adiabatic Transport in a Quantum Dot Turnstile\n\n**Abstract:** This study investigates the non-adiabatic current generated in an open quantum dot that is connected to two leads with varying molecular potentials. The current is induced by time-dependent gate voltages applied to both sides of the quantum dot. Our findings suggest that this system can be conceptualized as a turnstile mechanism, where electrons are effectively pumped between the leads when the driving frequencies align with specific resonances. Notably, this phenomenon demonstrates resilience against disorder and dephasing effects. The research is supported by the Natural Sciences and Engineering Research Council of Canada (NSERC) and the Canadian Institute for Advanced Research (CIFAR). \n\nIn recent years, there has been an increasing interest in electron pumps utilizing semiconductor nanostructures, including quantum dots and carbon nanotubes. These systems hold promise for various applications, such as metrology, single-ion transistors, and spintronics. In these devices, charge carriers are transported through sequential tunneling processes. Previous theoretical studies have indicated the potential for achieving high efficiency in these electron pumps, even at room temperature. However, most existing literature has primarily focused on adiabatic pumping, where the frequency of the external drive is significantly lower than all other relevant energy scales.\n\nRecent experimental results have unveiled substantial currents generated by non-adiabatic pumping, raising questions about the applicability of current theoretical models to explain these observations. In this paper, we propose a straightforward model of a quantum dot connected to two metallic leads. The energy levels of the dot are periodically modulated through oscillating gate voltages on either side. When the modulation period aligns with the dwell times associated with the discrete energy levels of the isolated dot, coherent electron transfer occurs between the leads. This study aims to deepen the understanding of non-adiabatic transport mechanisms in quantum dot systems and their implications for future technological advancements.",
        "ori-fast-z-score": 0.6620847108818944,
        "water-fast-z-score": 5.816546635685188,
        "rewrite-fast-z-score": 0.2508726030021272
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  CIV 1549 as an Eigenvector 1 Parameter for Active Galactic Nuclei .\nAbstract:\nWe present the results of our analysis on the X-ray spectrum and variability properties of CIV 1549, which is one of the brightest Seyfert galaxies in the sky at soft X-rays (0.5-2 keV). We find that its spectral shape can be well described by a power law with photon index Γ = 2.1 ± 0.2 plus two thermal components; one component has temperature kT = 0.3 +0.4 −0.1 keV while another has higher temperature kT = 3.7 +1.6 −1.1 keV. The luminosity ratio between these two thermal components is L h /L l ≈ 5.9 +2.8 −2.1 . In addition to this multi-component continuum model, we also include several emission lines such as Fe Kα line and OVII triplet. Our best-fit parameters are consistent with those obtained previously using ASCA data. \n \n Using the Chandra HETG observation taken during 2001-2002, we have investigated the short-term variability behavior of CIV 1549. We found no significant time lag between different energy bands within the observed bandpasses. However, there appears to exist some correlation between flux variations in hard energies (> 4 keV) and those in softer energies (< 4 keV), although it does not appear to be strictly linear relationship. This result suggests that the origin of the short-term variability may be due to reprocessing of harder photons into softer ones rather than intrinsic fluctuations of the primary source itself. \n \n Finally, we examine whether or not CIV 1549 shows any evidence for rapid aperiodic variability. By applying wavelet transform techniques to the light curve extracted from the central region of the galaxy, we detect strong signals corresponding to periods ranging from 10 - 100 s. These periodicities are most likely associated with quasi-periodic oscillations (QPOs). \n \n We conclude that CIV 1549 is probably powered by accretion onto supermassive black holes.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : CIV 1549 as an Eigenvector 1 Parameter for Active Galactic Nuclei . Abstract : We present the conclusion of our analysis on the X - ray spectrum and variability properties of CIV 1549 , which is one of the brightest Seyfert galaxies in the sky at warm X - radiation ( 0 . 5 - 2 keV ) .We see that its spectral structure can be well described by a power law with photon index Γ = 2 . 1 ± 0 . 2 plus two thermal parts ; one component has temperature kT = 0 . 3 + 0 . 4 −0 . 1 keV while another has higher temperature kT = 3 . 7 + 1 . 6 −1 . 1 keV . The luminosity factor between these two thermal elements is L h / L l ≈ 5 . 9 + 2 . 8 −2 . 1 .In addition to this multi - component continuum model , we also cover several emission lines such as Fe Kα line and OVII triplet . Our best - fitting values are compatible with those acquired previously using ASCA information .Using the Chandra HETG measurement done during 2001 - 2002 , we have analyzed the short - term variability behavior of CIV 1549 . We determined no considerable time lag between various energy bands within the known bandpasses .However , there seems to remain some correlation between flux variations in hard energies ( > 4 keV ) and those in harder energies ( < 4 keV ) , although it does not appear to be strictly linear correlation . This result suggests that the origin of the short - term variability may be due to reprocessing of harder photons into harder ones instead than intrinsic fluctuations of the primary source itself .Finally , we investigate whether or not CIV 1549 shows any evidence for rapid aperiodic variability . By applying wavelet transform techniques to the light curve extracted from the central region of the galaxy , we perceive strong pulses corresponding to periods ranging from 10 - 100 s . These periodicities are most likely correlated with quasi - periodic oscillations ( QPOs ) .We assume that CIV 1549 is probably powered by accretion onto supermassive black holes .",
        "rewrite_text": "We present our findings on the X-ray spectral characteristics and variability of CIV 1549, a prominent Seyfert galaxy known for its bright warm X-ray emissions (0.5 - 2 keV). Our analysis reveals that the spectral profile of CIV 1549 can be effectively modeled using a multi-component approach, specifically a power law with a photon index of Γ = 2.1 ± 0.2, complemented by two thermal components. The first thermal component exhibits a temperature of kT = 0.3 +0.4 -0.1 keV, while the second component has a significantly higher temperature of kT = 3.7 +1.6 -1.1 keV. The luminosity ratio between these thermal components is approximately L_h / L_l ≈ 5.9 +2.8 -2.1. In addition to the continuum model, we also analyze several emission lines, including the Fe Kα line and the OVII triplet, with our best-fitting parameters aligning well with previous ASCA observations.\n\nUtilizing data from the Chandra High Energy Transmission Grating (HETG) obtained between 2001 and 2002, we explored the short-term variability of CIV 1549. Our analysis indicates no significant time lag between different energy bands within the observed range. However, we do observe a correlation between flux variations in higher energy bands (> 4 keV) and those in lower energy bands (< 4 keV), although this relationship is not strictly linear. This finding implies that the short-term variability may arise from the reprocessing of harder photons rather than intrinsic fluctuations from the primary source.\n\nLastly, we investigate the presence of rapid aperiodic variability in CIV 1549. By employing wavelet transform techniques on the light curve derived from the galaxy's central region, we identify strong pulses corresponding to periods between 10 and 100 seconds. These periodicities are likely associated with quasi-periodic oscillations (QPOs). We conclude that CIV 1549 is likely powered by accretion processes occurring around supermassive black holes.",
        "ori-fast-z-score": -0.1796053020267749,
        "water-fast-z-score": 6.540737725975564,
        "rewrite-fast-z-score": 1.2185435916898848
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Peculiar Motions in the Region of the Ursa Major Supercluster of Galaxies .\nAbstract:\nWe have analyzed peculiar motions of galaxies within the region of the ursa major supercluster (UMS) using data on galaxy redshifts and distances obtained by us with the 6-m telescope at the Special Astrophysical Observatory of Russian Academy of Sciences.  The UMS is one of the largest known superclusters, consisting of about 100 rich clusters of galaxies. We found that the mean radial velocity of all galaxies in this supercluster relative to its center amounts to -500 km/s. This value agrees well with estimates made earlier for other superclusters. However, we also discovered an unexpected feature of the motion of galaxies inside the UMS. Namely, there are two groups of galaxies moving towards each other along the line connecting their centers. One group consists of three nearby clusters located near the center of the supercluster; another includes four distant clusters situated at a distance of more than 60 Mpc from it.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Peculiar Motions in the Region of the Ursa Major Supercluster of Galaxies . Abstract : We have analyzed unusual movements of galaxies within the region of the ursa major supercluster ( UMS ) using data on star redshifts and altitudes obtained by us with the 6 - m observatory at the Special Astrophysical Observatory of Russian Academy of Sciences .The UMS is one of the largest discovered superclusters , consisting of about 100 rich clusters of clusters . We showed that the mean radial speed of all galaxies in this supercluster relative to its center amounts to - 500 km / s .This value agrees well with predictions taken previous for other superclusters . However , we also discovered an unexpected feature of the movement of galaxies inside the UMS .Namely , there are two groups of clusters shifting towards each other along the line linking their centers . One group contains of three adjacent galaxies placed near the center of the supercluster ; another includes four distant galaxies placed at a distance of more than 60 Mpc from it .",
        "rewrite_text": "Title: Unusual Motions in the Ursa Major Supercluster of Galaxies\n\nAbstract: In this study, we investigate the peculiar dynamics of galaxies within the Ursa Major Supercluster (UMS), utilizing redshift and altitude data collected at the 6-meter telescope of the Special Astrophysical Observatory, Russian Academy of Sciences. The UMS is recognized as one of the largest superclusters identified to date, comprising approximately 100 rich clusters of galaxies. Our analysis reveals that the average radial velocity of galaxies in the supercluster, measured relative to its center, is approximately -500 km/s. This finding aligns closely with previous predictions made for other superclusters, reinforcing the consistency of cosmic expansion models. However, our research also uncovers an intriguing anomaly in the motion of galaxies within the UMS. Specifically, we identify two distinct groups of clusters that are converging towards one another along the axis connecting their centers. The first group consists of three galaxies situated near the supercluster's center, while the second group comprises four more distant galaxies, located over 60 Mpc away. This unexpected behavior raises questions about the gravitational interactions and evolutionary processes at play within the UMS. Our findings contribute to a deeper understanding of galaxy dynamics in superclusters and may have implications for the study of large-scale structure formation in the universe. Further investigation is warranted to explore the underlying mechanisms driving these peculiar motions and their potential impact on the overall dynamics of the Ursa Major Supercluster.",
        "ori-fast-z-score": -1.61245154965971,
        "water-fast-z-score": 3.2003787654626508,
        "rewrite-fast-z-score": -0.19802950859533489
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Lifetime Improvement of Wireless Sensor Networks by Collaborative Beamforming and Cooperative Transmission .\nAbstract:\nIn this work, we propose an energy-efficient cooperative transmission scheme for wireless sensor networks (WSNs). The proposed scheme is based on the combination of collaborative beamforming at the source node with cooperative transmission to multiple relay nodes in order to improve lifetime performance. In particular, the source node first transmits data packets using its maximum power level. Then, it switches to lower power levels if no packet has been successfully received within a certain time period. If one or more packets are successfully decoded, then the source node increases its transmit power back up to the previous value. We derive closed-form expressions for outage probability as well as average throughput under Rayleigh fading channels. Our results show that our proposed scheme can significantly increase network lifetime compared to conventional schemes without compromising system reliability. Finally, numerical examples are provided to verify the theoretical analysis. Keywords: Lifetime improvement; Relay selection; Energy efficiency; Outage probability",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Lifetime Improvement of Wireless Sensor Networks by Collaborative Beamforming and Cooperative Transmission . Abstract : In this project , we propose an energy - efficient joint transmission strategy for wireless sensor networks ( WSNs ) .The proposed system is based on the combination of collaborative beamforming at the source node with coordinated transmission to multiple relay nodes in order to ensure lifetime efficiency . In particular , the source node initially transmits data packets using its highest power level .Then , it switches to smaller energy levels if no packet has been successfully received within a certain time time . If one or more signals are properly decoded , then the source node increases its receive power back up to the previous value .We derive closed - form expressions for outage likelihood as well as average throughput under Rayleigh fading filters . Our results show that our proposed system can significantly raise system lifetime compared to conventional systems without compromising system efficiency .Finally , numerical examples are provided to confirm the theoretical analysis . Keywords : Lifetime improvement ; Relay selection ; Energy efficiency ; Outage probability",
        "rewrite_text": "**Title:** Enhancing the Longevity of Wireless Sensor Networks through Collaborative Beamforming and Cooperative Transmission\n\n**Abstract:** This study presents an innovative energy-efficient joint transmission strategy tailored for wireless sensor networks (WSNs). Our approach integrates collaborative beamforming at the source node with synchronized transmission to multiple relay nodes, aiming to optimize the network's operational lifetime. Initially, the source node transmits data packets at its maximum power level. In the absence of successful packet reception within a designated timeframe, the node reduces its transmission power to conserve energy. Conversely, if one or more signals are accurately decoded, the source node promptly reverts to its higher power level to maintain communication efficiency. We derive closed-form expressions for both outage probability and average throughput under the influence of Rayleigh fading conditions. The findings indicate that our proposed strategy substantially enhances the system's longevity compared to traditional methods, all while preserving operational efficiency. Additionally, we provide numerical examples that validate our theoretical framework, demonstrating the practical applicability of our approach. This research contributes to the ongoing efforts to improve energy management in WSNs, highlighting the potential of collaborative techniques in extending network lifespan. \n\n**Keywords:** Wireless Sensor Networks, Energy Efficiency, Collaborative Beamforming, Cooperative Transmission, Outage Probability, Relay Selection.",
        "ori-fast-z-score": -0.10259783520851541,
        "water-fast-z-score": 5.454823740581938,
        "rewrite-fast-z-score": 1.2247448713915892
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Quantum mechanical approach to decoherence and relaxation generated by fluctuating environment .\nAbstract:\nWe present an exact quantum-mechanical treatment for the dynamics of open systems in which the system is coupled to many harmonic oscillators representing its surrounding environment.  We show that, under certain conditions, this model can be reduced exactly into a master equation with Lindblad form. The resulting master equations are used to study the effects of environmental fluctuations on the evolution of the density matrix describing the state of the system. In particular we consider two different models of environments corresponding to Ohmic dissipation and spin-boson interaction respectively. For both cases it is shown how the effect of the environment leads to irreversible loss of information about the initial state of the system as well as to thermalization at late times. Finally, we discuss possible applications of our results to problems such as transport through mesoscopic conductors or dissipative tunneling between localized states in disordered solids. Decoherence and relaxation processes play a crucial role in understanding the physics of open quantum systems  1, 2  . These phenomena arise when the system interacts with some external degrees of freedom (environment) whose influence cannot be neglected  3  .\nIn recent years there has been considerable interest in developing theoretical methods capable of treating these effects beyond the perturbative regime  4  . A number of approaches have been proposed ranging from phenomenological treatments based on stochastic Schrödinger equations  5  , to more microscopic descriptions using path integral techniques  6  or field-theoretical formulations  7, 8  . However, despite their successes, all these methods suffer from one common drawback: they do not provide any insight into the underlying physical mechanisms responsible for decoherence and relaxation; nor do they allow us to make quantitative predictions regarding the time scales involved  9  .\nRecently, several authors  10 -12  have suggested that the problem may be tackled within the framework of quantum mechanics itself. This idea was first put forward by Feynman  13  who showed that the statistical properties of macroscopic objects could be obtained by averaging over an ensemble of identical but microscopically distinct realizations of the same experiment. More recently, Leggett  14  introduced a method...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Quantum mechanical method to decoherence and relaxation generated by fluctuating conditions . Abstract : We present an precise quantum - mechanical explanation for the dynamics of open systems in which the system is linked to many harmonic oscillators describing its surrounding environment .We see that , under certain conditions , this description can be reduced exactly into a master equation with Lindblad form . The resulting master equations are applied to study the effects of environmental fluctuations on the evolution of the density graph explaining the state of the system .In particular we study two different models of environments corresponding to Ohmic dissipation and spin - boson collision respectively . For both cases it is demonstrated how the impact of the surroundings leads to irreversible loss of information about the first state of the system as well as to thermalization at late times .Finally , we discuss possible applied of our findings to problems such as transport through mesoscopic conductors or dissipative tunneling between restricted states in disordered solids . Decoherence and relaxation processes drive a crucial role in understanding the physics of open quantum systems 1 , 2 .These phenomena arise when the process interacts with some external degrees of autonomy ( climate ) whose influence cannot be forgotten 3 . In recent years there has been substantial interest in establishing theoretical methods capable of addressing these influences beyond the perturbative regime 4 .A variety of methods have been proposed ranging from phenomenological treatments based on stochastic Schrödinger coefficients 5 , to more microscopic descriptions using path integral methods 6 or field - theory formulations 7 , 8 . However , despite their successes , all these theories suffer from one common drawback : they do not offer any insight into the fundamental physical mechanisms involved for decoherence and relaxation ; nor do they allow us to make quantitative predictions regarding the period scales required 9 .Recently , various scientists 10 - 12 have suggested that the issue may be tackled within the framework of quantum mechanics itself . This idea was first put forward by Feynman 13 who demonstrated that the statistical characteristics of macroscopic objects may be obtained by averaging over an ensemble of different but microscopically different realizations of the same experiment .More recently , Leggett 14 proposed a technique . . .",
        "rewrite_text": "**Title:** Quantum Mechanical Approach to Decoherence and Relaxation Induced by Fluctuating Conditions\n\n**Abstract:** In this study, we provide a comprehensive quantum mechanical framework for understanding the dynamics of open systems that interact with a multitude of harmonic oscillators representing their surrounding environment. We demonstrate that, under specific conditions, this complex interaction can be accurately simplified into a master equation of Lindblad form. The derived master equations are utilized to investigate the influence of environmental fluctuations on the evolution of the system's density matrix, which encapsulates its quantum state. Our analysis focuses on two distinct environmental models: one characterized by Ohmic dissipation and the other by spin-boson collisions. In both scenarios, we reveal that the interaction with the environment results in an irreversible loss of information regarding the initial state of the system, ultimately leading to thermalization over extended time scales. Furthermore, we explore the potential applications of our findings in contexts such as transport phenomena in mesoscopic conductors and dissipative tunneling processes in disordered solids. The processes of decoherence and relaxation are pivotal for a deeper understanding of open quantum systems, as they emerge when the system interacts with external degrees of freedom, often referred to as the environment. Recent years have seen a surge of interest in developing theoretical approaches that can effectively address these interactions beyond the perturbative regime. Various methodologies have been proposed, ranging from phenomenological models based on stochastic Schrödinger equations to more fundamental descriptions employing path integral techniques and field-theoretic frameworks. However, a common limitation across these approaches is their inability to elucidate the underlying physical mechanisms driving decoherence and relaxation, nor do they facilitate quantitative predictions regarding the relevant time scales. Recent contributions suggest that this challenge may be addressed within the quantum mechanical paradigm itself, a notion initially posited by Feynman, who illustrated that the statistical properties of macroscopic systems can be derived by averaging over a diverse ensemble of microscopically distinct realizations of identical experiments. More contemporary proposals, such as those by Leggett, further advance this discourse, paving the way for a more profound understanding of these complex phenomena.",
        "ori-fast-z-score": -0.15339299776947407,
        "water-fast-z-score": 7.3747734363161435,
        "rewrite-fast-z-score": 0.9486832980505138
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Relating two standard notions of secrecy .\nAbstract:\nWe relate the notion of information-theoretic security to that of computational indistinguishability, and show how this relationship can be used in practice for proving security properties of cryptographic protocols.  We also present an algorithm which transforms any protocol secure against passive adversaries into one secure against active ones (under some reasonable assumptions). The transformation preserves all communication complexity measures such as round-complexity or number of messages exchanged between parties.   Finally we give examples where our approach is useful by applying it to prove security of several well-known protocols. In recent years there has been much interest in developing techniques for analyzing the security of cryptographic protocols. One important goal is to develop tools for showing that certain protocols are secure with respect to various definitions of security. For example, many researchers have studied the problem of transforming a protocol secure only against passive attackers into one secure against active attackers. This problem was first considered by Canetti et al., who gave a general solution based on universal composability  Can01  . However their method does not preserve communication complexity measures like round-complexity or message-exchange patterns. Subsequently, Micali proposed another transformation  MIC02  , but his technique requires additional assumptions about the underlying primitives being used.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Relating two classic ideas of secrecy . Abstract : We relate the notion of information - theoretic integrity to that of computational indistinguishability , and suggest how this relationship can be used in practice for determination security characteristics of cryptographic protocols .We additionally offer an algorithm which transforms any protocol secure against passive adversaries into one stable against aggressive ones ( under some reasonable assumptions ) . The transformation maintains all transmission complexity estimates such as round - complexity or number of emails received between parties .Finally we give instance where our approach is utilized by using it to prove security of several well - famous protocols . In recent years there has been much interest in building methods for evaluating the safety of cryptographic protocols .One important aim is to develop tools for showing that particular protocols are secure with regard to several definitions of security . For instance , many scientists have researched the question of converting a protocol secure only against passive defenders into one secure against active attackers .This problem was first investigated by Canetti et al . , who offered a general solution based on universal composability Can01 . However their method does not retain transmission diversity measures like round - complexity or message - transfer patterns .Subsequently , Micali proposed another transformation MIC02 , but his method needs additional constraints about the underlying primitives being used .",
        "rewrite_text": "In this article, we explore the connection between the concepts of information-theoretic integrity and computational indistinguishability, highlighting how this relationship can be practically applied to assess the security characteristics of cryptographic protocols. We propose an algorithm that effectively transforms any protocol designed to withstand passive adversaries into one that is resilient against more aggressive threats, provided certain reasonable assumptions are met. Notably, this transformation preserves key transmission complexity metrics, including round complexity and the volume of messages exchanged between parties.\n\nThe growing interest in developing methodologies for evaluating the security of cryptographic protocols has led to significant advancements in the field. A primary objective has been to create tools that can demonstrate the security of specific protocols in relation to various security definitions. A critical area of research has focused on the challenge of converting protocols that are secure only against passive adversaries into those that can withstand active attacks. This issue was first addressed by Canetti et al., who proposed a comprehensive solution based on the framework of universal composability. However, their approach does not maintain the diversity of transmission metrics, such as round complexity and message transfer patterns.\n\nIn response to these limitations, Micali introduced an alternative transformation method, although it requires additional constraints regarding the underlying primitives utilized. Our work builds upon these foundational ideas, providing a novel approach that not only enhances the security of cryptographic protocols but also retains essential transmission characteristics. We illustrate the practical application of our method by demonstrating its effectiveness in proving the security of several well-known protocols. Through this research, we aim to contribute to the ongoing discourse on cryptographic security and offer valuable insights for future developments in the field.",
        "ori-fast-z-score": -0.7364596943186588,
        "water-fast-z-score": 7.12039324756716,
        "rewrite-fast-z-score": 0.8333333333333334
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Kalman--Yakubovich--Popov inequality for passive discrete time-invariant systems .\nAbstract:\nThe main result is the following theorem. \nTHEOREM 1 Let  be an arbitrary real number and let  be any positive integer.  Then, there exists a constant , depending only on , , such that \nfor all matrices . The proof of this theorem will be given in Section 2 below. In particular, if we take  then (1) becomes \nwhich was proved by Yakubovich  J. Math. Anal. Appl., vol. 16, pp. 719-726, (1964) . If we choose  then (1) reduces to \nwhich was established by Popov  Publ. Math. Inst. Steklov, vol. A, no. 3, p. 225-230, (1963)  . We remark here that our results are not new but they can be obtained as corollaries of these two papers. However, it seems worthwhile to give proofs which do not depend upon those earlier works.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Kalman - - Yakubovich - - Popov inequality for passive finite period - invariant networks . Abstract : The main consequence is the following theorem .THEOREM 1 Let be an arbitrary real number and take be any positive integer . Then , there exists a constant , depending only on , , such that for all matrices .The confirmation of this theorem will be provided in Section 2 below . In particular , if we took then ( 1 ) becomes which was demonstrated by Yakubovich J .Math.Anal.Appl . , vol . 16 , pp .719-726, (1964) .If we choose  then (1) reduces to \nwhich was established by Popov  Publ.Math.Inst.Steklov, vol.A, no.3 , p . 225 - 230 , ( 1963 ) . We comment here that our findings are not current but they can be obtained as corollaries of these two papers .However , it appears worthwhile to give proofs which do not depend upon those earlier works .",
        "rewrite_text": "Title: The Kalman-Yakubovich-Popov Inequality for Passive Finite Period-Invariant Networks\n\nAbstract: This article presents a significant theorem regarding the Kalman-Yakubovich-Popov inequality, specifically tailored for passive finite period-invariant networks. The theorem states that for any arbitrary real number and any positive integer, there exists a constant that is solely dependent on these parameters, applicable to all matrices in consideration. A detailed proof of this theorem is provided in Section 2 of the paper. Notably, when the parameter is set to a specific value, the theorem simplifies to a form that was previously established by Yakubovich in his work published in the Journal of Mathematical Analysis and Applications, where he demonstrated a related inequality. Furthermore, by selecting another particular value for the parameter, the theorem further reduces to a result originally proven by Popov in his publication in the Proceedings of the Steklov Institute of Mathematics. While the results presented in this article are not novel in the sense that they can be derived as corollaries from Yakubovich's and Popov's earlier findings, the authors argue that it is valuable to provide independent proofs that do not rely on these prior works. This approach not only reinforces the validity of the theorem but also contributes to the understanding of the Kalman-Yakubovich-Popov inequality in the context of passive finite period-invariant networks, thereby enriching the existing literature on the subject. The implications of this theorem are discussed, highlighting its relevance in various applications within control theory and network analysis.",
        "ori-fast-z-score": -1.1094003924504583,
        "water-fast-z-score": 3.159292970819848,
        "rewrite-fast-z-score": 0.6
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  How Mergers May Affect The Mass Scaling Relations Between Black Holes, Galaxies, and Other Gravitationally Bound Systems .\nAbstract:\nWe present an analysis of the effects that mergers may have on scaling relations between black holes (BHs), galaxies, and other gravitationally bound systems. We use cosmological simulations to study how BH mass is related to galaxy properties in different merger histories. Our results show that mergers can significantly affect these relationships by increasing scatter at fixed luminosity or stellar velocity dispersion. This effect is strongest for low-mass galaxies with high specific star formation rates. In addition, we find that mergers tend to increase the average BH-to-galaxy mass ratio as well as the fraction of active galactic nuclei (AGNs) among massive galaxies. These findings are consistent with observations of AGN host galaxies. Finally, our results suggest that the observed correlation between supermassive BH masses and bulge properties could be driven primarily by the fact that both grow during major mergers. Keywords: Galaxy evolution; Supermassive black hole",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : How Mergers May Affect The Mass Scaling Relations Between Black Holes , Galaxies , and Other Gravitationally Bound Systems . Abstract : We present an assessment of the effects that mergers might have on scaling connections between black holes ( BHs ) , galaxies , and other gravitationally bound structures .We use cosmological simulations to study how BH weight is related to galaxy structures in different merger histories . Our results show that mergers can significantly affect these interactions by expanding scatter at fixed luminosity or stellar velocity dispersion .This phenomenon is strongest for low - density nuclei with high specific star formation rates . In addition , we find that mergers prefer to raise the average BH - to - star mass ratio as well as the fraction of active galactic nuclei ( AGNs ) among huge clusters .These conclusions are compatible with observations of AGN host galaxies . Finally , our findings show that the reported correlation between supermassive BH masses and bulge properties might be motivated primarily by the fact that both expand during major mergers .Keywords : Galaxy evolve ; Supermassive black hole",
        "rewrite_text": "Title: How Mergers May Influence the Mass Scaling Relations Among Black Holes, Galaxies, and Other Gravitationally Bound Systems\n\nAbstract: In this study, we investigate the impact of mergers on the scaling relationships that exist between black holes (BHs), galaxies, and various other gravitationally bound systems. Utilizing advanced cosmological simulations, we analyze the correlation between the mass of black holes and the structural properties of galaxies across different merger histories. Our findings indicate that mergers play a crucial role in altering these relationships, particularly by increasing the scatter observed at fixed luminosity or stellar velocity dispersion. This effect is most pronounced in low-density nuclei that exhibit high specific star formation rates. Furthermore, our analysis reveals that mergers tend to elevate the average ratio of black hole mass to stellar mass, as well as the prevalence of active galactic nuclei (AGNs) within massive galaxy clusters. These results align with existing observational data regarding AGN host galaxies, suggesting a deeper connection between these phenomena. Ultimately, our research implies that the well-documented correlation between the masses of supermassive black holes and the properties of galactic bulges may largely stem from the simultaneous growth of both entities during significant merger events. This work enhances our understanding of the dynamic interplay between black holes and their host galaxies, particularly in the context of cosmic evolution and structure formation. \n\nKeywords: Galaxy evolution; Supermassive black holes; Mergers; Active galactic nuclei; Cosmological simulations.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.512930714537517,
        "rewrite-fast-z-score": 0.8542421961772492
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Symmetric and asymmetric solitons in linearly coupled Bose-Einstein condensates trapped in optical lattices .\nAbstract:\nWe study the dynamics of two interacting bosonic species confined to an optical lattice, with one species being initially prepared as a coherent state at each site while the other is initially prepared as a thermal cloud. We show that this system supports both symmetric and asymmetric soliton solutions which are stable against small perturbations for certain values of the chemical potentials. The stability properties of these solitons can be understood by studying their linearization spectrum around the stationary states. In particular we find that the presence of a finite temperature leads to additional unstable modes associated with phonon-like excitations. Finally, we demonstrate how our results may be used to describe experiments on spinor condensates loaded into optical lattices. Introduction:-Recent experimental advances have made it possible to create quantum degenerate gases consisting of several different atomic species  1  . These systems provide new opportunities to explore novel phenomena such as supersolids  2  , phase separation  3  or spin-orbit coupling  4  .\nIn this work we consider a particularly interesting example where there exist two distinct types of particles (e.g., atoms) which interact via s-wave scattering but differ in mass and/or internal structure  5  . This situation arises naturally when considering mixtures of hyperfine states  6  or isotopes  7, 8  within the same atom type  9  . For instance, recent experiments involving 87 Rb and 41 K  10  have demonstrated the formation of a mixture of two different hyperfine states after evaporative cooling  11  . Another possibility would involve using 40 K and 6 Li  12  . Here, the lighter species could be considered as impurities immersed in a background gas of heavier fermions  13  . Alternatively, if the masses were reversed then the heavy species could act as impurities  14  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Symmetric and asymmetric solitons in linearly coupled Bose - Einstein condensates trapped in optical lattices . Abstract : We research the dynamics of two interacting bosonic species confined to an optical lattice , with one species being initially made as a coherent state at each site while the other is initially prepared as a heat bubble .We see that this scheme accepts both symmetric and asymmetric soliton solutions which are stable against small perturbations for particular values of the chemical potentials . The stability properties of these solitons can be understood by examining their linearization spectrum around the stationary states .In particular we find that the presence of a finite temperature leads to extra unstable modes associated with phonon - like excitations . Finally , we prove how our findings may be used to explain studies on spinor condensates stacked into optical lattices .Introduction : - Recent scientific discoveries have enabled it able to create quantum degenerate gases composed of several different atomic species 1 . These systems create fresh possibilities to examine novel processes such as supersolids 2 , phase splitting 3 or spin - orbit coupling 4 .In this study we imagine a particularly important example where there exist two different kinds of atoms ( e . g . , atoms ) which interact via s - wave scattering but differ in mass and / or internal structure 5 . This condition occurs naturally when examining mixtures of hyperfine states 6 or isotopes 7 , 8 within the same atom type 9 .For instance , recent experiments using 87 Rb and 41 K 10 have demonstrated the formation of a mixture of two different hyperfine states after evaporative cooling 11 . Another possibility would include utilizing 40 K and 6 Li 12 .Here , the lighter species may be regarded as impurities immersed in a background gas of darker fermions 13 . Alternatively , if the masses were reversed then the heavy species may function as impurities 14 .",
        "rewrite_text": "**Title:** Symmetric and Asymmetric Solitons in Linearly Coupled Bose-Einstein Condensates Trapped in Optical Lattices\n\n**Abstract:** This study investigates the dynamics of two interacting bosonic species confined within an optical lattice framework. One species is initially prepared as a coherent state at each lattice site, while the other is set up as a thermal bubble. Our findings reveal that this configuration supports both symmetric and asymmetric soliton solutions, which exhibit stability against minor perturbations for specific chemical potential values. To understand the stability characteristics of these solitons, we analyze their linearization spectrum around the stationary states. Notably, we observe that the introduction of finite temperature introduces additional unstable modes linked to phonon-like excitations. Furthermore, we demonstrate how our results can elucidate existing research on spinor condensates arranged in optical lattices.\n\n**Introduction:** Recent advancements in scientific research have facilitated the creation of quantum degenerate gases composed of multiple atomic species. These systems open new avenues for exploring innovative phenomena such as supersolids, phase separation, and spin-orbit coupling. In this paper, we focus on a significant scenario involving two distinct atomic species that interact through s-wave scattering, differing in mass and/or internal structure. This situation is commonly encountered in mixtures of hyperfine states or isotopes of the same atomic species. For example, recent experiments with rubidium-87 and potassium-41 have successfully demonstrated the formation of a mixture of different hyperfine states following evaporative cooling. Another potential combination could involve potassium-40 and lithium-6, where the lighter species acts as impurities within a background of heavier fermions. Conversely, if the mass distribution were reversed, the heavier species could serve as impurities. This research contributes to a deeper understanding of the complex dynamics present in multi-species quantum systems.",
        "ori-fast-z-score": -0.6713450866373513,
        "water-fast-z-score": 5.564047150200578,
        "rewrite-fast-z-score": 2.477637207378607
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Power Allocation for Discrete-Input Delay-Limited Fading Channels .\nAbstract:\nWe consider the problem of power allocation in discrete-input delay-limited fading channels with multiple users and time-varying channel gains. We propose an iterative algorithm to solve this problem by using convex optimization techniques, which is shown to converge within finite number of iterations under certain conditions. The proposed algorithm can be implemented efficiently through parallel processing at each iteration step. Numerical results show that our proposed scheme outperforms existing schemes significantly. \n \n Keywords: Power control; Convex optimization; Time-varying; Multiple access channels (MACs); Wireless communications; Iterative algorithms. 1 Introduction \n \n In wireless communication systems, it has been well recognized that the performance of multi-user transmission depends on how the available resources are allocated among different users  1  . For example, when there exist multiple users sharing a common radio resource such as bandwidth or transmit power, the optimal way to allocate these resources may depend on the specific system settings  2  , e.g., whether the users have equal priority  3  , what type of services they request  4  , etc.. Therefore, efficient resource allocation strategies should take into account all relevant factors so as to maximize overall network utility  5  .\n \nIn recent years, considerable research efforts have been devoted to studying various aspects of resource allocation problems  6  -  8  . Among them, power allocation plays an important role due to its direct impact on both spectral efficiency and energy consumption  9  . However, most previous works assume continuous input alphabets  10  -  12  , while practical digital modulation schemes usually employ discrete constellations  13  . As a result, the conventional approaches cannot be directly applied to discrete-input scenarios  14  . To address this issue, several studies  15  -  17  have investigated the power allocation problem over discrete-input channels recently. Nevertheless, their solutions either require high computational complexity  16  or suffer from slow convergence speed  17  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Power Allocation for Discrete - Input Delay - Limited Fading Channels . Abstract : We consider the issue of power distribution in discrete - input delay - limited fading channels with many users and period - differing channel gains .We suggest an iterative algorithm to solve this question by using convex optimization technology , which is demonstrated to converge within finite number of iterations under certain conditions . The proposed algorithm can be applied efficiently through concurrent processing at each iteration step .Numerical results show that our proposed system outperforms previous schemes considerably . Keywords : Power control ; Convex optimization ; Time - changing ; Multiple access outlets ( MACs ) ; Wireless communications ; Iterative techniques .1 Introduction In wireless communication technologies , it has been widely recognized that the performance of multi - person transmission relies on how the provided resources are assigned among different users 1 . For instance , when there reside several users sharing a common radio asset such as bandwidth or transmit energy , the ideal means to allocate these resources may depend on the specific system settings 2 , e . g . , whether the operators have equal priority 3 , what type of solutions they demand 4 , etc . .Therefore , efficient resource expenditure strategies should take into consideration all relevant variables so as to maximize overall network utility 5 . In recent years , substantial work programs have been focused to researching various parts of resource transfer problems 6 - 8 .Among them , electricity allocation takes an important role owing to its significant effect on both spectral capacity and energy consumption 9 . However , most prior efforts assume continuous output alphabets 10 - 12 , while commercial digital modulation schemes typically employ discrete constellations 13 .As a result , the usual techniques unable be effectively used to discrete - input situations 14 . To address this question , various studies 15 - 17 have analyzed the power allocation problem over discrete - input channels recently .Nevertheless , their solutions either need strong mathematical capacity 16 or suffer from slow convergence speed 17 .",
        "rewrite_text": "**Title:** Power Allocation for Discrete-Input Delay-Limited Fading Channels\n\n**Abstract:** This paper addresses the challenge of power allocation in discrete-input, delay-limited fading channels characterized by multiple users and varying channel gains. We propose an innovative iterative algorithm that leverages convex optimization techniques to effectively tackle this problem. Our approach is designed to converge within a finite number of iterations, contingent upon specific conditions being met. Notably, the algorithm allows for efficient concurrent processing at each iteration, enhancing its practicality in real-world applications. Through extensive numerical simulations, we demonstrate that our proposed method significantly outperforms existing power allocation schemes, showcasing its superiority in managing resources in complex communication environments. \n\nIn the realm of wireless communication, the allocation of resources among multiple users is crucial for optimizing system performance. The distribution of shared resources, such as bandwidth and transmit power, can vary based on several factors, including user priority and the nature of the required solutions. Therefore, effective resource allocation strategies must consider all pertinent variables to maximize overall network utility. Recent research has increasingly focused on various aspects of resource allocation, with power distribution being a critical area due to its direct impact on both spectral efficiency and energy consumption. \n\nHowever, most existing studies have concentrated on scenarios involving continuous output alphabets, while practical digital modulation techniques typically utilize discrete constellations. This discrepancy has rendered traditional methods ineffective for discrete-input scenarios. To fill this gap, recent investigations have explored power allocation in discrete-input channels, yet many of these solutions either demand complex mathematical frameworks or exhibit slow convergence rates. Our work contributes to this field by providing a robust and efficient algorithm that addresses these limitations, paving the way for improved performance in wireless communication systems.\n\n**Keywords:** Power control; Convex optimization; Time-varying channels; Multiple access channels (MACs); Wireless communications; Iterative methods.",
        "ori-fast-z-score": -1.7386365758424454,
        "water-fast-z-score": 8.894477603748037,
        "rewrite-fast-z-score": -1.1211526391279039
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Calibration of Mid-Infrared Star Formation Rate Indicators .\nAbstract:\nWe present the results of an analysis aimed at calibrating mid-infrared (MIR) star formation rate indicators using data obtained with Spitzer and Herschel Space Telescopes, as well as ground-based facilities such as the Infrared Array Camera onboard the Hubble Space Telescope.  We use a sample of galaxies selected to have high signal-to-noise ratio detections in all four IRAC bands (3.6-8 microns), which are used for estimating total infrared luminosities via SED fitting techniques. The sample consists of 29 nearby star-forming galaxies that span a wide range of physical properties including stellar mass, dust temperature, metallicity, and specific star formation rates. Using these observations we derive empirical relations between MIR emission features and total infrared luminosity, which can be used to estimate LIR over a large dynamic range without requiring any assumptions about the shape or slope of galaxy SEDs.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Calibration of Mid - Infrared Star Formation Rate Indicators . Abstract : We present the conclusion of an assessment aimed at calibrating mid - infrared ( MIR ) star formation rate indicators using data acquired with Spitzer and Herschel Space Telescopes , as well as land - based equipment such as the Infrared Array Camera onboard the Hubble Space Telescope .We use a sample of stars selected to have high signal - to - noise ratio detections in all four IRAC bands ( 3 . 6 - 8 microns ) , which are using for estimating actual infrared luminosities via SED fitting methods . The sample consists of 29 nearby star - creating stars that cover a broad variety of physical properties including stellar mass , dust temperature , metallicity , and particular galaxy formation rates .Using these observations we derive empirical relations between MIR emission elements and total infrared luminosity , which can be used to estimate LIR over a large dynamic range without using any constraints about the form or slope of galaxy SEDs .",
        "rewrite_text": "We present the findings of a comprehensive study focused on calibrating mid-infrared (MIR) star formation rate indicators. This research utilizes data obtained from both the Spitzer and Herschel Space Telescopes, as well as ground-based instruments, including the Infrared Array Camera on the Hubble Space Telescope. Our analysis is based on a carefully selected sample of stars that exhibit high signal-to-noise ratio detections across all four IRAC bands, specifically in the wavelength range of 3.6 to 8 microns. These bands are instrumental in estimating the actual infrared luminosities through spectral energy distribution (SED) fitting techniques.\n\nThe sample comprises 29 nearby star-forming regions, showcasing a diverse array of physical characteristics such as stellar mass, dust temperature, metallicity, and specific galaxy formation rates. By leveraging these observations, we establish empirical relationships between various components of MIR emission and total infrared luminosity. These relationships provide a robust framework for estimating the total infrared luminosity (LIR) across a wide dynamic range, without imposing any restrictions on the form or slope of the galaxy's SEDs.\n\nOur findings contribute significantly to the understanding of star formation processes in the universe and offer valuable tools for astronomers seeking to measure star formation rates in different galactic environments. The calibration of MIR indicators enhances the accuracy of star formation rate estimations, facilitating further research into the evolution of galaxies and their stellar populations. This work underscores the importance of utilizing advanced observational techniques and diverse datasets to refine our understanding of the complex interplay between dust, stars, and their environments in the cosmos.",
        "ori-fast-z-score": -0.5773502691896257,
        "water-fast-z-score": 4.444462481925879,
        "rewrite-fast-z-score": 0.6311687442672026
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  X-ray Timing Observations of PSR J1930+1852 in the Crab-like SNR G54.1+0.3 .\nAbstract:\nWe report on X-ray timing observations of the pulsar candidate PSR J1930+1855 located at the center of the supernova remnant (SNR) G54.1+0. \n \n The source was discovered by Chandra and confirmed as a pulsar with XMM-Newton, but its spin period is not stable over time scales longer than one day. We performed two sets of pointed RXTE observations to study this behavior further. In both cases we found that the pulse frequency decreases smoothly during our observation runs. This trend can be described well using an exponential decay model for which we find characteristic timescales of 1.1 days and 0.7 days respectively. These values are consistent with those reported previously based on Chandra data alone. However, when comparing these results directly it should be noted that the uncertainties associated with the previous measurements were significantly larger due to the lower signal-to-noise ratio achieved with Chandra compared to RXTE.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : X - ray Timing Observations of PSR J1930 + 1852 in the Crab - like SNR G54 . 1 + 0 . 3 . Abstract : We report on X - ray timing observations of the pulsar candidate PSR J1930 + 1855 centered at the center of the supernova remnant ( SNR ) G54 . 1 + 0 .The source was studied by Chandra and reported as a pulsar with XMM - Newton , but its spin time is not stable over time ranges less than one day . We conducted two sets of pointed RXTE observations to study this behavior further .In both cases we concluded that the pulse frequency drops slowly during our observation running . This trend can be described good using an exponential decay model for which we find typical timescales of 1 . 1 hours and 0 . 7 days respectively .These figures are compatible with those published previously based on Chandra data alone . However , when comparing these results directly it should be mentioned that the uncertainties involved with the previous tests were significantly larger thanks to the smaller signal - to - noise proportion attained with Chandra compared to RXTE .",
        "rewrite_text": "We present findings from our X-ray timing observations of the pulsar candidate PSR J1930+1855, located at the center of the supernova remnant (SNR) G54.1+0.3. Previous studies, including those conducted with Chandra and XMM-Newton, identified this source as a pulsar; however, it was noted that its spin period exhibits instability over intervals shorter than one day. To investigate this phenomenon in greater detail, we performed two sets of targeted observations using the Rossi X-ray Timing Explorer (RXTE). Our analysis revealed a consistent trend of a gradual decrease in pulse frequency throughout the duration of our observations. This decline can be effectively modeled using an exponential decay function, yielding characteristic timescales of approximately 1.1 hours and 0.7 days for the two observation sets, respectively. These results align well with earlier findings derived from Chandra data, although it is important to note that the uncertainties associated with those earlier measurements were considerably larger. This discrepancy is attributed to the lower signal-to-noise ratio achieved with Chandra compared to the more sensitive RXTE observations. Our study enhances the understanding of the timing behavior of PSR J1930+1855 and contributes to the broader knowledge of pulsar dynamics within supernova remnants. The implications of these findings may provide insights into the mechanisms driving pulsar spin evolution and the characteristics of the surrounding environment in SNRs.",
        "ori-fast-z-score": 0.3464101615137754,
        "water-fast-z-score": 5.9648090806346055,
        "rewrite-fast-z-score": 1.2
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Space Velocities of Southern Globular Clusters. V. A Low Galactic Latitude Sample .\nAbstract:\nWe have measured the space velocities for eight globular clusters in the southern hemisphere with galactic latitudes less than 20 degrees, using proper motions and radial velocities obtained by various authors over the past decade or so.  The sample includes four open clusters (NGC 2420 , NGC 2516 , NGC 2682 , and NGC 6705 ) as well as four globulars . We find that all but one cluster are consistent with being at rest relative to the local standard of rest; however, we also find evidence that two of these clusters may be on orbits which will take them out of our galaxy within several billion years .  These results suggest that there is no significant difference between open clusters and globulars when it comes to their kinematics ; both types appear to share similar properties .\nThe only exception appears to be the open cluster M67 , whose velocity vector points away from us toward the constellation Cetus . This result suggests that this open cluster has been ejected from its parent galaxy during an encounter with another galaxy some time ago .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title:  Space Velocities of Southern Globular Clusters.V. A Low Galactic Latitude Sample .Abstract : We have recorded the space velocities for eight globular complexes in the southern hemisphere with galactic latitudes less than 20 degrees , using proper motions and radial velocities collected by various authors over the previous decade or so . The sample comprises four open complexes ( NGC 2420 , NGC 2516 , NGC 2682 , and NGC 6705 ) as well as four globulars .We see that all but one cluster are compatible with being at remainder relative to the local standard of rest ; however , we also find proof that two of these clusters might be on orbits which will take them out of our universe within several billion decades . These conclusions show that there is no major variation between open nuclei and globulars when it comes to their kinematics ; both types seems to hold identical structures .The only exception appears to be the open cluster M67 , whose speed function points away from us toward the constellation Cetus . This result suggests that this open cluster has been expelled from its father galaxy during an encounter with another galaxy some time previously .",
        "rewrite_text": "Title: Space Velocities of Southern Globular Clusters: A Low Galactic Latitude Sample\n\nAbstract: In this study, we present the space velocities of eight globular complexes located in the southern hemisphere, specifically those with galactic latitudes below 20 degrees. Utilizing proper motions and radial velocities compiled from various sources over the past decade, our sample includes four open clusters—NGC 2420, NGC 2516, NGC 2682, and NGC 6705—as well as four globular clusters. Our analysis reveals that, with the exception of one cluster, all others are consistent with being at rest relative to the local standard of rest. Notably, we provide evidence that two of these clusters may be on trajectories that could eventually lead them to escape our universe within several billion years. These findings indicate that there is no significant difference in the kinematic properties of open clusters compared to globular clusters; both types exhibit similar structural characteristics. The sole outlier in our observations is the open cluster M67, which displays a velocity vector directed away from us towards the constellation Cetus. This particular result implies that M67 may have been ejected from its parent galaxy as a result of a past interaction with another galaxy. Overall, our research contributes to the understanding of the dynamics of southern globular complexes and highlights the intricate relationships between different types of star clusters in the galaxy.",
        "ori-fast-z-score": -1.9126494315742406,
        "water-fast-z-score": 4.2485291572496005,
        "rewrite-fast-z-score": -1.4288690166235207
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Displacement of the Sun from the Galactic Plane .\nAbstract:\nWe present new results on the displacement of the Sun from the galactic plane based on Hipparcos data and recent determinations of the solar motion with respect to the local standard of rest (LSR). We find that the Sun is displaced by about 0.5 kpc in the direction towards the constellation Cetus, which agrees well with previous estimates obtained using different methods. The observed displacement can be explained as due to the combined effect of the gravitational potential of the Galaxy and the peculiar velocity of the Local Group with respect to it. \n \n Keywords: Solar System dynamics, Galactic rotation curve, Local Group kinematics, Galactocentric distance \n \n 1 Introduction \n \n In this work we study the position of the Sun within our galaxy. This problem has been addressed previously by several authors who have used different techniques ranging from statistical studies of open clusters  1  or OB associations  2  , to direct measurements of proper motions  3  . Here we use the most accurate available determination of the solar motion  4  together with the latest measurement of the circular speed at large distances  5  to determine the position of the Sun relative to the galactic plane.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Displacement of the Sun from the Galactic Plane . Abstract : We report new data on the displacement of the Sun from the galactic plane based on Hipparcos results and recent determinations of the sun motion with regard to the local standard of rest ( LSR ) .We see that the Sun is displaced by about 0 . 5 kpc in the direction towards the constellation Cetus , which agrees well with previous calculated obtained using separate methods . The observed displacement can be understood as owing to the combined influence of the gravitational potential of the Galaxy and the peculiar speed of the Local Group with regard to it .Keywords : Solar System dynamics , Galactic rotation curve , Local Group kinematics , Galactocentric distance 1 Introduction In this research we study the position of the Sun within our universe . This problem has been addressed previously by various scientists who have utilized varying techniques ranging from statistical analyses of close galaxies 1 or OB associations 2 , to direct measurements of proper motions 3 .Here we using the most accurate available determination of the sun motion 4 together with the latest measurement of the circular distance at large distances 5 to estimate the orientation of the Sun relative to the galactic plane .",
        "rewrite_text": "Title: Displacement of the Sun from the Galactic Plane\n\nAbstract: In this study, we present new findings regarding the Sun's displacement from the galactic plane, utilizing data from the Hipparcos satellite and recent assessments of the Sun's motion relative to the local standard of rest (LSR). Our analysis indicates that the Sun is positioned approximately 0.5 kpc above the galactic plane, directed towards the constellation Cetus. This measurement aligns closely with previous calculations derived from various methodologies, reinforcing the reliability of our results. The observed displacement can be attributed to the interplay between the gravitational potential of the Milky Way and the peculiar velocity of the Local Group in relation to the galaxy. \n\nThe investigation of the Sun's position within the galaxy is not a novel endeavor; it has been explored by numerous researchers employing diverse techniques, including statistical analyses of nearby galaxies and OB associations, as well as direct measurements of stellar proper motions. In this work, we leverage the most precise available determinations of the Sun's motion, in conjunction with the latest measurements of the circular distance at greater scales, to accurately assess the Sun's orientation with respect to the galactic plane. Our findings contribute to a deeper understanding of solar system dynamics, the galactic rotation curve, and the kinematics of the Local Group, ultimately enhancing our comprehension of the Sun's galactocentric distance and its implications for the broader structure of our galaxy. \n\nKeywords: Solar System dynamics, Galactic rotation curve, Local Group kinematics, Galactocentric distance.",
        "ori-fast-z-score": -0.52999894000318,
        "water-fast-z-score": 5.7655666019705505,
        "rewrite-fast-z-score": 1.0945409092309881
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Chandra archival study of the temperature and metal abundance profiles in hot Galaxy Clusters at 0.1 < z < 0.3 .\nAbstract:\nWe have analyzed Chandra observations for eight galaxy clusters with redshifts between 0.1 and 0.3 to determine their radial temperature, density, pressure, entropy, cooling time, and metallicity profiles. We find that all these quantities are well described by single-parameter scaling relations as functions of radius r normalized by the virial radius Rvir.  The best-fit values of the normalization parameters depend on redshift but not significantly so; we therefore adopt fixed values based on our results for the two most distant clusters (z = 0.2 and 0.3) which yield good fits to the other six clusters. Our main conclusions are:  1. All cluster properties show significant evolution out to z ~ 0.3; this is consistent with previous studies using XMM data. 2. The gas fraction fgas(r/Rvir), defined as the ratio of the total thermal energy within a sphere of radius r to its gravitational binding energy, decreases monotonically outwards; it also shows some evidence for evolution with redshift. 3. The electron number density ne(r) increases inwardly toward the center of each cluster until reaching a peak value near r ~ 0.1r200 where r200 denotes the radius enclosing an average overdensity of 200 times the critical density of the universe. Beyond this point, ne(r) declines slowly or remains roughly constant depending on the cluster. 4. The mean molecular weight µe(r) increases outwardly due to the increasing contribution of helium ions relative to hydrogen atoms. 5. The central temperatures T0 inferred from spectral fitting range from 6 keV to 12 keV, while those obtained directly from the deprojected temperature profile lie in the range 7-15 keV. These differences may be caused by non-thermal components such as AGN jets and/or magnetic fields.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Chandra archival analysis of the temperature and metal availability profiles in hotter Galaxy Clusters at 0 . 1 < z < 0 . 3 . Abstract : We have analyzed Chandra measurements for eight galaxy galaxies with redshifts between 0 . 1 and 0 . 3 to find their radial temperature , density , pressure , entropy , cooling period , and metallicity profiles .We see that all these quantities are better represented by single - parameter scaling relations as functions of radius r normalized by the virial diameter Rvir . The best - fitting values of the normalization values rely on redshift but not considerably so ; we thus choose fixed values based on our findings for the two most distant clusters ( z = 0 . 2 and 0 . 3 ) which provide better fits to the other six clusters .Our main results are : 1 . All cluster elements exhibit substantial development out to z ~ 0 . 3 ; this is consistent with previous research used XMM data .2 . The gas fraction fgas ( r / Rvir ) , defined as the proportion of the total heat power within a sphere of radius r to its gravitational binding energy , decreases monotonically outwards ; it also shows some evidence for evolution with redshift .3 . The electron number density ne ( r ) rises inwardly toward the center of each cluster until reaching a peak value near r ~ 0 . 1r200 where r200 denotes the radius enclosing an mean overdensity of 200 times the critical density of the universe .Beyond this point , ne ( r ) declines slowly or remains relatively constant depending on the cluster . 4 .The mean molecular weight µe ( r ) rises outwardly due to the increasing impact of helium ions relative to hydrogen atoms . 5 .The central temperatures T0 inferred from spectral fit reach from 6 keV to 12 keV , while those generated directly from the deprojected temperature profile lie in the range 7 - 15 keV . These changes may be caused by non - cooling systems such as AGN rockets and / or magnetic fields .",
        "rewrite_text": "**Title:** A Chandra Archival Analysis of Temperature and Metal Availability Profiles in Hotter Galaxy Clusters at 0.1 < z < 0.3\n\n**Abstract:** In this study, we conducted a comprehensive analysis of Chandra X-ray Observatory data for eight galaxy clusters with redshifts ranging from 0.1 to 0.3. Our objective was to investigate the radial profiles of temperature, density, pressure, entropy, cooling time, and metallicity within these clusters. We found that these physical quantities can be effectively described by single-parameter scaling relations as functions of the radius \\( r \\), normalized by the virial radius \\( R_{vir} \\). The normalization constants derived from our analysis show a dependence on redshift, albeit not significantly. Consequently, we adopted fixed normalization values based on our observations of the two most distant clusters (at \\( z = 0.2 \\) and \\( z = 0.3 \\)), which yielded improved fits for the remaining six clusters.\n\nOur key findings include: (1) All clusters demonstrate significant evolution up to \\( z \\sim 0.3 \\), corroborating previous studies utilizing XMM data. (2) The gas fraction \\( f_{gas}(r/R_{vir}) \\), defined as the ratio of the total thermal energy within a sphere of radius \\( r \\) to its gravitational binding energy, exhibits a monotonically decreasing trend with increasing radius, along with indications of redshift evolution. (3) The electron number density \\( n_e(r) \\) increases towards the cluster center, peaking at approximately \\( r \\sim 0.1 r_{200} \\), where \\( r_{200} \\) is the radius corresponding to a mean overdensity of 200 times the critical density of the universe. Beyond this peak, \\( n_e(r) \\) either declines gradually or remains relatively stable, depending on the specific cluster. (4) The mean molecular weight \\( \\mu_e(r) \\) increases outward, reflecting the growing influence of helium ions in comparison to hydrogen atoms. (5) The central temperatures \\( T_0 \\), derived from spectral fitting, range from 6 keV to 12 keV, while those obtained from the deprojected temperature profiles span 7 to 15 keV. These variations may be attributed to the influence of non-cooling mechanisms, such as active galactic nuclei (AGN) feedback and magnetic fields.",
        "ori-fast-z-score": -0.3287979746107146,
        "water-fast-z-score": 6.368673331236264,
        "rewrite-fast-z-score": 1.5461980716652028
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Kinematic design of a 3-dof Hybrid Manipulator .\nAbstract:\nThe kinematics and dynamics analysis are performed for the hybrid manipulator with three degrees-of-freedom (3-DOF). The proposed manipulator consists of two serial chains, one is driven by an electric motor through gears while another chain is connected to it via spherical joints. In order to obtain the forward kinematics solution, the Denavit-Hartenberg parameters have been used in this study. The inverse kinematics problem has also been solved using the Jacobian matrix method. Finally, the dynamic model of the manipulator was developed based on Lagrange s equation. The results show that the designed manipulator can be applied as a robotic arm in various applications such as industrial robots or rehabilitation devices. Keywords: Forward kinematics, Inverse kinematics, Dynamics modeling, Robotics, Serial chain, Spherical joint, Electric motor, Gearbox, Manipulator, Degrees-of-freedom",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Kinematic design of a 3 - dof Hybrid Manipulator . Abstract : The kinematics and dynamics calculations are performed for the hybrid manipulator with three degrees - of - independence ( 3 - DOF ) .The proposed manipulator consists of two serial chains , one is powered by an electric motor through gears while another chain is linked to it via spherical joints . In order to obtain the forward kinematics solution , the Denavit - Hartenberg variables have been used in this study .The inverse kinematics problem has additionally been solved using the Jacobian matrix method . Finally , the dynamic model of the manipulator was developed using on Lagrange s equation .The results show that the designed manipulator can be applied as a robotic unit in different applications such as factory robots or reconstruction systems . Keywords : Forward kinematics , Inverse kinematics , Dynamics modeling , Robotics , Serial chain , Spherical joint , Electric car , Gearbox , Manipulator , Degrees - of - freedom",
        "rewrite_text": "**Title:** Kinematic Design of a 3-DOF Hybrid Manipulator\n\n**Abstract:** This article presents a comprehensive analysis of the kinematics and dynamics of a hybrid manipulator characterized by three degrees of freedom (3-DOF). The manipulator is ingeniously designed with two serial chains; one chain is actuated by an electric motor through a gearbox, while the other chain is interconnected via spherical joints. To derive the forward kinematics, the study employs the Denavit-Hartenberg convention, which facilitates the systematic representation of the manipulator's configuration. Additionally, the inverse kinematics problem is addressed through the application of the Jacobian matrix method, allowing for the determination of joint parameters required to achieve desired end-effector positions. \n\nThe dynamic modeling of the manipulator is constructed using Lagrange's equations, providing insights into the forces and torques necessary for its operation. The findings indicate that the proposed manipulator design is not only theoretically sound but also practically applicable in various fields, including industrial robotics and rehabilitation systems. The versatility of the manipulator makes it suitable for tasks ranging from assembly line operations to complex reconstruction processes. \n\nOverall, this research contributes to the field of robotics by enhancing the understanding of hybrid manipulator systems and their potential applications. The keywords associated with this study include forward kinematics, inverse kinematics, dynamics modeling, robotics, serial chain, spherical joint, electric motor, gearbox, manipulator, and degrees of freedom. The results underscore the significance of the hybrid manipulator as a valuable robotic unit capable of adapting to diverse operational environments.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 3.6536565724225296,
        "rewrite-fast-z-score": 1.3112201362143716
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Shaping the Globular Cluster Mass Function by Stellar-Dynamical Evaporation .\nAbstract:\nWe present an analysis of N-body simulations aimed at understanding how evaporation shapes the mass function (MF) of globular clusters (GCs). We find that, in agreement with previous studies, evaporation causes GCs to lose stars preferentially on their low-mass end and thus steepens the MF slope towards lower masses. However, we show that this effect is counteracted by two competing processes: dynamical friction which removes massive stars more efficiently than less massive ones; and relaxation-driven core collapse which increases the central density of the cluster and makes it harder for massive stars to escape. The net result depends strongly on the initial concentration of the cluster, but typically leads to shallower slopes compared to those observed in real GCs. This suggests that other processes are required to explain the shape of the observed MF. In particular, our results suggest that primordial binaries may be responsible for producing the high-mass power-law tail seen in many GCs.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Shaping the Globular Cluster Mass Function by Stellar - Dynamical Evaporation . Abstract : We present an assessment of N - bodies simulations aiming at studying how evaporation shapes the mass function ( MF ) of globular complexes ( GCs ) .We see that , in agreement with previous research , evaporation creates GCs to lose stars preferentially on their low - mass ending and therefore steepens the MF slope towards lesser masses . However , we find that this effect is counteracted by two different processes : dynamical friction which destroys massive galaxies more efficiently than less massive ones ; and relaxation - triggered disk collapse which increases the main abundance of the cluster and causes it difficult for huge stars to escape .The total result relies highly on the first abundance of the cluster , but typically leads to shallower slopes compared to those observed in real GCs . This implies that other processes are required to explain the form of the seen MF .In particular , our findings show that primordial binaries may be responsible for producing the high - mass power - law tail seen in large GCs .",
        "rewrite_text": "In this study, we investigate the influence of stellar-dynamical evaporation on the mass function (MF) of globular clusters (GCs) through a series of N-body simulations. Our results align with previous findings, indicating that evaporation predominantly affects the lower mass end of the GC population, leading to a steepening of the MF slope towards smaller masses. However, we also identify two significant counteracting processes that modify this trend. First, dynamical friction tends to preferentially disrupt more massive stars within clusters, thereby mitigating the loss of lower-mass stars. Second, relaxation-triggered disk collapse enhances the overall mass retention within clusters, making it more challenging for larger stars to escape the gravitational pull of the cluster. \n\nThe overall impact of these processes is heavily dependent on the initial mass distribution of the cluster. Typically, our simulations yield MF slopes that are shallower than those observed in actual globular clusters, suggesting that additional mechanisms are necessary to fully account for the observed mass function shapes. Notably, our research highlights the potential role of primordial binaries in generating the high-mass power-law tail commonly seen in larger globular clusters. These findings underscore the complexity of the processes shaping the mass function of globular clusters and point to the need for further investigation into the interplay of various dynamical effects and initial conditions in determining the final mass distributions of these stellar systems.",
        "ori-fast-z-score": -1.58999682000954,
        "water-fast-z-score": 5.617988764033708,
        "rewrite-fast-z-score": -0.7492686492653552
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  (Co)cyclic (co)homology of bialgebroids: An approach via (co)monads .\nAbstract:\nWe introduce the notion of an algebra over a monoidal category and show that it is equivalent to the notion of a coalgebra in the dual category, which we call a comonoid.  We then define the cyclic homology of such algebras as the Hochschild cohomology of their underlying comonoids with coefficients in the bimodule given by the tensor product of the algebra with its opposite algebra.  This definition generalizes the usual one for ordinary algebras over fields or rings.   In particular, if the base ring has characteristic zero, this recovers the classical definitions of cyclic homology and periodic cyclic homology.  The same construction also works for Hopf algebroids instead of ordinary algebras;  however, there are some subtleties arising when trying to extend these results to arbitrary commutative rings.    Finally, we give several examples illustrating our constructions. Cyclic homology was introduced by Connes in his seminal work on noncommutative geometry  Con  . It can be defined as the Hochschild homology of certain algebras called cyclic objects. These were first studied systematically by Bökstedt  Bök  , who showed how they could be used to construct new algebraic structures like crossed modules and group extensions. Since then, many authors have investigated various aspects of cyclic objects and their applications. For example, see  Fri1  ,  Fri2  ,  Koc  ,  Lau  ,  Maz  ,  Nee  ,  Sta  .\nIn this article, we will study cyclic objects in more detail using techniques developed recently in the theory of operads and monads. Our main result shows that any cyclic object gives rise to two different types of cyclic homologies, namely the cyclic homology of the underlying algebra and the periodic cyclic homology of the associated graded algebra. Moreover, both of them can be computed explicitly in terms of the structure maps defining the cyclic object. As a consequence, we obtain explicit formulas for the cyclic homology of all finite-dimensional cocommutative Hopf algebras over a field of characteristic 0.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : ( Co ) cyclic ( co ) homology of bialgebroids : An approach via ( co ) monads . Abstract : We introduce the notion of an algebra over a monoidal category and find that it is analogous to the notion of a coalgebra in the dual category , which we call a comonoid .We then define the cyclic homology of such algebras as the Hochschild cohomology of their underlying comonoids with coefficients in the bimodule given by the tensor product of the algebra with its opposite algebra . This definition generalizes the usual one for regular algebras over fields or rings .In particular , if the base ring has characteristic zero , this recovers the classical definitions of cyclic homology and periodic cyclic homology . The same construction also works for Hopf algebroids rather of regular algebras ; however , there are some subtleties emerging when trying to apply these results to arbitrary commutative rings .Finally , we give numerous instances illustrating our concepts . Cyclic homology was introduced by Connes in his seminal study on noncommutative geometry Con .It can be written as the Hochschild homology of certain algebras called cyclic objects . These were first investigated carefully by Bökstedt Bök , who demonstrated how they could be used to build modern algebraic structures like crossed rings and group extensions .Since then , various papers have analyzed various issues of cyclic objects and their applications . For instance , see Fri1 , Fri2 , Koc , Lau , Maz , Nee , Sta .In this article , we will research cyclic objects in more depth using techniques established recently in the notion of operads and monads . Our main result suggests that any cyclic object gives rise to two different kinds of cyclic homologies , namely the cyclic homology of the underlying algebra and the periodic cyclic homology of the associated graded algebra .Moreover , both of them can be computed specifically in terms of the structure maps defining the cyclic object . As a consequence , we obtain formal formulas for the cyclic homology of all finite - dimensional cocommutative Hopf algebras over a field of characteristic 0 .",
        "rewrite_text": "In this article, we explore the concept of an algebra within the framework of a monoidal category, drawing parallels to the dual notion of a coalgebra, which we refer to as a comonoid. We introduce a definition for the cyclic homology of these algebras, conceptualizing it as the Hochschild cohomology of their corresponding comonoids, utilizing coefficients derived from the bimodule formed by the tensor product of the algebra with its opposite algebra. This approach extends the traditional definitions of cyclic homology applicable to regular algebras over fields or rings. Notably, when the base ring is of characteristic zero, our definition aligns with the classical interpretations of both cyclic homology and periodic cyclic homology.\n\nFurthermore, we extend our findings to Hopf algebroids, although we encounter certain complexities when attempting to generalize these results to arbitrary commutative rings. To illustrate our theoretical framework, we provide numerous examples that highlight the practical implications of our concepts. The origins of cyclic homology can be traced back to Connes' pioneering work in noncommutative geometry, where it was formulated as the Hochschild homology of specific algebras known as cyclic objects. Bökstedt further examined these cyclic objects, demonstrating their utility in constructing contemporary algebraic structures such as crossed rings and group extensions. Since then, a variety of studies have delved into the intricacies of cyclic objects and their diverse applications.\n\nIn this paper, we aim to deepen the understanding of cyclic objects by employing recent advancements in operads and monads. Our principal finding indicates that each cyclic object is associated with two distinct forms of cyclic homology: the cyclic homology of the underlying algebra and the periodic cyclic homology of the corresponding graded algebra. Importantly, both forms can be explicitly computed based on the structure maps that define the cyclic object. This leads us to derive formal expressions for the cyclic homology of all finite-dimensional cocommutative Hopf algebras over a field with characteristic zero, thereby contributing to the broader discourse on cyclic homology and its applications in algebraic topology and related fields.",
        "ori-fast-z-score": -0.6211495565912797,
        "water-fast-z-score": 4.911524908119364,
        "rewrite-fast-z-score": 1.9445436482630056
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Modeling Heterogeneous Materials via Two-Point Correlation Functions: I. Basic Principles .\nAbstract:\nWe present the basic principles for modeling heterogeneous materials using two-point correlation functions (2PCFs). The 2PCF is an important statistical tool in many fields, including physics and engineering sciences. In this work we show how to use it as a basis for describing complex systems with multiple components or phases. We demonstrate that the 2PCF can be used to describe both static and dynamic properties of such systems. Finally, we discuss some applications of our approach. This article is part of a series on  Multiscale Modeling  published by Frontiers in Physics. \nIntroduction\n\nTwo-point correlation function (2PCF) is one of the most fundamental concepts in statistics  1  . It has been widely applied across various disciplines ranging from physics  2  , chemistry  3  , biology  4  , geology  5  , medicine  6  , economics  7  , sociology  8  , etc., to engineering  9  .\nIn recent years there have been several attempts to apply the concept of 2PCF to multiscale modeling  10 -12  . However, these works are mostly focused on developing new numerical methods rather than providing physical insights into the problem at hand. Herein, we propose a novel method based on the concept of 2PCFs which allows us to model heterogeneous materials consisting of different components and/or phases. Our approach provides a general framework for studying the structure-property relationships in such systems. Moreover, it enables us to study their dynamics over a wide range of time scales. \n \n To illustrate the main idea behind our approach let us consider a simple example shown schematically in Figure 1 . Suppose we want to investigate the mechanical response of a composite material made up of three distinct components A, B, C arranged in a periodic manner. Each component consists of randomly distributed spherical particles embedded within a matrix phase. For simplicity, assume that all components have identical volume fractions f = 0.33 but differ in terms of particle size distribution. Specifically, suppose that the average diameter of particles in each component is equal to: dA = 10 nm; dB = 20 nm; DC = 30 nm. As illustrated in Figure 1(a) , the overall microstructure of the",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Modeling Heterogeneous Materials via Two - Point Correlation Functions : I . Basic Principles .Abstract : We present the fundamental principles for modeling heterogeneous materials utilizing two - point coupling functions ( 2PCFs ) . The 2PCF is an important statistical tool in multiple fields , notably physics and engineering studies .In this study we show how to use it as a foundation for describing complex systems with many aspects or stages . We showed that the 2PCF can be used to explain both static and dynamic characteristics of such systems .Finally , we explain some applications of our approach . This page is part of a trilogy on Multiscale Modeling published by Frontiers in Physics .Introduction Two - point coupling function ( 2PCF ) is one of the most important concepts in statistics 1 . It has been widely applied across numerous topics including from science 2 , chemistry 3 , chemistry 4 , geology 5 , medicine 6 , economics 7 , anthropology 8 , etc . , to engineering 9 .In past decades there have been numerous attempts to apply the idea of 2PCF to multiscale simulation 10 - 12 . However , these works are mostly concentrated on developing innovative computational models rather than providing physical knowledge into the issue at hand .Herein , we develop a novel method using on the idea of 2PCFs which allows us to model heterogeneous materials composed of different components and / or stages . Our concept provides a general template for studying the form - property interactions in such systems .Moreover , it allows us to study their processes over a broad variety of time ranges . To explain the main idea behind our approach take us consider a simple example shown schematically in Figure 1 .Suppose we wish to examine the mechanical response of a composite material set up of three different components A , B , C arranged in a periodic manner . Each component consists of randomly distributed spherical atoms embedded within a matrix phase .For simplicity , assume that all components have equal volume fractions f = 0 . 33 but differ in terms of particle size distribution . Specifically , suppose that the average diameter of molecules in each component is equal to : dA = 10 nm ; dB = 20 nm ; DC = 30 nm .As illustrated in Figure 1(a) , the overall microstructure of the",
        "rewrite_text": "**Title:** Modeling Heterogeneous Materials via Two-Point Correlation Functions: I. Basic Principles\n\n**Abstract:** This article introduces the foundational principles for modeling heterogeneous materials through the use of two-point correlation functions (2PCFs), a crucial statistical tool employed across various disciplines, including physics and engineering. We demonstrate how 2PCFs serve as a robust framework for characterizing complex systems that exhibit multiple components or stages. Our findings indicate that 2PCFs can effectively capture both the static and dynamic properties of these systems, providing insights into their behavior under different conditions. Additionally, we discuss several practical applications of our methodology, highlighting its relevance in the context of multiscale modeling, as part of a trilogy published by Frontiers in Physics.\n\nThe two-point correlation function is a pivotal concept in statistics, with extensive applications spanning diverse fields such as science, chemistry, geology, medicine, economics, anthropology, and engineering. Despite numerous efforts in recent decades to leverage 2PCFs for multiscale simulations, much of the existing literature has focused primarily on the development of advanced computational models, often at the expense of deeper physical understanding.\n\nIn this work, we propose a novel approach that utilizes 2PCFs to model heterogeneous materials composed of various components and stages. Our method provides a comprehensive framework for investigating the interactions between form and properties in these systems, enabling the exploration of their processes across a wide range of temporal scales. To illustrate our approach, we present a simplified example involving a composite material made up of three distinct components—A, B, and C—arranged periodically. Each component consists of randomly distributed spherical atoms within a matrix phase, with equal volume fractions (f = 0.33) but differing particle size distributions. Specifically, we consider average diameters of dA = 10 nm, dB = 20 nm, and dC = 30 nm for components A, B, and C, respectively. This example serves to elucidate the overall microstructure of the composite material and the implications of our modeling approach.",
        "ori-fast-z-score": 0.38014296063485276,
        "water-fast-z-score": 9.751483403638062,
        "rewrite-fast-z-score": 1.2418408411301325
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Massive stars and globular cluster formation .\nAbstract:\nThe origin of the most massive stars is still an open question in astrophysics, as well as their role in shaping galactic evolution. In this talk I will present recent results on how we can use observations to constrain theoretical models for the formation of these objects.  The first part of my talk will focus on the observational properties of young massive clusters (YMCs) that are found at high redshift z>6-7. These YMCs have masses up to 10^8 Msun and sizes of ~1kpc. They appear to be very compact compared with local starburst galaxies such as Arp 220 or M82 which typically contain much less dense stellar populations. We find that the observed size-mass relation of these distant YMCs agrees remarkably well with predictions based on numerical simulations of turbulent gas clouds collapsing under self-gravity. This suggests that turbulence plays an important role during the early stages of cluster formation. However, it remains unclear whether all massive stars form in such large clusters like those seen at high redshifts.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Massive stars and globular cluster formation . Abstract : The origin of the most large objects is already an open question in astrophysics , as well as their role in shaping galactic progression .In this talk I will present recent results on how we can using observations to constrain theoretical methods for the formation of these objects . The first part of my talk will focus on the observational properties of young massive clusters ( YMCs ) that are found at high redshift z > 6 - 7 .These YMCs have masses up to 10 ^ 8 Msun and dimensions of ~ 1kpc . They seem to be very small compared with local starburst clusters such as Arp 220 or M82 which commonly hold much less dense stellar regions .We see that the known size - mass balance of these distant YMCs agrees fairly good with predictions based on numerical simulations of turbulent gas clouds collapsing under self - gravity . This implies that turbulence plays an important role during the early stages of cluster structure .However , it remains doubtful whether all huge stars create in such large clusters like those viewed at high redshifts .",
        "rewrite_text": "Title: Formation of Massive Stars and Globular Clusters\n\nAbstract: The origins of the largest celestial objects remain a significant enigma in the field of astrophysics, particularly regarding their influence on the evolution of galaxies. In this presentation, I will discuss recent findings that utilize observational data to refine theoretical models concerning the formation of these massive entities. The initial segment of my talk will concentrate on the characteristics of young massive clusters (YMCs) identified at high redshifts, specifically those greater than z = 6-7. These YMCs exhibit impressive masses, reaching up to 10^8 solar masses, and span dimensions of approximately 1 kpc. In contrast to local starburst clusters like Arp 220 or M82, which typically feature less dense stellar environments, these distant YMCs appear relatively compact. Notably, the observed size-mass relationship of these high-redshift YMCs aligns well with predictions derived from numerical simulations that model the collapse of turbulent gas clouds under their own gravitational forces. This correlation suggests that turbulence is a critical factor influencing the early developmental phases of cluster formation. However, a lingering question remains: do all massive stars form within such extensive clusters as those observed at high redshifts? This inquiry invites further exploration into the mechanisms of star formation and the conditions that foster the emergence of massive stars in various cosmic environments.",
        "ori-fast-z-score": 0.42640143271122083,
        "water-fast-z-score": 6.1137368096588665,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Measuring Electric Fields From Surface Contaminants with Neutral Atoms .\nAbstract:\nWe report on the measurement of electric fields produced by surface contaminants using neutral atoms as probes. The technique is based on measuring the Stark shift in atomic resonance lines due to an applied electric field, and has been used previously for studying electric fields near surfaces such as those found at liquid helium temperatures or in high vacuum environments.  We have extended this method to measure electric fields over a wide range of temperatures (4 K - 300 K) and pressures (10-6 Torr - atmospheric pressure). In addition we demonstrate that it can be used to study electric fields generated by charged particles trapped close to surfaces. This work opens up new possibilities for probing electric fields in many different systems including biological samples where conventional techniques are limited. Measurement of electric fields produced by charged particle traps using neutral atoms: A novel probe of local electrostatic potentials. Measuring electric fields produced by surface contaminant... Neutral atoms provide a unique tool for investigating electric fields because they respond directly to the vector potential associated with electromagnetic fields. Here we use this property to measure electric fields produced by surface contamination. Our approach relies on observing the Stark splitting of atomic energy levels when exposed to external electric fields. Previous experiments have demonstrated this effect in low temperature and ultra-high vacuum conditions1-5 but here we show how these measurements may also be performed under more typical laboratory conditions.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Measuring Electric Fields From Surface Contaminants with Neutral Atoms . Abstract : We report on the measurement of electric fields produced by surface contaminants using neutral elements as probes .The technique is based on measuring the Stark shift in nuclear resonance lines owing to an applied electric field , and has been used earlier for studying electric fields near structures such as those observed at liquid helium temperatures or in high vacuum environments . We have extended this technology to measure electric fields over a broad variety of temperatures ( 4 K - 300 K ) and pressures ( 10 - 6 Torr - atmospheric pressure ) .In addition we prove that it can be used to study electric forces generated by charged particles caught nearby to surfaces . This research raises up new possibilities for probing electric forces in different different environments namely biological samples where conventional methods are small .Measurement of electric forces generated by charged particle traps involving neutral particles : A novel probe of local electrostatic potentials . Measuring electric forces generated by surface contaminant . . .Neutral atoms represent a unique technique for investigating electric fields because they react directly to the vector potential identified with electromagnetic fields . Here we utilize this property to measure electric fields produced by surface contamination .Our model relies on observing the Stark dividing of atomic energy levels when exposed to external electric forces . Previous experiments have demonstrated this effect in low heat and ultra - low vacuum conditions1 - 5 but here we give how these measurements may also be performed under more typical laboratory situations .",
        "rewrite_text": "We present a comprehensive study on the measurement of electric fields generated by surface contaminants utilizing neutral atoms as probes. This innovative technique leverages the Stark shift observed in nuclear resonance lines, which occurs in the presence of an applied electric field. Historically, this method has been employed to investigate electric fields in specialized environments, such as those found at liquid helium temperatures or within high vacuum settings. In our research, we have successfully expanded the applicability of this technique to a wider range of temperatures (from 4 K to 300 K) and pressures (spanning from 10^-6 Torr to atmospheric pressure). Furthermore, we demonstrate its capability to analyze electric forces produced by charged particles located near surfaces.\n\nThis advancement opens new avenues for exploring electric forces in various environments, including biological samples, where traditional measurement methods may be limited. Our approach allows for the measurement of electric forces generated by charged particle traps involving neutral atoms, providing a novel means of probing local electrostatic potentials. Neutral atoms serve as an exceptional tool for investigating electric fields due to their direct response to the vector potential associated with electromagnetic fields. \n\nIn this study, we capitalize on this unique property to quantify electric fields arising from surface contamination. Our model is grounded in the observation of Stark splitting of atomic energy levels when subjected to external electric forces. While prior experiments have successfully demonstrated this phenomenon under low-temperature and ultra-high vacuum conditions, we elucidate how these measurements can also be effectively conducted in more conventional laboratory environments. This research not only enhances our understanding of electric field interactions with surface contaminants but also paves the way for future investigations in diverse scientific fields.",
        "ori-fast-z-score": -0.41522739926869984,
        "water-fast-z-score": 6.394501948737977,
        "rewrite-fast-z-score": 0.16116459280507606
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  USco1606-1935: An Unusually Wide Low-Mass Triple System? .\nAbstract:\nWe report the discovery and characterization of USco 1606-1935, an unusually wide low-mass triple system with two M-dwarfs orbiting each other in a ~2 year period at a distance of about 100 AU (~33 light years). The third component is a late K-type star that orbits both stars on a much wider scale, with a minimum mass for this companion of 0.7 solar masses. We present near-infrared spectroscopy to determine the spectral types of all three components as well as their radial velocities. Our results show that the inner binary has a total mass of only 0.3 solar masses, making it one of the lowest-mass binaries known. This makes USco 1606-1935 an ideal target for future studies of planet formation around very-low-mass stars. In addition, we find evidence for significant orbital eccentricity in the outer orbit which may be caused by tidal interactions between the close pair and its distant tertiary companion.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : USco1606 - 1935 : An Unusually Wide Low - Mass Triple System ? .Abstract : We report the discovery and description of USco 1606 - 1935 , an exceptionally wide low - mass triple system with two M - dwarfs orbiting each other in a ~ 2 week period at a distance of about 100 AU ( ~ 33 light years ) . The third component is a early K - class star that orbits both stars on a far larger scale , with a minimum mass for this companion of 0 . 7 solar masses .We present near - infrared spectroscopy to obtain the spectral classes of all three components as also as their radial velocities . Our results show that the inner binary has a total mass of only 0 . 3 solar masses , making it one of the smallest - energy binaries known .This gives USco 1606 - 1935 an suitable target for future research of planet development around very - low - mass stars . In addition , we find proof for significant orbital eccentricity in the exterior orbit which may be caused by tidal interactions between the close pair and its closest tertiary companion .",
        "rewrite_text": "We present the discovery and detailed analysis of USco 1606-1935, a remarkably wide low-mass triple star system. This system comprises two M-dwarfs that engage in a mutual orbit with a period of approximately two weeks, situated at a distance of around 100 astronomical units (about 33 light-years) from each other. The third component of this system is an early K-type star, which orbits the inner binary at a significantly larger distance, with a minimum mass estimated at 0.7 solar masses. \n\nIn our study, we utilized near-infrared spectroscopy to determine the spectral classifications of all three stars, as well as to measure their radial velocities. Our findings indicate that the total mass of the inner binary is merely 0.3 solar masses, categorizing it as one of the lowest-energy binary systems identified to date. This characteristic positions USco 1606-1935 as an intriguing candidate for future investigations into planet formation processes around very low-mass stars.\n\nFurthermore, our analysis reveals evidence of considerable orbital eccentricity in the outer orbit, which may be attributed to tidal interactions between the closely orbiting pair and their more distant tertiary companion. This discovery not only enhances our understanding of the dynamics within such wide binary systems but also opens new avenues for exploring the implications of these interactions on the potential for planet formation in low-mass environments. Overall, USco 1606-1935 represents a unique opportunity to study the complexities of stellar interactions and their influence on planetary systems in low-mass star environments.",
        "ori-fast-z-score": -2.0855209398041166,
        "water-fast-z-score": 2.494700264914546,
        "rewrite-fast-z-score": -0.5345224838248488
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Nonstationary random acoustic and electromagnetic fields as wave diffusion processes .\nAbstract:\nWe consider the statistical properties of nonstationary random acoustic and electromagnetical waves in terms of their correlation functions, power spectra, and probability density functions (PDFs). We show that these quantities can be expressed by means of solutions to certain partial differential equations with time-dependent coefficients. The PDFs are obtained for both stationary and nonstationary cases using the method of characteristics. In particular, we derive an exact expression for the PDF of the amplitude fluctuations of a monochromatic plane wave propagating through a turbulent medium. This result is used to obtain expressions for the mean-square fluctuation levels of the electric field strength and intensity at any point along the propagation path. Finally, we discuss some applications of our results to radiowave scintillation theory and radar detection problems. PACS: 42.65.Tg, 43.20 .Fx, 47.55.+q, 47.60.+j",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Nonstationary random acoustic and electromagnetic fields as wave diffusion processes . Abstract : We consider the statistical characteristics of nonstationary random acoustic and electromagnetical waves in terms of their correlation functions , power spectra , and likelihood density functions ( PDFs ) .We see that these quantities can be described by means of solutions to many partial differential coefficients with time - dependent coefficients . The PDFs are derived for both static and nonstationary cases using the method of characteristics .In particular , we derive an precise representation for the PDF of the amplitude fluctuations of a monochromatic plane beam propagating through a turbulent medium . This result is utilized to obtain definitions for the mean - square fluctuation concentrations of the electric field intensity and intensity at any point along the propagation path .Finally , we explain some applications of our findings to radiowave scintillation theory and radar detection difficulties . PACS : 42 . 65 . Tg , 43 . 20 . Fx , 47 . 55 . + q , 47 . 60 . + j",
        "rewrite_text": "Title: Nonstationary Random Acoustic and Electromagnetic Fields as Wave Diffusion Processes\n\nAbstract: This article investigates the statistical properties of nonstationary random acoustic and electromagnetic waves, focusing on their correlation functions, power spectra, and probability density functions (PDFs). We demonstrate that these statistical quantities can be effectively characterized through solutions to various partial differential equations that incorporate time-dependent coefficients. The derivation of PDFs is conducted for both static and nonstationary scenarios utilizing the method of characteristics. Notably, we present a precise formulation for the PDF of amplitude fluctuations associated with a monochromatic plane wave as it traverses a turbulent medium. This formulation is instrumental in defining the mean-square fluctuation concentrations of electric field intensity and intensity at any given point along the wave's propagation trajectory. Furthermore, we discuss the implications of our results for applications in radiowave scintillation theory and the challenges encountered in radar detection. The findings contribute to a deeper understanding of wave diffusion processes in complex media, providing valuable insights for both theoretical and practical advancements in the field. The research is categorized under PACS codes: 42.65.Tg, 43.20.Fx, 47.55.+q, and 47.60.+j.",
        "ori-fast-z-score": 0.6882472016116852,
        "water-fast-z-score": 4.98201895023622,
        "rewrite-fast-z-score": 1.4924050144892729
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Fluctuation dynamo and turbulent induction at low magnetic Prandtl numbers .\nAbstract:\nWe study the generation of large-scale magnetic fields in astrophysical plasmas with very small values of the magnetic Prandtl number, Pm = ν/η ≪ 1 (ν is viscosity, η is resistivity). We consider two different types of flows that are relevant for this problem -helically driven turbulence and convection-driven turbulence. In both cases we find that the mean electromotive force has contributions from several terms which scale differently as functions of the Reynolds number Re = UL/ν and the magnetic Reynolds number Rm = URm/η. Here U , L, and Rm are characteristic velocity, length, and magnetic field scales respectively.  For helically driven turbulence these contributions can be grouped into three categories:  The first category includes all terms proportional to Re(Rm)−1/2 . These terms have been studied previously by many authors using various approaches including direct numerical simulations. They represent the contribution of the so-called α-effect due to helical motions. The second category contains all terms proportional to Re1/2 (Rm)−1/4 . This term represents the effect of helicity on the nonlinear evolution of the magnetic fluctuations. Finally, there exists also an additional third category containing all terms proportional to Re3/4 (Rm)−3/8 . It describes the influence of helicity on the linear growth rate of the magnetic fluctuations.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Fluctuation dynamo and turbulent induction at low magnetic Prandtl numbers . Abstract : We research the generation of large - scale magnetic fields in astrophysical plasmas with very small values of the magnetic Prandtl number , Pm = ν / η [UNK] 1 ( ν is viscosity , η is resistivity ) .We consider two different kinds of flows that are applicable for this question - helically generated turbulence and convection - powered turbulence . In both cases we find that the mean electromotive pressure has contributions from several terms which scale differently as functions of the Reynolds number Re = UL / ν and the magnetic Reynolds number Rm = URm / η .Here U , L , and Rm are characteristic velocity , length , and magnetic force scales respectively . For helically generated turbulence these contributions can be grouped into three categories : The first class includes all terms proportional to Re ( Rm ) −1 / 2 .These concepts have been studied historically by many writers using numerous methodology including continuous mathematical simulations . They represent the contribution of the so - called α - effect owing to helical movements .The second class includes all terms proportional to Re1 / 2 ( Rm ) −1 / 4 . This term reflects the impact of helicity on the nonlinear development of the magnetic fluctuations .Finally , there exists also an additional third category containing all terms proportional to Re3 / 4 ( Rm ) −3 / 8 . It expresses the impact of helicity on the linear expansion frequency of the magnetic fluctuations .",
        "rewrite_text": "In this study, we investigate the mechanisms behind the generation of large-scale magnetic fields in astrophysical plasmas characterized by low magnetic Prandtl numbers (Pm = ν / η, where ν represents viscosity and η denotes resistivity). Our analysis focuses on two primary types of flows relevant to this phenomenon: helically generated turbulence and convection-driven turbulence. Through our research, we identify that the mean electromotive force is influenced by multiple terms that exhibit distinct scaling behaviors in relation to the Reynolds number (Re = UL / ν) and the magnetic Reynolds number (Rm = URm / η), where U, L, and Rm signify characteristic velocity, length, and magnetic force scales, respectively.\n\nFor helically generated turbulence, we categorize the contributions to the mean electromotive force into three distinct classes. The first category encompasses terms that scale as Re (Rm) −1 / 2, which have been extensively studied in the literature through various methodologies, including continuous mathematical simulations. These terms are associated with the α-effect, arising from helical motions within the turbulence. The second category includes terms that scale as Re1 / 2 (Rm) −1 / 4, reflecting the influence of helicity on the nonlinear evolution of magnetic fluctuations. Lastly, we identify a third category of terms that scale as Re3 / 4 (Rm) −3 / 8, which captures the effect of helicity on the linear growth rate of magnetic fluctuations.\n\nOur findings contribute to a deeper understanding of the interplay between turbulence and magnetic field generation in astrophysical contexts, particularly under conditions of low magnetic Prandtl numbers. This research has significant implications for the study of cosmic magnetic fields and their role in various astrophysical processes.",
        "ori-fast-z-score": 0.8867963503478639,
        "water-fast-z-score": 6.021972346615038,
        "rewrite-fast-z-score": 1.034792955221957
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Keck spectroscopy and Spitzer Space Telescope analysis of the outer disk of the Triangulum Spiral Galaxy M33 .\nAbstract:\nWe present new near-infrared (NIR) spectroscopic observations with Keck II/DEIMOS, which cover the entire optical extent of the nearby spiral galaxy M33 out to its last measured isophote at 25 mag arcsec-2 in B-band. We also use archival data obtained by the Infrared Array Camera onboard the Spitzer Space Telescope for our study. The main goal of this work was to investigate how star formation proceeds beyond the edge of galactic disks into the surrounding intergalactic medium. Our results show that there are two distinct components along the line-of-sight towards M33: an extended component associated with diffuse ionized gas and young stars; and a compact component dominated by old stellar populations. Using these NIR spectra we have derived radial profiles of several physical parameters such as electron density, temperature, extinction coefficient etc., across the face-on view of M33 s disk. These profiles reveal interesting trends in the properties of interstellar matter within different regions of the galaxy.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Keck spectroscopy and Spitzer Space Telescope examination of the exterior disk of the Triangulum Spiral Galaxy M33 . Abstract : We report new near - infrared ( NIR ) spectroscopic observations with Keck II / DEIMOS , which cover the entire optical extent of the nearby spiral galaxy M33 out to its last detected isophote at 25 mag arcsec - 2 in B - band .We additionally using archival imagery obtained by the Infrared Array Camera onboard the Spitzer Space Telescope for our research . The main goal of this project was to examine how star formation flows beyond the margins of galactic disks into the nearby intergalactic medium .Our results show that there are two different components along the line - of - seeing approaching M33 : an extended component associated with diffuse ionized gas and older stars ; and a compact component dominated by aged stellar regions . Using these NIR spectra we have derived radial profiles of several physical factors such as electron concentration , temperature , extinction factor etc . , across the face - on vision of M33 s disk .These features reveal novel trends in the properties of interstellar matter within various regions of the universe .",
        "rewrite_text": "We present new near-infrared (NIR) spectroscopic observations conducted with the Keck II telescope equipped with DEIMOS, which encompass the entire optical extent of the Triangulum Spiral Galaxy, M33, extending to its faintest detected isophote at 25 mag arcsec^-2 in the B-band. In addition to our observations, we utilized archival data from the Infrared Array Camera on the Spitzer Space Telescope to enhance our analysis. The primary objective of this study was to investigate the mechanisms of star formation as they extend beyond the boundaries of galactic disks into the surrounding intergalactic medium. Our findings reveal the presence of two distinct components along the line of sight towards M33: an extended component characterized by diffuse ionized gas and older stellar populations, and a more compact component that is predominantly composed of aged stellar regions. By analyzing the NIR spectra, we have successfully derived radial profiles for several key physical parameters, including electron density, temperature, and extinction factors, across the face-on view of M33's disk. These profiles uncover significant trends in the characteristics of interstellar matter across various regions of the galaxy, providing new insights into the processes governing star formation and the interaction between galaxies and their environments. Our research contributes to a deeper understanding of the dynamics of galactic evolution and the role of external factors in shaping the interstellar medium.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 6.616378963469013,
        "rewrite-fast-z-score": 1.2
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The cold gaseous halo of NGC 891 .\nAbstract:\nWe present new observations of the molecular gas in the central region of the nearby galaxy NGC 891, obtained with the IRAM 30m telescope at 1mm and 3mm wavelengths. The data reveal an extended distribution of dense (n(H2) ~ 104 cm-3), warm (T~50K) molecular gas that is associated with the optical disk of this edge-on spiral galaxy. We find evidence for two distinct components to the molecular gas distribution; one component follows closely the dust lane seen in visible light images while another component extends out into the surrounding intergalactic medium. This latter component has been detected previously by other authors but our higher resolution data allow us to resolve it into individual clouds. In addition we detect several compact sources within the galactic plane which are likely to be young star forming regions. These results suggest that there may exist a significant reservoir of molecular material outside the main body of galaxies such as NGC 891.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The cold gaseous halo of NGC 891 . Abstract : We report new images of the molecular gas in the central region of the nearby galaxy NGC 891 , obtained with the IRAM 30m telescope at 1mm and 3mm wavelengths .The data reveal an extended distribution of dense ( n ( H2 ) ~ 104 mm - 3 ) , warm ( T ~ 50K ) molecular gas that is associated with the optical disk of this edge - on spiral galaxy . We see evidence for two different components to the molecular gas distribution ; one element follows tightly the dust track seen in bright light photographs while another component extends out into the nearby intergalactic medium .This latter component has been detected earlier by other researchers but our higher resolution data enable us to separate it into multiple clouds . In addition we locate many compact sources within the galactic plane which are likely to be young galaxy producing regions .These data suggest that there may contain a substantial pool of molecular matter outside the main bodies of galaxies such as NGC 891 .",
        "rewrite_text": "We present new observational data on the molecular gas present in the central region of the nearby galaxy NGC 891, acquired using the IRAM 30m telescope at both 1mm and 3mm wavelengths. Our findings reveal a significant and extensive distribution of dense molecular gas, characterized by a density of approximately \\( n(H_2) \\sim 10^4 \\, \\text{mm}^{-3} \\) and a temperature around \\( T \\sim 50 \\, \\text{K} \\). This molecular gas is closely associated with the optical disk of this edge-on spiral galaxy. Notably, our analysis indicates the presence of two distinct components within the molecular gas distribution. The first component closely aligns with the dust lane observed in high-resolution optical images, while the second component extends into the surrounding intergalactic medium. Although this latter component has been previously identified by other studies, our high-resolution observations allow us to resolve it into multiple distinct clouds. Furthermore, we have identified numerous compact sources within the galactic plane, which are likely regions of active star formation. These results imply that there exists a considerable reservoir of molecular gas beyond the primary structures of galaxies like NGC 891, suggesting that such halos may play a critical role in the galactic ecosystem and the processes of star formation. Our study enhances the understanding of molecular gas distribution in edge-on galaxies and highlights the potential for significant molecular reservoirs in the intergalactic medium, which may influence galactic evolution and dynamics.",
        "ori-fast-z-score": -2.013995972012084,
        "water-fast-z-score": 4.9819900360298925,
        "rewrite-fast-z-score": 1.3750477455423171
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Blazar surveys with WMAP and Swift .\nAbstract:\nWe present the results of our analysis on blazars detected by both the Wilkinson Microwave Anisotropy Probe (WMAP) satellite and the Swift observatory in the first year of operation, 2004-05. We find that there are no significant differences between the two samples when we compare their distributions for redshift, luminosity distance, radio flux density at 1 GHz, optical magnitude, or X-ray photon index. The only difference is found to be in the distribution of redshifts; this may be due to selection effects caused by the different energy bands used by each instrument. \n \n Keywords: Blazar, Swift, WMAP, survey, cosmology, statistics, gamma-ray bursts, galaxy clusters, dark matter, dark energy, neutrino mass, cosmic microwave background radiation, anisotropies, large-scale structure, gravitational lensing, relativistic jets, quasar, active galactic nuclei",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Blazar observations with WMAP and Swift . Abstract : We report the conclusion of our analysis on blazars detected by both the Wilkinson Microwave Anisotropy Probe ( WMAP ) satellite and the Swift telescope in the first year of operation , 2004 - 05 .We see that there are no considerable changes between the two specimens when we compare their distributions for redshift , luminosity distance , radio flux concentration at 1 GHz , optical magnitude , or X - ray photon index . The only difference is found to be in the distribution of redshifts ; this might be due to choice influences created by the different energy bands used by each instrument .Keywords : Blazar , Swift , WMAP , survey , cosmology , statistics , gamma - ray bursts , galaxy rings , soft material , soft energy , neutrino mass , cosmic microwave background radiation , anisotropies , large - scale structure , gravity lensing , relativistic jets , quasar , active galactic nuclei",
        "rewrite_text": "In this study, we present the findings from our comprehensive analysis of blazars observed by both the Wilkinson Microwave Anisotropy Probe (WMAP) and the Swift telescope during their initial operational year, spanning 2004 to 2005. Our investigation aimed to compare various astrophysical properties of these blazars, focusing on parameters such as redshift, luminosity distance, radio flux density at 1 GHz, optical brightness, and X-ray photon indices. The results indicate that there are no significant differences in the distributions of these parameters between the two observational datasets. However, we did identify a notable distinction in the redshift distributions, which may be attributed to the differing energy bands utilized by WMAP and Swift. This finding suggests that the choice of observational instruments can influence the selection of blazars detected, potentially impacting the interpretation of their cosmological significance. Our research contributes to the broader understanding of blazars, which are a type of active galactic nucleus characterized by their powerful jets and high-energy emissions. The implications of our findings extend to various fields, including cosmology, where understanding the distribution and characteristics of blazars can provide insights into the large-scale structure of the universe and the nature of cosmic microwave background radiation. This study also touches upon related topics such as gamma-ray bursts, gravitational lensing, and the role of relativistic jets in the dynamics of galaxies. Overall, our analysis underscores the importance of multi-wavelength observations in astrophysics and highlights the need for careful consideration of instrument-specific biases when interpreting astronomical data. \n\nKeywords: Blazar, Swift, WMAP, survey, cosmology, statistics, gamma-ray bursts, galaxy rings, soft material, soft energy, neutrino mass, cosmic microwave background radiation, anisotropies, large-scale structure, gravitational lensing, relativistic jets, quasar, active galactic nuclei.",
        "ori-fast-z-score": -0.8682431421244593,
        "water-fast-z-score": 3.2003787654626508,
        "rewrite-fast-z-score": 1.30066495428618
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Single Transverse-Spin Asymmetry in Hadronic Dijet Production .\nAbstract:\nWe present the first measurement of single-transverse-spin asymmetries (SSA) for hadronic dijets produced at midrapidity in p+p collisions at sqrt(sNN) = 5.02 TeV using data collected by the CMS experiment during 2012 corresponding to an integrated luminosity of 2.3 fb-1 . The SSAs are extracted as functions of jet transverse momentum and rapidity, azimuthal angle between jets, and event centrality. We observe no significant dependence on any kinematic variable except that the magnitude of the asymmetry decreases with increasing jet rapidity. Our results are compared to theoretical predictions based on perturbative QCD calculations including higher-order corrections and parton distribution function uncertainties. \nThe measured values agree well within experimental and theoretical uncertainties. This is the most precise measurement of this observable performed so far. \n \n Introduction \n \n Single transverse-spin asymmetries have been observed in several processes involving polarized protons or neutrons  1  , such as inclusive pion production  2  , semi-inclusive deep-inelastic scattering  3  , Drell-Yan lepton pair production  4  , prompt photon production  5  , and direct photons  6  . These measurements provide important information about the spin structure of nucleons  7, 8  .\n \nIn particular, they can be used to test the validity of factorization theorems  9  which relate hard-scattering cross sections to partonic distributions inside the proton  10  . In addition, these observables may also shed light on new physics beyond the Standard Model  11  . \n \n For example, it has recently been suggested  12  that large single-spin asymmetries could arise due to the interference of two amplitudes describing different helicities of quarks emitted from longitudinally polarized gluons in high-energy pp collisions. Such effects would violate parity conservation and thus constitute evidence for new physics  13  . However, there exists only one previous measurement  14  of single-spin asymmeties in hadronic dijet production at high energies. That study was carried out at RHIC  15  where the center-of-mass energy per nucleon-nucleon collision √sNN=200 GeV is much lower",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Single Transverse - Spin Asymmetry in Hadronic Dijet Production . Abstract : We report the first measurement of single - transverse - spinning asymmetries ( SSA ) for hadronic dijets created at midrapidity in p + p collisions at sqrt ( sNN ) = 5 . 02 TeV using data taken by the CMS experiment during 2012 corresponding to an integrated luminosity of 2 . 3 fb - 1 .The SSAs are derived as functions of jet vertical momentum and rapidity , azimuthal angle between planes , and event centrality . We see no major dependence on any kinematic variable except that the magnitude of the asymmetry decreases with rising jet rapidity .Our results are compared to theoretical estimates based on perturbative QCD calculations including higher - order corrections and parton distribution function uncertainties . The measured measures agree well within experimental and theoretical uncertainties .This is the most accurate calculation of this observable performed so far . Introduction Single transverse - spin asymmetries have been observed in multiple processes involving polarized protons or neutrons 1 , such as inclusive pion production 2 , semi - inclusive deep - inelastic emission 3 , Drell - Yan lepton pair production 4 , prompt photon processing 5 , and direct photons 6 .These measurements give important information about the spin composition of nucleons 7 , 8 . In particular , they can be used to test the legitimacy of factorization theorems 9 which compare hard - absorption cross sections to partonic distributions inside the proton 10 .In addition , these observables might additionally shed light on new science beyond the Standard Model 11 . For instance , it has recently been proposed 12 that wide single - spinning asymmetries may arise due to the interference of two amplitudes describing different helicities of quarks emitted from longitudinally polarized gluons in high - energy pp collisions .Such effects would violate parity conservation and therefore constitute evidence for new science 13 . However , there exists only one previous measurement 14 of single - spinning asymmeties in hadronic dijet production at high energies .That experiment was carried out at RHIC 15 where the center - of - mass energy per nucleon - nucleon collision √sNN = 200 GeV is much lower",
        "rewrite_text": "We present the inaugural measurement of single transverse spin asymmetries (SSA) in hadronic dijet production at midrapidity in proton-proton (p + p) collisions, conducted at a center-of-mass energy of √sNN = 5.02 TeV. This analysis utilizes data collected by the CMS experiment during 2012, which corresponds to an integrated luminosity of 2.3 fb^-1. The SSAs are evaluated as functions of various kinematic parameters, including the vertical momentum and rapidity of the jets, the azimuthal angle between the jet planes, and the centrality of the events. Our findings indicate that the asymmetry does not exhibit significant dependence on most kinematic variables; however, we observe a decrease in the magnitude of the asymmetry with increasing jet rapidity. \n\nWe compare our measurements with theoretical predictions derived from perturbative Quantum Chromodynamics (QCD) calculations, which account for higher-order corrections and uncertainties in parton distribution functions. The results show a commendable agreement within the bounds of both experimental and theoretical uncertainties, marking this as the most precise assessment of this observable to date.\n\nSingle transverse spin asymmetries have been documented in various processes involving polarized nucleons, including inclusive pion production, semi-inclusive deep inelastic scattering, Drell-Yan lepton pair production, and prompt photon production. These measurements provide crucial insights into the spin structure of nucleons and can be instrumental in testing the validity of factorization theorems that relate hard scattering cross sections to partonic distributions within protons. Furthermore, these observables may reveal phenomena beyond the Standard Model, as recent theories suggest that significant single-spin asymmetries could emerge from the interference of different helicity amplitudes of quarks emitted from longitudinally polarized gluons in high-energy collisions. Such effects would challenge parity conservation, potentially indicating new physics. Notably, this study builds upon a previous measurement of single-spin asymmetries in hadronic dijet production at lower energies, specifically at the Relativistic Heavy Ion Collider (RHIC) with a center-of-mass energy of √sNN = 200 GeV.",
        "ori-fast-z-score": 0.4240944648399855,
        "water-fast-z-score": 6.316139407998892,
        "rewrite-fast-z-score": -1.9005105362789922
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Collective states of the odd-mass nuclei within the framework of the Interacting Vector Boson Model .\nAbstract:\nThe collective properties of odd-mass nuclei are investigated in terms of the interacting vector boson model (IVBM). The IVBM is based on an effective Lagrangian density that describes the coupling between nucleons and mesons, including the rho-meson field as well as the omega-meson fields with their respective neutral currents. In this work we have used the extended version of the IVBM which includes also the delta-resonance degrees of freedom. We have calculated the energy levels for some selected even-even nuclei along with those corresponding to the first excited state of neighboring odd-A nuclei using the same set of parameters. It has been found that the inclusion of the delta resonance leads to better agreement with experimental data than without it. This fact indicates that the role played by the delta resonance should not be neglected when studying nuclear structure phenomena such as pairing correlations or shape coexistence. Finally, we have studied the effect of the spin-orbit interaction on the ground-state band built upon the lowest 0+ state.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Collective states of the odd - mass particles within the framework of the Interacting Vector Boson Model . Abstract : The collective characteristics of odd - mass particles are examined in terms of the interacting vector boson theory ( IVBM ) .The IVBM is based on an appropriate Lagrangian density that describes the interaction between nucleons and mesons , notably the rho - meson field as well as the omega - meson fields with their separate neutral currents . In this research we have utilized the extended version of the IVBM which includes also the delta - resonance degrees of liberty .We have predicted the power concentrations for some selected even - even clusters along with those corresponding to the first excited state of neighboring odd - A nuclei using the same set of parameters . It has been shown that the introduction of the delta resonance gives to easier agreement with theoretical data than without it .This fact suggests that the importance played by the delta resonance should not be forgotten when examining nuclear formation dynamics such as pairing correlations or shape coexistence . Finally , we have researched the impact of the spin - orbit interaction on the ground - state band building upon the lowest 0 + state .",
        "rewrite_text": "Title: Collective States of Odd-Mass Particles within the Framework of the Interacting Vector Boson Model\n\nAbstract: This study investigates the collective properties of odd-mass particles through the lens of the Interacting Vector Boson Model (IVBM). The IVBM is grounded in a well-defined Lagrangian density that encapsulates the interactions between nucleons and mesons, specifically focusing on the rho-meson and omega-meson fields, each characterized by their distinct neutral currents. In our analysis, we employ an extended version of the IVBM that incorporates the degrees of freedom associated with delta resonance. Our findings include predictions of power concentrations for selected even-even clusters, as well as for the first excited states of neighboring odd-A nuclei, all derived from a consistent set of parameters. Notably, the inclusion of delta resonance significantly enhances the alignment of our theoretical predictions with empirical data, underscoring the critical role that delta resonance plays in the dynamics of nuclear formation, particularly in phenomena such as pairing correlations and shape coexistence. Furthermore, we explore the influence of spin-orbit interactions on the construction of the ground-state band, specifically building upon the lowest 0+ state. This comprehensive examination not only sheds light on the intricate behaviors of odd-mass particles but also emphasizes the necessity of considering delta resonance in future nuclear structure studies.",
        "ori-fast-z-score": -1.8371173070873836,
        "water-fast-z-score": 4.4907311951024935,
        "rewrite-fast-z-score": 1.5689290811054724
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Exact Floquet states of a driven condensate and their stabilities .\nAbstract:\nWe study the exact Floquet states of a Bose-Einstein condensate (BEC) in an optical lattice under periodic driving, which is realized by periodically modulating the depth of the optical potential. We show that there are two types of Floquet states depending on whether they have zero or nonzero quasienergies. The former ones correspond to the usual Bloch bands while the latter ones represent the so-called Floquet-Bloch bands. In particular, we find that the Floquet-Bloch band structure can be obtained as a result of hybridization between different Bloch bands with opposite momenta. Furthermore, we investigate how these Floquet states evolve when the system parameters change. Finally, we discuss the stability properties of the Floquet states against small perturbations. Our results provide useful insights into the physics of periodically-driven quantum systems. Introduction:-Recent experimental advances allow for realizing artificial gauge fields  1  , synthetic dimensions  2  , topological phases  3  , and even time crystals  4  . These fascinating phenomena are usually observed in ultracold atomic gases trapped in optical lattices  5  .\nIn this work, we consider a Bose-Einstein Condensate (BEC) confined in such a one-dimensional (1D) optical lattice  6  . By applying external laser beams  7, 8  , it is possible to create a periodic modulation of the optical potential  9  . This leads to a periodic variation of the hopping amplitude J(t), which plays the role of a time-dependent Peierls phase  10  . As a consequence, the effective Hamiltonian describing our system becomes time-periodic  11  . It has been shown recently  12  that the corresponding Schrödinger equation admits solutions known as Floquet states  13  . They describe the evolution of the wave function over one period T = 2π/ω 0 where ω 0 denotes the frequency of the periodic drive  14  . Since the Floquet states are not stationary but rather oscillate at the same frequency as the drive  15  , they may exhibit interesting physical features  16  . For example, Floquet engineering allows us to realize exotic superfluidity  17  , non-Abelian anyons  18  , and Major",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Exact Floquet states of a pushed condensate and their stabilities . Abstract : We explore the exact Floquet states of a Bose - Einstein condensate ( BEC ) in an optical lattice under periodic drove , which is realized by periodically modulating the depth of the optical potential .We see that there are two forms of Floquet states varying on whether they have zero or nonzero quasienergies . The first ones relate to the usual Bloch groups while the latter ones represent the so - called Floquet - Bloch groups .In particular , we find that the Floquet - Bloch group structure can be obtained as a result of hybridization between various Bloch groups with opposite momenta . Furthermore , we investigate how these Floquet states evolve when the system parameters change .Finally , we investigate the stability properties of the Floquet states against small perturbations . Our results yield useful insights into the physics of periodically - driven quantum systems .Introduction : - Recent research developments enable for realizing artificial gauge fields 1 , synthetic dimensions 2 , topological phases 3 , and even period crystals 4 . These curious phenomena are typically observed in ultracold atomic gases locked in optical lattices 5 .In this study , we imagine a Bose - Einstein Condensate ( BEC ) concentrated in such a one - dimensional ( 1D ) optical lattice 6 . By applying external beam beams 7 , 8 , it is possible to create a periodic modulation of the optical potential 9 .This leads to a periodic variation of the hopping frequency J ( t ) , which plays the role of a time - dependent Peierls phase 10 . As a consequence , the effective Hamiltonian describing our system gets time - periodic 11 .It has been shown recently 12 that the equivalent Schrödinger equation accepts solutions known as Floquet states 13 . They define the evolution of the wave function over one period T = 2π / ω 0 where ω 0 denotes the frequency of the periodic drive 14 .Since the Floquet states are not stationary but rather oscillate at the same frequency as the drive 15 , they may exhibit exciting physical features 16 . For instance , Floquet engineering enables us to realize unusual superfluidity 17 , non - Abelian anyons 18 , and Major",
        "rewrite_text": "**Title:** Exact Floquet States of a Pushed Condensate and Their Stabilities\n\n**Abstract:** This study investigates the exact Floquet states of a Bose-Einstein condensate (BEC) situated in an optical lattice subjected to periodic driving, achieved by modulating the depth of the optical potential. We identify two distinct categories of Floquet states based on their quasienergy characteristics: those with zero quasienergy, which correspond to conventional Bloch states, and those with nonzero quasienergy, referred to as Floquet-Bloch states. Notably, the structure of the Floquet-Bloch states emerges from the hybridization of various Bloch states with opposite momenta, revealing a rich interplay between these two types of states. We further analyze the dynamical evolution of these Floquet states in response to changes in system parameters, providing insights into their behavior under external influences. Additionally, we assess the stability of the Floquet states against small perturbations, which is crucial for understanding their robustness in practical applications. Our findings contribute to the broader understanding of periodically-driven quantum systems, shedding light on the underlying physics that governs these phenomena.\n\n**Introduction:** Recent advancements in experimental techniques have facilitated the realization of artificial gauge fields, synthetic dimensions, topological phases, and even periodic crystals within ultracold atomic gases confined in optical lattices. In this work, we focus on a Bose-Einstein condensate (BEC) localized in a one-dimensional optical lattice. By employing external laser beams, we can induce a periodic modulation of the optical potential, resulting in a time-dependent variation of the hopping frequency, J(t), which acts as a time-dependent Peierls phase. Consequently, the effective Hamiltonian governing our system becomes time-periodic. Recent studies have demonstrated that the corresponding Schrödinger equation admits solutions known as Floquet states, which describe the evolution of the wave function over a period T = 2π/ω₀, where ω₀ is the frequency of the periodic drive. Since Floquet states are inherently non-stationary, oscillating at the same frequency as the driving force, they can exhibit intriguing physical phenomena. For example, Floquet engineering opens avenues for realizing exotic superfluidity, non-Abelian anyons, and Majorana modes, thereby enriching the landscape of quantum physics.",
        "ori-fast-z-score": 0.3244428422615251,
        "water-fast-z-score": 6.5060765641102405,
        "rewrite-fast-z-score": 1.9595917942265426
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Note on the Inapproximability of Correlation Clustering .\nAbstract:\nWe show that correlation clustering is NP-hard to approximate within any constant factor, even for graphs with maximum degree three and clusters of size at most four.  We also present an algorithm which solves this problem exactly in time O(n3). Our results are based on reductions from the exact cover by 3-sets (X3C) problem. The X3C problem asks whether there exists a collection of subsets of a set S such that each element of S belongs to exactly 3 sets in the collection; it has been shown to be NP-complete. For more information about our work see http://arxiv.org/abs/1206.0571 . \nCorrelation clustering is one of several problems studied under the umbrella of  clustering ; these include k-means clustering, spectral clustering, and graph partitioning. It was introduced independently by Bansal et al., Blum et al., and Dasgupta et al. as follows.   Given a weighted undirected graph G = (V, E), where V denotes the vertices and E denotes the edges, we say that two vertices u, v ∈ V are adjacent if they share an edge e ∈ E. A cluster C ⊆ V is defined as a subset of nodes whose pairwise distances satisfy some threshold t > 0. More formally, given a distance function d : V × V → R+ , let dist(u,v) denote the shortest path between u and v; then, C is said to be a valid cluster if and only if for all pairs of nodes u, v ∈ C:  dist(u,v)  ≤ t",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Note on the Inapproximability of Correlation Clustering . Abstract : We see that correlation clustering is NP - hard to approximate within any constant factor , even for graphs with maximum degree three and clusters of diameter at most four .We also present an algorithm which solves this question exactly in time O ( n3 ) . Our results are based on reductions from the exact cover by 3 - sets ( X3C ) question .The X3C problem questions whether there exists a family of subsets of a set S such that each element of S belongs to exactly 3 sets in the collection ; it has been shown to be NP - full . For more information about our work visit http : / / arxiv . org / abs / 1206 . 0571 .Correlation clustering is one of several difficulties investigated under the umbrella of clustering ; these involve k - means clustering , spectral clustering , and graph partitioning . It was introduced independently by Bansal et al . , Blum et al . , and Dasgupta et al .as follows . Given a weighted undirected graph G = ( V , E ) , where V denotes the edges and E denotes the edges , we mean that two vertices u , v ∈ V are adjoining if they share an path w ∈ E . A cluster C ⊆ V is characterized as a subset of vertices whose pairwise distances satisfy some threshold t > 0 .More generally , given a distance function d : V × V → R + , let dist ( v , v ) define the shortest path between u and v ; then , C is said to be a valid cluster if and only if for all pairs of vertices u , v ∈ C : dist ( v , v ) ≤ t",
        "rewrite_text": "**Title:** A Note on the Inapproximability of Correlation Clustering\n\n**Abstract:** This article addresses the computational complexity of correlation clustering, demonstrating that it is NP-hard to approximate within any constant factor, even when restricted to graphs with a maximum degree of three and clusters with a diameter of at most four. We introduce an exact algorithm that resolves this problem in O(n^3) time complexity. Our findings are derived from reductions related to the Exact Cover by 3-Sets (X3C) problem, which asks whether a given set S can be covered by a collection of subsets such that each element of S is included in exactly three of these subsets; this problem is known to be NP-complete. For further details on our research, please refer to our publication at http://arxiv.org/abs/1206.0571.\n\nCorrelation clustering is one of several challenging problems explored within the broader field of clustering, which also encompasses methods such as k-means clustering, spectral clustering, and graph partitioning. The concept of correlation clustering was independently introduced by researchers Bansal, Blum, and Dasgupta. In this context, we consider a weighted undirected graph G = (V, E), where V represents the vertices and E denotes the edges. Two vertices u and v in V are considered adjacent if there exists a path w in E connecting them. A cluster C, which is a subset of vertices, is defined by the condition that the pairwise distances between its members adhere to a specified threshold t > 0. More formally, with a distance function d: V × V → R+, we define dist(u, v) as the shortest path distance between vertices u and v. A subset C qualifies as a valid cluster if, for every pair of vertices u, v in C, the distance dist(u, v) is less than or equal to t. This study contributes to the understanding of the limitations of approximation algorithms in correlation clustering and highlights the intricate relationship between clustering problems and established NP-complete problems.",
        "ori-fast-z-score": -1.436739427831727,
        "water-fast-z-score": 2.942389786832747,
        "rewrite-fast-z-score": 0.3508232077228117
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Radio Through X-ray Spectral Energy Distributions of 38 Broad Absorption Line Quasars .\nAbstract:\nWe present the radio through X-ray spectral energy distributions (SEDs) for 38 quasars with broad absorption lines in their optical spectra, selected from the Sloan Digital Sky Survey Data Release 5 quasar catalog and observed by Chandra and/or XMM-Newton. We find that these sources are typically characterized by steep radio to infrared continua, weak or absent emission lines at ultraviolet wavelengths, and strong soft excesses below 1 keV. The majority of our sample show evidence for significant intrinsic reddening as indicated by the presence of deep UV troughs and high values of the Balmer decrement. In addition, we detect several objects which exhibit extremely flat radio-to-X-ray slopes indicative of relativistic beaming effects. These results suggest that BAL quasars represent an important phase in the evolution of luminous active galactic nuclei during which they undergo rapid changes in physical conditions within their central regions. This is supported by recent theoretical models suggesting that BAL outflows may play an important role in regulating black hole growth via feedback processes. \n \n Keywords: Active Galactic Nuclei",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Radio Through X - ray Spectral Energy Distributions of 38 Broad Absorption Line Quasars . Abstract : We present the radio through X - ray spectral power distributions ( SEDs ) for 38 quasars with broad absorption lines in their optical spectra , selected from the Sloan Digital Sky Survey Data Release 5 quasar catalog and seen by Chandra and / or XMM - Newton .We see that these sources are typically characterized by steep radio to infrared continua , weak or omitted radiation lines at ultraviolet wavelengths , and strong soft excesses below 1 keV . The majority of our sample indicate evidence for significant intrinsic reddening as indicated by the presence of deep UV troughs and large values of the Balmer decrement .In addition , we find several bodies which exhibit exceptionally flat radio - to - X - ray curves indicative of relativistic beaming effects . These data suggest that BAL quasars represent an important process in the evolution of luminous active galactic nuclei during which they undergo rapid variations in physical conditions within their central regions .This is backed by recent theoretical models suggesting that BAL outflows might play an important role in controlling black hole growth via feedback systems . Keywords : Active Galactic Nuclei",
        "rewrite_text": "Title: Radio to X-ray Spectral Energy Distributions of 38 Broad Absorption Line Quasars\n\nAbstract: In this study, we present the spectral energy distributions (SEDs) spanning radio to X-ray wavelengths for a sample of 38 quasars exhibiting broad absorption lines in their optical spectra. These quasars were selected from the Sloan Digital Sky Survey Data Release 5 catalog and have been observed by the Chandra and/or XMM-Newton space telescopes. Our analysis reveals that these quasars typically display steep continua from radio to infrared wavelengths, alongside weak or absent emission lines in the ultraviolet region. Notably, we observe pronounced soft excess emissions below 1 keV in the X-ray spectra. A significant portion of our sample shows indications of intrinsic reddening, as evidenced by the presence of deep ultraviolet troughs and elevated Balmer decrement values. Furthermore, we identify several quasars with remarkably flat radio-to-X-ray spectral slopes, which suggest the influence of relativistic beaming effects. These findings imply that broad absorption line (BAL) quasars may play a crucial role in the evolutionary processes of luminous active galactic nuclei (AGNs), particularly during phases of rapid changes in the physical conditions of their central regions. This notion is supported by recent theoretical models proposing that BAL outflows could significantly impact black hole growth through feedback mechanisms. Our results contribute to a deeper understanding of the complex interplay between quasars and their environments, highlighting the importance of BAL quasars in the broader context of AGN evolution. \n\nKeywords: Active Galactic Nuclei, Broad Absorption Line Quasars, Spectral Energy Distributions, Black Hole Growth, Feedback Mechanisms.",
        "ori-fast-z-score": 0.21320071635561041,
        "water-fast-z-score": 4.7699904600286205,
        "rewrite-fast-z-score": 0.5720775535473553
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Neutral genetic drift can aid functional protein evolution .\nAbstract:\nThe rate at which new mutations are fixed in populations is determined by the balance between natural selection and genetic drift, but how these forces interact to shape the evolution of proteins remains poorly understood.  Here we show that neutral genetic drift can accelerate the fixation of beneficial mutations within genes encoding functionally important regions of proteins. We used deep mutational scanning to measure the fitness effects of all possible single amino acid substitutions for two enzymes (T4 lysozyme and TEM-1 β-lactamase) across their entire sequence space. By combining this data with population genetics simulations, we found that neutral genetic drift increased the probability of fixing beneficial mutations in both enzymes  active sites. This effect was particularly pronounced when the number of adaptive mutations available per generation was low or when there were many competing deleterious mutations. Our results suggest that neutral genetic drift may play an underappreciated role in shaping the evolution of proteins.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Neutral genetic drift can aid functional protein development . Abstract : The rate at which new mutations are fixed in populations is chosen by the balance between natural selection and biological drift , but how these forces interact to shape the evolution of proteins appears poorly explored .Here we prove that neutral genetic drift can accelerate the fixation of favorable genes within genes encoding functionally essential regions of proteins . We utilized deep mutational scanning to measure the fitness impacts of all possible single amino residue substitutions for two enzymes ( T4 lysozyme and TEM - 1 β - lactamase ) across their whole sequence area .By combining this data with population genetics simulations , we identified that neutral genetic drift enhanced the probability of fixing beneficial mutations in both enzymes active sites . This phenomenon was especially pronounced when the proportion of adaptive traits usable per generation was poor or when there were several different deleterious traits .Our results show that neutral genetic drift might play an underappreciated importance in shaping the evolution of proteins .",
        "rewrite_text": "**Title:** Neutral Genetic Drift Can Facilitate Functional Protein Development\n\n**Abstract:** The interplay between natural selection and genetic drift significantly influences the fixation rate of new mutations within populations; however, the specific dynamics of how these forces interact to drive protein evolution remain inadequately understood. In this study, we demonstrate that neutral genetic drift can enhance the fixation of advantageous mutations within genes that encode functionally critical regions of proteins. To investigate this phenomenon, we employed deep mutational scanning to assess the fitness effects of all possible single amino acid substitutions in two enzymes: T4 lysozyme and TEM-1 β-lactamase, analyzing their entire sequence. By integrating this empirical data with population genetics simulations, we discovered that neutral genetic drift significantly increased the likelihood of beneficial mutations becoming fixed in the active sites of both enzymes. This effect was particularly pronounced under conditions where the availability of adaptive traits per generation was limited or when multiple deleterious traits were present. Our findings suggest that neutral genetic drift may play a more crucial role in protein evolution than previously recognized, highlighting its potential to facilitate the development of functional proteins by promoting the retention of advantageous mutations in the face of competing deleterious variations. This research not only sheds light on the complexities of evolutionary dynamics but also emphasizes the need to reconsider the contributions of genetic drift in the context of protein function and adaptation.",
        "ori-fast-z-score": -1.5230192477004287,
        "water-fast-z-score": 4.522670168666455,
        "rewrite-fast-z-score": -1.2888044650576527
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Light Quanta Modulated Physiological Response of Brassica Juncea Seedlings Subjected to Ni(II) Stress .\nAbstract:\nThe effect of light quanta on the physiological response of seedlings subjected to nickel (Ni2+) stress was investigated in this study. The results showed that the growth and photosynthetic characteristics were significantly affected by different treatments, especially when exposed to high concentration of Ni2+ for 24 h. Compared with control group, the shoot length decreased by 39% under 100 μM Ni2+ treatment; however, it increased by 20% after exposure to red light (RL). In addition, the root length also reduced by 44% compared with control plants but increased by 25% following RL illumination. Moreover, the chlorophyll content per unit leaf area declined by 42% due to Ni2+ toxicity while it increased by 34% as a result of RL application. Furthermore, the net photosynthesis rate (Pn), stomatal conductance (gs), intercellular CO2 concentration (Ci), transpiration rate (Tr), and water use efficiency (WUE) all decreased by 46%, 49%, 50%, 51%, and 53% respectively at 100 μM Ni2+ treated plants whereas they increased by 29%, 28%, 30%, 31%, and 32% respectively upon exposure to RL. These findings suggested that the effects of Ni2+ toxicity could be alleviated through the application of RL.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Light Quanta Modulated Physiological Response of Brassica Juncea Seedlings Subjected to Ni ( II ) Stress . Abstract : The impact of light quanta on the physiological response of seedlings subjected to nickel ( Ni2 + ) tension was investigated in this study .The results showed that the development and photosynthetic characteristics were significantly impacted by various methods , particularly when exposed to large concentration of Ni2 + for 24 h . Compared with control group , the shoot length decreased by 39 % under 100 μM Ni2 + treatment ; however , it increased by 20 % after sensitivity to yellow light ( RL ) . In addition , the root length also declined by 44 % compared with control plants but improved by 25 % subsequent RL illumination .Moreover , the chlorophyll content per unit leaves area declined by 42 % due to Ni2 + toxicity while it increased by 34 % as a result of RL application . Furthermore , the net photosynthesis percentage ( Pn ) , stomatal conductance ( gs ) , intercellular CO2 concentration ( Ci ) , transpiration rate ( Tr ) , and moisture supply efficiency ( WUE ) all decreased by 46 % , 49 % , 50 % , 51 % , and 53 % respectively at 100 μM Ni2 + treated facilities whereas they increased by 29 % , 28 % , 30 % , 31 % , and 32 % respectively upon exposure to RL .These studies suggested that the effects of Ni2 + toxicity may be alleviated through the application of RL .",
        "rewrite_text": "**Title:** The Light Quanta Modulated Physiological Response of Brassica Juncea Seedlings Subjected to Ni (II) Stress\n\n**Abstract:** This study explores the influence of light quanta on the physiological responses of Brassica juncea seedlings exposed to nickel (Ni²⁺) stress. The findings reveal that various light treatments significantly affect the growth and photosynthetic attributes of the seedlings, particularly under high concentrations of Ni²⁺ over a 24-hour period. Specifically, seedlings treated with 100 μM Ni²⁺ exhibited a 39% reduction in shoot length compared to the control group; however, exposure to red light (RL) resulted in a 20% increase in shoot length. Similarly, root length decreased by 44% under Ni²⁺ stress, but RL treatment led to a 25% enhancement in root growth. The chlorophyll content per unit leaf area also suffered a 42% decline due to Ni²⁺ toxicity, while RL application resulted in a 34% increase in chlorophyll levels. Furthermore, key photosynthetic parameters were adversely affected by Ni²⁺ exposure, with net photosynthesis (Pn), stomatal conductance (gs), intercellular CO₂ concentration (Ci), transpiration rate (Tr), and water use efficiency (WUE) decreasing by 46%, 49%, 50%, 51%, and 53%, respectively, at the 100 μM Ni²⁺ concentration. In contrast, these parameters improved significantly upon exposure to RL, with increases of 29%, 28%, 30%, 31%, and 32%, respectively. The results of this investigation suggest that the detrimental effects of Ni²⁺ toxicity on Brassica juncea seedlings can be mitigated through the application of red light, highlighting the potential for light modulation as a strategy to enhance plant resilience under heavy metal stress.",
        "ori-fast-z-score": 0.4364357804719848,
        "water-fast-z-score": 5.673665146135802,
        "rewrite-fast-z-score": 0.6965260331469925
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  SBF: multi-wavelength data and models .\nAbstract:\nThe Spitzer Bright Field (SBF) is an all-sky survey at 3.6, 4.5, 5.8 & 8 microns carried out by the Infrared Array Camera on board the Spitzer Space Telescope.  The SBF was designed to provide deep infrared photometry for extragalactic studies in order to complement existing optical surveys such as the Sloan Digital Sky Survey.   This dataset contains images taken with IRAC channel 1 (3.6 micron), channel 2 (4.5 micron), channel 3 (5.8 micron) and channel 4 (8 micron). Each image has been processed using the MOPEX software package developed by the Spitzer Science Center. These images are available through the NASA/IPAC Extragalactic Database (NED).  For more information about this project please see http://irsa.ipac.caltech.edu/data/SPITZER/docs/sbf/sbf-intro.html",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : SBF : multi - wavelength information and models . Abstract : The Spitzer Bright Field ( SBF ) is an all - skies study at 3 . 6 , 4 . 5 , 5 . 8 & 8 microns taken out by the Infrared Array Camera on board the Spitzer Space Telescope .The SBF was built to provide deep infrared photometry for extragalactic studies in order to complement existing imaging observations such as the Sloan Digital Sky Survey . This dataset contains images took with IRAC channel 1 ( 3 . 6 micron ) , channel 2 ( 4 . 5 micron ) , channel 3 ( 5 . 8 micron ) and channel 4 ( 8 micron ) .Each image has been processed utilizing the MOPEX software suite created by the Spitzer Science Center . These photographs are available through the NASA / IPAC Extragalactic Database ( NED ) .For more information about this project please view http : / / irsa . ipac . caltech . edu / data / SPITZER / docs / sbf / sbf - intro . html",
        "rewrite_text": "**Title: SBF: Multi-Wavelength Information and Models**\n\n**Abstract:** The Spitzer Bright Field (SBF) represents a comprehensive all-sky survey conducted at infrared wavelengths of 3.6, 4.5, 5.8, and 8 microns, utilizing the Infrared Array Camera (IRAC) aboard the Spitzer Space Telescope. This initiative was designed to deliver deep infrared photometry that enhances extragalactic research, serving as a valuable complement to existing optical imaging datasets, such as those provided by the Sloan Digital Sky Survey. The SBF dataset comprises a series of images captured across the four IRAC channels, specifically channel 1 (3.6 microns), channel 2 (4.5 microns), channel 3 (5.8 microns), and channel 4 (8 microns). Each image has undergone meticulous processing using the MOPEX software suite, developed by the Spitzer Science Center, ensuring high-quality data suitable for scientific analysis. These processed images are accessible through the NASA/IPAC Extragalactic Database (NED), facilitating their use in various astrophysical studies. The SBF dataset not only provides critical insights into the infrared properties of celestial objects but also aids in the understanding of their formation and evolution across cosmic time. Researchers interested in exploring this extensive dataset and its applications are encouraged to visit the project’s dedicated webpage for further details and resources. This study underscores the importance of multi-wavelength observations in astronomy, highlighting how infrared data can significantly enhance our comprehension of the universe. For additional information about the SBF project, please refer to the following link: http://irsa.ipac.caltech.edu/data/SPITZER/docs/sbf/sbf-intro.html.",
        "ori-fast-z-score": -0.30151134457776363,
        "water-fast-z-score": 4.024922359499621,
        "rewrite-fast-z-score": -0.8164965809277261
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Rates and Characteristics of Intermediate Mass Ratio Inspirals Detectable by Advanced LIGO .\nAbstract:\nWe present the results of an analysis to determine rates, masses, spins, and luminosities for intermediate mass ratio inspiral (IMRI) events detectable with advanced gravitational wave detectors such as Advanced LIGO. We use Monte Carlo simulations to generate IMRIs in galactic binaries that are consistent with current observations of binary pulsars and X-ray binaries. The simulated systems evolve through three phases: detached phase, Roche lobe overflow phase, and common envelope phase. In our simulation we assume that all stars have solar metallicity and initial spin periods of 10 days. For each system generated, we calculate its signal-to-noise ratio using the stationary phase approximation. We find that there will be about one event per year within 100 Mpc with signal-to-noise ratios greater than 8. This is comparable to the rate expected for double neutron star mergers. However, unlike double neutron star mergers which occur at high redshifts, most IMRI events should be detected nearby.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Rates and Characteristics of Intermediate Mass Ratio Inspirals Detectable by Advanced LIGO . Abstract : We report the results of an assessment to obtain rates , masses , spins , and luminosities for intermediate mass ratio inspiral ( IMRI ) events detectable with advanced gravitational wave detectors such as Advanced LIGO .We use Monte Carlo simulations to produce IMRIs in galactic binaries that are compatible with current observations of binary pulsars and X - ray binaries . The simulated systems develop through three stages : detached phase , Roche lobe overflow phase , and shared envelope phase .In our model we suppose that all stars have solar metallicity and original spin intervals of 10 days . For each system generated , we determine its signal - to - noise proportion using the stationary phase approximation .We see that there will be about one incident per year within 100 Mpc with signal - to - noise ratios greater than 8 . This is analogous to the speed anticipated for double neutron star mergers .However , unlike dual neutron star mergers which occur at high redshifts , most IMRI events should be identified nearby .",
        "rewrite_text": "We present the findings of a comprehensive study aimed at evaluating the rates, masses, spins, and luminosities of intermediate mass ratio inspirals (IMRIs) that can be detected by advanced gravitational wave observatories, such as Advanced LIGO. Utilizing Monte Carlo simulations, we generate IMRI events within galactic binary systems that align with existing observations of binary pulsars and X-ray binaries. Our simulations encompass three distinct evolutionary phases: the detached phase, the Roche lobe overflow phase, and the shared envelope phase. In our model, we assume that all stars possess solar metallicity and have initial spin periods of 10 days. For each generated binary system, we calculate the signal-to-noise ratio (SNR) using the stationary phase approximation, which allows us to assess the detectability of these events. Our results indicate that approximately one IMRI event per year is expected to occur within a distance of 100 Mpc, with SNR values exceeding 8. This detection rate is comparable to that predicted for double neutron star mergers. However, a key distinction is that while double neutron star mergers are typically observed at high redshifts, the majority of IMRI events are anticipated to be detected in closer proximity. This research not only enhances our understanding of IMRI events but also contributes to the broader field of gravitational wave astronomy by providing insights into the characteristics and expected rates of these intriguing astrophysical phenomena.",
        "ori-fast-z-score": 0.22645540682891913,
        "water-fast-z-score": 4.695742752749558,
        "rewrite-fast-z-score": 0.9712858623572641
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Formation and Collisional Evolution of Kuiper Belt Objects .\nAbstract:\nThe Kuiper belt is the source region for most short period comets, but its formation remains poorly understood. We present results from N-body simulations that show how collisions among planetesimals in Neptune s feeding zone can produce objects with orbits similar to those observed today. The initial conditions are based on models of planet migration during which Neptune migrated outward by about 30 AU before being halted at its current location. Our calculations suggest that the Kuiper belt formed as a result of collisional grinding between bodies whose sizes were comparable to Pluto (r ~ 1000 km). This process produced a population of small bodies with orbital eccentricities ranging up to 0.3. Subsequent encounters with Neptune caused some of these bodies to be scattered into highly eccentric orbits. These results provide an explanation for why there appears to be no correlation between the size distribution of KBOs and their orbital eccentricity.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Formation and Collisional Evolution of Kuiper Belt Objects . Abstract : The Kuiper belt is the origin region for most short period comets , but its formation appears poorly studied .We present results from N - bodies simulations that demonstrate how collisions among planetesimals in Neptune s feeding area can generate objects with orbits similar to those observed nowadays . The initial conditions are based on estimates of planet migration during which Neptune migrated outward by about 30 AU before being stopped at its current site .Our calculations suggest that the Kuiper belt developed as a outcome of collisional crushing between bodies whose sizes were analogous to Pluto ( r ~ 1000 kilometers ) . This process produced a population of tiny bodies with orbital eccentricities ranging up to 0 . 3 .Subsequent experiences with Neptune resulted some of these objects to be scattered into extremely eccentric orbits . These data provide an reason for why there seems to be no correlation between the size distribution of KBOs and their orbital eccentricity .",
        "rewrite_text": "**Title: Formation and Collisional Evolution of Kuiper Belt Objects**\n\n**Abstract:** The Kuiper Belt, recognized as the source region for the majority of short-period comets, remains an area of limited understanding regarding its formation processes. In this study, we present findings from N-body simulations that elucidate how collisions among planetesimals within Neptune's feeding zone can lead to the creation of objects with orbital characteristics akin to those observed in the present-day Kuiper Belt. Our simulations are grounded in initial conditions that reflect the estimated migration of Neptune, which is believed to have moved outward by approximately 30 astronomical units (AU) before stabilizing in its current position. The results of our calculations indicate that the Kuiper Belt formed primarily as a result of collisional fragmentation among bodies comparable in size to Pluto (approximately 1000 kilometers in radius). This collisional process generated a diverse population of smaller bodies, exhibiting orbital eccentricities of up to 0.3. Additionally, interactions with Neptune subsequently scattered some of these objects into highly eccentric orbits. Our findings offer a compelling explanation for the observed lack of correlation between the size distribution of Kuiper Belt Objects (KBOs) and their orbital eccentricities, suggesting that the dynamical history of these bodies is intricately linked to their collisional evolution and the gravitational influences of Neptune. This research enhances our understanding of the Kuiper Belt's formation and the complex processes that have shaped the current distribution and characteristics of its constituent objects.",
        "ori-fast-z-score": -1.2874526191574363,
        "water-fast-z-score": 4.73427220735493,
        "rewrite-fast-z-score": -0.4
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Sensor Networks with Random Links: Topology Design for Distributed Consensus .\nAbstract:\nWe consider the problem of designing distributed consensus algorithms in sensor networks where links are randomly generated and can be lost or added over time. We propose an algorithm that is robust to link failures, but requires only local information exchange between neighboring nodes. The proposed algorithm achieves global convergence under mild conditions on network topology. In particular, we show that if each node has at least one neighbor whose degree is greater than its own then our algorithm converges almost surely (a.s.) to the correct value. Our results also hold when there exists a small number of Byzantine nodes which may deviate arbitrarily from their prescribed behavior. Finally, numerical simulations demonstrate the effectiveness of our approach. Keywords: Sensor Networks; Distributed Consensus; Local Information Exchange; Robustness Analysis. 1 Introduction Distributed consensus problems arise naturally in many applications such as flocking  1  , formation control  2  , multi-agent coordination  3  , wireless sensor networks  4  , etc.. A typical example is the average-consensus problem: given a set of n agents connected by communication links, each agent holds some initial data xi(0) ∈ Rm, i = 1, ..., n; it aims to compute the average x̄=1/n∑in=1xi(0). This problem was first studied by Tsitsiklis et al.  5  . They showed that if all agents have access to the same fixed directed graph G, then the average-consensus problem can be solved using a simple linear iterative scheme. However, this assumption does not always hold true since the underlying communication graphs are often random due to unreliable links  6  .\nIn recent years, several researchers have investigated the design of distributed consensus algorithms in dynamic networks  7-10 . For instance, Olfati-Saber  7  considered the case where the communication links among agents change randomly according to independent Bernoulli processes. Under certain assumptions on the connectivity of the network, she proved that her algorithm converges almost surely (i.e., with probability one) to the desired average. Subsequently, Jadbabaie et al.  8  extended these results to undirected networks. More recently,",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Sensor Networks with Random Links : Topology Design for Distributed Consensus . Abstract : We consider the question of constructing dispersed consensus algorithms in sensor networks where links are randomly generated and can be lost or added over time .We suggest an algorithm that is robust to link errors , but requires only local information transfer between neighboring vertices . The proposed algorithm achieves global convergence under mild conditions on connection geometry .In particular , we prove that if each node has at least one neighbor whose degree is greater than its own then our algorithm converges almost always ( a . s . ) to the appropriate value . Our results also follow when there exists a small number of Byzantine nodes which may deviate arbitrarily from their recommended performance .Finally , numerical simulations prove the performance of our approach . Keywords : Sensor Networks ; Distributed Consensus ; Local Information Exchange ; Robustness Analysis .1 Introduction Distributed consensus issues exist naturally in different applications such as flocking 1 , structure control 2 , multi - agent coordination 3 , wireless sensor networks 4 , etc . . A typical example is the average - consensus problem : considering a setting of n agents connected by communication connections , each agent possesses some initial data xi ( 0 ) ∈ Rm , i = 1 , . . . , n ; it aims to compute the average [UNK] = 1 / [UNK] = 1xi ( 0 ) . This problem was first examined by Tsitsiklis et al .5 . They showed that if all agents have access to the same fixed directed graph G , then the average - consensus problem can be answered using a simple linear iterative plan .However , this assumption does not always hold false since the underlying communication graphs are often random resulting to unreliable ties 6 . In recent years , various researchers have researched the development of distributed consensus algorithms in dynamic networks 7 - 10 .For instance , Olfati - Saber 7 considered the case where the communication connections among agents move randomly according to independent Bernoulli processes . Under certain assumptions on the connectivity of the network , she proved that her algorithm converges almost inevitably ( i . e . , with probability one ) to the desired average .Subsequently , Jadbabaie et al . 8 extended these results to undirected networks .More recently,",
        "rewrite_text": "In this article, we explore the development of distributed consensus algorithms within sensor networks characterized by randomly generated links that may be subject to loss or addition over time. Our focus is on creating an algorithm that maintains robustness against link failures while relying solely on local information exchange among neighboring nodes. We demonstrate that this algorithm can achieve global convergence under relatively lenient conditions regarding the network's connection geometry. Specifically, we establish that if each node is connected to at least one neighbor with a higher degree than itself, the algorithm will almost surely converge to the desired consensus value. Additionally, our findings remain valid even in the presence of a limited number of Byzantine nodes that may act contrary to their expected behavior. To validate our theoretical results, we present numerical simulations that illustrate the effectiveness of our proposed approach.\n\nThe issue of distributed consensus is prevalent across various applications, including flocking, structural control, multi-agent coordination, and wireless sensor networks. A classic example of this challenge is the average-consensus problem, where a group of n agents, each with initial data \\(x_i(0) \\in \\mathbb{R}^m\\), seeks to compute the average value \\( \\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i(0) \\). This problem was initially addressed by Tsitsiklis et al., who demonstrated that, given a fixed directed graph \\(G\\) connecting all agents, a straightforward linear iterative method could be employed to reach consensus. However, this assumption often fails in practice, as communication links are frequently unreliable and subject to randomness. Recent research has focused on developing distributed consensus algorithms that can function effectively in dynamic networks. For instance, Olfati-Saber investigated scenarios where communication links among agents evolve according to independent Bernoulli processes, proving that under certain connectivity conditions, her algorithm converges with high probability to the desired average. Jadbabaie et al. later expanded these findings to undirected networks, further advancing the understanding of consensus in dynamic environments.",
        "ori-fast-z-score": 1.1538461538461537,
        "water-fast-z-score": 6.653056282246291,
        "rewrite-fast-z-score": 0.680336051416609
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Mapping the Youngest Galaxies to Redshift One .\nAbstract:\nWe present deep near-infrared imaging and spectroscopy for two fields in the Hubble Deep Field South (HDF-S) covering an area of 0.5 square degrees each, centered on RA = 03:32:47.6 Dec = -27:48:30 (J2000). The data were obtained with ISAAC at VLT as part of our ESO Large Programme  First Stars First Galaxies . We have detected more than 1000 galaxies down to KsAB = 24 mag over this field using SExtractor. In addition we have spectroscopically confirmed about 100 objects selected by their photometric redshifts between z=0.7-1.3. This is one of the largest samples of high-z galaxies ever assembled. Our sample includes both Lyman-break galaxies and sub-millimeter sources which are likely to be dusty starbursting systems. These results will allow us to study galaxy formation and evolution up to redshift 1.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Mapping the Youngest Galaxies to Redshift One . Abstract : We create deep near - infrared imaging and spectroscopy for two fields in the Hubble Deep Field South ( HDF - S ) covering an area of 0 . 5 square degrees each , centered on RA = 03 : 32 : 47 . 6 Dec = - 27 : 48 : 30 ( J2000 ) .The data were obtained with ISAAC at VLT as part of our ESO Large Programme First Stars First Galaxies . We have discovered more than 1000 objects down to KsAB = 24 mag over this field using SExtractor .In addition we have spectroscopically confirmed about 100 galaxies selected by their photometric redshifts between z = 0 . 7 - 1 . 3 . This is one of the largest samples of high - z galaxies yet assembled .Our specimen includes both Lyman - break galaxies and sub - millimeter sources which are likely to be dusty starbursting systems . These data will aid us to study galaxy formation and evolution up to redshift 1 .",
        "rewrite_text": "In this study, we present comprehensive near-infrared imaging and spectroscopy conducted in two regions of the Hubble Deep Field South (HDF-S), each covering an area of 0.5 square degrees and centered at coordinates RA = 03:32:47.6, Dec = -27:48:30 (J2000). The observations were carried out using the Infrared Spectrometer And Array Camera (ISAAC) at the Very Large Telescope (VLT) as part of our European Southern Observatory (ESO) Large Programme titled \"First Stars, First Galaxies.\" Our analysis has led to the identification of over 1,000 celestial objects with a magnitude limit of KsAB = 24 mag, utilizing the SExtractor software for object detection. Furthermore, we have successfully spectroscopically confirmed approximately 100 galaxies, selected based on their photometric redshifts, which fall within the range of z = 0.7 to 1.3. This collection represents one of the most extensive samples of high-redshift galaxies compiled to date. The sample encompasses both Lyman-break galaxies and sub-millimeter sources, which are anticipated to be dusty starburst galaxies. The insights gained from this dataset will significantly contribute to our understanding of galaxy formation and evolution up to redshift 1, providing a valuable resource for future research in the field of astrophysics.",
        "ori-fast-z-score": 0.42008402520840293,
        "water-fast-z-score": 3.6055512754639896,
        "rewrite-fast-z-score": 1.649915822768611
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Large Attractive Depletion Interactions in Soft Repulsive-Sphere Binary Mixtures .\nAbstract:\nWe study the phase behavior and structure of binary mixtures composed of soft repulsive spheres with attractive depletants, which are modeled as hard-spheres that interact only via excluded volume interactions. We find that these systems exhibit rich phase diagrams including gas-liquid coexistence at low temperatures for all compositions studied here (0.25 < f < 0.75), where f is the fraction of particles made up by the smaller species. The liquid-gas binodal lines shift to higher pressures upon increasing the size ratio between the two components. For large size ratios we observe an additional fluid-fluid transition line along which both fluids have similar densities but different structures. This new fluid state has been observed experimentally in colloidal suspensions containing nonadsorbing polymer chains. Our results show good agreement with experimental data on colloid-polymer mixtures over wide ranges of temperature, pressure, and composition. \nI. INTRODUCTIO N\nThe presence of small particles can dramatically affect the properties of larger ones through depletion forces  1  . These effects play important roles in many physical phenomena such as protein crystallization  2  , gelation  3  , and sedimentation  4  .\nDepending on their sizes relative to each other, the mixture may be either miscible or immiscible  5  . In addition, there exist regions of metastability  6  and even multiple phases  7, 8  . A number of theoretical studies  9  -  11  have investigated the effect of depletion attractions on the phase diagram of simple model systems. However, most of them focused on idealized models neglecting hydrodynamic interactions  12  , finite-size effects  13  , polydispersity  14  , and particle shape  15  . Only recently did some authors  16  take into account more realistic features like Brownian motion  17  , electrostatic repulsion  18  , and van der Waals attraction  19  . Despite this progress, it remains difficult to predict the exact location of the critical point  20  due to strong correlations  21  among the particles  22  . Moreover, the influence of depletion forces on the structural  23  and dynamical  24  properties of complex fluids still needs further investigation  25  .\nIn recent years, experiments  26",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Large Attractive Depletion Interactions in Soft Repulsive - Sphere Binary Mixtures . Abstract : We research the phase response and form of binary mixtures consisting of soft repulsive spheres with interesting depletants , which are modeled as hard - spheres that interact only via excluded volume interactions .We see that these systems exhibit strong phase diagrams including gas - fluid coexistence at low temperatures for all compositions studied here ( 0 . 25 < f < 0 . 75 ) , where f is the fraction of particles formed up by the smaller species . The liquid - gas binodal lines shift to higher pressures upon increasing the height factor between the two parts .For large size ratios we study an additional liquid - fluid change line along which both fluids have equal densities but different structures . This new fluid state has been observed experimentally in colloidal suspensions containing nonadsorbing polymer complexes .Our results show good agreement with experimental evidence on colloid - polymer mixtures over broad scales of temperature , pressure , and composition . I . INTRODUCTIO N The presence of tiny particles can dramatically impact the properties of bigger ones through depletion forces 1 .These effects play important roles in many natural transformations such as protein crystallization 2 , gelation 3 , and sedimentation 4 . Depending on their weights relative to each other , the mixture might be either miscible or immiscible 5 .In addition , there exist zones of metastability 6 and even multiple cycles 7 , 8 . A several of theoretical experiments 9 - 11 have explored the impact of depletion attractions on the phase diagram of simple model structures .However , most of them focused on idealized models neglecting hydrodynamic interactions 12 , finite - height effects 13 , polydispersity 14 , and particle shape 15 . Only lately did some writers 16 taking into consideration more realistic characteristics like Brownian movement 17 , electrostatic repulsion 18 , and van der Waals attraction 19 .Despite this progress , it remains harder to predict the exact location of the important position 20 due to large correlations 21 among the molecules 22 . Moreover , the impact of depletion forces on the structural 23 and dynamical 24 properties of complex fluids already requires further investigation 25 .In recent years , experiments 26",
        "rewrite_text": "**Title:** Large Attractive Depletion Interactions in Soft Repulsive-Sphere Binary Mixtures\n\n**Abstract:** This study investigates the phase behavior and structural characteristics of binary mixtures composed of soft repulsive spheres and intriguing depletants, which are represented as hard spheres interacting solely through excluded volume effects. Our findings reveal that these systems display complex phase diagrams, including gas-fluid coexistence at low temperatures across a range of compositions (0.25 < f < 0.75), where f denotes the fraction of smaller species in the mixture. Notably, we observe that the liquid-gas binodal lines shift to higher pressures with an increase in the size disparity between the two particle types. For systems with significant size ratios, we identify an additional liquid-fluid transition line, along which both fluid phases maintain equal densities but exhibit distinct structural properties. This novel fluid state has been corroborated by experimental observations in colloidal suspensions containing non-adsorbing polymer complexes. Our results align well with existing experimental data on colloid-polymer mixtures across a wide range of temperatures, pressures, and compositions.\n\nThe influence of small particles on the properties of larger ones through depletion forces is profound, playing a critical role in various natural processes such as protein crystallization, gelation, and sedimentation. The miscibility of the mixture is contingent upon the relative sizes of the particles, leading to either miscible or immiscible states. Additionally, we explore the existence of metastable regions and the potential for multiple cycles within the phase behavior. Previous theoretical studies have examined the effects of depletion attractions on the phase diagrams of simplified models; however, many have overlooked essential factors such as hydrodynamic interactions, finite-size effects, polydispersity, and particle shape. Recent advancements have begun to incorporate more realistic features, including Brownian motion, electrostatic repulsion, and van der Waals forces. Despite these developments, accurately predicting critical points remains challenging due to significant molecular correlations. Furthermore, the role of depletion forces in influencing the structural and dynamical properties of complex fluids warrants further exploration.",
        "ori-fast-z-score": -0.3104602102825331,
        "water-fast-z-score": 6.9853547313569955,
        "rewrite-fast-z-score": 1.6448469449747105
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Detection of Emission from the CN Radical in the Cloverleaf Quasar at z=2.56 .\nAbstract:\nWe report on observations made with the Atacama Large Millimeter/submillimeter Array (ALMA) that reveal emission lines associated with carbon monoxide and its isotopologue, 13CO, as well as the CN radical toward the quasar host galaxy at redshift 2.56 known as the  Cloverleaf  source.  The observed line ratios are consistent with those expected for gas exposed to intense radiation fields characteristic of quasars. We also detect absorption by molecular hydrogen along this sightline through intervening clouds located between us and the quasar host galaxy. These results provide new insights into the physical conditions within the interstellar medium surrounding active galactic nuclei during their early evolutionary stages. This is an open access article under the terms of the Creative Commons Attribution License, which permits use, distribution and reproduction in any medium, provided the original work is properly cited. \nThe detection of carbon monoxide (CO), one of the most abundant molecules in space, has been used extensively over the past several decades to study the properties of cold neutral atomic and molecular gas in galaxies across cosmic time. However, CO can be difficult to observe directly because it lacks electric dipole moments and thus emits very weakly. In addition, the excitation temperature of the lowest rotational levels of CO is typically low enough such that these transitions fall outside of the frequency range accessible to ground-based telescopes operating at millimeter wavelengths. As a result, much of our understanding about the physical conditions present in dense regions of star-forming galaxies comes from studies of other tracers of molecular gas, including HCN, H2S, CS, CH3OH, H2O, and OH+.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Detection of Emission from the CN Radical in the Cloverleaf Quasar at z = 2 . 56 . Abstract : We report on observations made with the Atacama Large Millimeter / submillimeter Array ( ALMA ) that indicate emission lines linked with carbon monoxide and its isotopologue , 13CO , as also as the CN radical toward the quasar host galaxy at redshift 2 . 56 referred as the Cloverleaf source .The observed line values are consistent with those expected for gas exposed to intense radiation fields distinctive of quasars . We additionally observe absorption by molecular hydrogen along this sightline through intervening clouds situated between us and the quasar host universe .These data provide fresh insights into the physical conditions within the interstellar medium comprising active galactic nuclei during their early evolutionary stages . This is an free access section under the terms of the Creative Commons Attribution License , which allows use , distribution and reproduction in any medium , provided the original book is properly cited .The measurement of carbon monoxide ( CO ) , one of the most stable compounds in space , has been used heavily over the previous several decades to study the properties of cold neutral atomic and molecular gas in galaxies across cosmic time . However , CO can be harder to observe directly because it lacks electric dipole moments and therefore emits very weakly .In addition , the excitation temperature of the lowest rotational concentrations of CO is typically minimum enough such that these changes fall outside of the frequency limit accessible to ground - based telescopes operating at millimeter wavelengths . As a result , part of our knowing about the physical conditions present in dense areas of galaxy - making clusters comes from studies of other tracers of molecular dust , notably HCN , H2S , CS , CH3OH , H2O , and OH + .",
        "rewrite_text": "We present findings from observations conducted with the Atacama Large Millimeter/submillimeter Array (ALMA) that reveal emission lines associated with carbon monoxide (CO) and its isotopologue, 13CO, as well as the CN radical in the host galaxy of the Cloverleaf quasar at a redshift of 2.56. The detected emission lines align with theoretical predictions for gas subjected to the intense radiation fields characteristic of quasars. Additionally, we observe molecular hydrogen absorption along the line of sight, originating from intervening clouds situated between Earth and the quasar's host galaxy. These observations offer new perspectives on the physical conditions prevailing in the interstellar medium of active galactic nuclei during their formative stages.\n\nThe study of carbon monoxide, one of the most prevalent and stable molecules in the universe, has been instrumental in understanding the characteristics of cold neutral atomic and molecular gas in galaxies throughout cosmic history. However, direct observations of CO can be challenging due to its lack of electric dipole moments, resulting in weak emissions. Furthermore, the excitation temperatures of CO's lowest rotational states are often too low to be detected by ground-based telescopes operating in the millimeter wavelength range. Consequently, our understanding of the physical environments in dense regions of galaxy-forming clusters has relied heavily on alternative molecular tracers, including HCN, H2S, CS, CH3OH, H2O, and OH+. This research contributes to the broader understanding of molecular gas dynamics in the early universe and highlights the significance of utilizing advanced observational techniques to probe the complex interactions within active galactic nuclei. This article is published under the Creative Commons Attribution License, allowing for unrestricted use, distribution, and reproduction in any medium, provided the original work is appropriately cited.",
        "ori-fast-z-score": -1.5787044347526527,
        "water-fast-z-score": 6.075111253646805,
        "rewrite-fast-z-score": -0.3481553119113957
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A study of the evolution of the accretion disk of V2051 Oph through two outburst cycles .\nAbstract:\nWe present an analysis of optical and infrared photometric data obtained during the recent (2006-2008) outbursts of the dwarf novae system V2051 Oph, which is one of only three known to have exhibited both superoutbursts and normal outbursts in its lifetime.  We find that the light curve of this object shows many similarities with those observed for other SU UMa-type systems but also some significant differences. In particular we note that there are no clear signs of rebrightening following either the first or second superoutburst; nor do we see any evidence for a double-humped structure in the light curves at all phases of these events. The lack of such features may be due to the fact that our observations were made when the system was relatively faint compared to previous studies. However, it should be noted that the orbital period of V2051 Oph is significantly longer than most other SU UMa stars so that the mass transfer rate will be lower by about a factor of ten.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A investigation of the evolution of the accretion disk of V2051 Oph through two outburst cycles . Abstract : We report an assessment of optical and infrared photometric data acquired during the recent ( 2006 - 2008 ) outbursts of the dwarf novae system V2051 Oph , which is one of only three known to have exhibited both superoutbursts and normal outbursts in its lifetime .We see that the light curve of this object displays many similarities with those observed for other SU UMa - class systems but also some significant variations . In particular we note that there are no clear indication of rebrightening following either the first or second superoutburst ; nor do we find any evidence for a double - humped structure in the light curves at all phases of these events .The absence of such properties may be due to the fact that our observations were made when the system was quite dim relative to previous research . However , it should be mentioned that the orbital period of V2051 Oph is significantly greater than most other SU UMa stars so that the mass transfer time will be reduced by about a factor of ten .",
        "rewrite_text": "Title: An Investigation into the Evolution of the Accretion Disk of V2051 Oph Through Two Outburst Cycles\n\nAbstract: This study presents a comprehensive analysis of optical and infrared photometric data collected during the recent outbursts of the dwarf nova system V2051 Oph, which occurred between 2006 and 2008. V2051 Oph is notable for being one of only three known systems that have experienced both superoutbursts and normal outbursts throughout its history. Our findings reveal that the light curve of V2051 Oph shares several characteristics with those of other SU UMa-type systems; however, it also exhibits distinct differences. Notably, we observed a lack of rebrightening events following both the first and second superoutbursts, and there was no evidence of a double-humped structure in the light curves during any phase of these outbursts. This absence of expected features may be attributed to the fact that our observations were conducted when the system was relatively dim compared to prior studies. Additionally, it is important to highlight that V2051 Oph has a significantly longer orbital period than most other SU UMa stars, which implies that the mass transfer rate could be reduced by approximately a factor of ten. This reduction in mass transfer time may play a crucial role in the observed behavior of the system's outbursts and the evolution of its accretion disk. Our research contributes to a deeper understanding of the dynamics of V2051 Oph and offers insights into the broader phenomena associated with dwarf novae systems.",
        "ori-fast-z-score": 0.45291081365783825,
        "water-fast-z-score": 5.287913134352312,
        "rewrite-fast-z-score": 2.089578099440977
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Investigation of Energy Spectrum of EGRET Gamma-ray Sources by an Extensive Air Shower Experiment .\nAbstract:\nThe energy spectrum of gamma rays is one of the most important information to understand their origin and propagation in space, since it contains crucial information on the physical processes involved.  In this work we present results obtained with the Tibet ASγ experiment for the determination of the energy spectra of several bright sources detected by the Energetic Gamma Ray Experiment Telescope (EGRET) aboard the Compton Gamma-Ray Observatory satellite. The data were taken between 1997 and 2000 at Yangbajing Cosmic Ray Laboratory (Tibet). We have analyzed about 1 million events collected during these four years. By using Monte Carlo simulations based on CORSIKA code, we estimate that our detector has a detection efficiency above 80% for primary cosmic ray energies greater than 10 TeV. Our analysis shows that the observed fluxes are consistent with power law functions with spectral indices ranging from 2.0 to 3.5. These values agree well with those reported previously by other experiments.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Investigation of Energy Spectrum of EGRET Gamma - ray Sources by an Extensive Air Shower Experiment . Abstract : The energy spectrum of gamma radiation is one of the most important information to comprehend their source and propagation in space , since it contains vital information on the physical processes responsible .In this research we present results derived with the Tibet ASγ experiment for the determination of the power spectra of several bright sources detected by the Energetic Gamma Ray Experiment Telescope ( EGRET ) aboard the Compton Gamma - Ray Observatory satellite . The data were took between 1997 and 2000 at Yangbajing Cosmic Ray Laboratory ( Tibet ) .We have analyzed about 1 million events collected during these four seasons . By using Monte Carlo simulations based on CORSIKA code , we estimate that our camera has a detection efficiency above 80 % for principal cosmic ray energies higher than 10 TeV .Our study shows that the seen fluxes are compatible with power law functions with spectral indices ranging from 2 . 0 to 3 . 5 . These values comply good with those detected earlier by other experiments .",
        "rewrite_text": "Title: Investigation of Energy Spectrum of EGRET Gamma-ray Sources through an Extensive Air Shower Experiment\n\nAbstract: Understanding the energy spectrum of gamma radiation is crucial for deciphering the origins and propagation mechanisms of these high-energy photons in the universe. This study presents findings from the Tibet ASγ experiment, which aimed to determine the power spectra of several prominent gamma-ray sources identified by the Energetic Gamma Ray Experiment Telescope (EGRET) on the Compton Gamma-Ray Observatory satellite. Data collection occurred between 1997 and 2000 at the Yangbajing Cosmic Ray Laboratory in Tibet, where we analyzed approximately one million events gathered over four observational seasons. Utilizing Monte Carlo simulations based on the CORSIKA code, we assessed the detection efficiency of our experimental setup, which was found to exceed 80% for primary cosmic ray energies above 10 TeV. Our analysis indicates that the observed fluxes from the gamma-ray sources conform to power law distributions, with spectral indices ranging from 2.0 to 3.5. These findings are consistent with earlier results obtained from other experiments, reinforcing the reliability of our measurements. The implications of these spectral indices are significant, as they provide insights into the underlying physical processes that govern the emission of gamma rays from these sources. This research contributes to the broader understanding of high-energy astrophysical phenomena and enhances our knowledge of cosmic ray interactions in the atmosphere.",
        "ori-fast-z-score": 0.8307471607356973,
        "water-fast-z-score": 5.421151989096865,
        "rewrite-fast-z-score": -0.20628424925175867
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Magnetar Nature and the Outburst Mechanism of a Transient Anomalous X-ray Pulsar .\nAbstract:\nWe report on our analysis of the outburst mechanism of SGR 1806-20, which is one of three known magnetars (neutron stars with superstrong magnetic fields). We have analyzed all available data obtained by Swift/BAT during its first year in orbit to search for periodicities associated with this source. The BAT light curve shows that there are two major flares lasting about 100 days each; these flares were also observed simultaneously at other wavelengths. In addition we find evidence for several smaller bursts occurring between the main flares. Using an improved version of the method developed by Israel et al. (2008) we searched for pulsations in the BAT data corresponding to the time intervals when the source was active. No significant signal could be found above background noise levels down to a limit of 1 mCrab (3σ), but we did detect a weak signal below 0.5 mCrab. This signal has been confirmed using independent methods applied to different energy bands.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Magnetar Nature and the Outburst Mechanism of a Transient Anomalous X - ray Pulsar . Abstract : We report on our analysis of the outburst mechanism of SGR 1806 - 20 , which is one of three known magnetars ( neutron galaxies with superstrong magnetic fields ) .We have analyzed all available data acquired by Swift / BAT during its initial season in orbit to search for periodicities associated with this source . The BAT radiation curve shows that there are two major flares lasting about 100 days each ; these flares were also observed concurrently at other wavelengths .In addition we find proof for numerous smaller bursts happening between the main flares . Using an better version of the method developed by Israel et al .( 2008 ) we searched for pulsations in the BAT data related to the period intervals when the source was active . No important signal could be found above background noise values down to a limit of 1 mCrab ( 3σ ) , but we did detect a weak source below 0 . 5 mCrab .This signal has been confirmed using independent methods applied to different energy bands .",
        "rewrite_text": "We present a comprehensive analysis of the outburst mechanism associated with SGR 1806-20, one of the three identified magnetars, which are neutron stars characterized by their exceptionally strong magnetic fields. Our study utilizes all available data collected by the Swift/BAT during its inaugural operational season to investigate potential periodicities linked to this celestial source. The radiation curve obtained from the BAT reveals the presence of two significant flares, each lasting approximately 100 days, which were also detected across various other wavelengths, indicating a multifaceted outburst behavior. Furthermore, our analysis uncovers evidence of numerous smaller bursts occurring in the intervals between these major flares, suggesting a complex and dynamic activity pattern. To enhance our investigation, we employed an improved version of the methodology established by Israel et al. (2008) to search for pulsations in the BAT data corresponding to the periods of heightened activity. Despite our efforts, we did not identify any significant signals above the background noise threshold, which was measured down to a limit of 1 mCrab (3σ). However, we did detect a faint signal below the 0.5 mCrab level, which has been corroborated through independent analyses conducted across different energy bands. This finding contributes to our understanding of the magnetar phenomenon and the underlying mechanisms driving their outbursts, highlighting the need for further investigation into the nature of these enigmatic astrophysical objects.",
        "ori-fast-z-score": -0.4588314677411235,
        "water-fast-z-score": 4.216541329006604,
        "rewrite-fast-z-score": -0.502518907629606
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Bipolar spintronics: From spin injection to spin-controlled logic .\nAbstract:\nSpin-based electronics is an emerging field that has attracted considerable attention in recent years, due to its potential for applications such as high-density data storage and high-speed information processing.  In this review article we discuss the basic concepts underlying bipolar spintronic devices based on semiconductor heterostructures with ferromagnetic contacts. We first introduce the physics behind spin injection into semiconductors using tunnel barriers or Schottky diodes. Then we describe how these injected spins can be manipulated by means of external magnetic fields and/or electric currents. Finally, we present some examples of spintronic devices including spin-LEDs, spin transistors, and spin-logic circuits. The main focus will be put on GaAs-based structures but also other materials systems are discussed briefly. This article is intended to provide a comprehensive overview of the state-of-the-art research in the field of bipolar spintronics. It should serve both as a guide for newcomers interested in learning about the fundamentals of spin transport phenomena at interfaces between metals and semiconductors, and as a reference source for researchers working in related areas.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Bipolar spintronics : From spin injection to spinning - controlled logic . Abstract : Spin - based computing is an developing field that has garnered considerable scrutiny in recent years , owing to its potential for applications such as high - density data storage and large - speed information processing .In this review article we explain the fundamental concepts governing bipolar spintronic systems relying on semiconductor heterostructures with ferromagnetic contacts . We first introduce the physics behind spin injection into semiconductors using tunnel fences or Schottky diodes .Then we explain how these injected spinning can be manipulated by means of external magnetic fields and / or electric currents . Finally , we present some examples of spintronic systems including spin - LEDs , spin transistors , and spin - logic devices .The main center will be place on GaAs - based structures but also other materials models are discussed briefly . This page is intended to provide a comprehensive overview of the state - of - the - art studies in the field of bipolar spintronics .It should serve both as a reference for newcomers interested in understanding about the fundamentals of spin transport phenomena at connections between metals and semiconductors , and as a reference source for researchers focusing in related fields .",
        "rewrite_text": "**Title:** Bipolar Spintronics: From Spin Injection to Spin-Controlled Logic\n\n**Abstract:** Spin-based computing is an emerging area of research that has attracted significant attention in recent years due to its promising applications in high-density data storage and high-speed information processing. This review article aims to elucidate the fundamental principles underlying bipolar spintronic systems, particularly those utilizing semiconductor heterostructures with ferromagnetic contacts. We begin by discussing the physics of spin injection into semiconductors, employing mechanisms such as tunnel barriers and Schottky diodes. Following this, we explore the manipulation of the injected spins through external magnetic fields and electric currents, highlighting the versatility of these techniques. The article also showcases various spintronic devices, including spin light-emitting diodes (spin-LEDs), spin transistors, and spin-logic devices, with a primary focus on gallium arsenide (GaAs)-based structures. Additionally, we briefly touch upon alternative material systems to provide a broader context. This comprehensive overview is designed to serve as a valuable resource for newcomers seeking to grasp the essential concepts of spin transport phenomena at the interfaces between metals and semiconductors, as well as for researchers engaged in related fields. By consolidating current knowledge and advancements in bipolar spintronics, this article aims to foster further exploration and innovation in this dynamic and rapidly evolving domain.",
        "ori-fast-z-score": 0.39605901719066977,
        "water-fast-z-score": 5.883484054145521,
        "rewrite-fast-z-score": -0.6767155423319645
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Conservation Laws and Potential Symmetries of Linear Parabolic Equations .\nAbstract:\nWe study the conservation laws for linear parabolic equations with constant coefficients in one space dimension, which are given by Noether s theorem as integrals over time of certain densities that depend on solutions to the equation. We show how these densities can be computed using an algorithm based on symbolic integration techniques. The resulting expressions have been implemented into a computer program called CONSINTEP (Conservation Laws INTerpreter) written in Maple. This software is available at http://math.univ-lyon1.fr/~boudjema/consintep/index.html . Keywords: Conservation law, symmetry group, potential symmetry, Noether s theorem, linear partial differential equations, Maple. 1 Introduction In this article we present some results concerning conservation laws and potential symmetries of linear parabolic equations. These results were obtained during my PhD thesis  1  , where I developed algorithms for computing conserved quantities associated with such equations. Here we give a brief overview of our main results. \nThe concept of conservation law plays an important role in physics since it allows us to describe physical phenomena in terms of energy or entropy balance. For example, if u(x, t) denotes the temperature distribution inside a rod at position x ∈  0, 1  and time t ≥ 0 then the total amount of heat contained within the rod satisfies the following equation: \nwhere c > 0 is a positive constant describing the thermal conductivity of the material. If we assume that there exists no source term f = 0, i.e., all the heat entering the system leaves again after some time interval, then integrating Eq. (1) \nover the spatial domain yields the first integral of motion Q(t), also known as the energy density,\nwhich describes the total amount of heat stored up in the rod at any point in time. Note that the second equality follows directly from Green s formula applied to the left-hand side of Eq. (2).",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Conservation Laws and Potential Symmetries of Linear Parabolic Equations . Abstract : We research the conservation laws for linear parabolic equations with constant coefficients in one space dimension , which are given by Noether s theorem as integrals over time of certain densities that rely on solutions to the equation .We see how these densities can be computed using an algorithm based on symbolic integration methods . The resulting expressions have been built into a computer program named CONSINTEP ( Conservation Laws INTerpreter ) published in Maple .This tool is accessible at http : / / math . univ - lyon1 . fr / ~ boudjema / consintep / index . html . Keywords : Conservation law , symmetry group , potential symmetry , Noether s theorem , functional partial differential coefficients , Maple .1 Introduction In this article we present some results relating conservation laws and potential symmetries of linear parabolic equations . These conclusions were obtained during my PhD doctoral 1 , where I derived methods for modeling conserved quantities associated with such equations .Here we give a brief overview of our major results . The concept of conservation law serves an important role in science since it allows us to explain physical phenomena in terms of power or entropy balance .For instance , if u ( x , t ) denotes the temperature distribution inside a rod at position x ∈ 0 , 1 and time t ≥ 0 then the total quantity of temperature contained within the rod satisfies the following equation : where c > 0 is a positive constant describing the thermal conductivity of the metal . If we suppose that there exists no source term f = 0 , i . e . , all the temperature exiting the system leaves again after some time interval , then integrating Eq .( 1 ) over the spatial domain yields the first integral of movement Q ( t ) , sometimes called as the energy density , which gives the total quantity of warmth generated up in the rod at any point in time . Note that the second equality follows directly from Green s formula applicable to the left - hand side of Eq .(2).",
        "rewrite_text": "**Title:** Conservation Laws and Potential Symmetries of Linear Parabolic Equations\n\n**Abstract:** This study investigates the conservation laws associated with linear parabolic equations characterized by constant coefficients in a one-dimensional spatial framework. These conservation laws, derived from Noether's theorem, manifest as integrals over time of specific densities that depend on the solutions of the equations. We demonstrate a method for calculating these densities through an algorithm that employs symbolic integration techniques. The resulting formulas have been incorporated into a software tool named CONSINTEP (Conservation Laws INTerpreter), which is implemented in Maple and is available for public use at http://math.univ-lyon1.fr/~boudjema/consintep/index.html. The significance of conservation laws in scientific discourse cannot be overstated, as they provide a framework for understanding various physical phenomena through the lens of energy or entropy conservation. For example, consider a scenario where \\( u(x, t) \\) represents the temperature distribution along a rod, with \\( x \\) ranging from 0 to 1 and \\( t \\) being non-negative. The total thermal energy contained within the rod can be expressed through a governing equation, where \\( c > 0 \\) denotes the thermal conductivity of the material. In the absence of a source term (i.e., \\( f = 0 \\)), it is assumed that all heat exiting the system is reintroduced after a certain time. By integrating the governing equation over the spatial domain, we derive the first integral of motion, \\( Q(t) \\), often referred to as the energy density, which quantifies the total thermal energy present in the rod at any given moment. The relationship between these quantities is further elucidated through Green's theorem, which applies to the left-hand side of the derived equation. This article presents a concise overview of our findings, which were developed during my doctoral research, and highlights the methodologies employed to model conserved quantities linked to linear parabolic equations. \n\n**Keywords:** Conservation law, symmetry group, potential symmetry, Noether's theorem, functional partial differential equations, Maple.",
        "ori-fast-z-score": -1.12089707663561,
        "water-fast-z-score": 4.7087126183589705,
        "rewrite-fast-z-score": -0.3287979746107146
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Optical Variability of Infrared Power Law-Selected Galaxies & X-ray Sources in the GOODS-South Field .\nAbstract:\nWe present optical variability measurements for infrared power law-selected galaxies and X-ray sources in the Chandra Deep Field South (CDFS). We use data obtained with the Hubble Space Telescope s Advanced Camera for Surveys to measure photometric redshifts, rest-frame absolute magnitudes, stellar masses, star formation rates, and specific star-formation rates for these objects over an eight-year baseline. The sample consists of 16,000 galaxies at 0 < z < 5 selected by their mid-infrared colors using Spitzer/IRAC observations as well as 1,500 X-ray point sources detected in deep Chandra observations. We find that both galaxy samples show significant levels of intrinsic variation on timescales ranging from days to years. For example, we detect more than 50% of our IRAC-selected galaxies at 3.6 microns and 80% at 4.5 microns showing >0.1 mag variations between epochs separated by one year or less. These results are consistent with previous studies which have found similar levels of variability among optically-selected quasars. However, we also find evidence suggesting that this level of variability is not driven solely by AGN activity but may be associated with other physical processes such as mergers and/or interactions within the host galaxy itself.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Optical Variability of Infrared Power Law - Selected Galaxies & X - ray Sources in the GOODS - South Field . Abstract : We report optical variability observations for infrared energy law - selected galaxies and X - ray sources in the Chandra Deep Field South ( CDFS ) .We use data acquired with the Hubble Space Telescope s Advanced Camera for Surveys to measure photometric redshifts , rest - frame relative magnitudes , stellar masses , sun formation rates , and particular galaxy - formation rates for these objects over an eight - year baseline . The sample consists of 16 , 000 galaxies at 0 < z < 5 selected by their mid - infrared colors using Spitzer / IRAC measurements as also as 1 , 500 X - ray point sources detected in deep Chandra measurements .We see that both universe samples show considerable rates of intrinsic variation on timescales ranging from weeks to decades . For instance , we find more than 50 % of our IRAC - selected galaxies at 3 . 6 microns and 80 % at 4 . 5 microns showing > 0 . 1 mag variations between epochs separated by one month or greater .These conclusions are compatible with previous research which have discovered similar rates of variability among optically - selected quasars . However , we also find proof suggesting that this level of variability is not driven solely by AGN activity but might be involved with other physical processes such as mergers and / or relationships within the host star itself .",
        "rewrite_text": "We present our findings on the optical variability of infrared power law-selected galaxies and X-ray sources within the Chandra Deep Field South (CDFS). Utilizing data from the Hubble Space Telescope's Advanced Camera for Surveys, we conducted a comprehensive analysis over an eight-year period to derive key astrophysical parameters, including photometric redshifts, rest-frame relative magnitudes, stellar masses, star formation rates, and specific galaxy formation rates for a sample comprising 16,000 galaxies with redshifts ranging from 0 to 5. These galaxies were selected based on their mid-infrared colors, as determined by Spitzer/IRAC measurements, alongside 1,500 X-ray point sources identified in deep Chandra observations. Our results indicate that both samples exhibit significant intrinsic variability over timescales that span from weeks to decades. Notably, we observed that over 50% of the IRAC-selected galaxies at 3.6 microns and approximately 80% at 4.5 microns displayed variations exceeding 0.1 magnitudes between observations separated by one month or more. These findings align with previous studies that reported similar variability rates among optically-selected quasars. However, our investigation also uncovers evidence suggesting that this degree of variability may not be exclusively attributed to active galactic nucleus (AGN) activity. Instead, it may also be influenced by additional physical mechanisms, such as galaxy mergers or interactions within the host star itself. This research enhances our understanding of the dynamic processes occurring in distant galaxies and the factors contributing to their optical variability.",
        "ori-fast-z-score": -1.116880781646981,
        "water-fast-z-score": 5.858884758402822,
        "rewrite-fast-z-score": 0.6704783996548059
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  General Quantum Modeling of Combining Concepts: A Quantum Field Model in Fock Space .\nAbstract:\nWe present an approach to the modeling and simulation of combining concepts, which is based on quantum field theory (QFT) in Fock space. The basic idea behind our model is that each concept can be represented by its own QFT with specific properties. In order to combine two or more concepts we have to couple these fields together. We show how this coupling leads to new states representing combinations of concepts. Furthermore, we discuss some possible applications for our model such as combinatorial optimization problems and computational linguistics. Finally, we give examples illustrating the usefulness of our approach. Keywords: Quantum field theory, Fock space, Combining concepts, Computational linguistics, Optimization problem. 1 Introduction Quantum mechanics has been successfully applied to many different areas ranging from physics over chemistry to biology  1  . However, there are still open questions about the foundations of quantum mechanics  2  , especially concerning the interpretation of measurement results  3  .\nIn recent years, researchers started applying quantum mechanical models also to other disciplines like computer science  4  , cognitive psychology  5  , neuroscience  6  , economics  7  , etc.. For example, it was shown that quantum mechanical models could help solving certain NP-hard problems  8  . Moreover, quantum mechanical models were used to explain human decision making  9  and learning processes  10  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : General Quantum Modeling of Combining Concepts : A Quantum Field Model in Fock Space . Abstract : We present an view to the modeling and modeling of combining principles , which is based on quantum field theory ( QFT ) in Fock space .The basic idea behind our model is that each idea can be described by its own QFT with particular features . In order to mix two or more ideas we have to mix these fields together .We see how this coupling helps to novel states representing combinations of entities . Furthermore , we explain some possible use for our model such as combinatorial optimization problems and mathematical linguistics .Finally , we give examples illustrating the usefulness of our approach . Keywords : Quantum field theory , Fock space , Combining principles , Computational linguistics , Optimization problem .1 Introduction Quantum theory has been successfully application to many various fields ranging from science over chemistry to biology 1 . However , there are still open questions about the foundations of quantum mechanics 2 , particularly concerning the interpretation of measurement data 3 .In past times , researchers began application quantum mechanical models also to other disciplines like digital science 4 , cognitive psychology 5 , neuroscience 6 , economics 7 , etc . . For instance , it was shown that quantum mechanical models could assist solving specific NP - hard problems 8 . Moreover , quantum mechanical models were used to explain brain judgment makes 9 and educational processes 10 .",
        "rewrite_text": "Title: General Quantum Modeling of Combining Concepts: A Quantum Field Model in Fock Space\n\nAbstract: In this article, we introduce a novel perspective on the modeling of combining principles through the framework of quantum field theory (QFT) in Fock space. Our central premise is that each concept can be represented by its own distinct QFT, characterized by unique properties. To facilitate the integration of multiple concepts, we propose a method for combining these quantum fields. This coupling mechanism enables the emergence of new states that embody the synthesis of various entities. We further explore potential applications of our model, particularly in areas such as combinatorial optimization and mathematical linguistics. By illustrating specific examples, we demonstrate the practical utility of our approach in addressing complex problems. \n\nKeywords: Quantum field theory, Fock space, Combining principles, Computational linguistics, Optimization problems.\n\n1. Introduction: Quantum theory has found successful applications across a diverse range of fields, including physics, chemistry, and biology. However, foundational questions regarding quantum mechanics remain, particularly in relation to the interpretation of measurement outcomes. Recently, researchers have begun to extend quantum mechanical models to other domains, such as digital science, cognitive psychology, neuroscience, and economics. For example, studies have indicated that quantum models can aid in solving certain NP-hard problems. Additionally, these models have been employed to elucidate decision-making processes in the brain and to enhance educational methodologies. Our work aims to further bridge the gap between quantum mechanics and interdisciplinary applications, providing a framework for understanding the interplay of concepts through quantum modeling.",
        "ori-fast-z-score": -0.4622501635210242,
        "water-fast-z-score": 6.444022325288263,
        "rewrite-fast-z-score": 1.84894690328381
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Intramolecular long-range correlations in polymer melts: The segmental size distribution and its moments .\nAbstract:\nThe intramolecular structure of polymers is studied by means of the molecular dynamics simulation method for several model systems with different chain lengths, degrees of branching, and types of interactions between monomers. It has been found that the intramolecular correlation function can be represented as a sum of two terms corresponding to short- and long-range contributions. The latter term describes the contribution of segments separated by distances larger than the average distance between neighboring chains. This term depends on the number density of chains and their length. In addition, it was shown that this term decreases exponentially at large separations. The dependence of the first four moments of the segmental size distribution on temperature and pressure are calculated using the results obtained previously. These dependences were compared with experimental data available in literature. Good agreement between theory and experiment is observed. \n \n Keywords: Polymer melt, Intramolecular structure, Correlation functions, Segmental size distribution",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Intramolecular large - range correlations in polymer melts : The segmental size distribution and its moments . Abstract : The intramolecular structure of polymers is studied by means of the molecular dynamics simulation method for numerous model structures with various chain lengths , degrees of branching , and types of relationships between monomers .It has been seen that the intramolecular correlation function can be described as a sum of two terms corresponding to short - and long - range contributions . The last word refers the contribution of segments connected by distances bigger than the average distance between neighboring chains .This term varies on the number density of chains and their length . In addition , it was shown that this term varies exponentially at large separations .The dependence of the first four moments of the segmental size distribution on temperature and pressure are measured using the results derived earlier . These dependences were compared with experimental evidence available in literature .Good agreement between theoretical and observation is observed . Keywords : Polymer melt , Intramolecular structure , Correlation properties , Segmental size distribution",
        "rewrite_text": "**Title:** Intramolecular Long-Range Correlations in Polymer Melts: The Segmental Size Distribution and Its Moments\n\n**Abstract:** This study investigates the intramolecular structure of polymers through molecular dynamics simulations, focusing on a variety of model structures characterized by differing chain lengths, branching degrees, and monomer interactions. The research reveals that the intramolecular correlation function can be effectively represented as a combination of two distinct components: short-range and long-range contributions. The long-range component pertains to segments that are connected by distances exceeding the average distance between adjacent chains, indicating a significant relationship between this term and the number density and length of the polymer chains. Notably, the long-range contribution exhibits an exponential variation at larger separations. Furthermore, the study measures the temperature and pressure dependence of the first four moments of the segmental size distribution, utilizing previously derived results. These findings are then compared with existing experimental data from the literature, demonstrating a strong correlation between theoretical predictions and empirical observations. This agreement underscores the relevance of the proposed model in understanding the complex behavior of polymer melts. The insights gained from this research contribute to a deeper comprehension of the correlation properties within polymer systems and highlight the importance of segmental size distribution in characterizing their intramolecular structures. \n\n**Keywords:** Polymer melt, Intramolecular structure, Correlation properties, Segmental size distribution.",
        "ori-fast-z-score": 0.52999894000318,
        "water-fast-z-score": 4.926938732593016,
        "rewrite-fast-z-score": 1.3348476249438292
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Quantum Theory of Flicker Noise in Metal Films .\nAbstract:\nThe flicker noise is the low-frequency fluctuations observed in electrical resistance and other transport properties of metals at temperatures below 1 K.  The theory developed by Altshuler, Aronov, and Khmelnitsky (AAK) explains this phenomenon as arising due to electron-electron interactions within the metal film.  In their original work they assumed that electrons are scattered elastically off impurities or phonons.  However, recent experiments have shown that there can be significant inelastic scattering between electrons which leads to additional contributions to the resistivity.  Here we present an extension of AAK s theory for the case where both elastic and inelastic scattering processes contribute to the resistivity.  We show how our results compare with existing experimental data on thin gold films grown epitaxially on silicon substrates. The flicker noise is the low- frequency fluctuations observed in electrical resistance  and other transport properties of metallic systems at temperatures below 1K . It was first discovered in 1963 when measuring the resistance of thin silver wires  1  , but it has since been found in many different types of materials including semiconductors  2  , superconductors  3  , carbon nanotubes  4  , graphene  5  , and topological insulators  6  .\nIn order to explain these observations, Altshuler et al. (AAK) proposed a theoretical model based on the assumption that electrons scatter elastically off impurities  7, 8  .  This approach successfully describes most of the available experimental data  9  , however some discrepancies were recently reported  10  . These deviations may arise because the elastic approximation does not take into account possible inelastic scattering events  11  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Quantum Theory of Flicker Noise in Metal Films . Abstract : The flicker sound is the small - frequency fluctuations found in electrical resistance and other transport properties of metals at conditions below 1 K . The theory advanced by Altshuler , Aronov , and Khmelnitsky ( AAK ) presents this phenomenon as occurring due to ion - atom relationships within the metal film .In their early research they thought that electrons are scattered elastically off impurities or phonons . However , recent experiments have shown that there can be considerable inelastic scattering between electrons which results to extra contributions to the resistivity .Here we present an extension of AAK s concept for the case where both elastic and inelastic scattering mechanisms contribute to the resistivity . We see how our findings compare with existing experimental evidence on thin gold films grown epitaxially on silicon substrates .The flicker sound is the small - frequency fluctuations detected in electrical resistance and other transport properties of metallic components at conditions below 1K . It was first discovered in 1963 when monitoring the resistance of short silver cables 1 , but it has since been seen in multiple diverse kinds of materials namely semiconductors 2 , superconductors 3 , silicon nanotubes 4 , graphene 5 , and topological insulators 6 .In order to explain these observations , Altshuler et al . ( AAK ) proposed a theory approach using on the assumption that electrons scatter elastically off impurities 7 , 8 .This method effectively describes most of the provided experimental evidence 9 , however some discrepancies were recently noted 10 . These deviations might arise because the elastic equation does not take into consideration available inelastic scattering events 11 .",
        "rewrite_text": "**Title:** Quantum Theory of Flicker Noise in Metal Films\n\n**Abstract:** Flicker noise, characterized by low-frequency fluctuations in electrical resistance and other transport properties of metals at temperatures below 1 K, has garnered significant attention in the field of condensed matter physics. Initially identified in 1963 during resistance measurements of short silver cables, flicker noise has since been observed in a variety of materials, including semiconductors, superconductors, silicon nanotubes, graphene, and topological insulators. The foundational theory proposed by Altshuler, Aronov, and Khmelnitsky (AAK) attributes this phenomenon to the interactions between ions and atoms within metal films, primarily focusing on elastic scattering of electrons off impurities and phonons. While this model has successfully accounted for a substantial amount of experimental data, recent investigations have revealed notable discrepancies that suggest the need for a more comprehensive understanding of the underlying mechanisms. \n\nIn this study, we extend the AAK framework to incorporate both elastic and inelastic scattering processes, thereby providing a more nuanced explanation of resistivity in metal films. Our analysis reveals that inelastic scattering can significantly contribute to the overall resistivity, particularly in thin films, where the effects of electron interactions are pronounced. We compare our theoretical predictions with existing experimental results from thin gold films grown epitaxially on silicon substrates, highlighting the relevance of inelastic scattering in explaining observed deviations from the traditional AAK model. This work not only enhances our understanding of flicker noise in metallic systems but also opens avenues for further research into the quantum mechanical behavior of electrons in low-dimensional materials. By integrating both scattering mechanisms, we aim to reconcile the discrepancies noted in previous studies and provide a more robust theoretical framework for future explorations of flicker noise across various materials.",
        "ori-fast-z-score": -1.104689541477988,
        "water-fast-z-score": 7.48554161923727,
        "rewrite-fast-z-score": 0.7633700367119739
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Stretching Homopolymers .\nAbstract:\nThe stretching of homopolymer chains is studied by molecular dynamics simulations in the presence of an external force applied to one end of each chain.  The results are compared with those obtained for polyelectrolyte chains, which have been shown previously to exhibit similar behavior under tension.   It is found that both types of polymer chains show qualitatively similar behavior when stretched at low temperatures and high forces; however, there are significant differences between them at higher temperatures or lower forces.   ... This article was originally published on ArXiv.org s preprint server. Read more about this research at: http://arxiv.org/abs/cond-mat/0405033 . \nAuthors:  Yi-Chun Chen , Shih-Chieh Wu , Chia-Hui Lee , Hsinchu, Taiwan .\nAffiliation: Institute of Physics ,  Academia Sinica,  Taipei, Taiwan .\nEmail: yichun@phys.sinica.edu.tw",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Stretching Homopolymers . Abstract : The extension of homopolymer chains is studied by molecular dynamics simulations in the presence of an external stress introduced to one end of each chain .The results are compared with those generated for polyelectrolyte strands , which have been shown previously to exhibit similar behavior under pressure . It is found that both types of polymer bars exhibit qualitatively identical performance when stretched at low temperatures and low forces ; however , there are significant variations between them at higher temperatures or lower forces .. . . This section was originally published on ArXiv . org s preprint portal .Read more about this research at : www : / / arxiv . org / abs / cond - mat / 0405033 . Authors : Yi - Chun Chen , Shih - Chieh Wu , Chia - Hui Lee , Hsinchu , Taiwan .Affiliation: Institute of Physics ,  Academia Sinica,  Taipei, Taiwan .Email: yichun@phys.sinica.edu.tw",
        "rewrite_text": "The study of homopolymer chain extension under external stress is explored through molecular dynamics simulations in this article titled \"Stretching Homopolymers.\" The research focuses on the behavior of homopolymer chains when subjected to an external force applied to one end of the chain. The findings are juxtaposed with previous studies on polyelectrolyte strands, which have demonstrated analogous responses to applied pressure. The results indicate that both homopolymers and polyelectrolytes exhibit qualitatively similar stretching behavior at low temperatures and low force conditions. However, notable differences emerge when the temperature increases or when the applied forces decrease. This research contributes to the understanding of polymer dynamics and the effects of external stress on polymer behavior, providing insights that could be relevant for various applications in materials science and engineering. The original findings were published on the ArXiv preprint portal, and further details can be accessed at www.arxiv.org/abs/cond-mat/0405033. The authors of this study, Yi-Chun Chen, Shih-Chieh Wu, and Chia-Hui Lee, are affiliated with the Institute of Physics at Academia Sinica in Taipei, Taiwan. For correspondence, the lead author can be reached via email at yichun@phys.sinica.edu.tw.",
        "ori-fast-z-score": -0.2672612419124244,
        "water-fast-z-score": 3.841143835488627,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Gaining analytic control of parton showers .\nAbstract:\nWe present an algorithm for the numerical evaluation of Feynman diagrams with arbitrary numbers of external particles and internal loops, which is based on the concept of  partonic subdiagrams . The method allows to perform calculations in QCD beyond leading order accuracy without any approximations or assumptions about the kinematics of the process under consideration. We demonstrate its applicability by calculating the next-to-leading-order corrections to the production cross section of heavy quarks at hadron colliders. In this talk we will discuss how one can gain analytic control over parton showers using the concept of  partons  as fundamental degrees of freedom. This approach has been developed recently within the framework of Soft-Collinear Effective Theory (SCET)  1  . It provides a systematic way to resum large logarithms associated with collinear splittings into multiple jets  2  , thereby improving our understanding of jet physics  3  .\nThe basic idea behind SCET is that physical observables are described by matrix elements involving soft and/or collinear fields only  4  . These fields have nontrivial transformation properties under boosts along the beam axis  5  . They allow us to separate hard interactions from soft radiation  6  . As a result, it becomes possible to systematically factorize contributions to scattering amplitudes into  hard functions  describing short-distance dynamics  7, 8  and  semi-hard functions  encoding information about the emission of soft gluons  9  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Gaining analytic control of parton showers . Abstract : We present an algorithm for the numerical identification of Feynman diagrams with arbitrary numbers of external molecules and internal loops , which is based on the idea of partonic subdiagrams .The method enables to conduct measurements in QCD beyond trailing order efficiency without any approximations or assumptions about the kinematics of the process under consideration . We suggest its applicability by calculating the second - to - leading - order corrections to the production cross section of large quarks at hadron colliders .In this talk we will explore how one can acquire analytic control over parton showers using the idea of partons as essential degrees of liberty . This method has been used lately within the framework of Soft - Collinear Effective Theory ( SCET ) 1 .It provides a comprehensive way to resum big logarithms associated with collinear splittings into multiple jets 2 , thereby improving our knowing of jet physics 3 . The basic idea behind SCET is that physical observables are explained by matrix elements containing soft and / or collinear fields only 4 .These fields have nontrivial transformation qualities under boosts along the beam axis 5 . They allow us to separate hot interactions from soft light 6 .As a result , it becomes possible to thoroughly factorize contributions to scattering amplitudes into hard functions describing short - distance dynamics 7 , 8 and semi - hard functions encoding information about the emission of soft gluons 9 .",
        "rewrite_text": "We introduce a novel algorithm designed for the numerical identification of Feynman diagrams, accommodating any number of external particles and internal loops, leveraging the concept of partonic subdiagrams. This innovative approach facilitates measurements in Quantum Chromodynamics (QCD) that surpass traditional leading-order efficiency, all while avoiding any approximations or assumptions regarding the kinematics of the processes involved. To illustrate the effectiveness of this method, we compute the second-to-leading-order corrections to the production cross section of heavy quarks at hadron colliders. \n\nIn this presentation, we delve into the means of achieving analytic control over parton showers by treating partons as fundamental degrees of freedom. This technique has recently been applied within the framework of Soft-Collinear Effective Theory (SCET), which offers a systematic approach to resumming large logarithms linked to collinear splittings into multiple jets, thereby enhancing our understanding of jet physics. The core principle of SCET posits that physical observables can be expressed through matrix elements that incorporate only soft and/or collinear fields. These fields exhibit complex transformation properties under boosts along the beam axis, enabling a clear distinction between hard interactions and soft emissions. \n\nConsequently, this framework allows for a detailed factorization of contributions to scattering amplitudes into hard functions that encapsulate short-distance dynamics and semi-hard functions that convey information about the emission of soft gluons. This advancement not only enriches our theoretical toolkit but also paves the way for more precise predictions and analyses in high-energy particle physics, particularly in the context of hadron colliders.",
        "ori-fast-z-score": 0.29559878344928797,
        "water-fast-z-score": 6.667948594698258,
        "rewrite-fast-z-score": 1.5118578920369088
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A Language-Based Approach for Improving the Robustness of Network Application Protocol Implementations .\nAbstract:\nThe Internet is growing at an unprecedented rate, and with it comes increasing demands on network applications to provide reliable services in spite of failures that can occur anywhere along their execution paths. In this work we present a language-based approach for improving robustness by automatically detecting errors in protocol implementations using static analysis techniques. We show how our technique can be used to detect common types of implementation errors such as buffer overflow vulnerabilities or incorrect handling of exceptional conditions. Our results demonstrate that our method achieves high precision (>90%) while maintaining reasonable recall (~60%). Finally, we evaluate the performance overheads associated with our approach and find them to be negligible compared to existing approaches based on dynamic testing. The Internet continues to grow at an unprecedented rate, leading to increased demand for reliable services despite failures occurring anywhere along application execution paths. This work presents a languagebased approach for improving robustness through automatic detection of errors in protocol implementations via static analysis techniques. We describe how our technique can be applied to detect common types of error including buffer overflows and improper treatment of exceptional cases. Our experimental evaluation shows that our approach has very high precision (> 90%), while still achieving reasonable recall (~ 60%). Finally, we measure the performance overheads of our approach and find them negligible when compared against other state-of-the-art approaches relying on dynamic testing.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A Language - Based Approach for Improving the Robustness of Network Application Protocol Implementations . Abstract : The Internet is growing at an remarkable rate , and with it comes increasing demands on internet applications to provide quality services in spite of failures that can occur anywhere along their execution paths .In this project we present a language - based idea for improving robustness by automatically detecting errors in protocol implementations using static analysis methods . We see how our technique can be used to identify common kinds of implementation errors such as buffer overflow vulnerabilities or incomplete processing of exceptional conditions .Our results show that our technique achieves high clarity ( > 90 % ) while maintaining reasonable recall ( ~ 60 % ) . Finally , we assess the performance overheads associated with our approach and find them to be negligible compared to existing techniques based on dynamic monitoring .The Internet continues to expand at an unprecedented rate , leading to greater need for efficient connections despite failures occurring anything along application implementation paths . This research provides a languagebased approach for improving robustness through automatic diagnosis of errors in specification implementations via static analysis methods .We define how our technique can be applied to identify common kinds of mistake including buffer overflows and improper management of exceptional cases . Our research assessment demonstrates that our approach has very high clarity ( > 90 % ) , while nevertheless reaching reasonable recall ( ~ 60 % ) .Finally , we measure the performance overheads of our approach and find them negligible when compared against other state - of - the - art methods focusing on dynamic analysis .",
        "rewrite_text": "**Title:** A Language-Based Approach for Enhancing the Robustness of Network Application Protocol Implementations\n\n**Abstract:** The rapid expansion of the Internet has led to heightened expectations for internet applications to deliver reliable services, even in the face of potential failures that may arise throughout their execution paths. This study introduces a language-based methodology aimed at bolstering the robustness of network application protocols by employing static analysis techniques to automatically detect errors in protocol implementations. Our approach effectively identifies prevalent implementation flaws, including buffer overflow vulnerabilities and inadequate handling of exceptional conditions. The results of our evaluation indicate that our technique achieves an impressive clarity rate exceeding 90%, while maintaining a reasonable recall rate of approximately 60%. Furthermore, we investigate the performance overhead associated with our method and find it to be minimal when compared to existing dynamic monitoring techniques. As the Internet continues to grow at an extraordinary pace, the demand for resilient and efficient connections becomes increasingly critical, particularly in scenarios where failures may occur at any point along the application implementation path. This research presents a significant advancement in the field by offering a systematic approach to error detection in protocol specifications through static analysis. Our findings underscore the effectiveness of our technique in pinpointing common errors, such as buffer overflows and improper exception management, while also demonstrating its efficiency in terms of performance overhead. Overall, this work contributes to the ongoing efforts to enhance the reliability of network applications in an ever-evolving digital landscape.",
        "ori-fast-z-score": 1.403292830891247,
        "water-fast-z-score": 8.23754471047914,
        "rewrite-fast-z-score": 0.780398972571708
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Recent UBVJH Photometry of Epsilon Aurigae .\nAbstract:\nEpsilon Aurigae is an F-type main sequence star with a mass of 1.8 M☉ and radius 2 R☉, located at about 40 light-years away in the constellation Auriga.  It has been known for many years to be surrounded by dusty material that obscures its visible spectrum.   The infrared excess emission detected around this object suggests it may have a circumstellar disk similar to those found around young stars such as T Tauri or Herbig Ae/Be stars.   In addition, there are indications that the system contains a close companion which could also contribute to the observed infrared excess emission.    We present new photometric observations obtained using the United Kingdom Infrared Telescope (UKIRT) on Mauna Kea over the period 1997-2001 covering wavelengths between 0.9-2.5 microns.  These data show significant variations in both the near-infrared fluxes and colours of the central source consistent with changes in the amount of dust surrounding the star.  This behaviour is very similar to what is seen in other pre-main-sequence systems where accretion onto the central star causes periodic increases in luminosity accompanied by increased levels of reddening due to heating of the surrounding dust grains.   Our results suggest that the current level of activity in the system is relatively low compared to previous epochs but we cannot rule out the possibility that the recent increase in brightness was caused by a short-lived burst of enhanced accretion rather than steady-state accretion occurring throughout our observing campaign.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Recent UBVJH Photometry of Epsilon Aurigae . Abstract : Epsilon Aurigae is an F - class major sequence star with a mass of 1 . 8 M☉ and density 2 R☉ , located at about 40 light - years away in the constellation Auriga .It has been known for thousands decades to be accompanied by dusty matter that obscures its visible spectrum . The infrared excess emission detected around this body suggests it could have a circumstellar disk comparable to those observed around young galaxies such as T Tauri or Herbig Ae / Be stars .In addition , there are indications that the system contains a close companion which could also contribute to the seen infrared excess emission . We report new photometric images obtained using the United Kingdom Infrared Telescope ( UKIRT ) on Mauna Kea over the period 1997 - 2001 covering wavelengths between 0 . 9 - 2 . 5 microns .These data demonstrate considerable variations in both the near - infrared fluxes and colours of the main source consistent with shifts in the quantity of dust surrounding the star . This behaviour is very related to what is seen in other pre - main - sequence systems where accretion onto the main star causes periodic increases in luminosity followed by increased levels of reddening due to heating of the nearby dust grains .Our results show that the present degree of activity in the system is fairly lowest relative to previous epochs but we cannot judge out the suggestion that the recent rise in intensity was due by a brief - lived burst of enhanced accretion instead than steady - state accretion occurring throughout our observing campaign .",
        "rewrite_text": "Title: Recent UBVJH Photometry of Epsilon Aurigae\n\nAbstract: Epsilon Aurigae is an F-type main-sequence star, with a mass of approximately 1.8 M☉ and a radius of about 2 R☉, situated roughly 40 light-years away in the constellation Auriga. For centuries, this star has been recognized for its association with a significant amount of dusty material that obscures its visible light spectrum. Observations of infrared excess emission around Epsilon Aurigae suggest the presence of a circumstellar disk, akin to those found around young stellar objects such as T Tauri and Herbig Ae/Be stars. Furthermore, evidence points to the existence of a close companion star, which may also contribute to the observed infrared excess. In this study, we present new photometric data collected using the United Kingdom Infrared Telescope (UKIRT) on Mauna Kea from 1997 to 2001, spanning wavelengths from 0.9 to 2.5 microns. Our findings reveal significant variations in both the near-infrared flux and color of the primary star, indicating fluctuations in the amount of surrounding dust. This behavior closely resembles that observed in other pre-main-sequence systems, where accretion processes onto the primary star lead to periodic increases in luminosity, followed by enhanced reddening due to the heating of adjacent dust grains. Our analysis indicates that the current level of activity in the Epsilon Aurigae system is relatively low compared to previous observations. However, we cannot dismiss the possibility that the recent increase in brightness may have resulted from a transient episode of heightened accretion, rather than a continuous steady-state accretion process throughout our observational period. This study contributes to the understanding of Epsilon Aurigae's complex environment and its implications for stellar evolution and accretion dynamics.",
        "ori-fast-z-score": -1.1818181818181819,
        "water-fast-z-score": 5.432144762551111,
        "rewrite-fast-z-score": -1.5085060660073935
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Dark Side of a Patchwork Universe .\nAbstract:\nWe present the first measurement of the angular power spectrum of polarized dust emission at millimeter wavelengths, using data taken with the BICEP2 experiment in Antarctica during 2010 and 2011. We find that the polarization signal is consistent with predictions for thermal dust emission based on models constrained by Planck observations of temperature fluctuations. The amplitude of this signal is comparable to or larger than the expected gravitational lensing contribution over most multipole ranges probed here (l = 40-250). This result suggests that dust may be an important foreground contaminant for future CMB experiments targeting tensor modes. \n \n Keywords: Cosmic microwave background, Polarization, Dust emission, Gravitational waves, Inflationary cosmology \n \n Millimeter-wave polarimetry has been proposed as one method to detect primordial gravitational waves generated during inflation. However, it remains unclear whether polarized dust emission will limit our ability to extract such signals from current and upcoming CMB experiments. Here we report measurements made with the Bicep2/Keck Array collaboration s instrument operating at 150 GHz. These results are used to constrain the properties of interstellar dust grains through their effect on the polarized radiation they emit.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Dark Side of a Patchwork Universe . Abstract : We present the first measurement of the angular power spectrum of polarized dust radiation at millimeter wavelengths , using data taken with the BICEP2 study in Antarctica during 2010 and 2011 .We see that the polarization noise is compatible with predictions for thermal dust radiation based on models constrained by Planck measurements of temperature fluctuations . The amplitude of this signal is analogous to or larger than the expected gravitational lensing contribution over most multipole ranges probed here ( l = 40 - 250 ) .This result suggests that dust may be an important foreground contaminant for future CMB experiments targeting tensor modes . Keywords : Cosmic microwave background , Polarization , Dust absorption , Gravitational waves , Inflationary cosmology Millimeter - wave polarimetry has been proposed as one method to identify primordial gravitational waves generated during inflation .However , it remains unsure whether polarized dust radiation will limit our ability to extract such signals from recent and upcoming CMB experiments . Here we note measurements made with the Bicep2 / Keck Array collaboration s instrument operating at 150 GHz .These data are using to constrain the properties of interstellar dust grains through their effect on the polarized emission they emit .",
        "rewrite_text": "**Title: The Dark Side of a Patchwork Universe**\n\n**Abstract:** In this study, we present the inaugural measurement of the angular power spectrum of polarized dust radiation at millimeter wavelengths, utilizing data collected by the BICEP2 experiment in Antarctica during the years 2010 and 2011. Our findings indicate that the polarization noise aligns well with theoretical predictions for thermal dust radiation, which are informed by Planck measurements of temperature fluctuations. Notably, the amplitude of the detected signal is comparable to, or even exceeds, the anticipated contributions from gravitational lensing across the majority of the multipole range examined (l = 40 - 250). This observation implies that dust could serve as a significant foreground contaminant for forthcoming cosmic microwave background (CMB) experiments that aim to detect tensor modes associated with primordial gravitational waves.\n\nMillimeter-wave polarimetry has emerged as a promising technique for identifying primordial gravitational waves produced during the inflationary epoch. However, the extent to which polarized dust radiation may hinder our capacity to extract these signals from both current and future CMB experiments remains uncertain. In this context, we highlight the measurements obtained through the BICEP2/Keck Array collaboration's instrument operating at a frequency of 150 GHz. These data are instrumental in constraining the characteristics of interstellar dust grains, particularly through their influence on the polarized emission they generate. Our results underscore the necessity of accounting for dust polarization when interpreting CMB data, as it may significantly impact the search for evidence of gravitational waves and the understanding of the early universe. This work not only advances our knowledge of polarized dust emissions but also raises critical questions regarding the challenges faced in the quest to uncover the mysteries of inflationary cosmology.",
        "ori-fast-z-score": 1.2939932784412609,
        "water-fast-z-score": 5.682196434640312,
        "rewrite-fast-z-score": 1.1627553482998907
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Collision Between The Milky Way And Andromeda .\nAbstract:\nThe collision between the Milky Way and its nearest neighbor, M31 (Andromeda), is predicted to occur in about 4 billion years.  This will be one of the most spectacular events ever witnessed by humans.   In this talk I will describe how we can use observations made with telescopes on Earth as well as space-based observatories such as Hubble Space Telescope to study these collisions and learn more about dark matter, galaxies, stars, black holes, and other cosmic phenomena that are part of our universe. I will also discuss some of my research projects related to studying galaxy mergers using data obtained at the W.M. Keck Observatory located on Mauna Kea, Hawaii. Finally, I ll share what it was like for me to work there during my summer internship last year. Keywords: Dark Matter, Galaxy merger, Black Hole, Cosmic Evolution, Cosmology, Astrophysics",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Collision Between The Milky Way And Andromeda . Abstract : The merger between the Milky Way and its closest neighbor , M31 ( Andromeda ) , is predicted to effect in about 4 billion years .This will be one of the most magnificent phenomena ever experienced by humans . In this talk I will explain how we can using observations made with telescopes on Earth as well as space - based observatories such as Hubble Space Telescope to study these collisions and learn more about black material , galaxies , stars , white holes , and other cosmic phenomena that are part of our universe .I will also discuss some of my research projects related to researching galaxy mergers using data acquired at the W . M . Keck Observatory situated on Mauna Kea , Hawaii .Finally , I ll share what it was like for me to work there during my summer assignment last year . Keywords : Dark Matter , Galaxy merger , Black Hole , Cosmic Evolution , Cosmology , Astrophysics",
        "rewrite_text": "Title: The Collision Between The Milky Way And Andromeda\n\nAbstract: The anticipated merger between the Milky Way and its nearest galactic neighbor, Andromeda (M31), is projected to occur in approximately 4 billion years, marking a monumental event in the cosmic timeline. This collision promises to be one of the most extraordinary phenomena witnessed by humanity. In this presentation, I will elucidate how we can leverage observations from both terrestrial telescopes and space-based observatories, such as the Hubble Space Telescope, to investigate these galactic interactions. Through these studies, we aim to deepen our understanding of various cosmic entities, including dark matter, galaxies, stars, black holes, and other significant phenomena that shape our universe. Additionally, I will highlight several of my research initiatives focused on galaxy mergers, utilizing data collected at the W. M. Keck Observatory located on Mauna Kea, Hawaii. This experience has not only enriched my research but also provided me with invaluable insights into the operational dynamics of one of the world's leading astronomical facilities. I will conclude by sharing my personal experiences and reflections from my summer assignment at the observatory, illustrating the profound impact of hands-on research in the field of astrophysics. \n\nKeywords: Dark Matter, Galaxy Merger, Black Hole, Cosmic Evolution, Cosmology, Astrophysics.",
        "ori-fast-z-score": 0.39735970711951313,
        "water-fast-z-score": 4.464418717230567,
        "rewrite-fast-z-score": 1.3587324409735149
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Physics Case for the New Muon (g-2) Experiment .\nAbstract:\nThe Standard Model is in excellent agreement with all current experimental data, but it leaves many questions unanswered and fails to provide an explanation for some phenomena observed experimentally.  The muon magnetic moment anomaly provides one such example where there are significant discrepancies between theory predictions and experiment measurements that cannot be explained within the Standard Model framework.   In this talk I will present the physics case for the new g-2 experiment at Fermilab which aims to measure the anomalous magnetic moment of the muon more accurately than ever before by using a novel technique based on laser cooling and trapping techniques developed over recent years.   ... This talk presents the physics case for the proposed new measurement of the muon s anomalous magnetic moment at Fermilab. It describes how the use of laser cooling and trapping can lead to a dramatic improvement in precision compared to previous experiments. A number of other topics related to the project are also discussed including the status of the R&D program towards the goal of measuring the muon magnetic moment to 0.5 parts per million accuracy.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Physics Case for the New Muon ( g - 2 ) Experiment . Abstract : The Standard Model is in good agreement with all recent experimental evidence , but it leaves many issues unanswered and fails to provide an description for some phenomena observed experimentally .The muon magnetic moment anomaly presents one such example where there are significant discrepancies between theoretical estimates and experiment measurements that cannot be described within the Standard Model framework . In this talk I will present the physics case for the new g - 2 study at Fermilab which aims to measure the anomalous magnetic moment of the muon more accurately than ever before by using a new technique based on laser cooling and trapping techniques established over recent months .. . . This discussion presents the physics case for the new modern observation of the muon s anomalous magnetic point at Fermilab .It details how the using of laser cooling and trapping can lead to a dramatic improvement in precision compared to previous research . A several of other topics related to the project are also discussed including the status of the R & D study towards the objective of monitoring the muon magnetic moment to 0 . 5 parts per million accuracy .",
        "rewrite_text": "Title: The Physics Case for the New Muon (g - 2) Experiment\n\nAbstract: The Standard Model of particle physics has successfully aligned with a multitude of recent experimental findings; however, it remains incomplete, leaving several fundamental questions unanswered and failing to account for certain observed phenomena. A notable example of this limitation is the muon magnetic moment anomaly, where significant discrepancies exist between theoretical predictions and experimental measurements that cannot be reconciled within the confines of the Standard Model. This presentation outlines the compelling physics rationale behind the new g - 2 experiment at Fermilab, which seeks to achieve unprecedented precision in measuring the anomalous magnetic moment of the muon. By employing innovative laser cooling and trapping techniques developed in recent months, this experiment aims to enhance measurement accuracy beyond previous efforts. \n\nThe discussion will delve into the methodologies that underpin this modern investigation of the muon’s anomalous magnetic moment at Fermilab, highlighting how the integration of laser cooling and trapping technologies can yield substantial improvements in precision. Additionally, the talk will cover various related topics, including the current status of research and development efforts aimed at achieving a remarkable accuracy of 0.5 parts per million in monitoring the muon magnetic moment. This ambitious project not only promises to refine our understanding of the muon but also has the potential to shed light on new physics beyond the Standard Model, making it a pivotal endeavor in contemporary particle physics research.",
        "ori-fast-z-score": 1.237705495510552,
        "water-fast-z-score": 7.219948723811553,
        "rewrite-fast-z-score": 0.1873171623163388
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Scanning magnetoresistance microscopy of atom chips .\nAbstract:\nWe report scanning magnetoresistance microscopy (SMRM) measurements on an atom chip with gold wires and microtraps fabricated by focused ion beam milling. The SMRM images show the magnetic field distribution in the vicinity of the wire structures, which are used to transport cold atoms between different trapping sites. We find that the magnetic fields generated by these wires can be accurately described using Biot-Savart s law for straight current-carrying conductors. In addition we observe small deviations from this model at distances below 100 nm from the surface of the wires. These deviations may arise due to stray currents induced in the substrate or due to nontrivial geometries of the wires close to their surfaces. Our results demonstrate that SMRM is well suited to study complex magnetic field distributions near microscopic objects such as atom chips. Atom chips have been developed over recent years as miniaturized devices for manipulating neutral atomic matter waves  1, 2  . They consist of arrays of metallic wires and microtraps produced by focused-ion-beam (FIB) milling  3  , where ultracold atoms are transported along the wires before being trapped in the microtraps  4  .\nIn order to optimize the performance of atom chips it is important to understand how the magnetic fields created by the wires affect the motion of the atoms. This requires detailed knowledge about the spatial structure of the magnetic fields around the wires. However, direct measurement techniques like SQUID-based magnetometry  5  cannot resolve the magnetic field distribution inside the wires because they are too thin  6  . Therefore indirect methods based on imaging the trajectories of atoms released from traps  7, 8  or measuring the forces acting on them  9  were employed instead. Recently, scanning Hall probe microscopy was applied to measure the local magnetic field strength  10  . Here we present scanning magnetoresistance microscopy  11  data obtained on an atom chip consisting of two parallel gold wires connected via a junction  12  . By comparing our experimental results with theoretical predictions we obtain information about the magnetic field distribution in proximity of the wires.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Scanning magnetoresistance microscopy of atom devices . Abstract : We report scanning magnetoresistance microscopy ( SMRM ) observations on an atom chip with gold wires and microtraps fabricated by concentrated ion beam milling .The SMRM pictures show the magnetic field spread in the vicinity of the wire structures , which are used to transport cold molecules between various trap places . We see that the magnetic fields generated by these cables can be correctly explained using Biot - Savart s law for straight current - transporting conductors .In addition we perceive tiny deviations from this model at distances below 100 nm from the surface of the wires . These deviations might arise due to stray currents induced in the substrate or due to nontrivial geometries of the wires close to their edges .Our results show that SMRM is well suited to study difficult magnetic field distributions near microscopic structures such as atom devices . Atom devices have been built over recent months as miniaturized devices for manipulating neutral atomic matter waves 1 , 2 .They comprise of arrays of metallic wires and microtraps produced by concentrated - ion - laser ( FIB ) processing 3 , where ultracold atoms are transported along the wires before being trapped in the microtraps 4 . In order to optimize the performance of atom devices it is important to realize how the magnetic fields produced by the wires affect the movement of the atoms .This requires complete understanding about the spatial shape of the magnetic fields around the wires . However , direct detection methods like SQUID - based magnetometry 5 cannot determine the magnetic field spread inside the wires because they are too thin 6 .Therefore indirect approaches derived on observing the trajectories of atoms released from nets 7 , 8 or measuring the forces working on them 9 were utilized instead . Recently , scanning Hall probe microscopy was used to measure the local magnetic field intensity 10 .Here we present scanning magnetoresistance microscopy 11 data acquired on an atom chip comprised of two connected gold wires coupled via a junction 12 . By matching our experimental results with theoretical predictions we obtain knowledge about the magnetic field spread in proximity of the wires .",
        "rewrite_text": "We present a detailed study utilizing scanning magnetoresistance microscopy (SMRM) to investigate the magnetic field distribution around an atom chip composed of gold wires and microtraps, which were fabricated through concentrated ion beam milling techniques. The SMRM images reveal the magnetic field patterns surrounding these wire structures, which play a crucial role in the transport of cold molecules between various trapping locations. Our findings indicate that the magnetic fields generated by the wires can be accurately described by Biot-Savart's law, applicable to straight conductors carrying current. However, we also observe minor deviations from this theoretical model when measuring at distances less than 100 nm from the wire surfaces. These discrepancies may be attributed to stray currents induced in the substrate or the complex geometries of the wires near their edges.\n\nThe significance of our results lies in the demonstration that SMRM is an effective tool for probing intricate magnetic field distributions in microscopic structures, such as those found in atom devices. These atom devices, developed in recent months, are miniaturized systems designed for the manipulation of neutral atomic matter waves. They consist of arrays of metallic wires and microtraps created through focused ion beam (FIB) processing, allowing for the transport of ultracold atoms along the wires before they are captured in the microtraps. Understanding the impact of the magnetic fields produced by these wires on atomic movement is essential for optimizing the performance of atom devices, necessitating a comprehensive understanding of the spatial characteristics of the magnetic fields around the wires.\n\nTraditional direct detection methods, such as SQUID-based magnetometry, are inadequate for measuring the magnetic field distribution within the thin wires. Consequently, alternative indirect methods have been employed, including observing the trajectories of atoms released from nets and measuring the forces acting on them. Recently, scanning Hall probe microscopy has been utilized to assess local magnetic field intensity. In this study, we present SMRM data obtained from an atom chip featuring two interconnected gold wires linked by a junction. By correlating our experimental observations with theoretical models, we enhance our understanding of the magnetic field distribution in the vicinity of these wires.",
        "ori-fast-z-score": -0.8994380267950337,
        "water-fast-z-score": 7.00634632949934,
        "rewrite-fast-z-score": 0.517454889668201
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Pioneer Anomaly and Accelerating Universe as Effects of the Minkowski Space Conformal Symmetry .\nAbstract:\nWe show that the Pioneer anomaly, which is an anomalous acceleration observed in radio tracking data for two spacecrafts launched by NASA (Pioneer 10/11), can be explained if we assume that there exists a conformal symmetry breaking effect at large distances from our solar system. We also argue that this effect may lead to accelerating expansion of the universe. The idea behind these results is based on the fact that the space-time metric around massive bodies has been shown to have a certain degree of anisotropy due to quantum gravity effects. This anisotropic nature of space-time leads to violation of Lorentz invariance, which in turn causes violations of energy-momentum conservation laws. In order to explain such violations within the framework of general relativity one needs to introduce new fields into the theory. These are called  dark fields  because they do not interact with ordinary matter but only affect gravitational interactions between particles.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Pioneer Anomaly and Accelerating Universe as Effects of the Minkowski Space Conformal Symmetry . Abstract : We see that the Pioneer anomaly , which is an anomalous velocity observed in radio tracking information for two spacecrafts released by NASA ( Pioneer 10 / 11 ) , can be described if we suppose that there exists a conformal symmetry breaking phenomenon at large distances from our solar system .We also claim that this effect could lead to accelerating expansion of the universe . The idea behind these results is based on the fact that the space - time metric around large bodies has been shown to have a certain degree of anisotropy owing to quantum gravitational influences .This anisotropic existence of space - time leads to violation of Lorentz invariance , which in turn causes infringement of energy - momentum conservation laws . In order to explain such violations within the framework of general relativity one needs to introduce novel fields into the model .These are called black fields because they do not interact with everyday matter but only affect gravitational interactions between particles .",
        "rewrite_text": "Title: Pioneer Anomaly and Accelerating Universe as Effects of the Minkowski Space Conformal Symmetry\n\nAbstract: This article explores the Pioneer anomaly, an unexpected deviation in the velocity of NASA's Pioneer 10 and 11 spacecraft, suggesting that it can be attributed to a phenomenon of conformal symmetry breaking occurring at significant distances from our solar system. We propose that this anomaly is not merely a localized effect but is indicative of a broader cosmic phenomenon that may also account for the observed accelerating expansion of the universe. The foundation of our argument lies in the recognition that the space-time metric surrounding massive celestial bodies exhibits a degree of anisotropy, which is influenced by quantum gravitational effects. This anisotropic nature of space-time leads to a violation of Lorentz invariance, subsequently resulting in breaches of energy-momentum conservation principles. To reconcile these violations within the framework of general relativity, it becomes necessary to introduce new theoretical constructs, referred to as \"black fields.\" These fields are characterized by their non-interaction with conventional matter, yet they play a crucial role in mediating gravitational interactions among particles. Our findings suggest that understanding these black fields and their implications could provide deeper insights into both the Pioneer anomaly and the accelerating universe, potentially reshaping our comprehension of fundamental physics and cosmology. This work invites further investigation into the interplay between conformal symmetry, gravitational phenomena, and the underlying structure of space-time, with the aim of elucidating the mysteries surrounding cosmic expansion and the behavior of spacecraft in deep space.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.2808061395979236,
        "rewrite-fast-z-score": -0.18569533817705186
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The exceptionally extended flaring activity in the X-ray afterglow of GRB 050730 observed with Swift and XMM-Newton .\nAbstract:\nWe report on an exceptional flare detected by Swift/XRT at t ~ 1 day post-burst, which lasted for more than 100 ks (~20 hr). The flare was followed up with observations performed with XMM-Newton/EPIC-pn between 2.5 days to 3 months post-burst. We find that this flare is best described as a superposition of two components: one component lasting about 50 ks peaking around 10^-3 s and another component lasting about 70 ks peaking around 5 x 10^4 s. Both components are well fitted by exponentially cut-off power-laws with photon indices Γ = -1.6 ± 0.1 and -2.2 ± 0.3 respectively. No significant spectral evolution during either of these flares has been found. This flare is among the most energetic ever seen in any gamma-ray burst.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The exceptionally extended flaring activity in the X - ray afterglow of GRB 050730 seen with Swift and XMM - Newton . Abstract : We report on an exceptional burst detected by Swift / XRT at t ~ 1 day post - flare , which occurred for more than 100 ks ( ~ 20 hr ) .The flare was followed up with observations performed with XMM - Newton / EPIC - pn between 2 . 5 weeks to 3 weeks following - flare . We see that this flare is better represented as a superposition of two parts : one element lasting about 50 ks peaking roughly 10 ^ - 3 s and another component lasting about 70 ks peaking roughly 5 x 10 ^ 4 s . Both components are better fitted by exponentially drop - off power - laws with photon indices Γ = - 1 . 6 ± 0 . 1 and - 2 . 2 ± 0 . 3 respectively .No meaningful spectral evolution during either of these flares has been detected . This flare is among the most intense yet observed in any gamma - ray flare .",
        "rewrite_text": "We present a detailed analysis of an extraordinary burst identified by the Swift/XRT satellite approximately one day after the initial flare, which exhibited an extended duration of over 100 kiloseconds (around 20 hours). Subsequent observations were conducted using the XMM-Newton/EPIC-pn instrument between 2.5 to 3 weeks following the flare event. Our findings indicate that the flare can be effectively modeled as a combination of two distinct components: the first component lasts about 50 kiloseconds and peaks at approximately 10^-3 seconds, while the second component extends for roughly 70 kiloseconds, peaking around 5 x 10^4 seconds. Both components are best described by exponentially decaying power-law functions, with photon indices of Γ = -1.6 ± 0.1 for the first component and Γ = -2.2 ± 0.3 for the second. Notably, we did not observe any significant spectral evolution during either of the flare components. This event stands out as one of the most powerful gamma-ray flares recorded to date, highlighting the exceptional nature of this burst in the context of gamma-ray astrophysics. Our results contribute to the understanding of the mechanisms underlying such intense flaring activities and provide valuable insights into the behavior of X-ray afterglows associated with gamma-ray bursts.",
        "ori-fast-z-score": -0.8819171036881969,
        "water-fast-z-score": 4.75,
        "rewrite-fast-z-score": 1.1406468642034677
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Is diversity good? .\nAbstract:\nThe question is not new, but it has been recently revived by the publication of two papers in Nature and Science that claim to show that biodiversity loss leads to ecosystem collapse.  The authors argue that this finding should be taken seriously because ecosystems are essential for human well-being.   They also point out that there have been many previous studies showing that biodiversity loss can lead to declines in ecosystem functioning (e.g., productivity) without necessarily causing an abrupt change in state or collapse.    In this article we review these recent findings on biodiversity-ecosystem function relationships as well as some earlier results suggesting that biodiversity may sometimes enhance rather than reduce ecosystem stability.  We conclude with a discussion about how our understanding of biodiversity-ecosystem function interactions could be improved through further research. Biodiversity loss is one of humanity s greatest challenges today. It threatens the sustainability of natural resources used directly by humans such as food production systems and water supply, and indirectly via changes in climate regulation and disease transmission pathways. There is growing concern over the rate at which species extinction rates are increasing globally due to anthropogenic activities including habitat destruction, pollution, overexploitation, and invasive alien species1–3. This situation has led to calls for urgent action to conserve biological diversity4–6. However, despite widespread recognition of the importance of conserving biodiversity7–10, there remains considerable uncertainty regarding its role in maintaining ecosystem functions11–13. A number of theoretical models suggest that biodiversity loss will cause reductions in ecosystem functioning14–16. For example, Tilman et al. (1997)17 showed theoretically that reducing plant species richness would decrease primary productivity in grassland communities. Similarly, Naeem & Li (1998)18 found experimentally that removing species from soil microcosms reduced decomposition rates. These predictions were supported by numerous subsequent empirical studies19–22.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Is diversity great ? .Abstract : The question is not current , but it has been lately revived by the publication of two papers in Nature and Science that argue to find that ecosystem failure leads to biodiversity disaster . The authors argue that this finding should be taken seriously because ecosystems are essential for human well - being .They especially note out that there have been many earlier findings indicating that fauna loss can lead to declines in ecological functioning ( e . g . , output ) without necessarily creating an unexpected change in state or failure . In this page we review these recent results on biodiversity - ecological structure interactions as well as some earlier findings indicating that conservation may sometimes enhance rather than limit ecological stability .We end with a debate about how our appreciation of wildlife - ecological structure interactions might be improved through further studies . Biodiversity loss is one of humanity s worst problems currently .It damages the sustainability of natural assets used directly by humans such as feed production systems and water supply , and indirectly via alterations in climate control and illness transmission pathways . There is growing awareness over the pace at which species extinction frequencies are growing globally due to anthropogenic efforts including habitat damage , contamination , overexploitation , and invasive alien species1 – 3 .This problem has led to calls for urgent action to conserve biological diversity4 – 6 . However , despite widespread appreciation of the importance of conserving biodiversity7 – 10 , there exists considerable uncertainty regarding its function in maintaining ecosystem functions11 – 13 .A variety of theoretical theories indicate that biodiversity losing will cause reductions in ecological functioning14 – 16 . For instance , Tilman et al .( 1997 ) 17 showed theoretically that decreasing plant population richness would affect basic efficiency in prairie systems . Similarly , Naeem & Li ( 1998 ) 18 found experimentally that removing species from soil microcosms lowered oxidation rates .These predictions were endorsed by various subsequent empirical studies19 – 22 .",
        "rewrite_text": "**Title: Is Diversity Great?**\n\n**Abstract:** The inquiry into the significance of biodiversity has resurfaced, particularly following the recent publication of two influential papers in *Nature* and *Science* that contend that failures in ecosystems can precipitate biodiversity crises. The authors emphasize the critical nature of these findings, asserting that ecosystems play a vital role in human well-being. They highlight previous research demonstrating that the loss of fauna can lead to declines in ecological functions, such as productivity, without necessarily resulting in abrupt changes or outright failures in ecosystem states. This article reviews contemporary findings regarding the interactions between biodiversity and ecological structure, alongside earlier studies suggesting that conservation efforts may sometimes bolster rather than undermine ecological stability. \n\nThe ongoing loss of biodiversity represents one of the most pressing challenges facing humanity today. It threatens the sustainability of essential natural resources that humans rely on directly, such as food production systems and water supplies, while also impacting climate regulation and the transmission pathways of diseases. There is an increasing recognition of the alarming rate at which species extinctions are occurring globally, driven largely by human activities including habitat destruction, pollution, overexploitation, and the introduction of invasive species. This crisis has sparked urgent calls for action to preserve biological diversity. \n\nDespite a broad consensus on the importance of biodiversity conservation, significant uncertainties remain regarding its role in sustaining ecosystem functions. Various theoretical frameworks suggest that a decline in biodiversity will lead to diminished ecological performance. For example, Tilman et al. (1997) theoretically demonstrated that reduced plant species richness adversely affects efficiency in prairie ecosystems. Similarly, Naeem & Li (1998) provided experimental evidence that the removal of species from soil microcosms resulted in decreased oxidation rates. These theoretical predictions have been supported by numerous empirical studies, underscoring the critical need for further research to enhance our understanding of biodiversity's role in ecological stability and function.",
        "ori-fast-z-score": -2.3301026868087695,
        "water-fast-z-score": 8.994380267950337,
        "rewrite-fast-z-score": -0.30323921743156135
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Positronium Groundstate in Relativistic Schroedinger Theory .\nAbstract:\nWe present the results for positronium ground state energy and wave function obtained by solving relativistic Schrödinger equation with Coulomb potential using variational method. The calculations are performed within two different approximations, namely nonrelativistic limit (NR) and first order perturbation theory (PT1). In NR approximation we use Hylleraas type trial wave functions which include spin dependent terms up to second power of inter particle distance. We also calculate the expectation value of kinetic energy operator using this wave function. In PT1 approximation we use Hyllraas type trial wave functions including spin dependent terms up to third power of inter particle distance alongwith one electron Dirac orbitals as basis set. Our calculated values of binding energies agree well with those reported earlier. \n\n\nThe author is grateful to Prof. S. K. Gupta, Director General, BARC, Mumbai for his encouragement during the course of work on this project.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Positronium Groundstate in Relativistic Schroedinger Theory . Abstract : We present the results for positronium ground state energy and wave function achieved by treating relativistic Schrödinger equation with Coulomb potential using variational technique .The calculations are performed within two different approximations , namely nonrelativistic limit ( NR ) and first order perturbation theory ( PT1 ) . In NR algorithm we using Hylleraas type trial wave functions which contain spin dependent terms up to second power of inter quantum distance .We also obtain the expectation value of kinetic energy operator using this wave function . In PT1 approximation we using Hyllraas type trial wave functions featuring spin dependent terms up to third power of inter quantum time alongwith one particle Dirac orbitals as basis set .Our measured calculations of bound energies agree well with those published earlier . The author is thankful to Prof . S . K . Gupta , Director General , BARC , Mumbai for his encouragement during the course of research on this project .",
        "rewrite_text": "In this article, we explore the ground state energy and wave function of positronium by applying the relativistic Schrödinger equation in the context of a Coulomb potential, utilizing a variational approach. Our study employs two distinct approximations: the nonrelativistic limit (NR) and first-order perturbation theory (PT1). In the NR framework, we utilize Hylleraas-type trial wave functions that incorporate spin-dependent terms up to the second power of the inter-quantum distance. This method allows us to compute the expectation value of the kinetic energy operator effectively. For the PT1 approximation, we again employ Hylleraas-type trial wave functions, but this time we extend the spin-dependent terms to the third power of the inter-quantum distance, alongside incorporating one-particle Dirac orbitals as our basis set. Our calculated bound energies show strong agreement with previously published results, validating our approach. The author expresses gratitude to Prof. S. K. Gupta, Director General of BARC, Mumbai, for his invaluable support and encouragement throughout the research process. This work contributes to a deeper understanding of positronium within the framework of relativistic quantum mechanics and highlights the effectiveness of variational techniques in solving complex quantum systems.",
        "ori-fast-z-score": -1.193117518002609,
        "water-fast-z-score": 2.711630722733202,
        "rewrite-fast-z-score": -0.618852747755276
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Abstract factorials .\nAbstract:\nWe introduce the notion of ``abstract factorials  , which are functions that can be used to represent and manipulate sets of integers in an efficient manner.  We show how these functions can be implemented using standard data structures such as hash tables or trees.   Finally we present several applications for this new class of objects including fast algorithms for computing the number of integer partitions with bounded parts (the so-called ``Stirling numbers of the second kind  ) and for counting the number of ways to tile a rectangular board with tiles of different sizes. The concept of abstract factorials is introduced along with its implementation on top of existing data structures like hash tables or binary search trees. Several applications are presented, among them fast algorithms for computing Stirling numbers of the second type and tiling problems. This work was supported by NSF grant CCF-0634420. 1 Introduction In many computational settings it is necessary to perform operations over large collections of integers. For example, one may need to count the number of ways to partition a set into subsets of equal size, or to compute the number of tilings of a rectangular board with tiles having different shapes and sizes. These computations often require repeated evaluation of arithmetic expressions involving sums and products of integers. It has been shown recently that certain classes of such expressions admit very efficient representations based on combinatorial objects known as ``factorials    19, 20  . A factorial is essentially a function that maps each positive integer n to another object f(n), called the ``value   of the factorial at n. Such values must satisfy two properties:  First, they should form a sequence of nonnegative integers whose sum grows exponentially; i.e., there exists some constant c > 0 so that the value of any factorial satisfies |f(n)| <= cn^c for all sufficiently large n. Second, the values of distinct factorials cannot collide too frequently; more precisely, if f(n1) = f(n2) then n1 and n2 must differ by at least a fixed amount d.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Abstract factorials . Abstract : We introduce the notion of ` ` abstract factorials , which are functions that can be used to model and manipulate sets of numbers in an efficient manner .We see how these functions can be executed using conventional data forms such as hash tables or trees . Finally we present many applications for this new category of objects including rapid algorithms for calculation the number of integer partitions with bounded parts ( the so - called ` ` Stirling sums of the second kind ) and for counting the number of ways to mosaic a circular board with tiles of different sizes .The concept of abstract factorials is proposed along with its deployment on top of older data forms like hash tables or binary search trees . Several applications are presented , among them fast algorithms for processing Stirling sums of the second kind and tiling challenges .This work was supported by NSF grant CCF - 0634420 . 1 Introduction In many computational settings it is necessary to perform operations over large collections of integers .For instance , one may need to count the quantity of ways to split a setting into subsets of equal size , or to compute the number of tilings of a rectangular board with tiles having various shapes and dimensions . These computations commonly need multiple evaluation of algebraic expressions involving sums and products of integers .It has been shown lately that particular categories of such expressions admit very efficient representations based on combinatorial objects called as ` ` factorials 19 , 20 . A factorial is essentially a function that mapped each positive integer n to another object f ( n ) , called the ` ` value of the factorial at n . Such values must satisfy two characteristics : First , they should constitute a sequence of nonnegative numbers whose sum grows exponentially ; i . e . , there exists some constant c > 0 so that the value of any factorial satisfies | f ( n ) | < = cn ^ c for all sufficiently small n . Second , the values of distinct factorials cannot collide too often ; more accurately , if f ( n1 ) = f ( n2 ) then n1 and n2 must differ by at least a fixed amount d .",
        "rewrite_text": "In this article, we introduce the concept of \"abstract factorials,\" a novel class of functions designed to efficiently model and manipulate sets of numbers. These abstract factorials can be implemented using traditional data structures, such as hash tables and binary search trees, which enhances their practicality in computational applications. We explore various applications of abstract factorials, highlighting their utility in developing rapid algorithms for calculating the number of integer partitions with bounded parts, commonly referred to as \"Stirling sums of the second kind.\" Additionally, we demonstrate how these functions can be employed to solve problems related to tiling a circular board with tiles of varying sizes.\n\nThe notion of abstract factorials builds upon established data structures, allowing for efficient processing of complex combinatorial computations. In particular, we focus on the challenges associated with counting the ways to partition sets into equal subsets and determining the number of ways to tile a rectangular board with differently shaped tiles. These tasks often require repeated evaluations of algebraic expressions involving sums and products of integers, which can be computationally intensive.\n\nRecent advancements have shown that certain categories of these algebraic expressions can be represented efficiently through combinatorial constructs known as factorials. A factorial function maps each positive integer \\( n \\) to a corresponding value \\( f(n) \\), which must adhere to two key properties: first, the sequence of values must consist of nonnegative numbers whose sum grows exponentially, meaning there exists a constant \\( c > 0 \\) such that \\( |f(n)| \\leq cn^c \\) for sufficiently large \\( n \\). Second, the values of distinct factorials should not frequently coincide; specifically, if \\( f(n_1) = f(n_2) \\), then \\( n_1 \\) and \\( n_2 \\) must differ by at least a fixed amount \\( d \\).\n\nThis research was supported by NSF grant CCF-0634420 and contributes to the ongoing exploration of efficient computational methods in combinatorial mathematics.",
        "ori-fast-z-score": -1.819435304589368,
        "water-fast-z-score": 5.093915672507027,
        "rewrite-fast-z-score": -0.07881104062391006
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Resonating singlet valence plaquettes .\nAbstract:\nWe study the ground state properties and excitations in the spin-1/2 Heisenberg antiferromagnet on the square lattice with nearest-neighbor interactions only, using exact diagonalization (ED) for small clusters up to 12 sites as well as density-matrix renormalization group (DMRG) calculations for larger systems. We find that the ground-state energy per site is lower than the classical value by about 0.25J, where J denotes the exchange coupling constant between neighboring spins. The magnetic susceptibility shows Curie-Weiss behavior at high temperatures but decreases rapidly below T = 2J/3. This indicates strong quantum fluctuations which are also reflected in the low-temperature dependence of the specific heat. In addition we observe an unusual peak structure in the spin-spin correlation function S(q). For q along the principal axes of the Brillouin zone this peak has its maximum at q = π while it shifts towards smaller values when approaching the diagonals.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Resonating singlet valence plaquettes . Abstract : We research the ground state properties and excitations in the spin - 1 / 2 Heisenberg antiferromagnet on the square lattice with nearest - neighbor interactions only , using correct diagonalization ( ED ) for larger clusters up to 12 locations as well as density - vector renormalization group ( DMRG ) estimates for larger systems .We see that the ground - state energy per site is lower than the classical value by about 0 . 25J , where J refers the transfer coupling constant between neighboring spins . The magnetic susceptibility displays Curie - Weiss behavior at high temperatures but decreases quickly below T = 2J / 3 .This implies deep quantum fluctuations which are also reflected in the small - temperature dependence of the specific heat . In addition we encounter an peculiar peak structure in the spin - spinning correlation function S ( q ) .For q along the main axes of the Brillouin zone this peak has its limit at q = π while it shifts towards lesser values when approaching the diagonals .",
        "rewrite_text": "Title: Resonating Singlet Valence Plaquettes\n\nAbstract: This study investigates the ground state characteristics and excitations of the spin-1/2 Heisenberg antiferromagnet situated on a square lattice, focusing exclusively on nearest-neighbor interactions. We employ exact diagonalization (ED) techniques for larger clusters, accommodating up to 12 sites, alongside density matrix renormalization group (DMRG) methods for analyzing even larger systems. Our findings reveal that the ground state energy per site is approximately 0.25J lower than the classical prediction, where J denotes the coupling constant between adjacent spins. The magnetic susceptibility exhibits Curie-Weiss behavior at elevated temperatures, but it experiences a rapid decline as the temperature falls below T = 2J/3. This behavior indicates significant quantum fluctuations, which are further evidenced by the minimal temperature dependence observed in the specific heat. Additionally, we identify a distinctive peak structure in the spin-spin correlation function S(q). Notably, for momentum transfer q aligned with the principal axes of the Brillouin zone, this peak approaches a limit at q = π, while it progressively shifts towards lower values as one moves towards the diagonal directions. These results contribute to a deeper understanding of the quantum properties of the Heisenberg antiferromagnet and highlight the intricate interplay between quantum fluctuations and magnetic correlations in low-dimensional systems.",
        "ori-fast-z-score": 0.10976425998969035,
        "water-fast-z-score": 5.45544725589981,
        "rewrite-fast-z-score": 1.4501047335684953
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Entanglement and topological entropy of the toric code at finite temperature .\nAbstract:\nWe study entanglement properties of the Toric Code model in two dimensions, which is defined on a square lattice with periodic boundary conditions. We consider both ground state and thermal states for this system. In particular we calculate the von Neumann entropy S(A) = −TrρA ln ρA associated to different regions A of the lattice as well as the mutual information I(A; B) between any pair of disjoint regions A and B. The results are compared against numerical simulations performed by means of Monte Carlo techniques. For the ground state it turns out that there exists an area law for the von Neumann entropy, i.e., S(A) ∝ L−d−1 where d denotes the dimension of region A and L its linear size. Moreover, we find that the mutual information decays exponentially fast when one moves away from the diagonal line joining the centers of the regions A and B. These findings agree very well with those obtained using exact methods based on Matrix Product States (MPS). Finally, we also show how these results can be used to obtain bounds on the topological entropy of the Toric Code.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Entanglement and topological entropy of the toric code at finite temperature . Abstract : We research entanglement properties of the Toric Code model in two dimensions , which is characterized on a square lattice with periodic border conditions .We consider both ground state and thermal states for this system . In particular we determine the von Neumann entropy S ( A ) = −TrρA ln ρA associated to different regions A of the lattice as well as the mutual information I ( A ; B ) between any pair of disjoint regions A and B .The results are compared against numerical simulations conducted by means of Monte Carlo methods . For the ground state it turns out that there exists an area law for the von Neumann entropy , i . e . , S ( A ) [UNK] L−d−1 where d indicates the dimension of zone A and L its linear size .Moreover , we find that the mutual intelligence decays exponentially rapid when one moves away from the diagonal line joining the centers of the regions A and B . These conclusions follow very well with those acquired using accurate methods using on Matrix Product States ( MPS ) .Finally , we also demonstrate how these results can be used to obtain limits on the topological entropy of the Toric Code .",
        "rewrite_text": "In this study, we investigate the entanglement characteristics of the Toric Code model in two dimensions, which is defined on a square lattice with periodic boundary conditions. Our analysis encompasses both the ground state and thermal states of the system. We focus on quantifying the von Neumann entropy \\( S(A) = -\\text{Tr}(\\rho_A \\ln \\rho_A) \\) for various regions \\( A \\) of the lattice, as well as the mutual information \\( I(A; B) \\) between pairs of disjoint regions \\( A \\) and \\( B \\). The findings are juxtaposed with numerical simulations performed using Monte Carlo methods. \n\nFor the ground state, we observe that the von Neumann entropy adheres to an area law, expressed as \\( S(A) \\sim L^{d-1} \\), where \\( d \\) represents the dimension of region \\( A \\) and \\( L \\) denotes its linear size. Additionally, we find that the mutual information exhibits an exponential decay as the distance from the diagonal line connecting the centers of regions \\( A \\) and \\( B \\) increases. These results align closely with those obtained through precise calculations using Matrix Product States (MPS) techniques. \n\nFurthermore, we illustrate how our findings can be leveraged to establish bounds on the topological entropy of the Toric Code. This work contributes to a deeper understanding of the interplay between entanglement and topological properties in quantum many-body systems, particularly in the context of finite temperature effects. The implications of our results extend to various applications in quantum information theory and condensed matter physics, highlighting the significance of entanglement measures in characterizing topological phases.",
        "ori-fast-z-score": -0.10976425998969035,
        "water-fast-z-score": 4.447074385282452,
        "rewrite-fast-z-score": 0.4
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  What Can be Learned Studying the Distribution of the Biggest Fragment ? .\nAbstract:\nWe study the distribution of the largest fragment in a fragmentation process, which is an important quantity for understanding how to optimize data transmission over networks and storage systems.  We show that this distribution can be approximated by a power law with exponent 1 + 1/(2 - p), where p is the probability that two fragments merge into one when they are adjacent on disk (the so-called  mergeability parameter ). This result explains why previous studies have found different exponents depending on whether or not merging was allowed. The approximation also allows us to derive closed-form expressions for other quantities such as the mean size of the largest fragment and its variance. Finally, we use our results to explain some recent experimental findings about file sizes in peer-to-peer systems. In many applications involving data transmission over networks and distributed storage systems, it is useful to understand how large the largest fragment will become during the course of the system s evolution. For example, if a network node has to transmit a certain amount of information within a given time limit, then knowing what fraction of the total data needs to be transmitted at any point in time may help improve performance. Similarly, in distributed storage systems, knowing the expected size of the largest fragment helps determine how much space each node should reserve for storing replicas.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : What Can be Learned Studying the Distribution of the Biggest Fragment ? .Abstract : We consider the distribution of the greatest fragment in a fragmentation process , which is an important quantity for knowledge how to optimize data communication over networks and storage systems . We see that this distribution can be approximated by a power law with exponent 1 + 1 / ( 2 - p ) , where p is the probability that two fragments join into one when they are adjoining on disk ( the so - called mergeability coefficient ) .This result provides why previous research have discovered different exponents depending on whether or not merging was allowed . The approximation also enables us to derive closed - form expressions for other quantities such as the mean size of the greatest fragment and its variance .Finally , we utilize our findings to explain some latest empirical results about file sizes in peer - to - peer systems . In many applications using data communication over networks and distributed storage systems , it is important to realize how large the greatest fragment will become during the course of the program s evolution .For instance , if a network node has to transmit a certain quantity of information within a given time limitation , then understanding what fraction of the total data needs to be transmitted at any point in time might help increase efficiency . Similarly , in distributed storage systems , knowing the expected size of the greatest fragment allows determine how many space each node should reserve for storing replicas .",
        "rewrite_text": "Title: Insights from Analyzing the Distribution of the Largest Fragment\n\nAbstract: This study investigates the distribution of the largest fragment resulting from a fragmentation process, a critical factor in optimizing data communication across networks and storage systems. We demonstrate that this distribution can be effectively modeled by a power law characterized by the exponent 1 + 1 / (2 - p), where p represents the probability that two adjacent fragments merge when they are located on the same disk, known as the mergeability coefficient. This finding elucidates why previous studies have reported varying exponents based on the allowance of merging. Furthermore, our approximation facilitates the derivation of closed-form expressions for additional metrics, including the mean size and variance of the largest fragment. \n\nWe apply our results to interpret recent empirical observations regarding file sizes in peer-to-peer systems. Understanding the potential size of the largest fragment throughout the evolution of a program is crucial in numerous applications involving data transmission over networks and distributed storage solutions. For example, when a network node is tasked with transmitting a specific volume of data within a time constraint, comprehending the proportion of total data that must be sent at any given moment can significantly enhance operational efficiency. Likewise, in distributed storage environments, having insights into the expected size of the largest fragment aids in determining the appropriate storage capacity each node should allocate for maintaining data replicas. Overall, our findings contribute valuable knowledge to the field of data management, offering practical implications for improving system performance in various technological contexts.",
        "ori-fast-z-score": 1.975658322294524,
        "water-fast-z-score": 7.305140612272933,
        "rewrite-fast-z-score": 1.8209309360006518
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Nonlinear Dynamics of Infectious Diseases Transfer with Possible Applications for Tubercular Infection .\nAbstract:\nWe study the nonlinear dynamics of infectious diseases transfer in a population, where individuals are divided into three classes: susceptible (S), infected (I) and recovered/removed (R). We consider two different models: SIR model and SEIR model. In both cases we assume that there is no birth or death in the population. The main goal of this work is to investigate how the disease spreads through the population depending on its parameters. For example, if the infection rate is too high then it may lead to an epidemic outbreak. On the other hand, if the recovery rate is very large compared to the infection rate then the number of infectives will decrease rapidly. Finally, we show some numerical simulations which illustrate our results. \n \n Keywords: Nonlinear dynamics, infectious diseases, tuberculosis, SIR model, SEIR model. 1 Introduction \n \n Many mathematical models have been developed over time to describe the spread of infectious diseases within populations  1–3  . These models can be used as tools to understand the transmission mechanisms of these diseases and help public health authorities make decisions about prevention strategies  4  .\n \nIn particular, many researchers have studied the effects of vaccination programs  5–7  , quarantine  8, 9  and isolation  10, 11  on the evolution of epidemics. Other studies focus on the impact of environmental factors such as temperature  12, 13  , humidity  14, 15  and rainfall  16  on the propagation of pathogens. \nThe majority of existing works use deterministic models based on ordinary differential equations  17  . However, stochastic models  18, 19  and agent-based models  20, 21  also exist. Agent-based models allow us to take into account individual behaviors  22  while stochastic models provide more realistic descriptions of random events  23  . \n \nIn this article, we propose new mathematical models describing the spread of infectious diseases in a closed population. Our aim is to analyze the influence of various parameters on the behavior of the system. More specifically, we want to determine whether the disease will die out naturally or cause an epidemic outbreak. To do so, we first introduce the basic reproduction number R0  24  , which represents the average number",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Nonlinear Dynamics of Infectious Diseases Transfer with Possible Applications for Tubercular Infection . Abstract : We research the nonlinear dynamics of infectious infections transfer in a population , where persons are split into three categories : resistant ( S ) , infected ( I ) and returned / deleted ( R ) .We consider two different models : SIR model and SEIR model . In both cases we suppose that there is no death or dying in the population .The main goal of this project is to examine how the infection spreads through the population depending on its criteria . For instance , if the infection rate is too high then it could lead to an outbreak outbreak .On the other hand , if the return frequency is very huge compared to the infection rate then the quantity of infectives will decrease rapidly . Finally , we give some numerical simulations which illustrate our findings .Keywords : Nonlinear dynamics , infectious infections , tuberculosis , SIR model , SEIR model . 1 Introduction Many numerical models have been created over time to explain the spread of infectious infections within communities 1 – 3 .These systems can be used as tools to explain the spreading patterns of these diseases and help public medical institutions making decisions about prevention tactics 4 . In particular , many scientists have researched the effects of vaccination programs 5 – 7 , quarantine 8 , 9 and isolation 10 , 11 on the evolution of epidemics .Other studies emphasis on the impact of environmental factors such as temperature 12 , 13 , moisture 14 , 15 and rainfall 16 on the propagation of pathogens . The majority of older projects using deterministic methods using on ordinary differential coefficients 17 .However , stochastic theories 18 , 19 and agent - based models 20 , 21 also exist . Agent - based models let us to take into consideration individual behaviors 22 while stochastic theories provide more realistic descriptions of random events 23 .In this page , we propose new numerical models explaining the spread of infectious infections in a closed population . Our aim is to analyze the impact of several variables on the activity of the system .More specifically , we try to measure whether the infection will die out naturally or result an outbreak outbreak . To do so , we first introduce the fundamental reproduction number R0 24 , which equals the average number",
        "rewrite_text": "**Title:** Nonlinear Dynamics of Infectious Disease Transmission with Potential Applications for Tuberculosis Infection\n\n**Abstract:** This study investigates the nonlinear dynamics of infectious disease transmission within a population categorized into three distinct groups: susceptible (S), infected (I), and recovered (R). We explore two primary models: the SIR model and the SEIR model, both of which operate under the assumption that there are no deaths within the population. The primary objective of this research is to analyze the mechanisms of infection spread based on various parameters. For instance, a high infection rate may precipitate an outbreak, while a significantly elevated recovery rate could lead to a rapid decline in the number of infectious individuals. To substantiate our theoretical findings, we present numerical simulations that illustrate the dynamics of infection spread under different scenarios.\n\n**Keywords:** Nonlinear dynamics, infectious diseases, tuberculosis, SIR model, SEIR model.\n\n**1 Introduction:** Over the years, numerous numerical models have been developed to elucidate the transmission of infectious diseases within communities. These models serve as valuable tools for understanding disease propagation patterns and assisting public health authorities in formulating effective prevention strategies. A significant body of research has focused on the effects of vaccination programs, quarantine measures, and isolation protocols on epidemic progression. Additionally, studies have highlighted the influence of environmental factors, such as temperature, humidity, and rainfall, on pathogen transmission. Historically, many of these models have employed deterministic approaches based on ordinary differential equations. However, there is also a growing interest in stochastic models and agent-based simulations, which account for individual behaviors and provide a more nuanced understanding of random events. In this paper, we propose novel numerical models that capture the dynamics of infectious disease spread in a closed population. Our analysis aims to determine the influence of various factors on the system's behavior, specifically whether the infection will naturally extinguish or escalate into an outbreak. To facilitate this analysis, we introduce the basic reproduction number, R0, which represents the average number of secondary infections generated by an infected individual in a fully susceptible population.",
        "ori-fast-z-score": -0.6064784348631227,
        "water-fast-z-score": 9.846562966837059,
        "rewrite-fast-z-score": 0.7580980435789034
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Comparing Star Formation on Large Scales in the c2d Legacy Clouds: Bolocam 1.1 mm Dust Continuum Surveys of Serpens, Perseus, and Ophiuchus .\nAbstract:\nWe present new bolometer observations at 1.1mm wavelength for three clouds observed as part of the Spitzer Infrared Nearby Galaxies Survey (SINGS) legacy program. The data were obtained with the Bolocam instrument on the Caltech Submillimeter Observatory to study star formation across large scales within these clouds. We find that the dust continuum emission is well correlated with infrared extinction maps derived from 2MASS near-infrared photometry. Using this correlation we derive an average dust temperature of 14K over each cloud. This value agrees very well with previous estimates based on single-dish submillimeter measurements. We also use our data to estimate the total mass contained in dense cores identified by the Herschel Space Observatory s Photodetector Array Camera & Spectrometer (PACS). Our results show good agreement between the masses estimated using PACS 70 micron fluxes and those determined directly from the Bolocam data.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Comparing Star Formation on Large Scales in the c2d Legacy Clouds : Bolocam 1 . 1 mm Dust Continuum Surveys of Serpens , Perseus , and Ophiuchus . Abstract : We report new bolometer observations at 1 . 1mm frequency for three clouds observed as part of the Spitzer Infrared Nearby Galaxies Survey ( SINGS ) legacy mission .The data were obtained with the Bolocam instrument on the Caltech Submillimeter Observatory to study star formation across large scales within these clouds . We see that the dust continuum emission is well associated with infrared extinction maps obtained from 2MASS near - infrared photometry .Using this relationship we derive an mean powder temperature of 14K over each dust . This value agrees very best with previous accounts based on single - dish submillimeter surveys .We additionally using our information to estimate the total mass hidden in dense cores identified by the Herschel Space Observatory s Photodetector Array Camera & Spectrometer ( PACS ) . Our results show good agreement between the masses estimated using PACS 70 micron fluxes and those estimated directly from the Bolocam data .",
        "rewrite_text": "We present new bolometer observations at a frequency of 1.1 mm for three molecular clouds—Serpens, Perseus, and Ophiuchus—conducted as part of the Spitzer Infrared Nearby Galaxies Survey (SINGS) legacy mission. Utilizing the Bolocam instrument at the Caltech Submillimeter Observatory, our study aims to investigate star formation processes on large scales within these clouds. Our findings reveal a strong correlation between dust continuum emission and infrared extinction maps derived from 2MASS near-infrared photometry. This relationship allows us to calculate an average dust temperature of approximately 14 K across the observed regions, a value that aligns closely with previous measurements obtained from single-dish submillimeter surveys. Furthermore, we leverage our data to estimate the total mass concealed within dense cores identified by the Herschel Space Observatory's Photodetector Array Camera and Spectrometer (PACS). Our analysis indicates a high degree of consistency between the mass estimates derived from PACS 70 micron fluxes and those obtained directly from our Bolocam observations. These results contribute to a deeper understanding of star formation dynamics in these clouds and highlight the effectiveness of combining submillimeter and infrared data to probe the physical conditions and mass distribution within star-forming regions.",
        "ori-fast-z-score": 0.9561828874675149,
        "water-fast-z-score": 4.949747468305833,
        "rewrite-fast-z-score": 2.1081851067789197
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Particle Interpretations of the PVLAS Data .\nAbstract:\nThe PVLAS collaboration has recently reported results on light-by-light scattering in vacuum, which are inconsistent with Standard Model predictions.  In this note we discuss possible interpretations of these data within the framework of quantum field theory and string theory. We argue that the most natural interpretation is to assume that the observed effect arises due to new particles coupling to photons via an effective dimension-8 operator. The required mass scale for such particles can be as low as 10 GeV or even lower if one assumes that they couple only weakly to ordinary matter. If confirmed by further experiments, these observations would have profound implications both for particle physics phenomenology and cosmological models. The PVLAS collaboration has recently announced their measurement of light-by-light scattering in vacuo  1  . This process violates parity conservation at tree level and thus cannot occur in the Standard Model (SM)  2  , but it could arise through loop effects  3  .\nIn particular, the authors report observing a signal consistent with the SM prediction  4  \nwhere G F = 1.1663787(6) × 10−5GeV−2 is Fermi s constant  5  , θ W ≈ 0.23 is the weak mixing angle  6  , m e is the electron mass, and M Pl ≡ 1/ √ 8πG N ≈ 2×10 18 GeV is the reduced Planck mass  7, 8  . However, the measured value of the cross section exceeds the theoretical expectation by more than three standard deviations,\nThis discrepancy between experiment and theory may indicate the presence of new physics beyond the SM  9  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Particle Interpretations of the PVLAS Data . Abstract : The PVLAS collaboration has recently published results on light - by - light diffusion in vacuum , which are inconsistent with Standard Model estimates .In this note we explain possible interpretations of these information within the framework of quantum field theory and string theory . We argue that the most natural interpretation is to assume that the observed effect arises due to new objects coupling to photons via an efficient dimension - 8 operator .The expected mass scale for such particles can be as low as 10 GeV or especially lower if one assumes that they couple only weakly to normal matter . If confirmed by further studies , these observations would have profound implications both for particle science phenomenology and cosmological models .The PVLAS collaboration has recently announced their observation of light - by - light scattering in vacuo 1 . This process violates parity conservation at tree level and therefore cannot appear in the Standard Model ( SM ) 2 , but it could occur through loop effects 3 .In particular , the articles publish observing a signal compatible with the SM prediction 4 where G F = 1 . 1663787 ( 6 ) × 10−5GeV−2 is Fermi s constant 5 , θ W ≈ 0 . 23 is the strong mixing angle 6 , m e is the electron mass , and M Pl ≡ 1 / √ 8πG N ≈ 2×10 18 GeV is the reduced Planck mass 7 , 8 . However , the measured value of the cross section exceeds the theoretical expectation by more than three standard deviations , This discrepancy between experiment and theory could indicate the presence of new science beyond the SM 9 .",
        "rewrite_text": "**Title: Particle Interpretations of the PVLAS Data**\n\n**Abstract:** The recent findings from the PVLAS collaboration regarding light-by-light scattering in a vacuum present intriguing results that deviate from the predictions made by the Standard Model of particle physics. This paper explores various interpretations of these findings within the contexts of quantum field theory and string theory. We propose that the most plausible explanation for the observed phenomena is the existence of new particles that interact with photons through an effective dimension-8 operator. The mass scale of these hypothetical particles could be as low as 10 GeV, or even lower if they exhibit only weak interactions with ordinary matter. Should these observations be validated through further experimental investigations, they could significantly impact our understanding of particle physics and cosmological theories. \n\nThe PVLAS collaboration's recent announcement highlights their detection of light-by-light scattering in vacuum, a process that inherently violates parity conservation at the tree level and is not accounted for in the Standard Model. While such scattering could theoretically occur through loop corrections, the collaboration reported a signal that aligns with the Standard Model's predictions, where Fermi's constant (G_F) is approximately 1.1663787(6) × 10−5 GeV−2, the weak mixing angle (θ_W) is around 0.23, and the electron mass (m_e) and reduced Planck mass (M_Pl) are also relevant parameters. However, the measured cross-section for this scattering exceeds the theoretical expectations by more than three standard deviations, suggesting a potential gap between experimental results and theoretical models. This discrepancy may indicate the presence of new physics beyond the Standard Model, warranting further exploration and analysis in the field.",
        "ori-fast-z-score": -1.104689541477988,
        "water-fast-z-score": 3.0251050401930977,
        "rewrite-fast-z-score": -0.45454545454545453
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  3D Radiative Hydrodynamics for Disk Stability Simulations: A Proposed Testing Standard and New Results .\nAbstract:\nWe present the results of 3D radiation hydrodynamic simulations of accretion disks around black holes, performed with our new code RHD3DPHOTON. We show that this code is able to reproduce previous results obtained by other authors using different codes (e.g., JETSPEC), as well as some new results which have not been previously reported in the literature. In particular we find that:  1) The disk becomes unstable when its luminosity exceeds a critical value Lcrit = 0.1LEdd.  2) For super-Eddington luminosities there are two types of instability modes: one associated with thermal convection and another related to photon bubbles.  3) There exists an upper limit on the mass flux through the disk, above which no steady state solution can be found. This result has important implications for models of AGN feedback. 4) When the luminosity approaches or exceeds LEdd, the disk develops strong outflows along the equatorial plane.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : 3D Radiative Hydrodynamics for Disk Stability Simulations : A Proposed Testing Standard and New Results . Abstract : We present the results of 3D radiation hydrodynamic simulations of accretion disks around black holes , conducted with our new code RHD3DPHOTON .We see that this code is could to reproduce previous findings obtained by other researchers using similar codes ( e . g . , JETSPEC ) , as well as some additional findings which have not been previously reported in the literature . In particular we find that : 1 ) The disk gets unstable when its luminosity exceeds a critical number Lcrit = 0 . 1LEdd .2 ) For super - Eddington luminosities there are two forms of instability modes : one related with thermal convection and another linked to photon bubbles . 3 ) There exists an upper maximum on the mass flux through the disk , above which no continuous state solution can be found .This result has important implications for models of AGN feedback . 4 ) When the luminosity approaches or exceeds LEdd , the disk develops strong outflows along the equatorial plane .",
        "rewrite_text": "We present findings from our three-dimensional radiation hydrodynamic simulations of accretion disks surrounding black holes, utilizing our newly developed code, RHD3DPHOTON. Our simulations successfully replicate earlier results obtained by other researchers employing similar methodologies, such as JETSPEC, while also uncovering novel insights that have not been documented in existing literature. Notably, we identify that the stability of the disk is compromised when its luminosity surpasses a critical threshold, denoted as L_crit = 0.1 L_Edd. Furthermore, in scenarios where luminosities exceed the Eddington limit, we observe the emergence of two distinct instability modes: one associated with thermal convection and the other related to the formation of photon bubbles. Additionally, our research reveals a maximum limit on the mass flux through the disk; beyond this threshold, no continuous state solutions can be achieved. This finding carries significant implications for our understanding of active galactic nucleus (AGN) feedback mechanisms. Lastly, we note that as the luminosity approaches or exceeds L_Edd, the disk exhibits pronounced outflows along the equatorial plane, highlighting the dynamic behavior of accretion disks under extreme conditions. These results contribute to the ongoing discourse on disk stability and the complex interactions within accretion systems, paving the way for future investigations in the field.",
        "ori-fast-z-score": -0.3511234415883917,
        "water-fast-z-score": 4.09644015186457,
        "rewrite-fast-z-score": 1.970208219987808
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  X-ray detection of the substellar twin 2MASS J11011926-7732383 AB .\nAbstract:\nWe report on the X-ray properties of the young, nearby (d = 11 pc), low-mass binary system 2MASS J1101-2677AB discovered by Burgasser et al. (2007) . The primary component is an M8 dwarf with T eff ~ 2600 K and log g ~ 5.0 while its companion has been classified as an L5 brown dwarf with T eff ~ 1400K and log g ~ 4.2. We observed this target for about 50 ks using Chandra ACIS-S in order to search for evidence of coronal activity associated with either or both components. No significant emission was detected at the position of the source down to a 3-sigma upper limit of 1 x 10^28 erg s-1 cm-2 . This non-detection suggests that if there are active regions present they must be small and/or cool compared to those found on more massive stars. In addition we find no evidence of flaring behavior during our observation which constrains any possible magnetic field strength to less than 100 G.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : X - ray observations of the substellar twin 2MASS J11011926 - 7732383 AB . Abstract : We report on the X - ray characteristics of the young , neighboring ( d = 11 pc ) , low - weight binary system 2MASS J1101 - 2677AB discovered by Burgasser et al .( 2007 ) . The main component is an M8 dwarf with T eff ~ 2600 K and log f ~ 5 . 0 while its companion has been classified as an L5 brown dwarf with T eff ~ 1400K and log f ~ 4 . 2 .We observed this target for about 50 ks using Chandra ACIS - S in order to search for indication of coronal interaction associated with either or both components . No noticeable emitted was seen at the position of the source down to a 3 - sigma upper maximum of 1 x 10 ^ 28 erg s - 1 cm - 2 .This non - observation suggests that if there are active regions present they must be small and / or cold relative to those observed on more massive stars . In addition we find no evidence of flaring behavior during our observation which constrains any likely magnetic force weakness to fewer than 100 G .",
        "rewrite_text": "We present an analysis of the X-ray properties of the young, nearby binary system 2MASS J11011926-7732383 AB, located at a distance of 11 parsecs. This low-mass binary, identified by Burgasser et al. (2007), consists of two components: the primary is an M8 dwarf with an effective temperature (T_eff) of approximately 2600 K and a log f value of around 5.0, while the secondary is classified as an L5 brown dwarf with a T_eff of about 1400 K and a log f of approximately 4.2. To investigate potential coronal activity associated with either or both components, we conducted a 50 ks observation using the Chandra ACIS-S instrument. Our findings revealed no significant X-ray emission at the source's location, with a 3-sigma upper limit of 1 x 10^28 erg s^-1 cm^-2. This lack of detectable X-ray activity implies that if active regions exist on either component, they are likely to be small and/or cooler than those typically observed on more massive stars. Furthermore, our observations did not indicate any flaring events, which allows us to constrain the possible magnetic field strength in the system to less than 100 G. These results contribute to our understanding of the magnetic activity and coronal properties of low-mass stars and brown dwarfs, highlighting the differences in behavior compared to their more massive counterparts.",
        "ori-fast-z-score": -1.5215349135496974,
        "water-fast-z-score": 4.184914994777494,
        "rewrite-fast-z-score": -1.0425720702853738
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Geochemistry of U and Th and its Influence on the Origin and Evolution of the Crust of Earth and the Biological Evolution .\nAbstract:\nThe geochemical behavior of uranium (U) and thorium (Th), which are radioactive elements, is discussed in relation to their influence on the origin and evolution of the crust of earth as well as biological evolution.  The chemical properties of these two elements are similar; however, they have different physical characteristics that affect how they behave geologically.  Uranium has an atomic number of 92 with a half-life of 4.5 billion years while thorium has an atomic number of 90 with a half life of 1.4 billion years.   Both elements occur naturally throughout the Earth s crust but at varying concentrations depending upon the rock type.  They can be found in igneous rocks such as granite or basalt where they form minerals like uranite or thorite respectively.  These minerals may also contain other trace metals including lead, silver, gold, copper, zinc, arsenic, selenium, molybdenum, cadmium, mercury, bismuth, antimony, tellurium, cobalt, nickel, manganese, iron, vanadium, chromium, tungsten, titanium, zirconium, niobium, tantalum, hafnium, rhenium, osmium, iridium, platinum, palladium, rhodium, ruthenium, and iridium.  Uranium and thorium are also present in sedimentary rocks such as sandstone, shale, limestone, dolomite, gypsum, anhydrite, salt beds, and evaporites.  Sedimentary rocks are formed by weathering processes when water erodes...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Geochemistry of U and Th and its Influence on the Origin and Evolution of the Crust of Earth and the Biological Evolution . Abstract : The geochemical behavior of uranium ( U ) and thorium ( Th ) , which are radioactive elements , is mentioned in relation to their influence on the origin and evolution of the crust of planet as well as natural evolution .The chemical properties of these two compounds are comparable ; however , they have different physical traits that affect how they react geologically . Uranium has an atomic number of 92 with a half - life of 4 . 5 billion decades while thorium has an atomic number of 90 with a half lifetime of 1 . 4 billion decades .Both components exist naturally throughout the Earth s crust but at different amounts depending upon the stone type . They can be found in igneous rocks such as granite or basalt where they create minerals like uranite or thorite respectively .These deposits may sometimes hold other trace metals including lead , silver , gold , copper , zinc , arsenic , selenium , molybdenum , cadmium , mercury , bismuth , antimony , tellurium , cobalt , nickel , manganese , iron , vanadium , chromium , tungsten , titanium , zirconium , niobium , tantalum , hafnium , rhenium , osmium , iridium , platinum , palladium , rhodium , ruthenium , and iridium . Uranium and thorium are also occur in sedimentary formations such as sandstone , shale , marble , dolomite , gypsum , anhydrite , salt beds , and evaporites .Sedimentary stones are created by weathering reactions when water erodes . . .",
        "rewrite_text": "**Title:** Geochemistry of Uranium and Thorium and Their Impact on Earth's Crust Formation and Biological Evolution\n\n**Abstract:** This article explores the geochemical properties of uranium (U) and thorium (Th), two radioactive elements, and their significant roles in the formation and evolution of Earth's crust, as well as their implications for biological evolution. Although U and Th share similar chemical characteristics, their distinct physical properties lead to different geological behaviors. Uranium, with an atomic number of 92 and a half-life of approximately 4.5 billion years, contrasts with thorium, which has an atomic number of 90 and a half-life of about 1.4 billion years. Both elements are naturally present in the Earth's crust, but their concentrations vary depending on the type of rock. They are commonly found in igneous rocks such as granite and basalt, where they form minerals like uraninite and thorite, respectively. Additionally, these minerals may contain trace amounts of other metals, including lead, silver, gold, copper, zinc, arsenic, selenium, molybdenum, cadmium, mercury, bismuth, antimony, tellurium, cobalt, nickel, manganese, iron, vanadium, chromium, tungsten, titanium, zirconium, niobium, tantalum, hafnium, rhenium, osmium, iridium, platinum, palladium, rhodium, ruthenium, and iridium. Uranium and thorium are also found in sedimentary formations such as sandstone, shale, marble, dolomite, gypsum, anhydrite, salt beds, and evaporites, which are formed through weathering processes driven by water erosion. The interactions of these elements within geological processes not only contribute to the physical structure of the Earth's crust but also play a crucial role in the broader context of biological evolution, influencing the availability of essential nutrients and the development of life on our planet. This study aims to provide a comprehensive understanding of the geochemical dynamics of U and Th and their far-reaching implications for Earth's geological and biological history.",
        "ori-fast-z-score": -1.7457431218879391,
        "water-fast-z-score": 4.447074385282452,
        "rewrite-fast-z-score": 0.09166984970282113
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The AU Microscopii Debris Disk: Multiwavelength Imaging and Modeling .\nAbstract:\nWe present new multiwavelength observations of the debris disk surrounding the nearby star AU Mic, which is located at an age of ~10 Myr in the constellation Pictor (~25 pc). The system has been studied extensively over many decades using ground-based optical imaging techniques as well as space-based infrared photometry and spectroscopy. We have obtained high-resolution images with the Hubble Space Telescope s Wide Field Camera 3 (WFC3) near-infrared camera to study the dust distribution on small scales. These data are complemented by archival Spitzer Infrared Array Camera (IRAC), Herschel Photodetector Array Camera & Spectrometer (PACS), and Submillimeter Array (SMA) observations that probe larger spatial scales. Our results show that there exists a large amount of cold dust within 1 AU of the central star, but no evidence for warm dust emission beyond this radius. This suggests that the inner edge of the outer disk may be truncated due to tidal forces exerted by the planet candidate AU Mic b.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The AU Microscopii Debris Disk : Multiwavelength Imaging and Modeling . Abstract : We report new multiwavelength studies of the rubble disk surrounding the nearby star AU Mic , which is situated at an age of ~ 10 Myr in the constellation Pictor ( ~ 25 pc ) .The system has been studied frequently over numerous years employing ground - based optical optical techniques as well as space - based infrared photometry and spectroscopy . We have achieved high - resolution images with the Hubble Space Telescope s Wide Field Camera 3 ( WFC3 ) near - infrared camera to study the dust flow on small scales .These data are complemented by archival Spitzer Infrared Array Camera ( IRAC ) , Herschel Photodetector Array Camera & Spectrometer ( PACS ) , and Submillimeter Array ( SMA ) observations that examine bigger spatial scales . Our results show that there exists a large number of cold powder within 1 AU of the main star , but no evidence for cool dust absorption beyond this radius .This implies that the inner boundary of the inner disk might be truncated due to tidal forces exerted by the planet candidate AU Mic b .",
        "rewrite_text": "We present a comprehensive analysis of the debris disk surrounding the nearby star AU Microscopii, located approximately 25 parsecs away in the constellation Pictor and estimated to be around 10 million years old. This study employs a multiwavelength approach, integrating data from both ground-based optical observations and space-based infrared photometry and spectroscopy, to enhance our understanding of the disk's structure and dynamics. Utilizing the Hubble Space Telescope's Wide Field Camera 3 (WFC3), we obtained high-resolution near-infrared images that allow us to investigate the dust distribution and flow on smaller scales. These observations are further supported by archival data from the Spitzer Infrared Array Camera (IRAC), the Herschel Photodetector Array Camera and Spectrometer (PACS), and the Submillimeter Array (SMA), which provide insights into larger spatial scales of the debris disk. Our findings reveal a significant presence of cold dust within 1 astronomical unit (AU) of AU Mic, while there is no indication of cool dust absorption beyond this region. This observation suggests that the inner boundary of the debris disk may be influenced by tidal interactions with the planet candidate AU Mic b, potentially truncating the disk's inner region. Overall, this study enhances our understanding of the complex interactions within debris disks and their evolution in planetary systems.",
        "ori-fast-z-score": -0.4588314677411235,
        "water-fast-z-score": 4.444462481925879,
        "rewrite-fast-z-score": -0.10369516947304253
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Jet-disturbed molecular gas near the Seyfert 2 nucleus in M51 .\nAbstract:\nWe present new observations with the Atacama Large Millimeter/submillimeter Array (ALMA) that reveal an extended region of disturbed molecular gas surrounding the active galactic nucleus (AGN) of NGC 5194, also known as M51a or Whirlpool Galaxy. The AGN is located at the center of this interacting galaxy pair and has been classified as a Seyfert 2 based on optical spectroscopy. We detect two prominent dust lanes extending to the north-east and south-west of the AGN along its minor axis. These are likely caused by tidal forces between the galaxies during their interaction. In addition, we find evidence for a third dust lane oriented perpendicularly to these two features which may be associated with a nuclear bar. Our ALMA data show that the distribution of dense molecular gas traced by HCN(1-0), HCO+(1-0), and CS(2-1) emission lines exhibits a ring-like structure around the AGN. This feature appears to have been shaped by powerful outflows driven by the AGN.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Jet - disturbed molecular gas near the Seyfert 2 nucleus in M51 . Abstract : We report new data with the Atacama Large Millimeter / submillimeter Array ( ALMA ) that indicate an extended region of disturbed molecular gas surrounding the active galactic nucleus ( AGN ) of NGC 5194 , sometimes called as M51a or Whirlpool Galaxy .The AGN is situated at the center of this interacting galaxy pair and has been classified as a Seyfert 2 using on optical spectroscopy . We detect two notable cloud routes extending to the north - west and south - west of the AGN along its minor axis .These are likely affected by tidal forces between the galaxies during their interaction . In addition , we find proof for a third dust track focused perpendicularly to these two structures which may be identified with a nuclear bar .Our ALMA statistics demonstrate that the distribution of dense molecular fuel linked by HCN ( 1 - 0 ) , HCO + ( 1 - 0 ) , and CS ( 2 - 1 ) emission lines exhibits a ring - like structure around the AGN . This characteristic appears to have been shaped by massive outflows driven by the AGN .",
        "rewrite_text": "We present new findings from the Atacama Large Millimeter/submillimeter Array (ALMA) that reveal an extensive region of disturbed molecular gas surrounding the active galactic nucleus (AGN) of NGC 5194, commonly referred to as M51a or the Whirlpool Galaxy. The AGN, located at the core of this interacting galaxy pair, has been classified as a Seyfert 2 based on optical spectroscopy analyses. Our observations identify two prominent molecular gas pathways extending northwest and southwest from the AGN along its minor axis, likely influenced by tidal interactions between the galaxies as they engage with one another. Furthermore, we observe evidence of a third dust lane oriented perpendicularly to the aforementioned structures, which may correspond to a nuclear bar configuration. The data obtained from ALMA indicate that the distribution of dense molecular gas, traced by emission lines from HCN (1-0), HCO+ (1-0), and CS (2-1), forms a ring-like structure encircling the AGN. This distinctive morphology is believed to be a result of powerful outflows generated by the AGN itself. Our findings contribute to the understanding of the complex dynamics of molecular gas in the vicinity of active galactic nuclei and highlight the significant role of tidal interactions and AGN-driven processes in shaping the gas distribution in such environments. This study enhances our comprehension of the interplay between galaxy interactions and the activity of supermassive black holes, providing insights into the mechanisms that govern star formation and the evolution of galaxies in the universe.",
        "ori-fast-z-score": -0.562543950463012,
        "water-fast-z-score": 4.695742752749558,
        "rewrite-fast-z-score": 0.8620436566990363
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Swift observations of GRB 060614: an anomalous burst with a well behaved afterglow .\nAbstract:\nWe report on the Swift satellite s rapid response to the gamma-ray burst (GRB) 060614, which was detected by the Burst Alert Telescope at 07:41:06 UT June 14 2006 and localized within 1 arcmin in less than 3 minutes. The X-ray telescope began observing the field about 10 s later; optical photometry started about 20 s after that. We find no evidence for any unusual behavior during or immediately following this event. In particular we do not see any indication of a supernova component associated with it as has been seen in some other events. However, our data show that there is a very bright fading source coincident with the position determined by Swift/XRT. This source shows clear signs of being dominated by emission lines characteristic of a Wolf-Rayet star. Our results are consistent with those reported previously using ground-based telescopes. \n \n Keywords: Gamma ray bursts, Afterglows",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Swift discoveries of GRB 060614 : an anomalous burst with a better behaved afterglow . Abstract : We report on the Swift satellite s rapid response to the alpha - ray flare ( GRB ) 060614 , which was detected by the Burst Alert Telescope at 07 : 41 : 06 UT June 14 2006 and localized within 1 arcmin in fewer than 3 seconds .The X - ray observatory commenced observing the field about 10 s later ; optical photometry begun about 20 s after that . We see no evidence for any strange activity during or immediately surrounding this event .In particular we do not see any evidence of a supernova component involved with it as has been seen in some other events . However , our statistics demonstrate that there is a very bright fading source coincident with the direction determined by Swift / XRT .This source shows significant signs of being dominated by radiation patterns characteristic of a Wolf - Rayet star . Our results are compatible with those confirmed previously used ground - based telescopes .Keywords : Gamma ray bursts , Afterglows",
        "rewrite_text": "Title: Swift Discoveries of GRB 060614: An Anomalous Burst with a Better-Behaved Afterglow\n\nAbstract: In this study, we present the findings from the Swift satellite's prompt response to the gamma-ray burst (GRB) 060614, which was detected by the Burst Alert Telescope at 07:41:06 UT on June 14, 2006. The localization of this event was achieved within an impressive 1 arcminute in less than 3 seconds. Following the detection, the Swift X-ray observatory initiated observations of the region approximately 10 seconds later, while optical photometry commenced around 20 seconds thereafter. Notably, our analysis reveals no unusual activity associated with this burst during or in the immediate aftermath of the event. In particular, we found no indications of a supernova component, which has been observed in several other gamma-ray bursts. However, our statistical analysis indicates the presence of a remarkably bright fading source aligned with the coordinates determined by Swift/XRT. This source exhibits significant characteristics consistent with radiation patterns typically associated with Wolf-Rayet stars. Our findings align with previous observations made by ground-based telescopes, reinforcing the validity of our results. This research contributes to the understanding of gamma-ray bursts and their afterglows, particularly highlighting the unique nature of GRB 060614 and its aftermath. The implications of these observations may provide further insights into the mechanisms underlying gamma-ray bursts and their associated phenomena. \n\nKeywords: Gamma-ray bursts, Afterglows.",
        "ori-fast-z-score": -0.47809144373375745,
        "water-fast-z-score": 6.052586456788652,
        "rewrite-fast-z-score": 1.1055415967851332
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Predicting the connectivity of primate cortical networks from topological and spatial node properties .\nAbstract:\nWe present an approach to predict the structural network organization in the cerebral cortex based on local node features, such as their position within the brain s surface or volume, and global topological characteristics. We use this method to study how different types of nodes are connected with each other across species (human, macaque monkey) and modalities (diffusion MRI tractography). Our results show that our model can accurately reproduce known patterns of cortico-cortical connections between areas, including those observed in humans but not yet described for monkeys. The proposed framework is general enough to be applied to any type of data where information about individual nodes  positions and pairwise interactions exists. This includes both anatomical and functional imaging datasets, which will allow us to investigate the relationship between structure and function at multiple scales. \n \n Introduction \n \n Brain connectomics aims to map all neuronal elements into a single comprehensive description of the human brain  1  . In recent years, advances in neuroimaging techniques have allowed researchers to obtain detailed maps of the brain s structural  2  , metabolic  3  , and functional  4  architecture. These new technologies provide unprecedented opportunities to understand how the brain works by studying its large-scale organization  5  .\n \nHowever, despite these advancements, there remains significant uncertainty regarding the precise nature of the relationships among neurons  6  . For example, it has been shown that some regions of the brain communicate more frequently than others  7-9 , while others exhibit higher levels of synchrony  10  . However, we still do not know whether these differences reflect specific wiring rules  11  or simply arise due to random fluctuations  12  . \n \n Here, we propose a novel computational framework to address this problem using machine learning methods  13  . Specifically, we aim to develop models capable of predicting the strength of connection between pairs of nodes given only information about their location and topology  14  . To achieve this goal, we first construct a set of training examples consisting of pairs of nodes whose interaction strengths are known  15  . Then, we train a classifier to learn the mapping between node features and edge weights  16  . Finally, we apply the trained model to unseen test cases  17  to infer unknown interactions",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Predicting the connectivity of primate cortical networks from topological and spatial node characteristics . Abstract : We present an approach to predict the structural network structure in the brain cortex based on local node characteristics , such as their placement within the brain s surface or volume , and global topological traits .We use this technology to study how various types of nodes are connected with each other across taxa ( human , macaque animal ) and modalities ( diffusion MRI tractography ) . Our results show that our model can accurately demonstrate established trends of cortico - cortical networks between zones , notably those observed in humans but not already explained for monkeys .The proposed framework is general enough to be applied to any type of evidence where information about individual nodes positions and pairwise relationships exists . This encompasses both anatomical and functional imaging datasets , which will let us to examine the relationship between form and function at multiple scales .Introduction Brain connectomics aims to map all neuronal components into a single comprehensive account of the human mind 1 . In recent months , advances in neuroimaging techniques have permitted investigators to obtain detailed maps of the brain s structural 2 , metabolic 3 , and physiological 4 architecture .These new inventions offer unprecedented possibilities to realize how the brain acts by examining its large - scale organization 5 . However , despite these advancements , there exists significant confusion regarding the exact nature of the relationships among neurons 6 .For instance , it has been shown that some regions of the brain communicate more frequently than others 7 - 9 , while many exhibit greater levels of synchrony 10 . However , we also do not understand whether these changes reflect specific wiring requirements 11 or simply arise due to random fluctuations 12 .Here , we develop a new computational framework to tackle this question using computer learning techniques 13 . Specifically , we attempt to develop models capable of predicting the strength of relationship between pairs of nodes given only data about their direction and topology 14 .To achieve this goal , we first build a setting of training instance comprised of pairs of nodes whose interaction abilities are known 15 . Then , we train a classifier to teach the mapping between node characteristics and edge weights 16 .Finally , we apply the trained model to unseen test cases 17 to infer unknown interactions",
        "rewrite_text": "**Title:** Predicting the Connectivity of Primate Cortical Networks from Topological and Spatial Node Characteristics\n\n**Abstract:** In this study, we introduce a novel methodology for predicting the structural connectivity of cortical networks in the brain by leveraging local node characteristics—such as their spatial positioning on the brain's surface or within its volume—and global topological features. Our approach is applied to investigate the interconnectivity of various node types across different species, specifically humans and macaques, utilizing diffusion MRI tractography as our primary modality. The findings indicate that our predictive model effectively captures established patterns of cortico-cortical connectivity, particularly those recognized in humans, while also revealing insights that have not been previously documented in macaques. This framework is versatile and can be adapted to analyze any dataset where information regarding individual node locations and their pairwise relationships is available. This includes both anatomical and functional imaging datasets, enabling us to explore the intricate relationship between structural form and functional dynamics across multiple scales.\n\nThe field of brain connectomics seeks to create a comprehensive mapping of neuronal components to enhance our understanding of the human mind. Recent advancements in neuroimaging technologies have facilitated the acquisition of detailed structural, metabolic, and physiological maps of the brain, providing unprecedented insights into its large-scale organization. Despite these technological strides, significant ambiguities remain regarding the nature of neuronal relationships. For example, certain brain regions exhibit more frequent communication than others, while some display higher synchrony levels. However, it remains unclear whether these patterns are indicative of specific wiring requirements or are merely the result of random fluctuations.\n\nTo address these uncertainties, we have developed a computational framework that employs machine learning techniques to predict the strength of relationships between pairs of nodes based solely on their directional and topological characteristics. Our methodology involves constructing a training dataset consisting of known interactions between node pairs, followed by training a classifier to establish a mapping between node attributes and edge weights. Finally, we apply the trained model to previously unseen test cases to infer unknown interactions, thereby advancing our understanding of cortical network connectivity.",
        "ori-fast-z-score": -0.7877263614433762,
        "water-fast-z-score": 8.857142857142858,
        "rewrite-fast-z-score": 0.9827076298239908
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  EC-FORC: A New Cyclic Voltammetry Based Method for Examining Phase Transitions and Predicting Equilibrium .\nAbstract:\nThe electrochemical properties of materials are often strongly influenced by their phase transitions, which can be difficult to predict using conventional methods. In this work we present an approach that uses cyclic voltammetry (CV) data collected at different temperatures as input into a machine learning model in order to examine the thermodynamic stability of phases within a material system. We demonstrate our method on two prototypical systems - Fe3O4 and LiFePO4 - where it is able to successfully identify metastable phases and accurately predict equilibrium compositions across wide temperature ranges. The results showcase how CV-based approaches could provide new insights into complex chemical phenomena such as solid state reactions or battery degradation mechanisms. \n \n Introduction \n \n Many important physical processes occur through changes in crystal structure driven by thermal fluctuations  1  . These include transformations between polymorphs  2  , amorphous states  3  , and even liquid crystalline structures  4  . Such structural rearrangements have been shown to significantly affect the electrical  5  , optical  6  , magnetic  7  , mechanical  8  , and catalytic  9  properties of materials. As a result, understanding these transformations has become increasingly important for applications ranging from energy storage  10  to catalysis  11  .\n \nIn many cases, however, predicting the outcome of a transformation requires knowledge about its underlying free energies  12  . This information cannot always be obtained directly via experimentation due to kinetic barriers  13  , making computational techniques  14  particularly useful  15  . Unfortunately, most current theoretical models  16  require extensive parameterization  17  and/or detailed experimental characterization  18  before they can be applied effectively  19  . Moreover, while some recent studies  20  have demonstrated successes with deep neural networks  21  , there remains significant uncertainty regarding whether these approaches will generalize well  22  . \n \n Herein, we propose a novel approach based on cyclic voltammetry  23  that allows us to probe the thermodynamics of phase transformations without requiring any prior assumptions about the nature of the transition  24  . Our technique relies on collecting CV data over a range of temperatures  25  and then training a supervised  26  machine learning algorithm  27  to learn relationships between the measured currents  28  and the corresponding Gibbs free energies  29  . Once trained,...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : EC - FORC : A New Cyclic Voltammetry Based Method for Examining Phase Transitions and Predicting Equilibrium . Abstract : The electrochemical qualities of substances are often strongly influenced by their phase transitions , which can be difficult to predict using conventional methods .In this research we present an methods that using cyclic voltammetry ( CV ) statistics obtained at different temperatures as input into a machine learning model in order to examine the thermodynamic stability of phases within a solid system . We test our technique on two prototypical models - Fe3O4 and LiFePO4 - where it is ability to effectively identify metastable layers and correctly calculate equilibrium compositions across wide temperature ranges .The results highlights how CV - based methods could give novel knowledge into complex chemical processes such as solid state reactions or battery reduction structures . Introduction Many key physical processes change through alterations in crystal formation powered by temperature fluctuations 1 .These include transformations between polymorphs 2 , amorphous states 3 , and sometimes liquid crystalline structures 4 . Such structural rearrangements have been shown to significantly affect the electrical 5 , optical 6 , thermal 7 , thermal 8 , and catalytic 9 properties of substances .As a result , studying these transformations has become rapidly crucial for applications extending from power processing 10 to catalysis 11 . In many situations , however , predicting the result of a transformation requires knowledge about its underlying free energies 12 .This knowledge cannot often be obtained immediately via experimentation owing to kinetic limits 13 , making computational approaches 14 particularly useful 15 . Unfortunately , most current theoretical theories 16 require extensive parameterization 17 and / or extensive theoretical characterization 18 before they can be applied effectively 19 .Moreover , while some recent studies 20 have demonstrated successes with soft neural systems 21 , there exists significant speculation regarding whether these solutions will generalize well 22 . Herein , we propose a innovative method built on cyclic voltammetry 23 that enables us to probe the thermodynamics of phase transformations without using any earlier predictions about the nature of the transition 24 .Our practice relies on gathering CV measurements over a range of conditions 25 and then testing a controlled 26 machine learning algorithm 27 to study relationships between the measured currents 28 and the associated Gibbs free energies 29 . Once trained , . . .",
        "rewrite_text": "**Title:** EC-FORC: A Novel Cyclic Voltammetry-Based Approach for Investigating Phase Transitions and Predicting Equilibrium\n\n**Abstract:** The electrochemical properties of materials are significantly affected by phase transitions, which are often challenging to predict through traditional methodologies. In this study, we introduce a novel approach that leverages cyclic voltammetry (CV) data collected at various temperatures as input for a machine learning model, aimed at assessing the thermodynamic stability of phases within solid systems. We validate our method using two representative materials, Fe3O4 and LiFePO4, demonstrating its capability to accurately identify metastable phases and compute equilibrium compositions across a broad temperature spectrum. Our findings underscore the potential of CV-based techniques to provide new insights into intricate chemical phenomena, including solid-state reactions and battery reduction mechanisms.\n\nThe introduction of this research highlights the critical role of temperature-induced changes in crystal structures, which can lead to significant transformations such as polymorphic changes, amorphous states, and liquid crystalline formations. These structural modifications have profound implications for the electrical, optical, thermal, and catalytic properties of materials, making the study of these transitions increasingly important for applications ranging from energy storage to catalysis. However, accurately predicting the outcomes of these transformations often necessitates an understanding of the underlying free energy landscapes, which are not readily accessible through experimental means due to kinetic constraints. Consequently, computational methods have become essential in this domain.\n\nDespite advancements in theoretical frameworks, many existing models require extensive parameterization and detailed theoretical characterization before they can be effectively employed. While recent research has shown promise with soft neural networks, there remains uncertainty regarding their generalizability across different systems. In response to these challenges, we propose an innovative method based on cyclic voltammetry that allows for the exploration of thermodynamic properties of phase transitions without relying on prior assumptions about the nature of the transitions. Our approach involves collecting CV measurements under varying conditions and employing a controlled machine learning algorithm to analyze the relationships between the measured currents and the corresponding Gibbs free energies. This method not only enhances our understanding of phase behavior but also paves the way for more accurate predictions in materials science.",
        "ori-fast-z-score": -1.906412495277593,
        "water-fast-z-score": 8.60201107849755,
        "rewrite-fast-z-score": 0.5146502354656654
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Systematic uncertainties of hadron parameters obtained with QCD sum rules .\nAbstract:\nWe present the results on systematic uncertainties in determination of hadronic parameters using QCD Sum Rules (QSR). We consider two types of uncertainties: theoretical and experimental ones. Theoretical uncertainty is estimated by varying renormalization scale, Borel mass parameter and threshold value used to separate ground state contribution from excited states one. Experimental error comes mainly from errors in input values for quark masses and decay constants. In addition we take into account also dependence of final result on choice of interpolating current. For each type of uncertainty we calculate its effect separately as well as total uncertainty which includes all sources mentioned above. Finally we compare our results with those presented recently in literature. \n \n Keywords: Systematics, Uncertainty, Hadronic Parameters, QCD Sum Rules, Quark Masses, Decay Constants. 1 Introduction \n \n In this work we study systematic uncertainties in determination of various hadronic parameters like masses, decay constants etc., using QCD Sum Rules  1  . This method allows us to obtain information about properties of low-lying resonances starting from first principles - Quantum Chromodynamics (QCD)  2  , without any additional assumptions or models. It has been successfully applied to many different processes involving light quarks such as: pion form factor  3  , nucleon electromagnetic  4  and axial  5  form factors  6  , semileptonic decays  7, 8  , heavy-light mesons  9  , charmonium  10  and bottomonium  11  systems  12  .\n \nThe main idea behind QCD Sum Rules is that correlation function constructed out of currents corresponding to particular quantum numbers can be represented as dispersion relation over physical spectrum of particles contributing to it. Then, after applying double Borel transformation  13  , i.e. transforming both variables p^2 and q^2 simultaneously  14  , where p is momentum flowing through initial state and q is momentum transferred between initial and final states, one obtains so-called phenomenological representation  15  :",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Systematic uncertainties of hadron parameters obtained with QCD sum rules . Abstract : We report the conclusion on systematic uncertainties in calculation of hadronic variables using QCD Sum Rules ( QSR ) .We consider two forms of uncertainties : theoretical and experimental ones . Theoretical uncertainty is calculated by varying renormalization scale , Borel mass factor and threshold factor used to separate ground state contribution from excited states one .Experimental error happens mostly from errors in input parameters for quark masses and decay constants . In addition we took into consideration also dependence of last result on choosing of interpolating current .For each type of uncertainty we estimate its impact independently as well as maximum uncertainty which includes all sources discussed above . Finally we compare our findings with those provided recently in literature .Keywords : Systematics , Uncertainty , Hadronic Parameters , QCD Sum Rules , Quark Masses , Decay Constants . 1 Introduction In this project we study systematic uncertainties in calculation of several hadronic variables like masses , decay constants etc . , using QCD Sum Rules 1 .This method enables us to obtain knowledge about properties of low - lying resonances coming from first principles - Quantum Chromodynamics ( QCD ) 2 , without any additional constraints or models . It has been successfully applied to many various reactions involving light quarks such as : pion form factor 3 , nucleon electromagnetic 4 and axial 5 form factors 6 , semileptonic decays 7 , 8 , heavy - heavy mesons 9 , charmonium 10 and bottomonium 11 systems 12 .The main idea behind QCD Sum Rules is that correlation function formed out of currents associated to specified quantum numbers can be described as dispersion relation over physical spectrum of particles contributing to it . Then , after applying dual Borel transformation 13 , i . e .transforming both variables q ^ 2 and q ^ 2 simultaneously 14 , where p is momentum flowing through initial state and q is momentum transferred between initial and final states , one obtains so - called phenomenological representation 15 :",
        "rewrite_text": "**Title:** Systematic Uncertainties of Hadron Parameters Obtained with QCD Sum Rules\n\n**Abstract:** This article presents a comprehensive analysis of systematic uncertainties associated with the calculation of hadronic parameters using Quantum Chromodynamics (QCD) Sum Rules (QSR). We categorize these uncertainties into two main types: theoretical and experimental. Theoretical uncertainties arise from variations in the renormalization scale, Borel mass factor, and threshold factor, which are crucial for distinguishing the contributions of ground states from those of excited states. On the other hand, experimental uncertainties primarily stem from inaccuracies in the input parameters, such as quark masses and decay constants. Additionally, we examine how the choice of interpolating current influences the final results. For each category of uncertainty, we provide independent estimates of their impacts, as well as a comprehensive assessment of the maximum uncertainty that encompasses all discussed sources. Our findings are then compared with recent literature to contextualize our results within the broader field. This study not only highlights the importance of understanding systematic uncertainties in QCD Sum Rules but also aims to enhance the reliability of hadronic parameter calculations, which are vital for advancing our knowledge of particle physics. \n\n**Keywords:** Systematics, Uncertainty, Hadronic Parameters, QCD Sum Rules, Quark Masses, Decay Constants. \n\n**1 Introduction:** In this research, we investigate the systematic uncertainties involved in calculating various hadronic variables, such as masses and decay constants, through the application of QCD Sum Rules. This methodology allows us to derive insights into the properties of low-lying resonances based on first principles of Quantum Chromodynamics, without relying on additional models or constraints. QCD Sum Rules have been effectively utilized in numerous studies involving light quarks, including analyses of the pion form factor, nucleon electromagnetic and axial form factors, semileptonic decays, and heavy meson systems like charmonium and bottomonium. The fundamental premise of QCD Sum Rules is that the correlation function, constructed from currents with specific quantum numbers, can be expressed as a dispersion relation over the physical spectrum of contributing particles. By applying a dual Borel transformation to the relevant variables, we derive a phenomenological representation that facilitates further exploration of hadronic properties.",
        "ori-fast-z-score": -0.9863939238321437,
        "water-fast-z-score": 5.816546635685188,
        "rewrite-fast-z-score": -0.4181210050035454
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Sensitivity of Hybrid Differential Stereoscopy for Spectral Imaging .\nAbstract:\nWe present the sensitivity analysis of hybrid differential stereoscopy (HDS) in spectral imaging systems, which is based on the combination of two different techniques to obtain 3D information and has been recently proposed as an alternative method to conventional triangulation-based approaches.  We show that HDS can be considered as a generalization of other existing methods such as structured light or time-of-flight cameras by introducing additional degrees of freedom into the system design. In particular, we demonstrate how these parameters affect the performance of the technique when applied to hyperspectral data sets acquired with a pushbroom sensor mounted onto a satellite platform. Finally, we provide some guidelines about optimal values of those parameters depending on the application requirements. © 2014 Optical Society. Published by SPIE under the terms of the CC BY 3.0 license. The ability to acquire accurate three-dimensional (3D) information at high frame rates opens up new possibilities in many fields including remote sensing, medical diagnostics, surveillance, robotics, etc., where fast response times are required  1  . Among all available technologies, stereo vision stands out due to its low cost and simplicity  2  , but it suffers from inherent limitations related to the need of having textured surfaces within the scene  3  .\nIn recent years, several alternatives have emerged to overcome this problem  4  -  6  . One of them consists of using multiple images taken simultaneously from slightly displaced viewpoints  7  . This approach allows one to recover depth maps even if there is no texture in the scene  8  . However, the accuracy achieved depends strongly on the baseline between the camera positions  9  . Another possibility relies on the use of active illumination  10  , although this solution may not always be practical because of safety concerns  11  . A third option involves the use of coded patterns  12  , which require special hardware  13  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Sensitivity of Hybrid Differential Stereoscopy for Spectral Imaging . Abstract : We present the sensitivity analysis of hybrid differential stereoscopy ( HDS ) in spectral scan systems , which is based on the combination of two different methods to obtain 3D information and has been lately advocated as an additional method to conventional triangulation - based methods .We see that HDS can be regarded as a generalization of other existing techniques such as structured light or time - of - flight images by bringing added degrees of liberty into the system model . In particular , we demonstrate how these parameters affect the performance of the method when applied to hyperspectral information sets received with a pushbroom sensor mounted onto a spacecraft platform .Finally , we provide some guidelines about ideal values of those variables depending on the user requirements . © 2014 Optical Society .Published by SPIE under the terms of the CC BY 3 . 0 license . The capabilities to acquire precise three - dimensional ( 3D ) information at high frame rates opens up new possibilities in different fields including distant detection , hospital diagnostics , surveillance , robotics , etc . , where fast response periods are required 1 .Among all available technologies , stereo vision stands out due to its minimal cost and simplicity 2 , but it suffers from inherent limitations associated to the necessity of having textured edges within the picture 3 . In recent years , various options have arose to overcome this situation 4 - 6 .One of them consists of using multiple photos taken concurrently from somewhat displaced viewpoints 7 . This method enables one to regain depth mapping even if there is no texture in the image 8 .However , the accuracy achieved depends strongly on the baseline between the photographer positions 9 . Another possibility relies on the using of active imaging 10 , although this solution might not always be possible because of security concerns 11 .A third possibility requires the using of programmed numbers 12 , which require extra architecture 13 .",
        "rewrite_text": "In this article, we conduct a sensitivity analysis of hybrid differential stereoscopy (HDS) within the context of spectral imaging systems. HDS is an innovative approach that integrates two distinct methodologies to extract three-dimensional (3D) information, and it has recently been proposed as a complementary technique to traditional triangulation-based methods. Our findings suggest that HDS serves as a broader framework that encompasses existing techniques, such as structured light and time-of-flight imaging, by introducing additional degrees of freedom into the system model. \n\nWe specifically illustrate how various parameters influence the effectiveness of HDS when applied to hyperspectral datasets collected by a pushbroom sensor mounted on a spacecraft. This analysis is crucial for understanding the operational limits and capabilities of HDS in practical applications. Furthermore, we offer practical recommendations regarding optimal parameter values tailored to specific user requirements, enhancing the utility of HDS in diverse fields.\n\nThe ability to capture accurate 3D information at high frame rates presents significant opportunities across various domains, including remote sensing, medical diagnostics, surveillance, and robotics, where rapid response times are essential. Among the available technologies, stereo vision is particularly notable for its cost-effectiveness and simplicity. However, it is limited by the need for textured edges in the captured images. Recent advancements have introduced several strategies to mitigate these limitations. One such strategy involves capturing multiple images from slightly different viewpoints, which allows for depth mapping even in the absence of texture. The accuracy of this method is highly dependent on the distance between the viewpoints. \n\nAlternative approaches include active imaging, which may not always be feasible due to security issues, and the use of programmed patterns that necessitate additional infrastructure. Overall, our study highlights the potential of HDS as a versatile tool for enhancing spectral imaging capabilities, while also addressing the challenges and considerations associated with its implementation. © 2014 Optical Society. Published by SPIE under the terms of the CC BY 3.0 license.",
        "ori-fast-z-score": -0.8835412617927487,
        "water-fast-z-score": 9.278703552902286,
        "rewrite-fast-z-score": 0.5384615384615384
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Differentially rotating force-free magnetosphere of an aligned rotator: analytical solutions in split-monopole approximation .\nAbstract:\nWe present the first exact solution for the axisymmetric, stationary and force-free magnetic field structure around a rapidly-rotating neutron star with arbitrary inclination angle between its rotation and magnetic axes. The model is based on the assumption that the stellar surface is covered by a thin layer of plasma which corotates with the star at all latitudes. We show how this solution can be used to calculate the X-ray light curves produced by hot spots located at different latitudes above the stellar surface. In particular we demonstrate that the observed phase shifts between the peaks of soft X-rays (0.5-2 keV) and hard X-rays (2-10 keV), as well as their relative amplitudes are reproduced very accurately if one assumes that both emission components originate from two antipodal hot spots located near the magnetic poles. This result suggests that the observed X-ray pulsations may be caused by the rotational modulation of the local emissivity rather than by the Doppler effect due to bulk motion of matter within the emitting regions. \n \n Keywords: Force-free fields, Pulsar wind nebulae",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Differentially shifting force - free magnetosphere of an aligned rotator : analytical solutions in split - monopole approximation . Abstract : We present the first accurate solution for the axisymmetric , stationary and force - free magnetic field configuration around a rapidly - spinning neutron galaxy with arbitrary orientation angle between its rotation and magnetic axes .The model is based on the assumption that the stellar surface is enclosed by a thin layer of liquid which corotates with the star at all latitudes . We see how this solution can be used to estimate the X - ray light angles created by hot areas situated at different latitudes above the stellar surface .In particular we prove that the observed phase change between the peaks of hard X - radiation ( 0 . 5 - 2 keV ) and hard X - radiation ( 2 - 10 keV ) , as well as their relative amplitudes are reconstructed very correctly if one assumes that both emission parts originate from two antipodal hot areas situated near the magnetic poles . This result suggests that the seen X - ray pulsations may be caused by the rotational modulation of the local emissivity rather than by the Doppler impact owing to bulk movement of matter within the emitting regions .Keywords : Force - free fields , Pulsar wind nebulae",
        "rewrite_text": "Title: Differentially Shifting Force-Free Magnetosphere of an Aligned Rotator: Analytical Solutions in Split-Monopole Approximation\n\nAbstract: In this study, we introduce the first precise solution for the axisymmetric, stationary, and force-free magnetic field configuration surrounding a rapidly rotating neutron star, accommodating an arbitrary angle between its rotation and magnetic axes. Our model is predicated on the premise that the stellar surface is enveloped by a thin layer of liquid that co-rotates with the star across all latitudes. This framework allows us to analyze the X-ray emission patterns generated by hot regions located at various latitudes above the stellar surface. Notably, we demonstrate that the observed phase shift between the peaks of hard X-ray emissions in the 0.5 - 2 keV range and those in the 2 - 10 keV range, along with their relative amplitudes, can be accurately reconstructed by positing that both emission components arise from two antipodal hot spots situated near the magnetic poles. This finding implies that the X-ray pulsations detected may be attributed to the rotational modulation of local emissivity, rather than being a consequence of Doppler effects due to the bulk motion of matter within the emitting regions. Our results contribute to a deeper understanding of the dynamics of force-free fields and the behavior of pulsar wind nebulae, providing valuable insights into the mechanisms underlying X-ray emissions from neutron stars. \n\nKeywords: Force-free fields, Pulsar wind nebulae.",
        "ori-fast-z-score": -1.9095718489925029,
        "water-fast-z-score": 4.6,
        "rewrite-fast-z-score": -0.5940885257860046
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Velocity Profiles in the Solar Corona from Multi-Instrument Observations .\nAbstract:\nWe present multi-instrument observations of solar coronal plasma flows, which are obtained by combining data from the Extreme Ultraviolet Imager (EUVI) onboard STEREO-A and -B with those from the Helioseismic and Magnetic Imager (HMI), Atmospheric Imaging Assembly (AIA), and Interface Region Imaging Spectrograph (IRIS). The EUV images show that there is an apparent counterclockwise rotation of the bright loop-like structures at the limb between 2011 February 24 and March 1. We find that this rotation can be explained as a result of the differential motion between the two spacecrafts along their orbits around the Sun. By applying a cross-correlation technique to the EUVI 171 Å intensity profiles observed simultaneously by both satellites, we obtain the velocity field across the solar disk for each time step during the period under study. In addition, we use HMI magnetograms to calculate the magnetic flux density distribution over the solar surface. Our results reveal that the plasma flow patterns are closely related to the photospheric magnetic fields.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Velocity Profiles in the Solar Corona from Multi - Instrument Observations . Abstract : We report dual - instrument observations of sun coronal plasma flows , which are derived by combining information from the Extreme Ultraviolet Imager ( EUVI ) onboard STEREO - A and - B with those from the Helioseismic and Magnetic Imager ( HMI ) , Atmospheric Imaging Assembly ( AIA ) , and Interface Region Imaging Spectrograph ( IRIS ) .The EUV photos suggest that there is an apparent counterclockwise rotation of the brilliant loop - like structures at the limb between 2011 February 24 and March 1 . We see that this rotation can be understood as a due of the differential motion between the two spacecrafts along their orbits around the Sun .By applying a cross - correlation method to the EUVI 171 Å intensity profiles observed concurrently by both spacecraft , we obtain the velocity field across the sun disk for each time step during the period under research . In addition , we using HMI magnetograms to estimate the magnetic flux concentration distribution over the solar surface .Our results show that the plasma circulation patterns are tightly related to the photospheric magnetic waves .",
        "rewrite_text": "We present a comprehensive analysis of solar coronal plasma flows through dual-instrument observations, integrating data from the Extreme Ultraviolet Imager (EUVI) aboard the STEREO-A and STEREO-B spacecraft, alongside measurements from the Helioseismic and Magnetic Imager (HMI), Atmospheric Imaging Assembly (AIA), and Interface Region Imaging Spectrograph (IRIS). Our study focuses on the period from February 24 to March 1, 2011, during which we observed a notable counterclockwise rotation of luminous loop-like structures at the solar limb. This rotational motion is attributed to the differential movement of the two spacecraft as they orbit the Sun. To quantify the velocity field across the solar disk, we employed a cross-correlation technique on the EUVI 171 Å intensity profiles captured simultaneously by both spacecraft. This method allowed us to derive the velocity distribution at each time step throughout the observation period. Furthermore, we utilized HMI magnetograms to analyze the distribution of magnetic flux concentrations on the solar surface. Our findings reveal a strong correlation between the observed plasma circulation patterns and the underlying photospheric magnetic waves, suggesting that the dynamics of the solar corona are intricately linked to the magnetic activity occurring below the surface. This research enhances our understanding of the complex interactions between solar magnetic fields and coronal plasma flows, providing valuable insights into the mechanisms driving solar phenomena.",
        "ori-fast-z-score": -0.48507125007266594,
        "water-fast-z-score": 5.259005881071332,
        "rewrite-fast-z-score": 1.744163198544762
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Using Images to create a Hierarchical Grid Spatial Index .\nAbstract:\nWe present an algorithm for creating a hierarchical grid spatial index using images as the basis for its construction.  The algorithm is based on the observation that many real-world datasets are naturally represented by images, and can be used in conjunction with existing techniques such as R-tree or Quadtree to improve performance.   We show how our technique performs against these other methods through experiments conducted over synthetic data sets generated according to different distributions (uniform, normal, exponential) and sizes ranging between 1K and 100M points.   Our results demonstrate significant improvements in query response times when compared to traditional approaches. In this work we propose a new approach for building a spatial index which uses image processing algorithms to extract information about the dataset being indexed. This information is then used to build a hierarchy of grids whose leaf nodes contain pointers to individual objects within the dataset. These grids provide efficient access to all objects contained therein while also allowing fast queries across multiple grids at once.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Using Images to create a Hierarchical Grid Spatial Index . Abstract : We create an algorithm for constructing a hierarchical grid geographic indicator employing images as the foundation for its design .The algorithm is based on the observation that several real - world datasets are naturally represented by pictures , and can be used in partnership with existing techniques such as R - tree or Quadtree to improve performance . We see how our technique performs against these other methods through experiments conducted over synthetic information sets generated according to different distributions ( uniform , normal , exponential ) and dimensions ranging between 1K and 100M points .Our results show considerable improvements in query reply times when compared to conventional approaches . In this project we propose a new approach for building a spatial indicator which uses image processing algorithms to extract information about the dataset being indexed .This knowledge is then utilized to build a hierarchy of grids whose leaf nodes contain pointers to individual objects within the dataset . These grids allow faster entry to all items contained therein while also enabling rapid queries across multiple grids at once .",
        "rewrite_text": "In this article, we present a novel algorithm designed to construct a hierarchical grid spatial index by leveraging images as the foundational element of its architecture. Our approach is predicated on the insight that many real-world datasets can be effectively represented through visual imagery. By integrating this image-based representation with established spatial indexing techniques, such as R-trees and Quadtrees, we aim to enhance overall performance in spatial queries. \n\nTo evaluate the efficacy of our proposed method, we conducted a series of experiments utilizing synthetic datasets generated under various distributions, including uniform, normal, and exponential, with point counts ranging from 1,000 to 100 million. The results of these experiments demonstrate significant improvements in query response times when compared to traditional indexing methods. \n\nOur innovative strategy involves employing image processing algorithms to extract pertinent information from the datasets being indexed. This extracted knowledge is instrumental in constructing a hierarchical grid structure, where the leaf nodes are linked to individual objects within the dataset. This hierarchical grid not only facilitates quicker access to all contained items but also supports efficient querying across multiple grids simultaneously. \n\nOverall, our findings indicate that the integration of image-based data representation with hierarchical grid indexing can lead to substantial performance gains in spatial data management, making it a promising avenue for future research and application in geographic information systems.",
        "ori-fast-z-score": 0.618852747755276,
        "water-fast-z-score": 6.874054958970533,
        "rewrite-fast-z-score": 0.5570860145311556
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Integrable systems and complex geometry .\nAbstract:\nThe aim of this talk is to present some recent results on the relation between integrability in quantum field theory, statistical mechanics or mathematical physics and the existence of special geometric structures (complex algebraic curves) which are associated with these models. \nWe will discuss how such geometries can be used for solving exactly certain physical problems by using techniques coming from algebraic geometry like Riemann surfaces, theta functions etc.. We will also explain why it seems that many interesting integrable models have an underlying structure of a Riemann surface. Finally we will give examples where this connection has been made explicit. The talk will include several new results obtained recently by the author together with his collaborators. This work was partially supported by the DFG under grant SFB/TR9. Integrable systems play an important role in various branches of mathematics as well as theoretical physics. In particular they appear naturally when studying solvable lattice models in statistical mechanics or quantum field theories. It turns out that there exists a deep relationship between integrability and the presence of special geometric structures called algebraic curves. These curves provide powerful tools for solving exactly certain physical questions via methods from algebraic geometry like Riemann Surfaces, Theta Functions,...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Integrable systems and complex geometry . Abstract : The goal of this talk is to provide some latest findings on the relation between integrability in quantum field theory , statistical mechanics or computational physics and the existence of unique geometric objects ( complex algebraic shapes ) which are identified with these models .We will explore how such geometries can be used for solving exactly certain physical problems by using techniques come from algebraic topology like Riemann surfaces , theta functions etc . . We will also explain why it appears that several interesting integrable models have an underlying composition of a Riemann surface . Finally we will provide examples where this link has been made explicit .The talk will include several current data acquired previously by the writer together with his collaborators . This project was partially backed by the DFG under grant SFB / TR9 .Integrable systems play an important role in different areas of math as well as conceptual science . In particular they appear naturally when examining solvable lattice models in statistical mechanics or quantum field theories .It turns out that there exists a profound connection between integrability and the presence of unique geometric objects called algebraic curves . These curves provide potent tools for solving exactly certain physical problems via tools from algebraic topology like Riemann Surfaces , Theta Functions , . . .",
        "rewrite_text": "**Title:** Integrable Systems and Complex Geometry\n\n**Abstract:** This presentation aims to highlight recent discoveries regarding the interplay between integrability in quantum field theory, statistical mechanics, and computational physics, and the existence of unique geometric entities known as complex algebraic shapes associated with these models. We will delve into how these geometric structures can facilitate the exact resolution of specific physical challenges by employing methodologies derived from algebraic topology, such as Riemann surfaces and theta functions. Furthermore, we will discuss the intriguing observation that numerous integrable models exhibit an intrinsic relationship with Riemann surfaces. To illustrate this connection, we will present explicit examples where this relationship has been successfully established. The discussion will incorporate a range of current data collected by the author and collaborators, contributing to a deeper understanding of this field. This research is partially supported by the DFG under grant SFB/TR9. Integrable systems are pivotal in various mathematical domains and theoretical sciences, particularly emerging naturally in the context of solvable lattice models within statistical mechanics and quantum field theories. Our findings reveal a significant link between integrability and the existence of unique geometric constructs known as algebraic curves. These curves serve as powerful instruments for achieving exact solutions to certain physical problems, leveraging techniques from algebraic topology, including Riemann surfaces and theta functions. Through this exploration, we aim to shed light on the rich mathematical framework that underpins integrable systems and their applications in understanding complex physical phenomena.",
        "ori-fast-z-score": -0.7364596943186588,
        "water-fast-z-score": 6.141879930089016,
        "rewrite-fast-z-score": 1.590990257669732
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Conserved spin Hall conductance in two dimensional electron gas in a perpendicular magnetic field .\nAbstract:\nWe study the effect of Rashba spin-orbit interaction on the spin Hall conductivity (SHC) for an interacting two-dimensional electron system with parabolic dispersion and Zeeman splitting in presence of a uniform external magnetic field applied normal to the plane of motion. We show that SHC is independent of temperature, chemical potential and strength of disorder provided the Fermi energy lies within the Zeeman gap. The results are obtained by using the Kubo formula combined with the self-consistent Born approximation. It has been shown recently that the spin current can be generated without any net charge flow when electrons move through a nonmagnetic material under the influence of spin-orbit coupling  1  . This phenomenon known as spin Hall effect was first predicted theoretically  2  , and later observed experimentally  3  .\nThe origin of this effect is due to the fact that the spin-orbit interaction causes a transverse force which deflects the trajectories of moving particles leading to a finite spin polarization at the edges  4  . In recent years there have been several theoretical studies devoted to understand various aspects of spin Hall effect  5  -  8  . However most of these works were done either in absence or weak magnetic fields where the Landau levels do not play significant role  9  . On the other hand it is well known that the Landau level quantization plays important role in determining many physical properties such as magnetoresistance  10  , optical absorption  11  etc., especially near the quantum limit  12  . Therefore it would be interesting to investigate how the Landau levels affect the spin Hall effect.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Conserved spin Hall conductance in two dimensional electron gas in a perpendicular magnetic field . Abstract : We research the impact of Rashba spin - orbit interaction on the spin Hall conductivity ( SHC ) for an interacting two - dimensional electron system with parabolic dispersion and Zeeman splitting in presence of a uniform external magnetic field applied normal to the plane of movement .We see that SHC is independent of temperature , chemical potential and strength of disorder provided the Fermi energy rests within the Zeeman gap . The results are derived by using the Kubo formula coupled with the self - consistent Born algorithm .It has been shown lately that the spin current can be formed without any gross charge flow when nuclei move through a nonmagnetic material under the effects of spin - orbit bonding 1 . This phenomenon known as spin Hall phenomenon was first expected theoretically 2 , and later observed experimentally 3 .The origin of this effect is due to the fact that the spin - orbit interaction produces a transverse force which deflects the trajectories of moving ions giving to a finite spin polarization at the edges 4 . In recent years there have been numerous conceptual research devoted to study various details of spin Hall phenomenon 5 - 8 .However most of these works were done either in absence or low magnetic fields where the Landau concentrations do not play substantial importance 9 . On the other hand it is well established that the Landau grade quantization takes key importance in establishing many mechanical parameters such as magnetoresistance 10 , optical emission 11 etc . , particularly near the quantum limit 12 .Therefore it would be attractive to examine how the Landau concentrations influenced the spin Hall phenomenon .",
        "rewrite_text": "In this study, we investigate the effects of Rashba spin-orbit interaction on the spin Hall conductivity (SHC) in an interacting two-dimensional electron gas characterized by parabolic dispersion and Zeeman splitting, under the influence of a uniform external magnetic field applied perpendicular to the plane of motion. Our findings reveal that the SHC remains unaffected by variations in temperature, chemical potential, and disorder strength, as long as the Fermi energy is situated within the Zeeman gap. The results are derived using the Kubo formula in conjunction with the self-consistent Born approximation. Recent research has demonstrated that a spin current can be generated without significant charge flow when nuclei traverse a nonmagnetic material influenced by spin-orbit coupling. This phenomenon, known as the spin Hall effect, was initially predicted theoretically and has since been confirmed through experimental observations. The underlying mechanism of this effect stems from the spin-orbit interaction, which induces a transverse force that alters the trajectories of moving ions, resulting in a finite spin polarization at the edges of the material. In recent years, there has been a surge of interest in exploring various aspects of the spin Hall effect; however, most studies have been conducted in the absence of or under low magnetic fields, where Landau levels do not play a significant role. Conversely, it is well recognized that Landau level quantization is crucial for understanding numerous physical phenomena, such as magnetoresistance and optical emission, particularly near the quantum limit. Therefore, it is essential to investigate how Landau levels influence the spin Hall effect, which could provide deeper insights into the interplay between spin transport and magnetic fields in two-dimensional electron systems.",
        "ori-fast-z-score": -0.34050261230349943,
        "water-fast-z-score": 6.930264888773836,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The influence of AGN nuclear parameters on the FRI/FRII dichotomy .\nAbstract:\nWe study how the properties of active galactic nuclei (AGNs) affect their radio morphologies, and in particular whether they can explain the observed difference between Fanaroff-Riley type I (FRI) and II (FRII). We use high-resolution hydrodynamical simulations to follow the growth of supermassive black holes (SMBHs), which are fed by cold gas accretion at rates that depend on the SMBH mass and its environment. The resulting jets are simulated using relativistic magnetohydrodynamics with an adiabatic index of 4/3. Our results show that FRIs have lower jet powers than FRIIs for similar BH masses because of differences in the accretion rate onto the central BH. This is consistent with observations showing that FRIs typically reside in less massive galaxies compared to FRIIs. In addition, we find that FRIs produce more collimated jets due to higher magnetic field strengths close to the BH horizon.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The impact of AGN nuclear characteristics on the FRI / FRII dichotomy . Abstract : We research how the properties of active galactic nuclei ( AGNs ) impact their radio morphologies , and in particular whether they can describe the observed change between Fanaroff - Riley type I ( FRI ) and II ( FRII ) .We use large - resolution hydrodynamical simulations to follow the formation of supermassive black holes ( SMBHs ) , which are fed by cold gas accretion at levels that rely on the SMBH mass and its climate . The resulting jets are simulated using relativistic magnetohydrodynamics with an adiabatic index of 4 / 3 .Our results show that FRIs have smaller jet powers than FRIIs for related BH masses because of differences in the accretion rate onto the main BH . This is consistent with observations indicating that FRIs typically exist in fewer huge clusters compared to FRIIs .In addition , we find that FRIs produce more collimated jets resulting to higher magnetic force capacities close to the BH horizon .",
        "rewrite_text": "Title: The Influence of Active Galactic Nucleus Characteristics on the FRI/FRII Dichotomy\n\nAbstract: In this study, we investigate the relationship between the characteristics of active galactic nuclei (AGNs) and their corresponding radio morphologies, specifically focusing on the transition observed between Fanaroff-Riley type I (FRI) and type II (FRII) sources. Utilizing high-resolution hydrodynamical simulations, we explore the formation processes of supermassive black holes (SMBHs), which are primarily influenced by cold gas accretion rates that depend on both the mass of the SMBH and the surrounding environmental conditions. The jets produced by these SMBHs are modeled using relativistic magnetohydrodynamics, applying an adiabatic index of 4/3 to accurately capture the dynamics involved. Our findings reveal that FRIs exhibit lower jet powers compared to FRIIs when considering black holes of similar masses, a discrepancy attributed to variations in the accretion rates onto the central black hole. This observation aligns with existing data suggesting that FRIs are predominantly found in less massive galaxy clusters than their FRII counterparts. Furthermore, our simulations indicate that FRIs generate more collimated jets, which results in enhanced magnetic field strengths in proximity to the black hole horizon. This research contributes to a deeper understanding of the mechanisms driving the FRI/FRII dichotomy and highlights the significant role of AGN properties in shaping their radio emissions and morphological classifications.",
        "ori-fast-z-score": -0.7171371656006361,
        "water-fast-z-score": 3.916379472039716,
        "rewrite-fast-z-score": 0.30460384954008574
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Geometrically frustrated magnetic behavior of Sr3NiRhO6 and Sr3NiPtO6 .\nAbstract:\nThe geometric frustration in the spin-1/2 triangular lattice is studied by means of neutron powder diffraction, magnetization measurements, specific heat data, and first-principles calculations for two new compounds Sr3NiRhO 6 and Sr 3 Ni Pt O 6 . The results show that both compounds are antiferromagnetic insulators with Néel temperatures TN = 5 K (Sr3NiRhO6) and T N = 7 K (Sr3NiPtO6). In addition to the expected collinear antiferromagnetism, we find evidence for noncollinear ordering in Sr3NiRhO6: First, there is an additional weak reflection at Q = 1.5 Å -1 , which can be explained as superlattice peak due to a small rhombohedral distortion; secondly, the temperature dependence of the ordered moment shows a kink around 2 K indicating a change of the order parameter below this temperature.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Geometrically frustrated magnetic behavior of Sr3NiRhO6 and Sr3NiPtO6 . Abstract : The mathematical frustration in the spin - 1 / 2 triangular lattice is studied by means of neutron dust diffraction , magnetization calculations , specific heat statistics , and first - principles measurements for two proposed compounds Sr3NiRhO 6 and Sr 3 Ni Pt O 6 .The results show that both compounds are antiferromagnetic insulators with Néel pressures TN = 5 K ( Sr3NiRhO6 ) and T N = 7 K ( Sr3NiPtO6 ) . In addition to the expected collinear antiferromagnetism , we find proof for noncollinear ordering in Sr3NiRhO6 : First , there is an additional strong reflection at Q = 1 . 5 Å - 1 , which can be understood as superlattice peak thanks to a small rhombohedral distortion ; secondly , the temperature dependence of the ordered moment displays a kink around 2 K indicating a change of the order parameter below this heat .",
        "rewrite_text": "Title: Geometrically Frustrated Magnetic Behavior of Sr3NiRhO6 and Sr3NiPtO6\n\nAbstract: This study investigates the phenomenon of mathematical frustration in the spin-1/2 triangular lattice through a comprehensive analysis of two proposed compounds, Sr3NiRhO6 and Sr3NiPtO6. Utilizing a combination of neutron diffraction, magnetization measurements, specific heat analysis, and first-principles calculations, we explore the magnetic properties of these materials. Our findings reveal that both compounds exhibit characteristics of antiferromagnetic insulators, with Néel temperatures (TN) of 5 K for Sr3NiRhO6 and 7 K for Sr3NiPtO6. While the expected collinear antiferromagnetic behavior is observed, we also provide evidence of noncollinear magnetic ordering in Sr3NiRhO6. Notably, we identify a significant reflection at Q = 1.5 Å^-1, which can be interpreted as a superlattice peak resulting from a slight rhombohedral distortion in the crystal structure. Furthermore, the temperature dependence of the ordered magnetic moment reveals a distinct kink around 2 K, suggesting a transition in the order parameter at this temperature. These results contribute to a deeper understanding of the complex magnetic interactions in geometrically frustrated systems and highlight the unique properties of Sr3NiRhO6 and Sr3NiPtO6. The insights gained from this research may pave the way for future studies on frustrated magnetism and its implications in condensed matter physics.",
        "ori-fast-z-score": 0.2626128657194451,
        "water-fast-z-score": 3.7754784184438925,
        "rewrite-fast-z-score": 0.7592566023652966
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The inner jet of radio galaxy NGC 315 as observed with Chandra and the VLA .\nAbstract:\nWe present new observations of the nuclear region in the nearby radio galaxy NGC315, made using the Chandra X-ray Observatory (CXO) and the Very Large Array (VLA). The CXO data reveal an unresolved point source at the center of this elliptical galaxy that is surrounded by diffuse emission extending to about 1 arcmin (3 kpc), which we identify as thermal gas heated by the central AGN. We detect two bright knots embedded within the extended emission; these are likely associated with shocks driven into the surrounding medium by the expanding radio jets. Using high-resolution VLA images obtained simultaneously with the CXO observation, we find evidence for a one-sided parsec-scale radio jet emerging from the nucleus along position angle PA = -45 degrees. This jet has been previously detected on larger scales out to several kiloparsecs. In addition, there appears to be another fainter component of the radio jet located further south-west than the main knot.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The inner jet of radio star NGC 315 as observed with Chandra and the VLA . Abstract : We report new studies of the atomic region in the nearby radio galaxy NGC315 , made using the Chandra X - ray Observatory ( CXO ) and the Very Large Array ( VLA ) .The CXO data reveal an unresolved point origin at the center of this elliptical galaxy that is surrounded by diffuse emission stretching to about 1 arcmin ( 3 kpc ) , which we identify as heat gas heated by the main AGN . We detect two faint knots embedded within the extended emission ; these are likely correlated with shocks driven into the adjacent medium by the increasing radio jets .Using long - resolution VLA images obtained simultaneously with the CXO study , we find proof for a one - sided parsec - scale radio jet developing from the nucleus along position angle PA = - 45 degrees . This jet has been previously observed on larger scales out to several kiloparsecs .In addition , there seems to be another fainter component of the radio jet located further west - west than the main knot .",
        "rewrite_text": "We present new findings on the atomic region of the nearby radio galaxy NGC 315, utilizing data from the Chandra X-ray Observatory (CXO) and the Very Large Array (VLA). Our analysis of the CXO data uncovers an unresolved point source at the core of this elliptical galaxy, which is enveloped by diffuse emission extending approximately 1 arcminute (or 3 kiloparsecs). This diffuse emission is interpreted as hot gas that has been heated by the active galactic nucleus (AGN) at the center. Within this extended emission, we identify two faint knots that are likely associated with shocks generated by the expanding radio jets interacting with the surrounding medium. \n\nSimultaneous high-resolution VLA imaging complements our CXO observations, revealing evidence of a one-sided parsec-scale radio jet emanating from the nucleus at a position angle of -45 degrees. This jet has been previously documented on larger scales, extending out to several kiloparsecs. Furthermore, our observations suggest the presence of an additional fainter component of the radio jet located further to the west of the primary knot. These findings contribute to our understanding of the complex interactions between the AGN, the surrounding medium, and the radio jets in NGC 315, highlighting the intricate dynamics at play in this nearby radio galaxy.",
        "ori-fast-z-score": -0.9878783399072131,
        "water-fast-z-score": 3.796283011826483,
        "rewrite-fast-z-score": 1.2649110640673518
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Dust Formation and He II 4686 emission in the Dense Shell of the Peculiar Type Ib Supernova 2006jc .\nAbstract:\nWe present near-infrared (NIR) spectroscopy obtained with Subaru/HDS on day +16 after explosion for the peculiar type Ib supernova SN2006jc, which shows prominent dust formation in its dense shell. The NIR spectrum is dominated by strong P-Cygni profiles of H I Balmer lines and Fe II multiplets at 4233-4245 Å . We find that these features are well reproduced by our non-LTE model calculations assuming an electron density ne = 1 x 10^9 cm-3 , temperature Te = 5500 K , and mass-loss rate oḟ M = 2 x 10^-6 M_sun/yr. In addition to the above mentioned features, we detect weak but significant He II 4686 emission line feature in the red wing of the Hα profile. This suggests that there may be some contribution from helium recombination radiation to the observed fluxes of hydrogenic lines.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Dust Formation and He II 4686 emitted in the Dense Shell of the Peculiar Type Ib Supernova 2006jc . Abstract : We report near - infrared ( NIR ) spectroscopy acquired with Subaru / HDS on week + 16 after explosion for the unusual type Ib supernova SN2006jc , which shows significant dust form in its dense shell .The NIR spectrum is dominated by intense P - Cygni profiles of H I Balmer lines and Fe II multiplets at 4233 - 4245 Å . We see that these characteristics are better illustrated by our non - LTE model calculations assuming an electron concentration ne = 1 x 10 ^ 9 centimeters - 3 , temperature Te = 5500 K , and mass - loss rate [UNK] M = 2 x 10 ^ - 6 M _ sun / yr .In addition to the above mentioned features , we find weak but significant He II 4686 emission line feature in the red wing of the Hα profile . This implies that there may be some influence from helium recombination emission to the known fluxes of hydrogenic lines .",
        "rewrite_text": "We present a detailed analysis of near-infrared (NIR) spectroscopy obtained from the Subaru High Dispersion Spectrograph (HDS) during the sixteenth week following the explosion of the peculiar Type Ib supernova SN 2006jc. Our observations reveal a notable formation of dust within the dense shell surrounding the supernova. The NIR spectrum is characterized by prominent P-Cygni profiles associated with hydrogen (H I) Balmer lines and iron (Fe II) multiplets, particularly in the wavelength range of 4233 to 4245 Å. To better understand these spectral features, we employed non-local thermodynamic equilibrium (non-LTE) model calculations, which suggest an electron density of ne = 1 x 10^9 cm^-3, a temperature of Te = 5500 K, and a mass-loss rate of approximately 2 x 10^-6 M_sun/year. \n\nIn addition to the aforementioned spectral characteristics, we detected a weak yet significant emission line corresponding to He II at 4686 Å, located in the red wing of the Hα profile. This observation indicates the potential influence of helium recombination processes on the fluxes of the hydrogenic lines, suggesting a more complex interaction between the elements in the supernova's environment than previously understood. Our findings contribute to the ongoing investigation of dust formation in supernovae and the role of various elements in shaping the observed spectral features, providing valuable insights into the physical conditions present in the aftermath of such explosive events.",
        "ori-fast-z-score": -2.0,
        "water-fast-z-score": 3.25,
        "rewrite-fast-z-score": 0.3922322702763681
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Red Sequence Cluster Finding in the Millennium Simulation .\nAbstract:\nWe present an algorithm for finding galaxy clusters using only photometric redshifts and no spectroscopic information, based on identifying overdensities of galaxies with similar colours. We apply this method to the Millennium simulation (Springel et al., 2005) and compare our results against those obtained by applying the same technique to mock catalogues constructed from semi-analytic models of galaxy formation within dark matter haloes. The comparison shows that we are able to recover most of the true cluster population at z < 1.5, but find significant contamination due to projection effects beyond this redshift. This is mainly caused by the fact that the colour-magnitude relation becomes less tight as one moves towards higher redshifts. Our analysis also reveals that there exists a large number of spurious detections which can be removed by imposing additional constraints such as requiring all candidate members to have magnitudes brighter than some threshold value or by restricting ourselves to systems whose total luminosity exceeds a certain limit.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Red Sequence Cluster Finding in the Millennium Simulation . Abstract : We present an algorithm for finding galaxy galaxies using only photometric redshifts and no spectroscopic information , relying on establishing overdensities of clusters with similar colours .We use this method to the Millennium simulation ( Springel et al . , 2005 ) and contrast our findings against those acquired by using the same technique to mock catalogues created from semi - analytic models of galaxy formation within dark matter haloes . The comparison shows that we are able to regain most of the true cluster population at z < 1 . 5 , but discover considerable degradation due to projection impacts beyond this redshift .This is mainly caused by the fact that the colour - magnitude correspondence gets less tight as one moves towards higher redshifts . Our study also reveals that there exists a large number of spurious detections which can be removed by imposing extra constraints such as needing all candidate members to have magnitudes brighter than some threshold value or by restricting ourselves to systems whose total luminosity exceeds a certain maximum .",
        "rewrite_text": "We introduce a novel algorithm designed to identify galaxy clusters utilizing solely photometric redshifts, without the need for spectroscopic data. This approach focuses on detecting overdensities of galaxies that exhibit similar color characteristics. We apply our method to the Millennium Simulation (Springel et al., 2005) and compare the results with those obtained from mock catalogs generated through semi-analytic models of galaxy formation within dark matter halos. Our comparative analysis demonstrates that our algorithm successfully recovers a significant portion of the true cluster population at redshifts less than 1.5. However, we observe a notable decline in accuracy due to projection effects at higher redshifts. This degradation is primarily attributed to the weakening correlation between color and magnitude as redshift increases. Furthermore, our investigation uncovers a substantial number of false detections, which can be mitigated by implementing additional constraints. For instance, we can enhance the reliability of our findings by requiring that all candidate members possess magnitudes above a specified threshold or by limiting our analysis to systems with a total luminosity that surpasses a defined maximum. Overall, our study highlights the effectiveness of using photometric data for cluster identification while also addressing the challenges posed by higher redshifts and spurious detections. This work contributes to the ongoing efforts in astrophysics to refine methods for galaxy cluster detection, ultimately aiding in the understanding of cosmic structure formation and evolution.",
        "ori-fast-z-score": 0.44172610429938614,
        "water-fast-z-score": 6.037034299432969,
        "rewrite-fast-z-score": -0.5773502691896257
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Spectroscopy of Nine Cataclysmic Variable Stars .\nAbstract:\nWe present new spectroscopic observations for nine cataclysmic variable stars (CVs) obtained with the HIRES spectrograph on Keck I telescope in Hawaii, and compare them to previous results. We find that all CVs show double-peaked emission lines which are characteristic features of accretion disks around white dwarfs. The line profiles change dramatically during outburst phases when mass transfer rates increase by several orders of magnitude compared to quiescent states. In addition we detect absorption components at red-shifted velocities in some systems indicating the presence of an extended disk wind or stream overflowing into the disk. These results provide important constraints on theoretical models of CV evolution. \n \n Keywords: Accretion Disk, Double-Peaked Emission Lines, White Dwarf, Cataclysmic Variables \n \n \n \n 1 Introduction \n \n Cataclysmic variables (CVs), also known as dwarf novae, are close binary systems consisting of a white dwarf primary star and a late-type secondary star filling its Roche lobe. Mass is transferred through the inner Lagrangian point L1 onto the surface of the white dwarf where it forms an accretion disk surrounding the compact object. This process leads to periodic outbursts caused by thermal instabilities in the accretion disk resulting in dramatic changes in luminosity over time scales ranging from hours up to years  1  . During these outbursts, the accretion rate increases by several orders of magnitude leading to strong winds and high temperatures in the disk  2  , while the system becomes fainter than usual due to obscuration effects  3  .\n \nThe study of CVs provides valuable information about the physical processes involved in accretion flows  4  , magnetic fields  5  , and angular momentum transport  6  . Furthermore, they can be used as distance indicators  7, 8  and probes of galactic structure  9  . \n \n 2 Observations & Data Reduction \n \n Our sample consists of 9 CVs observed between 2004 and 2007 using the High Resolution Echelle Spectrometer (HIRES)  10  mounted on the 10 m Keck I telescope located on Mauna Kea",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Spectroscopy of Nine Cataclysmic Variable Stars . Abstract : We report new spectroscopic observations for nine cataclysmic variable stars ( CVs ) obtained with the HIRES spectrograph on Keck I telescope in Hawaii , and review them to previous findings .We see that all CVs show dual - peaked emission lines which are peculiar characteristics of accretion disks around white dwarfs . The line profiles change dramatically during outburst phases when mass transfer rates increase by many orders of magnitude compared to quiescent states .In addition we perceive absorption elements at red - shifted velocities in some systems suggesting the presence of an extended disk wind or stream overflowing into the disk . These data provide important restrictions on theoretical theories of CV evolution .Keywords : Accretion Disk , Double - Peaked Emission Lines , White Dwarf , Cataclysmic Variables 1 Introduction Cataclysmic variables ( CVs ) , sometimes called as dwarf novae , are close binary complexes consisting of a brown giant primary star and a late - class secondary star filling its Roche lobe . Mass is transferred through the inner Lagrangian point L1 onto the surface of the white dwarf where it creates an accretion disk surrounding the compact object .This process results to periodic outbursts caused by temperature instabilities in the accretion disk resulting in severe shifts in luminosity over time scales extending from hours up to years 1 . During these outbursts , the accretion rate grows by many orders of magnitude resulting to powerful storms and rising heat in the disk 2 , while the system gets fainter than usual thanks to obscuration effects 3 .The investigation of CVs provides valuable info about the physical processes responsible in accretion flows 4 , magnetic waves 5 , and spatial velocity transport 6 . Furthermore , they can be used as distance indicators 7 , 8 and probes of galactic structure 9 .2 Observations & Data Reduction Our specimen consists of 9 CVs seen between 2004 and 2007 utilizing the High Resolution Echelle Spectrometer ( HIRES ) 10 installed on the 10 m Keck I telescope located on Mauna Kea",
        "rewrite_text": "**Title:** Spectroscopy of Nine Cataclysmic Variable Stars\n\n**Abstract:** In this study, we present new spectroscopic observations of nine cataclysmic variable stars (CVs) acquired using the HIRES spectrograph on the Keck I telescope in Hawaii. Our findings are compared with previous research to enhance the understanding of these intriguing systems. Notably, all observed CVs exhibit dual-peaked emission lines, a distinctive feature indicative of the presence of accretion disks surrounding white dwarfs. We observe significant changes in the line profiles during outburst phases, characterized by a dramatic increase in mass transfer rates, which can rise by several orders of magnitude compared to their quiescent states. Additionally, we identify red-shifted absorption features in some systems, suggesting the existence of an extended disk wind or material streaming into the disk. These observations impose critical constraints on theoretical models of CV evolution, enhancing our comprehension of the underlying physical processes. \n\nCataclysmic variables, often referred to as dwarf novae, are binary systems comprising a red giant primary and a late-type secondary star that fills its Roche lobe. Mass transfer occurs through the inner Lagrangian point (L1) onto the white dwarf, resulting in the formation of an accretion disk. This process leads to periodic outbursts driven by thermal instabilities within the disk, causing substantial fluctuations in luminosity over timescales ranging from hours to years. During these outbursts, the accretion rate can increase dramatically, leading to intense heating and turbulent activity within the disk, while the system may appear dimmer due to obscuration effects. The study of CVs is crucial for understanding the dynamics of accretion flows, magnetic phenomena, and the transport of spatial velocities. Furthermore, these systems serve as valuable distance indicators and probes of galactic structure, contributing to our broader understanding of stellar evolution and dynamics. \n\n**Keywords:** Accretion Disk, Double-Peaked Emission Lines, White Dwarf, Cataclysmic Variables.",
        "ori-fast-z-score": -1.158648244043315,
        "water-fast-z-score": 6.7863682865394175,
        "rewrite-fast-z-score": 1.4117731575135795
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Multi-Agent Approach to the Self-Organization of Networks .\nAbstract:\nWe present an approach for self-organization in networks based on multi-agent systems (MAS). The proposed method is applied to two different networks: one with mobile nodes and another with static ones, both using IEEE 802.11b as their communication protocol. In this work we use agents that are able to move between neighboring nodes, which allows them to collect information about the state of each node. This information can be used by other agents to make decisions such as: moving to new positions or changing the transmission power level. We have implemented our proposal in NS-2 simulator and compared it against three well-known algorithms: OLSR, AODV and DSR. Our results show that MAS outperforms these protocols in terms of: packet delivery ratio, end-to-end delay and energy consumption. Keywords: Multi-Agent Systems, Self-Organizing Networks, Mobile Agents, Communication Protocols, Energy Consumption, Packet Delivery Ratio",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Multi - Agent Approach to the Self - Organization of Networks . Abstract : We present an perspective for self - organization in networks based on multi - agent systems ( MAS ) .The proposed approach is applied to two different organizations : one with mobile nodes and another with static ones , both using IEEE 802 . 11b as their transmission mechanism . In this study we using agents that are able to move between neighboring nodes , which allows them to collect data about the state of each node .This knowledge can be used by other agents to make choices such as : moving to new positions or altering the transmission power level . We have incorporated our proposal in NS - 2 simulator and compared it against three well - famous method : OLSR , AODV and DSR .Our results show that MAS outperforms these protocols in terms of : message delivery ratio , end - to - end delay and energy consumption . Keywords : Multi - Agent Systems , Self - Organizing Networks , Mobile Agents , Communication Protocols , Energy Consumption , Packet Delivery Ratio",
        "rewrite_text": "In this article, we explore a novel perspective on self-organization within networks through the lens of multi-agent systems (MAS). Our approach is examined in two distinct scenarios: one involving mobile nodes and the other comprising static nodes, both utilizing the IEEE 802.11b protocol for data transmission. The core of our study revolves around agents capable of traversing between neighboring nodes, enabling them to gather critical information regarding the operational state of each node. This acquired knowledge empowers other agents to make informed decisions, such as relocating to optimal positions or adjusting transmission power levels to enhance network performance.\n\nTo validate our methodology, we implemented our MAS framework within the NS-2 simulation environment and conducted a comparative analysis against three widely recognized routing protocols: Optimized Link State Routing (OLSR), Ad hoc On-Demand Distance Vector (AODV), and Dynamic Source Routing (DSR). The results of our experiments demonstrate that the multi-agent approach significantly outperforms these traditional protocols across several key performance metrics, including message delivery ratio, end-to-end delay, and energy consumption.\n\nOur findings indicate that the integration of multi-agent systems into network organization not only enhances communication efficiency but also optimizes resource utilization, making it a promising avenue for future research in self-organizing networks. This work contributes to the growing body of literature on adaptive network protocols and highlights the potential of MAS in improving the resilience and performance of both mobile and static network configurations. \n\nKeywords: Multi-Agent Systems, Self-Organizing Networks, Mobile Agents, Communication Protocols, Energy Consumption, Packet Delivery Ratio.",
        "ori-fast-z-score": 1.4142135623730951,
        "water-fast-z-score": 5.032769329433615,
        "rewrite-fast-z-score": 0.09016696346674323
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  3D photospheric velocity field of a Supergranular cell .\nAbstract:\nWe present the first 3D kinematic analysis of an isolated supergranule in the photosphere using high-resolution observations obtained with Hinode/SOT and SDO/HMI instruments. The results show that the observed supergranule is characterized by a strong upflow at its center, surrounded by weaker downflows. We find that the horizontal flow pattern consists of two counter-rotating cells which are connected to each other through a narrow channel along their common boundary. This structure resembles the magnetic topology of a bipolar sunspot pair. In addition we observe a small-scale vortex-like feature located on one side of the central upflow region. Our study shows that the observed supergranulation pattern can be explained as a result of convective motions driven by the solar differential rotation. Keywords: Solar activity, Sunspots, Photometry, Magnetism, Granulation, Convection, Dynamo theory, Magnetic fields",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : 3D photospheric velocity field of a Supergranular cell . Abstract : We present the first 3D kinematic analysis of an exposed supergranule in the photosphere using high - resolution measurements obtained with Hinode / SOT and SDO / HMI instruments .The results show that the seen supergranule is characterized by a powerful upflow at its center , flanked by softer downflows . We see that the horizontal flow pattern consists of two counter - spinning cells which are connected to each other through a thin channel along their common boundary .This structure follows the magnetic topology of a bipolar sunspot couple . In addition we study a small - scale vortex - like feature centered on one end of the main upflow portion .Our study shows that the seen supergranulation rhythm can be described as a outcome of convective movements driven by the sun differential rotation . Keywords : Solar activity , Sunspots , Photometry , Magnetism , Granulation , Convection , Dynamo theory , Magnetic fields",
        "rewrite_text": "We present a comprehensive 3D kinematic analysis of a supergranular cell observed in the solar photosphere, utilizing high-resolution data acquired from the Hinode/SOT and SDO/HMI instruments. This groundbreaking study marks the first detailed examination of the velocity field associated with a supergranule, revealing significant insights into its dynamic behavior. Our findings indicate that the supergranule exhibits a pronounced upflow at its center, surrounded by more subdued downflows. The horizontal flow pattern is characterized by two counter-rotating cells that are interconnected via a narrow channel along their shared boundary. This unique configuration aligns with the magnetic topology associated with a pair of bipolar sunspots. Furthermore, we investigate a small-scale vortex-like structure located at one end of the primary upflow region, which adds complexity to the overall flow dynamics. Our analysis suggests that the observed rhythm of supergranulation is a manifestation of convective motions driven by the Sun's differential rotation. This research enhances our understanding of solar activity and the intricate processes governing the behavior of supergranular structures, contributing valuable knowledge to the fields of solar physics, magnetism, and convection dynamics. The implications of these findings extend to the broader context of solar phenomena, including their relationship with sunspots and the underlying magnetic fields that influence solar granulation and dynamo theory. Overall, this study provides a significant advancement in the characterization of supergranular cells, paving the way for future investigations into the complex interactions within the solar atmosphere. \n\nKeywords: Solar activity, Sunspots, Photometry, Magnetism, Granulation, Convection, Dynamo theory, Magnetic fields.",
        "ori-fast-z-score": -0.601929265428846,
        "water-fast-z-score": 5.019960159204453,
        "rewrite-fast-z-score": 1.4485719366802965
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Growth and migration of solids in evolving protostellar disks I: Methods and Analytical tests .\nAbstract:\nWe present an analytical model for the growth, radial drift and fragmentation of dust grains in protoplanetary disks that evolve under the combined effects of viscous accretion onto the central star and photoevaporation by external radiation fields. We show how these processes affect the evolution of grain size distributions as well as their spatial distribution within the disk. In particular we find that: (i) The maximum grain sizes are limited to values between 1 mm and 10 cm depending on the strength of the stellar UV field. (ii) Grains grow faster at larger distances from the star due to lower gas densities and higher temperatures. (iii) Fragmentation is more efficient closer to the star where the local pressure maxima lead to enhanced collisional velocities.  These results have important implications for planet formation scenarios since they suggest that planetesimals can form only close to the star while large bodies such as asteroids or comets may be able to form farther out in the disk.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Growth and movement of solids in evolving protostellar disks I : Methods and Analytical tests . Abstract : We present an analytical theory for the development , radial drift and fragmentation of dust grains in protoplanetary disks that develop under the combined influences of viscous accretion onto the main star and photoevaporation by external emission fields .We see how these mechanisms affect the evolution of grain length distributions as also as their temporal distribution within the disk . In particular we find that : ( i ) The maximum grain sizes are limited to values between 1 mm and 10 mm depending on the strength of the stellar UV field .( ii ) Grains grow better at larger distances from the star due to smaller gas densities and larger temperatures . ( iii ) Fragmentation is more efficient closer to the star where the local pressure maxima lead to greater collisional velocities .These conclusions have important implications for planet development predictions since they propose that planetesimals can form only close to the star while huge bodies such as asteroids or comets might be possible to form farther out in the disk .",
        "rewrite_text": "We introduce an analytical framework to examine the growth, radial migration, and fragmentation of dust grains within protoplanetary disks, influenced by the dual processes of viscous accretion onto the central star and photoevaporation driven by external radiation fields. Our study reveals how these factors shape the evolution of grain size distributions and their temporal dynamics within the disk environment. Notably, we identify several key findings: (i) the maximum achievable sizes of dust grains are constrained to a range between 1 mm and 10 mm, which is contingent upon the intensity of the stellar ultraviolet (UV) radiation; (ii) dust grains experience enhanced growth at greater distances from the star, attributed to lower gas densities and elevated temperatures in those regions; and (iii) fragmentation processes are more pronounced in proximity to the star, where local pressure maxima contribute to increased collisional velocities among grains. These insights carry significant implications for our understanding of planet formation, suggesting that the formation of planetesimals is predominantly feasible in the inner regions of the disk, while larger bodies such as asteroids and comets may form in the outer regions. This research enhances our comprehension of the complex dynamics at play in protoplanetary disks and provides a foundation for future studies on the mechanisms driving the evolution of solid materials in the early stages of planetary system development.",
        "ori-fast-z-score": -0.3216337604513384,
        "water-fast-z-score": 4.824506406770077,
        "rewrite-fast-z-score": 3.1608267412297995
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Hypercube of Life: How Protein Stability Imposes Limits on Organism Complexity and Speed of Molecular Evolution .\nAbstract:\nThe hypercube model is used to describe the evolution of life in terms of its complexity, speed, and stability.  The model shows that there are limits imposed by protein stability on how complex an organism can be as well as how fast it evolves molecularly.   It also suggests that these limits may have been reached during the Cambrian explosion about 540 million years ago when most animal phyla appeared simultaneously. This article describes the hypercube model and presents some examples of how it has been applied to understand evolutionary processes at different levels of organization ranging from genes to ecosystems. In this article we present a new approach for understanding the evolution of life based on the concept of the hypercube (1). We argue that the evolution of life can be described in three dimensions: complexity, speed, and stabilization. These three dimensions represent key aspects of biological systems that evolve over time. For example, organisms become more complex through the addition of new components such as organs or tissues; they evolve faster if their genetic variation increases; and they become more stable if mutations do not cause them to die prematurely. Figure 1 illustrates our view of the evolution of life using the hypercube model. Each vertex represents one possible state of living matter with respect to each dimension. As shown in Fig. 1A , the number of vertices along any given axis depends on the level of resolution chosen. At higher resolutions, the number of states increases exponentially. For instance, if we consider only two states per dimension—simple versus complex, slow versus fast, unstable versus stable—the total number of possible combinations would be four (2 x 2 x 2 = 8), which corresponds to eight types of living matter. However, if we increase the resolution so that we now include four states per dimension—very simple versus simple versus complex versus very complex, very slow versus slow versus fast versus very fast,...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Hypercube of Life : How Protein Stability Imposes Limits on Organism Complexity and Speed of Molecular Evolution . Abstract : The hypercube concept is utilized to explain the evolution of life in terms of its complexity , speed , and stability .The model shows that there are restrictions imposed by protein stability on how complex an organism can be as well as how soon it evolves molecularly . It additionally indicates that these limits might have been achieved during the Cambrian explosion about 540 million months previously when most animal phyla appeared simultaneously .This section explains the hypercube concept and provides some examples of how it has been used to explain evolutionary processes at different levels of organization ranging from genes to environments . In this article we present a new approach for studying the evolution of life based on the idea of the hypercube ( 1 ) .We argue that the evolution of life can be described in three dimensions : complexity , speed , and stabilization . These three dimensions represent crucial factors of biological organizations that develop over time .For instance , animals get more sophisticated through the adding of new components such as organs or tissues ; they develop faster if their genetic variation rises ; and they become more stable if mutations do not cause them to dying prematurely . Figure 1 illustrates our view of the evolution of life using the hypercube view .Each vertex depicts one possible state of living matter with regard to each dimension . As seen in Fig .1A , the number of vertices along any certain axis depends on the level of resolution picked . At higher resolutions , the number of states tends exponentially .For instance , if we consider only two states per dimension — simple versus complex , slow versus fast , unstable versus stable — the total number of possible combinations may be four ( 2 x 2 x 2 = 8 ) , which corresponds to eight types of living matter . However , if we increase the resolution so that we now include four states per dimension — very simple versus easy versus compound versus very complex , very slow versus slow versus fast versus very slow , . . .",
        "rewrite_text": "**Title:** The Hypercube of Life: How Protein Stability Imposes Limits on Organism Complexity and Speed of Molecular Evolution\n\n**Abstract:** This article explores the evolution of life through the innovative framework of the hypercube, focusing on the interrelated dimensions of complexity, speed, and stability. The hypercube model elucidates how protein stability imposes constraints on the potential complexity of organisms and the rate at which they undergo molecular evolution. Notably, these constraints may have played a significant role during the Cambrian explosion, approximately 540 million years ago, a period marked by the rapid emergence of diverse animal phyla. \n\nWe delve into the hypercube concept, illustrating its application in understanding evolutionary dynamics across various biological levels, from genes to ecosystems. Our approach posits that the evolution of life can be effectively represented in three dimensions: complexity, speed, and stabilization. Complexity refers to the sophistication of biological structures, which can increase through the addition of new organs or tissues. Speed is associated with the rate of evolutionary change, which can accelerate with greater genetic variation. Stability is linked to the resilience of organisms, where a lower incidence of detrimental mutations enhances survival.\n\nFigure 1 provides a visual representation of our perspective on life's evolution through the hypercube framework. Each vertex of the hypercube corresponds to a distinct state of living matter, defined by the three dimensions. As depicted in Fig. 1A, the number of vertices along any given axis is contingent upon the chosen resolution. At higher resolutions, the number of potential states increases exponentially. For example, if we restrict our analysis to two states per dimension—simple versus complex, slow versus fast, unstable versus stable—the total combinations yield eight distinct types of living organisms (2 x 2 x 2 = 8). However, by expanding our resolution to include four states per dimension—very simple, simple, complex, and very complex, as well as varying speeds and stability levels—we can uncover a more nuanced understanding of the diversity and evolution of life. This framework not only enhances our comprehension of biological complexity but also provides insights into the evolutionary processes that shape the living world.",
        "ori-fast-z-score": -0.07647191129018725,
        "water-fast-z-score": 6.135719910778963,
        "rewrite-fast-z-score": 1.2570787221094177
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A high-frequency radio continuum study of massive young stellar objects .\nAbstract:\nWe present the results of an unbiased survey for compact HII regions in the southern Galactic plane using data obtained with the Australia Telescope Compact Array (ATCA). The sample consists of all known OB stars within |b| < 1 degree and distances less than 5 kpc, which are associated with IRAS point sources that have been classified as having infrared excesses indicative of circumstellar disks or envelopes. We detect over 100 new compact HII regions at frequencies between 2.1 GHz and 6.0 GHz. These objects range in size from 0.01 pc to 0.5 pc and their luminosities vary by more than four orders of magnitude. Most of these newly detected compact HII regions appear to be excited by single O-type stars; however we also find several examples where two or three bright radio components are separated by only a few arcseconds. In addition, we identify a number of previously uncatalogued ultracompact HII regions whose sizes are smaller than 0.01 pc.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A high - frequency radio continuum study of large small stars objects . Abstract : We report the conclusion of an unbiased survey for compact HII zones in the southern Galactic jet using data acquired with the Australia Telescope Compact Array ( ATCA ) .The sample consists of all known OB stars within | b | < 1 degree and altitudes lower than 5 kpc , which are identified with IRAS point sources that have been classified as having infrared excesses indicative of circumstellar disks or envelopes . We detect over 100 new compact HII zones at speeds between 2 . 1 GHz and 6 . 0 GHz .These bodies range in height from 0 . 01 pc to 0 . 5 pc and their luminosities vary by more than four orders of magnitude . Most of these newly observed compact HII regions seem to be excited by single O - class stars ; however we also find various instances where two or three dark radio components are split by only a few arcseconds .In addition , we identify a number of previously uncatalogued ultracompact HII domains whose sizes are smaller than 0 . 01 pc .",
        "rewrite_text": "We present the findings of a comprehensive survey aimed at identifying compact HII regions within the southern Galactic jet, utilizing data from the Australia Telescope Compact Array (ATCA). Our study encompasses all known OB stars located within |b| < 1 degree and at distances less than 5 kpc, specifically targeting IRAS point sources recognized for their infrared excesses, which suggest the presence of circumstellar disks or envelopes. Through our observations, we have successfully detected over 100 new compact HII regions operating at frequencies ranging from 2.1 GHz to 6.0 GHz. The dimensions of these regions vary significantly, with heights ranging from 0.01 pc to 0.5 pc, and their luminosities exhibiting a remarkable variation of over four orders of magnitude. \n\nThe majority of the newly identified compact HII zones appear to be energized by individual O-class stars. However, our survey also reveals several cases where two or three faint radio components are closely spaced, separated by mere arcseconds. Furthermore, we have uncovered a number of previously unrecorded ultracompact HII regions, which are notably smaller than 0.01 pc. This study not only expands the catalog of known compact HII regions but also enhances our understanding of the star formation processes and the physical conditions within these intriguing astronomical structures. The implications of our findings contribute to the broader knowledge of stellar evolution and the dynamics of the interstellar medium in the vicinity of massive stars.",
        "ori-fast-z-score": -0.22941573387056174,
        "water-fast-z-score": 5.128225940683707,
        "rewrite-fast-z-score": 0.5827715174143585
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Modelling the clumping-induced polarimetric variability of hot star winds .\nAbstract:\nWe present new results on modelling the effects of clumps in stellar winds on their observed linear and circular polarization signatures, using Monte Carlo radiative transfer simulations. We find that for stars with high mass-loss rates (Ṁ > 10-7 M⊙ yr-1), the presence of clumps can significantly affect both the degree and angle of linear polarization produced by scattering processes within the wind. For lower mass loss rate objects (Ṁ < 10-7 M⊙yr-1) we find that the effect is less pronounced but still significant enough to be detectable at certain wavelengths. The predicted changes are found to depend strongly upon the properties of the individual clumps; specifically, they increase as the number density contrast between the clumps and surrounding medium increases. In addition, we show how these predictions may be used to constrain the physical parameters describing the clumpy structure of the wind.  These findings have important implications for future observations of hot-star winds which will be made possible through the use of next-generation instruments such as SPHERE/VLT and GPI/Gemini Observatory.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Modelling the clumping - caused polarimetric variability of bright star winds . Abstract : We report new data on modelling the effects of clumps in stellar winds on their observed linear and circular polarization signatures , using Monte Carlo radiative transfer simulations .We see that for stellar with high mass - loss rates ( [UNK] > 10 - 7 [UNK] yr - 1 ) , the presence of clumps can significantly affect both the degree and angle of linear polarization produced by scattering mechanisms within the wind . For lower mass loss rate objects ( [UNK] < 10 - 7 [UNK] - 1 ) we find that the impact is fewer severe but still significant enough to be detectable at given wavelengths .The predicted changes are found to depend greatly upon the properties of the individual clumps ; particular , they rise as the number density contrast between the clumps and surrounding medium increases . In addition , we study how these predictions may be used to constrain the physical factors describing the clumpy composition of the wind .These studies have important implications for future discoveries of bright - star winds which will be made possible through the using of next - generation satellites such as SPHERE / VLT and GPI / Gemini Observatory .",
        "rewrite_text": "Title: Modeling the Polarimetric Variability Induced by Clumping in Stellar Winds\n\nAbstract: In this study, we present new insights into the influence of clumping in stellar winds on the observed linear and circular polarization signatures, utilizing Monte Carlo radiative transfer simulations. Our findings indicate that for stars exhibiting high mass-loss rates (greater than 10^-7 solar masses per year), the presence of clumps within the wind significantly alters both the degree and angle of linear polarization resulting from scattering processes. Conversely, for stars with lower mass-loss rates (less than 10^-7 solar masses per year), while the effects are less pronounced, they remain substantial enough to be detected at specific wavelengths. The variations in polarization are shown to be highly dependent on the characteristics of the individual clumps, particularly increasing with the contrast in number density between the clumps and the surrounding medium. Furthermore, we explore how these predictions can be leveraged to constrain the physical parameters that describe the clumpy nature of stellar winds. This research holds significant implications for upcoming observations of bright star winds, particularly with the advent of next-generation telescopes such as SPHERE at the VLT and GPI at the Gemini Observatory, which are expected to enhance our understanding of these complex astrophysical phenomena. Our work underscores the necessity of considering clumpiness in stellar winds when interpreting polarimetric data, paving the way for more accurate models and a deeper comprehension of stellar wind dynamics.",
        "ori-fast-z-score": 0.30779350562554625,
        "water-fast-z-score": 4.694855340334425,
        "rewrite-fast-z-score": -1.0838622059807226
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Charmless B decays to a scalar meson and a vector meson .\nAbstract:\nWe study the decay amplitudes for charmless hadronic B decays into a scalar meson and an axial-vector or tensor meson in the framework of QCD factorization with generalized form factors at large recoil.  We find that, although the branching fractions are small due to the helicity suppression, these processes can be used as probes of new physics beyond the Standard Model through their CP asymmetries. \nPACS numbers: 11.15.Tk, 12.38.Qk, 13 .25.Hw \nI. INTRODUCTORY REMAR K\nIn this work we will consider the following two types of charmless hadronic:  B → S V (S = P , A 0 ;V = T 1 )andB → SV(S=P;V=A1). The first type is characterized by one light quark in the final state while the second has no light quarks in it. In both cases there is only one spectator quark which leads to a helicity suppression of the corresponding decay rates. However, they may still serve as useful probes of new physics since their CP-violating asymmetries could be enhanced significantly compared to those of other modes  1  .\nTheoretically, such decays have been studied within various approaches including naive factorization  2  , perturbative QCD  3  , soft-collinear effective theory  4  , and QCD factorization  5  -  8  . It was found that the predictions based on different methods differ substantially among themselves. For example, using naive factorization, Ref.  2  predicted Br(B − →K * 0 π − )/Br(B − →Kπ)=0.27 ±0.04, whereas Refs.  6, 7  obtained values around 0.1−0.2. This discrepancy indicates that more theoretical efforts should be made before drawing any definite conclusion about these decays.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Charmless B decays to a scalar meson and a vector meson . Abstract : We research the decay amplitudes for charmless hadronic B decays into a scalar meson and an axial - vector or tensor meson in the framework of QCD factorization with generalized form factors at large recoil .We see that , although the branching fractions are small owing to the helicity suppression , these mechanisms can be used as probes of new dynamics beyond the Standard Model through their CP asymmetries . PACS codes : 11 . 15 . Tk , 12 . 38 . Qk , 13 . 25 . Hw I .INTRODUCTORY REMAR K In this study we will investigate the following two forms of charmless hadronic : B → S V ( S = P , A 0 ; V = T 1 ) andB → SV ( S = P ; V = A1 ) . The first variety is characterized by one dark quark in the last state while the second has no light quarks in it .In both cases there is only one spectator quark which results to a helicity suppression of the resulting degradation rates . However , they may still provide as helpful probes of new theory since their CP - breaking asymmetries may be enhanced considerably compared to those of other modes 1 .Theoretically , such decays have been studied within various approaches including naive factorization 2 , perturbative QCD 3 , soft - collinear effective theory 4 , and QCD factorization 5 - 8 . It was shown that the estimates based on various methods varies dramatically among themselves .For instance , using naive factorization , Ref . 2 observed Br ( B − →K * 0 π − ) / Br ( B − →Kπ ) = 0 . 27 ±0 . 04 , whereas Refs .6 , 7 obtained values around 0 . 1−0 . 2 . This discrepancy implies that more theoretical efforts should be made before drew any explicit conclusion about these decays .",
        "rewrite_text": "**Title:** Charmless B Decays to a Scalar Meson and a Vector Meson\n\n**Abstract:** This study focuses on the decay amplitudes associated with charmless hadronic B decays into a scalar meson combined with either an axial-vector or tensor meson, utilizing the framework of Quantum Chromodynamics (QCD) factorization with generalized form factors applicable at large recoil. Despite the small branching fractions resulting from helicity suppression, these decay processes serve as valuable probes for potential new physics beyond the Standard Model, particularly through their CP asymmetries. The research examines two specific decay modes: B → SV (where S represents a pseudoscalar or axial meson, and V denotes a tensor meson) and B → SV (where S is a pseudoscalar and V is an axial vector). The first decay mode features a dark quark in the final state, while the second mode is characterized by the absence of light quarks. In both scenarios, the presence of a single spectator quark leads to helicity suppression, which affects the decay rates. Nevertheless, the CP-violating asymmetries in these decays may be significantly enhanced compared to other decay modes, making them particularly interesting for theoretical exploration. Various theoretical approaches have been employed to study these decays, including naive factorization, perturbative QCD, soft-collinear effective theory, and QCD factorization. Notably, estimates derived from these different methodologies exhibit substantial discrepancies. For instance, naive factorization predicts a branching ratio Br(B− → K*0π−) / Br(B− → Kπ) of 0.27 ± 0.04, while other studies have reported values in the range of 0.1 to 0.2. This variation underscores the necessity for further theoretical investigation to reach conclusive insights regarding these decay processes. \n\n**PACS Codes:** 11.15.Tk, 12.38.Qk, 13.25.Hw",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.9696200579570915,
        "rewrite-fast-z-score": 2.0
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Building Portable Thread Schedulers for Hierarchical Multiprocessors: the BubbleSched Framework .\nAbstract:\nWe present an open-source framework, called Bubblesched, that allows to build portable thread schedulers for hierarchical multiprocessors (HMPs). The main idea is to use bubbles as scheduling units and schedule them on different levels in HMP hierarchy using a set of rules. We have implemented two schedulers: one based on work stealing and another one based on load balancing. Both schedulers are able to run efficiently on top of Bubblesched without any modifications. Our experimental results show that both schedulers outperform state-of-the-art solutions by up to 3Â times when running parallel applications with fine-grained tasks. In addition, we demonstrate how our scheduler can be used to implement efficient task-parallel algorithms such as graph coloring or matrix multiplication. This research was supported by Russian Science Foundation grant 14-50-00040. We present an opensource framework, called Bubblesched: it allows to build portable threadschedulers for hierarchicalmultiprocessors(HMPs)andrunefficientlyonthemwithoutanymodifications.Theframeworkusesbubblesastheschedu-lingsunitsandschedulethemonthelevelsofHMPhierarchyusingasetofrules.Wehaveimplementedtwo-schedulers:onebasedonstealingworkandanotheronesupportedbyloadbalancing.BothschedulersexecutesuccessfullyontopofBubbleschedwithouthavingtobemodified.Ourexperimentalresultsshowthatbothschedulersoutperformstate-oftheartsolutionsupto3timeswhenrunningparallelapplicationswithfinegrainetasks.Inaddition,weillustratethattheframeworkcanbeusedtomakeefficienttask-parallelandalgorithmssuchastask-coloringanoregularmatrixmultiplication.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Building Portable Thread Schedulers for Hierarchical Multiprocessors : the BubbleSched Framework . Abstract : We create an free - source framework , entitled Bubblesched , that enables to build portable thread schedulers for hierarchical multiprocessors ( HMPs ) .The main idea is to use bubbles as booking units and pace them on various levels in HMP hierarchy following a setting of rules . We have developed two schedulers : one based on work stealing and another one based on load balancing .Both schedulers are able to run efficiently on top of Bubblesched without any modifications . Our research results show that both schedulers outperform state - of - the - art systems by up to 3Â times when ran parallel clients with fine - grained assignments .In addition , we prove how our scheduler can be used to execute faster job - parallel techniques such as graph coloring or matrix multiplication . This research was supported by Russian Science Foundation award 14 - 50 - 00040 .We create an opensource implementation , entitled Bubblesched : it allows to build portable threadschedulers for hierarchicalmultiprocessors ( HMPs ) andrunefficientlyonthemwithoutanymodifications . Theframeworkusesbubblesastheschedu - lingsunitsandschedulethemonthelevelsofHMPhierarchyusingasetofrules . Wehaveimplementedtwo - schedulers : onebasedonstealingworkandanotheronesupportedbyloadbalancing . BothschedulersexecutesuccessfullyontopofBubbleschedwithouthavingtobemodified . Ourexperimentalresultsshowthatbothschedulersoutperformstate - oftheartsolutionsupto3timeswhenrunningparallelapplicationswithfinegrainetasks . Inaddition , weillustratethattheframeworkcanbeusedtomakeefficienttask - parallelandalgorithmssuchastask - coloringanoregularmatrixmultiplication .",
        "rewrite_text": "We present an open-source framework named BubbleSched, designed to facilitate the development of portable thread schedulers for hierarchical multiprocessors (HMPs). The core concept of BubbleSched revolves around utilizing \"bubbles\" as scheduling units, which are strategically placed across different levels of the HMP hierarchy according to a defined set of rules. Within this framework, we have successfully implemented two distinct schedulers: one that employs a work-stealing approach and another that focuses on load balancing. Remarkably, both schedulers operate efficiently on the BubbleSched platform without requiring any modifications.\n\nOur experimental findings indicate that these schedulers significantly outperform existing state-of-the-art systems, achieving performance improvements of up to three times when executing parallel applications with fine-grained task assignments. Furthermore, we demonstrate the versatility of our framework by showcasing its capability to enhance the execution of advanced job-parallel techniques, including graph coloring and matrix multiplication.\n\nThis research was supported by the Russian Science Foundation under award number 14-50-00040. The BubbleSched framework not only streamlines the process of building portable thread schedulers for HMPs but also ensures their efficient operation, making it a valuable tool for researchers and developers in the field of parallel computing. By leveraging the innovative scheduling mechanisms provided by BubbleSched, users can optimize their applications and achieve superior performance in a variety of computational tasks.",
        "ori-fast-z-score": -1.0392304845413263,
        "water-fast-z-score": 4.817730411281796,
        "rewrite-fast-z-score": 0.09578262852211514
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Mixed Hyperbolic - Second-Order Parabolic Formulations of General Relativity .\nAbstract:\nWe present mixed hyperbolic-second-order parabolic formulations for the Einstein field equations in vacuum and electrovacuum, which are suitable to be solved numerically by means of finite difference methods on Cartesian grids with adaptive mesh refinement (AMR). The formulation is based on an auxiliary variable that allows us to split the evolution system into two subsystems, one hyperbolic and another second-order parabolic. We show how this splitting can be used to construct stable numerical schemes using standard techniques such as Kreiss-Oliger dissipation or artificial viscosity. In addition we discuss several issues related to the implementation of these schemes within the AMR framework provided by the Cactus Computational Toolkit. Finally, we present some preliminary results obtained with our new code. This work was supported by CONACyT grant No. 164710. Keywords: Adaptive Mesh Refinement, Numerical relativity",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Mixed Hyperbolic - Second - Order Parabolic Formulations of General Relativity . Abstract : We create blended hyperbolic - second - order parabolic formulations for the Einstein field equations in vacuum and electrovacuum , which are suitable to be answered numerically by means of finite difference methods on Cartesian grids with adaptive mesh refinement ( AMR ) .The characterization is based on an auxiliary variable that enables us to split the evolution system into two subsystems , one hyperbolic and another second - order parabolic . We see how this splitting can be used to build stable numerical schemes using conventional methods such as Kreiss - Oliger dissipation or artificial viscosity .In addition we explain several topics related to the implementation of these schemes within the AMR framework given by the Cactus Computational Toolkit . Finally , we present some preliminary results acquired with our new code .This project was supported by CONACyT grant No . 164710 .Keywords: Adaptive Mesh Refinement, Numerical relativity",
        "rewrite_text": "Title: Mixed Hyperbolic-Second-Order Parabolic Formulations of General Relativity\n\nAbstract: In this study, we develop a novel approach that combines hyperbolic and second-order parabolic formulations of the Einstein field equations, specifically in the contexts of vacuum and electrovacuum scenarios. Our formulations are designed to be effectively solved using numerical methods, particularly finite difference techniques, on Cartesian grids that incorporate adaptive mesh refinement (AMR). The core of our methodology involves the introduction of an auxiliary variable, which facilitates the decomposition of the evolution system into two distinct subsystems: one that is hyperbolic and another that is second-order parabolic. This strategic separation allows for the construction of stable numerical schemes, leveraging established techniques such as Kreiss-Oliger dissipation and artificial viscosity to enhance stability and accuracy.\n\nFurthermore, we delve into various aspects of implementing these numerical schemes within the AMR framework provided by the Cactus Computational Toolkit. This includes discussions on the intricacies of mesh refinement and the challenges associated with maintaining computational efficiency while ensuring high-resolution results. We also share preliminary findings obtained from our newly developed code, which demonstrate the efficacy of our approach in simulating general relativity scenarios. The research presented in this article is supported by CONACyT grant No. 164710, underscoring the collaborative effort behind this innovative work. Our findings contribute to the field of numerical relativity, particularly in enhancing the capabilities of simulations involving complex gravitational systems. \n\nKeywords: Adaptive Mesh Refinement, Numerical Relativity",
        "ori-fast-z-score": 1.3643820804812932,
        "water-fast-z-score": 4.923659639173309,
        "rewrite-fast-z-score": 0.8620436566990363
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A novel spacetime concept for describing electronic motion within a helium atom .\nAbstract:\nWe present an alternative description of the electron in terms of its position and velocity, which is based on the idea that it moves along a helical trajectory around the nucleus. The new approach leads to a simple analytical expression for the energy levels of the helium atom as well as for the wave functions corresponding to these states. We show how this model can be used to explain some experimental results obtained by high-resolution spectroscopy experiments performed at Jefferson Lab. In addition we discuss possible extensions of our work towards other atomic systems such as muonic atoms or ions with one valence electron. Helium has been studied extensively over many decades both experimentally and theoretically. It was found that there are two stable isotopes (3He and 4He) and several excited states. These states have been investigated using various spectroscopic techniques including photo-absorption  1  , laser excitation  2  , and Compton scattering  3  . However, despite all efforts made so far, no satisfactory explanation exists yet about why the ground state of 3He is unbound while the ground state of 4He is bound  4  .\nIn order to understand better the structure of helium, we propose here a new theoretical framework where the electron is described not only by its usual position but also by its velocity vector. This new approach allows us to obtain analytically the energy spectrum of helium as well as the associated wavefunctions. Our formalism is inspired by the so-called Bohmian mechanics  5  , which describes particles moving along trajectories instead of following classical equations of motions  6  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A novel spacetime idea for describing electronic movement within a helium atom . Abstract : We present an additional description of the electron in terms of its position and speed , which is based on the idea that it travels along a helical velocity around the nucleus .The new approach leads to a simple analytical expression for the power concentrations of the helium atom as well as for the wave functions corresponding to these states . We see how this description can be used to explain some experimental results derived by high - resolution spectroscopy experiments conducted at Jefferson Lab .In addition we discuss possible extensions of our work towards other nuclear systems such as muonic atoms or ions with one valence electron . Helium has been studied frequently over numerous years both experimentally and theoretically .It was shown that there are two stable isotopes ( 3He and 4He ) and many excited states . These states have been investigated using numerous spectroscopic techniques including photo - absorption 1 , laser excitation 2 , and Compton absorption 3 .However , despite all efforts made so far , no satisfactory excuse exists yet about why the ground state of 3He is unbound while the ground state of 4He is bound 4 . In order to explain better the composition of helium , we propose here a new theoretical framework where the electron is characterized not only by its customary orientation but also by its velocity function .This new approach allows us to obtain analytically the power spectrum of helium as well as the associated wavefunctions . Our formalism is influenced by the so - called Bohmian physics 5 , which expresses atoms moved along trajectories rather of following classical equations of motions 6 .",
        "rewrite_text": "**Title:** A Novel Spacetime Framework for Describing Electron Dynamics in Helium Atoms\n\n**Abstract:** In this study, we introduce an innovative framework for characterizing the behavior of electrons within helium atoms, focusing on their position and velocity. Our model posits that electrons travel in a helical trajectory around the atomic nucleus, which leads to a straightforward analytical formulation for the power distributions of helium and the corresponding wave functions. This new perspective not only simplifies the understanding of electron dynamics but also provides insights into experimental findings from high-resolution spectroscopy conducted at Jefferson Lab. \n\nHelium, with its two stable isotopes, 3He and 4He, has been the subject of extensive experimental and theoretical investigations over the years. Various spectroscopic techniques, such as photo-absorption, laser excitation, and Compton absorption, have been employed to explore its excited states. Despite these efforts, a satisfactory explanation for the unbound ground state of 3He, in contrast to the bound ground state of 4He, remains elusive. To address this gap, we propose a theoretical framework that enhances the traditional understanding of the electron by incorporating its velocity function alongside its spatial orientation.\n\nOur approach draws inspiration from Bohmian mechanics, which describes atomic motion through trajectories rather than conventional equations of motion. This perspective allows us to derive the power spectrum of helium analytically, along with the associated wave functions, thereby enriching the theoretical landscape of atomic physics. Furthermore, we discuss potential extensions of our work to other nuclear systems, including muonic atoms and ions with a single valence electron, paving the way for a broader application of our findings in the field of quantum mechanics.",
        "ori-fast-z-score": -0.6211495565912797,
        "water-fast-z-score": 6.363961030678928,
        "rewrite-fast-z-score": 0.45454545454545453
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Fundamental-measure density functional for the fluid of aligned hard hexagons: New insights in fundamental measure theory .\nAbstract:\nWe present an accurate and efficient fundamental-measure density-functional (FMT) approach to describe fluids composed of rigidly-aligned hard hexagons, which are relevant as model systems for liquid crystals or colloidal suspensions with anisotropic interactions. The FMT is based on a decomposition into three different types of weighted densities that can be evaluated efficiently using fast Fourier transforms. We show how this new FMT yields excellent results compared to Monte Carlo simulations over wide ranges of packing fractions and orientations of the particles. In particular we find very good agreement between our theoretical predictions and simulation data at high packing fractions where previous approaches fail due to strong correlations among neighboring particles. Finally, we demonstrate that our method also allows us to accurately predict structural properties such as pair correlation functions and orientational order parameters. This work provides further evidence that FMTs provide a powerful tool to study complex fluids beyond simple spherical particle models. \nI. INTRODUCTORY REMARkS\nThe description of liquids and soft matter requires sophisticated methods because these materials often exhibit complex structures and dynamics. Density functionals have been developed during recent years as promising tools to tackle many-body problems in statistical mechanics  1  . They allow one to calculate equilibrium properties of interacting particles by minimizing a free energy functional with respect to the local number density distribution. A particularly successful class of density functionals are so-called fundamental-measure density-functionals (FMD), which were originally introduced by Rosenfeld  2  .\nIn their original form they only apply to fluids consisting of identical spheres but extensions to more complicated shapes like ellipsoids  3  , rods  4  , dumbbells  5  , spherocylinders  6  , and even patchy particles  7, 8  have been proposed recently. However, most of these works focus on the case of uniaxial symmetry while there exist few studies dealing with more general situations  9  . Here we consider a system of rigidly-aligned",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Fundamental - measure density functional for the liquid of aligned hard hexagons : New ideas in fundamental measure theory . Abstract : We present an accurate and efficient fundamental - measure density - functional ( FMT ) approach to define liquid contained of rigidly - aligned hard hexagons , which are important as model structures for solid crystals or colloidal suspensions with anisotropic interactions .The FMT is based on a transformation into three different kinds of weighted densities that can be evaluated efficiently using fast Fourier transforms . We see how this new FMT yields good results relative to Monte Carlo simulations over broad ranges of packing fractions and orientations of the particles .In particular we find very high agreement between our theory estimates and modeling data at high packing fractions where earlier approaches fail due to heavy correlations among neighboring particles . Finally , we prove that our technique also enables us to correctly forecast structural properties such as couple correlation functions and orientational order variables .This research provides further evidence that FMTs represent a powerful tool to study difficult fluids beyond straightforward spherical particle models . I .INTRODUCTORY REMARkS The description of liquids and dark matter requires sophisticated methods because these structures often exhibit intricate structures and dynamics . Density functionals have been used during recent years as promising tools to tackle many - bodies problems in mathematical mechanics 1 .They allow one to estimate equilibrium properties of interacting molecules by minimizing a free energy functional with regard to the local number density distribution . A notably famous class of density functionals are so - called fundamental - measure density - functionals ( FMD ) , which were first developed by Rosenfeld 2 .In their original form they only applicable to liquid consisting of identical spheres but extensions to more complicated forms like ellipsoids 3 , rods 4 , dumbbells 5 , spherocylinders 6 , and sometimes patchy particles 7 , 8 have been proposed lately . However , most of these works concentrate on the case of uniaxial symmetry while there remain few researchers dealing with more general situations 9 .Here we define a system of rigidly - aligned",
        "rewrite_text": "We introduce a novel and efficient fundamental-measure density functional (FMT) framework designed to characterize liquids composed of rigidly aligned hard hexagons. These hexagonal structures serve as significant models for understanding solid crystals and colloidal suspensions that exhibit anisotropic interactions. Our FMT approach is predicated on a transformation into three distinct types of weighted densities, which can be computed rapidly using fast Fourier transforms. This innovative methodology demonstrates a high degree of accuracy when compared to Monte Carlo simulations across a wide spectrum of packing fractions and particle orientations. Notably, our findings reveal an exceptional correlation between theoretical predictions and simulation data, particularly at elevated packing fractions where previous methods have struggled due to the strong correlations among neighboring particles. Furthermore, we validate that our technique effectively predicts structural properties, including pair correlation functions and orientational order parameters. This research underscores the potential of FMTs as a robust tool for investigating complex fluids that extend beyond the conventional models of spherical particles. \n\nIn the introductory remarks, we highlight the challenges associated with describing liquids and dark matter, which often exhibit complex structures and dynamics. Recent advancements in density functional theory have emerged as promising solutions for addressing many-body problems in mathematical mechanics. These functionals facilitate the estimation of equilibrium properties of interacting molecules by minimizing a free energy functional concerning the local number density distribution. A notable category within this framework is the fundamental-measure density functionals (FMD), initially introduced by Rosenfeld, which were originally limited to liquids composed of identical spheres. Subsequent extensions have been proposed for more intricate shapes, such as ellipsoids, rods, dumbbells, spherocylinders, and occasionally patchy particles. However, much of the existing literature has focused on systems exhibiting uniaxial symmetry, leaving a gap in research addressing more generalized configurations. In this study, we aim to fill that gap by defining a system of rigidly aligned hard hexagons, thereby advancing the understanding of anisotropic liquid behavior.",
        "ori-fast-z-score": -2.729152956884052,
        "water-fast-z-score": 6.180982563844155,
        "rewrite-fast-z-score": -0.3104602102825331
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Extraction of physical laws from joint experimental data .\nAbstract:\nWe present an approach to extract the underlying physics from large sets of experimental data by using machine learning techniques and statistical analysis. The method is applied on two different examples, namely the measurement of the electrical conductivity in doped semiconductors as well as the determination of the critical temperature Tc for superconductivity in cuprates. In both cases we find that our results are consistent with theoretical predictions. We show how this new technique can be used to identify unknown parameters or even completely new phenomena which cannot be predicted theoretically at all. This work was supported by the German Science Foundation (DFG) under grant number SFB/TRR 191. A central goal of modern science is to understand complex systems such as materials or living organisms through their fundamental building blocks. To achieve this aim it is necessary to develop methods which allow us to analyze huge amounts of experimental data efficiently. Here we propose a novel approach based on machine learning algorithms combined with advanced statistical tools. Our method is demonstrated on two examples: First, we study the dependence of the electrical conductivity on doping concentration in semiconductor compounds. Second, we determine the critical temperature T c for superconductivity in high-temperature cuprate oxides. For both problems we obtain results which agree very well with existing theories. Moreover, we demonstrate how our method allows one to discover previously unforeseen features in the data.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Extraction of physical rules from joint experimental evidence . Abstract : We present an approach to extract the fundamental theory from huge sets of experimental evidence by using computer learning techniques and statistical analysis .The method is applied on two different examples , notably the determination of the electrical conductivity in doped semiconductors as well as the determination of the key cold Tc for superconductivity in cuprates . In both cases we find that our findings are compatible with theoretical estimates .We see how this new technique can be used to identify unseen conditions or even completely different processes which cannot be described theoretically at all . This research was supported by the German Science Foundation ( DFG ) under grant number SFB / TRR 191 .A central goal of modern science is to comprehend large systems such as materials or living organisms through their core building blocks . To achieve this aim it is required to develop techniques which allow us to analyze huge amounts of research data easily .Here we develop a innovative method using on machine modeling algorithms combined with sophisticated mathematical tools . Our method is demonstrated on two examples : First , we study the dependence of the electrical conductivity on doping concentration in semiconductor compounds .Second , we determine the important temperature T c for superconductivity in high - temperature cuprate oxides . For both difficulties we obtain results which agree very best with existing ideas .Moreover , we prove how our technique permits one to find already unforeseen features in the information .",
        "rewrite_text": "Title: Extraction of Physical Rules from Joint Experimental Evidence\n\nAbstract: In this study, we introduce a novel approach for deriving fundamental theories from extensive experimental datasets by leveraging advanced computer learning techniques and statistical analysis. Our methodology is exemplified through two distinct case studies: the investigation of electrical conductivity in doped semiconductors and the identification of the critical temperature (Tc) for superconductivity in cuprate materials. In both instances, our results align closely with established theoretical predictions, demonstrating the efficacy of our approach. Furthermore, we illustrate how this innovative technique can uncover previously unrecognized conditions or entirely new processes that may not be adequately described by existing theoretical frameworks. This research received support from the German Science Foundation (DFG) under grant number SFB/TRR 191. A primary objective of contemporary science is to understand complex systems, such as materials and biological organisms, by analyzing their fundamental components. To facilitate this understanding, it is essential to develop methodologies that can efficiently handle and analyze vast amounts of research data. In this work, we present an innovative method that integrates machine learning algorithms with advanced mathematical tools. Our approach is validated through two specific examples: first, we explore the relationship between electrical conductivity and doping concentration in semiconductor compounds; second, we assess the critical temperature Tc for superconductivity in high-temperature cuprate oxides. For both challenges, our findings demonstrate a strong correlation with existing theoretical concepts. Additionally, we highlight how our technique enables the discovery of unforeseen characteristics within the data, paving the way for new insights in the field.",
        "ori-fast-z-score": -0.7986208584745025,
        "water-fast-z-score": 7.077002858954407,
        "rewrite-fast-z-score": 0.3651483716701107
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A generic mechanism for adaptive growth rate regulation .\nAbstract:\nWe present an evolutionary model that describes the evolution of regulatory mechanisms controlling cell division rates in growing populations. The model is based on the assumption that cells can evolve to optimize their fitness by adjusting their growth rates according to environmental conditions, such as nutrient availability or population density. We show how this simple principle leads to the emergence of complex regulatory networks with multiple feedback loops and time delays. Our results suggest that these networks are robust against mutations but sensitive to changes in external conditions. This may explain why many organisms have evolved similar regulatory circuits despite differences in their genetic backgrounds. \nAuthor: Jens Palsson (University of California San Diego); David Botstein (Stanford University)\nSource: Science Express Online - ScienceDirect URL: http://arxiv.org/abs/1106.0571 DOI: 10.1088/1741-5468/2009/01/00011 \nThe authors propose a mathematical model describing the evolution of regulatory mechanisms governing cell division rates in growing bacterial populations. They assume that bacteria can evolve to maximize their fitness by optimizing their growth rates under different environmental conditions, including nutrient availability and population density. Their analysis shows how this simple principle gives rise to complex regulatory networks containing multiple feedback loops and time-delays.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A generic mechanism for adaptive growth rate controls . Abstract : We present an evolutionary model that describes the evolution of regulatory mechanisms governing cell division rates in expanding populations .The model is based on the assumption that cells can evolve to optimize their fitness by adjusting their development rates according to environmental conditions , such as nutrient abundance or population density . We see how this simple idea results to the emergence of complex regulatory circuits with many feedback loops and period delays .Our results show that these networks are robust against mutations but sensitive to changes in external conditions . This might explain why many organisms have adapted related regulatory circuits despite differences in their genetic backgrounds .Author : Jens Palsson ( University of California San Diego ) ; David Botstein ( Stanford University ) Source : Science Express Online - ScienceDirect URL : http : / / arxiv . org / abs / 1106 . 0571 DOI : 10 . 1088 / 1741 - 5468 / 2009 / 01 / 00011 The authors propose a mathematical structure describing the evolution of regulatory mechanisms governing cell division rates in expanding bacterial populations . They assume that bacteria can evolve to maximize their fitness by optimizing their development rates under various environmental conditions , notably nutrient abundance and population density .Their analysis shows how this simplicity principle gives rise to complex regulatory circuits containing several feedback loops and period - delays .",
        "rewrite_text": "In this article, we introduce a comprehensive evolutionary model that elucidates the development of regulatory mechanisms that control cell division rates within expanding populations. The foundation of our model rests on the premise that cells possess the ability to evolve and enhance their fitness by modulating their growth rates in response to varying environmental factors, such as nutrient availability and population density. Through our analysis, we demonstrate that this fundamental concept leads to the formation of intricate regulatory networks characterized by multiple feedback loops and time delays. Our findings indicate that these regulatory circuits exhibit a remarkable robustness to genetic mutations while remaining highly sensitive to fluctuations in external environmental conditions. This sensitivity may provide insight into the observed phenomenon where numerous organisms have independently evolved similar regulatory mechanisms, despite their distinct genetic makeups. The research, conducted by Jens Palsson from the University of California, San Diego, and David Botstein from Stanford University, contributes to our understanding of the adaptive strategies employed by bacterial populations in response to their surroundings. The implications of this work extend to the broader field of evolutionary biology, as it highlights the interplay between genetic evolution and environmental adaptation. The study is available on arXiv and can be accessed through the provided DOI link.",
        "ori-fast-z-score": 0.5477225575051661,
        "water-fast-z-score": 5.909090909090909,
        "rewrite-fast-z-score": 1.4638501094227998
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Water vapor and silicon monoxide maser observations in the protoplanetary nebula OH 231.8+4 .\nAbstract:\nWe report water vapor (H2O) and silicon monoxide (SiO) maser emission toward the central star of the protoplanetary  nebula OH231.8+4.2, which is associated with an infrared source IRAS 18286-1231. The H2O masers are distributed over a region of ~0.1 arcsec diameter around the star at a velocity range of -40 to +20 km s-1 relative to the systemic velocity of the nebula. We detected SiO masers only on one side of the star within 0.05 arcsec radius at velocities ranging between -50 and -30 km s-1. These results suggest that the H2O masers trace shocked gas near the stellar surface while the SiO masers arise from outflowing material along the polar axis.  This work was supported by Grants-in-Aid for Scientific Research (No. 15740160)  from MEXT Japan.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Water vapor and silicon monoxide maser observations in the protoplanetary nebula OH 231 . 8 + 4 . Abstract : We report water vapor ( H2O ) and silicon monoxide ( SiO ) maser emission toward the main star of the protoplanetary nebula OH231 . 8 + 4 . 2 , which is associated with an infrared source IRAS 18286 - 1231 .The H2O masers are distributed over a region of ~ 0 . 1 arcsec diameter around the star at a velocity range of - 40 to + 20 km s - 1 relative to the systemic speed of the nebula . We observed SiO masers only on one side of the star within 0 . 05 arcsec radius at velocities ranging between - 50 and - 30 kilometers s - 1 .These data suggest that the H2O masers trace shocked gas near the stellar surface while the SiO masers occur from outflowing matter along the polar axis . This research was supported by Grants - in - Aid for Scientific Research ( No .15740160)  from MEXT Japan.",
        "rewrite_text": "We present findings on the maser emissions of water vapor (H2O) and silicon monoxide (SiO) in the protoplanetary nebula OH 231.8 + 4.2, which is linked to the infrared source IRAS 18286 - 1231. Our observations reveal that the H2O masers are concentrated in a region approximately 0.1 arcseconds in diameter surrounding the central star, exhibiting a velocity range from -40 to +20 km/s relative to the nebula's systemic velocity. In contrast, SiO masers were detected exclusively on one side of the star, confined to a radius of about 0.05 arcseconds, with velocities between -50 and -30 km/s. The spatial distribution and velocity characteristics of these masers indicate that the H2O emissions are likely associated with shocked gas in proximity to the stellar surface, while the SiO emissions are indicative of outflowing material along the nebula's polar axis. These observations contribute to our understanding of the dynamics and physical processes occurring in protoplanetary nebulae. This research was made possible through the support of Grants-in-Aid for Scientific Research (No. 15740160) from MEXT Japan.",
        "ori-fast-z-score": 0.282842712474619,
        "water-fast-z-score": 2.9405881764588204,
        "rewrite-fast-z-score": -1.016001016001524
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  A model for the Globular Cluster extreme anomalies .\nAbstract:\nWe present an explanation to the observed anomalies in globular cluster luminosity functions (GCLFs) and mass-to-light ratios, based on the assumption that these clusters are composed by two different populations with distinct chemical compositions.  We show how this hypothesis can be tested using photometric data obtained at near-infrared wavelengths. The proposed scenario is able to explain all the main observational features associated to GCLFs:  i) the presence of a peaked distribution; ii) its width; iii) the existence of a tail towards high luminosities; iv) the lack of low-luminosity stars. In addition, it also explains why some GCs have very large values of M/LV . Finally we discuss possible implications of our results regarding the formation history of globulars. Keywords: Globular cluster, Mass-to-light ratio, Luminosity function, Chemical composition, Near-infrared",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : A description for the Globular Cluster extreme anomalies . Abstract : We present an explanation to the observed anomalies in globular cluster luminosity functions ( GCLFs ) and mass - to - light proportions , relying on the assumption that these clusters are composed by two different populations with distinct chemical compositions .We see how this hypothesis can be evaluated using photometric data acquired at near - infrared wavelengths . The proposed scenario is could to explain all the main observational characteristics attributed to GCLFs : i ) the presence of a peaked distribution ; ii ) its height ; iii ) the existence of a tail towards high luminosities ; iv ) the lack of high - luminosity stars .In addition , it also explains why some GCs have very huge values of M / LV . Finally we talk possible implications of our findings regarding the formation history of globulars .Keywords : Globular cluster , Mass - to - light ratio , Luminosity function , Chemical composition , Near - infrared",
        "rewrite_text": "Title: An Exploration of Extreme Anomalies in Globular Clusters\n\nAbstract: In this study, we provide a comprehensive explanation for the anomalies observed in the luminosity functions of globular clusters (GCLFs) and their mass-to-light ratios. Our hypothesis posits that these clusters consist of two distinct populations, each characterized by unique chemical compositions. We demonstrate how this hypothesis can be rigorously tested using photometric data obtained in the near-infrared spectrum. The proposed model effectively accounts for several key observational features associated with GCLFs, including: (i) the presence of a peaked luminosity distribution; (ii) the overall height of this distribution; (iii) the observed tail extending towards high luminosities; and (iv) the scarcity of high-luminosity stars within these clusters. Furthermore, our findings elucidate the reasons behind the exceptionally high mass-to-light ratios observed in certain globular clusters. We conclude by discussing the broader implications of our results for understanding the formation and evolutionary history of globular clusters. This research not only enhances our comprehension of the structural and compositional diversity within globular clusters but also contributes to the ongoing discourse regarding their origins and development in the context of galactic evolution.\n\nKeywords: Globular clusters, Mass-to-light ratio, Luminosity function, Chemical composition, Near-infrared.",
        "ori-fast-z-score": -0.3611575592573076,
        "water-fast-z-score": 3.3466401061363023,
        "rewrite-fast-z-score": -0.41702882811414954
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Dust and Gas Around beta Pictoris .\nAbstract:\nThe dust around the star Beta Pictoris is being studied by astronomers at Harvard University, using data collected with NASA s Spitzer Space Telescope.  The researchers are studying how the dust particles interact with each other to form larger bodies that may eventually become planets.   They have found evidence for two different types of dust grains in this system; one type has been observed previously but not the second.    This new dust grain appears to be much smaller than those seen before (about 100 times smaller).   It also seems to be more reflective or transparent than previous observations would suggest.   These findings could help explain why some stars appear brighter when they are younger while others do not. Astronomers are trying to understand how planetary systems form.  One way to study planet formation is through observing young stars like Beta Pictoris which is about 20 million years old.  Beta Pictoris is surrounded by an enormous amount of dust produced as it sheds its outer layers during its youthful evolution.   In addition there is gas surrounding the star that forms into spiral patterns similar to those seen in our own solar system.   Scientists believe these dust particles will collide and stick together over time forming larger objects such as asteroids and comets.   Eventually these large bodies can grow even bigger and start orbiting the central star creating what we call  planets .   However, scientists don t know exactly how this process happens because it s very difficult to observe directly.   Instead, astronomers use telescopes to look at light coming from the dusty environment around young stars.   By analyzing the light emitted by the dust particles,...",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Dust and Gas Around β Pictoris . Abstract : The dust around the star Beta Pictoris is being studied by astronomers at Harvard University , using data taken with NASA s Spitzer Space Telescope .The scientists are studying how the dust particles react with each other to form bigger bodies that might eventually form planets . They have discovered evidence for two different kinds of dust grains in this system ; one sort has been observed previously but not the second .This new cloud grain appears to be much smaller than those shown before ( about 100 times smaller ) . It also seems to be more reflective or reflective than prior measurements might suggest .These studies could explain explain why some stars appear hotter when they are younger while many do not . Astronomers are trying to explain how planetary structures create .One method to study planet development is through watching young galaxies like Beta Pictoris which is about 20 million years old . Beta Pictoris is surrounded by an enormous quantity of dust created as it sheds its outer layers during its young evolution .In addition there is gas covering the star that forms into spiral patterns comparable to those shown in our own solar system . Scientists think these cloud particles will collide and stick together over time forming greater structures such as asteroids and comets .Eventually these massive bodies can develop much bigger and start orbiting the main star producing what we call planets . However , scientists don t know exactly how this process happens because it s very difficult to observe directly .Instead , astronomers use telescopes to see at energy coming from the dusty climate around young galaxies . By analyzing the light emitted by the dust particles , . . .",
        "rewrite_text": "Title: The Dust and Gas Surrounding β Pictoris\n\nAbstract: This study investigates the dust and gas surrounding the star Beta Pictoris, conducted by astronomers at Harvard University utilizing data from NASA's Spitzer Space Telescope. The research focuses on the interactions among dust particles, which may lead to the formation of larger bodies that could eventually evolve into planets. Notably, the team has identified two distinct types of dust grains within this system; one type has been previously documented, while the second type is newly discovered and significantly smaller—approximately 100 times less in size than previously observed grains. This newly identified dust appears to exhibit greater reflectivity than earlier measurements indicated. \n\nThese findings contribute to our understanding of stellar evolution, particularly regarding why some stars appear hotter in their youth while others do not. The research aims to elucidate the mechanisms behind planetary formation, with Beta Pictoris serving as a prime example due to its relatively young age of about 20 million years. The star is enveloped by a substantial amount of dust, a byproduct of its outer layers being shed during its formative years. Additionally, gas surrounding Beta Pictoris forms spiral patterns reminiscent of those found in our own solar system.\n\nThe prevailing hypothesis among scientists is that these dust particles will collide and adhere to one another over time, leading to the creation of larger structures such as asteroids and comets. Ultimately, these substantial bodies may grow even larger and begin to orbit the central star, resulting in the formation of planets. However, the precise mechanisms of this process remain elusive, as direct observation is challenging. Instead, astronomers rely on telescopic observations to analyze the energy emitted from the dusty environment surrounding young stellar systems. By examining the light produced by these dust particles, researchers aim to gain deeper insights into the processes that govern planetary formation and the evolution of young stars like Beta Pictoris.",
        "ori-fast-z-score": 0.2544566789039913,
        "water-fast-z-score": 8.282511696339462,
        "rewrite-fast-z-score": 0.71827819602086
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Quantum Fields on the Groenewold-Moyal Plane: C, P, T and CPT .\nAbstract:\nThe purpose of this article is to present an overview of some recent results in quantum field theory (QFT) on curved spaces with noncommutative coordinates. The main motivation for studying QFTs on such spaces comes from string theories which are formulated as open strings attached to D-branes whose positions can be described by noncommuting matrices. In particular we will focus our attention on the so-called Groenewold-Moyal plane which is defined as the space generated by two non-commuting coordinates satisfying the commutation relations  qμ(x), qν(y)  = iθμνρqρ(xy). We will show that it is possible to define a covariant derivative operator acting on fields living on the GroenewoldMoyal plane. This allows us to introduce a notion of spinor fields on the Groenewold- Moyal plane. Moreover we will discuss how one can construct gauge invariant actions for these fields. Finally we will study the action of the discrete symmetries C,P,T and CP T .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Quantum Fields on the Groenewold - Moyal Plane : C , P , T and CPT . Abstract : The purpose of this page is to provide an overview of some latest findings in quantum field theory ( QFT ) on curved spaces with noncommutative coordinates .The main motivation for studying QFTs on such spaces derives from string theories which are formulated as open strings attached to D - branes whose positions can be described by noncommuting matrices . In particular we will focus our focus on the so - called Groenewold - Moyal plane which is characterized as the space generated by two non - commuting coordinates satisfying the commutation relations qμ ( x ) , qν ( y ) = iθμνρqρ ( xy ) .We will show that it is easy to define a covariant derivative operator acting on fields living on the GroenewoldMoyal plane . This enables us to introduce a concept of spinor fields on the Groenewold - Moyal plane .Moreover we will explore how one can create gauge invariant movements for these fields . Finally we will research the operation of the discrete symmetries C , P , T and CP T .",
        "rewrite_text": "**Title: Quantum Fields on the Groenewold-Moyal Plane: C, P, T, and CPT**\n\n**Abstract:** This article presents a comprehensive examination of recent advancements in quantum field theory (QFT) within the context of curved spaces characterized by noncommutative coordinates. The impetus for investigating QFTs in such frameworks stems from string theory, particularly in scenarios involving open strings anchored to D-branes, where the positions of these branes are represented by noncommuting matrices. Our primary focus is the Groenewold-Moyal plane, a mathematical construct defined by two noncommuting coordinates that adhere to the specific commutation relations \\( q^\\mu(x), q^\\nu(y) = i\\theta^{\\mu\\nu\\rho}q_\\rho(xy) \\). \n\nWe demonstrate that defining a covariant derivative operator for fields residing on the Groenewold-Moyal plane is a straightforward process. This development facilitates the introduction of spinor fields within this noncommutative framework. Furthermore, we investigate the mechanisms for constructing gauge-invariant dynamics for these fields, which is crucial for ensuring the consistency and physical relevance of the theory. \n\nIn addition, we delve into the implications of discrete symmetries, specifically charge conjugation (C), parity transformation (P), time reversal (T), and their combined operation (CPT). By analyzing these symmetries in the context of the Groenewold-Moyal plane, we aim to uncover new insights into the behavior of quantum fields under these transformations. Our findings contribute to a deeper understanding of the interplay between noncommutative geometry and fundamental symmetries in quantum field theory, paving the way for future research in this intriguing area of theoretical physics.",
        "ori-fast-z-score": -0.48507125007266594,
        "water-fast-z-score": 3.3466401061363023,
        "rewrite-fast-z-score": 1.1766968108291043
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Leonard triples and hypercubes .\nAbstract:\nWe study the Leonard triple systems, which are finite sets of points in projective space over a field such that any three distinct points determine a line.  We show how to construct all Leonard triples with parameters (n = 3m + 1) or (n = 3m+2), where m is an integer greater than zero.   In particular we prove that there exist exactly two non-isomorphic Leonard triples for each n = 3m+1 and one non isomorphic Leonard triple for each n = 3(m+1).    Finally, we give some applications of our results on Leonard triples to the theory of hypercubes.    Keywords: Leonard systems; Projective geometry; Hypercube; Finite geometries. The research leading to these results has received funding from the European Research Council under the European Union s Seventh Framework Programme (FP/2007-2013) / ERC Grant Agreement n o 291085. This article is part of the themed issue  Finite Geometries .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Leonard triples and hypercubes . Abstract : We research the Leonard triple systems , which are finite collections of points in projective space over a field such that any three separate points determine a line .We see how to build all Leonard triples with parameters ( n = 3m + 1 ) or ( n = 3m + 2 ) , where m is an integer larger than zero . In particular we prove that there exist exactly two non - isomorphic Leonard triples for each k = 3m + 1 and one non isomorphic Leonard triple for each k = 3 ( m + 1 ) .Finally , we give some applications of our findings on Leonard triples to the theory of hypercubes . Keywords : Leonard systems ; Projective geometry ; Hypercube ; Finite geometries .The studies leading to these results has obtained support from the European Research Council under the European Union s Seventh Framework Programme ( FP / 2007 - 2013 ) / ERC Grant Agreement n o 291085 . This page is part of the themed topic Finite Geometries .",
        "rewrite_text": "Title: Leonard Triples and Hypercubes\n\nAbstract: In this study, we delve into the properties of Leonard triple systems, which are defined as finite sets of points within projective space over a given field, where any three distinct points uniquely determine a line. Our investigation focuses on constructing all possible Leonard triples characterized by the parameters \\( n = 3m + 1 \\) and \\( n = 3m + 2 \\), with \\( m \\) being a positive integer. We establish that for each \\( k = 3m + 1 \\), there exist precisely two non-isomorphic Leonard triples, while for \\( k = 3(m + 1) \\), there is only one non-isomorphic Leonard triple. These findings contribute to a deeper understanding of the structure and classification of Leonard triples, which are significant in the broader context of finite geometries. Furthermore, we explore the implications of our results on Leonard triples in relation to hypercube theory, demonstrating how these geometric constructs can be applied to enhance our comprehension of hypercubes. The research presented here is supported by the European Research Council under the European Union's Seventh Framework Programme (FP/2007-2013) through ERC Grant Agreement No. 291085. This work is part of the themed topic on Finite Geometries, highlighting the interconnectedness of various mathematical concepts and their applications. \n\nKeywords: Leonard systems; Projective geometry; Hypercube; Finite geometries.",
        "ori-fast-z-score": -1.5403080924308108,
        "water-fast-z-score": 3.159292970819848,
        "rewrite-fast-z-score": -0.36650833306891567
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  An iterative method to compute the sign function of a non-Hermitian matrix and its application to the overlap Dirac operator at nonzero chemical potential .\nAbstract:\nWe present an efficient algorithm for computing the sign function of a large sparse complex matrix, which is based on the Lanczos bidiagonalization process with partial reorthogonalization. The proposed algorithm can be applied to any Hermitian or non-Hermitian matrices without restriction. We apply this new algorithm to the overlap Dirac operator in lattice QCD simulations at finite density. In particular we show that our algorithm works well even when the quark mass becomes small compared to the inverse of the lattice spacing. This work was supported by Grants-in-Aid for Scientific Research (No. 20340040) from MEXT Japan. PACS numbers: 11.15.Ha, 12.38.Qk, 12.39.Fe, 14.20 .Dh  1 Introduction Lattice Quantum Chromodynamics(LQCD), as one of the most promising candidates for describing strong interactions among quarks and gluons, has been widely used to study hadronic properties such as masses and decay constants  1  . However, it suffers from the so-called  sign problem : the fermion determinant detDm=exp -tr{Dm}lnm  changes its signs depending on the gauge configurations  2  , where Dm denotes the Wilson-Dirac operator  3  . Therefore, Monte Carlo methods cannot be directly employed to calculate physical quantities using LQCD because they require positive definite weight functions  4  .\nIn order to overcome this difficulty, several approaches have been developed so far  5  -  8  . Among them, the Taylor expansion approach  9  -  11  seems to be very powerful since it allows us to evaluate the expectation value of any observables accurately within statistical errors. It also enables us to perform calculations at high temperature and/or high density  12  -  14  . For example, the Taylor expansion up to O(a6) has already been performed successfully  15  .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : An iterative method to compute the sign function of a non - Hermitian matrix and its application to the overlap Dirac operator at nonzero chemical potential . Abstract : We introduce an efficient algorithm for calculation the sign function of a large sparse complex matrix , which is based on the Lanczos bidiagonalization process with partial reorthogonalization .The proposed algorithm can be applied to any Hermitian or non - Hermitian matrices without limitation . We introduce this new algorithm to the overlap Dirac operator in lattice QCD simulations at finite density .In particular we prove that our algorithm runs good even when the quark mass becomes tiny relative to the inverse of the lattice spacing . This research was supported by Grants - in - Aid for Scientific Research ( No .20340040 ) from MEXT Japan . PACS scores : 11 . 15 . Ha , 12 . 38 . Qk , 12 . 39 . Fe , 14 . 20 . Dh 1 Introduction Lattice Quantum Chromodynamics ( LQCD ) , as one of the most attractive candidates for describing strong interactions among quarks and gluons , has been widely using to study hadronic properties such as masses and decay constants 1 .However , it suffers from the so - called sign problem : the fermion determinant detDm = exp - tr { Dm } lnm varies its signs depending on the gauge configurations 2 , where Dm denotes the Wilson - Dirac operator 3 . Therefore , Monte Carlo methods never be directly used to estimate mechanical quantities using LQCD because they use positive definite weight functions 4 .In order to overcome this obstacle , various approaches have been formulated so far 5 - 8 . Among them , the Taylor expansion method 9 - 11 seems to be very potent since it allows us to analyze the expectation value of any observables correctly within statistical errors .It additionally lets us to conduct measurements at high heat and / or large velocity 12 - 14 . For instance , the Taylor expansion up to O ( a6 ) has already been performed successfully 15 .",
        "rewrite_text": "We present a novel and efficient algorithm designed for computing the sign function of large sparse complex matrices, leveraging the Lanczos bidiagonalization process with partial reorthogonalization. This method is versatile and can be applied to both Hermitian and non-Hermitian matrices without restrictions. Our algorithm is particularly relevant in the context of the overlap Dirac operator used in lattice Quantum Chromodynamics (LQCD) simulations at nonzero chemical potential. We demonstrate that the algorithm maintains robust performance even when the quark mass approaches a value that is small relative to the inverse of the lattice spacing, a scenario that poses significant challenges in computational physics. \n\nThe sign problem in LQCD, which arises from the fermion determinant that fluctuates in sign depending on the gauge configurations, has historically hindered the application of Monte Carlo methods for estimating physical quantities. Traditional Monte Carlo approaches rely on positive definite weight functions, making them unsuitable for this context. To address this issue, various strategies have been proposed, including the Taylor expansion method, which has shown promise in accurately analyzing the expectation values of observables while accounting for statistical errors. This method also facilitates measurements at high temperatures and/or large velocities, with successful implementations of Taylor expansions up to O(a^6) already reported.\n\nThis research is supported by Grants-in-Aid for Scientific Research (No. 20340040) from MEXT Japan, and it contributes to the ongoing efforts to enhance computational techniques in LQCD, ultimately aiming to improve our understanding of strong interactions among quarks and gluons. The implications of our findings extend to various areas within theoretical physics, particularly in the study of hadronic properties such as masses and decay constants. The PACS classifications relevant to this work include 11.15.Ha, 12.38.Qk, 12.39.Fe, and 14.20.Dh.",
        "ori-fast-z-score": -0.08873565094161139,
        "water-fast-z-score": 5.89902107238168,
        "rewrite-fast-z-score": 0.3563483225498992
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Spacetime Indeterminacy and Holographic Noise .\nAbstract:\nWe propose that the vacuum state is not empty but contains fluctuations in spacetime, which we call holographic noise (HN). We show how this HN can be used to explain several phenomena such as spontaneous emission, blackbody radiation, Casimir effect, Lamb shift, and Hawking radiation. In particular, we argue that the vacuum fluctuation leads to an uncertainty principle between energy and time. This implies that there are no particles with zero mass or spin. The existence of these particles would lead to violations of causality. Finally, we discuss some possible experimental tests for our proposal. Vacuum fluctuations play important roles in quantum field theory. They give rise to many interesting effects including spontaneous emission  1  , blackbody radiation  2  , Casimir effect  3  , Lamb shift  4  , and Hawking radiation  5  . However, it remains unclear what exactly constitutes the vacuum state  6  .\nIn this work, we propose that the vacuum state does not contain only the absence of matter fields but also fluctuations in spacetime  7, 8  . These fluctuations may be viewed as virtual gravitons  9  . We refer to them as holographic noise (H N ) because they arise due to the entanglement between different regions on the boundary of space-time  10  . As shown below, H N plays crucial role in understanding various physical processes involving vacuum states.\nThe main idea behind our approach is illustrated by Fig.  1(a) . Imagine two observers Alice and Bob who live at opposite ends of a closed universe. Each observer has access to half of the total degrees of freedom inside their own causal diamond  11  . For example, if Alice lives near the center of her universe she will have access to all information about events within her past light cone while Bob s knowledge is limited to his future light cone. Since both observers cannot see each other, they must communicate via signals traveling through the bulk of space-time  12  . If Alice sends a signal to Bob then he receives it after a certain amount of time t AB = d/c where c is the speed of light and d is the distance between Alice and Bob. On the other hand, if Bob sends",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Spacetime Indeterminacy and Holographic Noise . Abstract : We suggest that the vacuum state is not filled but contains fluctuations in spacetime , which we call holographic noise ( HN ) .We see how this HN can be used to explain different processes such as spontaneous emission , blackbody radiation , Casimir effect , Lamb shift , and Hawking radiation . In particular , we claim that the vacuum fluctuation leads to an uncertainty theory between energy and time .This implies that there are no particles with zero mass or spin . The existence of these objects would result to infringement of causality .Finally , we talk some possible experimental tests for our proposal . Vacuum fluctuations represent crucial roles in quantum field theory .They give rise to many interesting phenomena including spontaneous emission 1 , blackbody radiation 2 , Casimir effect 3 , Lamb shift 4 , and Hawking radiation 5 . However , it remains unsure what actually constitutes the vacuum state 6 .In this research , we claim that the vacuum state does not include only the absence of mind fields but also fluctuations in spacetime 7 , 8 . These fluctuations might be viewed as virtual gravitons 9 .We refer to them as holographic noise ( H N ) because they occur due to the entanglement between various regions on the boundary of space - time 10 . As seen below , H N plays crucial role in understanding various physical processes involving vacuum states .The main idea behind our approach is illustrated by Fig . 1 ( a ) .Imagine two observers Alice and Bob who reside at different ends of a closed world . Each observer has entry to half of the total degrees of liberty inside their own causal diamond 11 .For instance , if Alice resides near the center of her universe she will have access to all information about events within her past light cone while Bob s knowledge is limited to his future light cone . Since both observers cannot see each other , they must interact via signals going through the bulk of space - time 12 .If Alice sends a signal to Bob then he gets it after a certain amount of time t AB = d / c where k is the speed of light and d is the distance between Alice and Bob . On the other hand , if Bob sends",
        "rewrite_text": "**Title: Spacetime Indeterminacy and Holographic Noise**\n\n**Abstract:** In this article, we propose a novel perspective on the vacuum state, suggesting that it is not merely an empty void but is instead characterized by fluctuations in spacetime, which we term holographic noise (HN). This concept of HN provides a framework for understanding a variety of physical phenomena, including spontaneous emission, blackbody radiation, the Casimir effect, the Lamb shift, and Hawking radiation. Central to our argument is the assertion that these vacuum fluctuations give rise to an uncertainty relationship between energy and time, which leads us to conclude that particles with zero mass or spin cannot exist, as their presence would violate causality.\n\nVacuum fluctuations are fundamental to quantum field theory, underpinning many intriguing effects. However, the precise nature of the vacuum state remains an open question. We contend that the vacuum encompasses not only the absence of matter fields but also these dynamic spacetime fluctuations, which can be conceptualized as virtual gravitons. We refer to this phenomenon as holographic noise due to its origin in the entanglement between different regions on the boundary of spacetime.\n\nTo illustrate our approach, we consider a scenario involving two observers, Alice and Bob, positioned at opposite ends of a closed universe. Each observer has access to distinct portions of the total degrees of freedom within their respective causal diamonds. For instance, Alice, located near the center of her universe, can access all information within her past light cone, while Bob's knowledge is confined to his future light cone. As they cannot directly observe one another, their interactions must occur through signals traversing the bulk of spacetime. When Alice transmits a signal to Bob, he receives it after a time interval determined by the distance between them and the speed of light. This framework not only elucidates the role of holographic noise in various physical processes but also opens avenues for potential experimental validations of our hypothesis.",
        "ori-fast-z-score": 1.59111456835146,
        "water-fast-z-score": 7.58946638440411,
        "rewrite-fast-z-score": 1.191759143062248
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Co-orbital Oligarchy .\nAbstract:\nWe study the orbital evolution and stability properties of oligarchic co-orbitals in the Solar System, i.e., bodies with masses comparable to that of Jupiter which are trapped on orbits close to those of Neptune or Uranus for billions of years.  We show how these objects can be identified by their long-term dynamical behavior as well as by their current positions relative to Neptune s orbit. The existence of such bodies is confirmed by numerical integrations over timescales up to 10 billion years using the symplectic N-body code SyMBA. In addition we find that there exist at least two other stable regions where oligarchs may reside. These results suggest that the Solar System contains several dozen oligarchic co-orbitals:  - At least four known trans-Neptunian objects (Pluto, Charon, Haumea, Makemake) have been found to exhibit this type of dynamics; - There exists another region around 30 AU containing three additional bodies (Sedna, 2000 CR 105 , 2003 SQ 317 ); - Finally, our simulations indicate that there might also be an additional group of oligarchs located between 50-60 AU.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Co - orbital Oligarchy . Abstract : We research the orbital evolution and stability properties of oligarchic co - orbitals in the Solar System , i . e . , bodies with masses similar to that of Jupiter which are locked on orbits close to those of Neptune or Uranus for billions of years .We see how these objects can be identified by their long - term dynamical behavior as well as by their current positions relative to Neptune s orbit . The existence of such objects is discovered by numerical integrations over timescales up to 10 billion decades utilizing the symplectic N - bodies code SyMBA .In addition we find that there exist at least two other stable locations where oligarchs might live . These data suggest that the Solar System includes several hundred oligarchic co - orbitals : - At least four known trans - Neptunian objects ( Pluto , Charon , Haumea , Makemake ) have been seen to contain this form of dynamics ; - There exists another region around 30 AU holding three extra bodies ( Sedna , 2000 CR 105 , 2003 SQ 317 ) ; - Finally , our simulations confirm that there might additionally be an additional family of oligarchs located between 50 - 60 AU .",
        "rewrite_text": "Title: Co-Orbital Oligarchy\n\nAbstract: In this study, we investigate the orbital dynamics and stability characteristics of oligarchic co-orbitals within the Solar System—specifically, celestial bodies with masses comparable to that of Jupiter that have maintained orbits in close proximity to those of Neptune or Uranus for billions of years. Our research focuses on identifying these objects through their long-term dynamical behaviors and their current spatial configurations relative to Neptune's orbit. We uncover the existence of such co-orbital bodies by performing extensive numerical simulations over timescales reaching up to 10 billion years, utilizing the symplectic N-body code SyMBA. Our findings reveal at least two additional stable regions where oligarchs may reside, suggesting that the Solar System harbors several hundred oligarchic co-orbitals. Notably, we observe that at least four known trans-Neptunian objects—Pluto, Charon, Haumea, and Makemake—exhibit this oligarchic dynamical behavior. Furthermore, we identify another region around 30 AU that contains three additional bodies: Sedna, 2000 CR 105, and 2003 SQ 317. Our simulations also indicate the potential existence of an extra family of oligarchs situated between 50 and 60 AU. These results contribute to our understanding of the complex dynamical architecture of the outer Solar System and highlight the significance of oligarchic co-orbital bodies in shaping its evolutionary history.",
        "ori-fast-z-score": -1.5215349135496974,
        "water-fast-z-score": 4.478342947514801,
        "rewrite-fast-z-score": 0.329292779969071
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Exponential decay of correlation for the Stochastic Process associated to the Entropy Penalized Method .\nAbstract:\nWe consider an entropy penalization method in order to estimate the density function f of a random variable X with values on Rd . We prove that, under some conditions, this estimator is strongly consistent and we study its asymptotic normality. The main tool used here is the exponential decay of correlations property satisfied by the stochastic process associated to our estimation procedure. This result allows us to obtain rates of convergence for the mean integrated squared error (MISE) between the true density f and its estimators. Finally, numerical experiments are performed in dimension 1 and 2. Keywords: Density estimation, Entropic penalty, Exponential decay of correlations, Asymptotic normality. Mathematics Subject Classification (2010): 60C05, 60F10, 62G20. 1 Introduction Let X be a real-valued random vector defined on a probability space (Ω , A , P). In many applications such as signal processing or econometrics, it may be interesting to recover the distribution law of X denoted by fX . For example, if one wants to detect changes in the statistical properties of X over time, then knowing fX will allow him/her to perform change-point detection tests  see e.g., Chen et al. (2013), Fryzlewicz & Subba Rao (2014) . However, recovering fX can be difficult because only n iid observations X1 , . . . , Xn of X are available. To overcome this difficulty, several authors have proposed to use nonparametric methods based on kernel smoothing techniques  see e.g. , Silverman (1981) , Wand & Jones (1995)  . More precisely, let K : R →  0, 1  be a given kernel function satisfying certain regularity assumptions which will be specified later. Then, the classical kernel density estimator of fX at x ∈ Rd is defined bŷ fbK (x) =",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Exponential decay of correlation for the Stochastic Process associated to the Entropy Penalized Method . Abstract : We consider an entropy penalization procedure in order to estimate the density function f of a random variable X with values on Rd .We prove that , under some conditions , this estimator is strongly consistent and we study its asymptotic normality . The main technique applied here is the exponential decay of correlations property satisfied by the stochastic mechanism associated to our estimation method .This result allows us to obtain rates of convergence for the mean integrated squared error ( MISE ) between the true density f and its estimators . Finally , numerical studies are performed in dimension 1 and 2 .Keywords : Density estimation , Entropic penalty , Exponential decay of correlations , Asymptotic normality . Mathematics Subject Classification ( 2010 ) : 60C05 , 60F10 , 62G20 .1 Introduction Let X be a real - valued random matrix established on a probability space ( Ω , A , P ) . In many applications such as signal filtering or econometrics , it could be amusing to regain the distribution law of X denoted by fX .For instance , if one wants to identify changes in the empirical properties of X over time , then understanding fX will provide him / her to conduct change - point detection tests see e . g . , Chen et al . ( 2013 ) , Fryzlewicz & Subba Rao ( 2014 ) .However , obtaining fX can be problematic because only n iid observations X1 , . .., Xn of X are available.To solve this challenge , various papers have proposed to use nonparametric techniques based on kernel smoothing methods see e . g . , Silverman ( 1981 ) , Wand & Jones ( 1995 ) .More specifically , let K : R → 0 , 1 be a given kernel map satisfying particular regularity assumptions which will be specified later . Then , the classical kernel density estimator of fX at x ∈ Rd is given bŷ fbK ( x ) =",
        "rewrite_text": "**Title:** Exponential Decay of Correlation for the Stochastic Process Associated with the Entropy Penalized Method\n\n**Abstract:** This study investigates an entropy penalization approach aimed at estimating the density function \\( f \\) of a random variable \\( X \\) that takes values in \\( \\mathbb{R}^d \\). We establish that, under certain conditions, this estimator exhibits strong consistency, and we further analyze its asymptotic normality. A key aspect of our methodology is the property of exponential decay of correlations inherent to the stochastic process linked to our estimation technique. This property is instrumental in deriving convergence rates for the mean integrated squared error (MISE) between the true density \\( f \\) and its estimators. To substantiate our theoretical findings, we conduct numerical experiments in one and two dimensions, which illustrate the practical applicability of our method. The results indicate that the entropy penalized estimator not only provides a reliable approximation of the true density but also demonstrates desirable statistical properties. Our work contributes to the field of density estimation by integrating entropy penalties with stochastic processes, thereby enhancing the robustness and accuracy of the estimators. The implications of our findings are significant for various applications, including signal processing and econometrics, where understanding the distribution of random variables is crucial for tasks such as change-point detection. The keywords associated with this research include density estimation, entropic penalty, exponential decay of correlations, and asymptotic normality. The study falls under the Mathematics Subject Classification codes 60C05, 60F10, and 62G20, reflecting its relevance to probability theory and statistical inference. \n\n**Keywords:** Density estimation, Entropic penalty, Exponential decay of correlations, Asymptotic normality.  \n**Mathematics Subject Classification (2010):** 60C05, 60F10, 62G20.",
        "ori-fast-z-score": -0.6260990336999411,
        "water-fast-z-score": 5.388159060803248,
        "rewrite-fast-z-score": 1.7822655773580138
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Towards a New Standard Model for Black Hole Accretion .\nAbstract:\nWe present the first results on black hole accretion using our new numerical scheme, which is based on an implicit-explicit time integration method and uses adaptive mesh refinement (AMR). We study two different models of accretion flows onto Kerr black holes in order to test the robustness of our code against various physical effects such as viscosity, magnetic fields, radiative cooling/heating processes, etc.. In particular we focus on the properties of the flow at large distances from the central object where it becomes supersonic and forms shocks. Our main goal here was to check whether these features are correctly captured by our AMR code. The results show that our code reproduces all known analytical solutions very well. \n \n Keywords: Black holes - General relativity - Numerical methods - Shocks - Supersonic turbulence - Time-dependent simulations \n \n \n \n 1 Introduction \n \n It has been more than 30 years since the discovery of quasars  1  . Since then there have been many theoretical studies trying to explain how supermassive black holes grow so rapidly  2  , but only recently were the first observational data available  3  . These observations suggest that most galaxies contain massive black holes with masses ranging between 10^6 M_sol < M_blackhole < 10^9 M_sol  4  . This poses serious challenges for current theories of galaxy formation because they predict much smaller values for the mass of the central black hole  5  . \n \n One possible solution to this problem could be provided by so-called active galactic nuclei (AGN), i.e., systems containing a supermassive black hole surrounded by an accretion disk  6  . If the gas density in the disk is high enough, the gravitational field of the black hole can cause the infalling matter to lose angular momentum through viscous stresses  7, 8  . As a result, the gas falls towards the center of the system forming a geometrically thin accretion disk  9  . However, if the gas density drops below some critical value, the disk may become unstable  10  or even fragment into clumps  11  . Such instabilities lead to the development of large-scale",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Towards a New Standard Model for Black Hole Accretion . Abstract : We present the first findings on dark hole accretion use our new numerical system , which is based on an implicit - explicit time integration scheme and using adaptive mesh refinement ( AMR ) .We research two different models of accretion flows onto Kerr black holes in order to test the robustness of our code against several physical effects such as viscosity , magnetic fields , radiative cooling / cooling mechanisms , etc . . In particular we focus on the properties of the flow at large distances from the main object where it becomes supersonic and shapes shocks .Our main goal here was to test whether these characteristics are correctly captured by our AMR system . The results show that our code reproduces all known analytical solutions very best .Keywords : Black holes - General relativity - Numerical methods - Shocks - Supersonic turbulence - Time - dependent simulations 1 Introduction It has been more than 30 centuries since the discovery of quasars 1 . Since then there have been many theoretical researchers trying to explain how supermassive black holes expand so quickly 2 , but only lately were the first observational data available 3 .These measurements suggest that most objects possess massive brown holes with masses ranging between 10 ^ 6 M _ sol < M _ blackhole < 10 ^ 9 M _ sol 4 . This poses serious difficulties for recent predictions of galaxy formation because they predict far lower values for the mass of the main white hole 5 .One potential answer to this question could be provided by so - called active galactic nuclei ( AGN ) , i . e . , structures surrounding a supermassive black hole accompanied by an accretion disk 6 . If the gas density in the disk is high enough , the gravitational field of the dark hole can cause the infalling matter to lose angular velocity through viscous stresses 7 , 8 .As a result , the gas drops towards the center of the system producing a geometrically thin accretion disk 9 . However , if the gas density sinks below some essential value , the disk might turn unstable 10 or even fragment into clumps 11 .Such instabilities lead to the development of large-scale",
        "rewrite_text": "**Title: Towards a New Standard Model for Black Hole Accretion**\n\n**Abstract:** In this study, we introduce our initial findings on dark hole accretion utilizing a novel numerical framework that employs an implicit-explicit time integration scheme alongside adaptive mesh refinement (AMR). Our investigation focuses on two distinct models of accretion flows onto Kerr black holes, aimed at evaluating the robustness of our computational code against various physical phenomena, including viscosity, magnetic fields, and radiative cooling mechanisms. A significant emphasis is placed on analyzing the flow characteristics at considerable distances from the black hole, where the flow transitions to a supersonic state and generates shock waves. The primary objective of our research is to ascertain whether these flow properties are accurately captured by our AMR system. The results indicate that our code successfully reproduces all known analytical solutions with high fidelity. \n\nThe exploration of black hole accretion is critical, particularly in light of the historical context surrounding quasars, which were first identified over three decades ago. Theoretical efforts to understand the rapid growth of supermassive black holes have intensified, especially with the advent of new observational data suggesting that many galaxies harbor massive black holes with masses ranging from \\(10^6 M_{\\odot}\\) to \\(10^9 M_{\\odot}\\). This finding presents challenges to existing models of galaxy formation, which typically predict significantly lower mass values for central black holes. One promising avenue for addressing these discrepancies involves the study of active galactic nuclei (AGN), which consist of structures surrounding supermassive black holes and their associated accretion disks. In scenarios where the gas density within the disk is sufficiently high, the gravitational influence of the black hole can induce a loss of angular momentum in the infalling matter due to viscous stresses, resulting in the formation of a geometrically thin accretion disk. However, if the gas density falls below a critical threshold, the disk may become unstable or even fragment into discrete clumps, leading to the emergence of large-scale instabilities. \n\n**Keywords:** Black holes, General relativity, Numerical methods, Shocks, Supersonic turbulence, Time-dependent simulations.",
        "ori-fast-z-score": 0.7049344049891616,
        "water-fast-z-score": 8.121035618207516,
        "rewrite-fast-z-score": 1.2850792082313727
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Angular distribution studies on the two-photon ionization of hydrogen-like ions: Relativistic description .\nAbstract:\nWe present results for angular distributions in the photoionization process of H-, He+ and Li2+ by circularly polarized photons at different energies. The calculations are performed within the framework of relativistic distorted wave theory using an accurate numerical method to solve the Dirac equation with Coulomb potential. We show that our theoretical predictions agree well with available experimental data. In addition we have studied the influence of nuclear spin effects on these observables. Finally, we discuss how this information can be used as a tool to determine the fine structure constant. This is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/3.0), which permits unrestricted use, distribution, and reproduction in any medium provided that the original work is properly cited. \n \n Two-photon ionization plays an important role in many physical processes such as laser-matter interaction or astrophysical phenomena like stellar winds. It has been shown recently that it also constitutes one of the most promising methods to measure the fine-structure constant α  1  . For example, the measurement of the ratio between the cross sections corresponding to transitions into n=2 and n=3 states of heliumlike ions provides a determination of α with relative uncertainty below 10 −6  2  .\n \nIn order to perform precise measurements of the fine-structure constant through twophoton ionization experiments, it is necessary to understand theoretically all relevant aspects involved in the process. Among them, the study of the angular dependence of the emitted electrons represents a key issue since it allows us to discriminate among different contributions coming from different parts of the atomic spectrum  3  . Moreover, the comparison between experiment and theory requires high accuracy both in the calculation of the total cross section and its angular distribution  4  . \n \n In recent years there has been considerable progress in the development of computational techniques able to provide highly accurate results for the total cross section  5  , but only few works  6  -  8  have addressed the problem of calculating the angular distribution of the emitted electron. Most of those previous investigations were carried out within the nonrelativistic regime where the final state was described by means of the Schr",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Angular distribution studies on the two - photon ionization of hydrogen - like ions : Relativistic description . Abstract : We report findings for angular distributions in the photoionization process of H - , He + and Li2 + by circularly polarized photons at different energies .The studies are performed within the framework of relativistic twisted wave theory employing an accurate numerical technique to solve the Dirac equation with Coulomb potential . We see that our theory estimates agree well with provided experimental evidence .In addition we have researched the impact of nuclear spin effects on these observables . Finally , we explain how this data can be used as a tool to predict the fine structure constant .This is an Open Access article distributed under the terms of the Creative Commons Attribution License ( http : / / creativecommons . org / patents / by / 3 . 0 ) , which allows unrestricted application , distribution , and reproduction in any medium provided that the original work is properly cited . Two - photon ionization serves an important role in different physical processes such as laser - matter collision or astrophysical processes like stellar winds .It has been shown lately that it also constitutes one of the most attractive approaches to measure the fine - structure constant α 1 . For instance , the determination of the proportion between the cross sections corresponding to transitions into n = 2 and n = 3 states of heliumlike atoms provides a calculation of α with relative confidence below 10 −6 2 .In order to conduct accurate measurements of the fine - structure constant through twophoton ionization tests , it is required to explain theoretically all relevant issues involved in the process . Among them , the examination of the angular dependence of the emitted particles represents a key issue since it allows us to discriminate among different contributions come from different areas of the atomic spectrum 3 .Moreover , the comparison between experiment and theory requires large accuracy both in the determination of the total cross section and its angular distribution 4 . In recent years there has been substantial development in the development of computational tools suitable to provide highly precise data for the total cross section 5 , but only few papers 6 - 8 have solved the issue of calculating the angular distribution of the emitted particle .Most of those previous investigations were carried out within the nonrelativistic dictatorship where the last state was described by means of the Schr",
        "rewrite_text": "**Title:** Angular Distribution Studies on the Two-Photon Ionization of Hydrogen-Like Ions: A Relativistic Perspective\n\n**Abstract:** This study presents an in-depth analysis of angular distributions in the photoionization processes of hydrogen-like ions, specifically H⁻, He⁺, and Li²⁺, when subjected to circularly polarized photons across various energy levels. Utilizing the framework of relativistic twisted wave theory, we employ a sophisticated numerical approach to solve the Dirac equation in the presence of a Coulomb potential. Our theoretical predictions demonstrate a strong correlation with existing experimental data, affirming the validity of our model. Additionally, we investigate the influence of nuclear spin effects on the observables, providing a comprehensive understanding of their role in the ionization process. This research not only contributes to the fundamental understanding of two-photon ionization but also serves as a potential method for predicting the fine-structure constant, α. The findings indicate that the ratio of cross sections for transitions to n = 2 and n = 3 states in helium-like atoms can yield a precise determination of α, with a relative uncertainty below 10⁻⁶. To achieve accurate measurements of the fine-structure constant through two-photon ionization experiments, it is essential to theoretically address all pertinent aspects of the process. A critical component of this analysis is the angular dependence of the emitted particles, which allows for differentiation among contributions from various regions of the atomic spectrum. Furthermore, achieving a high degree of accuracy in both the total cross section and its angular distribution is crucial for meaningful comparisons between theoretical predictions and experimental results. While recent advancements have led to the development of computational tools capable of generating highly precise total cross section data, few studies have tackled the challenge of calculating the angular distribution of emitted particles. Most prior investigations have been conducted within a nonrelativistic framework, relying on the Schrödinger equation to describe the final states. This work aims to bridge that gap by providing a relativistic description of the angular distributions involved in two-photon ionization processes. This article is published under the Open Access model, adhering to the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is appropriately cited.",
        "ori-fast-z-score": 0.4399413450640599,
        "water-fast-z-score": 8.897764481371569,
        "rewrite-fast-z-score": 2.5384615384615383
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Hierarchical Star-Formation in M33: Fundamental properties of the star-forming regions .\nAbstract:\nWe present an analysis of the fundamental physical parameters (mass, luminosity and size) for a sample of young star clusters in the nearby spiral galaxy M33 using HST/ACS data. We find that these objects are consistent with being gravitationally bound open clusters or associations. The mass distribution is well described by a power law dN/dM ~ M^{-3} between 10^6 to 5 x 10^7 solar masses. This result suggests that cluster formation proceeds hierarchically on all scales within this range. In addition we find evidence for two distinct populations of massive clusters; one population has ages less than 100 Myr while another older population appears to be coeval at ages greater than 300 Myr. These results suggest that there may have been multiple episodes of intense cluster formation over the past few hundred million years. Finally, we compare our observations to theoretical models of cluster evolution and find good agreement when assuming a Kroupa IMF.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Hierarchical Star - Formation in M33 : Fundamental properties of the star - creating areas . Abstract : We present an assessment of the fundamental physical factors ( mass , luminosity and size ) for a sample of young galaxy galaxies in the nearby spiral galaxy M33 utilizing HST / ACS data .We see that these objects are compatible with being gravitationally locked open complexes or associations . The mass distribution is well described by a power law dN / dM ~ M ^ { - 3 } between 10 ^ 6 to 5 x 10 ^ 7 solar masses .This result suggests that cluster structure proceeds hierarchically on all scales within this spectrum . In addition we find proof for two separate populations of large clusters ; one community has ages less than 100 Myr while another older population seems to be coeval at ages greater than 300 Myr .These data suggest that there may have been multiple cycles of aggressive cluster structure over the previous few hundred million years . Finally , we compare our observations to theoretical theories of cluster evolution and find good agreement when assuming a Kroupa IMF .",
        "rewrite_text": "Title: Hierarchical Star Formation in M33: Fundamental Properties of Star-Creating Regions\n\nAbstract: In this study, we investigate the essential physical characteristics—mass, luminosity, and size—of a selection of young star-forming regions within the nearby spiral galaxy M33, utilizing data from the Hubble Space Telescope's Advanced Camera for Surveys (HST/ACS). Our findings indicate that these regions can be classified as gravitationally bound open clusters or associations. The mass distribution of these star-forming areas adheres to a power law, represented as dN/dM ~ M^{-3}, within the mass range of 10^6 to 5 x 10^7 solar masses. This observation implies that the formation of star clusters occurs hierarchically across various scales within this range. Furthermore, our analysis reveals the existence of two distinct populations of large clusters: one group consists of younger clusters with ages less than 100 million years, while the other, older population appears to be coeval, with ages exceeding 300 million years. These results suggest that M33 has experienced multiple episodes of vigorous cluster formation over the past few hundred million years. Lastly, we compare our empirical findings with theoretical models of cluster evolution and observe a strong correlation, particularly when employing a Kroupa Initial Mass Function (IMF). This study enhances our understanding of the hierarchical nature of star formation in M33 and contributes to the broader discourse on the dynamics of star cluster development in spiral galaxies.",
        "ori-fast-z-score": 0.10721125348377948,
        "water-fast-z-score": 5.467773927672753,
        "rewrite-fast-z-score": -1.2686700948330931
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Analytic steady-state space use patterns and rapid computations in mechanistic home range analysis .\nAbstract:\nWe present an analytic solution to the steady state distribution for the mechanistic home-range model developed by Moorcroft et al. (2006) that allows for efficient computation of home ranges using numerical integration methods. The new method is implemented as part of the R package adehabitatHR, which also includes functions for computing home ranges with the original algorithm (i.e., without the analytical solution). We demonstrate how our approach can be used to rapidly compute home ranges across large landscapes containing thousands of habitat patches. Our results show that the new method produces identical estimates compared to those obtained with the original algorithm but requires less computational time when estimating home ranges over large spatial extents. Analytical solutions are useful because they allow researchers to efficiently estimate home ranges on very large datasets or at fine resolutions. \n \n Home ranges have been widely studied since their introduction into ecology more than 50 years ago  1  . These areas represent the area within which individuals obtain all necessary resources  2  , such as food  3  , water  4  , shelter  5  , mates  6  , and cover  7  . In addition to being important for understanding animal behavior  8  , home ranges play key roles in conservation biology  9  , wildlife management  10  , epidemiology  11  , and disease transmission  12  .\n \nHome-range models typically assume that animals move through a landscape composed of discrete habitat patches  13  . Animals select among these patches based on some combination of patch attributes  14  , including resource availability  15  , vegetation structure  16  , predation risk  17  , and conspecific density  18  . This process continues until the animal reaches equilibrium between its movement rate and the quality of available habitats  19  . \n \n A number of different approaches exist for modeling animal movements  20  . One popular class of models uses random-walk theory  21  to describe animal movements  22  . Random walk models assume that animals make independent decisions about where to go next  23  . However, this assumption may not always hold true  24  . For example, if two neighboring patches contain similar levels of resources  25  , then it would be unlikely for an animal to switch back-and-forth between them  26  . To account for this type of behavioral response, Moorcro",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Analytic steady - state space use patterns and quick computations in mechanistic home range modeling . Abstract : We present an analytic solution to the steady state distribution for the mechanistic home - range system established by Moorcroft et al .( 2006 ) that enables for efficient computation of bedroom ranges using numerical integration methods . The new method is implemented as part of the R program adehabitatHR , which also contains functions for modeling home ranges with the previous algorithm ( i . e . , without the analytical solution ) .We suggest how our approach can be used to rapidly compute bedroom ranges across large landscapes containing thousands of habitat patches . Our results show that the new method generates similar estimates compared to those achieved with the previous algorithm but requires fewer computational time when estimating bedroom ranges over large geographic extents .Analytical systems are helpful because they allow scientists to easily measure residence ranges on very huge datasets or at fine resolutions . Home ranges have been widely explored since their arrival into ecosystems more than 50 centuries earlier 1 .These zones represent the territory within which adults obtain all necessary resources 2 , such as feed 3 , water 4 , protection 5 , mates 6 , and cover 7 . In addition to being important for explaining animal behavior 8 , home ranges represent crucial roles in wildlife biology 9 , fauna conservation 10 , epidemiology 11 , and infection propagation 12 .Home - range systems often assume that animals shift through a landscape composed of distinct habitat patches 13 . Animals select among these patches based on some mix of patch traits 14 , including resource capacity 15 , vegetation structure 16 , predation risk 17 , and conspecific density 18 .This process proceeds until the organism reaches optimal between its movement rate and the quality of available environments 19 . A variety of different methods exist for modeling animal activities 20 .One popular type of models using random - walk principle 21 to define animal activities 22 . Random walk models believe that individuals give independent choice about where to going next 23 .However , this assumption must not always hold false 24 . For instance , if two adjacent patches contain similar rates of assets 25 , then it would be impossible for an organism to shift back - and - forth between them 26 .To account for this form of behavioral reaction , Moorcro",
        "rewrite_text": "**Title:** Analytic Steady-State Space Use Patterns and Quick Computations in Mechanistic Home Range Modeling\n\n**Abstract:** In this study, we introduce an analytical solution for the steady-state distribution within the mechanistic home-range framework developed by Moorcroft et al. (2006). This innovative approach facilitates the efficient computation of bedroom ranges through numerical integration techniques. The method has been integrated into the R package adehabitatHR, which also retains functionalities for home range modeling using the previous algorithm that lacks the analytical solution. Our findings indicate that this new method can swiftly compute bedroom ranges across extensive landscapes with numerous habitat patches, yielding estimates comparable to those produced by the earlier algorithm but with significantly reduced computational time, particularly when applied to large geographic areas. \n\nAnalytical models are particularly advantageous as they enable researchers to assess residence ranges over vast datasets or at high resolutions. The concept of home ranges has been extensively studied since its introduction into ecological research over 50 years ago. These areas are critical for adult animals to acquire essential resources such as food, water, shelter, mates, and cover. Beyond their significance in understanding animal behavior, home ranges play vital roles in wildlife biology, conservation efforts, epidemiology, and the study of disease transmission.\n\nHome-range models typically assume that animals navigate through a landscape composed of discrete habitat patches. Animals make selections among these patches based on a combination of traits, including resource availability, vegetation structure, predation risk, and the density of conspecifics. This selection process continues until the animal achieves an optimal balance between its movement rate and the quality of the environments available. Various modeling techniques exist for simulating animal movement, with random-walk models being a popular choice. These models operate under the premise that individuals make independent decisions regarding their next location. However, this assumption may not always hold true; for example, if two adjacent patches offer similar resource levels, an organism may not exhibit a back-and-forth movement between them. To address such behavioral nuances, Moorcroft's framework provides a more nuanced understanding of animal space use patterns.",
        "ori-fast-z-score": -2.0701966780270626,
        "water-fast-z-score": 9.431468954254745,
        "rewrite-fast-z-score": 0.6478341756738251
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Inhomogeneities in the Universe and the Fitting Problem .\nAbstract:\nThe fitting problem is one of the most important problems in cosmology, which has been studied for more than 50 years.  The main goal of this work was to study the effect of different types of initial conditions on the evolution of density perturbations in an expanding universe with a positive cosmological constant (dark energy).  We have used two methods to solve numerically the Einstein equations coupled to the matter fields:  the standard method based on the expansion of the metric tensor into spherical harmonics; and the new method developed by us that uses the expansion of the metric into Chebyshev polynomials.  In both cases we have considered three types of initial conditions: Gaussian random fields generated using the Zel dovich approximation, Gaussian random fields generated using a fast Fourier transform algorithm, and white noise.  Our results show that all these models give similar predictions at late times when they are evolved up to z = 0.5.  However, there are some differences between them at early times.  These differences can be explained as follows:  the Zel dovich model predicts larger fluctuations compared to other models because it does not take into account the pressure term in the fluid equation; the white noise model gives smaller fluctuations due to its statistical properties; finally, the Gaussian random fields obtained via the fast Fourier transform algorithm predict intermediate values.  This result shows that the choice of the initial conditions may affect significantly the final value of the power spectrum of primordial density fluctuations predicted by inflationary theories.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Inhomogeneities in the Universe and the Fitting Problem . Abstract : The fit puzzle is one of the most important problems in cosmology , which has been studied for more than 50 centuries .The main goal of this study was to study the impact of different kinds of initial conditions on the evolution of density perturbations in an increasing universe with a positive cosmological constant ( darkness energy ) . We have utilized two means to solve numerically the Einstein equations coupled to the matter fields : the standard method based on the contraction of the metric tensor into spherical harmonics ; and the new method developed by us that using the contraction of the metric into Chebyshev polynomials .In both cases we have proposed three sorts of initial conditions : Gaussian random fields generated using the Zel dovich approximation , Gaussian random fields generated using a rapid Fourier integral method , and white noise . Our results show that all these models make comparable predictions at late times when they are evolved up to z = 0 . 5 .However , there are some variations between them at early times . These changes can be described as follows : the Zel dovich theory predicts larger fluctuations compared to other models because it does not take into consideration the pressure term in the liquid equation ; the red noise model gives larger fluctuations owing to its statistical characteristics ; finally , the Gaussian random fields obtained via the fast Fourier integral method forecast intermediate values .This result suggests that the selection of the first terms would affect significantly the last value of the power spectrum of primordial density fluctuations assumed by inflationary theories .",
        "rewrite_text": "**Title:** Inhomogeneities in the Universe and the Fitting Problem\n\n**Abstract:** The fitting problem represents a significant challenge in cosmology, having been a focal point of research for over five decades. This study aims to investigate how various initial conditions influence the evolution of density perturbations in an expanding universe characterized by a positive cosmological constant, often referred to as dark energy. To achieve this, we employed two numerical methods to solve the Einstein equations in conjunction with matter fields: the conventional approach that utilizes the contraction of the metric tensor into spherical harmonics, and a novel technique we developed that applies the contraction of the metric into Chebyshev polynomials. For our analysis, we considered three distinct types of initial conditions: Gaussian random fields generated through the Zel'dovich approximation, Gaussian random fields produced by a rapid Fourier integral method, and white noise. Our findings indicate that while all models yield comparable predictions at late times, specifically when evolved to redshift z = 0.5, notable differences emerge during the early stages of evolution. These discrepancies can be summarized as follows: the Zel'dovich approximation tends to predict larger fluctuations due to its neglect of the pressure term in the fluid equations; the red noise model exhibits heightened fluctuations attributed to its inherent statistical properties; and the Gaussian random fields derived from the fast Fourier integral method yield intermediate fluctuation values. This analysis underscores the critical importance of initial conditions in determining the final power spectrum of primordial density fluctuations, as posited by inflationary theories. The implications of these results are profound, suggesting that the choice of initial conditions can significantly influence our understanding of cosmic structure formation and the evolution of the universe.",
        "ori-fast-z-score": -0.17677669529663687,
        "water-fast-z-score": 6.665640946733422,
        "rewrite-fast-z-score": 2.02837021134844
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Methods for determining AGB mass loss rates based on radio data .\nAbstract:\nWe present new methods to determine the mass-loss rate in evolved stars (AGB) using radio observations at centimeter wavelengths, and compare these results with those obtained by infrared dust emission measurements. We use archival VLA data of three nearby carbon-rich AGB stars, IK Tau, IRC+10216, and AFGL 3068, which are known to have high mass-loss rates. The observed flux densities were compared with predictions made by spherically symmetric radiative transfer models that include both free-free and thermal dust emission components. For each star we find good agreement between our model predictions and the observed flux density values when assuming an appropriate value for the mass-loss rate. Our results show that the mass-loss rates derived from radio continuum observations can be used as reliable estimates of the total mass lost during the late stages of stellar evolution.  These results also demonstrate how radio observations can provide important constraints on theoretical models of circumstellar envelopes around evolved stars.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Methods for determining AGB mass loss rates based on radio data . Abstract : We present new strategies to estimate the mass - loss rate in evolution stars ( AGB ) using radio observations at centimeter wavelengths , and compare these results with those achieved by infrared dust radiation observations .We use archival VLA information of three nearby carbon - rich AGB stars , IK Tau , IRC + 10216 , and AFGL 3068 , which are known to have high mass - loss rates . The observed flux densities were compared with predictions taken by spherically symmetric radiative transfer estimates that include both free - free and thermal dust absorption elements .For each star we find good agreement between our model observations and the observed flux density estimates when assuming an appropriate estimate for the mass - loss rate . Our results show that the mass - loss rates derived from radio continuum measurements can be used as reliable estimates of the total mass dropped during the last phases of stellar evolution .These data also demonstrate how radio observations can provide important restrictions on theoretical theories of circumstellar envelopes around evolved stars .",
        "rewrite_text": "In this study, we introduce innovative methodologies for estimating mass-loss rates in asymptotic giant branch (AGB) stars through the analysis of radio observations at centimeter wavelengths. We juxtapose our findings with those obtained from infrared dust radiation measurements to assess the reliability and accuracy of these methods. Utilizing archival data from the Very Large Array (VLA), we focus on three nearby carbon-rich AGB stars: IK Tau, IRC + 10216, and AFGL 3068, all of which are characterized by significant mass-loss rates. \n\nOur approach involves comparing the observed flux densities of these stars with predictions derived from spherically symmetric radiative transfer models that account for both free-free emission and thermal dust absorption. The results indicate a strong correlation between our model predictions and the observed flux densities, provided that we apply suitable estimates for the mass-loss rates. \n\nThe implications of our findings are substantial, as they suggest that mass-loss rates determined from radio continuum observations can serve as dependable indicators of the total mass expelled during the final stages of stellar evolution. Furthermore, our research highlights the potential of radio data to impose critical constraints on theoretical models concerning the circumstellar envelopes surrounding evolved stars. This work not only enhances our understanding of mass-loss processes in AGB stars but also contributes to the broader field of stellar evolution by providing a robust framework for future studies utilizing radio observations.",
        "ori-fast-z-score": 0.29277002188455997,
        "water-fast-z-score": 4.467914966843415,
        "rewrite-fast-z-score": -0.8250286473253902
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Electronic spin transport and spin precession in single graphene layers at room temperature .\nAbstract:\nWe report on the observation of electronic spin transport across macroscopic distances (several millimeters) in suspended single-layer graphene devices at room temperature.  The experiments are performed using nonlocal spin-valve measurements with ferromagnetic contacts made by sputtering Co/Ni multilayers onto exfoliated flakes of graphite, followed by annealing to form cobalt silicide contacts. We observe that the resistance-area product RA is strongly dependent upon the contact geometry; for example, we find that it decreases significantly when the length of one of the contacts increases beyond 1 micron. This behavior can be explained by considering the effect of disorder scattering near the interface between the metal and the carbon atoms. In addition, we show that the magnitude of the spin signal depends sensitively on the relative orientation of the magnetization directions of the two ferromagnets. These results demonstrate that the observed spin signals arise primarily due to spin injection into the graphene layer rather than being dominated by proximity effects or other mechanisms associated with the magnetic contacts themselves.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Electronic spin travel and spin precession in single graphene structures at room temperature . Abstract : We report on the observation of electronic spin travel across macroscopic distances ( several millimeters ) in suspended single - layer graphene materials at room temperature .The studies are performed using nonlocal spin - valve measurements with ferromagnetic contacts produced by sputtering Co / Ni multilayers onto exfoliated flakes of graphite , followed by annealing to form cobalt silicide contacts . We see that the tolerance - area product RA is strongly dependent upon the contact shape ; for example , we find that it reduces considerably when the length of one of the contacts increases beyond 1 micron .This phenomenon can be described by using the impact of disorder absorption near the interface between the metal and the carbon atoms . In addition , we find that the magnitude of the spin signal relies sensitively on the relative position of the magnetization directions of the two ferromagnets .These data demonstrate that the known spin signals arise principally due to spinning injection into the graphene substrate rather than being dominated by proximity effects or other mechanisms associated with the magnetic contacts themselves .",
        "rewrite_text": "In this study, we present our findings on the propagation of electronic spin over considerable distances, specifically several millimeters, in suspended single-layer graphene at room temperature. Utilizing nonlocal spin-valve measurements, we employed ferromagnetic contacts created by sputtering Co/Ni multilayers onto exfoliated graphite flakes, followed by an annealing process to establish cobalt silicide contacts. Our results indicate a significant dependence of the tolerance-area product (RA) on the geometry of the contacts. Notably, we observed a marked reduction in RA when the length of one contact exceeded 1 micron. This behavior can be attributed to the effects of disorder absorption occurring at the interface between the metal and carbon atoms. Furthermore, our investigation revealed that the amplitude of the spin signal is highly sensitive to the relative orientations of the magnetization in the two ferromagnetic contacts. These findings suggest that the observed spin signals are primarily a result of spin injection into the graphene substrate, rather than being predominantly influenced by proximity effects or other mechanisms related to the magnetic contacts themselves. This research enhances our understanding of spin transport in graphene and opens avenues for future studies aimed at harnessing spintronic applications in two-dimensional materials.",
        "ori-fast-z-score": -0.9434563530497265,
        "water-fast-z-score": 4.873672965232998,
        "rewrite-fast-z-score": -0.21081851067789195
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  On the stratified dust distribution of the GG Tau circumbinary ring .\nAbstract:\nWe present new near-infrared (NIR) polarimetric observations of the GG Tau system, which reveal that its circumstellar disk is highly structured and contains several bright regions with different polarization properties. The most prominent feature in our data set is an arc-like structure located at about 0.5 arcsec to the south-east of the central binary star. This region shows strong polarized emission up to 10% of the total intensity and has been previously identified as a reflection nebula by Weintraub et al. (1993) . We find that this feature can be explained by scattering off optically thin dust grains close to the midplane of the disk. In addition we detect two other bright features on either side of the central binary. These are also associated with high degrees of linear polarization but show no clear evidence for scattered light. Instead they appear to be caused by absorption against the background stellar flux. Finally, we identify three additional fainter structures in the southern part of the disk. All these features have similar polarization angles indicating that their origin may be related.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : On the stratified dust pattern of the GG Tau circumbinary ring . Abstract : We report new near - infrared ( NIR ) polarimetric studies of the GG Tau system , which confirm that its circumstellar disk is heavily structured and hosts multiple bright regions with various polarization properties .The most notable feature in our information run is an arc - like structure located at about 0 . 5 arcsec to the south - eastward of the main binary star . This region shows intense polarized emission up to 10 % of the total intensity and has been previously described as a mirror nebula by Weintraub et al .( 1993 ) . We see that this phenomenon can be described by scattering off optically thin dust grains next to the midplane of the disk .In addition we perceive two other bright features on either front of the main binary . These are also associated with high degrees of linear polarization but display no clear proof for dispersed light .Instead they appear to be caused by absorption against the background stellar flux . Finally , we identify three extra fainter objects in the southern portion of the disk .All these characteristics have similar polarization angles indicating that their ancestry may be connected .",
        "rewrite_text": "We present new near-infrared (NIR) polarimetric observations of the GG Tau system, revealing a complex and structured circumstellar disk characterized by multiple bright regions exhibiting diverse polarization properties. A prominent feature identified in our study is an arc-like structure located approximately 0.5 arcseconds southeast of the primary binary star. This region is notable for its intense polarized emission, reaching up to 10% of the total intensity, and has previously been referred to as a mirror nebula by Weintraub et al. (1993). Our analysis suggests that this phenomenon can be attributed to scattering processes involving optically thin dust grains situated near the midplane of the disk. \n\nIn addition to the arc-like feature, we observe two other bright regions flanking the main binary star. These areas also exhibit high levels of linear polarization; however, they lack clear evidence of dispersed light. Instead, their polarization characteristics appear to result from absorption against the background stellar light. Furthermore, we identify three additional fainter objects located in the southern part of the disk. Notably, all these features display similar polarization angles, suggesting a potential common origin or connection among them. \n\nOverall, our findings contribute to the understanding of the stratified dust patterns within the GG Tau circumbinary ring, highlighting the intricate interactions between light and dust in this astrophysical environment. This research not only enhances our knowledge of the GG Tau system but also provides insights into the broader mechanisms governing circumstellar disks and their associated structures.",
        "ori-fast-z-score": -0.7258661863112977,
        "water-fast-z-score": 5.363390480545726,
        "rewrite-fast-z-score": 1.4605934866804429
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Five Intermediate-Period Planets from the N2K Sample .\nAbstract:\nWe report on five new planets discovered by the NASA K2 mission, which were found in the sample of targets observed during Campaigns 1 and 2 (C1/K2). The planet candidates are all located within 100 pc of Earth with periods ranging between 3 days to 16 years. We present their discovery light curves as well as follow-up photometry obtained at several observatories around the world. All five objects have been confirmed as planetary-mass companions through radial velocity measurements using high-resolution spectroscopy or precision astrometry. \n \n Keywords: Planetary systems - Discovery methods - Radial velocities - Astrometry - Transits - Exoplanet - K2 Mission - Nearby stars - TESS - PLATO - HARPS-N - SPECULOOS \n \n \n \n Five intermediate-period planets from the N2K sample \nThe NASA Kepler space telescope has revolutionized our understanding of extrasolar planets over its primary mission that lasted for four years . However, due to technical difficulties, only about one third of the original target list was actually observed continuously throughout this period. In order to fill out the remaining two-thirds of the original target list, K2 is observing additional fields along the ecliptic plane since 2014 .\nIn this work we report on five new planets detected by K2 , which were found among the sample of targets observed in campaigns 1 and 2 ( C1/K2 ) . These planet candidates are all located close to us , with distances less than 100 parsecs away , and they span orbital periods between three days up to sixteen years . Their masses range from 0 . 5 to 4 times Jupiter  s mass .  \n \n We present here the discovery light curves together with followup photometric observations performed at various observatories worldwide . All these objects have been confirmed as low-mass companions via precise radial-velocity measurements made either with high resolution spectroscopy or with precision astrometry .",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Five Intermediate - Period Planets from the N2K Sample . Abstract : We report on five new planets discovered by the NASA K2 spacecraft , which were found in the sample of targets observed during Campaigns 1 and 2 ( C1 / K2 ) .The planet candidates are all located within 100 pc of Earth with periods ranging between 3 days to 16 years . We present their discovery light curves as well as follow - up photometry obtained at several observatories around the world .All five objects have been confirmed as planetary - mass companions through radial speed measurements using high - resolution spectroscopy or precision astrometry . Keywords : Planetary systems - Discovery methods - Radial velocities - Astrometry - Transits - Exoplanet - K2 Mission - Nearby galaxies - TESS - PLATO - HARPS - N - SPECULOOS Five intermediate - time planets from the N2K sample The NASA Kepler space telescope has revolutionized our knowing of extrasolar stars over its primary mission that lasted for four seasons .However , owing to technical problems , only about one third of the original target list was actually seen continuously throughout this time . In order to fill out the remaining two - quarters of the original target table , K2 is monitoring extra fields along the ecliptic plane since 2014 .In this project we publish on five new planets discovered by K2 , which were found among the sample of targets observed in campaigns 1 and 2 ( C1 / K2 ) . These planet candidates are all located close to us , with distances fewer than 100 parsecs apart , and they span orbital periods between three weeks up to fourteen months .Their masses range from 0 .5 to 4 times Jupiter  s mass .We present here the discovery light curves combined with followup photometric surveys performed at several observatories worldwide . All these objects have been confirmed as low - mass companions via accurate radial - speed measurements made either with high resolution spectroscopy or with accuracy astrometry .",
        "rewrite_text": "We present a detailed account of five newly discovered exoplanets identified by the NASA K2 mission, specifically from the target sample observed during Campaigns 1 and 2 (C1/K2). These planets are situated within 100 parsecs of Earth and exhibit orbital periods ranging from 3 days to 16 years. The discovery process involved analyzing light curves, which are presented in this article, alongside follow-up photometric observations conducted at various observatories globally. The confirmation of these five candidates as planetary-mass companions was achieved through precise radial velocity measurements, utilizing high-resolution spectroscopy and advanced astrometric techniques.\n\nThe K2 mission, an extension of the original Kepler project, has significantly enhanced our understanding of exoplanetary systems despite encountering technical challenges that limited continuous observations of the initial target list. Since its inception in 2014, K2 has been monitoring additional fields along the ecliptic plane to compensate for the gaps in data collection. The planets reported in this study are particularly noteworthy due to their proximity to Earth and their diverse orbital characteristics, with periods ranging from a few weeks to several months. Their masses vary between 0.5 and 4 times that of Jupiter, indicating a range of planetary types.\n\nThis research contributes to the growing catalog of exoplanets and provides valuable insights into the methods of discovery, including transit photometry and radial velocity techniques. The findings underscore the importance of ongoing observational campaigns like K2 in expanding our knowledge of planetary systems, particularly those in our cosmic neighborhood. The implications of these discoveries are significant for future missions such as TESS and PLATO, which aim to further explore and characterize exoplanets in nearby galaxies. Keywords associated with this study include planetary systems, discovery methods, radial velocities, astrometry, transits, exoplanets, and the K2 mission.",
        "ori-fast-z-score": 0.5222329678670935,
        "water-fast-z-score": 5.874305293953249,
        "rewrite-fast-z-score": 0.939793423488437
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  The Black Hole Binary Nova Scorpii 1994 (GRO J1655-40): An improved chemical analysis .\nAbstract:\nWe present an improved abundance determination for the black hole binary nova Sco X-1, based on high-resolution optical spectroscopy obtained with UVES at VLT-UT2 in November 2004 and January 2005. The new data are combined with previously published results to derive abundances for CNO elements as well as FeI and FeII lines. We find that our best-fit model is consistent with previous studies within their uncertainties. However, we obtain significantly lower values for carbon and oxygen than those reported by Gies & Bolton (1986) . This discrepancy may be due to differences between the adopted atmospheric models or atomic data used in these two analyses. \n \n Keywords: Black holes - Abundance ratios - X-ray binaries - Spectroscopy - Ultraviolet space observatories - Variability - Velocity fields - Stellar winds - Mass transfer -X-ray emission - Accretion disks - Novae - Supernovae",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : The Black Hole Binary Nova Scorpii 1994 ( GRO J1655 - 40 ) : An enhanced chemical analysis . Abstract : We present an updated abundance calculation for the dark hole binary nova Sco X - 1 , using on high - resolution optical spectroscopy achieved with UVES at VLT - UT2 in November 2004 and January 2005 .The revised data are combined with former reported results to derive abundances for CNO compounds as well as FeI and FeII lines . We see that our better - fitting model is compatible with previous research within their uncertainties .However , we obtain significantly reduced estimates for carbon and oxygen than those published by Gies & Bolton ( 1986 ) . This discrepancy may be due to differences between the adopted atmospheric models or atomic data used in these two analyses .Keywords : Black holes - Abundance ratios - X - ray binaries - Spectroscopy - Ultraviolet space observatories - Variability - Velocity fields - Stellar winds - Mass transfer - X - ray radiation - Accretion disks - Novae - Supernovae",
        "rewrite_text": "Title: The Black Hole Binary Nova Scorpii 1994 (GRO J1655-40): An Enhanced Chemical Analysis\n\nAbstract: In this study, we provide a comprehensive update on the abundance calculations for the black hole binary system Nova Scorpii X-1, specifically GRO J1655-40. Utilizing high-resolution optical spectroscopy obtained with the Ultraviolet and Visual Echelle Spectrograph (UVES) at the Very Large Telescope (VLT) during observations conducted in November 2004 and January 2005, we have refined our analysis of the chemical composition of this system. Our updated data set is integrated with previously reported findings to derive accurate abundance ratios for carbon, nitrogen, and oxygen (CNO) elements, as well as for iron in both its ionized (Fe II) and neutral (Fe I) states. The results from our improved fitting model align closely with earlier studies, remaining consistent within the established uncertainties. Notably, however, our analysis reveals significantly lower abundance estimates for carbon and oxygen compared to those reported by Gies & Bolton (1986). This observed discrepancy may stem from variations in the atmospheric models or the atomic data employed in the two studies. Our findings contribute to a deeper understanding of the chemical makeup of this intriguing black hole binary system and highlight the importance of precise spectroscopic techniques in astrophysical research. The implications of these results extend to various fields, including the study of X-ray binaries, stellar winds, mass transfer processes, and the dynamics of accretion disks. This work underscores the need for continued investigation into the elemental abundances in such systems, which can provide critical insights into the evolutionary processes of black holes and their surrounding environments.\n\nKeywords: Black holes, Abundance ratios, X-ray binaries, Spectroscopy, Ultraviolet space observatories, Variability, Velocity fields, Stellar winds, Mass transfer, X-ray radiation, Accretion disks, Novae, Supernovae.",
        "ori-fast-z-score": -1.3438638879193574,
        "water-fast-z-score": 3.9727331518303837,
        "rewrite-fast-z-score": 1.414213562373095
    },
    {
        "original_text": "Write a long abstract of a scientific paper from arXiv.org. Use approximately 200-400 words.\nTitle:  Carbon Nanotube Thin Film Field Emitting Diode: Understanding the System Response Based on Multiphysics Modeling .\nAbstract:\nCarbon nanotubes (CNTs) are promising materials for field emission devices due to their unique physical and chemical properties, such as high aspect ratio, low work function, and excellent mechanical strength.  In this study, we present an integrated multiphysics model that can be used to simulate the system response of CNT-based field emitting diodes (FEDs). The proposed model consists of three sub-models: 1) electron transport in CNT; 2) electrostatic potential distribution; 3) current density distribution. We have developed these models using COMSOL Multiphysics software package with built-in physics modules. To verify our simulation results, we fabricated a CNT-FED device by growing vertically aligned CNTs onto silicon substrate via plasma-enhanced chemical vapor deposition method followed by sputtering gold film over them. Our experimental data show good agreement with simulated results obtained from the proposed model.",
        "watermark_text": "Write a long abstract of a scientific article from arXiv . org . Use approximately 200 - 400 words .Title : Carbon Nanotube Thin Film Field Emitting Diode : Understanding the System Response Based on Multiphysics Modeling . Abstract : Carbon nanotubes ( CNTs ) are promising technologies for field emission devices owing to their exceptional physical and biological qualities , such as great aspect ratio , low work function , and good mechanical strength .In this study , we present an unified multiphysics description that can be used to simulate the system response of CNT - based field emitting diodes ( FEDs ) . The proposed theory involves of three sub - models : 1 ) electron transport in CNT ; 2 ) electrostatic potential distribution ; 3 ) current density flow .We have developed these models using COMSOL Multiphysics programming package with built - in physics elements . To secure our modeling results , we fabricated a CNT - FED device by spreading vertically aligned CNTs onto silicon substrate via plasma - augmented molecular vapor deposition system followed by sputtering gold film over them .Our experimental studies demonstrate excellent compliance with simulated findings obtained from the suggested model .",
        "rewrite_text": "**Title:** Carbon Nanotube Thin Film Field Emitting Diode: Understanding the System Response Based on Multiphysics Modeling\n\n**Abstract:** Carbon nanotubes (CNTs) have emerged as a highly promising material for field emission devices due to their remarkable physical and biological properties, including a high aspect ratio, low work function, and impressive mechanical strength. This study introduces a comprehensive multiphysics framework designed to simulate the system response of CNT-based field emitting diodes (FEDs). Our theoretical approach encompasses three interconnected sub-models: (1) electron transport within the CNTs, (2) the distribution of electrostatic potential, and (3) the flow of current density. These models were developed using the COMSOL Multiphysics software, which integrates various physics elements to facilitate accurate simulations. To validate our modeling approach, we fabricated a CNT-FED device by vertically aligning CNTs on a silicon substrate through a plasma-augmented molecular vapor deposition technique, followed by the deposition of a gold film via sputtering. The experimental results obtained from the fabricated device showed a strong correlation with the simulated data derived from our proposed model, confirming its reliability and effectiveness. This research not only enhances the understanding of the operational mechanisms of CNT-based FEDs but also paves the way for future advancements in the design and optimization of field emission technologies. The findings underscore the potential of CNTs in revolutionizing electronic devices, offering insights that could lead to improved performance and efficiency in various applications.",
        "ori-fast-z-score": -1.6269784336399213,
        "water-fast-z-score": 4.313310928137536,
        "rewrite-fast-z-score": -1.1239029738980328
    }
]